- Decoupling input and output vocabularies
- Scaling input vocabulary size
- Utilizing multi-gram tokens for input
- Implementing Over-Encoding (OE) strategy
- Exploring log-linear relationship between vocabulary size and training loss
- Investigating performance impacts of larger tokenizers on different model sizes
- Designing efficient engineering solutions for large vocabularies
- Analyzing the effects of n-gram modeling on language modeling performance
- Comparing multi-token prediction methods with n-gram models
- Establishing independent scaling strategies for input and output vocabularies
- Addressing computational inefficiencies in tokenization
- Proposing tiled matrix parameterization for n-gram embedding
- Evaluating the trade-offs of larger output vocabularies for smaller models
- Bridging the gap between tokenizer design and model scaling
- Conducting synthetic experiments on context-free grammar modeling
- Defining the role of input and output vocabularies in model performance
- Investigating the impact of embedding parameters on training and inference costs
- Developing Over-Tokenized Transformers as a novel framework
- Analyzing the implications of vocabulary size on training efficiency and model performance
- Exploring hierarchical n-gram input vocabulary design