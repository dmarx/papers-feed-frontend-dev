- **Over-Tokenized Transformers (OTTs)**: Framework that decouples input and output vocabularies to enhance language modeling performance.
  
- **Key Concept - Over-Encoding (OE)**: Utilizes large hierarchical n-gram input vocabulary, improving model scalability without additional training costs.

- **Log-Linear Relationship**: Demonstrated between input vocabulary size and training loss; larger input vocabularies lead to consistent performance improvements.

- **Decoupling Input and Output Vocabularies**: Input vocabulary scaling positively impacts model performance, while larger output vocabularies can harm smaller models.

- **Performance Comparison**: 
  - Larger tokenizers (e.g., 3-gram) improve performance for larger models (e.g., 85M parameters) but can degrade performance for smaller models (e.g., 2.4M parameters).
  
- **Multi-Token Prediction (MTP)**: Shares principles with n-gram modeling; performance gains from MTP and over-encoded models are complementary.

- **Embedding Parameters**: Despite introducing many embedding parameters for OE, training and inference costs remain low due to sparse usage.

- **Efficient Engineering Solutions**: Proposed to mitigate computational and memory challenges from large vocabularies, with additional training overhead < 5%.

- **Tokenization Design**: Importance of tokenization in scaling laws; larger input vocabularies enhance training efficiency and model performance.

- **Matrix Decomposition for n-gram Vocabulary**: Approximates large embedding tables to manage impractical vocabulary sizes.

- **Formula for n-gram Input Token**: 
  \[
  x_{(-n)i} = f(x_i, x_{i-1}, \ldots, x_{i-n+1})
  \]
  where \( f \) is an index mapping function.

- **Bijective Mapping Function**: 
  \[
  f(z_1, \ldots, z_n) = \prod_{i=1}^{n} z_i p^{i-1}
  \]
  with \( p \geq V \) to ensure compactness.

- **Practical Implications**: Findings suggest that tokenization is critical for the evolution of large language models, influencing both design and performance.