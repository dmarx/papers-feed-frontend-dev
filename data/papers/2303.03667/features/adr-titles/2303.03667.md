- Decision to focus on reducing memory access in neural network operators
- Choice of depthwise convolution (DWConv) as a primary target for optimization
- Introduction of the partial convolution (PConv) as a novel operator
- Design considerations for PConv to maintain high FLOPS while reducing FLOPs
- Decision to append pointwise convolution (PWConv) to PConv for enhanced feature extraction
- Selection of FasterNet architecture as a family of networks built on PConv
- Trade-off analysis between accuracy and latency in network design
- Choice of evaluation metrics for performance comparison (e.g., FLOPs, FLOPS, latency)
- Decision to conduct extensive experiments across various devices (GPU, CPU, ARM)
- Assumptions regarding the redundancy in feature maps and its exploitation
- Design philosophy prioritizing computational efficiency over traditional FLOP reduction
- Decision to maintain a balance between model complexity and performance
- Choice of datasets for benchmarking (e.g., ImageNet-1k)
- Decision to open-source the code for community collaboration and validation
- Assumptions about the scalability of PConv across different neural network architectures
- Decision to compare FasterNet with existing state-of-the-art models for validation