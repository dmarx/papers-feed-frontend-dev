# Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks

## Abstract

## 

To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise convolution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is 2.8√ó, 3.3√ó, and 2.4√ó faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher inference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at [https://github. com/JierunChen/FasterNet](https://github.com/JierunChen/FasterNet).

## Introduction

Neural networks have undergone rapid development in various computer vision tasks such as image classification, detection and segmentation. While their impressive performance has powered many applications, a roaring trend is to pursue fast neural networks with low latency and high throughput for great user experiences, instant responses, safety reasons, etc.

How to be fast? Instead of asking for more costly computing devices, researchers and practitioners prefer to design cost-effective fast neural networks with reduced computational complexity, mainly measured in the number of floating-point operations (FLOPs) [1](#foot_0) . MobileNets [[24,](#b23)[25,](#b24)[54]](#b53), ShuffleNets [[46,](#b45)[84]](#b83) and GhostNet [[17]](#b16), among others, leverage the depthwise convolution (DWConv) [[55]](#b54) and/or group convolution (GConv) [[31]](#b30) to extract spatial features. However, in the effort to reduce FLOPs, the operators often suffer from the side effect of increased memory access. MicroNet [[33]](#b32) further decomposes and sparsifies the network to push its FLOPs to an extremely low level. Despite its improvement in FLOPs, this approach experiences inefficient fragmented computation. Besides, the above networks are often accompanied by additional data manipulations, such as concatenation, shuffling, and pooling, whose running time tends to be significant for tiny models. Apart from the above pure convolutional neural networks (CNNs), there is an emerging interest in making vision transformers (ViTs) [[12]](#b11) and multilayer perceptrons (MLPs) architectures [[64]](#b63) smaller and faster. For example, MobileViTs [[48,](#b47)[49,](#b48)[70]](#b69) and MobileFormer [[6]](#b5) reduce the computational complexity by combining DWConv with a modified attention mechanism. However, they still suffer from the aforementioned issue with DWConv and also need dedicated hardware support for the modified attention mechanism. The use of advanced yet time-consuming nor- malization and activation layers may also limit their speed on devices.

All these issues together lead to the following question: Are these "fast" neural networks really fast? To answer this, we examine the relationship between latency and FLOPs, which is captured by

$Latency = F LOP s F LOP S ,(1)$where FLOPS is short for floating-point operations per second, as a measure of the effective computational speed. While there are many attempts to reduce FLOPs, they seldom consider optimizing FLOPS at the same time to achieve truly low latency. To better understand the situation, we compare the FLOPS of typical neural networks on an Intel CPU. The results in Fig. [2](#fig_1) show that many existing neural networks suffer from low FLOPS, and their FLOPS is generally lower than the popular ResNet50. With such low FLOPS, these "fast" neural networks are actually not fast enough. Their reduction in FLOPs cannot be translated into the exact amount of reduction in latency. In some cases, there is no improvement, and it even leads to worse latency. For example, CycleMLP-B1 [[5]](#b4) has half of FLOPs of ResNet50 [[20]](#b19) but runs more slowly (i.e., CycleMLP-B1 vs. ResNet50: 116.1ms vs. 73.0ms). Note that this discrepancy between FLOPs and latency has also been noticed in previous works [[46,](#b45)[48]](#b47) but remains unresolved partially because they employ the DWConv/GConv and various data manipulations with low FLOPS. It is deemed there are no better alternatives available. This paper aims to eliminate the discrepancy by developing a simple yet fast and effective operator that maintains high FLOPS with reduced FLOPs. Specifically, we reexamine existing operators, particularly DWConv, in terms of the computational speed -FLOPS. We uncover that the main reason causing the low FLOPS issue is frequent memory access. We then propose a novel partial convolution (PConv) as a competitive alternative that reduces the computational redundancy as well as the number of memory access. Fig. [1](#fig_0) illustrates the design of our PConv. It takes advantage of redundancy within the feature maps and systematically applies a regular convolution (Conv) on only a part of the input channels while leaving the remaining ones untouched. By nature, PConv has lower FLOPs than the regular Conv while having higher FLOPS than the DWConv/GConv. In other words, PConv better exploits the on-device computational capacity. PConv is also effective in extracting spatial features as empirically validated later in the paper.

We further introduce FasterNet, which is primarily built upon our PConv, as a new family of networks that run highly fast on various devices. In particular, our FasterNet achieves state-of-the-art performance for classification, detection, and segmentation tasks while having much lower latency and higher throughput. For example, our tiny FasterNet-T0 is 2.8√ó, 3.3√ó, and 2.4√ó faster than MobileViT-XXS [[48]](#b47) on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate on ImageNet-1k. Our large FasterNet-L achieves 83.5% top-1 accuracy, on par with the emerging Swin-B [[41]](#b40), while offering 36% higher throughput on GPU and saving 37% compute time on CPU. To summarize, our contributions are as follows:

‚Ä¢ We point out the importance of achieving higher FLOPS beyond simply reducing FLOPs for faster neural networks.

‚Ä¢ We introduce a simple yet fast and effective operator called PConv, which has a high potential to replace the existing go-to choice, DWConv.

‚Ä¢ We introduce FasterNet which runs favorably and universally fast on a variety of devices such as GPU, CPU, and ARM processors.

‚Ä¢ We conduct extensive experiments on various tasks and validate the high speed and effectiveness of our PConv and FasterNet.

## Related Work

We briefly review prior works on fast and efficient neural networks and differentiate this work from them.

CNN. CNNs are the mainstream architecture in the computer vision field, especially when it comes to deployment in practice, where being fast is as important as being accurate. Though there have been numerous studies [[7,](#b6)[8,](#b7)[21,](#b20)[33,](#b32)[55,](#b54)[56,](#b55)[83,](#b82)[86]](#b85) to achieve higher efficiency, the rationale behind them is more or less to perform a low-rank approximation. Specifically, the group convolution [[31]](#b30) and the depthwise separable convolution [[55]](#b54) (consisting of depthwise and pointwise convolutions) are probably the most popular ones. They have been widely adopted in mobile/edge-oriented networks, such as Mo-bileNets [[24,](#b23)[25,](#b24)[54]](#b53), ShuffleNets [[46,](#b45)[84]](#b83), GhostNet [[17]](#b16), EfficientNets [[61,](#b60)[62]](#b61), TinyNet [[18]](#b17), Xception [[8]](#b7), Con-denseNet [[27,](#b26)[78]](#b77), TVConv [[4]](#b3), MnasNet [[60]](#b59), and FB-Net [[74]](#b73). While they exploit the redundancy in filters to reduce the number of parameters and FLOPs, they suffer from increased memory access when increasing the network width to compensate for the accuracy drop. By contrast, we consider the redundancy in feature maps and propose a partial convolution to reduce FLOPs and memory access simultaneously.

ViT, MLP, and variants. There is a growing interest in studying ViT ever since Dosovitskiy et al. [[12]](#b11) expanded the application scope of transformers [[69]](#b68) from machine translation [[69]](#b68) or forecasting [[73]](#b72) to the computer vision field. Many follow-up works have attempted to improve ViT in terms of training setting [[58,](#b57)[65,](#b64)[66]](#b65) and model design [[15,](#b14)[40,](#b39)[41,](#b40)[72,](#b71)[85]](#b84). One notable trend is to pursue a better accuracy-latency trade-off by reducing the complexity of the attention operator [[1,](#b0)[29,](#b28)[45,](#b44)[63,](#b62)[68]](#b67), incorporating convolution into ViTs [[6,](#b5)[10,](#b9)[57]](#b56), or doing both [[3,](#b2)[34,](#b33)[49,](#b48)[52]](#b51). Besides, other studies [[5,](#b4)[35,](#b34)[64]](#b63) propose to replace the attention with simple MLP-based operators. However, they often evolve to be CNN-like [[39]](#b38). In this paper, we focus on analyzing the convolution operations, particularly DWConv, due to the following reasons: First, the advantage of attention over convolution is unclear or debatable [[42,](#b41)[71]](#b70). Second, the attention-based mechanism generally runs slower than its convolutional counterparts and thus becomes less favorable for the current industry [[26,](#b25)[48]](#b47). Finally, DW-Conv is still a popular choice in many hybrid models, so it is worth a careful examination.

## Design of PConv and FasterNet

In this section, we first revisit DWConv and analyze the issue with its frequent memory access. We then introduce PConv as a competitive alternative operator to resolve the issue. After that, we introduce FasterNet and explain its details, including design considerations.

## Preliminary

DWConv is a popular variant of Conv and has been widely adopted as a key building block for many neural networks. For an input I ‚àà R c√óh√ów , DWConv applies c filters W ‚àà R k√ók to compute the output O ‚àà R c√óh√ów . As shown in Fig. [1(b)](#fig_0), each filter slides spatially on one input channel and contributes to one output channel. This depthwise computation makes DWConv have as low FLOPs as h √ó w √ó k 2 √ó c compared to a regular Conv with h √ó w √ó k 2 √ó c 2 . While effective in reducing FLOPs, a DWConv, which is typically followed by a pointwise convolution, or PWConv, cannot be simply used to replace a regular Conv as it would incur a severe accuracy drop. Thus, in practice the channel number c (or the network width) of DWConv is increased to c ‚Ä≤ (c ‚Ä≤ > c) to compensate the accuracy drop, e.g., the width is expanded by six times for the DWConv in the inverted residual blocks [[54]](#b53). This, however, results in much higher memory access that can cause non-negligible delay and slow down the overall computation, especially for I/O-bound devices. In particular, the number of memory access now escalates to

$h √ó w √ó 2c ‚Ä≤ + k 2 √ó c ‚Ä≤ ‚âà h √ó w √ó 2c ‚Ä≤ ,(2)$which is higher than that of a regular Conv, i.e.,

$h √ó w √ó 2c + k 2 √ó c 2 ‚âà h √ó w √ó 2c.(3)$Note that the h √ó w √ó 2c ‚Ä≤ memory access is spent on the I/O operation, which is deemed to be already the minimum cost and hard to optimize further.

## Partial convolution as a basic operator

We below demonstrate that the cost can be further optimized by leveraging the feature maps' redundancy. As visualized in Fig. [3](#fig_2), the feature maps share high similarities among different channels. This redundancy has also been covered in many other works [[17,](#b16)[82]](#b81), but few of them make full use of it in a simple yet effective way.

Specifically, we propose a simple PConv to reduce computational redundancy and memory access simultaneously. The bottom-left corner in Fig. [4](#fig_3) illustrates how our PConv works. It simply applies a regular Conv on only a part of the input channels for spatial feature extraction and leaves the remaining channels untouched. For contiguous or regular memory access, we consider the first or last consecutive c p channels as the representatives of the whole feature maps for computation. Without loss of generality, we consider the input and output feature maps to have the same number of channels. Therefore, the FLOPs of a PConv are only

$h √ó w √ó k 2 √ó c 2 p .(4)$With a typical partial ratio r = cp c = 1 4 , the FLOPs of a PConv is only 1  16 of a regular Conv. Besides, PConv has a smaller amount of memory access, i.e.,

$h √ó w √ó 2c p + k 2 √ó c 2 p ‚âà h √ó w √ó 2c p ,(5)$which is only 1 4 of a regular Conv for r = 1 4 . Since there are only c p channels utilized for spatial feature extraction, one may ask if we can simply remove the remaining (c -c p ) channels? If so, PConv would degrade to a regular Conv with fewer channels, which deviates from our objective to reduce redundancy. Note that we keep the remaining channels untouched instead of removing them from the feature maps. It is because they are useful for a subsequent PWConv layer, which allows the feature information to flow through all channels.

## PConv followed by PWConv

To fully and efficiently leverage the information from all channels, we further append a pointwise convolution (PW-Conv) to our PConv. Their effective receptive field together on the input feature maps looks like a T-shaped Conv, which focuses more on the center position compared to a regular Conv uniformly processing a patch, as shown in Fig. [5](#fig_4). To justify this T-shaped receptive field, we first evaluate the importance of each position by calculating the position-wise Frobenius norm. We assume that a position tends to be more important if it has a larger Frobenius norm than other positions. For a regular Conv filter F ‚àà R k 2 √óc , the Frobenius norm at position i is calculated by

$‚à•F i ‚à• = c j=1 |f ij | 2 , for i = 1, 2, 3..., k 2 .$We consider a salient position to be the one with the maximum Frobenius norm. We then collectively examine each filter in a pre-trained ResNet18, find out their salient positions, and plot a histogram of the salient positions. Results in Fig. [6](#fig_5) show that the center position turns out to be the salient position most frequently among the filters. In other words, the center position weighs more than its surrounding neighbors. This is consistent with the T-shaped computation which concentrates on the center position.

While the T-shaped Conv can be directly used for efficient computation, we show that it is better to decompose the T-shaped Conv into a PConv and a PWConv because the decomposition exploits the inter-filter redundancy and further saves FLOPs. For the same input I ‚àà R c√óh√ów and output O ‚àà R c√óh√ów , a T-shaped Conv's FLOPs can be calculated as

$h √ó w √ó k 2 √ó c p √ó c + c √ó (c -c p ) ,(6)$which is higher than the FLOPs of a PConv and a PWConv,

$i.e., h √ó w √ó (k 2 √ó c 2 p + c 2 ),(7)$where (k 2 -1)c > k 2 c p , e.g. when c p = c 4 and k = 3. Besides, we can readily leverage the regular Conv for the two-step implementation.

## FasterNet as a general backbone

Given our novel PConv and off-the-shelf PWConv as the primary building operators, we further propose FasterNet, a new family of neural networks that runs favorably fast and is highly effective for many vision tasks. We aim to keep the architecture as simple as possible, without bells and whistles, to make it hardware-friendly in general.

We present the overall architecture in Fig. [4](#fig_3). It has four hierarchical stages, each of which is preceded by an embedding layer (a regular Conv 4 √ó 4 with stride 4) or a merging layer (a regular Conv 2 √ó 2 with stride 2) for spatial downsampling and channel number expanding. Each stage has a stack of FasterNet blocks. We observe that the blocks in the last two stages consume less memory access and tend to have higher FLOPS, as empirically validated in Tab. 1. Thus, we put more FasterNet blocks and correspondingly assign more computations to the last two stages. Each FasterNet block has a PConv layer followed by two PWConv (or Conv 1 √ó 1) layers. Together, they appear as inverted residual blocks where the middle layer has an expanded number of channels, and a shortcut connection is placed to reuse the input features.

In addition to the above operators, the normalization and activation layers are also indispensable for high-performing neural networks. Many prior works [[17,](#b16)[20,](#b19)[54]](#b53), however, overuse such layers throughout the network, which may limit the feature diversity and thus hurt the performance. It can also slow down the overall computation. By contrast, we put them only after each middle PWConv to preserve the feature diversity and achieve lower latency. Besides, we use the batch normalization (BN) [[30]](#b29) instead of other alternative ones [2, 67, 75]. The benefit of BN is that it can be merged into its adjacent Conv layers for faster inference * = FasterNet Block Input FasterNet Block Stage 1 ùëê 1 x ‚Ñé 4 x ùë§ 4 FasterNet Block Stage 2 ùëê 2 x ‚Ñé 8 x ùë§ 8 FasterNet Block Stage 3 ùëê 3 x ‚Ñé 16 x ùë§ 16 FasterNet Block Stage 4 ùëê 4 x ‚Ñé 32 x ùë§ 32 Global Pool Conv 1x1 FC Output Conv 1x1 Conv 1x1 Embedding Merging Merging Merging PConv 3x3 BN, ReLU ‚®Å x ùëô 2 x ùëô 3 x ùëô 4 x ùëô 1 Input ùëê ùëù Filters Output ‚Ñé ùë§ ùëê ùëù ‚Ñé ùë§ ùëê ùëù ùëò ùëò Partial Convolution (PConv) ùëê ùëù ‚Ä¶ Identity  while being as effective as the others. As for the activation layers, we empirically choose GELU [[22]](#b21) for smaller FasterNet variants and ReLU [[51]](#b50) for bigger FasterNet variants, considering both running time and effectiveness. The last three layers, i.e. a global average pooling, a Conv 1 √ó 1, and a fully-connected layer, are used together for feature transformation and classification.

$ùëê ùëù ùëê k k 1 1 ùëê ùëù k k ùëê ùëê -ùëê ùëù$To serve a wide range of applications under different computational budgets, we provide tiny, small, medium, and large variants of FasterNet, referred to as FasterNet-T0/1/2, FasterNet-S, FasterNet-M, and FasterNet-L, respectively. They share a similar architecture but vary in depth and width. Detailed architecture specifications are provided in the appendix.

## Experimental Results

We first examine the computational speed of our PConv and its effectiveness when combined with a PWConv. We then comprehensively evaluate the performance of our FasterNet for classification, detection, and segmentation tasks. Finally, we conduct a brief ablation study.

To benchmark the latency and throughput, we choose the following three typical processors, which cover a wide range of computational capacity: GPU (2080Ti), CPU (Intel i9-9900X, using a single thread), and ARM (Cortex-A72, using a single thread). We report their latency for inputs with a batch size of 1 and throughput for inputs with a batch size of 32. During inference, the BN layers are merged to their adjacent layers wherever applicable.

## PConv is fast with high FLOPS

We below show that our PConv is fast and better exploits the on-device computational capacity. Specifically, we stack 10 layers of pure PConv and take feature maps of typical dimensions as inputs. We then measure FLOPs and latency/throughput on GPU, CPU, and ARM processors, which also allow us to further compute FLOPS. We repeat the same procedure for other convolutional variants and make comparisons.

Results in Tab. latency/throughput are unaffordable. GConv and DWConv, despite their significant reduction in FLOPs, suffer from a drastic decrease in FLOPS. In addition, they tend to increase the number of channels to compensate for the performance drop, which, however, increase their latency.

## PConv is effective together with PWConv

We next show that a PConv followed by a PWConv is effective in approximating a regular Conv to transform the feature maps. To this end, we first build four datasets by feeding the ImageNet-1k val split images into a pre-trained ResNet50, and extract the feature maps before and after the first Conv 3 √ó 3 in each of the four stages. Each feature map dataset is further spilt into the train (70%), val (10%), and test (20%) subsets. We then build a simple network consisting of a PConv followed by a PWConv and train it on the feature map datasets with a mean squared error loss. For comparison, we also build and train networks for DWConv + PWConv and GConv + PWConv under the same setting.

Tab. 2 shows that PConv + PWConv achieve the lowest test loss, meaning that they better approximate a regular Conv in feature transformation. The results also suggest that it is sufficient and efficient to capture spatial features from only a part of the feature maps. PConv shows a great potential to be the new go-to choice in designing fast and effective neural networks.

## FasterNet on ImageNet-1k classification

To verify the effectiveness and efficiency of our Faster-Net, we first conduct experiments on the large-scale ImageNet-1k classification dataset [[53]](#b52). It covers 1k categories of common objects and contains about 1.3M la-

Stage DWConv+PWConv GConv+PWConv (16 groups) PConv+PWConv r = 1 4 1 0.0089 0.0065 0.0069 2 0.0158 0.0137 0.0136 3 0.0214 0.0202 0.0172 4 0.0130 0.0128 0.0115 Average 0.0148 0.0133 0.0123 Table 2. A PConv followed by a PWConv well approximates the regular Conv 3 √ó 3 at different stages of a pre-trained ResNet50. PConv + PWConv together have the lowest test loss on average.

beled images for training and 50k labeled images for validation. We train our models for 300 epochs using AdamW optimizer [[44]](#b43). We set the batch size to 2048 for the FasterNet-M/L and 4096 for other variants. We use cosine learning rate scheduler [[43]](#b42) with a peak value of 0.001 ‚Ä¢ batch size/1024 and a 20-epoch linear warmup. We apply commonly-used regularization and augmentation techniques, including Weight Decay [[32]](#b31), Stochastic Depth [[28]](#b27), Label Smoothing [[59]](#b58), Mixup [[81]](#b80), Cutmix [[80]](#b79) and Rand Augment [[9]](#b8), with varying magnitudes for different FasterNet variants. To reduce the training time, we use 192√ó192 resolution for the first 280 training epochs and 224√ó224 for the remaining 20 epochs. For fair comparison, we do not use knowledge distillation [[23]](#b22) and neural architecture search [[87]](#b86). We report our top-1 accuracy on the validation set with a center crop at 224 √ó 224 resolution and a 0.9 crop ratio. Detailed training and validation settings are provided in the appendix. To save space and make the plots more proportionate, we showcase network variants within a certain range of latency. Full plots can be found in the appendix, which show consistent results.

the new state-of-the-art in balancing accuracy and latency/throughput among all the networks examined. From another perspective, FasterNet runs faster than various CNN, ViT and MLP models on a wide range of devices, when having similar top-1 accuracy. As quantitatively shown in Tab. 3, FasterNet-T0 is 2.8√ó, 3.3√ó, and 2.4√ó faster than MobileViT-XXS [[48]](#b47) on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves 83.5% top-1 accuracy, comparable to the emerging Swin-B [[41]](#b40) and ConvNeXt-B [[42]](#b41) while having 36% and 28% higher inference throughput on GPU, as well as saving 37% and 15% compute time on CPU. Given such promising results, we highlight that our FasterNet is much simpler than many other models in terms of architectural design, which showcases the feasibility of designing simple yet powerful neural networks.

## FasterNet on downstream tasks

To further evaluate the generalization ability of Faster-Net, we conduct experiments on the challenging COCO dataset [[36]](#b35) for object detection and instance segmentation. As a common practice, we employ the ImageNet pre-trained FasterNet as a backbone and equip it with the popular Mask R-CNN detector [[19]](#b18). To highlight the effectiveness of the backbone itself, we simply follow Pool-Former [[79]](#b78)  Table 5. Ablation on the partial ratio, normalization, and activation of FasterNet. Rows highlighted in grey are the default settings. T0 * denotes T0 variants with modified network width and depth. schedule (12 epochs), a batch size of 16, and other training settings without further hyper-parameter tuning.

Tab. 4 shows the results for comparison between Faster-Net and representative models. FasterNet consistently outperforms ResNet and ResNext by having higher average precision (AP) with similar latency. Specifically, FasterNet-S yields +1.9 higher box AP and +2.4 higher mask AP compared to the standard baseline ResNet50. FasterNet is also competitive against the ViT variants. Under similar FLOPs, FasterNet-L reduces PVT-Large's latency by 38%, i.e., from 152.2 ms to 93.8 ms on GPU, and achieves +1.1 higher box AP and +0.4 higher mask AP.

## Ablation study

We conduct a brief ablation study on the value of partial ratio r and the choices of activation and normalization layers. We compare different variants in terms of ImageNet top-1 accuracy and on-device latency/throughput. Results are summarized in Tab. 5. For the partial ratio r, we set it to 1  4 for all FasterNet variants by default, which achieves higher accuracy, higher throughput, and lower latency at similar complexity. A too large partial ratio r would make PConv degrade to a regular Conv, while a too small value would render PConv less effective in capturing the spatial features. For the normalization layers, we choose Batch-Norm over LayerNorm because BatchNorm can be merged into its adjacent convolutional layers for faster inference while it is as effective as LayerNorm in our experiment. For the activation function, interestingly, we empirically found that GELU fits FasterNet-T0/T1 models more efficiently than ReLU. It, however, becomes opposite for FasterNet-T2/S/M/L. Here we only show two examples in Tab. 5 due to space constraint. We conjecture that GELU strengthens FasterNet-T0/T1 by having higher non-linearity, while the benefit fades away for larger FasterNet variants.

## Conclusion

In this paper, we have investigated the common and unresolved issue that many established neural networks suffer from low floating-point operations per second (FLOPS). We have revisited a bottleneck operator, DWConv, and analyzed its main cause for a slowdown -frequent memory access. To overcome the issue and achieve faster neural networks, we have proposed a simple yet fast and effective operator, PConv, that can be readily plugged into many existing networks. We have further introduced our generalpurpose FasterNet, built upon our PConv, that achieves state-of-the-art speed and accuracy trade-off on various devices and vision tasks. We hope that our PConv and Faster-Net would inspire more research on simple yet effective neural networks, going beyond academia to impact the industry and community directly.

## A. ImageNet-1k experimental settings

We provide ImageNet-1k training and evaluation settings in Tab. 6. They can be used for reproducing our main results in Tab. 3 and Fig. [7](#). Different FasterNet variants vary in the magnitude of regularization and augmentation techniques. The magnitude increases as the model becomes larger to alleviate overfitting and improve accuracy. Note that most of the compared works in Tab. 3 and Fig. [7](#), e.g., Mobile-ViT, EdgeNext, PVT, CycleMLP, ConvNeXt, Swin, etc., also adopt such advanced training techniques (ADT). Some even heavily rely on the hyper-parameter search. For others w/o ADT, i.e., ShuffleNetV2, MobileNetV2, and GhostNet, though the comparison is not totally fair, we include them for reference.

## B. Downstream tasks experimental settings

For object detection and instance segmentation on the COCO2017 dataset, we equip our FasterNet backbone with the popular Mask R-CNN detector. We use ImageNet-1k pre-trained weights to initialize the backbone and Xavier to initialize the add-on layers. Detailed settings are summarized in Tab. 7.

## C. Full comparison plots on ImageNet-1k

Fig. [8](#) shows the full comparison plots on ImageNet-1k, which is the extension of Fig. [7](#) in the main paper with a larger range of latency. Fig. [8](#) shows consistent results that FasterNet strikes better trade-offs than others in balancing accuracy and latency/throughput on GPU, CPU, and ARM processors.

## D. Detailed architectural configurations

We present the detailed architectural configurations in Tab. 8. While different FasterNet variants share a unified architecture, they vary in the network width (the number of channels) and network depth (the number of FasterNet blocks at each stage). The classifier at the end of the architecture is used for classification tasks but removed for other downstream tasks.

## E. More comparisons with related work

Improving FLOPS. There are a few other works [[11,](#b10)[76]](#b75) also looking into the FLOPS issue and trying to improve Table 8. Configurations of different FasterNet variants. "Conv k c s" means a convolutional layer with the kernel size of k, the output channels of c, and the stride of s. "PConv k c s r" means a partial convolution with an extra parameter, the partial ratio of r. "FC 1000" means a fully connected layer with 1000 output channels. h √ó w is the input size while bi is the number of FasterNet blocks at stage i. The FLOPs are calculated given the input size of 224 √ó 224.

ViT to use fewer activation functions, while we intentionally remove them from the middle of PConv and PWConv, to minimize their error in approximating a regular Conv.

Other paradigms for efficient inference. Our work focuses on efficient network design, orthogonal to the other paradigms, e.g., neural architecture search (NAS) [[13]](#b12), network pruning [[50]](#b49), and knowledge distillation [[23]](#b22). They can be applied in this paper for better performance. However, we opt not to do so to keep our core idea centered and to make the performance gain clear and fair.

Other partial/masked convolution works. There are several works [[14,](#b13)[37,](#b36)[38]](#b37) sharing similar names with our PConv. However, they differ a lot in objectives and methods. For example, they apply filters on partial pixels to exclude invalid patches [[38]](#b37), enable self-supervised learning [[14]](#b13), or synthesize novel images [[37]](#b36), while we target at the channel dimension for efficient inference.

## F. Limitations and future work

We have demonstrated that PConv and FasterNet are fast and effective, being competitive with existing operators and networks. Yet there are some minor technical limitations of this paper. For one thing, PConv is designed to apply a regular convolution on only a part of the input channels while leaving the remaining ones untouched. Thus, the stride of the partial convolution should always be 1, in order to align the spatial resolution of the convolutional output and that of the untouched channels. Note that it is still feasible to down-sample the spatial resolution as there can be addi-tional downsampling layers in the architecture. And for another, our FasterNet is simply built upon convolutional operators with a possibly limited receptive field. Future efforts can be made to enlarge its receptive field and combine it with other operators to pursue higher accuracy.

![Figure1. Our partial convolution (PConv) is fast and efficient by applying filters on only a few input channels while leaving the remaining ones untouched. PConv obtains lower FLOPs than the regular convolution and higher FLOPS than the depthwise/group convolution.]()

![Figure 2. (a) FLOPS under varied FLOPs on CPU. Many existing neural networks suffer from low computational speed issues. Their effective FLOPS are lower than the popular ResNet50. By contrast, our FasterNet attains higher FLOPS. (b) Latency under varied FLOPs on CPU. Our FasterNet obtains lower latency than others with the same amount of FLOPs.]()

![Figure 3. Visualization of feature maps in an intermediate layer of a pre-trained ResNet50, with the top-left image as the input. Qualitatively, we can see the high redundancies across different channels.]()

![Figure 4. Overall architecture of our FasterNet. It has four hierarchical stages, each with a stack of FasterNet blocks and preceded by an embedding or merging layer. The last three layers are used for feature classification. Within each FasterNet block, a PConv layer is followed by two PWConv layers. We put normalization and activation layers only after the middle layer to preserve the feature diversity and achieve lower latency.]()

![Figure 5. Comparison of convolutional variants. A PConv followed by a PWConv (a) resembles a T-shaped Conv (b), which spends more computation on the center position compared to a regular Conv (c).]()

![Figure 6. Histogram of salient position distribution for the regular Conv 3 √ó 3 filters in a pre-trained ResNet18. The histogram contains four kinds of bars, corresponding to different stages in the network. In all stages, the center position (position 5) appears as a salient position most frequently.]()

![Figure 7. FasterNet has the highest efficiency in balancing accuracy-throughput and accuracy-latency trade-offs for different devices. To save space and make the plots more proportionate, we showcase network variants within a certain range of latency. Full plots can be found in the appendix, which show consistent results.]()

![1 show that PConv is overall an appealing choice for high FLOPS with reduced FLOPs. It has only On-device FLOPS for different operations. PConv appears as an appealing choice for high FLOPS with reduced FLOPs.]()

![Comparison on ImageNet-1k benchmark. Models with similar top-1 accuracy are grouped together. For each group, our FasterNet achieves the highest throughput on GPU and the lowest latency on CPU and ARM. All models are evaluated at 224 √ó 224 resolution except for the MobileViT and EdgeNeXt with 256 √ó 256. OOM is short for out of memory.]()

![and adopt an AdamW optimizer, a 1√ó training Results on COCO object detection and instance segmentation benchmarks. FLOPs are calculated with image size (1280, 800).]()

![ImageNet-1k training and evaluation settings for different FasterNet variants.]()

![Experimental settings of object detection and instance segmentation on the COCO2017 dataset.]()

We follow a widely adopted definition of FLOPs, as the number of multiply-adds[[42,](#b41)[84]](#b83).

[16](#b15) FLOPs of a regular Conv and achieves 10.5√ó, 6.2√ó, and

22.8√ó higher FLOPS than the DWConv on GPU, CPU, and ARM, respectively. We are unsurprised to see that the regular Conv has the highest FLOPS as it has been constantly optimized for years. However, its total FLOPs and

