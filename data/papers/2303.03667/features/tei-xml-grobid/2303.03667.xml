<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Run, Don&apos;t Walk: Chasing Higher FLOPS for Faster Neural Networks</title>
				<funder ref="#_AQrQ6zV">
					<orgName type="full">Hong Kong General Research Fund</orgName>
				</funder>
				<funder ref="#_mSMdVy3">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-21">21 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jierun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shiu-Hong</forename><surname>Kao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weipeng</forename><surname>Zhuo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chul-Ho</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Texas State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S.-H</forename><forename type="middle">Gary</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName><surname>Hkust</surname></persName>
						</author>
						<title level="a" type="main">Run, Don&apos;t Walk: Chasing Higher FLOPS for Faster Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-21">21 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">0E7F37639CD6D59CAB7A1E0E10EBD826</idno>
					<idno type="arXiv">arXiv:2303.03667v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise convolution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is 2.8√ó, 3.3√ó, and 2.4√ó faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher inference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at <ref type="url" target="https://github.com/JierunChen/FasterNet">https://github. com/JierunChen/FasterNet</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks have undergone rapid development in various computer vision tasks such as image classification, detection and segmentation. While their impressive performance has powered many applications, a roaring trend is to pursue fast neural networks with low latency and high throughput for great user experiences, instant responses, safety reasons, etc.</p><p>How to be fast? Instead of asking for more costly computing devices, researchers and practitioners prefer to design cost-effective fast neural networks with reduced computational complexity, mainly measured in the number of floating-point operations (FLOPs) <ref type="foot" target="#foot_0">1</ref> . MobileNets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b53">54]</ref>, ShuffleNets <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b83">84]</ref> and GhostNet <ref type="bibr" target="#b16">[17]</ref>, among others, leverage the depthwise convolution (DWConv) <ref type="bibr" target="#b54">[55]</ref> and/or group convolution (GConv) <ref type="bibr" target="#b30">[31]</ref> to extract spatial features. However, in the effort to reduce FLOPs, the operators often suffer from the side effect of increased memory access. MicroNet <ref type="bibr" target="#b32">[33]</ref> further decomposes and sparsifies the network to push its FLOPs to an extremely low level. Despite its improvement in FLOPs, this approach experiences inefficient fragmented computation. Besides, the above networks are often accompanied by additional data manipulations, such as concatenation, shuffling, and pooling, whose running time tends to be significant for tiny models. Apart from the above pure convolutional neural networks (CNNs), there is an emerging interest in making vision transformers (ViTs) <ref type="bibr" target="#b11">[12]</ref> and multilayer perceptrons (MLPs) architectures <ref type="bibr" target="#b63">[64]</ref> smaller and faster. For example, MobileViTs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b69">70]</ref> and MobileFormer <ref type="bibr" target="#b5">[6]</ref> reduce the computational complexity by combining DWConv with a modified attention mechanism. However, they still suffer from the aforementioned issue with DWConv and also need dedicated hardware support for the modified attention mechanism. The use of advanced yet time-consuming nor- malization and activation layers may also limit their speed on devices.</p><p>All these issues together lead to the following question: Are these "fast" neural networks really fast? To answer this, we examine the relationship between latency and FLOPs, which is captured by</p><formula xml:id="formula_0">Latency = F LOP s F LOP S ,<label>(1)</label></formula><p>where FLOPS is short for floating-point operations per second, as a measure of the effective computational speed. While there are many attempts to reduce FLOPs, they seldom consider optimizing FLOPS at the same time to achieve truly low latency. To better understand the situation, we compare the FLOPS of typical neural networks on an Intel CPU. The results in Fig. <ref type="figure" target="#fig_1">2</ref> show that many existing neural networks suffer from low FLOPS, and their FLOPS is generally lower than the popular ResNet50. With such low FLOPS, these "fast" neural networks are actually not fast enough. Their reduction in FLOPs cannot be translated into the exact amount of reduction in latency. In some cases, there is no improvement, and it even leads to worse latency. For example, CycleMLP-B1 <ref type="bibr" target="#b4">[5]</ref> has half of FLOPs of ResNet50 <ref type="bibr" target="#b19">[20]</ref> but runs more slowly (i.e., CycleMLP-B1 vs. ResNet50: 116.1ms vs. 73.0ms). Note that this discrepancy between FLOPs and latency has also been noticed in previous works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref> but remains unresolved partially because they employ the DWConv/GConv and various data manipulations with low FLOPS. It is deemed there are no better alternatives available. This paper aims to eliminate the discrepancy by developing a simple yet fast and effective operator that maintains high FLOPS with reduced FLOPs. Specifically, we reexamine existing operators, particularly DWConv, in terms of the computational speed -FLOPS. We uncover that the main reason causing the low FLOPS issue is frequent memory access. We then propose a novel partial convolution (PConv) as a competitive alternative that reduces the computational redundancy as well as the number of memory access. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the design of our PConv. It takes advantage of redundancy within the feature maps and systematically applies a regular convolution (Conv) on only a part of the input channels while leaving the remaining ones untouched. By nature, PConv has lower FLOPs than the regular Conv while having higher FLOPS than the DWConv/GConv. In other words, PConv better exploits the on-device computational capacity. PConv is also effective in extracting spatial features as empirically validated later in the paper.</p><p>We further introduce FasterNet, which is primarily built upon our PConv, as a new family of networks that run highly fast on various devices. In particular, our FasterNet achieves state-of-the-art performance for classification, detection, and segmentation tasks while having much lower latency and higher throughput. For example, our tiny FasterNet-T0 is 2.8√ó, 3.3√ó, and 2.4√ó faster than MobileViT-XXS <ref type="bibr" target="#b47">[48]</ref> on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate on ImageNet-1k. Our large FasterNet-L achieves 83.5% top-1 accuracy, on par with the emerging Swin-B <ref type="bibr" target="#b40">[41]</ref>, while offering 36% higher throughput on GPU and saving 37% compute time on CPU. To summarize, our contributions are as follows:</p><p>‚Ä¢ We point out the importance of achieving higher FLOPS beyond simply reducing FLOPs for faster neural networks.</p><p>‚Ä¢ We introduce a simple yet fast and effective operator called PConv, which has a high potential to replace the existing go-to choice, DWConv.</p><p>‚Ä¢ We introduce FasterNet which runs favorably and universally fast on a variety of devices such as GPU, CPU, and ARM processors.</p><p>‚Ä¢ We conduct extensive experiments on various tasks and validate the high speed and effectiveness of our PConv and FasterNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review prior works on fast and efficient neural networks and differentiate this work from them.</p><p>CNN. CNNs are the mainstream architecture in the computer vision field, especially when it comes to deployment in practice, where being fast is as important as being accurate. Though there have been numerous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b85">86]</ref> to achieve higher efficiency, the rationale behind them is more or less to perform a low-rank approximation. Specifically, the group convolution <ref type="bibr" target="#b30">[31]</ref> and the depthwise separable convolution <ref type="bibr" target="#b54">[55]</ref> (consisting of depthwise and pointwise convolutions) are probably the most popular ones. They have been widely adopted in mobile/edge-oriented networks, such as Mo-bileNets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b53">54]</ref>, ShuffleNets <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b83">84]</ref>, GhostNet <ref type="bibr" target="#b16">[17]</ref>, EfficientNets <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, TinyNet <ref type="bibr" target="#b17">[18]</ref>, Xception <ref type="bibr" target="#b7">[8]</ref>, Con-denseNet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b77">78]</ref>, TVConv <ref type="bibr" target="#b3">[4]</ref>, MnasNet <ref type="bibr" target="#b59">[60]</ref>, and FB-Net <ref type="bibr" target="#b73">[74]</ref>. While they exploit the redundancy in filters to reduce the number of parameters and FLOPs, they suffer from increased memory access when increasing the network width to compensate for the accuracy drop. By contrast, we consider the redundancy in feature maps and propose a partial convolution to reduce FLOPs and memory access simultaneously.</p><p>ViT, MLP, and variants. There is a growing interest in studying ViT ever since Dosovitskiy et al. <ref type="bibr" target="#b11">[12]</ref> expanded the application scope of transformers <ref type="bibr" target="#b68">[69]</ref> from machine translation <ref type="bibr" target="#b68">[69]</ref> or forecasting <ref type="bibr" target="#b72">[73]</ref> to the computer vision field. Many follow-up works have attempted to improve ViT in terms of training setting <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref> and model design <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b84">85]</ref>. One notable trend is to pursue a better accuracy-latency trade-off by reducing the complexity of the attention operator <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref>, incorporating convolution into ViTs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b56">57]</ref>, or doing both <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref>. Besides, other studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b63">64]</ref> propose to replace the attention with simple MLP-based operators. However, they often evolve to be CNN-like <ref type="bibr" target="#b38">[39]</ref>. In this paper, we focus on analyzing the convolution operations, particularly DWConv, due to the following reasons: First, the advantage of attention over convolution is unclear or debatable <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b70">71]</ref>. Second, the attention-based mechanism generally runs slower than its convolutional counterparts and thus becomes less favorable for the current industry <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b47">48]</ref>. Finally, DW-Conv is still a popular choice in many hybrid models, so it is worth a careful examination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Design of PConv and FasterNet</head><p>In this section, we first revisit DWConv and analyze the issue with its frequent memory access. We then introduce PConv as a competitive alternative operator to resolve the issue. After that, we introduce FasterNet and explain its details, including design considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>DWConv is a popular variant of Conv and has been widely adopted as a key building block for many neural networks. For an input I ‚àà R c√óh√ów , DWConv applies c filters W ‚àà R k√ók to compute the output O ‚àà R c√óh√ów . As shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, each filter slides spatially on one input channel and contributes to one output channel. This depthwise computation makes DWConv have as low FLOPs as h √ó w √ó k 2 √ó c compared to a regular Conv with h √ó w √ó k 2 √ó c 2 . While effective in reducing FLOPs, a DWConv, which is typically followed by a pointwise convolution, or PWConv, cannot be simply used to replace a regular Conv as it would incur a severe accuracy drop. Thus, in practice the channel number c (or the network width) of DWConv is increased to c ‚Ä≤ (c ‚Ä≤ &gt; c) to compensate the accuracy drop, e.g., the width is expanded by six times for the DWConv in the inverted residual blocks <ref type="bibr" target="#b53">[54]</ref>. This, however, results in much higher memory access that can cause non-negligible delay and slow down the overall computation, especially for I/O-bound devices. In particular, the number of memory access now escalates to</p><formula xml:id="formula_1">h √ó w √ó 2c ‚Ä≤ + k 2 √ó c ‚Ä≤ ‚âà h √ó w √ó 2c ‚Ä≤ ,<label>(2)</label></formula><p>which is higher than that of a regular Conv, i.e.,</p><formula xml:id="formula_2">h √ó w √ó 2c + k 2 √ó c 2 ‚âà h √ó w √ó 2c.<label>(3)</label></formula><p>Note that the h √ó w √ó 2c ‚Ä≤ memory access is spent on the I/O operation, which is deemed to be already the minimum cost and hard to optimize further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Partial convolution as a basic operator</head><p>We below demonstrate that the cost can be further optimized by leveraging the feature maps' redundancy. As visualized in Fig. <ref type="figure" target="#fig_2">3</ref>, the feature maps share high similarities among different channels. This redundancy has also been covered in many other works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b81">82]</ref>, but few of them make full use of it in a simple yet effective way.</p><p>Specifically, we propose a simple PConv to reduce computational redundancy and memory access simultaneously. The bottom-left corner in Fig. <ref type="figure" target="#fig_3">4</ref> illustrates how our PConv works. It simply applies a regular Conv on only a part of the input channels for spatial feature extraction and leaves the remaining channels untouched. For contiguous or regular memory access, we consider the first or last consecutive c p channels as the representatives of the whole feature maps for computation. Without loss of generality, we consider the input and output feature maps to have the same number of channels. Therefore, the FLOPs of a PConv are only</p><formula xml:id="formula_3">h √ó w √ó k 2 √ó c 2 p .<label>(4)</label></formula><p>With a typical partial ratio r = cp c = 1 4 , the FLOPs of a PConv is only 1  16 of a regular Conv. Besides, PConv has a smaller amount of memory access, i.e.,</p><formula xml:id="formula_4">h √ó w √ó 2c p + k 2 √ó c 2 p ‚âà h √ó w √ó 2c p ,<label>(5)</label></formula><p>which is only 1 4 of a regular Conv for r = 1 4 . Since there are only c p channels utilized for spatial feature extraction, one may ask if we can simply remove the remaining (c -c p ) channels? If so, PConv would degrade to a regular Conv with fewer channels, which deviates from our objective to reduce redundancy. Note that we keep the remaining channels untouched instead of removing them from the feature maps. It is because they are useful for a subsequent PWConv layer, which allows the feature information to flow through all channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">PConv followed by PWConv</head><p>To fully and efficiently leverage the information from all channels, we further append a pointwise convolution (PW-Conv) to our PConv. Their effective receptive field together on the input feature maps looks like a T-shaped Conv, which focuses more on the center position compared to a regular Conv uniformly processing a patch, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. To justify this T-shaped receptive field, we first evaluate the importance of each position by calculating the position-wise Frobenius norm. We assume that a position tends to be more important if it has a larger Frobenius norm than other positions. For a regular Conv filter F ‚àà R k 2 √óc , the Frobenius norm at position i is calculated by</p><formula xml:id="formula_5">‚à•F i ‚à• = c j=1 |f ij | 2 , for i = 1, 2, 3..., k 2 .</formula><p>We consider a salient position to be the one with the maximum Frobenius norm. We then collectively examine each filter in a pre-trained ResNet18, find out their salient positions, and plot a histogram of the salient positions. Results in Fig. <ref type="figure" target="#fig_5">6</ref> show that the center position turns out to be the salient position most frequently among the filters. In other words, the center position weighs more than its surrounding neighbors. This is consistent with the T-shaped computation which concentrates on the center position.</p><p>While the T-shaped Conv can be directly used for efficient computation, we show that it is better to decompose the T-shaped Conv into a PConv and a PWConv because the decomposition exploits the inter-filter redundancy and further saves FLOPs. For the same input I ‚àà R c√óh√ów and output O ‚àà R c√óh√ów , a T-shaped Conv's FLOPs can be calculated as</p><formula xml:id="formula_6">h √ó w √ó k 2 √ó c p √ó c + c √ó (c -c p ) ,<label>(6)</label></formula><p>which is higher than the FLOPs of a PConv and a PWConv,</p><formula xml:id="formula_7">i.e., h √ó w √ó (k 2 √ó c 2 p + c 2 ),<label>(7)</label></formula><p>where (k 2 -1)c &gt; k 2 c p , e.g. when c p = c 4 and k = 3. Besides, we can readily leverage the regular Conv for the two-step implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">FasterNet as a general backbone</head><p>Given our novel PConv and off-the-shelf PWConv as the primary building operators, we further propose FasterNet, a new family of neural networks that runs favorably fast and is highly effective for many vision tasks. We aim to keep the architecture as simple as possible, without bells and whistles, to make it hardware-friendly in general.</p><p>We present the overall architecture in Fig. <ref type="figure" target="#fig_3">4</ref>. It has four hierarchical stages, each of which is preceded by an embedding layer (a regular Conv 4 √ó 4 with stride 4) or a merging layer (a regular Conv 2 √ó 2 with stride 2) for spatial downsampling and channel number expanding. Each stage has a stack of FasterNet blocks. We observe that the blocks in the last two stages consume less memory access and tend to have higher FLOPS, as empirically validated in Tab. 1. Thus, we put more FasterNet blocks and correspondingly assign more computations to the last two stages. Each FasterNet block has a PConv layer followed by two PWConv (or Conv 1 √ó 1) layers. Together, they appear as inverted residual blocks where the middle layer has an expanded number of channels, and a shortcut connection is placed to reuse the input features.</p><p>In addition to the above operators, the normalization and activation layers are also indispensable for high-performing neural networks. Many prior works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b53">54]</ref>, however, overuse such layers throughout the network, which may limit the feature diversity and thus hurt the performance. It can also slow down the overall computation. By contrast, we put them only after each middle PWConv to preserve the feature diversity and achieve lower latency. Besides, we use the batch normalization (BN) <ref type="bibr" target="#b29">[30]</ref> instead of other alternative ones [2, 67, 75]. The benefit of BN is that it can be merged into its adjacent Conv layers for faster inference * = FasterNet Block Input FasterNet Block Stage 1 ùëê 1 x ‚Ñé 4 x ùë§ 4 FasterNet Block Stage 2 ùëê 2 x ‚Ñé 8 x ùë§ 8 FasterNet Block Stage 3 ùëê 3 x ‚Ñé 16 x ùë§ 16 FasterNet Block Stage 4 ùëê 4 x ‚Ñé 32 x ùë§ 32 Global Pool Conv 1x1 FC Output Conv 1x1 Conv 1x1 Embedding Merging Merging Merging PConv 3x3 BN, ReLU ‚®Å x ùëô 2 x ùëô 3 x ùëô 4 x ùëô 1 Input ùëê ùëù Filters Output ‚Ñé ùë§ ùëê ùëù ‚Ñé ùë§ ùëê ùëù ùëò ùëò Partial Convolution (PConv) ùëê ùëù ‚Ä¶ Identity  while being as effective as the others. As for the activation layers, we empirically choose GELU <ref type="bibr" target="#b21">[22]</ref> for smaller FasterNet variants and ReLU <ref type="bibr" target="#b50">[51]</ref> for bigger FasterNet variants, considering both running time and effectiveness. The last three layers, i.e. a global average pooling, a Conv 1 √ó 1, and a fully-connected layer, are used together for feature transformation and classification.</p><formula xml:id="formula_8">ùëê ùëù ùëê k k 1 1 ùëê ùëù k k ùëê ùëê -ùëê ùëù</formula><p>To serve a wide range of applications under different computational budgets, we provide tiny, small, medium, and large variants of FasterNet, referred to as FasterNet-T0/1/2, FasterNet-S, FasterNet-M, and FasterNet-L, respectively. They share a similar architecture but vary in depth and width. Detailed architecture specifications are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We first examine the computational speed of our PConv and its effectiveness when combined with a PWConv. We then comprehensively evaluate the performance of our FasterNet for classification, detection, and segmentation tasks. Finally, we conduct a brief ablation study.</p><p>To benchmark the latency and throughput, we choose the following three typical processors, which cover a wide range of computational capacity: GPU (2080Ti), CPU (Intel i9-9900X, using a single thread), and ARM (Cortex-A72, using a single thread). We report their latency for inputs with a batch size of 1 and throughput for inputs with a batch size of 32. During inference, the BN layers are merged to their adjacent layers wherever applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">PConv is fast with high FLOPS</head><p>We below show that our PConv is fast and better exploits the on-device computational capacity. Specifically, we stack 10 layers of pure PConv and take feature maps of typical dimensions as inputs. We then measure FLOPs and latency/throughput on GPU, CPU, and ARM processors, which also allow us to further compute FLOPS. We repeat the same procedure for other convolutional variants and make comparisons.</p><p>Results in Tab. latency/throughput are unaffordable. GConv and DWConv, despite their significant reduction in FLOPs, suffer from a drastic decrease in FLOPS. In addition, they tend to increase the number of channels to compensate for the performance drop, which, however, increase their latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PConv is effective together with PWConv</head><p>We next show that a PConv followed by a PWConv is effective in approximating a regular Conv to transform the feature maps. To this end, we first build four datasets by feeding the ImageNet-1k val split images into a pre-trained ResNet50, and extract the feature maps before and after the first Conv 3 √ó 3 in each of the four stages. Each feature map dataset is further spilt into the train (70%), val (10%), and test (20%) subsets. We then build a simple network consisting of a PConv followed by a PWConv and train it on the feature map datasets with a mean squared error loss. For comparison, we also build and train networks for DWConv + PWConv and GConv + PWConv under the same setting.</p><p>Tab. 2 shows that PConv + PWConv achieve the lowest test loss, meaning that they better approximate a regular Conv in feature transformation. The results also suggest that it is sufficient and efficient to capture spatial features from only a part of the feature maps. PConv shows a great potential to be the new go-to choice in designing fast and effective neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">FasterNet on ImageNet-1k classification</head><p>To verify the effectiveness and efficiency of our Faster-Net, we first conduct experiments on the large-scale ImageNet-1k classification dataset <ref type="bibr" target="#b52">[53]</ref>. It covers 1k categories of common objects and contains about 1.3M la-</p><p>Stage DWConv+PWConv GConv+PWConv (16 groups) PConv+PWConv r = 1 4 1 0.0089 0.0065 0.0069 2 0.0158 0.0137 0.0136 3 0.0214 0.0202 0.0172 4 0.0130 0.0128 0.0115 Average 0.0148 0.0133 0.0123 Table 2. A PConv followed by a PWConv well approximates the regular Conv 3 √ó 3 at different stages of a pre-trained ResNet50. PConv + PWConv together have the lowest test loss on average.</p><p>beled images for training and 50k labeled images for validation. We train our models for 300 epochs using AdamW optimizer <ref type="bibr" target="#b43">[44]</ref>. We set the batch size to 2048 for the FasterNet-M/L and 4096 for other variants. We use cosine learning rate scheduler <ref type="bibr" target="#b42">[43]</ref> with a peak value of 0.001 ‚Ä¢ batch size/1024 and a 20-epoch linear warmup. We apply commonly-used regularization and augmentation techniques, including Weight Decay <ref type="bibr" target="#b31">[32]</ref>, Stochastic Depth <ref type="bibr" target="#b27">[28]</ref>, Label Smoothing <ref type="bibr" target="#b58">[59]</ref>, Mixup <ref type="bibr" target="#b80">[81]</ref>, Cutmix <ref type="bibr" target="#b79">[80]</ref> and Rand Augment <ref type="bibr" target="#b8">[9]</ref>, with varying magnitudes for different FasterNet variants. To reduce the training time, we use 192√ó192 resolution for the first 280 training epochs and 224√ó224 for the remaining 20 epochs. For fair comparison, we do not use knowledge distillation <ref type="bibr" target="#b22">[23]</ref> and neural architecture search <ref type="bibr" target="#b86">[87]</ref>. We report our top-1 accuracy on the validation set with a center crop at 224 √ó 224 resolution and a 0.9 crop ratio. Detailed training and validation settings are provided in the appendix. To save space and make the plots more proportionate, we showcase network variants within a certain range of latency. Full plots can be found in the appendix, which show consistent results.</p><p>the new state-of-the-art in balancing accuracy and latency/throughput among all the networks examined. From another perspective, FasterNet runs faster than various CNN, ViT and MLP models on a wide range of devices, when having similar top-1 accuracy. As quantitatively shown in Tab. 3, FasterNet-T0 is 2.8√ó, 3.3√ó, and 2.4√ó faster than MobileViT-XXS <ref type="bibr" target="#b47">[48]</ref> on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves 83.5% top-1 accuracy, comparable to the emerging Swin-B <ref type="bibr" target="#b40">[41]</ref> and ConvNeXt-B <ref type="bibr" target="#b41">[42]</ref> while having 36% and 28% higher inference throughput on GPU, as well as saving 37% and 15% compute time on CPU. Given such promising results, we highlight that our FasterNet is much simpler than many other models in terms of architectural design, which showcases the feasibility of designing simple yet powerful neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">FasterNet on downstream tasks</head><p>To further evaluate the generalization ability of Faster-Net, we conduct experiments on the challenging COCO dataset <ref type="bibr" target="#b35">[36]</ref> for object detection and instance segmentation. As a common practice, we employ the ImageNet pre-trained FasterNet as a backbone and equip it with the popular Mask R-CNN detector <ref type="bibr" target="#b18">[19]</ref>. To highlight the effectiveness of the backbone itself, we simply follow Pool-Former <ref type="bibr" target="#b78">[79]</ref>  Table 5. Ablation on the partial ratio, normalization, and activation of FasterNet. Rows highlighted in grey are the default settings. T0 * denotes T0 variants with modified network width and depth. schedule (12 epochs), a batch size of 16, and other training settings without further hyper-parameter tuning.</p><p>Tab. 4 shows the results for comparison between Faster-Net and representative models. FasterNet consistently outperforms ResNet and ResNext by having higher average precision (AP) with similar latency. Specifically, FasterNet-S yields +1.9 higher box AP and +2.4 higher mask AP compared to the standard baseline ResNet50. FasterNet is also competitive against the ViT variants. Under similar FLOPs, FasterNet-L reduces PVT-Large's latency by 38%, i.e., from 152.2 ms to 93.8 ms on GPU, and achieves +1.1 higher box AP and +0.4 higher mask AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>We conduct a brief ablation study on the value of partial ratio r and the choices of activation and normalization layers. We compare different variants in terms of ImageNet top-1 accuracy and on-device latency/throughput. Results are summarized in Tab. 5. For the partial ratio r, we set it to 1  4 for all FasterNet variants by default, which achieves higher accuracy, higher throughput, and lower latency at similar complexity. A too large partial ratio r would make PConv degrade to a regular Conv, while a too small value would render PConv less effective in capturing the spatial features. For the normalization layers, we choose Batch-Norm over LayerNorm because BatchNorm can be merged into its adjacent convolutional layers for faster inference while it is as effective as LayerNorm in our experiment. For the activation function, interestingly, we empirically found that GELU fits FasterNet-T0/T1 models more efficiently than ReLU. It, however, becomes opposite for FasterNet-T2/S/M/L. Here we only show two examples in Tab. 5 due to space constraint. We conjecture that GELU strengthens FasterNet-T0/T1 by having higher non-linearity, while the benefit fades away for larger FasterNet variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have investigated the common and unresolved issue that many established neural networks suffer from low floating-point operations per second (FLOPS). We have revisited a bottleneck operator, DWConv, and analyzed its main cause for a slowdown -frequent memory access. To overcome the issue and achieve faster neural networks, we have proposed a simple yet fast and effective operator, PConv, that can be readily plugged into many existing networks. We have further introduced our generalpurpose FasterNet, built upon our PConv, that achieves state-of-the-art speed and accuracy trade-off on various devices and vision tasks. We hope that our PConv and Faster-Net would inspire more research on simple yet effective neural networks, going beyond academia to impact the industry and community directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ImageNet-1k experimental settings</head><p>We provide ImageNet-1k training and evaluation settings in Tab. 6. They can be used for reproducing our main results in Tab. 3 and Fig. <ref type="figure">7</ref>. Different FasterNet variants vary in the magnitude of regularization and augmentation techniques. The magnitude increases as the model becomes larger to alleviate overfitting and improve accuracy. Note that most of the compared works in Tab. 3 and Fig. <ref type="figure">7</ref>, e.g., Mobile-ViT, EdgeNext, PVT, CycleMLP, ConvNeXt, Swin, etc., also adopt such advanced training techniques (ADT). Some even heavily rely on the hyper-parameter search. For others w/o ADT, i.e., ShuffleNetV2, MobileNetV2, and GhostNet, though the comparison is not totally fair, we include them for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Downstream tasks experimental settings</head><p>For object detection and instance segmentation on the COCO2017 dataset, we equip our FasterNet backbone with the popular Mask R-CNN detector. We use ImageNet-1k pre-trained weights to initialize the backbone and Xavier to initialize the add-on layers. Detailed settings are summarized in Tab. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Full comparison plots on ImageNet-1k</head><p>Fig. <ref type="figure">8</ref> shows the full comparison plots on ImageNet-1k, which is the extension of Fig. <ref type="figure">7</ref> in the main paper with a larger range of latency. Fig. <ref type="figure">8</ref> shows consistent results that FasterNet strikes better trade-offs than others in balancing accuracy and latency/throughput on GPU, CPU, and ARM processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed architectural configurations</head><p>We present the detailed architectural configurations in Tab. 8. While different FasterNet variants share a unified architecture, they vary in the network width (the number of channels) and network depth (the number of FasterNet blocks at each stage). The classifier at the end of the architecture is used for classification tasks but removed for other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More comparisons with related work</head><p>Improving FLOPS. There are a few other works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b75">76]</ref> also looking into the FLOPS issue and trying to improve Table 8. Configurations of different FasterNet variants. "Conv k c s" means a convolutional layer with the kernel size of k, the output channels of c, and the stride of s. "PConv k c s r" means a partial convolution with an extra parameter, the partial ratio of r. "FC 1000" means a fully connected layer with 1000 output channels. h √ó w is the input size while bi is the number of FasterNet blocks at stage i. The FLOPs are calculated given the input size of 224 √ó 224.</p><p>ViT to use fewer activation functions, while we intentionally remove them from the middle of PConv and PWConv, to minimize their error in approximating a regular Conv.</p><p>Other paradigms for efficient inference. Our work focuses on efficient network design, orthogonal to the other paradigms, e.g., neural architecture search (NAS) <ref type="bibr" target="#b12">[13]</ref>, network pruning <ref type="bibr" target="#b49">[50]</ref>, and knowledge distillation <ref type="bibr" target="#b22">[23]</ref>. They can be applied in this paper for better performance. However, we opt not to do so to keep our core idea centered and to make the performance gain clear and fair.</p><p>Other partial/masked convolution works. There are several works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> sharing similar names with our PConv. However, they differ a lot in objectives and methods. For example, they apply filters on partial pixels to exclude invalid patches <ref type="bibr" target="#b37">[38]</ref>, enable self-supervised learning <ref type="bibr" target="#b13">[14]</ref>, or synthesize novel images <ref type="bibr" target="#b36">[37]</ref>, while we target at the channel dimension for efficient inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Limitations and future work</head><p>We have demonstrated that PConv and FasterNet are fast and effective, being competitive with existing operators and networks. Yet there are some minor technical limitations of this paper. For one thing, PConv is designed to apply a regular convolution on only a part of the input channels while leaving the remaining ones untouched. Thus, the stride of the partial convolution should always be 1, in order to align the spatial resolution of the convolutional output and that of the untouched channels. Note that it is still feasible to down-sample the spatial resolution as there can be addi-tional downsampling layers in the architecture. And for another, our FasterNet is simply built upon convolutional operators with a possibly limited receptive field. Future efforts can be made to enlarge its receptive field and combine it with other operators to pursue higher accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Our partial convolution (PConv) is fast and efficient by applying filters on only a few input channels while leaving the remaining ones untouched. PConv obtains lower FLOPs than the regular convolution and higher FLOPS than the depthwise/group convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) FLOPS under varied FLOPs on CPU. Many existing neural networks suffer from low computational speed issues. Their effective FLOPS are lower than the popular ResNet50. By contrast, our FasterNet attains higher FLOPS. (b) Latency under varied FLOPs on CPU. Our FasterNet obtains lower latency than others with the same amount of FLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visualization of feature maps in an intermediate layer of a pre-trained ResNet50, with the top-left image as the input. Qualitatively, we can see the high redundancies across different channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Overall architecture of our FasterNet. It has four hierarchical stages, each with a stack of FasterNet blocks and preceded by an embedding or merging layer. The last three layers are used for feature classification. Within each FasterNet block, a PConv layer is followed by two PWConv layers. We put normalization and activation layers only after the middle layer to preserve the feature diversity and achieve lower latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of convolutional variants. A PConv followed by a PWConv (a) resembles a T-shaped Conv (b), which spends more computation on the center position compared to a regular Conv (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Histogram of salient position distribution for the regular Conv 3 √ó 3 filters in a pre-trained ResNet18. The histogram contains four kinds of bars, corresponding to different stages in the network. In all stages, the center position (position 5) appears as a salient position most frequently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 andFigure 7 .</head><label>77</label><figDesc>Figure 7. FasterNet has the highest efficiency in balancing accuracy-throughput and accuracy-latency trade-offs for different devices. To save space and make the plots more proportionate, we showcase network variants within a certain range of latency. Full plots can be found in the appendix, which show consistent results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>1 show that PConv is overall an appealing choice for high FLOPS with reduced FLOPs. It has only On-device FLOPS for different operations. PConv appears as an appealing choice for high FLOPS with reduced FLOPs.</figDesc><table><row><cell>Operator</cell><cell>Feature map size</cell><cell>FLOPs (M), √ó10 layers</cell><cell cols="6">GPU Throughput (fps) FLOPS (G/s) Latency (ms) FLOPS (G/s) Latency (ms) FLOPS (G/s) CPU ARM</cell></row><row><cell></cell><cell>96√ó56√ó56</cell><cell>2601</cell><cell>3010</cell><cell>7824</cell><cell>35.67</cell><cell>72.90</cell><cell>779.57</cell><cell>3.33</cell></row><row><cell></cell><cell>192√ó28√ó28</cell><cell>2601</cell><cell>4893</cell><cell>12717</cell><cell>28.41</cell><cell>91.53</cell><cell>619.64</cell><cell>4.19</cell></row><row><cell>Conv 3√ó3</cell><cell>384√ó14√ó14</cell><cell>2601</cell><cell>4558</cell><cell>11854</cell><cell>31.85</cell><cell>81.66</cell><cell>595.09</cell><cell>4.37</cell></row><row><cell></cell><cell>768√ó7√ó7</cell><cell>2601</cell><cell>3159</cell><cell>8212</cell><cell>62.71</cell><cell>41.47</cell><cell>662.17</cell><cell>3.92</cell></row><row><cell></cell><cell cols="2">Average</cell><cell>-</cell><cell>10151</cell><cell>-</cell><cell>71.89</cell><cell>-</cell><cell>3.95</cell></row><row><cell></cell><cell>96√ó56√ó56</cell><cell>162</cell><cell>2888</cell><cell>469</cell><cell>21.90</cell><cell>7.42</cell><cell>166.30</cell><cell>0.97</cell></row><row><cell>GConv 3√ó3 (16 groups)</cell><cell>192√ó28√ó28 384√ó14√ó14 768√ó7√ó7</cell><cell>162 162 162</cell><cell>10811 15534 16000</cell><cell>1754 2514 2598</cell><cell>7.58 4.40 4.28</cell><cell>21.44 36.88 37.97</cell><cell>96.22 63.57 65.20</cell><cell>1.68 2.55 2.49</cell></row><row><cell></cell><cell cols="2">Average</cell><cell>-</cell><cell>1833</cell><cell>-</cell><cell>25.93</cell><cell>-</cell><cell>1.92</cell></row><row><cell></cell><cell>96√ó56√ó56</cell><cell>27.09</cell><cell>11940</cell><cell>323</cell><cell>3.59</cell><cell>7.52</cell><cell>108.70</cell><cell>0.24</cell></row><row><cell></cell><cell>192√ó28√ó28</cell><cell>13.54</cell><cell>23358</cell><cell>315</cell><cell>1.97</cell><cell>6.86</cell><cell>82.01</cell><cell>0.16</cell></row><row><cell>DWConv 3√ó3</cell><cell>384√ó14√ó14</cell><cell>6.77</cell><cell>46377</cell><cell>313</cell><cell>1.06</cell><cell>6.35</cell><cell>94.89</cell><cell>0.07</cell></row><row><cell></cell><cell>768√ó7√ó7</cell><cell>3.38</cell><cell>88889</cell><cell>302</cell><cell>0.68</cell><cell>4.93</cell><cell>150.89</cell><cell>0.02</cell></row><row><cell></cell><cell cols="2">Average</cell><cell>-</cell><cell>313</cell><cell>-</cell><cell>6.42</cell><cell>-</cell><cell>0.12</cell></row><row><cell></cell><cell>96√ó56√ó56</cell><cell>162</cell><cell>9215</cell><cell>1493</cell><cell>5.46</cell><cell>29.67</cell><cell>85.30</cell><cell>1.90</cell></row><row><cell>PConv 3√ó3 (ours, with r = 1 4 )</cell><cell>192√ó28√ó28 384√ó14√ó14 768√ó7√ó7</cell><cell>162 162 162</cell><cell>14360 24408 32866</cell><cell>2326 3954 5324</cell><cell>3.09 3.58 5.02</cell><cell>52.43 45.25 32.27</cell><cell>66.46 49.98 48.30</cell><cell>2.44 3.24 3.35</cell></row><row><cell></cell><cell cols="2">Average</cell><cell>-</cell><cell>3274</cell><cell>-</cell><cell>39.91</cell><cell>-</cell><cell>2.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison on ImageNet-1k benchmark. Models with similar top-1 accuracy are grouped together. For each group, our FasterNet achieves the highest throughput on GPU and the lowest latency on CPU and ARM. All models are evaluated at 224 √ó 224 resolution except for the MobileViT and EdgeNeXt with 256 √ó 256. OOM is short for out of memory.</figDesc><table><row><cell>Network</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>Throughput on GPU (fps) ‚Üë</cell><cell>Latency on CPU (ms) ‚Üì</cell><cell>Latency on ARM (ms) ‚Üì</cell><cell>Acc. (%)</cell></row><row><cell cols="3">ShuffleNetV2 √ó1.5 [46] 3.5 0.30</cell><cell>4878</cell><cell>12.1</cell><cell cols="2">266 72.6</cell></row><row><cell>MobileNetV2 [54]</cell><cell cols="2">3.5 0.31</cell><cell>4198</cell><cell>12.2</cell><cell cols="2">442 72.0</cell></row><row><cell>MobileViT-XXS [48]</cell><cell cols="2">1.3 0.42</cell><cell>2393</cell><cell>30.8</cell><cell cols="2">348 69.0</cell></row><row><cell>EdgeNeXt-XXS [47]</cell><cell cols="2">1.3 0.26</cell><cell>2765</cell><cell>15.7</cell><cell cols="2">239 71.2</cell></row><row><cell>FasterNet-T0</cell><cell cols="2">3.9 0.34</cell><cell>6807</cell><cell>9.2</cell><cell cols="2">143 71.9</cell></row><row><cell>GhostNet √ó1.3 [17]</cell><cell cols="2">7.4 0.24</cell><cell>2988</cell><cell>17.9</cell><cell cols="2">481 75.7</cell></row><row><cell cols="3">ShuffleNetV2 √ó2 [46] 7.4 0.59</cell><cell>3339</cell><cell>17.8</cell><cell cols="2">403 74.9</cell></row><row><cell cols="3">MobileNetV2 √ó1.4 [54] 6.1 0.60</cell><cell>2711</cell><cell>22.6</cell><cell cols="2">650 74.7</cell></row><row><cell>MobileViT-XS [48]</cell><cell cols="2">2.3 1.05</cell><cell>1392</cell><cell>40.8</cell><cell cols="2">648 74.8</cell></row><row><cell>EdgeNeXt-XS [47]</cell><cell cols="2">2.3 0.54</cell><cell>1738</cell><cell>24.4</cell><cell cols="2">434 75.0</cell></row><row><cell>PVT-Tiny [72]</cell><cell cols="2">13.2 1.94</cell><cell>1266</cell><cell>55.6</cell><cell cols="2">708 75.1</cell></row><row><cell>FasterNet-T1</cell><cell cols="2">7.6 0.85</cell><cell>3782</cell><cell>17.7</cell><cell cols="2">285 76.2</cell></row><row><cell>CycleMLP-B1 [5]</cell><cell cols="2">15.2 2.10</cell><cell>865</cell><cell>116.1</cell><cell cols="2">892 79.1</cell></row><row><cell cols="3">PoolFormer-S12 [79] 11.9 1.82</cell><cell>1439</cell><cell>49.0</cell><cell cols="2">665 77.2</cell></row><row><cell>MobileViT-S [48]</cell><cell cols="2">5.6 2.03</cell><cell>1039</cell><cell>56.7</cell><cell cols="2">941 78.4</cell></row><row><cell>EdgeNeXt-S [47]</cell><cell cols="2">5.6 1.26</cell><cell>1128</cell><cell>39.2</cell><cell cols="2">743 79.4</cell></row><row><cell>ResNet50 [20, 42]</cell><cell cols="2">25.6 4.11</cell><cell>959</cell><cell>73.0</cell><cell cols="2">1131 78.8</cell></row><row><cell>FasterNet-T2</cell><cell cols="2">15.0 1.91</cell><cell>1991</cell><cell>33.5</cell><cell cols="2">497 78.9</cell></row><row><cell>CycleMLP-B2 [5]</cell><cell cols="2">26.8 3.90</cell><cell>528</cell><cell cols="3">186.3 1502 81.6</cell></row><row><cell cols="3">PoolFormer-S24 [79] 21.4 3.41</cell><cell>748</cell><cell>92.8</cell><cell cols="2">1261 80.3</cell></row><row><cell cols="3">PoolFormer-S36 [79] 30.9 5.00</cell><cell>507</cell><cell cols="3">138.0 1860 81.4</cell></row><row><cell>ConvNeXt-T [42]</cell><cell cols="2">28.6 4.47</cell><cell>657</cell><cell>86.3</cell><cell cols="2">1889 82.1</cell></row><row><cell>Swin-T [41]</cell><cell cols="2">28.3 4.51</cell><cell>609</cell><cell cols="3">122.2 1424 81.3</cell></row><row><cell>PVT-Small [72]</cell><cell cols="2">24.5 3.83</cell><cell>689</cell><cell>89.6</cell><cell cols="2">1345 79.8</cell></row><row><cell>PVT-Medium [72]</cell><cell cols="2">44.2 6.69</cell><cell>438</cell><cell cols="3">143.6 2142 81.2</cell></row><row><cell>FasterNet-S</cell><cell cols="2">31.1 4.56</cell><cell>1029</cell><cell>71.2</cell><cell cols="2">1103 81.3</cell></row><row><cell cols="3">PoolFormer-M36 [79] 56.2 8.80</cell><cell>320</cell><cell cols="3">215.0 2979 82.1</cell></row><row><cell>ConvNeXt-S [42]</cell><cell cols="2">50.2 8.71</cell><cell>377</cell><cell cols="3">153.2 3484 83.1</cell></row><row><cell>Swin-S [41]</cell><cell cols="2">49.6 8.77</cell><cell>348</cell><cell cols="3">224.2 2613 83.0</cell></row><row><cell>PVT-Large [72]</cell><cell cols="2">61.4 9.85</cell><cell>306</cell><cell cols="3">203.4 3101 81.7</cell></row><row><cell>FasterNet-M</cell><cell cols="2">53.5 8.74</cell><cell>500</cell><cell cols="3">129.5 2092 83.0</cell></row><row><cell cols="3">PoolFormer-M48 [79] 73.5 11.59</cell><cell>242</cell><cell cols="3">281.8 OOM 82.5</cell></row><row><cell>ConvNeXt-B [42]</cell><cell cols="2">88.6 15.38</cell><cell>253</cell><cell cols="3">257.1 OOM 83.8</cell></row><row><cell>Swin-B [41]</cell><cell cols="2">87.8 15.47</cell><cell>237</cell><cell cols="3">349.2 OOM 83.5</cell></row><row><cell>FasterNet-L</cell><cell cols="2">93.5 15.52</cell><cell>323</cell><cell cols="3">219.5 OOM 83.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>and adopt an AdamW optimizer, a 1√ó training Results on COCO object detection and instance segmentation benchmarks. FLOPs are calculated with image size (1280, 800).</figDesc><table><row><cell cols="2">Backbone</cell><cell></cell><cell>Params (M)</cell><cell cols="2">FLOPs (G)</cell><cell cols="2">Latency on GPU (ms)</cell><cell>AP b</cell><cell>AP b 50</cell><cell>AP b 75</cell><cell>AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell></row><row><cell cols="2">ResNet50 [20]</cell><cell></cell><cell>44.2</cell><cell cols="2">253</cell><cell></cell><cell>54.9</cell><cell>38.0</cell><cell>58.6</cell><cell>41.4</cell><cell>34.4</cell><cell>55.1</cell><cell>36.7</cell></row><row><cell cols="2">PoolFormer-S24 [79]</cell><cell></cell><cell>41.0</cell><cell cols="2">233</cell><cell></cell><cell>111.0</cell><cell>40.1</cell><cell>62.2</cell><cell>43.4</cell><cell>37.0</cell><cell>59.1</cell><cell>39.6</cell></row><row><cell cols="2">PVT-Small [72]</cell><cell></cell><cell>44.1</cell><cell cols="2">238</cell><cell></cell><cell>89.5</cell><cell>40.4</cell><cell>62.9</cell><cell>43.8</cell><cell>37.8</cell><cell>60.1</cell><cell>40.3</cell></row><row><cell cols="2">FasterNet-S</cell><cell></cell><cell>49.0</cell><cell cols="2">258</cell><cell></cell><cell>54.3</cell><cell>39.9</cell><cell>61.2</cell><cell>43.6</cell><cell>36.9</cell><cell>58.1</cell><cell>39.7</cell></row><row><cell cols="2">ResNet101 [20]</cell><cell></cell><cell>63.2</cell><cell cols="2">329</cell><cell></cell><cell>68.9</cell><cell>40.4</cell><cell>61.1</cell><cell>44.2</cell><cell>36.4</cell><cell>57.7</cell><cell>38.8</cell></row><row><cell cols="3">ResNeXt101-32√ó4d [77]</cell><cell>62.8</cell><cell cols="2">333</cell><cell></cell><cell>80.5</cell><cell>41.9</cell><cell>62.5</cell><cell>45.9</cell><cell>37.5</cell><cell>59.4</cell><cell>40.2</cell></row><row><cell cols="2">PoolFormer-S36 [79]</cell><cell></cell><cell>50.5</cell><cell cols="2">266</cell><cell></cell><cell>146.9</cell><cell>41.0</cell><cell>63.1</cell><cell>44.8</cell><cell>37.7</cell><cell>60.1</cell><cell>40.0</cell></row><row><cell cols="2">PVT-Medium [72]</cell><cell></cell><cell>63.9</cell><cell cols="2">295</cell><cell></cell><cell>117.3</cell><cell>42.0</cell><cell>64.4</cell><cell>45.6</cell><cell>39.0</cell><cell>61.6</cell><cell>42.1</cell></row><row><cell cols="2">FasterNet-M</cell><cell></cell><cell>71.2</cell><cell cols="2">344</cell><cell></cell><cell>71.4</cell><cell>43.0</cell><cell>64.4</cell><cell>47.4</cell><cell>39.1</cell><cell>61.5</cell><cell>42.3</cell></row><row><cell cols="3">ResNeXt101-64√ó4d [77]</cell><cell>101.9</cell><cell cols="2">487</cell><cell></cell><cell>112.9</cell><cell>42.8</cell><cell>63.8</cell><cell>47.3</cell><cell>38.4</cell><cell>60.6</cell><cell>41.3</cell></row><row><cell cols="2">PVT-Large [72]</cell><cell></cell><cell>81.0</cell><cell cols="2">358</cell><cell></cell><cell>152.2</cell><cell>42.9</cell><cell>65.0</cell><cell>46.6</cell><cell>39.5</cell><cell>61.9</cell><cell>42.5</cell></row><row><cell cols="2">FasterNet-L</cell><cell></cell><cell>110.9</cell><cell cols="2">484</cell><cell></cell><cell>93.8</cell><cell>44.0</cell><cell>65.6</cell><cell>48.2</cell><cell>39.9</cell><cell>62.3</cell><cell>43.0</cell></row><row><cell>Ablation</cell><cell>Variant</cell><cell cols="2">Throughput on GPU (fps)</cell><cell>Latency on CPU (ms)</cell><cell cols="2">Latency on ARM (ms)</cell><cell>Acc. (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">T0  *  w/ r = 1/2 6626</cell><cell>9.6</cell><cell cols="3">145 71.7</cell><cell></cell><cell></cell></row><row><cell>Partial ratio</cell><cell>T0 w/ r = 1/4</cell><cell></cell><cell>6807</cell><cell>9.2</cell><cell cols="3">143 71.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">T0  *  w/ r = 1/8 6204</cell><cell>8.9</cell><cell cols="3">140 71.3</cell><cell></cell><cell></cell></row><row><cell>Normalization</cell><cell>T0 w/ BN T0 w/ LN</cell><cell></cell><cell>6807 5515</cell><cell>9.2 10.7</cell><cell cols="3">143 71.9 159 71.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>T0 w/ ReLU</cell><cell></cell><cell>6929</cell><cell>8.2</cell><cell cols="3">114 71.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>T0  *  w/ ReLU</cell><cell></cell><cell>5866</cell><cell>9.3</cell><cell cols="3">143 71.7</cell><cell></cell><cell></cell></row><row><cell>Activation</cell><cell>T0 w/ GELU</cell><cell></cell><cell>6807</cell><cell>9.2</cell><cell cols="3">143 71.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>T2 w/ ReLU</cell><cell></cell><cell>1991</cell><cell>33.5</cell><cell cols="3">497 78.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>T2 w/ GELU</cell><cell></cell><cell>1985</cell><cell>35.4</cell><cell cols="3">557 78.7</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>ImageNet-1k training and evaluation settings for different FasterNet variants.</figDesc><table><row><cell>Variants</cell><cell>T0</cell><cell>T1</cell><cell>T2</cell><cell>S</cell><cell>M</cell><cell>L</cell></row><row><cell>Train Res</cell><cell cols="6">192 for epoch 1‚àº280, 224 for epoch 281‚àº300</cell></row><row><cell>Test Res</cell><cell></cell><cell></cell><cell>224</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Epochs</cell><cell></cell><cell></cell><cell>300</cell><cell></cell><cell></cell><cell></cell></row><row><cell># of forward pass</cell><cell></cell><cell></cell><cell>188k</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell cols="6">4096 4096 4096 4096 2048 2048</cell></row><row><cell>Optimizer</cell><cell></cell><cell></cell><cell cols="2">AdamW</cell><cell></cell><cell></cell></row><row><cell>Momentum</cell><cell></cell><cell></cell><cell cols="2">0.9/0.999</cell><cell></cell><cell></cell></row><row><cell>LR</cell><cell cols="6">0.004 0.004 0.004 0.004 0.002 0.002</cell></row><row><cell>LR decay</cell><cell></cell><cell></cell><cell cols="2">cosine</cell><cell></cell><cell></cell></row><row><cell>Weight decay</cell><cell cols="2">0.005 0.01</cell><cell>0.02</cell><cell>0.03</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Warmup epochs</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Warmup schedule</cell><cell></cell><cell></cell><cell>linear</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label smoothing</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropout</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stoch. Depth</cell><cell>‚úó</cell><cell>0.02</cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>Repeated Aug</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gradient Clip.</cell><cell>‚úó</cell><cell>‚úó</cell><cell>‚úó</cell><cell>‚úó</cell><cell>1</cell><cell>0.01</cell></row><row><cell>H. flip</cell><cell></cell><cell></cell><cell>‚úì</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RRC</cell><cell></cell><cell></cell><cell>‚úì</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rand Augment</cell><cell>‚úó</cell><cell cols="5">3/0.5 5/0.5 7/0.5 7/0.5 7/0.5</cell></row><row><cell>Auto Augment</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixup alpha</cell><cell>0.05</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell></row><row><cell>Cutmix alpha</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Erasing prob.</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color Jitter</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCA lighting</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SWA</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMA</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layer scale</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE loss</cell><cell></cell><cell></cell><cell>‚úì</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BCE loss</cell><cell></cell><cell></cell><cell>‚úó</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixed precision</cell><cell></cell><cell></cell><cell>‚úì</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test crop ratio</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 acc. (%)</cell><cell>71.9</cell><cell>76.2</cell><cell>78.9</cell><cell>81.3</cell><cell>83.0</cell><cell>83.5</cell></row><row><cell>Variants</cell><cell>S</cell><cell></cell><cell>M</cell><cell></cell><cell>L</cell><cell></cell></row><row><cell>Train and test Res</cell><cell cols="6">shorter side = 800, longer side ‚â§ 1333</cell></row><row><cell>Batch size</cell><cell></cell><cell></cell><cell cols="2">16 (2 on each GPU)</cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell></cell><cell></cell><cell cols="2">AdamW</cell><cell></cell><cell></cell></row><row><cell>Train schedule</cell><cell></cell><cell cols="4">1√ó schedule (12 epochs)</cell><cell></cell></row><row><cell>Weight decay</cell><cell></cell><cell></cell><cell cols="2">0.0001</cell><cell></cell><cell></cell></row><row><cell>Warmup schedule</cell><cell></cell><cell></cell><cell cols="2">linear</cell><cell></cell><cell></cell></row><row><cell>Warmup iterations</cell><cell></cell><cell></cell><cell>500</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR decay</cell><cell cols="6">StepLR at epoch 8 and 11 with decay rate 0.1</cell></row><row><cell>LR</cell><cell cols="2">0.0002</cell><cell>0.0001</cell><cell></cell><cell cols="2">0.0001</cell></row><row><cell>Stoch. Depth</cell><cell cols="2">0.15</cell><cell>0.2</cell><cell></cell><cell>0.3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Experimental settings of object detection and instance segmentation on the COCO2017 dataset.</figDesc><table><row><cell></cell><cell>84 82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FasterNet (ours) ShuffleNetV2</cell><cell cols="2">EdgeNeXt ResNet50</cell><cell>CycleMLP PVT</cell></row><row><cell>Top-1 Acc. (%)</cell><cell>72 74 76 78 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GhostNet MobileNetV2</cell><cell cols="2">PoolFormer MobileViT</cell><cell>Swin ConvNeXt</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>68</cell><cell>0</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell><cell>5000</cell><cell>6000</cell><cell>7000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Throughput on GPU (fps)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Acc. (%)</cell><cell>74 76 78 80 72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FasterNet (ours) ShuffleNetV2 GhostNet</cell><cell cols="2">EdgeNeXt ResNet50 PoolFormer</cell><cell>CycleMLP PVT Swin</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>MobileViT</cell><cell></cell><cell>ConvNeXt</cell></row><row><cell></cell><cell>68</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Latency on CPU (ms)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Acc. (%)</cell><cell>74 76 78 80 72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FasterNet (ours) ShuffleNetV2 GhostNet</cell><cell cols="2">EdgeNeXt ResNet50 PoolFormer</cell><cell>CycleMLP PVT Swin</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>MobileViT</cell><cell></cell><cell>ConvNeXt</cell></row><row><cell></cell><cell>68</cell><cell>0</cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell><cell>2500</cell><cell>3000</cell><cell>3500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Latency on ARM (ms)</cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 8. Comparison of FasterNet with state-of-the-art networks. FasterNet consistently achieves better accuracy-throughput (the top</cell></row><row><cell cols="7">plot) and accuracy-latency (the medium and bottom plots) trade-offs than others.</cell><cell></cell><cell></cell></row><row><cell cols="6">it. They generally follow existing operators and try to find</cell><cell cols="4">the sense that it prevents the operator from excessive mem-</cell></row><row><cell cols="6">their proper configurations, e.g., RepLKNet [11] simply in-</cell><cell cols="4">ory access and is computationally more efficient. From the</cell></row><row><cell cols="6">creases the kernel size while TRT-ViT [76] reorders differ-</cell><cell cols="4">perspective of low-rank approximations, PConv improves</cell></row><row><cell cols="6">ent blocks in the architecture. By contrast, this paper ad-</cell><cell cols="4">GConv by further reducing the intra-filter redundancy be-</cell></row><row><cell cols="6">vances the field by proposing a novel and efficient PConv,</cell><cell cols="3">yond the inter-filter redundancy [16].</cell></row><row><cell cols="6">opening up new directions and potentially larger room for FLOPS improvement.</cell><cell cols="4">FasterNet vs. ConvNeXt. Our FasterNet appears simi-lar to ConvNeXt [42] after substituting DWConv with our</cell></row><row><cell cols="6">PConv vs. GConv. PConv is schematically equivalent to</cell><cell cols="4">PConv. However, they are different in motivations. While</cell></row><row><cell cols="6">a modified GConv [31] that operates on a single group and</cell><cell cols="4">ConvNeXt searches for a better structure by trial and er-</cell></row><row><cell cols="6">leaves other groups untouched. Though simple, such a mod-</cell><cell cols="4">ror, we append PWConv after PConv to better aggregate in-</cell></row><row><cell cols="6">ification remains unexplored before. It's also significant in</cell><cell cols="4">formation from all channels. Moreover, ConvNeXt follows</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We follow a widely adopted definition of FLOPs, as the number of multiply-adds<ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b83">84]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p><ref type="bibr" target="#b15">16</ref> FLOPs of a regular Conv and achieves 10.5√ó, 6.2√ó, and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>22.8√ó higher FLOPS than the DWConv on GPU, CPU, and ARM, respectively. We are unsurprised to see that the regular Conv has the highest FLOPS as it has been constantly optimized for years. However, its total FLOPs and</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was supported, in part, by <rs type="funder">Hong Kong General Research Fund</rs> under grant number <rs type="grantNumber">16200120</rs>. The work of C.-H. Lee was supported, in part, by the <rs type="funder">NSF</rs> under Grant <rs type="grantNumber">IIS-2209921</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AQrQ6zV">
					<idno type="grant-number">16200120</idno>
				</org>
				<org type="funding" xml:id="_mSMdVy3">
					<idno type="grant-number">IIS-2209921</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we provide further details on the experimental settings, full comparison plots, architectural configurations, PConv implementations, comparisons with related work, limitations, and future work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2003">20014-20027, 2021. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14756</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tvconv: Efficient translation variant convolution for layout-aware visual processing</title>
		<author>
			<persName><forename type="first">Jierun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangtae</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-H</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12548" to="12558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<author>
			<persName><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10224</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mobileformer: Bridging mobilenet and transformer</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3435" to="3444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
	<note>Franc ¬∏ois Chollet</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Trans-formers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1997" to="2017" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.03892</idno>
		<title level="m">Convmae: Masked convolution meets masked autoencoders</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12259" to="12269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking depthwise separable convolutions: How intra-kernel correlations lead to improved mobilenets</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007">2020. 1, 3, 4, 7</date>
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model rubik&apos;s cube: Twisting resolution, depth and width for tinynets</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiulin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="19353" to="19364" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Piotr Doll√°r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008">2016. 2, 4, 7, 8</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tackling multipath and biased training data for imu-assisted ble proximity detection</title>
		<author>
			<persName><forename type="first">Tianlang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Printz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-H</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2022-IEEE Conference on Computer Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1259" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2752" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Lightvit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05557</idno>
		<title level="m">Towards light-weight convolutionfree vision transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 1, 3, 10</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Micronet: Improving image recognition with extremely low flops</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficientformer: Vision transformers at mobilenet speed</title>
		<author>
			<persName><forename type="first">Yanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01191</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Asmlp: An axial shifted mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Partial convolution for padding, inpainting, and image synthesis</title>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Are we ready for a new paradigm shift? a survey on visual deep mlp</title>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmi</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">100520</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2022. 1, 3, 7</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Soft: softmax-free transformer with linear complexity</title>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21297" to="21309" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007">2018. 1, 2, 3, 7</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10589</idno>
		<title level="m">Syed Waqas Zamir, Rao Muhammad Anwer, and Fahad Shahbaz Khan. Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 3, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Separable selfattention for mobile vision transformers</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02680</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.03436</idno>
		<title level="m">Edgevits: Competing light-weight cnns on mobile devices with vision transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007">2018. 1, 3, 4, 7</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St√©phane</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1687</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hetconv: Heterogeneous kernel-based convolutions for deep cnns</title>
		<author>
			<persName><forename type="first">Pravendra</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><forename type="middle">Kumar</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4835" to="4844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Quadtree attention for vision transformers</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02767</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07118</idno>
		<title level="m">Deit iii: Revenge of the vit</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Wadekar</surname></persName>
		</author>
		<author>
			<persName><surname>Chaurasia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15159</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">When shift operation meets vision transformer: An extremely simple alternative to attention mechanism</title>
		<author>
			<persName><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10801</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2021. 3, 7, 8</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Social ode: Multi-agent trajectory forecasting with neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="217" to="233" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXII</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<title level="m">Trt-vit: Tensorrt-oriented vision transformer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Condensenet v2: Sparse feature reactivation for deep networks</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3569" to="3578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Qiulin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qishuo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxin</forename><surname>Jia'nan Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Men</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12085</idno>
		<title level="m">Split to be slim: An overlooked redundancy in vanilla convolution</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4373" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A tree-based structure-aware transformer decoder for image-to-markup generation</title>
		<author>
			<persName><forename type="first">Shuhan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sizhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-H</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5751" to="5760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><forename type="middle">Ho</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jierun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Sumpena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-H</forename><surname>Gary Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangtae</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chul-Ho</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07889</idno>
		<title level="m">Semi-supervised learning with network embedding on ambient rf signals for geofencing services</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
