# LEAPS: A discrete neural sampler via locally equivariant networks

## Abstract

## 

We propose LEAPS, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call locally equivariant functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics.

## Introduction

A prevailing task across statistics and the sciences is to draw samples from a probability distribution whose probability density is known up to normalization. Solutions to this problem have applications in topics ranging across Bayesian uncertainty quantification [(Gelfand & Smith, 1990)](#b17), capturing the molecular dynamics of chemical compounds [(Berendsen et al., 1984;](#b7)[Allen & Tildesley, 2017)](#b5), and computational approaches to statistical and quantum physics [(Wilson, 1974;](#b49)[Duane et al., 1987;](#b13)[Faulkner & Livingstone, 2023)](#b14).


Recently, there has been rapid progress in development of generative models using techniques from dynamical measure transport, i.e. where data from a base distribution is transformed into samples from the target distribution via flow or diffusion processes [(Ho et al., 2020;](#b19)[Song et al., 2020;](#b39)[Albergo & Vanden-Eijnden, 2022;](#b1)[Albergo et al., 2023;](#b4)[Lipman et al., 2023;](#b22)[Liu et al., 2022)](#b24). More recently, these models could also be developed for discrete state spaces [(Campbell et al., 2022;](#b9)[Gat et al., 2025;](#b16)[Shaul et al., 2024;](#b37)[Campbell et al., 2024)](#b10) and general state spaces and Markov processes [(Holderrieth et al., 2024)](#).

While there have been various developments on adapting these non-equilibrium dynamics for sampling in continuous state spaces [(Zhang & Chen, 2022;](#b50)[Vargas et al., 2023;](#b45)[Máté & Fleuret, 2023;](#b28)[Tian et al., 2024;](#b42)[Albergo & Vanden-Eijnden, 2024;](#b2)[Richter & Berner, 2024;](#)[Akhound-Sadegh et al., 2024;](#b0)[Sun et al., 2024)](#b40), there is a lack of existing literature on such sampling approaches for discrete distributions. However, discrete data are prevalent in various applications, such as in the study of spin models in statistical physics, protein and genomic data, and language. To this end, we provide a new solution to the discrete sampling problem via CTMCs. Our method is similar in spirit to the results in [(Albergo & Vanden-Eijnden, 2024;](#b2)[Vargas et al., 2024)](#b46) but takes the necessary theoretical and computational leaps to make these approaches possible for discrete distributions. Our main contributions are:

• We introduce LEAPS, a non-equilibrium transport sampler for discrete distributions via CTMCs that combines annealed importance sampling and sequential Monte Carlo with learned measure transport.

• To define the importance weights, we derive a Radon-Nikodym derivative for reverse-time CTMCs, control of which minimizes the variance of these weights.

• We show that the measure transport can be learnt and the variance of the importance weights minimized by optimizing a physics-informed neural network (PINN) loss function.

• We make the computation of the PINN objective scalable by introducing the notion of a locally equivariant network. We show how to build locally equivariant versions of common neural network architectures, including attention and convolutions.

• We experimentally verify the correctness and efficacy of the resulting LEAPS algorithm in high dimensions via simulation of the Ising model.

## Setup and Assumptions

In this work, we are interested in the problem of sampling from a target distribution ρ 1 on a finite state space S. We refer to ρ 1 by its probability mass function (pmf) given by

$ρ 1 (x) = 1 Z 1 exp(-U 1 (x)) (x ∈ S),(1)$where we assume that we do not know the normalization constant Z 1 but only the function potential U 1 . Our goal is to produce samples X ∼ ρ 1 . To achieve this goal, it is common to construct a time-dependent probability mass function (pmf) (ρ t ) 0≤t≤1 over S which fulfils that ρ 0 has a distribution from which we can sample easily, e.g. ρ 0 = Unif S , and ρ 1 is our target of interest. We write ρ t as:

$ρ t (x) = 1 Z t exp(-U t (x)),(2)$$Z t = y∈S exp(-U t (y)), F t = -log Z t (3)$where Z t (or equivalently F t ) is unknown. The value F t is also called the free energy. Throughout, we assume that U t is continuously differentiable in t. For example, we can set U t (x) = tU 1 (x) so that ρ 0 = Unif S and we get that ρ t ∝ exp(-tU 1 (x)) that can be considered a form of temperature annealing.

## Sampling with CTMCs

In this work, we seek to sample from ρ 1 using continuoustime Markov chains (CTMCs). A CTMC (X t ) 0≤t≤1 is given by a set of random variables X t ∈ S (0 ≤ t ≤ 1) whose evolution is determined by a time-dependent rate matrix Q t (y, x) ∈ R (0 ≤ t ≤ 1, x, y ∈ S) which fulfills the conditions:

$Q t (y; x) ≥0 (for y ̸ = x) (4a) Q t (x; x) = - y̸ =x Q t (y, x) (for x ∈ S)(4b)$The rate matrix Q t determines the generator equation

$P[X t+h = y|X t = x] = 1 x=y + hQ t (y, x) + o(h) (5)$for all x, y ∈ S and h > 0 where o(h) describes an error function such that lim h→0 o(h)/h = 0. Because this equation describes the infinitesimal transition probabilities of the CTMC, we can sample from X t approximately via the following iterative Euler scheme:

$X t+h ∼ P[•|X t ] := (1 Xt=y + hQ t (y, X t )) y∈S (6)$where P[•|X t ] describes a valid probability distribution for small h > 0 by the conditions we imposed on Q t (see (4)).

Our goal is to find a Q t that is a solution to the Kolmogorov forward equation (KFE)

$∂ t ρ t (x) = y∈S Q t (x, y)ρ t (y), ρ t=0 = ρ 0 .(7)$Fulfilling the KFE is a necessary and sufficient condition to ensure that the distribution of walkers initialized as X 0 ∼ ρ 0 and evolving according to (6) follow the prescribed path ρ t , in particular such that X t=1 ∼ ρ 1 . Remark 3.1. While the problem we focus on this work is sampling, the contributions of this work hold more generally for the problem of finding a CTMC that follows a prescribed time-varying density ρ t , i.e. that the condition in (7) holds.

## Proactive Importance Sampling

In general, the CTMC (X t ) 0≤t≤1 with arbitrary Q t will have different marginals than ρ t . To still obtain an unbiased estimator, it is common to use importance sampling (IS) to reweight samples obtained while simulating X t or sequential Monte Carlo (SMC) [(Doucet et al., 2001)](#b12) to resample the walkers along the trajectory. Here, we introduce a time-evolving set of log-weights A t ∈ R for 0 ≤ t ≤ 1 to re-weight the distribution of X t to a distribution µ t defined such that for all h :

$S → R E x∼µt [h(x)] = E[exp(A t )h(X t )] E[exp(A t )] ⇔ µ t (x) = E[exp(A t )|X t = x] y∈S E[exp(A t )|X t = y] ,$where E[•] denotes expectation over the process (X t , A t ). Intuitively, the distribution µ t is obtained by re-weighting samples from the current distribution of X t . This effectively means that from a finite number of samples (X 1 t , A 1 t ), . . . , (X n t , A n t ), we can obtain a Monte Carlo estimator via

$E x∼µt [h(x)] ≈ n i=1 exp(A i t ) n j=1 exp(A j t ) h(X i t )(8)$Our goal is to find a scheme of computing A t such that its reweighted distribution coincides with the target densities ρ t :

$µ t = ρ t (0 ≤ t ≤ 1)(9)$In particular, this would mean that ( [8](#formula_8)) is a good approximation for large n.

Proactive importance sampling. We next propose an IS scheme of computing weights A t . Before we provide a formal derivation, we provide a heuristic derivation of our proposed scheme in the following paragraph. Intuitively, the log-weights A t should accumulate the deviation from the true distribution of X t to the desired distribution ρ t . We can rephrase this as "accumulating the error of the KFE" that one may want to write as the difference between both sides of (7):

$∂ t ρ t (x) - y∈S Q t (x, y)ρ t (y)$As we do not know the normalization constant Z t , it is intuitive to divide by ρ t (x) to get

$∂ t ρ t (x) ρ t (x) - y∈S Q t (x, y) ρ t (y) ρ t (x) Using that ∂ t U t (x) = -∂ t log ρ t (x) = -∂ t ρ t (x)/ρ t (x),$we obtain equivalently:

$K t ρ t (x) = -∂ t U t (x) - y∈S Q t (x, y) ρ t (y) ρ t (x)(10)$where we defined a new operator K t ρ t . Intuitively, the operator K t measures the violation from the KFE in logspace and it is intuitive to define A t as the accumulated error of that violation, i.e. as the integral

$A t = t 0 K s ρ s (X s )ds(11)$To simulate A t alongside X t , we use the approximate update role:

$A t+h = A t + hK t ρ t (X t )$We call this proactive importance sampling (proactivate IS) as the update operator K t anticipates where X t is jumping to. We next provide a rigorous characterization of A t defined in this manner.

## Proactivate IS via Radon-Nikodym Derivatives

A priori, it is not clear that the log-weights A t that we obtain via the proactive rule fulfil the desired condition in (9) (i.e. provide a valid IS scheme). Beyond showing this property, we show that there are many possible IS schemes but the proactive update rule is optimal among a natural family of IS schemes. To do so, we present a set of Radon-Nikodym derivatives in path space.

Let X be a state space and P, Q be two probability measures over X . Then the Radon-Nikodym derivative (RND) dQ dP allows to express expected values of Q via expected values of P. Specifically, the RND is a function dQ dP : X → R such that for any (bounded, measurable) function G : X → R it holds that:

$E X∼Q [G(X)] = E X∼P G(X) dQ dP (X)$The state space that we are interested in is the space X of CTMC trajectories. Specifically, for a trajectory we denote with X t -= lim t ′ ↑t X t ′ the left limit and with X t + = lim t ′ ↓t X t ′ the right limit. The space X of CTMC trajectories is then defined as

$X = {X : [0, 1] → S|X t -exists and X t + = X t },$i.e. all trajectories that are continuous from the right with left limits. Such trajectories are commonly called càdlàg trajectories. In other words, jumps (switches between states) happen if and only if X t -̸ = X t . We consider path distributions (or path measures), i.e. probability distributions over trajectories. For a CTMC X = (X t ) 0≤t≤1 with rate matrix Q t and initial distribution µ, we denote the corresponding path distribution as -→ P µ,Q where the arrow -→ P denotes that we go forward in time. Similarly, we denote with ← -P ν,Q ′ a CTMC running in reverse time initialized with ν. We present the following proposition whose proof can be found in Appendix A: Proposition 5.1. Let µ, ν be two initial distributions over S. Let Q t , Q ′ t be two rate matrices. Then the Radon-Nikodym derivative of the corresponding path distributions running in opposite time over the time interval [0, t] is given by:

$log d ← - P ν,Q ′ d - → P µ,Q (X) = log(ν(X t )) -log(µ(X 0 )) + t 0 Q ′ s (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Q ′ s (X - s , X s ) Q s (X s , X - s )$where we sum over all points where X s jumps in the last term.

Let us now revisit our goal of finding an IS scheme to sample from the target distribution ρ 1 . The key idea is to construct a CTMC running in reverse-time with initial distribution ρ t and then use the RND from Proposition 5.1. For a function h : S → R, we can then express its expectation under ρ t as:

$E x∼ρt [h(x)] = E X∼ ← - P ρ t ,Q ′ [h(X t )] = E Y∼ - → P ρ 0 ,Q h(X t ) d ← - P ρt,Q ′ d - → P ρ0,Q (X)(12)$i.e. the RND d ← -

$P ρ 1 ,Q ′ d - → P ρ 0 ,Q (X)$gives a valid set of importance weights. Note that this holds for arbitrary Q ′ t .

However, to sample efficiently, it is crucial that the IS weights have low variance. Therefore, we will now derive the optimal IS scheme of this form. Ideally the weights will have zero variance -in other words the RND d ← -

$P ρ 1 ,Q ′ d - → P ρ 0 ,Q (X)$will be constant = 1. This is the case if and only if the path measures are the same, i.e. if the CTMC in reverse time is a time-reversal of the CTMC running in forward time. It is well-known that this is equivalent to:

$Q ′ t (y, x) = Q t (x, y) q t (y) q t (x) for all y ̸ = x (13$$)$where q t denotes the true marginal of X t , i.e. X t ∼ q t . As we strive to make q t = ρ t , it is natural to set q t = ρ t in (13) and define

$Q ′ t = Qt as Qt (y, x) =Q t (x, y) ρ t (y) ρ t (x) for all y ̸ = x (14a) Qt (x, x) = - y∈S,y̸ =x Q t (x, y) ρ t (y) ρ t (x)(14b)$Let us now return to the proactive update that we defined in (11). We can now rigorously characterize it. Plugging in the definition of Q, we can use Proposition 5.1 to obtain the main result of this section:

Theorem 5.2. For the proactivate updates A t as defined in ( [11](#formula_13)) and Qt as defined in ( [14](#formula_23)), it holds:

$A t + F t -F 0 = log d ← - P ρt, Q d - → P ρ0,Q (X)(15)$This implies that we obtain a valid IS scheme fulfilling:

$E x∼ρt [h(x)] = E[exp(A t )h(X t )] E[exp(A t )] (0 ≤ t ≤ 1)(16)$i.e. ( [9](#formula_9)) holds. Further, A t will have zero variance for every

$0 ≤ t ≤ 1 if and only if X t ∼ ρ t for all 0 ≤ t ≤ 1.$A proof can be bound in Appendix B. Note that Theorem 5.2 is useful because we can, in principle, compute A t , i.e. there are no unknown variables, and that this holds for arbitrary Q t . This theorem can be seen as a discrete state space equivalent of the generalized version of the Jarzynski equality [(Jarzynski, 1997;](#b20)[Vaikuntanathan & Jarzynski, 2008)](#b44) that has also recently been used for sampling in continuous spaces [(Vargas et al., 2024;](#b46)[Albergo & Vanden-Eijnden, 2024)](#b2). Finally, it is important to note that the IS scheme will not have zero variance if it does not hold that X t ∼ ρ t .

## PINN objective

As a next step, we introduce a learning procedure for learning an optimal rate matrix of a CTMC. For this, we denote with Q θ t a parameterized rate matrix with parameters θ (e.g. represented in a neural network). Our goal is to learn Q θ t such that X t ∼ ρ t is fulfilled for all 0 ≤ t ≤ 1. By Theorem 5.2 this equivalent to minimizing the variance of the IS weights. To measure the variance the weights, it is common to use the log-variance divergence [(Nüsken & Richter, 2023;](#b32)[Richter & Berner, 2023)](#b33) given by

$L log-var (θ; t) =V X∼Q [log d ← - P ρt, Qθ d - → P ρ0,Q θ (X)] =V X∼Q [A t + F t -F 0 ] =V X∼Q [A t ]$where Q is a reference measuring whose support covers the support of ← -P ρt, Qθ and -→ P ρ0,Q θ and where we used that F 0 , F t are constants. The above loss is tractable but we can bound it by a loss that is computationally more efficient. To do so, we use an auxiliary free energy network F ϕ t : R → R with parameters ϕ. Note that F ϕ t is a onedimensional function and therefore induces minimal additional computational cost. As before, let the operator K θ t be defined as:

$K θ t ρ t (x) = -∂ t U t (x) - y∈S Q θ t (x, y) ρ t (y) ρ t (x)(17)$Proposition 6.1. For any reference measure Q, the PINNobjective defined by

$L(θ, ϕ; t) = E s∼Unif [0,t] ,xs∼Qs |K θ s ρ s (x s ) -∂ s F ϕ s | 2$has a unique minimizer (θ * , ϕ * ) such that Q θ * t satisfies the KFE and F ϕ * t = F t is the free energy. Further, this objective is an upper bound to the log-variance divergence:

$L log-var (θ; t) ≤ t 2 L(θ, ϕ; t)$In particular, if L(θ, ϕ; t) = 0, then also L log-var (θ; t) = 0 and the variance of the IS weights is zero.

A proof can be found in Appendix C. Note that we can easily minimize the PINN objective via stochastic gradient descent (see Algorithm 1). It is "off-policy" as the reference distribution Q is arbitrary. This objective can be seen as the CTMC equivalent of that in [(Máté & Fleuret, 2023;](#b28)[Albergo & Vanden-Eijnden, 2024;](#b2)[Tian et al., 2024;](#b42)[Sun et al., 2024)](#b40).

## Adding Annealed IS and SMC

It is possible to add an arbitrary MCMC scheme to the above dynamics. This effectively combines an "unlearned" MCMC scheme with a learned transport. Most of the time, MCMC schemes are formulated as discrete time Markov chains. Hence, we first describe how they can be formulated as CTMCs. For every fixed 0 ≤ t ≤ 1, an MCMC scheme for the distribution ρ t is given by a stochastic matrix M t (y, x), i.e. M t (y, x) ≥ 0 with y∈S M t (y, x) = 1. These are constructed to satisfy the corresponding detailed balance condition:  



$y∈S Q MCMC t (x, y) ρ t (y) ρ t (x) = y∈S Q MCMC t (y, x) = 0$where we used (4). This implies that the rate matrix

$Q θ t (y, x) + Q MCMC t (y, x)$will have the same PINN loss and the same IS weights for the same trajectories -because the K θ t remains unchanged (Note that while the RND in (15) is the same, the path measures do change). Specifically, this means that we can sample and compute the weights via Algorithm 2. The parameter ϵ t controls "how much local MCMC mixing" we want to induce. Note that the IS weights can be used both for reweighting at the final time or in addition to resample the walkers along the trajectories, connecting it to the SMC literature [(Doucet et al., 2001)](#b12). We specify that one may want to do this whenever the effective sample size (ESS) (see Appendix G) drops below a threshold.

Generalization of AIS and SMC. For Q θ t = 0, the above dynamics describe a continuous formulation of AIS that can be simulated approximately via Algorithm 2. In particular, this means that the algorithm presented here is a strict generalization of AIS and SMC [(Neal, 2001;](#b29)[Doucet et al., 2001)](#b12). Note that in the h → 0, ϵ t → ∞ limit, LEAPS would

Algorithm 1 LEAPS training with optional replay buffer Require: B batch size, N time steps, model G θ t , free energy net F ϕ t , learning rate η, replay buffer B. 1: while not converged do 2: if use buffer then 3: (X m tm , A m tm , t m ) m=1,...,B ← SampleBatch(B) 4: else 5: (X m tm , A m tm , t m ) m=1,...,B ← Algorithm 2 6: end if 7:

$L(θ, ϕ) = 1 B m |K θ tm ρ tm (X m tm ) -∂ tm F ϕ tm | 2 8: θ ← θ -η ∇ θ L(θ, ϕ) 9: ϕ ← ϕ -η ∇ ϕ L(θ, ϕ) 10: end while Algorithm 2 LEAPS Sampling 1: Require: N time steps, M walkers, model F θ t , replay buffer B, MCMC kernel M t , density ρ t , coeff. ϵ t ≥ 0, resample thres. 0 ≤ δ ≤ 1 2: Init: X m 0 ∼ ρ 0 , A m 0 = 0 (m = 1, . . . , M ) 3: Set h = 1/N 4: for n = 0 to N -1 do 5: for m = 1 to M do 6: X m t ∼ M t (•, X m t ) with prob. hϵ t else X m t 7: X m t+h ∼ (1 Xt=y + hQ θ t (y, X t )) y∈S 8: A m t+h = A m t + hK θ t ρ t (X m t ) 9:$end for 10:

$t ← t + h 11:$if ESS(A t ) ≤ δ then 12:

X t = resample(X t , A t ) (m = 1, . . . , M )

13:

A t = 0 (m = 1, . . . , M )

14:

end if 15: end for 16: Optional: Store {(X m t , A m t , t)} t,m in B.

recover the exact distributions ρ t with AIS (i.e. even with Q θ t = 0). Of course, this asymptotic limit is not realizable in practice with finite number of steps, and the inclusion of Q θ t allows us to sample much more efficiently while still maintaining statistical guarantees.

## Efficient IS and training via local equivariance

We now turn to the question of how to make the above training procedure efficient. Note that for small state spaces S we could rely on analytical solutions to the KFE [(Campbell et al., 2022;](#b9)[Shaul et al., 2024)](#b37). In many applications, though, the state space S is so large that we cannot store |S| elements efficiently in a computer. Often state spaces S are of the form S = T d where T = {1, . . . , N } is a set of N tokens. We use the notation τ for a token, i.e. an element τ ∈ T . One then defines a notion of a neighbor y of x, i.e. an element y = (y 1 , . . . , y d ) that differs from x in at most one dimension (i.e. y i ̸ = x i for at most one i).

We denote as N (x) the set of all neighbors of x. We then restrict functional form of the rate matrices to only allow for jumps to neighbors, i.e. Q θ t (y, x) = 0 if y / ∈ N (x). One can then use a neural network Q θ t represented by the function

$Q θ t : S → (R N -1 ) d x → (Q θ t (τ, i|x)) i=1,...,d,τ ∈T \{xi}$i.e. the neural network is given by the function x → Q θ t (•|x) that returns for very dimension i a value for every token τ different from x i . We then parameterize a rate matrix via

$Q θ t (y, x) =        0 if y / ∈ N (x) Q θ t (y j , j|x) else if y j ̸ = x j - i,τ Q θ t (τ, i|x) if x = y (19)$This parameterization is commonly used in the context of discrete markov models ("discrete diffusion models") [(Campbell et al., 2022;](#b9)[2024)](#). With that, the operator K θ t in (10) becomes:

$K θ t ρ t (x) + ∂ t U t (x) = i=1,...,d y∈N (x), yi̸ =xi Q θ t (y i , i|x) -Q θ t (x i , i|y) ρ t (y) ρ t (x)$The key problem with the above update is that it requires us to evaluate the neural network |N (x)| times (for every neighbor y). This makes computing K θ t computationally prohibitively expensive. Hence, with the standard rate matrix parameterization, the proactive IS sampling scheme and training via the PINN-objective is very inefficient.

To make the computation of K θ t efficient, we choose to induce an inductive bias into our neural network architecture to compute K θ t with no additional cost. Specifically, we introduce here the notion of local equivariance. A neural network G θ t represented by the function

$G θ t : S → (R N -1 ) d x → (G θ t (τ, i|x)) i=1,...,d,τ ∈T \{xi}$is called locally equivariant if the following condition holds:

$G θ t (τ, i|x) = -G θ t (x i , i|Swap(x, i, τ )) (i = 1, . . . , d) where Swap(x, i, τ ) = (x 1 , . . . , x i-1 , τ, x i+1 , . . . , x d )$In other words, the function G θ t gives the "flux of probability" going from x to each neighbor. Local equivariance says that the flux from x to its neighbor is negative the flux from the neighbor to x (see Figure [2](#fig_3)). Therefore, every coordinate map F j is equivariant with respect to transformations of the j-th input ("locally" equivariant). Note that we do not specify how F i transforms for i ̸ = j under transformations of x j . This distinguishes it from "full" equivariance as, for example, used in geometric deep learning [(Bronstein et al., 2021;](#b8)[Weiler & Cesa, 2019;](#b48)[Thomas et al., 2018)](#b41). We can use a locally equivariant neural network to parameterize a rate matrix via:

$Q θ t (τ, j|x) = [G θ t (τ, j|x)] +(20)$where [z] + = max(z, 0) describes the ReLU operation. This representation is not a restriction (see Appendix D for a proof):

Proposition 8.1 (Universal representation theorem). For any CTMC as in ( [19](#)) with marginals ρ t , there is a corresponding CTMC with the same marginals ρ t and a rate matrix that can be written as in ( [20](#formula_45)) for a locally equivariant function G θ t .

Crucially, this representation allows to efficiently compute K θ t in one forward pass of the neural network:

$K θ t ρ t (x) + ∂ t U t (x) = i=1,...,d y∈N (x), yi̸ =xi [G θ t (y i , i|x)] + -[-G θ t (y i , i|x)] + ρ t (y) ρ t (x)$With this, we can efficiently compute the proactive IS update A t and evaluate the PINN-objective. Therefore, this construction allows for scalable training and efficient proactivate importance sampling. We call the resulting algorithm LEAPS (Locally Equivariant discrete Annealed Proactivate Sampler). The acronym also highlights that we use a Markov jump process to sample (i.e. that takes "leaps" through space).

Remark 8.2. It is important to note that with the above construction, we would need to naively evaluate ρ t (x) as often as d times for a single computation of K θ t . However, note that the sum goes over all neighbors over x. Therefore, this can be a considered as computing a discrete gradient. Such ratios can often be computed efficiently, e.g. for many scientific and physical models it is often only 2× the computation compared to a single evaluation of ρ t (x).

## Design of locally equivariant networks

It remains to be stated how to construct locally equivariant neural networks -a question we turn to in this section. We will focus on three fundamental designs used throughout deep learning: Multilayer perceptrons (MLPs), attention layers, and convolutional neural networks. Usually, tokens are embedded as token vectors e τ ∈ R cin where c in is the embedding dimension. We therefore consider the embedded sequence of vectors: x = (x 1 , . . . , x d ) ∈ (R cin ) d as the input to the neural network.

## Multilayer perceptron (MLP).

Let us set c in = 1 in this paragraph for readability. Let W 1 , . . . , W k ∈ R d×d be a set of weight matrices with a zero diagonal, i.e. W ii = 0 for i = 1, . . . , d. Further, let σ : R → R be an activation function and ω τ ∈ R k be a learnable projection vector for every token τ ∈ T . Then define the map:

$G θ t (τ, j|x) = k i=1 (ω i τ -ω i xj )σ(W i x) j$where σ(W i x) j denotes the j-th element of the vector obtained by multiplying the vector x with the matrix W i and applying the activation function a componentwise. One can easily show that this is a locally equivariant neural network corresponding to a MLP with one hidden layer.

Locally-equivariant attention (LEA) layer. Let us consider a self-attention layer operating on keys k j = k j (x j ), queries q j = q j (x j ), and values v j = v j (x j ) -each of which is a function of element x j . We define the locally equivariant attention layer then as:

$G θ t (τ, j|x) = (ω τ -ω xj ) T    s̸ =j exp(k T s q j ) t̸ =j exp(k T t q j ) v s   $It can be shown that this layer is locally equivariant if the queries q j are independent of the sign of x j (i.e. q j (x j ) = q j (-x j )) which can be easily achieved. By stacking across multiple attention heads, one can create a locally equivariant MultiHeadAttention (LEA) with this.

## Hierchical local equivariance.

Local equivariance is different from "proper" equivariance in that the composition of locally equivariant functions is not locally equivariant in general. Therefore, we cannot simply compose locally equivariant neural network layers as we would do with "proper" equivariant neural networks. In particular, the above MLP and the attention layers cannot simply be composed as their composition would violate the local equivariance. This fundamentally changes considerations about how to compose layers and how to construct deep neural networks. We will now illustrate this for the case of convolutional neural networks.

Locally-equivariant convolutional (LEC) network. To construct a locally equivariant convolutional neural network (LEC), we assume that our data lies on a grid. A convolutional layer is characterized by a matrix W ∈ R (2k-1)×(2k-1) and its operation is denoted via k(W ) * x

where k denotes the convolutional kernel with weights W .

Here, we set the center of W to zero: W kk = 0 (i.e. such that corresponding location is effectively ignored). To stack such layers, we can make the output of the previous layer feed into the weights of the next layer:

$h 0 =(1, . . . , 1) T W i =σ(A i h i + b i ) + c j (i = 1, . . . , L) h i+1 =k t (W i ) * x (i = 1, . . . , L) H θ t (x) =h L where A i ∈ R di×di , b i ∈ R di , c i ∈ R di$are learnable tensors which operate on each coordinate independently (i.e. a 1x1 convolution) and σ : R → R is an activation function to make it non-linear. We call the resulting network

$H θ t : (R cin ) d →(R cout ) d x →(H θ t (1|x), . . . , H θ t (d|x))$the prediction head. Combined with a small network P θ t : T → R k that we call token projector, we define the full neural network as

$G θ t (τ, j|x) = (P θ t (e τ ) -P θ t (x j )) T H θ t (j|x)$In Appendix E, we verify that G θ t defined in this way is locally equivariant. With this construction, one can stack deep highly complex convolutional neural networks. Note that this convolutional neural network has two (separate) symmetries: it is geometrically translation equivariant and locally equivariant in the sense defined in this work.

## Related Work

CTMCs. CTMCs [(Campbell et al., 2022)](#b9) have been used for various applications in generative modeling ("discrete diffusion" models), including text and image generation [(Shi et al., 2024;](#b38)[Gat et al., 2025;](#b16)[Shaul et al., 2024;](#b37)[Sahoo et al., 2024)](#b36) and molecular design [(Gruver et al., 2023;](#b18)[Campbell et al., 2024;](#b10)[Lisanza et al., 2024)](#). While here we use a RND for CTMCs running in reverse time, one recovers the loss functions of these generative models considering a RND of two forward time CTMCs (in Appendix A, we show this in detail).

Transport and sampling. Over the past decade there has been continued interest in combining the statistical guarantees of MCMC and IS with learning transport maps. A nonparametric version of this is described in [(Marzouk et al., 2016)](#b25), and a parametric version through coupling-based normalizing flows was used to study systems in molecular dynamics and statistical physics [(Noé et al., 2019;](#b31)[Albergo et al., 2019;](#b3)[Gabrié et al., 2022;](#b15)[Wang et al., 2022)](#b47). These methods were extended to weave normalizing flows with SMC moves [(Arbel et al., 2021;](#b6)[Matthews et al., 2022)](#b26). More recent research focuses on replacing the generative model with a continuous flow or diffusion [(Zhang & Chen, 2022;](#b50)[Vargas et al., 2023;](#b45)[Akhound-Sadegh et al., 2024;](#b0)[Sun et al., 2024)](#b40). Our method is inspired by approaches combining measure transport with MCMC schemes [(Albergo & Vanden-Eijnden, 2024;](#b2)[Vargas et al., 2024)](#b46) and other samplers relying on PINN-based objectives in continuous spaces [(Máté & Fleuret, 2023;](#b28)[Tian et al., 2024;](#b42)[Sun et al., 2024)](#b40).

## Discrete Neural samplers. The primary alternative to what

we propose is to correct using importance weights arising from the estimate of the probability density computed using an autoregressive model [(Nicoli et al., 2020)](#b30). However, the computational cost of producing samples in this case scales naively as O(d), whereas we have no such constraint a priori in our case so long as the error in the Euler sampling scheme is kept small. Other work focuses on discrete formulations of normalizing flows, but the performant version reduces to an autoregressive model [(Tran et al., 2019)](#b43). Recent work has considered using CTMCs for sampling by parameterizing their evolution operators directly via tensor networks [(Causer et al., 2025)](#b11) as opposed to neural network representations of rate matrices here.

## Experiments

As a demonstration of the validity of LEAPS in high dimensions, we sample the Gibbs distribution associated to a 2-dimensional Ising model. We choose the Ising model because it is a well-studied model. In particular, it is a solvable model, which allows us to construct a robust ground truth Center: Difference in the histograms of the magnetization M (x) of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as M * . We denote by "no transport" the case of using annealed dynamics with just the marginal preserving MCMC updates to show that the transport from Qt is essential in our construction. Right: Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.

against which we can assess the various neural architectures underlying our algorithm. Configurations of the L × L Ising lattice follow the target distribution ρ 1 (x) = e -βH(x)+F1 where β is the inverse temperature of the system, F 1 the free energy, and H(x) : {-1, 1} L×L → R is the Hamiltonian for the model defined as

$H(x) = -J ⟨i,j⟩ x i x j + µ i x i .(21)$Here, J is the interaction strength, ⟨i, j⟩ denotes summation over nearest neighbors of spins x i , x j and µ is the magnetic moment. Neighboring spins are uncorrelated at high temperature but reach a critical correlation when the temperature drops behold a certain threshold. We use LEAPS to reproduce the statistics of the theory on a 15 × 15 lattice at parameters β = 0.7, J = 0.4, which approach this threshold, and compare our results against a ground truth of long-run Glauber dynamics, an efficient algorithm for simulation in this parameter regime. Note that this corresponds to a d = 15 × 15 = 225 dimensional space. To make ρ t time dependent, we make the parameters of J t , µ t , β t linear functions of time, starting from the non-interacting case J 0 = 0.

Results. We compare three different realizations of our method, one using LEA, and the other two using deep LEC that vary in depth. For all generated samples, we use 100 steps of integration of (6). We benchmark them on the effective sample size (ESS), which is a standard measure of how many effective samples our model gives according to the variance of the importance weights (see details Appendix G). In addition, we compute various observables using the Ising configurations generated by our model, such as histograms of the magnetization M (x) = i x i compared to ground truth, as well as the two point connected correlation function

$G conn (r) = E[x i x i+r ] -E[x i ] E[x i+r ]. (22$$)$The latter is a measure of the dependency between spin values a distance r in lattice separation. In Figure [4](#fig_5), we show in the leftmost panel that the convolutional architecture outperforms the attention-based method, and the performance gap grows as we make the LEC network deeper. In the center panel, the difference in histograms of the magnetization of lattice configurations for our models as compared to ground truth samples is shown to be statistically zero, whereas relying on local MCMC alone for the same number of sampling steps (plotted in purple) illustrates that the dynamics have not converged. In the right plot, we see clear agreement between our learned sampler and the ground truth for the connected two point function. These results show that LEAPS can be an efficient simulator of complex, high dimensional target distributions.

## Discussion

In this work, we present the LEAPS algorithm that allows to learn a non-equilibrium sampler via CTMCs parameterized by a locally equivariant neural network that is trained to minimize the variance of proactivate IS weights. A natural direction of future work will be to connect the ideas presented here with guidance or reward fine-tuning of generative CTMC models (discrete diffusion) -a problem known to be strongly tied to sampling. Further, LEAPS could easily be extended to sample across a whole family of distributions as opposed to only for a single, fixed target. Finally, we anticipate that the use of locally equivariant neural networks as well as the proactive IS scheme presented here might be useful more broadly for probabilistic models.

## Impact Statement

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

A. Proof of Proposition 5.1

Without loss of generality, we set the final time point to be t = 1. We compute for a bounded continuous function Φ : X → R:

$E X∼ - → P µ,Q [Φ(X)] = lim n→∞ E X∼ - → P µ,Q [Φ(X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 )] = lim n→∞ E X∼ ← - P ν,Q ′   Φ(X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 ) - → P µ,Q (X 0 , X 1/n , . . . , X n-1 n , X 1 ) ← - P ν,Q ′ (X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 )   = lim n→∞ E X∼ ← - P ν,Q ′   Φ(X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 ) µ(X 0 ) ν(X 1 ) s=0,1/n,2/n,..., n-1 n - → P µ,Q (X s+h |X s ) ← - P ν,Q ′ (X s |X s+h )   = lim n→∞ E X∼ ← - P ν,Q ′   Φ(X) µ(X 0 ) ν(X 1 ) exp   h s,X s+h =Xs Q s (X s , X s ) -Q ′ s+h (X s , X s )   s,X s+h ̸ =Xs Q s (X s+h , X s ) Q ′ s+h (X s , X s+h )   =E X∼ ← - P ν,Q ′   Φ(X) µ(X 0 ) ν(X 1 ) exp   1 0 Q s (X s , X s ) -Q ′ s (X s , X s )ds   s,X s -̸ =Xs Q s (X s , X s -) Q ′ s (X s -, X s )  $where we used the definition of the rate matrix Q t , Q ′ t , the continuity of Q ′ t in t and the fact that the left and right Riemann integral coincide. As Φ was arbitrary, the RND is given by:

$log d - → P µ,Q d ← - P ν,Q ′ (X) = log(µ(X 0 )) -log(ν(X 1 )) + 1 0 Q s (X s , X s ) -Q ′ s (X s , X s )ds + s,X - s ̸ =Xs log Q s (X s , X - s ) Q ′ s (X - s , X s )$B. Proof of Theorem 5.2

Specifically, we use Proposition 5.1 to compute

$log d ← - P ρt, Qt d - → P ρ0,Qt (X) = log(ρ t (X t )) -log(ρ 0 (X 0 )) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s ) =F t -F 0 -U t (X t ) + U 0 (X 0 ) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s )$Note that the function t → U t (X t ) is a piecewise differentiable function. Therefore, we can apply the fundamental theorem on every differentiable "piece" and get:

$U t (X t ) -U 0 (X 0 ) = t 0 ∂ s U t (X t )ds + s,X - s ̸ =Xs U s (X s ) -U s (X - s ) = t 0 ∂ s U s (X s )ds + s,X - s ̸ =Xs log ρ s (X - s ) ρ s (X s )$Next, we can insert the above equation and get:

$log d ← - P ρt, Qt d - → P ρ0,Qt (X) =F t -F 0 -U t (X t ) + U 0 (Y 0 ) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s ) =F t -F 0 - t 0 ∂ s U s (X s )ds - s,X - s ̸ =Xs log ρ s (X - s ) ρ s (X s ) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s ) =F t -F 0 - t 0 ∂ s U s (X s )ds + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log     Qs (X - s , X s ) Q s (X s , X - s ) ρ s (X s ) ρ s (X - s ) =1     =F t -F 0 - t 0 ∂ s U s (X s )ds + t 0 - y̸ =Xs Q s (X s , y) ρ t (y) ρ t (X s ) -Q s (X s , X s )ds + 0 =F t -F 0 +   - t 0 ∂ s U s (X s )ds - t 0 y∈S Q s (X s , y) ρ t (y) ρ t (X s ) ds   =F t -F 0 + A t$where we used the definition of A t in (11) and the definition of Qt in ( [14](#formula_23)). Note that for h = 1, we get that

$1 = E x∼ρt [h(x)] = E[exp(A t + F t -F 0 )] = E[exp(A t )] exp(F t -F 0 ) because F t , F 0 are constants. Therefore, in particular E[exp(A t )] = exp(F 0 -F t ) = Z t /Z 0 .$Note that we assume that Z 0 = 1 as we know ρ 0 . Therefore, E[exp(A t )] = Z t . This proves (16).

C. Proof of Proposition 6.1

We can use the variational formulation of the variance as the minimizer of the mean squared error to derive a computationally more efficient upper bound, i.e. we can re-express for every 0 ≤ t ≤ 1:

$L log-var (θ; t) =V X∼Q [A t ] = min Ft∈R E X∼Q [|A t -Ft | 2 ] =t 2 min ∂s Fs∈R,0≤s≤t$E X∼Q   | 1 t t 0 K θ s ρ s (X s ) -∂ s Fs ds| 2   ≤t 2 min ∂s Fs∈R,0≤s≤t E X∼Q   1 t t 0 |K θ s ρ s (X s ) -∂ s Fs | 2 ds   =t 2 min ∂s Fs∈R,0≤s≤t E s∼Unif [0,1] ,Xs∼Qs |K θ s ρ s (X s ) -∂ s Fs | 2

where we used Jensen's inequality and denote with Q s the marginal of Q at time s. We now arrive at the result by replacing the above with the free energy network F ϕ t . Further, note that the above bound is tight for Q-almost every X: K θ s ρ s (X s ) -∂ s F s = C(X 0:t ) is a constant in time s. However, this constant may depend on X. Further, note that

$Q s (X s ) = |K θ s ρ s (X s ) -∂ s F s | 2$Now, let us return to the proof of Proposition 8.1. Given a rate matrix Q t , we can now use a one-way rate matrix Qt with the same marginals and define function:

F t (τ, i|x) = Qt (y, x) if Q t (y, x) > 0 -Qt (x, y) otherwise where y = Swap(x, i, τ )

By construction, it holds that F t (τ, i|x) is locally equivariant and that [F t (τ, i|x)] + = Qt (y, x). This finishes the proof.

## E. Local equivariance of ConvNet

To verify the local equivariance, one can compute G θ t (τ, i|x) =(P θ t (e τ ) -P θ t (x i )) T H θ t (i|x) = -(P θ t (x i ) -P θ t (e τ )) T H θ t (i|x) = -(P θ t (x i ) -P θ t (e τ )) T H θ t (i|Swap(x, i, τ )) = -F t (x i , i|Swap(x, i, τ )), where we have used the invariance of the projection head H θ t (i|x) to changes in the i-th dimension. This shows the local equivariance.

## F. Recovering loss functions for CTMC models via RNDs

We discuss here in more detail how the Radon-Nikodym derivatives (RNDs) presented in Proposition 5.1 relate to the construction of loss function for CTMC generative models, also called "discrete diffusion" models. The connection lies in the fact that the loss function of these models relies on RNDs of two CTMCs running both in forward time. We can prove the following statement: Proposition F.1. Let µ, ν be two initial distributions over S. Let Q t , Q ′ t be two rate matrices. Then the Radon-Nikodym derivative of the corresponding path distributions in forward time over the interval [0, t] is given by:

$log d - → P µ,Q d - → P ν,Q ′ (X) = log dµ dν (X 0 ) + t 0 Q s (X s , X s ) -Q ′ s (X s , X s )ds + s,X - s ̸ =Xs log Q s (X s , X - s ) Q ′ s (X s , X - s )$where we sum over all points where X s jumps in the last term.

The proof of the above formula is very similar to the proof of Proposition 5.1 and an analogous formula also appeared in [(Campbell et al., 2024)](#b10), for example. The above proposition allows us to by compute the KL-divergence:

$D KL ( - → P µ,Q 1 || - → P ν,Q ′ 1 ) ≤D KL ( - → P µ,Q || - → P ν,Q ′ ) =E X∼ - → P µ,Q log d - → P µ,Q d - → P ν,Q ′ (X) =D KL (µ||ν) + E X∼ - → P µ,Q   1 0 Q t (X t , X t ) -Q ′ t (X t , X t )dt + t,X - t ̸ =Xt log Q t (X t , X - t ) Q ′ t (X t , X - t )   =D KL (µ||ν) + E t∼Unif [0,1] ,xt∼ - → P µ,Q t [Q t (X t , X t ) -Q ′ t (X t , X t )] + E X∼ - → P µ,Q   t,X - t ̸ =Xt log Q t (X t , X - t ) Q ′ t (X t , X - t )   =D KL (µ||ν) + E t∼Unif [0,1] ,xt∼ - → P µ,Q t [Q t (X t , X t ) -Q ′ t (X t , X t )] + 1 0 E Xt∼ - → P µ,Q t   y̸ =Xt Q t (y; X t ) log Q t (y; X t ) Q ′ t (y, X t )   dt =D KL (µ||ν) + E t∼Unif [0,1] ,Xt∼ - → P µ,Q t   y̸ =Xt Q ′ t (y, X t ) -Q t (y, X t ) + Q t (y; X t ) log Q t (y; X t ) Q ′ t (y, X t )  $where we have used the data processing inequality in the first term. Having a parameterized model Q ′ t = Q θ t , this leads to the following loss:

$L(θ) =D KL - → P µ,Q || - → P µ,Q θ t =D KL (µ||ν) + E t∼Unif [0,1] ,Xt∼ - → P µ,Q t   y̸ =Xt Q θ t (y, X t ) -Q t (y, X t ) log Q θ t (y, X t )   + C$where Q t is some reference process. The above recovers loss functions in the context of CTMC and jump generative models [(Campbell et al., 2022;](#b9)[Gat et al., 2025;](#b16)[Shaul et al., 2024;](#b37)[Campbell et al., 2024)](#b10) and Euclidean jump models [(Holderrieth et al., 2024, Section D.1.)](#). Note that the above loss cannot be used for the purposes of sampling in a straight-forward manner because we do not have access to samples from the marginals of the ground reference -→ P µ,Q .

## G. Numerical experiments

Effective Sample Size We use the self-normalized definition of the effective sample size such that, given the log weights A t associated to N CTMC instances, the ESS at time t in the generation is given by:

$ESS t = N -1 N t=1 exp A i t 2 N -1 N i=1 exp 2A i i(23)$
## G.1. Ising model experiments

Here we provide details of the numerical implementation of our study of the L = 15 Ising model. For the locally equivariant attention (LEA) mechanism, we use 40 attention heads, each with query, key, and value matrices of dimension 40x40. As such, there are about 350,000 parameters in the model. In addition, the locally equivariant convolutional net (LEC) of depth three uses kernel sizes of [[5,](#)[7,](#)[15]](#), while the depth five version uses [[3,](#)[5,](#)[7,](#)[9,](#)[15]](#), amounting to around 100,000 parameters.

![Figure 1. Illustration of the LEAPS algorithm. LEAPS allows to learn a dynamical transport of discrete distributions from t = 0 to t = 1 (blue). Sample are generated via the simulation of a Continuous-time Markov chain (yellow). Further, importance sampling weights allow to correct training errors trading off sample efficiency (red).]()



![Figure 2. Visualization of local equivariance. Two tokens T = {-1, +1} and d = 6. Local equivariance means that the "flux" to transition to a neighbor is the negative of the flux of transitioning from that neighbor back.]()

![Figure 3. Overview of locally equivariant convolutional neural network architecture.]()

![Figure 4. Performance metrics of LEAPS on the L = 15, J = 0.4, β = 0.7 Ising model with the LEA and LEC architectures, using 100 annealing steps for sample generation. Left: Effective sample size of LEAPS samplers over training. Green area denotes an annealing phase during training where t is increased from 0 to 1. Increasing the depth of LEC significantly improves performance.Center: Difference in the histograms of the magnetization M (x) of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as M * . We denote by "no transport" the case of using annealed dynamics with just the marginal preserving MCMC updates to show that the transport from Qt is essential in our construction. Right: Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.]()

