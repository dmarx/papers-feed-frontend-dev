<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEAPS: A discrete neural sampler via locally equivariant networks</title>
				<funder>
					<orgName type="full">DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE)</orgName>
				</funder>
				<funder ref="#_rGJrjcS">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_6UmWAkz #_UAwQZUd">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Machine Learning for Pharmaceutical Discovery and Synthesis</orgName>
					<orgName type="abbreviated">MLPDS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-31">January 31, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peter</forename><surname>Holderrieth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Society of Fellow</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Artificial Intelligence and Fundamental Interactions</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEAPS: A discrete neural sampler via locally equivariant networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-31">January 31, 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">A305010DD65C29AF7CDA60F6B2267819</idno>
					<idno type="arXiv">arXiv:2502.10843v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose LEAPS, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call locally equivariant functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A prevailing task across statistics and the sciences is to draw samples from a probability distribution whose probability density is known up to normalization. Solutions to this problem have applications in topics ranging across Bayesian uncertainty quantification <ref type="bibr" target="#b17">(Gelfand &amp; Smith, 1990)</ref>, capturing the molecular dynamics of chemical compounds <ref type="bibr" target="#b7">(Berendsen et al., 1984;</ref><ref type="bibr" target="#b5">Allen &amp; Tildesley, 2017)</ref>, and computational approaches to statistical and quantum physics <ref type="bibr" target="#b49">(Wilson, 1974;</ref><ref type="bibr" target="#b13">Duane et al., 1987;</ref><ref type="bibr" target="#b14">Faulkner &amp; Livingstone, 2023)</ref>.</p><p>⇢ 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U N d t F a 2 3 i g 2 Q V F x B l n 4 U 7 5 q n j h c = " &gt; A A A D / X i c h Z N f b 9 M w E M C 9 F d g o f 9 b B I y 8 R F R J C o U r a t Z S 3 a S D B C 2 I g u k 1 q q 8 p x r p l V x 4 5 s Z 1 2 J I j 4 I r + w N 8 c o 3 Q e L b c O k y a W 0 Q n O T o d P e 7 8 + X u H C S C G + t 5 v z c 2 a z d u 3 t r a v l 2 / c / f e / Z 3 G 7 o M j o 1 L N Y M C U U P o k o A Y E l z C w 3 A o 4 S T T Q O B B w H M x e F f 7 j M 9 C G K / n J L h I Y x z S S f M o Z t W i a N H Z G B z y K R q G a S 6 q 1 m k 8 a T a / l L c W p K n 6 p N E k p h 5 P d z V 8 Y z d I Y p G W C G j P 0 v c S O M 6 o t Z w L y + i g 1 k F A 2 o x E M w z O e G E l j M O P s f F l 8 7 j x B f + h M l c Y j r b O 0 X g / K a G z M I g 6 Q j K k 9 N e u + w v h X X 6 D p D O x K A R m j k o H I / 8 c N U z v t j z M u k 9 S C Z J d F T l P h W O U U X X R C r o F Z s U C F M s 3 x V x 1 2 S j V l F n u 9 W q G I F A K n 8 e q l l s 8 + V y 3 P O 2 E i V F l L Y R A c q 9 O L L A S m 9 H J k p h V T P e M y M v n 6 P Y m B F E e h Q u x 6 f f Q a c B w a 3 m F f 3 i e A w U o / y 0 Z U R z E 9 z 3 E 8 0 c g t t H + B X F 6 B q G H K E K a 4 Z s s J Z T Q N U o 1 + H Q V 5 5 r X 2 O i 6 u S d s v v n 6 + i s a L Q K S Q Z x / f H C D q + r 2 X b r t T h R Y g h J q X W L u L Y L / r + t 0 K F 2 k A W W L 9 I l 0 f T x X T E F 7 l 6 m A e z 3 P 7 V U j C / D q 3 5 3 Z 9 t 4 M Y C r 4 E f 3 3 v q 8 p R u + X 3 W r 0 P e 8 3 9 d v k m t s k j 8 p g 8 J T 5 5 Q f b J W 3 J I B o S R l H w l 3 8 h F 7 U v t o v a 9 9 u M S 3 d w o Y x 6 S F a n 9 / A P x 8 l L j &lt; / l a t e x i t &gt; The most salient approach to such sampling problems is Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b27">(Metropolis et al., 1953;</ref><ref type="bibr" target="#b35">Robert et al., 1999)</ref>, in which a randomized process is simulated whose equilibrium is the distribution of interest. While powerful and widely applied, MCMC methods can be inefficient as they suffer from slow convergence times into equilibrium, especially for distributions exhibiting multimodality. Therefore, MCMC is often combined with other techniques that rely on non-equilibrium dynamics, e.g. via annealing from a simpler distribution with annealed importance sampling (AIS) <ref type="bibr" target="#b21">(Kahn &amp; Harris, 1951;</ref><ref type="bibr" target="#b29">Neal, 2001)</ref> or sequential Monte Carlo methods (SMC) <ref type="bibr" target="#b12">(Doucet et al., 2001)</ref>. Even then, the variance of these importance weights may be untenably large, and making sampling algorithms more efficient remains an active area of research. Inspired by the rapid progress in deep learning, there has been extensive interest in augmenting contemporary sampling algorithms with learning <ref type="bibr" target="#b31">(Noé et al., 2019;</ref><ref type="bibr" target="#b3">Albergo et al., 2019;</ref><ref type="bibr" target="#b15">Gabrié et al., 2022;</ref><ref type="bibr" target="#b30">Nicoli et al., 2020;</ref><ref type="bibr" target="#b26">Matthews et al., 2022)</ref>, while still maintaining their statistical guarantees.</p><p>Recently, there has been rapid progress in development of generative models using techniques from dynamical measure transport, i.e. where data from a base distribution is transformed into samples from the target distribution via flow or diffusion processes <ref type="bibr" target="#b19">(Ho et al., 2020;</ref><ref type="bibr" target="#b39">Song et al., 2020;</ref><ref type="bibr" target="#b1">Albergo &amp; Vanden-Eijnden, 2022;</ref><ref type="bibr" target="#b4">Albergo et al., 2023;</ref><ref type="bibr" target="#b22">Lipman et al., 2023;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>. More recently, these models could also be developed for discrete state spaces <ref type="bibr" target="#b9">(Campbell et al., 2022;</ref><ref type="bibr" target="#b16">Gat et al., 2025;</ref><ref type="bibr" target="#b37">Shaul et al., 2024;</ref><ref type="bibr" target="#b10">Campbell et al., 2024)</ref> and general state spaces and Markov processes <ref type="bibr">(Holderrieth et al., 2024)</ref>.</p><p>While there have been various developments on adapting these non-equilibrium dynamics for sampling in continuous state spaces <ref type="bibr" target="#b50">(Zhang &amp; Chen, 2022;</ref><ref type="bibr" target="#b45">Vargas et al., 2023;</ref><ref type="bibr" target="#b28">Máté &amp; Fleuret, 2023;</ref><ref type="bibr" target="#b42">Tian et al., 2024;</ref><ref type="bibr" target="#b2">Albergo &amp; Vanden-Eijnden, 2024;</ref><ref type="bibr">Richter &amp; Berner, 2024;</ref><ref type="bibr" target="#b0">Akhound-Sadegh et al., 2024;</ref><ref type="bibr" target="#b40">Sun et al., 2024)</ref>, there is a lack of existing literature on such sampling approaches for discrete distributions. However, discrete data are prevalent in various applications, such as in the study of spin models in statistical physics, protein and genomic data, and language. To this end, we provide a new solution to the discrete sampling problem via CTMCs. Our method is similar in spirit to the results in <ref type="bibr" target="#b2">(Albergo &amp; Vanden-Eijnden, 2024;</ref><ref type="bibr" target="#b46">Vargas et al., 2024)</ref> but takes the necessary theoretical and computational leaps to make these approaches possible for discrete distributions. Our main contributions are:</p><p>• We introduce LEAPS, a non-equilibrium transport sampler for discrete distributions via CTMCs that combines annealed importance sampling and sequential Monte Carlo with learned measure transport.</p><p>• To define the importance weights, we derive a Radon-Nikodym derivative for reverse-time CTMCs, control of which minimizes the variance of these weights.</p><p>• We show that the measure transport can be learnt and the variance of the importance weights minimized by optimizing a physics-informed neural network (PINN) loss function.</p><p>• We make the computation of the PINN objective scalable by introducing the notion of a locally equivariant network. We show how to build locally equivariant versions of common neural network architectures, including attention and convolutions.</p><p>• We experimentally verify the correctness and efficacy of the resulting LEAPS algorithm in high dimensions via simulation of the Ising model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Setup and Assumptions</head><p>In this work, we are interested in the problem of sampling from a target distribution ρ 1 on a finite state space S. We refer to ρ 1 by its probability mass function (pmf) given by</p><formula xml:id="formula_0">ρ 1 (x) = 1 Z 1 exp(-U 1 (x)) (x ∈ S),<label>(1)</label></formula><p>where we assume that we do not know the normalization constant Z 1 but only the function potential U 1 . Our goal is to produce samples X ∼ ρ 1 . To achieve this goal, it is common to construct a time-dependent probability mass function (pmf) (ρ t ) 0≤t≤1 over S which fulfils that ρ 0 has a distribution from which we can sample easily, e.g. ρ 0 = Unif S , and ρ 1 is our target of interest. We write ρ t as:</p><formula xml:id="formula_1">ρ t (x) = 1 Z t exp(-U t (x)),<label>(2)</label></formula><formula xml:id="formula_2">Z t = y∈S exp(-U t (y)), F t = -log Z t (3)</formula><p>where Z t (or equivalently F t ) is unknown. The value F t is also called the free energy. Throughout, we assume that U t is continuously differentiable in t. For example, we can set U t (x) = tU 1 (x) so that ρ 0 = Unif S and we get that ρ t ∝ exp(-tU 1 (x)) that can be considered a form of temperature annealing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sampling with CTMCs</head><p>In this work, we seek to sample from ρ 1 using continuoustime Markov chains (CTMCs). A CTMC (X t ) 0≤t≤1 is given by a set of random variables X t ∈ S (0 ≤ t ≤ 1) whose evolution is determined by a time-dependent rate matrix Q t (y, x) ∈ R (0 ≤ t ≤ 1, x, y ∈ S) which fulfills the conditions:</p><formula xml:id="formula_3">Q t (y; x) ≥0 (for y ̸ = x) (4a) Q t (x; x) = - y̸ =x Q t (y, x) (for x ∈ S)<label>(4b)</label></formula><p>The rate matrix Q t determines the generator equation</p><formula xml:id="formula_4">P[X t+h = y|X t = x] = 1 x=y + hQ t (y, x) + o(h) (5)</formula><p>for all x, y ∈ S and h &gt; 0 where o(h) describes an error function such that lim h→0 o(h)/h = 0. Because this equation describes the infinitesimal transition probabilities of the CTMC, we can sample from X t approximately via the following iterative Euler scheme:</p><formula xml:id="formula_5">X t+h ∼ P[•|X t ] := (1 Xt=y + hQ t (y, X t )) y∈S (6)</formula><p>where P[•|X t ] describes a valid probability distribution for small h &gt; 0 by the conditions we imposed on Q t (see (4)).</p><p>Our goal is to find a Q t that is a solution to the Kolmogorov forward equation (KFE)</p><formula xml:id="formula_6">∂ t ρ t (x) = y∈S Q t (x, y)ρ t (y), ρ t=0 = ρ 0 .<label>(7)</label></formula><p>Fulfilling the KFE is a necessary and sufficient condition to ensure that the distribution of walkers initialized as X 0 ∼ ρ 0 and evolving according to (6) follow the prescribed path ρ t , in particular such that X t=1 ∼ ρ 1 . Remark 3.1. While the problem we focus on this work is sampling, the contributions of this work hold more generally for the problem of finding a CTMC that follows a prescribed time-varying density ρ t , i.e. that the condition in (7) holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proactive Importance Sampling</head><p>In general, the CTMC (X t ) 0≤t≤1 with arbitrary Q t will have different marginals than ρ t . To still obtain an unbiased estimator, it is common to use importance sampling (IS) to reweight samples obtained while simulating X t or sequential Monte Carlo (SMC) <ref type="bibr" target="#b12">(Doucet et al., 2001)</ref> to resample the walkers along the trajectory. Here, we introduce a time-evolving set of log-weights A t ∈ R for 0 ≤ t ≤ 1 to re-weight the distribution of X t to a distribution µ t defined such that for all h :</p><formula xml:id="formula_7">S → R E x∼µt [h(x)] = E[exp(A t )h(X t )] E[exp(A t )] ⇔ µ t (x) = E[exp(A t )|X t = x] y∈S E[exp(A t )|X t = y] ,</formula><p>where E[•] denotes expectation over the process (X t , A t ). Intuitively, the distribution µ t is obtained by re-weighting samples from the current distribution of X t . This effectively means that from a finite number of samples (X 1 t , A 1 t ), . . . , (X n t , A n t ), we can obtain a Monte Carlo estimator via</p><formula xml:id="formula_8">E x∼µt [h(x)] ≈ n i=1 exp(A i t ) n j=1 exp(A j t ) h(X i t )<label>(8)</label></formula><p>Our goal is to find a scheme of computing A t such that its reweighted distribution coincides with the target densities ρ t :</p><formula xml:id="formula_9">µ t = ρ t (0 ≤ t ≤ 1)<label>(9)</label></formula><p>In particular, this would mean that ( <ref type="formula" target="#formula_8">8</ref>) is a good approximation for large n.</p><p>Proactive importance sampling. We next propose an IS scheme of computing weights A t . Before we provide a formal derivation, we provide a heuristic derivation of our proposed scheme in the following paragraph. Intuitively, the log-weights A t should accumulate the deviation from the true distribution of X t to the desired distribution ρ t . We can rephrase this as "accumulating the error of the KFE" that one may want to write as the difference between both sides of (7):</p><formula xml:id="formula_10">∂ t ρ t (x) - y∈S Q t (x, y)ρ t (y)</formula><p>As we do not know the normalization constant Z t , it is intuitive to divide by ρ t (x) to get</p><formula xml:id="formula_11">∂ t ρ t (x) ρ t (x) - y∈S Q t (x, y) ρ t (y) ρ t (x) Using that ∂ t U t (x) = -∂ t log ρ t (x) = -∂ t ρ t (x)/ρ t (x),</formula><p>we obtain equivalently:</p><formula xml:id="formula_12">K t ρ t (x) = -∂ t U t (x) - y∈S Q t (x, y) ρ t (y) ρ t (x)<label>(10)</label></formula><p>where we defined a new operator K t ρ t . Intuitively, the operator K t measures the violation from the KFE in logspace and it is intuitive to define A t as the accumulated error of that violation, i.e. as the integral</p><formula xml:id="formula_13">A t = t 0 K s ρ s (X s )ds<label>(11)</label></formula><p>To simulate A t alongside X t , we use the approximate update role:</p><formula xml:id="formula_14">A t+h = A t + hK t ρ t (X t )</formula><p>We call this proactive importance sampling (proactivate IS) as the update operator K t anticipates where X t is jumping to. We next provide a rigorous characterization of A t defined in this manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proactivate IS via Radon-Nikodym Derivatives</head><p>A priori, it is not clear that the log-weights A t that we obtain via the proactive rule fulfil the desired condition in (9) (i.e. provide a valid IS scheme). Beyond showing this property, we show that there are many possible IS schemes but the proactive update rule is optimal among a natural family of IS schemes. To do so, we present a set of Radon-Nikodym derivatives in path space.</p><p>Let X be a state space and P, Q be two probability measures over X . Then the Radon-Nikodym derivative (RND) dQ dP allows to express expected values of Q via expected values of P. Specifically, the RND is a function dQ dP : X → R such that for any (bounded, measurable) function G : X → R it holds that:</p><formula xml:id="formula_15">E X∼Q [G(X)] = E X∼P G(X) dQ dP (X)</formula><p>The state space that we are interested in is the space X of CTMC trajectories. Specifically, for a trajectory we denote with X t -= lim t ′ ↑t X t ′ the left limit and with X t + = lim t ′ ↓t X t ′ the right limit. The space X of CTMC trajectories is then defined as</p><formula xml:id="formula_16">X = {X : [0, 1] → S|X t -exists and X t + = X t },</formula><p>i.e. all trajectories that are continuous from the right with left limits. Such trajectories are commonly called càdlàg trajectories. In other words, jumps (switches between states) happen if and only if X t -̸ = X t . We consider path distributions (or path measures), i.e. probability distributions over trajectories. For a CTMC X = (X t ) 0≤t≤1 with rate matrix Q t and initial distribution µ, we denote the corresponding path distribution as -→ P µ,Q where the arrow -→ P denotes that we go forward in time. Similarly, we denote with ← -P ν,Q ′ a CTMC running in reverse time initialized with ν. We present the following proposition whose proof can be found in Appendix A: Proposition 5.1. Let µ, ν be two initial distributions over S. Let Q t , Q ′ t be two rate matrices. Then the Radon-Nikodym derivative of the corresponding path distributions running in opposite time over the time interval [0, t] is given by:</p><formula xml:id="formula_17">log d ← - P ν,Q ′ d - → P µ,Q (X) = log(ν(X t )) -log(µ(X 0 )) + t 0 Q ′ s (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Q ′ s (X - s , X s ) Q s (X s , X - s )</formula><p>where we sum over all points where X s jumps in the last term.</p><p>Let us now revisit our goal of finding an IS scheme to sample from the target distribution ρ 1 . The key idea is to construct a CTMC running in reverse-time with initial distribution ρ t and then use the RND from Proposition 5.1. For a function h : S → R, we can then express its expectation under ρ t as:</p><formula xml:id="formula_18">E x∼ρt [h(x)] = E X∼ ← - P ρ t ,Q ′ [h(X t )] = E Y∼ - → P ρ 0 ,Q h(X t ) d ← - P ρt,Q ′ d - → P ρ0,Q (X)<label>(12)</label></formula><p>i.e. the RND d ← -</p><formula xml:id="formula_19">P ρ 1 ,Q ′ d - → P ρ 0 ,Q (X)</formula><p>gives a valid set of importance weights. Note that this holds for arbitrary Q ′ t .</p><p>However, to sample efficiently, it is crucial that the IS weights have low variance. Therefore, we will now derive the optimal IS scheme of this form. Ideally the weights will have zero variance -in other words the RND d ← -</p><formula xml:id="formula_20">P ρ 1 ,Q ′ d - → P ρ 0 ,Q (X)</formula><p>will be constant = 1. This is the case if and only if the path measures are the same, i.e. if the CTMC in reverse time is a time-reversal of the CTMC running in forward time. It is well-known that this is equivalent to:</p><formula xml:id="formula_21">Q ′ t (y, x) = Q t (x, y) q t (y) q t (x) for all y ̸ = x (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>where q t denotes the true marginal of X t , i.e. X t ∼ q t . As we strive to make q t = ρ t , it is natural to set q t = ρ t in (13) and define</p><formula xml:id="formula_23">Q ′ t = Qt as Qt (y, x) =Q t (x, y) ρ t (y) ρ t (x) for all y ̸ = x (14a) Qt (x, x) = - y∈S,y̸ =x Q t (x, y) ρ t (y) ρ t (x)<label>(14b)</label></formula><p>Let us now return to the proactive update that we defined in (11). We can now rigorously characterize it. Plugging in the definition of Q, we can use Proposition 5.1 to obtain the main result of this section:</p><p>Theorem 5.2. For the proactivate updates A t as defined in ( <ref type="formula" target="#formula_13">11</ref>) and Qt as defined in ( <ref type="formula" target="#formula_23">14</ref>), it holds:</p><formula xml:id="formula_24">A t + F t -F 0 = log d ← - P ρt, Q d - → P ρ0,Q (X)<label>(15)</label></formula><p>This implies that we obtain a valid IS scheme fulfilling:</p><formula xml:id="formula_25">E x∼ρt [h(x)] = E[exp(A t )h(X t )] E[exp(A t )] (0 ≤ t ≤ 1)<label>(16)</label></formula><p>i.e. ( <ref type="formula" target="#formula_9">9</ref>) holds. Further, A t will have zero variance for every</p><formula xml:id="formula_26">0 ≤ t ≤ 1 if and only if X t ∼ ρ t for all 0 ≤ t ≤ 1.</formula><p>A proof can be bound in Appendix B. Note that Theorem 5.2 is useful because we can, in principle, compute A t , i.e. there are no unknown variables, and that this holds for arbitrary Q t . This theorem can be seen as a discrete state space equivalent of the generalized version of the Jarzynski equality <ref type="bibr" target="#b20">(Jarzynski, 1997;</ref><ref type="bibr" target="#b44">Vaikuntanathan &amp; Jarzynski, 2008)</ref> that has also recently been used for sampling in continuous spaces <ref type="bibr" target="#b46">(Vargas et al., 2024;</ref><ref type="bibr" target="#b2">Albergo &amp; Vanden-Eijnden, 2024)</ref>. Finally, it is important to note that the IS scheme will not have zero variance if it does not hold that X t ∼ ρ t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PINN objective</head><p>As a next step, we introduce a learning procedure for learning an optimal rate matrix of a CTMC. For this, we denote with Q θ t a parameterized rate matrix with parameters θ (e.g. represented in a neural network). Our goal is to learn Q θ t such that X t ∼ ρ t is fulfilled for all 0 ≤ t ≤ 1. By Theorem 5.2 this equivalent to minimizing the variance of the IS weights. To measure the variance the weights, it is common to use the log-variance divergence <ref type="bibr" target="#b32">(Nüsken &amp; Richter, 2023;</ref><ref type="bibr" target="#b33">Richter &amp; Berner, 2023)</ref> given by</p><formula xml:id="formula_27">L log-var (θ; t) =V X∼Q [log d ← - P ρt, Qθ d - → P ρ0,Q θ (X)] =V X∼Q [A t + F t -F 0 ] =V X∼Q [A t ]</formula><p>where Q is a reference measuring whose support covers the support of ← -P ρt, Qθ and -→ P ρ0,Q θ and where we used that F 0 , F t are constants. The above loss is tractable but we can bound it by a loss that is computationally more efficient. To do so, we use an auxiliary free energy network F ϕ t : R → R with parameters ϕ. Note that F ϕ t is a onedimensional function and therefore induces minimal additional computational cost. As before, let the operator K θ t be defined as:</p><formula xml:id="formula_28">K θ t ρ t (x) = -∂ t U t (x) - y∈S Q θ t (x, y) ρ t (y) ρ t (x)<label>(17)</label></formula><p>Proposition 6.1. For any reference measure Q, the PINNobjective defined by</p><formula xml:id="formula_29">L(θ, ϕ; t) = E s∼Unif [0,t] ,xs∼Qs |K θ s ρ s (x s ) -∂ s F ϕ s | 2</formula><p>has a unique minimizer (θ * , ϕ * ) such that Q θ * t satisfies the KFE and F ϕ * t = F t is the free energy. Further, this objective is an upper bound to the log-variance divergence:</p><formula xml:id="formula_30">L log-var (θ; t) ≤ t 2 L(θ, ϕ; t)</formula><p>In particular, if L(θ, ϕ; t) = 0, then also L log-var (θ; t) = 0 and the variance of the IS weights is zero.</p><p>A proof can be found in Appendix C. Note that we can easily minimize the PINN objective via stochastic gradient descent (see Algorithm 1). It is "off-policy" as the reference distribution Q is arbitrary. This objective can be seen as the CTMC equivalent of that in <ref type="bibr" target="#b28">(Máté &amp; Fleuret, 2023;</ref><ref type="bibr" target="#b2">Albergo &amp; Vanden-Eijnden, 2024;</ref><ref type="bibr" target="#b42">Tian et al., 2024;</ref><ref type="bibr" target="#b40">Sun et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Adding Annealed IS and SMC</head><p>It is possible to add an arbitrary MCMC scheme to the above dynamics. This effectively combines an "unlearned" MCMC scheme with a learned transport. Most of the time, MCMC schemes are formulated as discrete time Markov chains. Hence, we first describe how they can be formulated as CTMCs. For every fixed 0 ≤ t ≤ 1, an MCMC scheme for the distribution ρ t is given by a stochastic matrix M t (y, x), i.e. M t (y, x) ≥ 0 with y∈S M t (y, x) = 1. These are constructed to satisfy the corresponding detailed balance condition:  </p><formula xml:id="formula_31">M t (y, x)ρ t (x) = M t (x, y)ρ t (y) (18) = = &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x D B S + 9 o q x D Z X 6 G L F X i Y y H o b f M z c = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a N r y O J F 4 9 g 5 J H A h s w O D Y z M z m 5 m Z o 1 k w x d 4 8 a A x X v 0 k b / 6 N A + x B w U o 6 q V R 1 p 7 s r i A X X x n W / n d z K 6 t r 6 R n 6 z s L W 9 s 7 t X 3 D 9 o 6 C h R D O s s E p F q B V S j 4 B L r h h u B r V g h D Q O B z W B 0 M / W b j 6 g 0 j + S 9 G c f o h 3 Q g e Z 8 z a q x U e + o W S 2 7 Z n Y E s E y 8 j J c h Q 7 R a / O r 2 I J S F K w w T V u u 2 5 s f F T q g x n A i e F T q I x p m x E B 9 i 2 V N I Q t Z / O D p 2 Q E 6 v 0 S D 9 S t q Q h M / X 3 R E p D r c d h Y D t D a o Z 6 0 Z u K / 3 n t x P S v / Z T L O D E o 2 X x R P x H E R G T 6 N e l x h c y I s S W U K W 5 v J W x I F W X G Z l O w I X i L L y + T x l n Z u y x f 1 M 5 L l b s s j j w c w T G c g g d X U I F b q E I d G C A 8 w y u 8 O Q / O i / P u f M x b c 0 4 2 c w h / 4 H z + A O 5 J j R Y = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N n 5 j p L c s h E 6 e Z X A R P C h 6 o C u a I s k = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 3 x d Q x 4 8 Z i I e U C y h N l J b z J m d n a Z m R X C k i / w 4 k E R r 3 6 S N / / G S b I H T S x o K K q 6 6 e 4 K E s G 1 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n f q t J 1 S a x / L B j B P 0 I z q Q P O S M G i v V x 7 1 S 2 a 2 4 M 5 B l 4 u W k D D l q v d J X t x + z N E J p m K B a d z w 3 M X 5 G l e F M 4 K T Y T T U m l I 3 o A D u W S h q h 9 r P Z o R N y a p U + C W N l S x o y U 3 9 P Z D T S e h w F t j O i Z q g X v a n 4 n 9 d J T X j j Z 1 w m q U H J 5 o v C V B A T k + n X p M 8 V M i P G l l C m u L 2 V s C F V l B m b T d G G 4 C 2 + v E y a 5 x X v q n J Z v y h X 7 / M 4 C n A M J 3 A G H l x D F e 6 g B</formula><formula xml:id="formula_32">+ A = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a N r y O J F 4 9 g 5 J H A h s w O D Y z M z m 5 m Z k 1 w w x d 4 8 a A x X v 0 k b / 6 N A + x B w U o 6 q V R 1 p 7 s r i A X X x n W / n d z K 6 t r 6 R n 6 z s L W 9 s 7 t X 3 D 9 o 6 C h R D O s s E p F q B V S j 4 B L r h h u B r V g h D Q O B z W B 0 M / W b j 6 g 0 j + S 9 G c f o h 3 Q g e Z 8 z a q x U e + o W S 2 7 Z n Y E s E y 8 j J c h Q 7 R a / O r 2 I J S F K w w T V u u 2 5 s f F T q g x n A i e F T q I x p m x E B 9 i 2 V N I Q t Z / O D p 2 Q E 6 v 0 S D 9 S t q Q h M / X 3 R E p D r c d h Y D t D a o Z 6 0 Z u K / 3 n t x P S v / Z T L O D E o 2 X x R P x H E R G T 6 N e l x h c y I s S W U K W 5 v J W x I F W X G Z l O w I X i L L y + T x l n Z u y x f 1 M 5 L l b s s j j w c w T G c g g d X U I F b q E I d G C A 8 w y u 8 O Q / O i / P u f M x b c 0 4 2 c w h / 4 H z + A P F R j R g = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i M r B u d 5 T z n f 2 G p A 8 5 X L s g U + S 6 X Y = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 3 x d Q x 4 8 Z i I e U C y h N l J b z J m d n a Z m R X C k i / w 4 k E R r 3 6 S N / / G S b I H T S x o K K q 6 6 e 4 K E s G 1 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n f q t J 1 S a x / L B j B P 0 I z q Q P O S M G i v V v V 6 p 7 F b c G c g y 8 X J S h h y 1 X u m r 2 4 9 Z G q E 0 T F C t O 5 6 b G D + j y n A m c F L s p h o T y k Z 0 g B 1 L J Y 1 Q + 9 n s 0 A k 5 t U q f h L G y J Q 2 Z q b 8 n M h p p P Y 4 C 2 x l R M 9 S L 3 l T 8 z + u k J r z x M y 6 T 1 K B k 8 0 V h K o i J y f R r 0 u c K m R F j S y h T 3 N 5 K 2 J A q y o z N p m h D 8 B Z f X i b N 8 4 p 3 V b m s X 5 S r 9 3 k c B T i G E z g D D 6 6 h C n d Q g w Y w Q H i G V 3 h z H p 0 X 5 9 3 5 m L e u O P n M E f y B 8 / k D g q 2 M z w = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p F a 6 6 n j y K N K W X W K C U O g Y a 9 X f 1 p 8 = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a J r y O J F 4 9 g 5 J H A h s w O v T A y O 7 u Z m T U h h C / w 4 k F j v P p J 3 v w b B 9 i D g p V 0 U q n q T n d X k A i u j e t + O 7 m 1 9 Y 3 N r f x 2 Y W d 3 b / + g e H j U 1 H G q G D Z Y L G L V D q h G w S U 2 D D c C 2 4 l C G g U C W 8 H o d u a 3 n l B p H s s H M 0 7 Q j + h A 8 p A z a q x U r / S K J b f s z k F W i Z e R E m S o 9 Y p f 3 X 7 M 0 g i l Y Y J q 3 f H c x P g T q g x n A q e F b q o x o W x E B 9 i x V N I I t T + Z H z o l Z 1 b p k z B W t q Q h c / X 3 x I R G W o + j w H Z G 1 A z 1 s j c T / / M 6 q Q l v / A m X S W p Q s s W i M B X E x G T 2 N e l z h c y I s S W U K W 5 v J W x I F W X G Z l O w I X j L L 6 + S Z q X s X Z U v 6 x e l 6 n 0 W R x 5 O 4 B T O w Y N r q M I d 1 K A B D B C e 4 R X e n E f n x X l 3 P h a t O S e b O Y Y / c D 5 / A I Q x j N A = &lt; / l a t e x i t &gt; 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 0 8 H P C 5 i h O m y 3 k 2 Y O n k q q S w m G 5 A = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 3 f x 4 A X j w m Y B y R L m J 3 0 J m N m Z 5 e Z W S E s + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g E V w b 1 / 1 2 V l b X 1 j c 2 C 1 v F 7 Z 3 d v f 3 S w W F T x 6 l i 2 G C x i F U 7 o B o F l 9 g w 3 A h s J w p p F A h s B a O 7 q d 9 6 Q q V 5 L B / M O E E / o g P J Q 8 6 o s V L 9 o l c q u x V 3 B r J M v J y U I U e t V / r q 9 m O W R i g N E 1 T r j u c m x s + o M p w J n B S 7 q c a E s h E d Y M d S S S P U f j Y 7 d E J O r d I n Y a x s S U N m 6 u + J j E Z a j 6 P A d k b U D P W i N x X / 8 z q p C W / 9 j M s k N S j Z f F G Y C m J i M v 2 a 9 L l C Z s T Y E s o U t 7 c S N q S K M m O z K d o Q v M W X l 0 n z v O J d V 6 7 q l + V q O 4 + j A M d w A m f g w Q 1 U 4 R 5 q 0 A A G C M / w C m / O o / P i v D s f 8 9 Y V J 5 8 5 g j 9 w P n 8 A h 4 O M 1 w = = &lt; / l a t e x i t &gt; 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k o l C 7 1 1 N + m 8 I E H i 6 z Z T M f i T / R h c = " &gt; A A A B 5 H i c b V B N S 8 N A E J 3 U r x q / q l c v i 0 X w V B L R 6 r H g x W M F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p X f 5 M 3 / 4 3 b N g d t f T D w e G + G m X l h K r g 2 n v f t l D Y 2 t 7 Z 3 y r v u 3 v 7 B 4 V H F P W 7 r J F M M W y w R i e q G V K P g E l u G G 4 H d V C G N Q 4 G d c H I 3 9 z v P q D R P 5 K O Z p h j E d C R 5 x B k 1 V n q 4 G l S q X s 1 b g K w T v y B V K N A c V L 7 6 w 4 R l M U r D B N W 6 5 3 u p C X K q D G c C Z 2 4 / 0 5 h S N q E j 7 F k q a Y w 6 y B e H z s i 5 V Y Y k S p Q t a c h C / T 2 R 0 1 j r a R z a z p i a s V 7 1 5 u J / X i 8 z 0 W 2 Q c 5 l m B i V b L o o y Q U x C 5 l + T I V f I j J h a Q p n i 9 l b C x l R R Z m w 2 r g 3 B X 3 1 5 n b Q v a 3</formula><formula xml:id="formula_33">g i H x a Y 2 p k 7 N f L x v 7 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l Y 9 V j w 4 r E F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p 4 9 S d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X J I J r 4 7 r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u 6 T h V D J s s F r H q B F S j 4 B K b h h u B n U Q h j Q K B 7 W B 8 N / P b T 6 g 0 j + W D m S T o R 3 Q o e c g Z N V Z q V P u l s l t x 5 y C r x M t J G X L U + 6 W v 3 i B m a Y T S M E G 1 7 n p u Y v y M K s O Z w G m x l 2 p M K B v T I X Y t l T R C 7 W f z Q 6 f k 3 C o D E s b K l j R k r v 6 e y G i k 9 S Q K b G d E z U g v e z P x P 6 + b m v D W z 7 h M U o O S L R a F q S A m J r O v y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 m 6 I N w V t + e Z W 0 L i v e d a X a u C r X 2 n k c B T i F M 7 g A D 2 6 g B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D i j 6 M 2 A = = &lt; / l a t e x i t &gt; 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 p Y R 2 c j + D s 1 F B n C M i U c a 2 F z + y x c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l o 9 V j w 4 r E F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p 4 9 S d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X J I J r 4 7 r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u 6 T h V D J s s F r H q B F S j 4 B K b h h u B n U Q h j Q K B 7 W B 8 N / P b T 6 g 0 j + W D m S T o R 3 Q o e c g Z N V Z q V P u l s l t x 5 y C r x M t J G X L U + 6 W v 3 i B m a Y T S M E G 1 7 n p u Y v y M K s O Z w G m x l 2 p M K B v T I X Y t l T R C 7 W f z Q 6 f k 3 C o D E s b K l j R k r v 6 e y G i k 9 S Q K b G d E z U g v e z P x P 6 + b m v D W z 7 h M U o O S L R a F q S A m J r O v y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 m 6 I N w V t + e Z W 0 L i t e t X L d u C r X 2 n k c B T i F M 7 g A D 2 6 g B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D i 8 K M 2 Q = = &lt; / l a t e x i t &gt; 6 = = &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g m + 0 T 0 L 4 g K k a a H Q / Z 9 I e G n 7 U 4 Y o = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u g k W o q C W R U l 0 W X e i y g n 1 A G 8 N k O r F D J w 9 m b s Q Q 6 6 + 4 c a G I W z / E n X / j t M 1 C W w 9 c O J x z L / f e 4 0 a c S T D N b y 2 3 s L i 0 v J J f L a y t b 2 x u 6 d s 7 L R n G g t A m C X k o O i 6 W l L O A N o E B p 5 1 I U O y 7 n L b d 4 c X Y b 9 9 T I V k Y 3 E A S U d v H d w H z G M G g J E c v H l 8 6 c N u D A Q V c P r S O q o 8 P B 4 5 e M i v m B M Y 8 s T J S Q h k a j v 7 V 6 4 c k 9 m k A h G M p u 5 Y Z g Z 1 i A Y x w O i r 0 Y k k j T I b 4 j n Y V D b B P p Z 1 O j h 8 Z + 0 r p G 1 4 o V A V g T N T f E y n 2 p U x 8 V 3 X 6 G A Z y 1 h u L / 3 n d G L w z O 2 V B F A M N y H S R F 3 M D Q m O c h N F n g h L g i S K Y C K Z u N c g A C 0 x A 5 V V Q I V i z L 8 + T 1 k n F q l V q 1 9 V S / T y L I 4 9 2 0 R 4 q I w u d o j q 6 Q g 3 U R A Q l 6 B m 9 o j f t S X v R 3 r W P a W t O y 2 a K 6 A + 0 z x + 7 m J O M &lt; / l a t e x i t &gt; G ✓ t (+1, 4|x) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G x j i h h 2 c l L W U a U m a 6 Y 6 B V 2 P + r 8 8 = " &gt; A A A B + 3 i c b V D L S g N B E J y N r x h f a z x 6 G Q x C B A 2 7 E q L H o A c 9 R j A P S O I y O 5 l N h s w + m O k V l z W / 4 s W D I l 7 9 E W / + j Z N k D x o t a C i q u u n u c i P B F V j W l 5 F b W l 5 Z X c u v F z Y 2 t 7 Z 3 z N 1 i S 4 W x p K x J Q x H K j k s U E z x g T e A g W C e S j P i u Y G 1 3 f D n 1 2 / d M K h 4 G t 5 B E r O + T Y c A 9 T g l o y T G L V w 7 c 9 W D E g J R P 7 O P q Y 3 L k m C W r Y s 2 A / x I 7 I y W U o e G Y n 7 1 B S G O f B U A F U a p r W x H 0 U y K B U 8 E m h V 6 s W E T o m A x Z V 9 O A + E z 1 0 9 n t E 3 y o l Q H 2 Q q k r A D x T f 0 6 k x F c q 8 V 3 d 6 R M Y q U V v K v 7 n d W P w z v s p D 6 I Y W E D n i 7 x Y Y A j x N A g 8 4 J J R E I k m h E q u b 8 V 0 R C S h o O M q 6 B D s x Z f / k t Z p x a 5 V a j f V U v 0 i i y O P 9 t E B K i M b n a E 6 u k Y N 1 E Q U P a A n 9 I J e j Y n x b L w Z 7 / P W n J H N 7 K F f M D 6 + A V O u k 1 g = &lt; / l a t e x i t &gt; G ✓ t ( 1, 4|y) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G D c J S W q f d i d 5 3 Z J P R b O n e r S w B o Q = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u g k W o Y E u i U l 0 W X e i y g n 1 A G 8 N k O m m H T h 7 M 3 A g h 1 l 9 x 4 0 I R t 3 6 I O / / G a Z u F t h 6 4 c D j n X u 6 9 x 4 0 4 k 2 C a 3 1 p u a X l l d S 2 / X t j Y 3 N r e 0 X f 3 W j K M B a F N E v J Q d F w s K W c B b Q I D T j u R o N h 3 O W 2 7 o 6 u J 3 3 6 g Q r I w u I M k o r a P B w H z G M G g J E c v V q 4 d u O / B k A I u V 6 z j 0 8 f k y N F L Z t W c w l g k V k Z K K E P D 0 b 9 6 / Z D E P g 2 A c C x l 1 z I j s F M s g B F O x 4 V e L G m E y Q g P a F f R A P t U 2 u n 0 + L F x q J S + 4 Y V C V Q D G V P 0 9 k W J f y s R 3 V a e P Y S j n v Y n 4 n 9 e N w b u w U x Z E M d C A z B Z 5 M T c g N C Z J G H 0 m K A G e K I K J Y O p W g w y x w A R U X g U V g j X / 8 i J p n V S t W r V 2 e 1 a q X 2 Z x 5 N E + O k B l Z K F z V E c 3 q I G a i K A E P a N X 9 K Y 9 a S / a u / Y x a 8 1 p 2 U w R / Y H 2 + Q O + q p O O &lt; / l a t e x i t &gt; G ✓ t ( 1, 3|y) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 W o A k 5 C c V y q U a Z o / m S s r 3 f l 1 B V 0 = " &gt; A A A B + 3 i c b V D L T g J B E J z F F + J r x a O X i c Q E o y G 7 a t A j 0 Y M e M Z F H A u t m d h h g w u w j M 7 1 G X P k V L x 4 0 x q s / 4 s 2 / c Y A 9 K F h J J 5 W q 7 n R 3 e Z H g C i z r 2 8 g s L C 4 t r 2 R X c 2 v r G 5 t b 5 n a + r s J Y U l a j o Q h l 0 y O K C R 6 w G n A Q r B l J R n x P s I Y 3 u B z 7 j X s m F Q + D W x h G z P F J L + B d T g l o y T X z V y 7 c t a H P g B Q P 7 a O T p 8 c D 1 y x Y J W s C P E / s l B R Q i q p r f r U 7 I Y 1 9 F g A V R K m W b U X g J E Q C p 4 K N c u 1 Y s Y j Q A e m</formula><p>x l q Y B 8 Z l y k s n t I 7 y v l Q 7 u h l J X A H i i / p 5 I i K / U 0 P d 0 p 0 + g r 2 a 9 s f i f 1 4 q h e + 4 k P I h i Y A G d L u r G A k O I x 0 H g D p e M g h h q Q q j k + l Z M + 0 Q S C j q u n A 7 B n n 1 5 n t S P S 3 a 5 We can convert this into an annealed CTMC scheme by only accepting updates with a probability scaled by a parameter ϵ t ≥ 0:</p><formula xml:id="formula_34">V L 4 5 L V Q u 0 j i y a B f t o S K y 0 R m q o G t U R T V E 0 Q N 6 R q / o z R g Z L 8 a 7 8 T F t z R j p z A 7 6 A + P z B 1 C Y k 1 Y = &lt; / l a t e x i t &gt; G ✓ t (+1, 3|z)</formula><formula xml:id="formula_35">Q MCMC t (y, x) = ϵ t M t (y, x) if y ̸ = x ϵ t (M t (x, x) -1) if y = x</formula><p>The rate matrix Q M CM C t satisfies equation 18 so that the second term of the K t -operator (see equation 10) vanishes:</p><formula xml:id="formula_36">y∈S Q MCMC t (x, y) ρ t (y) ρ t (x) = y∈S Q MCMC t (y, x) = 0</formula><p>where we used (4). This implies that the rate matrix</p><formula xml:id="formula_37">Q θ t (y, x) + Q MCMC t (y, x)</formula><p>will have the same PINN loss and the same IS weights for the same trajectories -because the K θ t remains unchanged (Note that while the RND in (15) is the same, the path measures do change). Specifically, this means that we can sample and compute the weights via Algorithm 2. The parameter ϵ t controls "how much local MCMC mixing" we want to induce. Note that the IS weights can be used both for reweighting at the final time or in addition to resample the walkers along the trajectories, connecting it to the SMC literature <ref type="bibr" target="#b12">(Doucet et al., 2001)</ref>. We specify that one may want to do this whenever the effective sample size (ESS) (see Appendix G) drops below a threshold.</p><p>Generalization of AIS and SMC. For Q θ t = 0, the above dynamics describe a continuous formulation of AIS that can be simulated approximately via Algorithm 2. In particular, this means that the algorithm presented here is a strict generalization of AIS and SMC <ref type="bibr" target="#b29">(Neal, 2001;</ref><ref type="bibr" target="#b12">Doucet et al., 2001)</ref>. Note that in the h → 0, ϵ t → ∞ limit, LEAPS would</p><p>Algorithm 1 LEAPS training with optional replay buffer Require: B batch size, N time steps, model G θ t , free energy net F ϕ t , learning rate η, replay buffer B. 1: while not converged do 2: if use buffer then 3: (X m tm , A m tm , t m ) m=1,...,B ← SampleBatch(B) 4: else 5: (X m tm , A m tm , t m ) m=1,...,B ← Algorithm 2 6: end if 7:</p><formula xml:id="formula_38">L(θ, ϕ) = 1 B m |K θ tm ρ tm (X m tm ) -∂ tm F ϕ tm | 2 8: θ ← θ -η ∇ θ L(θ, ϕ) 9: ϕ ← ϕ -η ∇ ϕ L(θ, ϕ) 10: end while Algorithm 2 LEAPS Sampling 1: Require: N time steps, M walkers, model F θ t , replay buffer B, MCMC kernel M t , density ρ t , coeff. ϵ t ≥ 0, resample thres. 0 ≤ δ ≤ 1 2: Init: X m 0 ∼ ρ 0 , A m 0 = 0 (m = 1, . . . , M ) 3: Set h = 1/N 4: for n = 0 to N -1 do 5: for m = 1 to M do 6: X m t ∼ M t (•, X m t ) with prob. hϵ t else X m t 7: X m t+h ∼ (1 Xt=y + hQ θ t (y, X t )) y∈S 8: A m t+h = A m t + hK θ t ρ t (X m t ) 9:</formula><p>end for 10:</p><formula xml:id="formula_39">t ← t + h 11:</formula><p>if ESS(A t ) ≤ δ then 12:</p><p>X t = resample(X t , A t ) (m = 1, . . . , M )</p><p>13:</p><p>A t = 0 (m = 1, . . . , M )</p><p>14:</p><p>end if 15: end for 16: Optional: Store {(X m t , A m t , t)} t,m in B.</p><p>recover the exact distributions ρ t with AIS (i.e. even with Q θ t = 0). Of course, this asymptotic limit is not realizable in practice with finite number of steps, and the inclusion of Q θ t allows us to sample much more efficiently while still maintaining statistical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Efficient IS and training via local equivariance</head><p>We now turn to the question of how to make the above training procedure efficient. Note that for small state spaces S we could rely on analytical solutions to the KFE <ref type="bibr" target="#b9">(Campbell et al., 2022;</ref><ref type="bibr" target="#b37">Shaul et al., 2024)</ref>. In many applications, though, the state space S is so large that we cannot store |S| elements efficiently in a computer. Often state spaces S are of the form S = T d where T = {1, . . . , N } is a set of N tokens. We use the notation τ for a token, i.e. an element τ ∈ T . One then defines a notion of a neighbor y of x, i.e. an element y = (y 1 , . . . , y d ) that differs from x in at most one dimension (i.e. y i ̸ = x i for at most one i).</p><p>We denote as N (x) the set of all neighbors of x. We then restrict functional form of the rate matrices to only allow for jumps to neighbors, i.e. Q θ t (y, x) = 0 if y / ∈ N (x). One can then use a neural network Q θ t represented by the function</p><formula xml:id="formula_40">Q θ t : S → (R N -1 ) d x → (Q θ t (τ, i|x)) i=1,...,d,τ ∈T \{xi}</formula><p>i.e. the neural network is given by the function x → Q θ t (•|x) that returns for very dimension i a value for every token τ different from x i . We then parameterize a rate matrix via</p><formula xml:id="formula_41">Q θ t (y, x) =        0 if y / ∈ N (x) Q θ t (y j , j|x) else if y j ̸ = x j - i,τ Q θ t (τ, i|x) if x = y (19)</formula><p>This parameterization is commonly used in the context of discrete markov models ("discrete diffusion models") <ref type="bibr" target="#b9">(Campbell et al., 2022;</ref><ref type="bibr">2024)</ref>. With that, the operator K θ t in (10) becomes:</p><formula xml:id="formula_42">K θ t ρ t (x) + ∂ t U t (x) = i=1,...,d y∈N (x), yi̸ =xi Q θ t (y i , i|x) -Q θ t (x i , i|y) ρ t (y) ρ t (x)</formula><p>The key problem with the above update is that it requires us to evaluate the neural network |N (x)| times (for every neighbor y). This makes computing K θ t computationally prohibitively expensive. Hence, with the standard rate matrix parameterization, the proactive IS sampling scheme and training via the PINN-objective is very inefficient.</p><p>To make the computation of K θ t efficient, we choose to induce an inductive bias into our neural network architecture to compute K θ t with no additional cost. Specifically, we introduce here the notion of local equivariance. A neural network G θ t represented by the function</p><formula xml:id="formula_43">G θ t : S → (R N -1 ) d x → (G θ t (τ, i|x)) i=1,...,d,τ ∈T \{xi}</formula><p>is called locally equivariant if the following condition holds:</p><formula xml:id="formula_44">G θ t (τ, i|x) = -G θ t (x i , i|Swap(x, i, τ )) (i = 1, . . . , d) where Swap(x, i, τ ) = (x 1 , . . . , x i-1 , τ, x i+1 , . . . , x d )</formula><p>In other words, the function G θ t gives the "flux of probability" going from x to each neighbor. Local equivariance says that the flux from x to its neighbor is negative the flux from the neighbor to x (see Figure <ref type="figure" target="#fig_3">2</ref>). Therefore, every coordinate map F j is equivariant with respect to transformations of the j-th input ("locally" equivariant). Note that we do not specify how F i transforms for i ̸ = j under transformations of x j . This distinguishes it from "full" equivariance as, for example, used in geometric deep learning <ref type="bibr" target="#b8">(Bronstein et al., 2021;</ref><ref type="bibr" target="#b48">Weiler &amp; Cesa, 2019;</ref><ref type="bibr" target="#b41">Thomas et al., 2018)</ref>. We can use a locally equivariant neural network to parameterize a rate matrix via:</p><formula xml:id="formula_45">Q θ t (τ, j|x) = [G θ t (τ, j|x)] +<label>(20)</label></formula><p>where [z] + = max(z, 0) describes the ReLU operation. This representation is not a restriction (see Appendix D for a proof):</p><p>Proposition 8.1 (Universal representation theorem). For any CTMC as in ( <ref type="formula">19</ref>) with marginals ρ t , there is a corresponding CTMC with the same marginals ρ t and a rate matrix that can be written as in ( <ref type="formula" target="#formula_45">20</ref>) for a locally equivariant function G θ t .</p><p>Crucially, this representation allows to efficiently compute K θ t in one forward pass of the neural network:</p><formula xml:id="formula_46">K θ t ρ t (x) + ∂ t U t (x) = i=1,...,d y∈N (x), yi̸ =xi [G θ t (y i , i|x)] + -[-G θ t (y i , i|x)] + ρ t (y) ρ t (x)</formula><p>With this, we can efficiently compute the proactive IS update A t and evaluate the PINN-objective. Therefore, this construction allows for scalable training and efficient proactivate importance sampling. We call the resulting algorithm LEAPS (Locally Equivariant discrete Annealed Proactivate Sampler). The acronym also highlights that we use a Markov jump process to sample (i.e. that takes "leaps" through space).</p><p>Remark 8.2. It is important to note that with the above construction, we would need to naively evaluate ρ t (x) as often as d times for a single computation of K θ t . However, note that the sum goes over all neighbors over x. Therefore, this can be a considered as computing a discrete gradient. Such ratios can often be computed efficiently, e.g. for many scientific and physical models it is often only 2× the computation compared to a single evaluation of ρ t (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Design of locally equivariant networks</head><p>It remains to be stated how to construct locally equivariant neural networks -a question we turn to in this section. We will focus on three fundamental designs used throughout deep learning: Multilayer perceptrons (MLPs), attention layers, and convolutional neural networks. Usually, tokens are embedded as token vectors e τ ∈ R cin where c in is the embedding dimension. We therefore consider the embedded sequence of vectors: x = (x 1 , . . . , x d ) ∈ (R cin ) d as the input to the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilayer perceptron (MLP).</head><p>Let us set c in = 1 in this paragraph for readability. Let W 1 , . . . , W k ∈ R d×d be a set of weight matrices with a zero diagonal, i.e. W ii = 0 for i = 1, . . . , d. Further, let σ : R → R be an activation function and ω τ ∈ R k be a learnable projection vector for every token τ ∈ T . Then define the map:</p><formula xml:id="formula_47">G θ t (τ, j|x) = k i=1 (ω i τ -ω i xj )σ(W i x) j</formula><p>where σ(W i x) j denotes the j-th element of the vector obtained by multiplying the vector x with the matrix W i and applying the activation function a componentwise. One can easily show that this is a locally equivariant neural network corresponding to a MLP with one hidden layer.</p><p>Locally-equivariant attention (LEA) layer. Let us consider a self-attention layer operating on keys k j = k j (x j ), queries q j = q j (x j ), and values v j = v j (x j ) -each of which is a function of element x j . We define the locally equivariant attention layer then as:</p><formula xml:id="formula_48">G θ t (τ, j|x) = (ω τ -ω xj ) T    s̸ =j exp(k T s q j ) t̸ =j exp(k T t q j ) v s   </formula><p>It can be shown that this layer is locally equivariant if the queries q j are independent of the sign of x j (i.e. q j (x j ) = q j (-x j )) which can be easily achieved. By stacking across multiple attention heads, one can create a locally equivariant MultiHeadAttention (LEA) with this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierchical local equivariance.</head><p>Local equivariance is different from "proper" equivariance in that the composition of locally equivariant functions is not locally equivariant in general. Therefore, we cannot simply compose locally equivariant neural network layers as we would do with "proper" equivariant neural networks. In particular, the above MLP and the attention layers cannot simply be composed as their composition would violate the local equivariance. This fundamentally changes considerations about how to compose layers and how to construct deep neural networks. We will now illustrate this for the case of convolutional neural networks.</p><p>Locally-equivariant convolutional (LEC) network. To construct a locally equivariant convolutional neural network (LEC), we assume that our data lies on a grid. A convolutional layer is characterized by a matrix W ∈ R (2k-1)×(2k-1) and its operation is denoted via k(W ) * x</p><p>where k denotes the convolutional kernel with weights W .</p><p>Here, we set the center of W to zero: W kk = 0 (i.e. such that corresponding location is effectively ignored). To stack such layers, we can make the output of the previous layer feed into the weights of the next layer:</p><formula xml:id="formula_49">h 0 =(1, . . . , 1) T W i =σ(A i h i + b i ) + c j (i = 1, . . . , L) h i+1 =k t (W i ) * x (i = 1, . . . , L) H θ t (x) =h L where A i ∈ R di×di , b i ∈ R di , c i ∈ R di</formula><p>are learnable tensors which operate on each coordinate independently (i.e. a 1x1 convolution) and σ : R → R is an activation function to make it non-linear. We call the resulting network</p><formula xml:id="formula_50">H θ t : (R cin ) d →(R cout ) d x →(H θ t (1|x), . . . , H θ t (d|x))</formula><p>the prediction head. Combined with a small network P θ t : T → R k that we call token projector, we define the full neural network as</p><formula xml:id="formula_51">G θ t (τ, j|x) = (P θ t (e τ ) -P θ t (x j )) T H θ t (j|x)</formula><p>In Appendix E, we verify that G θ t defined in this way is locally equivariant. With this construction, one can stack deep highly complex convolutional neural networks. Note that this convolutional neural network has two (separate) symmetries: it is geometrically translation equivariant and locally equivariant in the sense defined in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Related Work</head><p>CTMCs. CTMCs <ref type="bibr" target="#b9">(Campbell et al., 2022)</ref> have been used for various applications in generative modeling ("discrete diffusion" models), including text and image generation <ref type="bibr" target="#b38">(Shi et al., 2024;</ref><ref type="bibr" target="#b16">Gat et al., 2025;</ref><ref type="bibr" target="#b37">Shaul et al., 2024;</ref><ref type="bibr" target="#b36">Sahoo et al., 2024)</ref> and molecular design <ref type="bibr" target="#b18">(Gruver et al., 2023;</ref><ref type="bibr" target="#b10">Campbell et al., 2024;</ref><ref type="bibr">Lisanza et al., 2024)</ref>. While here we use a RND for CTMCs running in reverse time, one recovers the loss functions of these generative models considering a RND of two forward time CTMCs (in Appendix A, we show this in detail).</p><p>Transport and sampling. Over the past decade there has been continued interest in combining the statistical guarantees of MCMC and IS with learning transport maps. A nonparametric version of this is described in <ref type="bibr" target="#b25">(Marzouk et al., 2016)</ref>, and a parametric version through coupling-based normalizing flows was used to study systems in molecular dynamics and statistical physics <ref type="bibr" target="#b31">(Noé et al., 2019;</ref><ref type="bibr" target="#b3">Albergo et al., 2019;</ref><ref type="bibr" target="#b15">Gabrié et al., 2022;</ref><ref type="bibr" target="#b47">Wang et al., 2022)</ref>. These methods were extended to weave normalizing flows with SMC moves <ref type="bibr" target="#b6">(Arbel et al., 2021;</ref><ref type="bibr" target="#b26">Matthews et al., 2022)</ref>. More recent research focuses on replacing the generative model with a continuous flow or diffusion <ref type="bibr" target="#b50">(Zhang &amp; Chen, 2022;</ref><ref type="bibr" target="#b45">Vargas et al., 2023;</ref><ref type="bibr" target="#b0">Akhound-Sadegh et al., 2024;</ref><ref type="bibr" target="#b40">Sun et al., 2024)</ref>. Our method is inspired by approaches combining measure transport with MCMC schemes <ref type="bibr" target="#b2">(Albergo &amp; Vanden-Eijnden, 2024;</ref><ref type="bibr" target="#b46">Vargas et al., 2024)</ref> and other samplers relying on PINN-based objectives in continuous spaces <ref type="bibr" target="#b28">(Máté &amp; Fleuret, 2023;</ref><ref type="bibr" target="#b42">Tian et al., 2024;</ref><ref type="bibr" target="#b40">Sun et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete Neural samplers. The primary alternative to what</head><p>we propose is to correct using importance weights arising from the estimate of the probability density computed using an autoregressive model <ref type="bibr" target="#b30">(Nicoli et al., 2020)</ref>. However, the computational cost of producing samples in this case scales naively as O(d), whereas we have no such constraint a priori in our case so long as the error in the Euler sampling scheme is kept small. Other work focuses on discrete formulations of normalizing flows, but the performant version reduces to an autoregressive model <ref type="bibr" target="#b43">(Tran et al., 2019)</ref>. Recent work has considered using CTMCs for sampling by parameterizing their evolution operators directly via tensor networks <ref type="bibr" target="#b11">(Causer et al., 2025)</ref> as opposed to neural network representations of rate matrices here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Experiments</head><p>As a demonstration of the validity of LEAPS in high dimensions, we sample the Gibbs distribution associated to a 2-dimensional Ising model. We choose the Ising model because it is a well-studied model. In particular, it is a solvable model, which allows us to construct a robust ground truth Center: Difference in the histograms of the magnetization M (x) of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as M * . We denote by "no transport" the case of using annealed dynamics with just the marginal preserving MCMC updates to show that the transport from Qt is essential in our construction. Right: Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.</p><p>against which we can assess the various neural architectures underlying our algorithm. Configurations of the L × L Ising lattice follow the target distribution ρ 1 (x) = e -βH(x)+F1 where β is the inverse temperature of the system, F 1 the free energy, and H(x) : {-1, 1} L×L → R is the Hamiltonian for the model defined as</p><formula xml:id="formula_52">H(x) = -J ⟨i,j⟩ x i x j + µ i x i .<label>(21)</label></formula><p>Here, J is the interaction strength, ⟨i, j⟩ denotes summation over nearest neighbors of spins x i , x j and µ is the magnetic moment. Neighboring spins are uncorrelated at high temperature but reach a critical correlation when the temperature drops behold a certain threshold. We use LEAPS to reproduce the statistics of the theory on a 15 × 15 lattice at parameters β = 0.7, J = 0.4, which approach this threshold, and compare our results against a ground truth of long-run Glauber dynamics, an efficient algorithm for simulation in this parameter regime. Note that this corresponds to a d = 15 × 15 = 225 dimensional space. To make ρ t time dependent, we make the parameters of J t , µ t , β t linear functions of time, starting from the non-interacting case J 0 = 0.</p><p>Results. We compare three different realizations of our method, one using LEA, and the other two using deep LEC that vary in depth. For all generated samples, we use 100 steps of integration of (6). We benchmark them on the effective sample size (ESS), which is a standard measure of how many effective samples our model gives according to the variance of the importance weights (see details Appendix G). In addition, we compute various observables using the Ising configurations generated by our model, such as histograms of the magnetization M (x) = i x i compared to ground truth, as well as the two point connected correlation function</p><formula xml:id="formula_53">G conn (r) = E[x i x i+r ] -E[x i ] E[x i+r ]. (<label>22</label></formula><formula xml:id="formula_54">)</formula><p>The latter is a measure of the dependency between spin values a distance r in lattice separation. In Figure <ref type="figure" target="#fig_5">4</ref>, we show in the leftmost panel that the convolutional architecture outperforms the attention-based method, and the performance gap grows as we make the LEC network deeper. In the center panel, the difference in histograms of the magnetization of lattice configurations for our models as compared to ground truth samples is shown to be statistically zero, whereas relying on local MCMC alone for the same number of sampling steps (plotted in purple) illustrates that the dynamics have not converged. In the right plot, we see clear agreement between our learned sampler and the ground truth for the connected two point function. These results show that LEAPS can be an efficient simulator of complex, high dimensional target distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Discussion</head><p>In this work, we present the LEAPS algorithm that allows to learn a non-equilibrium sampler via CTMCs parameterized by a locally equivariant neural network that is trained to minimize the variance of proactivate IS weights. A natural direction of future work will be to connect the ideas presented here with guidance or reward fine-tuning of generative CTMC models (discrete diffusion) -a problem known to be strongly tied to sampling. Further, LEAPS could easily be extended to sample across a whole family of distributions as opposed to only for a single, fixed target. Finally, we anticipate that the use of locally equivariant neural networks as well as the proactive IS scheme presented here might be useful more broadly for probabilistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p><p>A. Proof of Proposition 5.1</p><p>Without loss of generality, we set the final time point to be t = 1. We compute for a bounded continuous function Φ : X → R:</p><formula xml:id="formula_55">E X∼ - → P µ,Q [Φ(X)] = lim n→∞ E X∼ - → P µ,Q [Φ(X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 )] = lim n→∞ E X∼ ← - P ν,Q ′   Φ(X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 ) - → P µ,Q (X 0 , X 1/n , . . . , X n-1 n , X 1 ) ← - P ν,Q ′ (X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 )   = lim n→∞ E X∼ ← - P ν,Q ′   Φ(X 0 , X 1/n , X 2/n , . . . , X n-1 n , X 1 ) µ(X 0 ) ν(X 1 ) s=0,1/n,2/n,..., n-1 n - → P µ,Q (X s+h |X s ) ← - P ν,Q ′ (X s |X s+h )   = lim n→∞ E X∼ ← - P ν,Q ′   Φ(X) µ(X 0 ) ν(X 1 ) exp   h s,X s+h =Xs Q s (X s , X s ) -Q ′ s+h (X s , X s )   s,X s+h ̸ =Xs Q s (X s+h , X s ) Q ′ s+h (X s , X s+h )   =E X∼ ← - P ν,Q ′   Φ(X) µ(X 0 ) ν(X 1 ) exp   1 0 Q s (X s , X s ) -Q ′ s (X s , X s )ds   s,X s -̸ =Xs Q s (X s , X s -) Q ′ s (X s -, X s )  </formula><p>where we used the definition of the rate matrix Q t , Q ′ t , the continuity of Q ′ t in t and the fact that the left and right Riemann integral coincide. As Φ was arbitrary, the RND is given by:</p><formula xml:id="formula_56">log d - → P µ,Q d ← - P ν,Q ′ (X) = log(µ(X 0 )) -log(ν(X 1 )) + 1 0 Q s (X s , X s ) -Q ′ s (X s , X s )ds + s,X - s ̸ =Xs log Q s (X s , X - s ) Q ′ s (X - s , X s )</formula><p>B. Proof of Theorem 5.2</p><p>Specifically, we use Proposition 5.1 to compute</p><formula xml:id="formula_57">log d ← - P ρt, Qt d - → P ρ0,Qt (X) = log(ρ t (X t )) -log(ρ 0 (X 0 )) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s ) =F t -F 0 -U t (X t ) + U 0 (X 0 ) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s )</formula><p>Note that the function t → U t (X t ) is a piecewise differentiable function. Therefore, we can apply the fundamental theorem on every differentiable "piece" and get:</p><formula xml:id="formula_58">U t (X t ) -U 0 (X 0 ) = t 0 ∂ s U t (X t )ds + s,X - s ̸ =Xs U s (X s ) -U s (X - s ) = t 0 ∂ s U s (X s )ds + s,X - s ̸ =Xs log ρ s (X - s ) ρ s (X s )</formula><p>Next, we can insert the above equation and get:</p><formula xml:id="formula_59">log d ← - P ρt, Qt d - → P ρ0,Qt (X) =F t -F 0 -U t (X t ) + U 0 (Y 0 ) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s ) =F t -F 0 - t 0 ∂ s U s (X s )ds - s,X - s ̸ =Xs log ρ s (X - s ) ρ s (X s ) + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log Qs (X - s , X s ) Q s (X s , X - s ) =F t -F 0 - t 0 ∂ s U s (X s )ds + t 0 Qs (X s , X s ) -Q s (X s , X s )ds + s,X - s ̸ =Xs log     Qs (X - s , X s ) Q s (X s , X - s ) ρ s (X s ) ρ s (X - s ) =1     =F t -F 0 - t 0 ∂ s U s (X s )ds + t 0 - y̸ =Xs Q s (X s , y) ρ t (y) ρ t (X s ) -Q s (X s , X s )ds + 0 =F t -F 0 +   - t 0 ∂ s U s (X s )ds - t 0 y∈S Q s (X s , y) ρ t (y) ρ t (X s ) ds   =F t -F 0 + A t</formula><p>where we used the definition of A t in (11) and the definition of Qt in ( <ref type="formula" target="#formula_23">14</ref>). Note that for h = 1, we get that</p><formula xml:id="formula_60">1 = E x∼ρt [h(x)] = E[exp(A t + F t -F 0 )] = E[exp(A t )] exp(F t -F 0 ) because F t , F 0 are constants. Therefore, in particular E[exp(A t )] = exp(F 0 -F t ) = Z t /Z 0 .</formula><p>Note that we assume that Z 0 = 1 as we know ρ 0 . Therefore, E[exp(A t )] = Z t . This proves (16).</p><p>C. Proof of Proposition 6.1</p><p>We can use the variational formulation of the variance as the minimizer of the mean squared error to derive a computationally more efficient upper bound, i.e. we can re-express for every 0 ≤ t ≤ 1:</p><formula xml:id="formula_61">L log-var (θ; t) =V X∼Q [A t ] = min Ft∈R E X∼Q [|A t -Ft | 2 ] =t 2 min ∂s Fs∈R,0≤s≤t</formula><p>E X∼Q   | 1 t t 0 K θ s ρ s (X s ) -∂ s Fs ds| 2   ≤t 2 min ∂s Fs∈R,0≤s≤t E X∼Q   1 t t 0 |K θ s ρ s (X s ) -∂ s Fs | 2 ds   =t 2 min ∂s Fs∈R,0≤s≤t E s∼Unif [0,1] ,Xs∼Qs |K θ s ρ s (X s ) -∂ s Fs | 2</p><p>where we used Jensen's inequality and denote with Q s the marginal of Q at time s. We now arrive at the result by replacing the above with the free energy network F ϕ t . Further, note that the above bound is tight for Q-almost every X: K θ s ρ s (X s ) -∂ s F s = C(X 0:t ) is a constant in time s. However, this constant may depend on X. Further, note that</p><formula xml:id="formula_62">Q s (X s ) = |K θ s ρ s (X s ) -∂ s F s | 2</formula><p>Now, let us return to the proof of Proposition 8.1. Given a rate matrix Q t , we can now use a one-way rate matrix Qt with the same marginals and define function:</p><p>F t (τ, i|x) = Qt (y, x) if Q t (y, x) &gt; 0 -Qt (x, y) otherwise where y = Swap(x, i, τ )</p><p>By construction, it holds that F t (τ, i|x) is locally equivariant and that [F t (τ, i|x)] + = Qt (y, x). This finishes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Local equivariance of ConvNet</head><p>To verify the local equivariance, one can compute G θ t (τ, i|x) =(P θ t (e τ ) -P θ t (x i )) T H θ t (i|x) = -(P θ t (x i ) -P θ t (e τ )) T H θ t (i|x) = -(P θ t (x i ) -P θ t (e τ )) T H θ t (i|Swap(x, i, τ )) = -F t (x i , i|Swap(x, i, τ )), where we have used the invariance of the projection head H θ t (i|x) to changes in the i-th dimension. This shows the local equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Recovering loss functions for CTMC models via RNDs</head><p>We discuss here in more detail how the Radon-Nikodym derivatives (RNDs) presented in Proposition 5.1 relate to the construction of loss function for CTMC generative models, also called "discrete diffusion" models. The connection lies in the fact that the loss function of these models relies on RNDs of two CTMCs running both in forward time. We can prove the following statement: Proposition F.1. Let µ, ν be two initial distributions over S. Let Q t , Q ′ t be two rate matrices. Then the Radon-Nikodym derivative of the corresponding path distributions in forward time over the interval [0, t] is given by:</p><formula xml:id="formula_63">log d - → P µ,Q d - → P ν,Q ′ (X) = log dµ dν (X 0 ) + t 0 Q s (X s , X s ) -Q ′ s (X s , X s )ds + s,X - s ̸ =Xs log Q s (X s , X - s ) Q ′ s (X s , X - s )</formula><p>where we sum over all points where X s jumps in the last term.</p><p>The proof of the above formula is very similar to the proof of Proposition 5.1 and an analogous formula also appeared in <ref type="bibr" target="#b10">(Campbell et al., 2024)</ref>, for example. The above proposition allows us to by compute the KL-divergence:</p><formula xml:id="formula_64">D KL ( - → P µ,Q 1 || - → P ν,Q ′ 1 ) ≤D KL ( - → P µ,Q || - → P ν,Q ′ ) =E X∼ - → P µ,Q log d - → P µ,Q d - → P ν,Q ′ (X) =D KL (µ||ν) + E X∼ - → P µ,Q   1 0 Q t (X t , X t ) -Q ′ t (X t , X t )dt + t,X - t ̸ =Xt log Q t (X t , X - t ) Q ′ t (X t , X - t )   =D KL (µ||ν) + E t∼Unif [0,1] ,xt∼ - → P µ,Q t [Q t (X t , X t ) -Q ′ t (X t , X t )] + E X∼ - → P µ,Q   t,X - t ̸ =Xt log Q t (X t , X - t ) Q ′ t (X t , X - t )   =D KL (µ||ν) + E t∼Unif [0,1] ,xt∼ - → P µ,Q t [Q t (X t , X t ) -Q ′ t (X t , X t )] + 1 0 E Xt∼ - → P µ,Q t   y̸ =Xt Q t (y; X t ) log Q t (y; X t ) Q ′ t (y, X t )   dt =D KL (µ||ν) + E t∼Unif [0,1] ,Xt∼ - → P µ,Q t   y̸ =Xt Q ′ t (y, X t ) -Q t (y, X t ) + Q t (y; X t ) log Q t (y; X t ) Q ′ t (y, X t )  </formula><p>where we have used the data processing inequality in the first term. Having a parameterized model Q ′ t = Q θ t , this leads to the following loss:</p><formula xml:id="formula_65">L(θ) =D KL - → P µ,Q || - → P µ,Q θ t =D KL (µ||ν) + E t∼Unif [0,1] ,Xt∼ - → P µ,Q t   y̸ =Xt Q θ t (y, X t ) -Q t (y, X t ) log Q θ t (y, X t )   + C</formula><p>where Q t is some reference process. The above recovers loss functions in the context of CTMC and jump generative models <ref type="bibr" target="#b9">(Campbell et al., 2022;</ref><ref type="bibr" target="#b16">Gat et al., 2025;</ref><ref type="bibr" target="#b37">Shaul et al., 2024;</ref><ref type="bibr" target="#b10">Campbell et al., 2024)</ref> and Euclidean jump models <ref type="bibr">(Holderrieth et al., 2024, Section D.1.)</ref>. Note that the above loss cannot be used for the purposes of sampling in a straight-forward manner because we do not have access to samples from the marginals of the ground reference -→ P µ,Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Numerical experiments</head><p>Effective Sample Size We use the self-normalized definition of the effective sample size such that, given the log weights A t associated to N CTMC instances, the ESS at time t in the generation is given by:</p><formula xml:id="formula_66">ESS t = N -1 N t=1 exp A i t 2 N -1 N i=1 exp 2A i i<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Ising model experiments</head><p>Here we provide details of the numerical implementation of our study of the L = 15 Ising model. For the locally equivariant attention (LEA) mechanism, we use 40 attention heads, each with query, key, and value matrices of dimension 40x40. As such, there are about 350,000 parameters in the model. In addition, the locally equivariant convolutional net (LEC) of depth three uses kernel sizes of <ref type="bibr">[5,</ref><ref type="bibr">7,</ref><ref type="bibr">15]</ref>, while the depth five version uses <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">9,</ref><ref type="bibr">15]</ref>, amounting to around 100,000 parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the LEAPS algorithm. LEAPS allows to learn a dynamical transport of discrete distributions from t = 0 to t = 1 (blue). Sample are generated via the simulation of a Continuous-time Markov chain (yellow). Further, importance sampling weights allow to correct training errors trading off sample efficiency (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>g 1 g g P A M r / D m P D o v z r v z M W 9 d c f K Z I / g D 5 / M H 7 8 2 N F w = = &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c l v 6 l l 4 b B 9 F C g 6 S / z e 5 p F y o g q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>6 9 d l 1 t d I o w y n A K Z 3 A B P t x A A + 6 h C S 1 g g P A C b / D u P D m v z s e y s e Q U E y f w B 8 7 n D x 4 S i 6 0 = &lt; / l a t e x i t &gt; 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I 0 T c S B 4 F 5 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visualization of local equivariance. Two tokens T = {-1, +1} and d = 6. Local equivariance means that the "flux" to transition to a neighbor is the negative of the flux of transitioning from that neighbor back.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Overview of locally equivariant convolutional neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance metrics of LEAPS on the L = 15, J = 0.4, β = 0.7 Ising model with the LEA and LEC architectures, using 100 annealing steps for sample generation. Left: Effective sample size of LEAPS samplers over training. Green area denotes an annealing phase during training where t is increased from 0 to 1. Increasing the depth of LEC significantly improves performance.Center: Difference in the histograms of the magnetization M (x) of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as M * . We denote by "no transport" the case of using annealed dynamics with just the marginal preserving MCMC updates to show that the transport from Qt is essential in our construction. Right: Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="14.">Acknowledgements</head><p>We thank <rs type="person">Eric Vanden-Eijnden</rs> for useful discussions. PH acknowledges support from the <rs type="funder">Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS)</rs> consortium, the <rs type="funder">DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE)</rs> threats program, and the <rs type="funder">NSF</rs> <rs type="grantName">Expeditions grant</rs> (award 1918839) Understanding the <rs type="institution">World Through Code</rs>. MSA is supported by a Junior Fellowship at the <rs type="institution">Harvard Society of Fellows</rs> as well as the <rs type="funder">National Science Foundation</rs> under Cooperative Agreement PHY-2019786 (The <rs type="institution">NSF AI Institute for Artificial Intelligence and Fundamental Interactions</rs>, <ref type="url" target="http://iaifi.org/">http://iaifi.org/</ref>).</p></div>
			</div>
			<div type="funding">
<div><p>i O 8 K d i 0 B Z p U B s z S n n n x 4 Z 8 y a q p o = " &gt; A A A D 8 n i c h Z N L b 9 <rs type="programName">N A E I C 3 D Y 8 S H m 3 h y M U i Q k L I R H Z e h F s F S H B B F E T S S k k U</rs> r d c T Z 8 l 6 1 9 p d t w 2 W / w N X e k N c + T 1 I</p><p>a Y q 0 u g v 2 u 6 3 c r X K Q B Z I n 1 i 3 R 9 P F V M Q 3 i Z q 4 1 5 P M / t V y E J p 1 e 5 j t v 1 3 T Z i K P g S / M 2 9 r y r D V t P v N X s f O o 2 D V v k m d s h D 8 o g 8 <rs type="programName">I T 5 5 T g 7 I W 3 J I B o S R</rs> z</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rGJrjcS">
					<orgName type="grant-name">Expeditions grant</orgName>
				</org>
				<org type="funding" xml:id="_6UmWAkz">
					<orgName type="program" subtype="full">N A E I C 3 D Y 8 S H m 3 h y M U i Q k L I R H Z e h F s F S H B B F E T S S k k U</orgName>
				</org>
				<org type="funding" xml:id="_UAwQZUd">
					<orgName type="program" subtype="full">I T 5 5 T g 7 I W 3 J I B o S R</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Proposition 8.1</head><p>Before we prove the statement, we prove an auxiliary statement about one-way rate matrices. We call a rate matrix Q t a one-way rate matrix if</p><p>Intuitively, a rate matrix Q t is a one-way rate matrix if we can always only go from x → y or from y → x. The next proposition shows that there is no problem restricting ourselves to one-way rate matrices.</p><p>Lemma D.1. For every CTMC with rate matrix Q t and marginals q t , there is a one-way rate matrix Qt such that its corresponding CTMC X t has marginals q t if X 0 ∼ q 0 is initialized with the same initial distribution. Furhter, if Q t (y, x) = 0 for y ̸ = x, then also Qt (y, x) = 0.</p><p>Proof. Let Q t be a rate matrix defining a CTMC with marginals q t . Then</p><p>Qt (x, y)q t (y) -Qt (y, x)q t (x) = y∈S Qt (x, y)q t (y)</p><p>where we defined</p><p>Therefore, we learn that Qt fulfils the desired condition and fulfils the KFE. Therefore, we have proved that we can swap out Q t for Qt and we will have an CTMC with the same marginals.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Iterated denoising energy matching for sampling from boltzmann densities</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akhound-Sadegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sendera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.06121</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building normalizing flows with stochastic interpolants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<author>
			<persName><surname>Nets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02711</idno>
		<title level="m">A non-equilibrium transport sampler</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flowbased generative models for markov chain monte carlo in lattice field theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kanwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.100.034515</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevD.100.034515" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">34515</biblScope>
			<date type="published" when="2019-08">Aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Boffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08797</idno>
		<title level="m">Stochastic interpolants: A unifying framework for flows and diffusions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computer simulation of liquids</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Tildesley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Annealed flow transport monte carlo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<meeting>the 38th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Molecular dynamics with coupling to an external bath</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J C</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P M</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Van Gunsteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dinola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Haak</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.448118</idno>
		<ptr target="https://doi.org/10.1063/1.448118" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<idno type="ISSN">0021-9606</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3684" to="3690" />
			<date type="published" when="1984">10 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A continuous time framework for discrete denoising models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="28266" to="28279" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04997</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discrete generative diffusion models without stochastic differential equations: a tensor network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Causer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Rotskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Garrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25302</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo Methods in Practice. Statistics for Engineering and Information Science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4757-3437-9</idno>
		<ptr target="https://doi.org/10.1007/978-1-4757-3437-9" />
		<editor>N. J.</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid monte carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
		<idno type="DOI">10.1016/0370-2693(87)91197-X.URLhttps://www.sciencedirect.com/science/article/pii/037026938791197X</idno>
		<ptr target="https://doi.org/10.1016/0370-2693(87)91197-X.URLhttps://www.sciencedirect.com/science/article/pii/037026938791197X" />
	</analytic>
	<monogr>
		<title level="j">Physics Letters B</title>
		<idno type="ISSN">0370-2693</idno>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sampling algorithms in statistical physics: a guide for statistics and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Livingstone</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2208.04751" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive monte carlo augmented with normalizing flows</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gabrié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Rotskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2109420119</idno>
		<ptr target="https://www.pnas.org/doi/abs/10.1073/pnas.2109420119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2109420119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discrete flow matching</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="133345" to="133385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sampling-based approaches to calculating marginal densities</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2289776" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<idno type="ISSN">01621459</idno>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">410</biblScope>
			<biblScope unit="page" from="398" to="409" />
			<date type="published" when="1990">1990. 1537274</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Protein design with guided discrete diffusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gruver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G J</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hotzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafrance-Vanasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MfiK69Ga6p" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Holderrieth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.20587</idno>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/4" />
	</analytic>
	<monogr>
		<title level="m">Generator matching: Generative modeling with arbitrary markov processes</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 2024</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>c5bcfec8584af0d967f1ab10179ca4b-Paper. pdf Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonequilibrium equality for free energy differences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jarzynski</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.78.2690</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevLett.78.2690" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="2690" to="2693" />
			<date type="published" when="1997-04">Apr 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimation of particle transmission by random sampling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Bureau of Standards applied mathematics series</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="27" to="30" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flow matching for generative modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PqvMRDCJT9t" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multistate and functional protein design using rosettafold sequence space diffusion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lisanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gershon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W K</forename><surname>Tipps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Sims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Arnoldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Simma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Tharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brackenbrough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Mcshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41587-024-02395-w</idno>
		<ptr target="https://doi.org/10.1038/s41587-024-02395-w" />
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.03003</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sampling via measure transport: An introduction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moselhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spantini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of uncertainty quantification</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continual repeated annealed flow transport Monte Carlo</title>
		<author>
			<persName><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v162/matthews22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
	<note>Szepesvari</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fast computing machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teller</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.1699114</idno>
		<ptr target="https://doi.org/10.1063/1.1699114" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<idno type="ISSN">0021-9606</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953-06">06 1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning interpolations between boltzmann densities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Máté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2301.07388" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asymptotically unbiased estimation of physical observables with neural samplers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Nicoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kessel</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.101.023304</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevE.101.023304" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">23304</biblScope>
			<date type="published" when="2020-02">Feb 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaw1147</idno>
		<ptr target="https://www.science.org/doi/abs/10.1126/science.aaw1147" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6457</biblScope>
			<biblScope unit="page">1147</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Solving high-dimensional hamilton-jacobi-bellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nüsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Richter</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.05409" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improved sampling via learned diffusions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01198</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved sampling via learned diffusions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=h4pNROsO06" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Monte Carlo statistical methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple and effective masked diffusion language models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arriola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Marroquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=L4uaAR4ArM" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Holderrieth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03487</idno>
		<title level="m">Flow matching with general discrete paths: A kinetic-optimal perspective</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplified and generalized masked diffusion for discrete data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xcqSOfHt4g" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dynamical measure transport and neural pde solvers for sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeinhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.07873" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Liouville flow importance sampler</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><surname>Scarlett</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v235/tian24c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Berkenkamp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename></persName>
		</editor>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2024-07">Jul 2024</date>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.10347" />
		<title level="m">Discrete flows: Invertible generative models of discrete data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Escorted free energy simulations: Improving convergence by reducing dissipation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vaikuntanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jarzynski</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.100.190601</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevLett.100.190601" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">190601</biblScope>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Denoising diffusion samplers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=8pvnfTAbu1f" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transport meets variational inference: Controlled monte carlo diffusions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blessing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nusken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations: ICLR 2024</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From data to noise to data for mixing physics across temperatures with generative artificial intelligence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Herron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tiwary</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2203656119</idno>
		<ptr target="https://www.pnas.org/doi/abs/10.1073/pnas.2203656119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page">2203656119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Confinement of quarks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.10.2445</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevD.10.2445" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2445" to="2459" />
			<date type="published" when="1974-10">Oct 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Path integral sampler: A stochastic control approach for sampling</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=_uCb2ynRu7Y" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
