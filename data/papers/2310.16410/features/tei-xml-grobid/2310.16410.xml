<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-26">October 26, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Schut</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution" key="instit1">OATML</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nenad</forename><surname>Tomašev</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Mcgrath</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-26">October 26, 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">761094C27719DA25705D61E361A7E3E3</idno>
					<idno type="arXiv">arXiv:2310.16410v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial Intelligence (AI) systems are typically treated as problem-solving machines; they can carry out the jobs humans are already capable of but more efficiently or with less effort, which brings clear benefits in several domains. In this paper, we pursue a different goal: treat AI systems as learning machines and demand from them to teach us the fundamental principles behind their decisions to extend upon and complement our knowledge. We can imagine many benefits of learning from machines. For example, while a system capable of producing a more accurate cancer diagnosis or effective personalised treatment than human experts is useful, transferring the rationale behind their decisions to human doctors could not only bring advances in medicine but also leverage human doctors' strength and generalisation ability to enable new breakthroughs. There is a tremendous untapped opportunity across various domains where the capabilities of AI systems are reaching or exceeding those of human experts (super-human AI systems). This work is one of the very first steps towards the development of tools and methods that allow us to uncover hidden knowledge in highly capable AI systems, and empower human experts by helping them further improve their skills and understanding.</p><p>The super-human ability of AI systems may arise in a few different ways: pure computational power of machines, a new way of reasoning over existing knowledge, or super-human knowledge we do not yet possess. This work focuses on the last two cases. For simplicity, we refer to both as super-human knowledge from now on.</p><p>What does this mean from a research standpoint? The human representational space (H) has some overlap with the machine representational space (M ) (see Figure <ref type="figure">1</ref>  <ref type="bibr" target="#b56">(Kim, 2022)</ref>). A representational space forms the basis of and gives rise to knowledge and abilities, which we are ultimately interested in. Thus, we use representational space and knowledge interchangeablyroughly speaking, H to represent what humans know and M to represent what a machine knows. There are things that both AI and humans know (M ∩ H), things that only humans know (H -M ), Figure <ref type="figure">1</ref>: Learning from machine-unique knowledge. and things only machines know (M -H). Most existing research efforts only focus on (M ∩ H), e.g., interpretability has tried to shoehorn M into (M ∩ H), with limited success <ref type="bibr" target="#b1">(Adebayo et al., 2018;</ref><ref type="bibr" target="#b79">Nie et al., 2018;</ref><ref type="bibr">Bilodeau et al., 2022)</ref>. We believe that the knowledge gap represented by (M -H) holds the crucial key to empowering humans by identifying new concepts and new connections between existing concepts within highly performant AI systems. We already have evidence of cases when certain AI generations captivated the human imagination with ideas that were initially hard to grasp. One prominent example in the history of AI is the move 37 that AlphaGo made in a match with Lee Sedol. This move came as a complete surprise to the commentators and the player, and is still discussed to this day as an example of machine-unique knowledge. The vision to pursue super-human knowledge is ultimately for human-centered AI, and a world where human agency and capability do not come second. However, the question is-is this even possible?</p><p>This work is the first step towards discovering super-human knowledge and new connections of existing knowledge in (M -H). We focus on a domain that has inspired AI practitioners for decades, and captivated human imagination for centuries: the game of chess. Chess is an excellent playground to validate the existence and usefulness of set (M -H) for many reasons: chess knowledge has been developed over a long period of time, and the ground truth is much easier to validate compared to the frontiers of other fields, such as science or medicine. We also have a quantitative measure of the quality of play, both for human experts as well as machines, known as the Elo rating (Wikipedia contributors, 2023a).</p><p>Chess engines have performed at a super-human level for a long time, ever since DeepBlue's match against Garry Kasparov. While early engines were based on human knowledge, the advent of AlphaZero <ref type="bibr" target="#b94">(Silver et al., 2017)</ref> (AZ) showed a self-taught deep learning model achieve a superhuman capability in chess without any human knowledge. However, as humans, we have not yet been able to tap into their knowledge fully. Through analysis of AZ's games, humans manually distilled patterns, such as its proclivity for playing on the flanks with moves like a4 or h4 <ref type="bibr" target="#b90">(Sadler and Regan, 2019)</ref>. However, this still analyses M through the lens of H, a bias that limits what we can find from M ∩ H.</p><p>In this work, we aim to take the first step to change that by facilitating learning from the super-human knowledge in the (M -H) set of AZ. We hypothesise that (M -H) exists, and can be taught to humans.</p><p>We validate our hypothesis by showing that we can teach new chess concepts to four top human grandmasters, the best chess players in the world. Also, due to their undeniable strength, and talent, (M -H) may fall into their 'proximal zone of development' in Vygotsky's education theory: "the space between what a learner can do without assistance and what a learner can do with adult guidance or in collaboration with more capable peers". While communicating (M -H) may require new language <ref type="bibr" target="#b56">(Kim, 2022)</ref>, we bypass this need in this work by leveraging chess champions' ability to connect the dots and generalise from patterns that arise in chess positions.</p><p>We find evidence that suggests (M -H) exists through analysing the dimension of the span of the latent representations of AZ's and human's games ( §4.2.2). Next, we develop a new framework to search for concepts in (M -H), i.e., unearth AZ's super-human knowledge. In our framework, we:</p><p>• develop a new method for finding unsupervised concepts in the latent space. By using the full AZ machinery, both the policy value network and MCTS tree, our method discovers dynamic concepts that motivate a sequence of actions in chess. We show that our method can find vector representations of concepts in a data-efficient manner ( §4.1).</p><p>• ensure concepts are novel. Through spectral analysis, our framework only select concepts that contain information unique to the vector space of AZ's games compared to that of human games.<ref type="foot" target="#foot_0">foot_0</ref> </p><p>• ensure that concepts are teachable. We develop a new metric that evaluates whether concepts are teachable to another AI agent with no prior knowledge of the concept ( §4.2.1). Through this metric, we select concepts based on their informativeness (i.e., useful for the AI agent in a downstream task).</p><p>most chess players would continue to play on the kingside with Rxh5. However, AZ finds the only plan to maintain an advantage: Qc1 with the idea of re-manoeuvring the pieces to the queenside. The results of our study show an improvement in the grandmasters' ability to find conceptbased moves aligned with AZ's choices, as compared to their performance prior to observing AZ's moves. Further, their qualitative feedback indicated an understanding and appreciation of AZ's plans. The discovered concepts often combine and apply chess concepts in a way that deviates from the traditional human principles of chess. We conjecture that the differences in humans' and AZ's play may stem from their differences in how position-concept relationships are built. While humans develop prior biases over which concepts may be relevant in given chess positions, AZ has developed its own unconstrained understanding of concepts and chess positions, enabling flexibility and creativity in its strategies.</p><p>Our paper is structured as follows. First, we summarise related work in §2. Next, we discuss the definition of concepts and how we operationalise it in §3. We introduce our method for finding concepts in §4.1 and filtering them in §4.2 to ensure concepts are informative, teachable, and novel. We demonstrate the efficacy and performance of our method on supervised concepts in §5. Finally, in §6, we lay out the human experiment protocols and results and show how our framework can enable bridging the (M -H) gap. We conclude the paper in §7 by summarising our main findings, and discussing limitations and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Here, we review relevant prior work on concept discovery, interpretability of reinforcement learning systems, and the intersection of AI and chess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept-based explanations</head><p>In contrast to traditional feature or data-centric interpretability methods <ref type="bibr" target="#b86">(Ribeiro et al., 2016;</ref><ref type="bibr" target="#b69">Lundberg and Lee, 2017;</ref><ref type="bibr" target="#b103">Sundararajan et al., 2017;</ref><ref type="bibr" target="#b60">Koh and Liang, 2017)</ref>, concept-based methods use high-level abstraction, concepts, with the goal of providing model explanations to inform human practitioners <ref type="bibr" target="#b11">(Bau et al., 2017;</ref><ref type="bibr" target="#b57">Kim et al., 2018;</ref><ref type="bibr" target="#b5">Alvarez-Melis and Jaakkola, 2018;</ref><ref type="bibr" target="#b61">Koh et al., 2020;</ref><ref type="bibr" target="#b9">Bai et al., 2022;</ref><ref type="bibr" target="#b0">Achtibat et al., 2022;</ref><ref type="bibr" target="#b23">Crabbé and van der Schaar, 2022)</ref>. These types of explanations are shown to be useful in scientific and biomedical domains <ref type="bibr" target="#b45">(Graziani et al., 2018;</ref><ref type="bibr" target="#b99">Sprague et al., 2019;</ref><ref type="bibr" target="#b19">Clough et al., 2019;</ref><ref type="bibr" target="#b15">Bouchacourt and Denoyer, 2019;</ref><ref type="bibr" target="#b119">Yeche et al., 2019;</ref><ref type="bibr">Sreedharan et al., 2020a;</ref><ref type="bibr" target="#b92">Schwalbe and Schels, 2020;</ref><ref type="bibr" target="#b76">Mincu et al., 2021;</ref><ref type="bibr" target="#b54">Jia et al., 2022)</ref>, where experts' concepts are highly relevant in decision making rather than individual low-level features.</p><p>More in line with the work presented in this paper, concept-based explanation methods have been studied in board game playing agents, including in Hex <ref type="bibr" target="#b37">(Forde et al., 2022)</ref> and Go <ref type="bibr" target="#b108">(Tomlin et al., 2022)</ref>. Establishing a causal link between concepts and predictions is non-trivial and a topic of ongoing research <ref type="bibr" target="#b44">(Goyal et al., 2019;</ref><ref type="bibr" target="#b8">Bahadori and Heckerman, 2020;</ref><ref type="bibr" target="#b117">Wu et al., 2023)</ref>.</p><p>Shortcomings of supervised concept-based methods have been studied. When leveraging a set of exemplars of a concept (a probe dataset), <ref type="bibr" target="#b85">Ramaswamy et al. (2023)</ref> showed that different probe datasets can lead to inconsistent conclusions. Further, they showed that the number of concepts in probe datasets exceeds those used by humans. The linearity assumption has limitations <ref type="bibr" target="#b17">(Chen et al., 2020;</ref><ref type="bibr" target="#b98">Soni et al., 2020)</ref>, and the faithful alignment between the vector and humans' mental models of the concept was challenged in <ref type="bibr" target="#b71">Mahinpei et al. (2021)</ref>.</p><p>Going beyond supervised concepts and probe datasets has also been investigated <ref type="bibr" target="#b120">(Yeh et al., 2020;</ref><ref type="bibr" target="#b41">Ghorbani et al., 2019;</ref><ref type="bibr" target="#b40">Ghandeharioun et al., 2021)</ref> to discover concepts that a model represents without being limited to human labelled concepts. The concept is expressed using examples of training data <ref type="bibr" target="#b120">(Yeh et al., 2020;</ref><ref type="bibr" target="#b41">Ghorbani et al., 2019)</ref> or by generating new data <ref type="bibr" target="#b40">(Ghandeharioun et al., 2021)</ref>. This work falls under methods to discover concepts but with a different goal of discovering and teaching humans new concepts rather than finding existing human concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating explanations in Reinforcement Learning</head><p>Generating explanations in Reinforcement Learning (RL) methods <ref type="bibr" target="#b3">(Alharin et al., 2020;</ref><ref type="bibr" target="#b51">Heuillet et al., 2020;</ref><ref type="bibr" target="#b42">Glanois et al., 2021;</ref><ref type="bibr" target="#b63">Krajna et al., 2022;</ref><ref type="bibr" target="#b112">Vouros, 2022;</ref><ref type="bibr" target="#b75">Milani et al., 2022;</ref><ref type="bibr" target="#b26">Dazeley et al., 2023;</ref><ref type="bibr" target="#b81">Omidshafiei et al., 2022;</ref><ref type="bibr" target="#b24">Das et al., 2023)</ref> is of particular interest, as these methods are increasingly deployed in real-world applications, and the explanation requirements differ compared to the more traditional supervised learning setting. This is due to the temporal dependency between states, actions, and subsequent states, where an agent's historical, current, and future state-action sequences may relate to some long-term goal <ref type="bibr" target="#b26">(Dazeley et al., 2023)</ref>. Explainability methods in RL can help identify issues with agents related to over-fitting the training data, poor out-of-distribution performance <ref type="bibr" target="#b6">(Annasamy and Sycara, 2019)</ref> and inter-agent dynamics <ref type="bibr" target="#b81">(Omidshafiei et al., 2022)</ref>.</p><p>Several efforts focused on designing more interpretable model architectures and training proce-dures in representation learning <ref type="bibr" target="#b84">(Raffin et al., 2019</ref><ref type="bibr" target="#b83">(Raffin et al., , 2018;;</ref><ref type="bibr" target="#b65">Lesort et al., 2019;</ref><ref type="bibr" target="#b110">Traoré et al., 2019;</ref><ref type="bibr" target="#b31">Doncieux et al., 2020</ref><ref type="bibr" target="#b32">Doncieux et al., , 2018) )</ref> and symbolic and relational methods <ref type="bibr" target="#b101">(Sreedharan et al., 2020b;</ref><ref type="bibr" target="#b39">Garnelo et al., 2016;</ref><ref type="bibr">d'Avila Garcez et al., 2018;</ref><ref type="bibr" target="#b123">Zambaldi et al., 2018;</ref><ref type="bibr" target="#b50">Hazra and De Raedt, 2023)</ref>, which may involve intermediate perceptual processing steps, like object recognition <ref type="bibr" target="#b43">(Goel et al., 2018;</ref><ref type="bibr" target="#b66">Li et al., 2018)</ref>. <ref type="bibr">Different RL methods (value-based, policy-based, model-based, fully</ref> or partially observable states) <ref type="bibr" target="#b3">(Alharin et al., 2020)</ref> may lend themselves to different explainability approaches or variations thereof. Likewise, the explanations themselves may vary in scope, e.g., local explanations of individual agent actions or value assessments, or the overall high-level explanations of the agent policy <ref type="bibr" target="#b124">(Zrihem et al., 2016;</ref><ref type="bibr" target="#b101">Sreedharan et al., 2020b;</ref><ref type="bibr" target="#b109">Topin et al., 2021)</ref>. The importance of treating explanations as a reward to ensure consistency is explored in <ref type="bibr" target="#b118">Yang et al. (2023)</ref>.</p><p>For trained RL systems, there is a pressing need for post-hoc RL interpretability methods. Input saliency maps <ref type="bibr" target="#b113">(Wang et al., 2016;</ref><ref type="bibr" target="#b93">Selvaraju et al., 2019;</ref><ref type="bibr" target="#b46">Greydanus et al., 2018;</ref><ref type="bibr" target="#b77">Mundhenk et al., 2020)</ref> and tree-based models <ref type="bibr" target="#b10">(Bastani et al., 2018;</ref><ref type="bibr" target="#b88">Roth et al., 2019;</ref><ref type="bibr" target="#b21">Coppens et al., 2019;</ref><ref type="bibr" target="#b68">Liu et al., 2019;</ref><ref type="bibr" target="#b111">Vasic et al., 2019;</ref><ref type="bibr" target="#b70">Madumal et al., 2020)</ref> have been a common approach. Saliencybased RL explainability approaches are not without issues, as they may suffer from unfalsifiability and be subject to cognitive bias <ref type="bibr" target="#b7">(Atrey et al., 2019)</ref> as well as provably wrong results <ref type="bibr">(Bilodeau et al., 2022)</ref>. Visualizing the agent memory over trajectories <ref type="bibr" target="#b52">(Jaunet et al., 2020)</ref> or extracting finite-state models <ref type="bibr" target="#b62">(Koul et al., 2018)</ref> are explored to improve understanding of agents' behavior, as well as leveraging Markov decision processes <ref type="bibr" target="#b36">(Finkelstein et al., 2022;</ref><ref type="bibr" target="#b121">Zahavy et al., 2016)</ref> to generate explanations or detect sub-goals or emerging structures <ref type="bibr" target="#b89">(Rupprecht et al., 2019)</ref>. As RL methods may sometimes learn spurious correlations, interpretability methods were used to help identify and resolve the causal confusion <ref type="bibr" target="#b38">(Gajcin and Dusparic, 2022)</ref> and further our understanding using counterfactuals <ref type="bibr" target="#b28">(Deshmukh et al., 2023;</ref><ref type="bibr" target="#b80">Olson et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Chess and Artificial Intelligence</head><p>Chess has been a test bed for AI ideas for decades. Early engines were based on human knowledge, and their super-human strength came from their computational capacity, which allowed them to consider a number of variations orders of magnitude above the abilities of human chess players. The introduction of neural network and RL-based approaches aimed to revitalise the field, which led to a surge of improvements in computer chess engines. These advances were in part inspired by the prominent results of AZ in chess and its variants <ref type="bibr" target="#b95">(Silver et al., 2018;</ref><ref type="bibr" target="#b106">Tomašev et al., 2020;</ref><ref type="bibr" target="#b107">Tomašev et al., 2022;</ref><ref type="bibr" target="#b122">Zahavy et al., 2023), and</ref><ref type="bibr">Lc0 (LCZero Development Community, 2018)</ref>, an open-source re-implementation of the original model, is still competing at the highest level of computer chess.</p><p>As interactions with chess engines play a key role in chess players' preparation and training, interpretability helps chess players understand the underlying positional and tactical motifs. To this end, prior work has looked at piece saliency <ref type="bibr" target="#b47">(Gupta et al., 2020)</ref>, tree-based explanations <ref type="bibr" target="#b55">(Kerner, 1995)</ref> and natural language <ref type="bibr" target="#b53">(Jhamtani et al., 2018)</ref>. At the intersection of chess and language, ChessGPT has recently been proposed <ref type="bibr" target="#b33">(Feng et al., 2023)</ref> to bridge the modality of policy and language. DecodeChess is a software project aimed at deriving explanations from engine search trees <ref type="bibr" target="#b27">(DecodeChess, 2017)</ref>.</p><p>Recently, AZ has was shown to encode human-like concepts in its network <ref type="bibr" target="#b73">(McGrath et al., 2022)</ref>, and concept probing techniques have also been explored with the network-based Stockfish chess engine <ref type="bibr" target="#b82">(Pálsson and Björnsson, 2023)</ref>. This prior investigation of concepts in AZ did not consider search and move sequences, and was largely restricted to identifying pre-existing human concepts. Preliminary questions have been raised regarding whether human players have been adopting AZ's ideas <ref type="bibr" target="#b44">(González-Díaz and Palacios-Huerta, 2022)</ref>, as some prominent motifs had been analysed in detail in Game Changer <ref type="bibr" target="#b90">(Sadler and Regan, 2019)</ref>. Recently, it was also shown that AZ may be susceptible to adversarial perturbations <ref type="bibr" target="#b64">(Lan et al., 2022)</ref>, underscoring the need for a better understanding of the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">What are concepts?</head><p>There are several possible definitions of a concept -varying from a human-understandable high-level feature to an abstract idea. In this work, we define concepts as a unit of knowledge. There are two key properties we focus on. The first is that a concept contains knowledge: information that is useful; in the context of machine learning, we take this to mean that it can be used to solve a task. For example, consider the concept of a beak. We can teach an algorithm or person (transfer of the knowledge) what a beak is. If the person grasps the beak concept, they can use it to identify birds. Second, a unit implies minimality; it is concise and irrelevant information has been removed.</p><p>There are many ways to operationalise this definition and properties, and we choose one of them: showing a concept can be transferred to another agent to help them solve a task (e.g., follow the strategy represented in a concept). Being able to do so implies that the concept is self-contained and useful for the task.</p><p>How do we represent concepts? We leverage rich literature that assumes concepts are linearly encoded in the latent space of a neural network <ref type="bibr" target="#b73">(McGrath et al., 2022;</ref><ref type="bibr" target="#b57">Kim et al., 2018;</ref><ref type="bibr" target="#b48">Gurnee et al., 2023;</ref><ref type="bibr" target="#b20">Conneau et al., 2018;</ref><ref type="bibr" target="#b104">Tenney et al., 2019;</ref><ref type="bibr" target="#b78">Nanda, 2023)</ref>. The latent space refers to the space spanned by post-activation features of a neural network. Although our assumption of linearity is a strong assumption, it has a surprising amount of empirical support: linear probing and related techniques have successfully extracted a wide range of complex concepts from neural networks across multiple domains <ref type="bibr" target="#b73">(McGrath et al., 2022;</ref><ref type="bibr" target="#b57">Kim et al., 2018;</ref><ref type="bibr" target="#b48">Gurnee et al., 2023;</ref><ref type="bibr" target="#b20">Conneau et al., 2018;</ref><ref type="bibr" target="#b104">Tenney et al., 2019;</ref><ref type="bibr" target="#b78">Nanda, 2023)</ref>. Although we may miss concepts with nonlinear representations, we nevertheless show that we can find useful concepts for our goal using purely linear representations.</p><p>What types of concepts do we aim to discover in the RL setting? We aim to discover concepts that give rise to a plan, where a plan is a deliberate sequence of actions optimizing for one or more relevant concepts. We take deliberate to mean that there is an underlying reason. More specifically, we assume a plan is motivated by one or more concepts. Although the terminal goal of a plan the same across states -maximizing the outcome (win or draw) -plans in a specific state will have more context-specific instrumental goals along the way, for instance, capturing a particular piece in an advantageous position, or maximising one's board control. We assume that plans in similar contexts will share similar instrumental goals, and thus give rise to similar concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discovering concepts</head><p>Our method can be summarised into (1) excavating vectors that represent concepts in AZ using convex optimisation, (2) filtering the concepts based on teachability (whether it is transferable to another AI agent) and novelty (whether it contains some information that is not present in human games). The resulting set of concept vectors is then used to generate chess puzzles (chess positions and solutions), which are presented to human experts (top chess grandmasters) for final validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Excavating concept vectors</head><p>To find concepts, we develop a new method since (1) the model input is a mix of binary and realvalued inputs (e.g., saliency maps typically take as input continuous values and are generally not suitable for binary values) and (2) we want to develop an interpretability tool to analyse both parts of AZ machinery -the policy-value network and MCTS. Leveraging both the network and MCTS is crucial, since each component plays a different yet important role in deciding the move (see §8.3 for more detail). We formulate concept discovery as a convex optimisation problem. Using a convex optimisation framework is not new; many existing methods for finding concept vectors, such as non-negative matrix formulation, can often approximated as a convex optimisation problem <ref type="bibr" target="#b30">(Ding et al., 2008)</ref>.</p><p>For each concept vector we want to find, we formulate a separate convex optimisation problem. As mentioned in §3, we define a concept as a unit of knowledge. Minimality is achieved by encouraging sparsity <ref type="bibr" target="#b105">(Tibshirani, 1996)</ref> </p><formula xml:id="formula_0">through the L 1 norm min ∥v c,l ∥ 1 such that concept constraints hold,<label>(1)</label></formula><p>where v c,l ∈ R d l is a vector that lives in latent space of layer l to represent concept c, and d l is the dimension of layer l.</p><p>We outline the concept constraints used for two different types of concepts: static and dynamic concepts. Static concepts are defined to be found in a single state, whereas dynamic concepts are found in a sequence of states. An example of a static concept in autonomous driving is that the car is located on the highway. A dynamic concept is that the car is accelerating. While our framework only aims to discover dynamic concepts, we use static concepts to validate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Concept constraints for static concepts</head><p>Static concepts are defined as concepts that only involve a single state. We use supervised data (labels indicate whether a state contains a concept c) to learn static concept vectors. These concepts encode human knowledge, and therefore, we can use them to validate our approach. One example of a static concept is the concept of 'space', which we can infer from a single state. For now, assume we have binary concepts<ref type="foot" target="#foot_1">foot_1</ref> and denote the presence of concept c (concept score) in chess position x by c(x) = 1, and c(x) = 0 otherwise. For each concept c, we can split a general set of chess positions X into positive examples X + , where the concept is present, and X -, where it is absent</p><formula xml:id="formula_1">X + = {x ∈ X : c(x) = 1} X -= {x ∈ X : c(x) = 0}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>These positive and negative examples allow us to generate corresponding positive and negative examples of latent representations (intermediate post-activation representations in the network).</head><p>The function f l (x) generates an activation for layer l given an input x:</p><formula xml:id="formula_2">Z + l = {f l (x) : x ∈ X + } Z - l = {f l (x) : x ∈ X -},</formula><p>where z l = f l (x) denotes the latent representation obtained at layer l by passing input x through the network. See §8.2 for further details on how z l is extracted.</p><p>The convex optimisation goal is to learn a sparse vector v c,l that represents a concept c. We hypothesise that the inner product v c,l • z l is higher<ref type="foot" target="#foot_2">foot_2</ref> for activations from Z + l (the set where the concept is present) than for activations from Z - l (the set where the concept is absent). Thus, the formulation becomes</p><formula xml:id="formula_3">min ∥v c,l ∥ 1 such that v c,l • z + l ≥ v c,l • z - l ∀ z + l ∈ Z + l , z - l ∈ Z - l .<label>(2)</label></formula><p>We can evaluate how well a concept is represented by v c,l in the supervised setting by splitting X into two sets: X train and X test and then v c,l only using X train . We then measure the fraction of elements in X test on which the concept constraints hold. If v c,l represents the concept c well, we expect the concept constraint to hold on previously unseen activations derived from X test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Concept constraints for dynamic concepts</head><p>Dynamic concepts are defined as concepts found in a sequence of states. While v c,l is found in the activation space of the policy-value network, we use the Monte Carlo Tree Search (MCTS) statistics to find candidates for meaningful sequences of states. MCTS generates a tree of possible moves and subsequent responses from the current chess position x 0 (for details on the implementation of MCTS see <ref type="bibr" target="#b91">Schrittwieser et al. (2019)</ref>). The exact details are not essential to understand for our procedure; what matters is that AZ chooses a rollout X + ≤T = (x + 1 , x + 2 , x + 3 , . . . , x + T ), where T is the maximum depth of the rollout, which terminates in the most favourable state according to AZ. We contrast this optimal rollout X + ≤T with a sub-par rollout X - ≤T , which is defined as a path in the MCTS search tree that is suboptimal, according to the value estimate or visit count in MCTS.</p><p>The intuition behind our procedure is that X + ≤T is chosen over X - ≤T because of a concept c, and we assume the concept c is detectable by a linear probe at some layer l. The concept presence may affect planning in different ways. Consider two rollouts in MCTS, one chosen by AZ (X + ≤T ), and one not chosen by AZ, (X - ≤T ). There are three different possible explanations for why AZ chooses X + ≤T over X - ≤T :</p><p>1. Active planning X + ≤T increases the presence of a concept c. For example, the rollout X + ≤T may increase the concept of piece activity.</p><p>2. Prophylactic planning X + ≤T avoids increasing the presence of a concept c. An example may be that the plan in X + ≤T avoids losing a piece.</p><p>3. Random X + ≤T is arbitrarily chosen above the X - ≤T , as all concepts are equally present in both rollouts and the value estimates of the final states are approximately equal.</p><p>We are interested in scenarios 1 and 2 but not scenario 3. Scenario 3 can be filtered out by leveraging the fact that the two rollouts will have similar value estimates and visit counts in the MCTS statistics.</p><p>Using a similar approach to static concepts, we derive our concept constraints on the vector v c,l by contrasting the positive and negative examples, except that this time our contrasting examples are pairs from the chosen rollout X + ≤T and the subpar rollout X - ≤T . We denote the activations at layer l at depth t by z + t,l and z - t,l for positive and negative examples, respectively. A single pair of positive and negative rollouts gives rise to the following convex optimisation problem min ∥v c,l ∥ 1</p><p>(3)</p><formula xml:id="formula_4">such that v c,l • z + t,l ≥ v c,l • z - t,l ∀ t ≤ T,<label>(4)</label></formula><p>for scenario 1, with the inequality reversed for scenario 2.</p><p>Figure <ref type="figure">3</ref>: Contrasting the optimal rollout with subpar MCTS rollouts at different time steps. The green rollout shows the optimal rollout, and the red rollouts depict subpar trajectories. At each time step, MCTS finds subpar trajectories. We include each of these pairs in the concept constraints.</p><p>We extend this idea by contrasting the optimal trajectory with multiple subpar trajectories across different MCTS depths. Figure <ref type="figure">3</ref> shows this idea. On the left side of Figure <ref type="figure">3</ref>, we find the optimal and subpar trajectory at the initial chess position, t = 1. However, we can also use the MCTS statistics (value estimate and visit count) to find subpar trajectories at t = 2 (shown in the middle) and t = 3 (shown on the right). The idea behind using multiple subpar trajectories is to further reduce the solution space with the goal of reducing noise (thereby increasing the likelihood that the concept vector is meaningful) and decreasing the likelihood of learning a polysemantic vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let Z +</head><p>≤T,l denote the latent representations in layer l corresponding to the optimal rollout X + ≤T , and Z - ≤T,l,j denote the latent representations in layer l corresponding to the subpar rollout X - ≤T,j selected at time step j. We find the dynamic concept as follows:</p><formula xml:id="formula_5">min ||v c,l || 1 (5) such that v c,l • z + t,l ≥ v c,l • z - t,l,j ∀t ≤ T, j ≤ T ,</formula><p>where T denotes the maximum depth at which we find suboptimal rollouts. T = 3 in Figure <ref type="figure">3</ref>. In general, we set T = T -5 to ensure the rollout is sufficiently deep. Details on how T is set can be found in §8.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Filtering concepts</head><p>Our approach (described in §4.1) provides many concept vectors, some or many of which represent known concepts or non-generalisable concepts (i.e., only applicable to a single chess position). In this section, we describe how we further filter concepts to ensure that they are useful (transferable) and novel. Our first filtering for usefulness is to see if we can teach a concept to a student network such that it leads to an improved performance on concept test positions. We describe this process of selecting only teachable concepts in §4.2.1. We further filter concepts based on novelty ( §4.2.2) by finding representations in AZ's self-play games that do not occur in a top-level human play dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Teachability</head><p>Recall that we defined a concept as a unit of knowledge -a key aspect is that it is teachable to another AI agent or person, who can apply the concept to solve an unseen task ( §3). To ensure our concepts are teachable, we use teachability as a selection criterion for the final concepts. The idea is simple:</p><p>1. Phase 1: Baseline find an agent that does not know the concept. We can estimate the agent's knowledge by evaluating its performance on a concept-related task.</p><p>2. Phase 2: Teach the agent the concept using concept prototypes.</p><p>3. Phase 3: Evaluate the agent's performance on a concept-related task If a concept is teachable, we expect the agent's performance to improve between the first and third steps. We use a similar process to evaluate when we evaluate our approach with humans ( §6).</p><p>Selecting prototypes. In Phase 2, we use AZ as a teacher to supervise a student network on a set of chess positions called 'prototypes' -chess positions that exemplify the use of a concept.</p><p>For each candidate concept c, we have a concept vector v c,l . We want chess positions x from a dataset X that epitomises the concept c. For static concepts, we do this by computing a concept score c(x) = v c,l • f l (x) for every x ∈ X and then selecting the top 2.5% of X according to the concept score c(x). We use 2.5%, as we found the concept score c(x) to be comparable to v c,l • z + l , where z + l ∈ Z + l is the training set used to find v c,l . This procedure gives us a prototype set X proto = {x ∈ X : c(x) in 2.5th percentile of c(x)}. For dynamic concepts, we find chess positions using the MCTS statistics. For each chess position x ∈ X, we ran MCTS. Next, we found the chosen rollout X + (and the corresponding latent representations Z + ) and subpar rollout X -(and the corresponding latent representations Z -) as in §4.1.2. For a prototype</p><formula xml:id="formula_6">x i ∈ X proto , we require that v c,l • z + i,t ≥ v c,l • z - i,t ∀ t ≤ T .</formula><p>Teaching and measuring learning. Intuitively, we can interpret the set of prototypes as a curriculum. We can split X proto into a train set X train and a test set X test . The teacher (AZ) teaches the student by minimizing the KL divergence between the policies of the teacher (π t ) and the student (π s ) on the training prototypes:</p><formula xml:id="formula_7">xi∈Xtrain KL[π t (x i ), π s (x i )].</formula><p>Then, to determine whether the student has acquired the new knowledge, we evaluate the student's performance on the test set prototypes by estimating how often the student and teacher select the same top-1 move</p><formula xml:id="formula_8">T = xi∈Xtest 1[argmax(π s (x i )), argmax(π t (x i ))]. (6)</formula><p>Teaching using any curriculum may improve students' performance on the task. To distill general learning from concept-specific learning, we compare the student's performance when taught using concept prototypes versus random chess positions sampled from AZ's games (but with meaningful plans). We sample the chess positions from AZ's games instead of human games for two reasons:</p><p>(1) AZ's games tend to be of a higher quality than human games (as AZ has a higher Elo rating), and (2) the data is closer to AZ's natural training data (avoiding any confounding effects due out of distribution data). Figure <ref type="figure" target="#fig_1">4</ref> shows the student's performance in four settings: (1) student trained on concept c and evaluated on concept c (dark blue line); (2) student trained on concept c and evaluated on random data from AZ's games (dark green line); (3) student trained on random data and evaluated on concept c (light blue line); and (4) student trained on random data and evaluated on random data (light green line). When teaching a student with concept-specific prototypes, the student also improves its performance on a random set of prototypes (dark green line) but less than on conceptspecific prototypes (dark blue line). When a student was taught with randomly sampled chess positions, labelled with optimal play, it improves their performance significantly less (light lines) than when it was taught with concept-specific prototypes (dark lines). Naturally, the student learns quicker when taught with concept-specific prototypes (dark blue line) than random prototypes (light blue line). We also observe that concepts can be taught efficiently. The student's performance after training for 50 epochs on a small set of prototypes would have taken 10K -100K epochs using self-play. Recall that the student is evaluated on a holdout test set, ruling out the possibility that the student network memorised the chess positions.</p><p>We select the student to be the latest checkpoint for which the top-1 policy overlap is less than 0.2, resulting in 97.6% of the concepts being filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Novelty</head><p>There are many different ways to ensure the novelty of concepts. We take two simple approaches:</p><p>(1) ensure concepts are learnt during the later stages of AZ's training and (2) filter concepts based on a novelty metric.</p><p>Concepts are learnt during a late stage in AZ's training. To find chess positions that are complex, we leverage AZ's training history. We use two versions of AZ that differ by 75 Elo points. <ref type="foot" target="#foot_3">4</ref>To find interesting chess positions, we run through each game and select chess positions where the two versions of AZ disagree on the best move according to their policies. We only use these chess positions to discover concepts.</p><p>Setup to measure novelty. While the previous section ensures that concepts that emerge in the final stages of training in AZ are complex by construction, it does not ensure that the concepts are novel to humans. One way to validate novelty is to determine whether the concepts arise in AZ's games but not in human games. Leveraging the fact that concepts are represented in the latent space as a vector, we can compare the vector space of AZ games to that of human games. Specifically, let Z a l denote a matrix where we stack the latent representations in layer l of 17, 184 chess positions sampled from AZ's games. Each row represents a chess position, and each column represents a dimension in latent space in layer l. Similarly, we define Z h l as a matrix of 17, 184 chess positions sampled from human games. Using the latent representations, (1) we first get evidence that AZ's game is likely to contain new concepts using a rank experiment, and then (2) measure the novelty scores by regressing concepts onto AZ's games vector space and human games vector space on which we filter concepts based.</p><p>AZ games likely to contain new concepts compared to human games. First, we aim to establish whether AZ games contain something new compared to human games. We approach the question by contrasting the number of concepts encoded in both. The number of the basis vectors (or ranks of Z h l and Z a l ) estimates the size of the span of the latent representations of the gamesinformally, we can think of it as a proxy for the number of concepts.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, the ranks for Z h l and Z a l vary across different layers. We focus on the final layers in the architecture as these more directly impact the output. Note that while the rank of inputs for both humans and AZ games are similar, AZ games' rank is higher than the humans games' rank at layers 19 (final layer in main bottleneck) and 23 (policy layer), hinting that there might be concepts present in AZ games that are not in human games. Therefore, we focus on finding new concepts in these layers. Novelty Scores for filtering We define the novelty scores based on how well a concept vector can be reconstructed using a set of basis vectors that arise in AZ's games. Naturally, a lower reconstruction loss means that the concept is better represented using a set of given basis vectors.</p><p>In other words, we look for concepts that are better explained using AZ's language (basis vectors) than humans'. We define novelty score as the difference between concept's reconstruction loss (see Equation <ref type="formula" target="#formula_10">7</ref>) when using basis vectors from humans' game and AZ games. A higher score means a closer alignment with the basis vectors arising from AZ's games.</p><p>Specifically, for Z h l and Z a l , we find the singular value decomposition to find the basis of the space spanned by AZ's and human games</p><formula xml:id="formula_9">Z h l = U h l Σ h l V h⊤ l , Z a l = U a l Σ a l V a⊤ l ,</formula><p>where the columns of U h l and U a l form an orthonormal basis for the rows of Z h l and Z a l , respectively; Σ h l and Σ a l are the singular value matrices; and the columns of V h⊤ l and V a⊤ l form the orthonormal basis for the columns of Z h l and Z a l respectively.</p><p>The novelty score of concept vector v c,l is defined as min</p><formula xml:id="formula_10">β i,l v c,l - k i=1 β i,l u h i,l 2 -min γ i,l v c,l - k i=1 γ i,l u a i,l 2 , (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where β i,l , γ i,l are coefficients estimated using linear regression, u a i,l and u h i,l are columns of U a l and U h l , respectively, and k is the number of basis vectors used. We do not set k as the rank of the matrix because they differ for Z a l and Z h l , and doing so would favour the matrix with the largest rank. Instead, we estimate Equation <ref type="formula" target="#formula_10">7</ref>for various values for k.</p><p>Figure <ref type="figure">5</ref> shows the novelty scores for concepts in layer 19 for 120 concepts. We accept concepts for which the reconstruction error using AZ's basis vectors is less than that of the human game's basis vectors for every k. The light green lines denote the novelty scores for the concepts we accept, and the light blue lines denote the novelty scores for the concepts we reject.</p><p>Figure <ref type="figure">5</ref>: Filtering concepts based on novelty scores. Concepts for which the reconstruction error using AZ's basis vectors is less than the reconstruction error using human game's basis vectors for every k are accepted (not filtered). The darker green and blue lines show the average over the accepted and rejected concepts.</p><p>Of the remaining concepts after teachability-based filtering, we remove a further 27.1% using the novelty metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method evaluation</head><p>This section includes algorithmic evaluations of our proposed concept discovery method. Table <ref type="table" target="#tab_1">2</ref> summarises the datasets used; further details on each dataset can be found in §8.4.2, and implementation details for each dataset can be found in §8.4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation of the proposed convex optimisation framework for finding concept vectors</head><p>Using the datasets mentioned above, we find concept vectors using the approaches described in §4.1.1 and §4.1.2. While we use layer 19 for all other sections due to its potential novelty according to spectral analysis in §4.2.2, we conduct our evaluation on a few additional layers here: the first latent representation in the policy head (layer 23); and the latent representations in the value head (layer 20 and 21) (See §8.2). These layers are selected due to their proximity to the network outputs -the policy and value estimate.</p><p>We first validate our approach by showing the convex optimisation formulation can be used to find the vector representations of a concept using labelled data ( §5.1.1) and show that this can be done efficiently with a small number of labels ( §5.1.2). Next, we further validate our approach by showing that amplifying the concept vector in the latent representation makes AZ's moves to be more similar to the concept ( §5.1.3). This section focuses on validating the convex optimisation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Do the concept constraints hold for a test dataset?</head><p>To evaluate our framework and thus the quality of the vector representations of the supervised concept, we measure the percentage of times the concept constraints hold on the test set (80/20 train/test split), as shown in Table <ref type="table" target="#tab_2">3</ref>. We find that most datasets led to a high accuracy, which is an indication of quality concept vectors. A concept-level breakdown of the accuracy for each dataset can be found in §8.9.1. 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) Openings (per line) 0.99 (0.13) 0.99 (0.14) 0.99 (0.14) 0.99 (0.13) 5.1.2 How many data points do we need to learn a concept?</p><p>We can use labelled examples to evaluate the concept vector quality (as in the previous section), but they can be hard to come by in practice. This section shows that our formulation can find concept vectors efficiently using a few examples (for the concept constraints). We measure the test set accuracy (as in §5.1.1) while varying sizes of the training set for two datasets: pieces and the strategic test suite (STS). Here, we discuss the result for the piece dataset. The results for the STS dataset are similar and can be found in §8.10.</p><p>As shown in Figure <ref type="figure" target="#fig_2">6</ref>, we find that the method reaches close to full-set accuracy with only a few samples -often as little as 10 data points on the pieces dataset. Interestingly, we observe relatively lower performance in the value head (layer 20) than the policy head (layer 23). One potential explanation is that the concept of a specific piece no longer has to exist when estimating the value, which is a scalar -a highly compressed representation of the state of the game. We speculate that it is possible that simple concepts are combined with other concepts. For example, the network may encode the presence of light pieces, i.e., bishops and knights, rather than the presence of bishops. This may explain the relatively low, but not significantly lower performance in layer 20. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Does amplifying concept vectors increase concept-related behavior?</head><p>We want to (1) determine whether the concept vector captures the intended concept and (2) understand whether the concept influences AZ's output (policy). ( <ref type="formula" target="#formula_3">2</ref>) is important as a concept may exist in a latent representation but not be used by the network for its predictions. To investigate these properties, we use concept amplification. Let z l denote the latent representation in layer l of a chess position x. To amplify the presence of a concept, we nudge the latent representation in the direction of the concept vector v c,l</p><formula xml:id="formula_12">zl = (1 -α)z l + αβ ∥z l ∥ ∥v c,l ∥ v c,l ,<label>(8)</label></formula><p>where α ∈ [0, 1] and β are hyper-parameters for the size of the perturbation; and || • || is the ℓ 2 norm. We use cross-validation to determine β, and find that β = 0.01 is the best overall (see §8.5).</p><p>We report our results for various α in Figure <ref type="figure" target="#fig_3">7</ref> using the STS dataset. The STS dataset includes different types of puzzles grouped according to a strategic theme (such as 'square vacancy'). For each concept, there are 100 chess positions (X) and a solution set S i for each chess position x i ∈ X.</p><p>The solution moves require applying the concept given a chess position. As a baseline, we first evaluate AZ's performance by recording the percentage of times the move selected by AZ under the policy is in the solution set:</p><formula xml:id="formula_13">A = i 1[argmax π l (z i,l ) ∈ S i ],<label>(9)</label></formula><p>where π l (z i,l ) is AZ's policy on the latent representation z i,l , and 1[•] is an indicator function that is equal to 1 if the move selected by AZ is in the solution set S i . We compare the performance difference between with and without concept amplification and report the normalised values ( Ã -A)/A in Figure <ref type="figure" target="#fig_3">7</ref>.</p><p>As this experiment focuses on the impact of the concept on the predicted move (only the policy output, not the value), we analyse the performance of concepts found in layers 18 and 19 (layers before policy and value head split, see §8.2), and layer 23 (policy head). Each line in Figure <ref type="figure" target="#fig_3">7</ref> represents the results for a different concept quality, where concept quality is measured by test accuracy as in §5.1.1. We observe that amplifying the concept can improve AZ's performance on the puzzles of the concept. Naturally, the quality of concepts influences this; concepts with higher test accuracy lead to a larger performance improvement, suggesting a higher test score is a good proxy for how well the concept vector captures the semantic meaning of the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human evaluation</head><p>We investigate whether top chess grandmasters can successfully learn and subsequently apply the concepts we discovered (in §4). In particular, we investigate if it is possible to learn these concepts via exposure to a small set of prototypes of each concept. These prototypes are found using a concept vector as in §4.2.1, and then filtered based on a set of criteria aimed at identifying the most relevant high-quality prototypes (see §8.8 for more information). Learning from prototypes is similar to the established approaches to teaching chess. Chess students are often presented with puzzles sampled according to a theme (opening, chess position type, piece sacrifices, etc.), and practicing puzzle solving (i.e., finding the correct next moves) is one way of improving their overall strength and ability (Alisa <ref type="bibr" target="#b4">Melekhina, 2014)</ref>.</p><p>Henceforth, we use puzzles to denote prototypes and their 'solutions' (AZ's selected move). Human evaluation with grandmasters follows three phases, similar to how teachability is measured §4.2.1:</p><p>• Phase 1: Measuring baseline performance. Each grandmaster provides solutions for a set of provided puzzles corresponding to a set of concepts. This phase determines the baseline performance: the number of puzzles in which the chess grandmaster gets the continuation correct before the learning phase.</p><p>• Phase 2: Learning from AZ's calculations. The same puzzles as in Phase 1 are shown to chess grandmasters alongside the associated AZ's suggested top line based on MCTS calculations for each puzzle. This serves as the simplest way of teaching.</p><p>• Phase 3: Measuring final performance. Grandmasters are tasked with providing solutions for a test set of unseen puzzles sampled from the same concepts they have seen in Phase 1. We compute the grandmasters' accuracy on the puzzle test set and compare it to their performance on the puzzle training set in Phase 1 to measure whether their performance changes.</p><p>We work with four players, all of whom hold the grandmaster title; one of our participants is rated 2600-2700, and three are rated 2700-2800. At each stage, we also asked the grandmasters to provide a summary of their thought process in free form. As each puzzle is complex, the study participants would likely spend considerable time on each puzzle to analyse the chess positions in great depth. Therefore, to avoid over-burdening the study participants, they were presented with four puzzles (per concept) for 3-4 discovered concepts at each stage of the study. In total, each grandmaster saw 36 to 48 chess puzzles. While there was some overlap between study participants in terms of puzzles shown, different participants were shown different concepts. Given that the participants had a limited amount of time and the high time investment per puzzle, this randomisation allows us to explore a more extensive concept set (compared to showing every grandmaster the same concepts and puzzles). While some chess concepts may be more teachable than others, it is difficult to establish this beforehand without inserting human bias. Overall, we find that all study participants improve notably between phases 1 and 3, as shown in Table <ref type="table" target="#tab_3">4</ref>, suggesting that the chess grandmasters were able to learn and apply their understanding of the represented AZ chess concepts. The magnitude of improvement does not correlate with the chess player's strength (i.e., Elo rating). Below, we discuss factors that may have influenced performance:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Grandmaster performance</head><p>• Variability in difficulty and quality. We filter prototypes (as described in §4.2) to ensure quality and complexity. However, the difficulty and quality of puzzles players received may vary across puzzles.</p><p>• Variability in teachability. While we filter based on teachability (as described in §4.2.1), the teachability metric is based on teaching the concept to another AI agent, which may be inherently different from teaching humans.</p><p>• Overthinking. We observed that grandmasters often mention AZ's move in Phase 3 in their free-form comments but ultimately did not choose the move (which was not counted as 'correct'). We speculate that this may be because players are more familiar with existing strategies in their decision process, despite having learnt the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative analysis of the concepts and illustrative examples</head><p>In this section, we provide a qualitative analysis of some concepts with the associated puzzles (plotted using the chess python package <ref type="bibr" target="#b35">(Fiekas, 2023</ref>)), along with the grandmasters' analysis of the puzzles and the provided AZ suggestions. Any text or notation referring to the chess board or moves is written in this font. When giving direct quotes, we refrain from providing player's names to preserve anonymity. These examples will cover some cases of successful and some cases of unsuccessful learning. We discuss potential sources of differences between humans and AZ in §6.2.3.</p><p>Overall, the grandmasters appreciated the concepts, describing them as 'clever' (Figure <ref type="figure" target="#fig_4">8</ref>), 'very interesting' (Figure <ref type="figure" target="#fig_2">16</ref>), and 'very nice' (Figure <ref type="figure" target="#fig_12">18</ref>). Further, they found that the ideas often contained novel elements, commenting that the moves were 'something new' and even 'not natural' (Figures 16) and 10). Often, the grandmasters found the positions were very complex -making remarks such as that it was "very complicated -not easy to understand what to do". Even when seeing AZ's solutions, they remarked that it was a 'very nice idea which is hard to spot' (Figure <ref type="figure" target="#fig_0">22</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Concept example: positive knowledge transfer</head><p>Here, we explore a concept that possesses both strategic and prophylactic characteristics, involving plans that improve the player's piece placement while restricting the opponent's piece activity. This concept contains an additional element of exploiting tactical motifs and weaknesses, combining strategic and tactical play. We speculate that the concept is learnable to humans, as the grandmaster who trained on the concept puzzles improved their performance between Phases 1 and 3. In Phase 1, they were not able to identify AZ's plan in any concept puzzle (0/4), whereas they successfully identified the correct AZ plan in 2/4 of the concept puzzles in Phase 3.</p><p>Figures 8 and 10 show two of the puzzles provided to a grandmaster in Phase 1. In Figure <ref type="figure" target="#fig_4">8</ref>, AZ plays the move 9.Bg5; the idea is to provoke 9...h6 before retreating to the square e3, thereby inducing a structural weakness. Instead, the grandmaster chose 9.Be3, a natural move to develop the bishop. After seeing AZ's calculations, the grandmaster acknowledged the strength of provoking this weakness: "9.Be3 allows Black the clever option of playing [9...Nxf3 10.Qxf3] Nh5 [as provided by AZ, followed by] f5 ... but 9.Bg5 is clever as it provokes h6 after which f5 is not great and also [the pawn on] h6 serves as a hook for the pawn advance g4-g5. Blacks' plan of stopping c5 with b6 and playing h4-h5 is interesting too but with g4 anyways White manages to open lines so I would prefer White there."</p><p>The idea of playing provocative bishop moves to induce pawn weaknesses is not new and arises in human play. However, the interesting and potentially novel element here lies in the planned strategic queen sacrifice that emerges in one of the critical lines in the MCTS calculations. Consider one such critical continuation: 9.Bg5 h6 10.Be3 O-O 11.Nxd4 exd4 12.Qxd4 Ng4 13.hxg4! Queen sacrifices are among the most beautiful (and rare) tactical motifs in chess, as they go against the established chess principles -do not trade more valuable pieces (i.e., the queen) for less valuable pieces (a knight and bishop). However, here AZ's queen sacrifice is strategicafter sacrificing the queen, White continues developing their pieces. The line continues 13...Bxd4 14.Bxd4, as shown in the left of Figure <ref type="figure">9</ref>. Here, due to the pawn on h6, Black's king is vulnerable, and White is better. Therefore, it was critical to play 9.Bg5, rather than 9.Be3, to make the queen sacrifice feasible by creating this weakness. The puzzle on the right of Figure <ref type="figure">9</ref> arises in the same line if one opts for immediate 9.Be3 instead, failing to induce h6 first. Without the pawn on h6, White is lost, highlighting this is the critical positional element.</p><p>In general, some moves in the AZ's calculations are more important for the concept than others. In our convex optimisation formulation, we do not require the concept to be equally important for every chess position (see Equation <ref type="formula">5</ref>). This is illustrated in the previous example, where Bg5 is more important for the concept. The next puzzle from the same concept, also given in Phase 1, is shown in Figure <ref type="figure" target="#fig_5">10</ref>. This puzzle is of particular interest due to the unconventional plan of AZ -it increases space on both sides of the board, expanding with the pawn move b4 while the king is still on b1. To this end, AZ initiates this plan with the prophylactic move 21.Qd2, preventing the immediate 21...Ne4 by Black.</p><p>Unlike AZ, the grandmaster's suggestion in this puzzle was 21.g5, with the intention of following up with 21..Nh5 22.Bxh5 Rxh5 23.Rh3 Rdh8 24.Rdh1. Upon seeing the suggested AZ line starting with 21.Qd2, however, the grandmaster study participant remarked "21.Qd2 is a useful move as it stops Ne4 and protects f4 and can be better placed in case of b4 in the future. One curious line [given by AZ] is 21...Rh7 [22.h5 Rdh8] 23.Rh3 gxh5 24.g5 Ng4 White can just play 25.Rf1 and then focus on getting the b4 [pawn] break, which is not natural."</p><p>The unconventional plan of pushing the pawn to b4 with the king potentially exposed on b1 is particularly strong in this chess position (shown in Figure <ref type="figure" target="#fig_6">11</ref>) as it allows White to gain space and open up the chess position under unfavourable circumstances for Black, and claim an advantage. Therefore, the more general rules here are discarded based on concrete analysis. For example 21.Qd2 Rh7 22.h5 Rdh8 23.Rh3 gxh5 24.g5 Ng4 25.Rf1!? Nf8 26.b4!! Qxb4+ 27.Ka1 In this line, we see the dynamic play of AZ: the rooks from f1 and h3, switch over to the b-file to attack Black's king.</p><p>So far, the ideas in both positions were missed by the grandmaster. AZ's ideas require unconventional continuations that go against common human chess principles. Both of these observations hint at the existence of super-human knowledge (M -H). Figure <ref type="figure" target="#fig_7">12</ref> shows the puzzle from the same concept provided to the same grandmaster in Phase 3; Phase 3 tested whether the grandmasters had learnt the concept. As before, the puzzles underscore the concept's multifaceted attributes, encompassing its prophylactic characteristics and its integration of tactical and strategic elements.</p><p>In the puzzle (from Phase 3) shown on the left of Figure <ref type="figure" target="#fig_7">12</ref>, the grandmaster correctly found the move suggested by AZ: 24.Bb3, with the idea of forcing the Black rook on e8 into a more passive position (d8 to defend the pawn) prior to commencing activity on the other side of the board. The idea can be seen in a possible continuation: 24...Rd8 25.Ba4 Na5 26.Rdh1 Nc4? 27.Bh6 and White picks up the pawn on f7.</p><p>As in the previous puzzles, AZ uses both sides of the board to optimise piece activity.</p><p>In the puzzle, shown on the right in Figure <ref type="figure" target="#fig_7">12</ref>, the grandmaster again found the correct idea: 22...Bc6, with the idea of preventing the critical pawn advance 23.f4 because of: 23.f4 Qb7 24.Nc3 exf4 25.Rxf4, resulting in a weakened pawn structure, where White cannot recapture with 25.gxf4 as e4 is hanging. The move 22...Bc6 is both prophylactic and tactical; it prevents White from executing their plan to advance the kingside pawns while improving Black's position by activating the bishop and rook.</p><p>The overall improvement of the grandmaster on this concept suggests that they may have learnt AZ's concept, thereby expanding H with (M -H). Other examples of concept learning can be found in §8.1. Understanding AZ's concept using graph analysis and human-labelled concepts. Using a simple way to learn a graph (see §8.7) between concept vectors, we discover strong relationships between existing and discovered concepts to gain further insight into the concept meaning (shown in Figure <ref type="figure" target="#fig_8">13</ref>). Edge weight is influenced by (1) the strength of the relation between two concepts and ( <ref type="formula" target="#formula_3">2</ref>) the frequencies at which concepts co-occur. Below, we further discuss the two concepts with the largest edge weights.</p><p>Space. AZ's concept has a strong positive weight on the outgoing edge with the (White-side) space concept. In puzzles where White is to move <ref type="bibr">(Figures 8,</ref><ref type="bibr">10 and 12)</ref>, an important component of the plan is to increase space. In a similar vein, given that the idea is to increase space, which is 'easier'/more likely if the initial value is lower, AZ has a negatively weighted incoming edge with the concept space.</p><p>Recapture. We observe positive incoming and outgoing edge weights with the recapture concept. Recall that we have dynamic concepts, which refer to a sequence of states. As such, we postulate that this connection is because the plan may be to recapture/gain material in the subsequent chess positions, as in the puzzles in Figures <ref type="figure" target="#fig_4">8</ref> and <ref type="figure" target="#fig_7">12</ref> (left side).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Concept example: unsuccessful learning</head><p>This concept is related to gaining and playing with a space advantage with positional advantage despite less material. In this section, we provide an example of when a grandmaster found the correct move in Phase 1, but provided an incorrect (i.e., not AZ's choice) move in Phase 3. The puzzle on the left side in Figure <ref type="figure" target="#fig_9">14</ref> was provided to the grandmaster in Phase 1, and they correctly chose the same move as AZ: g4. However, while the grandmaster found the correct move, there were further finesses in AZ's calculations that the grandmaster missed during their time-constrained analysis:</p><p>White is a pawn down, [and AZ plays the move I suggested] g4 ... The computer plays with h4-h5 <ref type="bibr">[and]</ref> g4 and the queen on f2 or g2. White is playing on the kingside.<ref type="foot" target="#foot_4">foot_4</ref> Black has no active moves, is playing Rd8 or [R]f8, Kh8. Seems convincing to me. On g6 [AZ] goes [Q]g2 which is nice, it didn't occur to me. I was mainly focused on making h4 work for White. White is not in a hurry, will at some point play g5. Compensation, zero counterplay, and AZ is acting on these premises.</p><p>As remarked by the grandmaster, AZ is a pawn down, however, Black's pieces, particularly the Bishop on b7, are placed passively. Instead of prioritising regaining material, AZ focuses on improving the kingside position. While the grandmaster understood the general plan, they missed the intricate idea Qg2, and slowly advancing the g and h pawns.</p><p>The puzzle on the right in Figure <ref type="figure" target="#fig_9">14</ref> was provided to the grandmaster in Phase 3. Similar to the previous example, AZ focuses on space rather than recapturing material. It continues with the move 18.a4, which was rejected by the grandmaster on account of 18.Rc5, where Black tries to maintain the material advantage. However, AZ finds the rook on c5 misplaced and continues 18.a4 Rc5 19.a5 Be7 20.a6 where after Bf6? White has 21.d4</p><p>The idea behind the pawn advance is to weaken Black's pawn structure.</p><p>Overall, the evidence suggests that the grandmaster did not learn this concept. Here, AZ selects stronger moves due to its prioritisation of concepts (e.g., focusing on space and activity). Humans tend to prioritise these concepts differently (e.g., prioritising bringing the king to a safe location as soon as possible). This concept may be inherently difficult and require further examples to learn. Understanding AZ's concept using graph analysis and human-labelled concepts. Figure <ref type="figure" target="#fig_10">15</ref> shows the relationship between AZ's concept and high-quality human-labelled concepts. The graph is dense, and we elaborate on the two concepts with the largest edge weight.</p><p>Rook. AZ's concept has an incoming positive edge with the rook (activity) concept. In the puzzles in Figure <ref type="figure" target="#fig_9">14</ref>, we observe that white White has active rooks (the rooks on a1 and c5, in the left chess position) or plans to activate the rook (the rook on a1, in the chess position on the right).</p><p>King. AZ's concept has an outgoing negative edge with king safety. In the chess positions in Figure <ref type="figure" target="#fig_9">14</ref>, the king is less safe than usual. In the left chess position in Figure <ref type="figure" target="#fig_9">14</ref>, AZ pushes forward the pawn to g4 (and later the pawn to h4) around the king -thereby removing some of the king's defenders. In the right position, White does not castle to improve the king's safety but instead leaves the king in the centre.</p><p>Further examples of concept puzzles can be found in Section 8.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Differences between humans and AZ</head><p>In this section, we share a few observed differences between the grandmasters and AZ and speculate where they come from. While we do not have definitive answers, the discussion may lead to further research.</p><p>The qualitative examples suggest that AZ has different priors over the relevance of concepts in a chess position than humans. Human chess players formulate and adopt heuristic chess principles to inform their analysis, predisposing them to biases that influence which concepts they deem relevant for specific chess positions. An example is the three 'golden rules' of the opening: control the centre, develop your pieces, and bring your king to safety <ref type="bibr" target="#b49">(Hansen, 2021;</ref><ref type="bibr" target="#b16">Brunia and van Wijgerden, 2021;</ref><ref type="bibr" target="#b58">King, 2000)</ref>. Consequentially, in opening, humans may focus on moves that align with these guidelines. Instead, AZ is self-taught and does not seem to have the same priors over chess concepts as humans. We believe this lack of prior allows AZ to be more flexible -it can apply concepts to various different chess positions and change plans quickly. In essence, AZ formulates its own priors over the relevance of chess concepts for a given chess position. Examples of this behavior are that AZ plays over the entire board, as opposed to focusing on a specific side (see, e.g., <ref type="bibr">Figures 16,</ref><ref type="bibr">17,</ref><ref type="bibr">12,</ref><ref type="bibr">and 19)</ref>; places less importance on the material value of pieces, and prioritises space and piece activity (see, e.g., Figures <ref type="figure" target="#fig_9">9 or 14</ref>). This may result in the super-human application of concepts, and new concepts.</p><p>One may ask where do the differences between AZ's and humans' play come from? We conjecture that they may arise from differences in objectives and capabilities. AZ learnt to play chess against itself. As such, AZ assumes optimal play and information symmetry. <ref type="foot" target="#foot_5">6</ref> On the other hand, humans play chess against other humans and, therefore, may assume information asymmetry and imperfect play. This leads to a difference in behaviour: while AZ focuses on finding the best move, human chess players often make practical choices. Humans' choices do not always increase the expected outcome against an optimal opponent (their choices may even slightly decrease the expected outcome) but may increase their odds against another human. For example, in drawn chess positions, humans may try to complicate the chess position or opt for continuations where the best moves are less clear-cut in hopes that their opponent makes a mistake (see Figure <ref type="figure" target="#fig_16">23</ref>). However, AZ will try to find the optimal plan, disregarding aspects such as complexity. Therefore, AZ's play may be fundamentally different and better reflect conceptually relevant plans in a chess position.</p><p>Another difference between humans and AZ's play is the role of time. Humans have limited energy<ref type="foot" target="#foot_6">foot_6</ref> and time allocation for a game. In chess positions where humans are better, they often simplify the chess position to try to secure the win as quickly as possible and minimise risk (see, e.g., the grandmasters' chosen moves in Figures <ref type="figure" target="#fig_0">22</ref> and <ref type="figure" target="#fig_15">21</ref>). However, AZ does not care about how quickly the game finishes. The training loss function does not have a penalty term to encourage winning as quickly as possible. As a result, it has a different treatment of time. This results in sometimes choosing slow strategic wins (as can be seen in the chess positions in Figure <ref type="figure" target="#fig_15">21</ref>). While the lack of time constraint may lead to super-human concepts, it also may result in complex concepts that are difficult for humans to learn.</p><p>Naturally, AZ and humans have different computational capacities. As a result, AZ can opt for more computationally expensive moves and, therefore, defend complicated chess positions where humans might be more hesitant. In terms of playing style, AZ will often opt for what it believes is the optimal move, while humans have a more limited computing budget and may opt for a safer move. In chess, safer is used to describe continuation where there is less probability of making an incorrect move. Sometimes, humans may even play a slightly suboptimal move to minimise the risk. We see this phenomenon in Figures <ref type="figure" target="#fig_0">22</ref> and <ref type="figure" target="#fig_16">23</ref>. While computational capacity cannot be transferred, it may still lead to super-human concepts, as AZ can find new ideas that can still be taught to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our research represents a first step toward understanding the potential of human learning from Artificial Intelligence (AI). In this work, we focused on AlphaZero (AZ) -an AI model that learned to play chess at a super-human level through self-play without prior knowledge or human bias. Through spectral analysis, we show that AZ's games encode features that are not present in human games, providing evidence for the existence of super-human knowledge. To extract knowledge from AZ's representational space, we developed a framework to uncover new chess concepts in an unsupervised fashion. We discover unsupervised concepts by leveraging AZ's training history to curate a set of complex chess positions. We ensured each concept was informative, by verifying that the concept can be taught to another AI agent, and novel, through a spectral analysis of human and AZ games. Communicating novel concepts requires a common language between humans and AI. We bypass the need for this language by creating puzzles for each concept.</p><p>We collaborated with four world-top grandmasters to (1) validate the human capacity to comprehend and apply these concepts by studying AZ's concept prototypes and (2) improve our understanding of the differences between AZ's and humans' chess representation space. All four grandmasters improved their performance after learning concepts compared to baseline performance. We speculate that the differences between AZ and humans may stem from (1) prior biases over concepts, including their perceived applicability, importance, and how they can be combined with other concepts. For example, AZ shows a reduced emphasis on factors such as material value and is more agile in switching between playing on different sides of the board. (2) a difference in the motivation and objectives when playing chess; AZ is trained to accurately evaluate the current chess position, assuming optimal play. There are several aspects of the work that could be further explored. In our work, we found a subset of all possible concepts. For example, we limited our investigation to linear sparse concept vectors. However, other concepts may be discovered in the form of non-linear vectors. Additionally, the current work focuses on finding a single concept to explain a plan. However, a plan may contain multiple concepts. As such, an interesting aspect to further explore is how these concepts relate to each other and influence the plan.</p><p>Further work could also explore the optimal conditions for humans to learn novel concepts. We allotted a fixed time budget for grandmasters to assimilate the concepts. However, it is plausible that an unlimited time budget could yield more profound and more intricate insights. In our research, we provided grandmasters with part of AZ's Monte Carlo Tree Search (MCTS), in which the rollouts are motivated by the concept, as an explanation for the concept. We used this approach to keep the explanations as familiar and simple as possible. Nonetheless, it would be interesting to augment this phase with an interactive component: e.g., for each puzzle, humans can actively engage with AZ by playing moves and asking AZ what its response is. This interactive element would allow humans to investigate counterfactual scenarios, allowing for a deeper understanding why AZ did not select their solutions or approaches. 13.Qc2 Rh6 14.0-0-0 b5!?.</p><p>The resulting chess position is shown in the right chess position in Figure <ref type="figure" target="#fig_11">17</ref>. Both continuations are unorthodox; conventional human-designed chess principles emphasise completing piece development, securing the king's safety and maintaining the bishop pair over trading it for a knight, as outlined in <ref type="bibr" target="#b58">King (2000)</ref>. However, AZ deviates from these principles favouring a continuation that prioritises a strong control of the centre, space, and piece activity. Another puzzle from the same concept is shown in Figure <ref type="figure" target="#fig_12">18</ref>, given in Phase 3. Here, the grandmaster found the best continuation according to AZ: 10.Ndf4 threatening d5. The ideas are 10...a6 11.Qa4 10...Bd7 11.Bc4 gaining control over the square e6 10...d5? 11.Qb3 and the pawn on d5 is lost 10...Bf6 Black's best option 11.Bxf6 exf6 (12.d5? a6 13.Qa4 then Black has the intermediate move 13...Re4!) 12.Ng1 a6 13.Ba4 Bd7 14.Nge2</p><p>The knight manoeuvres 10.Ndf4 and 12.Ng1 are against the common rules which advocate for finishing piece development and bringing the king to safety, above further improving a developed piece <ref type="bibr" target="#b58">(King, 2000;</ref><ref type="bibr" target="#b16">Brunia and van Wijgerden, 2021;</ref><ref type="bibr" target="#b49">Hansen, 2021)</ref>. As in the previous puzzle, AZ prioritises controlling the centre and piece activity.</p><p>The grandmaster missed the idea 12.Ng1, although did appreciate it remarking that "Ng1 [is] quite nice actually, [knight] on h3 is gone, and then we probably go for h4 at some point."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Informative puzzles</head><p>In some puzzles, we observe informative manoeuvres from AZ. We provide a few examples here. In the chess position in Figure <ref type="figure" target="#fig_13">19</ref>, AZ plays Qc1 with the idea of manoeuvring it to c4. Most human chess players would find this idea unconventional, as White's pieces seem to be active on the kingside. However, there is no way to break through Black's position. AZ's idea is the only way to maintain an advantage. The plan is to re-position the pieces to the queenside, with ideas like Re1, Ba4 and e5.</p><p>When analysing this position (and only spending a fixed amount of time), the grandmaster misted the idea and opted for Rxh5, which was the only way for White to equalise according to the grandmaster. We speculate that the difference between AZ and humans is because AZ is more flexible in changing its plan. In this position, humans are likely primed to continue playing on the kingside. The puzzles in Figure <ref type="figure" target="#fig_14">20</ref> correspond to the same concept. In both positions, AZ uses tactics to obtain a positional advantage by maintaining a space advantage. In the left puzzle, the best move is 21.Nc5, which stops Black from advancing their c5 pawn to control the center. The tactic behind the idea is 21.Nc5 dxc5 22.Bxf6 Bxf6 23.Rxd7.</p><p>In the puzzle on the right of Figure <ref type="figure" target="#fig_14">20</ref>, Black plays 18.Rad1 which is prophylactic against 18...b5 as White has tactic 19.c5 dxc5 20.Nc6. Both of these continuations were found by the grandmaster, who appreciated the importance of Nc5. The grandmaster explained that they took a long time to analyse this position as they found it complicated. While recognising Black's threat of c5 (followed by Bc6 or Nc6), they first explored several other options, including moves such as a3, c5, Bf2 or Na4. However, after exploring other moves, they found Nc5. The grandmaster commented that the idea was very strong and 'by far the best move'.</p><p>In this puzzle, the grandmaster opted for the tactical move Bxh7 but also considered quieter moves such as Re3, Be4 or Be1. When analysing this position, they commented "This is tricky, if White decides to protect the pawn, it's clearly better due to the weakness in Black's structure, but somehow getting addicted to more forced attacking lines. Bxh7 is hard to figure out, maybe Kxh7 [followed by] Qh5-Qf7 then Rd3, Bxg2 ... [I] didn't calculate until clear much better position, but thought even with some play, h-file the long problem, maybe certain chances [to win]." However, this sequence ends in a draw, and AZ instead opts for f3, maintaining the advantage for White. When reading AZ's analysis, the grandmaster commented: "Wow, it's a completely positional play. Well, my decision to [sacrifice] is too emotional. To be honest, this choice f3 ... [makes] sense as Black does not have any breakthrough idea, so if White successful [in] controlling both c5 and e5 square then its clearly much better. [My conclusion is] technically strong but again within [the] human perspective."</p><p>Here, we see a difference in style between humans and AZ. AZ opts for a slower, longer-dominance play in chess positions where grandmasters tend to consider more forcing sequences.</p><p>Human vs AI Play: AZ opts for less forced lines than humans </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficult or non-instructive puzzles</head><p>There may exist concepts that are intrinsically hard to understand and learn for human chess players due to differences in ways of abstract thinking, overall capabilities, and their computational Figure <ref type="figure" target="#fig_0">22</ref>: AZ takes on more risk than humans due to computational capacity. White is to move.</p><p>AZ's calculations: 31.Qa1 f4 (31...Qe7 32.Re8 Qf6 33.Qxf6 Rxf6 34.Rd8 Ne5 35.Ne8) (31...h4 32.Re8 f4 33.exf4 gxf4 34.Bxe4 Nxe4 35.Rxe4 Qf5 36.Qe1 Ne5 37.Qe2 hxg3 38.hxg3 fxg3 39.fxg3) 32.exf4 gxf4 33.Ne6 Nxe6 34.Bh3 Qb5 35.dxe6 Rg7 36.Ra5 White is slightly better budgets. The example in Figure <ref type="figure" target="#fig_0">22</ref> highlights that humans and AI have different computational capacity, allowing AZ to make moves that appear risky to humans.</p><p>In the puzzle in Figure <ref type="figure" target="#fig_0">22</ref>, AZ plays Qa1 to activate the queen. This move requires calculating carefully to ensure that Black has no counterplay due to an attack on the kingside. As such, humans may perceive this move as risky, and it was not chosen by the grandmaster. When seeing AZ's calculations, they remarked "I would be really worrie <ref type="bibr">[d]</ref> to keep the queens on the board because of the threat with f4 but AZ has a tactical solution. 33.Ne6 Nxe6 [34.]Bh3 is a very nice idea which is quite hard to spot. Black should probably stay still and try to hold with 31...h4 <ref type="bibr">[32]</ref>.Re8 Re7."</p><p>Here, we see that humans are more risk-averse than AZ. This is logical, given that AZ has a much larger computational capacity and can calculate more/deeper than humans can to more accurately assess the chess position (and thereby take on less risk).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences in motivation in human and AI play</head><p>The next example shows that AZ and humans play chess with different motivations. AZ forces the draw with g4. Upon seeing AZ's calculations, the grandmaster commented: "... this is a very clear and important theme to understand. So, g4, the move it proposes, in a practical sense it's a very big move, because you see, in such situations, the engine already knows the final result. For engine it doesn't matter which move it plays because it calculated it's a draw, but g4 is basically forcing it. After g4 Black has no winning chances, but otherwise I have a feeling that after Black plays let's say Qd2, it's not ... easy practically for White to make the draw. For an engine it's ok, but practically no one would play it because g4 is basically offering a draw -and with other moves Black is running zero risk, yet has practical chances to win the game if White makes a mistake. An engine doesn't understand the concept of practical play -while this is a draw, it's not an easy draw for White. g4 is one of many moves leading to a draw, but in a practical sense the worst one as it gives Black zero chances to win. So that is my understanding, that it's not the objective best move. Practically definitely a wrong move."</p><p>This underscores a fundamental difference between AZ's playing style and human's playing style. AZ was trained to obtain the expected outcome without an explicit term in the loss function, encouraging it to win. The incentive to find the best move comes from the exploration and move selection criteria in MCTS. Further, AZ assumes an equally strong opponent. For AZ, there is no difference between different equalising moves, even if one move requires a much more precise sequence of moves to equalise. In contrast, humans assume that their opponent may make suboptimal moves. As human chess players play competitively (i.e., their goal is to maximise the outcome), they will try to leverage these chances. This example highlights how the difference in objectives and assumptions may lead to different behaviour of AZ and humans when playing chess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shortcomings of method for generating prototypes</head><p>In this puzzle, AZ chooses Rd1 whereas the grandmaster wanted to play Re1 or Ke1. When seeing AZ's calculations, the grandmaster commented: "Drawing position? At first trying to find winning moves for White, but really didn't see any plan to make improvements. In the meantime, considering the possibility for Black to push h pawn to h3, maybe tiny chances, its better to plan Ke1-Qf1-Qf3 at the beginning, or moving the rook to e1 with the idea Re7, forcing ... Ra1 check then White rook retreat to e1, ... [draw by] repetition." This puzzle can be seen as a shortcoming of our method for finding prototypes. We only filter positions based on the criteria described in §8.8; however, this position does not fall under one of our categories. This puzzle is not informative for humans as there are many other viable options. As such, it is more difficult to understand the concept from the sequence of moves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Background: how humans and AZ play chess</head><p>In this section, we provide further intuition on how humans play chess, and how this relates to AZ's system. This section provides context as to why concepts should explain the policy value network and MCTS to provide a holistic view of chess. 2. Based on step 1, a chess player will find a couple candidate moves -actions they could play in the chess position.</p><p>3. For each move, they may calculate a likely continuation -i.e., what is the likely sequence of moves to follow?</p><p>This process loops until the player has considered all candidate moves, calculated the relevant move sequences, and determined the optimal trajectory.<ref type="foot" target="#foot_7">foot_7</ref> To play well, chess players must understand the important features of a chess position and calculate move sequences to understand the correct evaluation of the chess position. However, there are several different types of chess positions (e.g., endgames or attacking chess positions) where principles alone are insufficient to determine the optimal continuation, and calculation is necessary.</p><p>AZ uses a similar approach. In a given chess position, AZ extracts features using the layers in the policy-value network and outputs a policy and value estimate. The policy weighs the different possible actions, and the moves with the most probability mass can be interpreted as the candidate moves. Next, the policy is passed on as an input to MCTS, a search algorithm that calculates the optimal move. By drawing parallels between AZ's system and how humans play chess, we highlight </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Further details: convex optimisation formulation for concepts</head><p>This section provides further details on our convex optimisation framework to find concepts. Subsection 8.4.1 describes how we set the dynamic concept hyperparameters. §8.4.3 explains how the convex optimisation formulations were implemented for the different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.1">Dynamic concept hyperparameters</head><p>As AZ learnt to play chess through self-play, the latent representations alternates between the player's and the opponent's perspective within a rollout. For concepts, we may want to find a concept that influences a single player or both players. Therefore, we consider two different ways of using rollouts. For a rollout {z t } T t=0 , we use every other latent representation, i.e., {z 2t } floor(T /2) t=0</p><p>, to find concepts for a single player (i.e., for the player to move, or their opponent). These concepts are referred to as 'single'. We use every latent representation to find concepts for both players, i.e. {z t } T t=0 . These concepts will be referred to as 'both'. For the rollout depth used in our dynamic concept formulations, we consider T = 5 and T = 10. For the subpar variations, we required AZ to estimate a minimum value difference of 0.20 and/or a visit count difference of 10% (of the most visited move).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.2">Datasets</head><p>We use labelled and unlabelled datasets to construct concept constraints for the convex optimisation formulation. We use a different convex optimisation formulation for each concept we want to discover. Therefore, for each concept, we need a set of chess positions X + that contains a concept. We leverage educational resources designed to teach humans chess, which contains themed chess puzzles: positions designed to encapsulate single important concepts and test the degree to which a chess player can deploy them in realistic situations. We go beyond chess puzzles, delving into chess positions arising from different openings and searching for AZ-specific concepts to find new ones.</p><p>Factors in datasets. The datasets we use vary in the following ways:</p><p>• Concept type e.g., the concepts can be strategic or tactical (see Wikipedia contributors (2023b) and Wikipedia contributors (2023c) for a further explanation on strategy and tactics), or correspond to different periods of the game (opening/middle game/endgame).</p><p>• Degree of human knowledge some datasets contain human games while others contain AZ's games. The degree of human knowledge (or style of play) may vary across the datasets.</p><p>• Complexity some concepts are elementary (i.e., can be learned by beginner-level chess players), whereas others are highly complex (i.e., can only be understood by top-level grandmasters).</p><p>Below, we briefly describe the different data sets used 1. Piece dataset We construct a labelled dataset that contains paired chess positions that either contain or do not contain a chess piece. These simple concepts indicate the presence of a piece -queen, rook, bishop, knight, or pawn. We exclude the concept of 'king' as this piece is always present in a chess game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Stockfish dataset</head><p>We construct a labelled dataset for each concept in the Stockfish engine <ref type="bibr">(Stockfish Community, 2018)</ref>. Each dataset contains two sets of chess positions -with and without the concept. Humans use Stockfish concepts in chess position evaluation (such as piece placement, open files, etc.)</p><p>3. STS puzzles dataset This is a labelled dataset containing 15 different categories (see <ref type="bibr" target="#b22">Corbit et al. (2014)</ref> for further details). These puzzles capture different types of strategic themes.</p><p>4. Chess openings (e.g., concepts in Grünfeld vs. Najdorf vs. Queens Gambit) Openings (i.e., the first couple of moves) determine the pawn structure that acts as the backbone of a chess position -it determines crucial aspects of the game, such as optimal piece placement, square weaknesses and strengths, and more generally, plans. We use the Encyclopedia of Chess Openings (ECO) to create a set of labelled chess opening positions (LiChess, 2023). For each chess position, we use AZ's MCTS rollouts to construct sequences of the opening moves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AZ self-play games</head><p>We construct an unlabelled dataset. We sample 30, 000 chess positions from AZ's games. To ensure that the chess positions we analyze contain complex concepts, we only select chess positions where two versions of AZ at different points in training select a different move. These versions differ by 75 Elo points.</p><p>Table <ref type="table">5</ref> briefly summarises the different datasets.</p><p>each concept. Then, using the formulation for static concepts, we found the concept vector using the following formulation</p><formula xml:id="formula_14">min ||v c,l || 1 (13) such that v c,l • z + i,l ≥ v c,l • z - j,l ∀i : x i ∈ X + , j : x j ∈ X -.<label>(14)</label></formula><p>Strategic. We use the strategic test suite to extract strategic concepts <ref type="bibr" target="#b22">(Corbit et al., 2014)</ref>. In this dataset, there are 15 different concepts. We omit the 12th concept due to irregular data formatting.</p><p>The remaining concepts are undermine, open files, knight outposts, square vacancy, bishop vs. knight, recapture, offer of simplification, fgh-pawn, abc-pawn, simplification, king activity, pawn push center, 7th rank, avoiding an exchange (see <ref type="bibr">Corbit et al., 2014, for further details)</ref>.</p><p>Each concept has a set of 100 chess positions, X, and the solution (move) requires applying a strategic concept in each chess position. In our analysis, we run MCTS on the chess position and store the search statistics. We store the optimal trajectory for each chess position x i , denoted as X + i,≤T , where T is the maximum rollout depth. Similarly we select a subpar rollout X - i,≤T . To find the subpar rollout, we find a rollout in the MCTS tree with the most visits where (1) the estimated difference in value is at least 0.2, and (2) the visit difference is at least 10%. As in the main text, for X + i,≤T and X - i,≤T , we find the corresponding latent representations in layer l: Z + i,≤T and Z - i,≤T , respectively. Then, we can find the concept vector using the dynamic concept formulations (see</p><formula xml:id="formula_15">§4.1.2) as follows min ||v c,l || 1 (15) such that v c,l • z + i,t ≥ v c,l • z - i,t ∀t ≤ T, i : x i ∈ X.<label>(16)</label></formula><p>In our analysis, we use a maximum depth of T = 5.</p><p>Openings. For the openings, we focus on the English, Dutch, Scandinavian, Najdorf, French, Tarrasch, Winawer, Ruy Lopez, Grünfeld, King's Indian, Queen's Gambit Declined and Queen's Gambit Accepted. We consider a subset of all openings due to computational costs. For each opening, we use the encyclopedia of chess openings to find relevant starting chess positions to construct X + (see the encyclopedia of chess openings). For each chess position, we ran MCTS to obtain the search statistics and used the formula in Equation 5 to find a concept for each opening. Further, we create a concept set X + for each ECO index belonging to one of the aforementioned openings.</p><p>AZ Games. We simulated 1, 308 games. Conditional on hardware, AZ's play is deterministic (after training). To create diverse games, we sample different starting chess positions. We use the ECO to find starting chess positions (see the encyclopedia of chess openings) and we simulate games from these initial chess positions. For each chess position, we ran MCTS to obtain the search statistics.</p><p>We leverage AZ's training history to find interesting chess positions. We select a version of AZ that is 75 Elo points weaker than the final model. To construct X + , we run through each game and select chess positions where the two AZ versions choose a different move. Using this approach, we constructed a dataset with 3, 974 chess positions and used the formulation provided in Equation <ref type="formula">5</ref>to find concept vectors. Other Implementation Details. We solve the convex optimisation problem using a standard solver in the package cvxpy <ref type="bibr" target="#b29">(Diamond and Boyd, 2016;</ref><ref type="bibr" target="#b2">Agrawal et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Beta hyperparameter tuning</head><p>In this section, we provide the validation values for the concept amplification experiments in §5.1.3.</p><p>For the values in Table <ref type="table" target="#tab_5">6</ref>, we randomly chose 2 concept sets from the STS dataset and estimated the amplification results for different values of β. Overall, we observe that the results are not very sensitive to the value of β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Teachability implementation</head><p>In this section, we provide further details on the implementation of teachability. We assume that we have a concept vector v c,l that was found in layer l. To construct prototypes, we sample 30, 000 chess positions from AZ games. For each concept, we find chess positions for which v c,l • z i,l is in the top 2.5%, and store these as prototypes {x 1,l , . . . x n,l }. For dynamic concepts, we found prototypes using MCTS statistics. For each chess position, we ran MCTS. Next, we found the optimal and subpar line (similar to the convex optimisation formulation constraint). For a prototype x i , we required that v c,l • z + i,t,l ≥ v c,l z - i,t,l , ∀t.</p><p>We use AZ as the teacher network. For the student network, we want to find an agent that does not know the concept but does understand chess. As chess is a complex game, we cannot train an agent from scratch (using only the curriculum). Instead, we take a training checkpoint of AZ and estimate its knowledge of the concept using</p><formula xml:id="formula_16">T = xi∈X 1[argmax(π s (x i )), argmax(π t (x i ))],<label>(17)</label></formula><p>where π s () is the student policy and π t () is the teacher policy. This measures how often the teacher and student agree on the best move. We select the student as the latest checkpoint for which the top-1 policy overlap is less than 0.2. We use the prototypes as a curriculum. We train the student network by minimizing the KL divergence between the policies, xi∈Xtrain KL[π t (x i ), π s (x i )]. We use the Adam optimiser <ref type="bibr" target="#b59">(Kingma and Ba, 2014)</ref> with learning rate 1 × 10 -4 . As we have several concepts, we train each student for 5 epochs, as we find that this is sufficiently indicative of performance if we train for longer.</p><p>We benchmark the performance by comparing it to a student network trained on a random concept and evaluated on (1) the concept data and (2) the random data. A random concept is a vector with the same shape as the latent dimension and sampled from a standard normal distribution.</p><p>Graph Verification. To verify the graph, we run an experiment to test whether two concepts with an edge contain related knowledge. If a model learns a concept c, this should improve the model's performance on another related concept c e more than on an unrelated concept c n . For a concept c, let C e denote the set of concepts with an edge and C n denote the set of concepts with no edge in the graph. Following the teachability procedure in §4.2.1, we train a student model using prototypes of concept c. Next, using Equation <ref type="formula">6</ref>, we evaluated student's performance on:</p><p>• concepts with an edge C e ; we denote the performance by T c,ce</p><p>• concepts without an edge (to v c,l ) C n ; we denote the performance by T c,cn .</p><p>If the graph correctly captures the relationships between concepts, then we expect that a model trained on c performs better on C e than C n , i.e. T c,ce &gt; T c,cn . However, we must consider that the concepts in C e may be inherently easier to learn than those in C n . To account for this, we train a model on a random set of data (as in §4.2.1), which we use as a benchmark. We estimate the performance of this model on C e and C n , which we denote by T r,ce and T r,cn , respectively.</p><p>If our graph accurately captures the underlying relationships, we expect that T c,ce -T r,ce &gt; T c,cn -T r,cn . Figure <ref type="figure" target="#fig_20">28</ref> shows training curves for two concepts each of which with 5 related concepts and 5 unrelated concepts. We find that the performance on related concepts is significantly better than unrelated concepts at a 5% significance level. This suggests that the graph structure may accurately capture the relationship between concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8">Human experiments</head><p>Recruitment. We recruited four chess players based on their Elo rating. All the participants hold the grandmaster title; one of our participants is rated 2600-2700, and three are rated 2700-2800.</p><p>Experiment Instructions. Each grandmaster was asked to spend two hours on Phase 1, one hour on Phase 2 and two hours on Phase 3. We ask the grandmasters to provide (1) the move they would play or their ranked candidate moves and (2) a thought record -the idea is to capture any thoughts about the chess position. The grandmasters were sent the chess positions to solve at home, in their own time. We explained that the chess positions could vary in nature. The chess positions could be better, equal or worse for the player to move. Similarly, the continuation may require calculation or finding a general plan.</p><p>Evaluation. We evaluated how often the grandmasters find the move selected by AZ. Note that if grandmasters made the right move but incorrect reasoning appeared in their free-form comments, we counted this as an incorrect answer.</p><p>Prototype Filtering. To ensure the quality of the prototype selection, we filter them according to the following criteria:</p><p>• Quality of the value estimate. We ensure that the AZ value estimate is close to the correct assessment of the prototype by running self-play and computing the expected score. If the expected score and the value estimate are in concordance, the prototype chess position is kept, otherwise, it is discarded from consideration for the human study.</p><p>• Chess position complexity. For the concepts to be sufficiently complex to be of interest to the top grandmasters, we use prototypes where the policies of the 512K step checkpoint and fully trained models disagree on their top move. The 512K checkpoint model is 75 Elo points weaker than the final model, and therefore, if the policies differ, AZ learned the continuation during a late stage of training when it was already strong.</p><p>• Solution complexity. We manually remove trivial chess positions where the solution is theoretically known (e.g., present as an entry in pre-computed tablebases such as the Syzygy tablebase (Bojun Guo, 2023)). Tablebases are sets of chess positions where the ground truth evaluation (outcome with ideal play) is known.</p><p>• Reliability. We reject chess positions where AZ's limited compute budget may lead to an unreliable chess position evaluation (i.e., where we observe abrupt changes in the predicted outcome). Therefore, we require that the evaluation stays approximately consistent (i.e., the predicted outcome (win/loss/draw) does not change) throughout the provided lines.</p><p>We did not filter based on the difference in the value or the policy probability mass of the optimal move compared to other moves. The reasons are that (1) AZ's value estimate is noisy, and (2) either filter could remove potentially interesting chess positions. For example, requiring a small entropy and large value estimate difference (between moves) would result in predominantly tactical chess positions, thereby omitting interesting strategic puzzles.</p><p>AZ's calculations. In the second stage of the human experiment, we provide part of the MCTS statistics. We ran MCTS without a depth limit for a maximum of 10, 000 simulations. We pruned the MCTS tree to avoid providing too many lines, or lines that were insufficiently explored. We provided the main line (most frequently visited), second and third moves, ranked according to visits. We did this for depth t ≤ 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a concept prototype. Most chess players would opt for Rxh5, however, AZ plays Qc1, with the idea of regrouping the pieces to the queenside. Further details can be found in §8.1.</figDesc><graphic coords="4,200.44,185.89,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Teachability: AZ Concepts. The y-axis shows how often the student and teacher select the same move (normalised version of Equation 6), and the x-axis shows the training time step. The dark dotted lines show the level of a training checkpoint at which AZ obtains the same level on the concept set as our student. Each plot is a different concept found in layer 19 (top) and in layer 23 (bottom).</figDesc><graphic coords="12,81.64,270.68,432.01,115.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sample efficiency of convex optimisation framework, averaged across 10 seeds, for the bottleneck layer (19), value head (20) and policy head (23).</figDesc><graphic coords="17,86.95,138.68,138.24,88.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance improvement in solving puzzles using STS dataset across different α values (xaxis). Layers 18 (left) and 19 (centre) are before value/policy head split, and the policy head (layer 23) (right). Each line indicates a set of concepts with different quality (measured by test accuracy as done in §5.1.1). Higher quality (high threshold, orange line) achieves the highest improvement.</figDesc><graphic coords="18,86.29,163.14,129.60,100.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Puzzle of shown in Phase 1. White is to play.</figDesc><graphic coords="21,200.44,126.17,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Second puzzle shown in Phase 1. White is to move.</figDesc><graphic coords="22,200.44,185.89,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Expanding on the critical line in the puzzle shown in Figure 10. White has just played b4.</figDesc><graphic coords="23,200.44,136.19,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: In the left puzzle, White is to move. In the right puzzle, Black is to move.</figDesc><graphic coords="23,93.94,474.43,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Graph of AZ's concept in Figures 8, 10 9 and 12 between AZ's (white), strategic (green) and Stockfish concepts (purple). The information in the parentheses means the layer in which the concept is found, and w = white, b=black, eg= endgame, mg = middlegame, ph=phased. The edge color denotes the edge weight.</figDesc><graphic coords="24,166.38,471.38,259.20,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: In both puzzles, White is to move.</figDesc><graphic coords="26,95.60,126.17,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Graph of AZ Concept in Figure 14 between AZ's (white), strategic (green) and Stockfish concepts (purple). The information in the parentheses is the layer number in which the concept is found. w = white, b=black, eg= endgame, mg = middlegame, ph=phased. The edge color denotes the edge weight.</figDesc><graphic coords="26,166.38,479.67,259.20,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Digging Deeper into the Concept in Figure 16. In both positions, White is to play.</figDesc><graphic coords="39,93.94,126.17,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Concept Puzzle 2: White is to play.</figDesc><graphic coords="39,198.78,479.58,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Queen Manoeuvre. White is to move.</figDesc><graphic coords="40,200.44,367.66,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Positional Tactics. White is to move in both positions.</figDesc><graphic coords="41,101.58,197.81,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: White is to move.</figDesc><graphic coords="42,189.64,345.73,215.96,215.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: AZ simplifies the chess position for the draw whereas humans would continue to try to win. Black is to move.</figDesc><graphic coords="44,189.64,136.17,215.96,215.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: White is to move</figDesc><graphic coords="45,189.64,250.64,215.96,215.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Simplified Summary of How Humans Play Chess</figDesc><graphic coords="47,112.38,192.33,367.18,171.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Simplified Summary of How AZ Plays Chess</figDesc><graphic coords="48,112.38,126.18,367.18,171.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Teachability score on related and unrelated concepts</figDesc><graphic coords="54,93.94,391.12,194.40,145.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Rank of latent representation of Human Games and AZ's Games</figDesc><table><row><cell>Input Layer 19 Layer 20 Layer 21 Layer 23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Datasets Summaries: from concepts more known to humans (top rows) to AZ (bottom rows). S denotes strategic and T denotes tactical.</figDesc><table><row><cell>Name</cell><cell cols="2">Concept Type</cell><cell>Type of</cell><cell>Complexity</cell></row><row><cell></cell><cell cols="3">S vs. T Game Phase Knowledge</cell><cell></cell></row><row><cell>Piece</cell><cell>N/A</cell><cell>All</cell><cell>Human</cell><cell>Low</cell></row><row><cell>Stockfish</cell><cell>Both</cell><cell>All</cell><cell>Human</cell><cell>Varies</cell></row><row><cell>Strategic Test Suite (STS)</cell><cell>S</cell><cell>Middle/End</cell><cell>Human</cell><cell>Medium</cell></row><row><cell>Opening</cell><cell>S</cell><cell>Begin</cell><cell>Human/AZ</cell><cell>Varies</cell></row><row><cell>AlphaZero (games)</cell><cell>Both</cell><cell>All</cell><cell>AZ</cell><cell>High</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation: % the concept constraints hold on test data. The standard error is shown in parentheses.</figDesc><table><row><cell>Concept</cell><cell>Layer 19</cell><cell>Layer 20</cell><cell>Layer 21</cell><cell>Layer 23</cell></row><row><cell>Pieces</cell><cell cols="4">0.99 (0.00) 0.98 (0.00) 0.95 (0.00) 0.99 (0.00)</cell></row><row><cell>Stockfish</cell><cell cols="4">0.76 (0.03) 0.75 (0.03) 0.73 (0.02) 0.77 (0.02)</cell></row><row><cell>STS</cell><cell cols="4">0.92 (0.09) 0.92 (0.09) 0.92 (0.06) 0.90 (0.06)</cell></row><row><cell>Opening (general)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Improvements in grandmasters' performance. The percentage scores are the % of puzzles that the grandmaster solved correctly (according to AZ's solution). # Puzzles is the number of puzzles shown to the grandmaster in total.</figDesc><table><row><cell>Grandmaster</cell><cell cols="3">Percentage Score Phase 1 Phase 3 Improvement</cell><cell># Puzzles</cell></row><row><cell>1</cell><cell>0</cell><cell>42</cell><cell>+42</cell><cell>36</cell></row><row><cell>2</cell><cell>33</cell><cell>58</cell><cell>+25</cell><cell>36</cell></row><row><cell>3</cell><cell>25</cell><cell>42</cell><cell>+16</cell><cell>36</cell></row><row><cell>4</cell><cell>38</cell><cell>44</cell><cell>+6</cell><cell>48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter selection for β Layer 1858.42 58.58 58.50 58.42 58.17 58.17 57.42 56.75 55.00  Layer 19 60.17 60.42 60.42 60.17 60.08 60.17 59.75 58.00 55.83  Layer 23 58.83 59.42 59.33 58.83 58.58 56.25 53.92 47.75 40.33    </figDesc><table><row><cell>0.05</cell><cell>0.01 0.025</cell><cell>0.05</cell><cell>0.1</cell><cell>0.25</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This difference is not due to variance, as the human games have a larger span in input space.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We show how to handle non-binary concepts in §8.4.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A larger inner product corresponds to a higher cosine similarly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For reference, the expected score of a player rated 75 Elo points higher is 0.68, where 1 point is given for a win, 0.5 for a draw, and 0 for a loss. 75 Elo points is a large Elo difference, particularly at AZ's playing strength.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For readers -the queenside refers to the left side marked a-d and the kingside refers to the right side of the board marked e-h.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>With information symmetry, we mean that Black and White have the same general knowledge and perform the same calculations in a given chess position.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Classical time control (see match time controls in FIDE, 2019) chess games take hours.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>This is a simplified model -in practice, other factors such as time are important.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the four grandmasters who participated in our study -<rs type="person">Vladimir Kramnik</rs>, <rs type="person">Dommaraju Gukesh</rs>, <rs type="person">Hou Yifan</rs>, and <rs type="person">Maxime Vachier-Lagrave</rs>. Without them, this work would not have been possible. We would also like to thank <rs type="person">Tom Zahavy</rs>, <rs type="person">Adam Pearce</rs>, <rs type="person">Kevin Waugh</rs>, <rs type="person">Julian Schrittwieser</rs>, and <rs type="person">Blair Bilodeau</rs> for their help, discussions and feedback on this work.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Human experiments: more concept puzzle examples</head><p>In this section, we provide more examples of concept puzzles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional concept example: positive knowledge transfer</head><p>On a high level, this concept appears to be intrinsically related to centre control and improving piece activity. However, a more detailed analysis unveils a nuanced dimension to this concept, as AZ leverages unconventional manoeuvres to achieve these goals. The grandmaster improved performance by +2/4 between Phases 1 and 3, suggesting that this concept was not part of their existing knowledge and is human-learnable. Figure <ref type="figure">16</ref> shows a concept puzzle that was shown in Phase 1. Here, AZ plays the move 5...Bf5 to control the square e4. The grandmaster chose 5...Bh5 while also considering the moves 5...Bxf3 and 5...Qf6. After seeing the solution, the grandmaster commented "5...Bh5 line looks quite natural ... [however, AZ's move] 5... Bf5 with the concept in mind is very interesting as after 8.d4 [ the continuation for Black of] Nbd7-Bd6 is more natural but Bb4 is something new. I was curious about the idea after 11.Bb2 Nd7 12. Bd3 where h5!? was probably the point."</p><p>We explore the question posed by the grandmaster -what happens after 11.Bb2? The ideas is 5...Bf5 6.Nc3 h6 7.Bb2 Nf6 8.d4 Bb4 9.a3 Bxc3+ 10.Bxc3 Ne4 11.Bb2 Nbd7 12.Bd3 h5 13.0-0 g5!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Background: AZ policy value network</head><p>For a complete description of AZ, see <ref type="bibr" target="#b91">Schrittwieser et al. (2019)</ref> or <ref type="bibr" target="#b73">McGrath et al. (2022)</ref>. Here, we provide a brief description. AZ has two main components: a policy value network and MCTS. Below, we describe the policy-value network.</p><p>For a given input (consisting of a chess position and metadata), the network outputs a policy and value estimate. As described in <ref type="bibr" target="#b73">McGrath et al. (2022)</ref>, the main 'body' of the network has a ResNet backbone. The body consists of 19 residual blocks. Each block contains two convolutional layers, followed by a skip connection. Let z l denote the post-activation latent representation corresponding to block l:</p><p>where g l (•) is the composition of two convolution layers.</p><p>In the main body of the text, we refer to layers using integers. For the main body of the network, these integers denote the ResNet block. In the value head, we have three layers, which we will refer to as layers 20, 21 and 22 (from the body to value output). In the policy head, we have two layers, which we refer to as layers 23 and 24 (from body to policy output).   The human crafted datasets are used to (1) validate the convex optimisation framework and (2) explain the novel concepts by relating discovered concepts to something humans know (by learning a graph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.3">Convex formulation for different datasets</head><p>In this section, we provide further details on the convex optimisation formulations for each dataset. Unless otherwise stated, we use a 90-10 train-test split for the supervised datasets. In our analysis, we generally consider layers 19, 20, 21 and 23. However, as layers 19 and 23 showed the largest difference in rank compared to human data in our analysis §4.2.2, we use layer these layers to discover the concepts shown to grandmasters.</p><p>Piece. We artificially created a piece dataset by sampling chess positions from grandmaster games sampled from ChessBase (2021). For each concept, we sampled 100 chess positions with the concept to create the set X + and then created 100 chess positions without the concept to create the set X -(by removing the piece).</p><p>We formulated the convex optimisation problem as follows. For pair of chess positions, x + i ∈ X + and x - i ∈ X -, we find the corresponding latent representations in layer l to create Z + l and Z - l , respectively. Using these latent representations, we can search for the piece presence concept using the following formulation</p><p>Note that this is the same formulation as for static concepts more generally (see §4.1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stockfish. Following McGrath et al. (2022)</head><p>, we extract the concepts encoded in the Stockfish engine. We use the Stockfish engine code to extract a concept value for each position. We sampled 30, 000 chess positions from ChessBase (2021). For each of these chess positions, we found the chess positions that were in the top 5th percentile (assumed to contain the concept) to construct X + and the bottom 95th percentile (assumed not to contain the concept) for the concept score to construct X -. We randomly paired the chess positions that contained the concept, x + i , with chess positions that scored low for the concept x - j . Similarly to before, we extracted the latent representations for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Discovery and Transfer in AlphaZero</head><p>The vector is then re-scaled by a factor of (1/n) where n is the number of hidden units in the layer. We use the random concept vector to find random prototypes. The randomly sampled prototypes are still informative as they are paired with AZ's policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Graph analysis</head><p>We build a directed graph between AZ's concept vectors and human-labelled concept vectors to improve our understanding of the new, unlabelled concepts. In our work, we found a set of concept vectors V (see §4). Assume we have two different concept vectors v c,l , v k,l ∈ V, where c ̸ = k. Our idea is simple: if concepts v c,l and v k,l are related, they will arise in the same chess positions. Let s c,i,l denote whether concept v c,l is present in a chess position i using the latent representation z i,l in layer l. For each concept c, we estimate the regression model (similar to <ref type="bibr" target="#b74">Meinshausen and Bühlmann (2006)</ref>)</p><p>where β k,l is the regression coefficient and X is a set of positions (we will elaborate on this later). If β k,l is significant at a 5% level, we add a directed edge from v k,l to v c,l . We use a regression as a concept v c,l may be important for v k,l but not vice-versa.</p><p>The next question is: how do we define s c,i,l ? Following the ideas in §4, we assume that product v c,l • z i,l is higher for z i,l , corresponding to positions that contain concept c. Therefore, for static concepts, we measure the concept presence as</p><p>where z i,l ∈ Z s l , and Z s l is a set of latent representations in layer l corresponding to a set of chess positions X s . We sample 2, 000 positions from human games to create X s as static concepts are found using human-labelled positions.</p><p>For dynamics concepts, we estimate the concept relevance as</p><p>where z i,l,t ∈ Z i,l,≤T are the latent representations corresponding to the AZ's chosen MCTS rollout for a starting chess position x i and maximum depth T , and x i ∈ X d . We sample 2, 000 positions from AZ's games to construct X d , as the dynamic concepts are predominantly found in AZ's games.</p><p>As we want to analyze the relationship between static and dynamic concepts, we estimate s static c,i,l and s dynamic c,i,l</p><p>for each v c,l .</p><p>In our analysis, we only keep human concepts of high quality, i.e., that have a concept constraint score of higher than 0.90. Further, for our analysis to be stable, we removed highly correlated variables -i.e., variables that had a correlation coefficient higher than 0.99 <ref type="bibr" target="#b87">(Robinson, 1974;</ref><ref type="bibr" target="#b96">Snee, 1973;</ref><ref type="bibr" target="#b97">Snee and Marquardt, 1984)</ref> dropping 580 concepts (out of 1, 371). Graph Summary. The graph is dense -the graph contains 29% (180, 609/625, 681) of the possible directed edges. As the graph is very dense, we do not provide the entire graph. Instead, we provide parts of the graph in §6 when presenting examples of AZ's concepts.    .00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) both (k=10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.9">Extra results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.9.1">Concept constraint satisfaction</head><p>1.00 (0.00) 0.99 (0.00) 0.99 (0.01) 1.00 (0.00) both (k=5)</p><p>1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.10">Sample Efficiency Results</head><p>Figure <ref type="figure">29</ref> shows the result for sample efficiency experiment ( §5.1.2) on the STS dataset. As with the piece dataset, we find that the convex optimisation method reaches close to full-set accuracy with as little as 10 or 25 data points. However, contrary to the piece dataset, we find that the performance is relatively similar across all layers, and slightly lower for the policy head (layer 23). One reason may be that these concepts are more important for the value than the policy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Discovery and Transfer in AlphaZero</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Achtibat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eisenbraun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>From &quot;where&quot; to &quot;what&quot;: Towards human-understandable explanations through concept relevance propagation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9525" to="9536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A rewriting system for convex optimization problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verschueren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Control and Decision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="60" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforcement learning interpretation methods: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alharin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-N</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sartipi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="171058" to="171077" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interview with gm chuchelov -caruana&apos;s coach</title>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Melekhina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-08">2014. August-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards robust interpretability with self-explaining neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7786" to="7795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards better interpretability in deep q-networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Annasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4561" to="4569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploratory not explanatory: Counterfactual analysis of saliency maps for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05743</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Heckerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11500</idno>
		<title level="m">Debiasing concept bottleneck models with instrumental variables</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Concept gradient: Concept-based interpretation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>without linear assumption</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Verifiable reinforcement learning via policy extraction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Impossibility theorems for feature attribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.11870</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Impossibility theorems for feature attribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.11870</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Syzygy tablebase</title>
		<author>
			<persName><forename type="first">Bojun</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-07">2023. July-2023</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11852</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Brunia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Wijgerden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Steps Method. Self-published</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Concept whitening for interpretable image recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="772" to="782" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Chessbase Mega Database</title>
		<author>
			<persName><surname>Chessbase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global and local interpretability for cardiac MRI classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Puyol-Antón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ruijsink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01070</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distilling deep reinforcement learning policies in soft decision trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Coppens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Efthymiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lenaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nowé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Magazzeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI 2019 workshop on explainable artificial intelligence</title>
		<meeting>the IJCAI 2019 workshop on explainable artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Corbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mosca</surname></persName>
		</author>
		<title level="m">Strategic test suite</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Concept activation regions: A generalized framework for concept-based explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Crabbé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2590" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">State2explanation: Concept-based explanations to benefit agent learning and user understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards symbolic reinforcement learning with common sense</title>
		<author>
			<persName><forename type="first">A</forename><surname>Avila Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R R</forename><surname>Dutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alonso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explainable reinforcement learning for broad-xai: a conceptual framework and survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dazeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Decodechess</surname></persName>
		</author>
		<ptr target="https://decodechess.com/about/" />
		<title level="m">Understanding Chess with Explainable AI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<title level="m">Counterfactual explanation policies in rl</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CVXPY: A Python-embedded modeling language for convex optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">83</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Doncieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bredeche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coninx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khamassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Dream architecture: a developmental approach to open-ended learning in robotics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open-ended learning: A conceptual framework based on representational redescription</title>
		<author>
			<persName><forename type="first">S</forename><surname>Doncieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coninx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roijers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mguni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Chessgpt: Bridging policy learning and language modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fide handbook c.02</title>
		<author>
			<persName><surname>Fide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10">2019. October-2023</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Fiekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>python-chess</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Explainable reinforcement learning via model transforms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Schlot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kolumbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keren</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Where, when &amp; which concepts does AlphaZero learn? Lessons from the game of Hex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Forde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lovering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Reinforcement Learning in Games</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reccover: Detecting causal confusion for explainable reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gajcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dusparic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<title level="m">Towards deep symbolic reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dissect: Disentangled simultaneous explanations via concept traversals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghandeharioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15164</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards automatic concept-based explanations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9273" to="9282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Glanois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13112</idno>
		<title level="m">A survey on interpretable reinforcement learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Alphazero ideas. Available at SSRN 4140916</title>
		<author>
			<persName><forename type="first">J</forename><surname>González-Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Palacios-Huerta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07165</idno>
	</analytic>
	<monogr>
		<title level="m">Explaining classifiers with causal concept effect (CaCE)</title>
		<imprint>
			<date type="published" when="2019">2022. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Regression concept vectors for bidirectional explanations in histopathology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Graziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Understanding and Interpreting Machine Learning in Medical Image Computing Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and understanding atari agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fern</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1792" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Explain your move: understanding agent actions using specific and relevant feature attribution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Troitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01610</idno>
		<title level="m">Finding neurons in a haystack: Case studies with sparse probing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Back to Basics: Chess Openings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Russel Enterprises</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep explainable relational reinforcement learning: A neurosymbolic approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08349</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Explainability in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Couthouis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Drlviz: Understanding decisions and memory in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="49" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to generate move-by-move commentary for chess games from large-scale social forum data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The role of explainability in assuring safety of machine learning in healthcare</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdermid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lawton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Habli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1746" to="1760" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Case-based evaluation in computer chess</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Case-Based Reasoning</title>
		<editor>
			<persName><forename type="first">J.-P</forename><surname>Haton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Keane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Manago</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="240" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Beyond interpretability: developing a language to shape our relationships with ai</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>How to Win at Chess. Everyman Chess</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04612</idno>
		<title level="m">Concept bottleneck models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Learning finite state representations of recurrent policy networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fern</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12530</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Krajna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lipic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doncevic</surname></persName>
		</author>
		<title level="m">Explainability in reinforcement learning: perspective and position</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Are alphazero-like agents robust to adversarial perturbations?</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-11-20">2022. 2018. 20 November 2019</date>
			<publisher>Leela Chess Zero</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep unsupervised state representation learning with robotic priors: a robustness analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06064</idno>
		<title level="m">Object-sensitive deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Encyclopedia of chess openings</title>
		<author>
			<persName><surname>Lichess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-05">2023. May-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Toward interpretable deep reinforcement learning with linear model u-trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2019. September 10-14, 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="414" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Distal explanations for explainable reinforcement learning agents</title>
		<author>
			<persName><forename type="first">P</forename><surname>Madumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sonenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vetere</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10284</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Promises and pitfalls of black-box concept learning models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahinpei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno>CoRR, abs/2106.13314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapishnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomašev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kramnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09259</idno>
		<title level="m">Acquisition of chess knowledge in alphazero</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Acquisition of chess knowledge in alphazero</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapishnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomašev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kramnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">47</biblScope>
			<biblScope unit="page">2206625119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">High-dimensional graphs and variable selection with the lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08434</idno>
		<title level="m">A survey of explainable reinforcement learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Concept-based model explanations for electronic health records</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loreaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Protsyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mottram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrouff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning</title>
		<meeting>the Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Efficient saliency maps for explainable ai</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Actually, othello-gpt has a linear emergent world model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">A theoretical explanation for perplexing behaviors of backpropagation-based visualizations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Counterfactual states for atari agents via generative deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12969</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Beyond rewards: a hierarchical perspective on offline multiagent behavioral analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapishnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Assogba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3444" to="3460" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unveiling concepts learned by a world-class chess-playing agent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pálsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Björnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">S-rl toolbox: Environments, datasets and evaluation metrics for state representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Traoré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Traoré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Overlooked factors in concept-based explanations: Dataset choice, concept learnability, and human capability</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10932" to="10941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fitting equations to data: Computer analysis of multifactor data for scientists and engineers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">000003</biblScope>
			<biblScope unit="page">346</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
	<note>JMR pre-1986</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01180</idno>
		<title level="m">Conservative q-improvement: Reinforcement learning for an interpretable decision-tree policy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01318</idno>
		<title level="m">Finding and visualizing weaknesses of deep reinforcement learning agents</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Game Changer: AlphaZero&apos;s Groundbreaking Chess Strategies and the Promise of AI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Regan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>New In Chess</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Mastering Atari, Go, chess and shogi by planning with a learned model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Concept enforcement and modularization as methods for the iso 26262 safety argumentation of neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwalbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>University of Bamberg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01815</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Some aspects of nonorthogonal data analysis: Part i. developing prediction equations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Snee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Quality Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Comment: Collinearity diagnostics depend on the domain of prediction, the model, and the data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Snee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Marquardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="87" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03549</idno>
		<title level="m">Adversarial tcav-robust and effective interpretation of intermediate layers in neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Interpretable AI for deep learning-based meteorological applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Wendoloski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">American Meteorological Society Annual Meeting</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Bridging the gap: Providing post-hoc symbolic explanations for sequential decision-making problems with black box simulators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sreedharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01080</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Tldr: Policy summarization for factored ssp problems using temporal abstractions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sreedharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling</title>
		<meeting>the International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="272" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<orgName type="collaboration">Stockfish Community</orgName>
		</author>
		<ptr target="https://stockfishchess.org/" />
		<title level="m">Stockfish Chess</title>
		<imprint>
			<date type="published" when="2018-07-23">2018. 23 July 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<title level="m">Bert rediscovers the classical nlp pipeline</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tomašev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kramnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04374</idno>
		<title level="m">Assessing game balance with AlphaZero: Exploring alternative rule sets in chess</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Reimagining chess with AlphaZero</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomašev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kramnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Understanding game-playing agents with natural language annotations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07531</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Iterative bounding mdps: Learning interpretable policies via non-interpretable methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9923" to="9931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Traoré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Caselles-Dupré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<title level="m">Discorl: Continual reinforcement learning via policy distillation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Moët: Interpretable and verifiable reinforcement learning via mixture of expert trees</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khurshid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Review</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Explainable deep reinforcement learning: state of the art and challenges</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Vouros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Wikipedia contributors (2023a)</title>
	</analytic>
	<monogr>
		<title level="m">Chess rating system</title>
		<imprint>
			<date type="published" when="2023-07">July-2023</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Chess strategy</title>
	</analytic>
	<monogr>
		<title level="m">Wikipedia contributors (2023b)</title>
		<imprint>
			<date type="published" when="2023-10">October-2023</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Wikipedia contributors (2023c)</title>
		<imprint>
			<date type="published" when="2023-10">October-2023</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>Chess tactic</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Causal proxy models for concept-based model explanations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>D'oosterlinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="37313" to="37334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.01458</idno>
		<title level="m">Leveraging reward consistency for interpretable feature discovery in reinforcement learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Ubs: A dimension-agnostic metric for concept vector interpretability applied to radiomics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yeche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">On completenessaware concept-based explanations in deep neural networks</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20554" to="20565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Graying the black box: Understanding dqns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ben-Zrihem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1899" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Diversifying ai: Towards creative chess with alphazero</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<title level="m">Relational deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Zrihem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07112</idno>
		<title level="m">Visualizing dynamics: from t-sne to semi-mdps</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
