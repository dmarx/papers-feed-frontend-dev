# Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero

## Abstract

## 

Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.

## Introduction

Artificial Intelligence (AI) systems are typically treated as problem-solving machines; they can carry out the jobs humans are already capable of but more efficiently or with less effort, which brings clear benefits in several domains. In this paper, we pursue a different goal: treat AI systems as learning machines and demand from them to teach us the fundamental principles behind their decisions to extend upon and complement our knowledge. We can imagine many benefits of learning from machines. For example, while a system capable of producing a more accurate cancer diagnosis or effective personalised treatment than human experts is useful, transferring the rationale behind their decisions to human doctors could not only bring advances in medicine but also leverage human doctors' strength and generalisation ability to enable new breakthroughs. There is a tremendous untapped opportunity across various domains where the capabilities of AI systems are reaching or exceeding those of human experts (super-human AI systems). This work is one of the very first steps towards the development of tools and methods that allow us to uncover hidden knowledge in highly capable AI systems, and empower human experts by helping them further improve their skills and understanding.

The super-human ability of AI systems may arise in a few different ways: pure computational power of machines, a new way of reasoning over existing knowledge, or super-human knowledge we do not yet possess. This work focuses on the last two cases. For simplicity, we refer to both as super-human knowledge from now on.

What does this mean from a research standpoint? The human representational space (H) has some overlap with the machine representational space (M ) (see Figure [1](#)[(Kim, 2022)](#b56)). A representational space forms the basis of and gives rise to knowledge and abilities, which we are ultimately interested in. Thus, we use representational space and knowledge interchangeablyroughly speaking, H to represent what humans know and M to represent what a machine knows. There are things that both AI and humans know (M ∩ H), things that only humans know (H -M ), Figure [1](#): Learning from machine-unique knowledge. and things only machines know (M -H). Most existing research efforts only focus on (M ∩ H), e.g., interpretability has tried to shoehorn M into (M ∩ H), with limited success [(Adebayo et al., 2018;](#b1)[Nie et al., 2018;](#b79)[Bilodeau et al., 2022)](#). We believe that the knowledge gap represented by (M -H) holds the crucial key to empowering humans by identifying new concepts and new connections between existing concepts within highly performant AI systems. We already have evidence of cases when certain AI generations captivated the human imagination with ideas that were initially hard to grasp. One prominent example in the history of AI is the move 37 that AlphaGo made in a match with Lee Sedol. This move came as a complete surprise to the commentators and the player, and is still discussed to this day as an example of machine-unique knowledge. The vision to pursue super-human knowledge is ultimately for human-centered AI, and a world where human agency and capability do not come second. However, the question is-is this even possible?

This work is the first step towards discovering super-human knowledge and new connections of existing knowledge in (M -H). We focus on a domain that has inspired AI practitioners for decades, and captivated human imagination for centuries: the game of chess. Chess is an excellent playground to validate the existence and usefulness of set (M -H) for many reasons: chess knowledge has been developed over a long period of time, and the ground truth is much easier to validate compared to the frontiers of other fields, such as science or medicine. We also have a quantitative measure of the quality of play, both for human experts as well as machines, known as the Elo rating (Wikipedia contributors, 2023a).

Chess engines have performed at a super-human level for a long time, ever since DeepBlue's match against Garry Kasparov. While early engines were based on human knowledge, the advent of AlphaZero [(Silver et al., 2017)](#b94) (AZ) showed a self-taught deep learning model achieve a superhuman capability in chess without any human knowledge. However, as humans, we have not yet been able to tap into their knowledge fully. Through analysis of AZ's games, humans manually distilled patterns, such as its proclivity for playing on the flanks with moves like a4 or h4 [(Sadler and Regan, 2019)](#b90). However, this still analyses M through the lens of H, a bias that limits what we can find from M ∩ H.

In this work, we aim to take the first step to change that by facilitating learning from the super-human knowledge in the (M -H) set of AZ. We hypothesise that (M -H) exists, and can be taught to humans.

We validate our hypothesis by showing that we can teach new chess concepts to four top human grandmasters, the best chess players in the world. Also, due to their undeniable strength, and talent, (M -H) may fall into their 'proximal zone of development' in Vygotsky's education theory: "the space between what a learner can do without assistance and what a learner can do with adult guidance or in collaboration with more capable peers". While communicating (M -H) may require new language [(Kim, 2022)](#b56), we bypass this need in this work by leveraging chess champions' ability to connect the dots and generalise from patterns that arise in chess positions.

We find evidence that suggests (M -H) exists through analysing the dimension of the span of the latent representations of AZ's and human's games ( §4.2.2). Next, we develop a new framework to search for concepts in (M -H), i.e., unearth AZ's super-human knowledge. In our framework, we:

• develop a new method for finding unsupervised concepts in the latent space. By using the full AZ machinery, both the policy value network and MCTS tree, our method discovers dynamic concepts that motivate a sequence of actions in chess. We show that our method can find vector representations of concepts in a data-efficient manner ( §4.1).

• ensure concepts are novel. Through spectral analysis, our framework only select concepts that contain information unique to the vector space of AZ's games compared to that of human games.[foot_0](#foot_0)

• ensure that concepts are teachable. We develop a new metric that evaluates whether concepts are teachable to another AI agent with no prior knowledge of the concept ( §4.2.1). Through this metric, we select concepts based on their informativeness (i.e., useful for the AI agent in a downstream task).

most chess players would continue to play on the kingside with Rxh5. However, AZ finds the only plan to maintain an advantage: Qc1 with the idea of re-manoeuvring the pieces to the queenside. The results of our study show an improvement in the grandmasters' ability to find conceptbased moves aligned with AZ's choices, as compared to their performance prior to observing AZ's moves. Further, their qualitative feedback indicated an understanding and appreciation of AZ's plans. The discovered concepts often combine and apply chess concepts in a way that deviates from the traditional human principles of chess. We conjecture that the differences in humans' and AZ's play may stem from their differences in how position-concept relationships are built. While humans develop prior biases over which concepts may be relevant in given chess positions, AZ has developed its own unconstrained understanding of concepts and chess positions, enabling flexibility and creativity in its strategies.

Our paper is structured as follows. First, we summarise related work in §2. Next, we discuss the definition of concepts and how we operationalise it in §3. We introduce our method for finding concepts in §4.1 and filtering them in §4.2 to ensure concepts are informative, teachable, and novel. We demonstrate the efficacy and performance of our method on supervised concepts in §5. Finally, in §6, we lay out the human experiment protocols and results and show how our framework can enable bridging the (M -H) gap. We conclude the paper in §7 by summarising our main findings, and discussing limitations and future work.

## Related work

Here, we review relevant prior work on concept discovery, interpretability of reinforcement learning systems, and the intersection of AI and chess.

## Concept-based explanations

In contrast to traditional feature or data-centric interpretability methods [(Ribeiro et al., 2016;](#b86)[Lundberg and Lee, 2017;](#b69)[Sundararajan et al., 2017;](#b103)[Koh and Liang, 2017)](#b60), concept-based methods use high-level abstraction, concepts, with the goal of providing model explanations to inform human practitioners [(Bau et al., 2017;](#b11)[Kim et al., 2018;](#b57)[Alvarez-Melis and Jaakkola, 2018;](#b5)[Koh et al., 2020;](#b61)[Bai et al., 2022;](#b9)[Achtibat et al., 2022;](#b0)[Crabbé and van der Schaar, 2022)](#b23). These types of explanations are shown to be useful in scientific and biomedical domains [(Graziani et al., 2018;](#b45)[Sprague et al., 2019;](#b99)[Clough et al., 2019;](#b19)[Bouchacourt and Denoyer, 2019;](#b15)[Yeche et al., 2019;](#b119)[Sreedharan et al., 2020a;](#)[Schwalbe and Schels, 2020;](#b92)[Mincu et al., 2021;](#b76)[Jia et al., 2022)](#b54), where experts' concepts are highly relevant in decision making rather than individual low-level features.

More in line with the work presented in this paper, concept-based explanation methods have been studied in board game playing agents, including in Hex [(Forde et al., 2022)](#b37) and Go [(Tomlin et al., 2022)](#b108). Establishing a causal link between concepts and predictions is non-trivial and a topic of ongoing research [(Goyal et al., 2019;](#b44)[Bahadori and Heckerman, 2020;](#b8)[Wu et al., 2023)](#b117).

Shortcomings of supervised concept-based methods have been studied. When leveraging a set of exemplars of a concept (a probe dataset), [Ramaswamy et al. (2023)](#b85) showed that different probe datasets can lead to inconsistent conclusions. Further, they showed that the number of concepts in probe datasets exceeds those used by humans. The linearity assumption has limitations [(Chen et al., 2020;](#b17)[Soni et al., 2020)](#b98), and the faithful alignment between the vector and humans' mental models of the concept was challenged in [Mahinpei et al. (2021)](#b71).

Going beyond supervised concepts and probe datasets has also been investigated [(Yeh et al., 2020;](#b120)[Ghorbani et al., 2019;](#b41)[Ghandeharioun et al., 2021)](#b40) to discover concepts that a model represents without being limited to human labelled concepts. The concept is expressed using examples of training data [(Yeh et al., 2020;](#b120)[Ghorbani et al., 2019)](#b41) or by generating new data [(Ghandeharioun et al., 2021)](#b40). This work falls under methods to discover concepts but with a different goal of discovering and teaching humans new concepts rather than finding existing human concepts.

## Generating explanations in Reinforcement Learning

Generating explanations in Reinforcement Learning (RL) methods [(Alharin et al., 2020;](#b3)[Heuillet et al., 2020;](#b51)[Glanois et al., 2021;](#b42)[Krajna et al., 2022;](#b63)[Vouros, 2022;](#b112)[Milani et al., 2022;](#b75)[Dazeley et al., 2023;](#b26)[Omidshafiei et al., 2022;](#b81)[Das et al., 2023)](#b24) is of particular interest, as these methods are increasingly deployed in real-world applications, and the explanation requirements differ compared to the more traditional supervised learning setting. This is due to the temporal dependency between states, actions, and subsequent states, where an agent's historical, current, and future state-action sequences may relate to some long-term goal [(Dazeley et al., 2023)](#b26). Explainability methods in RL can help identify issues with agents related to over-fitting the training data, poor out-of-distribution performance [(Annasamy and Sycara, 2019)](#b6) and inter-agent dynamics [(Omidshafiei et al., 2022)](#b81).

Several efforts focused on designing more interpretable model architectures and training proce-dures in representation learning [(Raffin et al., 2019](#b84)[(Raffin et al., , 2018;;](#b83)[Lesort et al., 2019;](#b65)[Traoré et al., 2019;](#b110)[Doncieux et al., 2020](#b31)[Doncieux et al., , 2018) )](#b32) and symbolic and relational methods [(Sreedharan et al., 2020b;](#b101)[Garnelo et al., 2016;](#b39)[d'Avila Garcez et al., 2018;](#)[Zambaldi et al., 2018;](#b123)[Hazra and De Raedt, 2023)](#b50), which may involve intermediate perceptual processing steps, like object recognition [(Goel et al., 2018;](#b43)[Li et al., 2018)](#b66). [Different RL methods (value-based, policy-based, model-based, fully](#) or partially observable states) [(Alharin et al., 2020)](#b3) may lend themselves to different explainability approaches or variations thereof. Likewise, the explanations themselves may vary in scope, e.g., local explanations of individual agent actions or value assessments, or the overall high-level explanations of the agent policy [(Zrihem et al., 2016;](#b124)[Sreedharan et al., 2020b;](#b101)[Topin et al., 2021)](#b109). The importance of treating explanations as a reward to ensure consistency is explored in [Yang et al. (2023)](#b118).

For trained RL systems, there is a pressing need for post-hoc RL interpretability methods. Input saliency maps [(Wang et al., 2016;](#b113)[Selvaraju et al., 2019;](#b93)[Greydanus et al., 2018;](#b46)[Mundhenk et al., 2020)](#b77) and tree-based models [(Bastani et al., 2018;](#b10)[Roth et al., 2019;](#b88)[Coppens et al., 2019;](#b21)[Liu et al., 2019;](#b68)[Vasic et al., 2019;](#b111)[Madumal et al., 2020)](#b70) have been a common approach. Saliencybased RL explainability approaches are not without issues, as they may suffer from unfalsifiability and be subject to cognitive bias [(Atrey et al., 2019)](#b7) as well as provably wrong results [(Bilodeau et al., 2022)](#). Visualizing the agent memory over trajectories [(Jaunet et al., 2020)](#b52) or extracting finite-state models [(Koul et al., 2018)](#b62) are explored to improve understanding of agents' behavior, as well as leveraging Markov decision processes [(Finkelstein et al., 2022;](#b36)[Zahavy et al., 2016)](#b121) to generate explanations or detect sub-goals or emerging structures [(Rupprecht et al., 2019)](#b89). As RL methods may sometimes learn spurious correlations, interpretability methods were used to help identify and resolve the causal confusion [(Gajcin and Dusparic, 2022)](#b38) and further our understanding using counterfactuals [(Deshmukh et al., 2023;](#b28)[Olson et al., 2019)](#b80).

## Chess and Artificial Intelligence

Chess has been a test bed for AI ideas for decades. Early engines were based on human knowledge, and their super-human strength came from their computational capacity, which allowed them to consider a number of variations orders of magnitude above the abilities of human chess players. The introduction of neural network and RL-based approaches aimed to revitalise the field, which led to a surge of improvements in computer chess engines. These advances were in part inspired by the prominent results of AZ in chess and its variants [(Silver et al., 2018;](#b95)[Tomašev et al., 2020;](#b106)[Tomašev et al., 2022;](#b107)[Zahavy et al., 2023), and](#b122)[Lc0 (LCZero Development Community, 2018)](#), an open-source re-implementation of the original model, is still competing at the highest level of computer chess.

As interactions with chess engines play a key role in chess players' preparation and training, interpretability helps chess players understand the underlying positional and tactical motifs. To this end, prior work has looked at piece saliency [(Gupta et al., 2020)](#b47), tree-based explanations [(Kerner, 1995)](#b55) and natural language [(Jhamtani et al., 2018)](#b53). At the intersection of chess and language, ChessGPT has recently been proposed [(Feng et al., 2023)](#b33) to bridge the modality of policy and language. DecodeChess is a software project aimed at deriving explanations from engine search trees [(DecodeChess, 2017)](#b27).

Recently, AZ has was shown to encode human-like concepts in its network [(McGrath et al., 2022)](#b73), and concept probing techniques have also been explored with the network-based Stockfish chess engine [(Pálsson and Björnsson, 2023)](#b82). This prior investigation of concepts in AZ did not consider search and move sequences, and was largely restricted to identifying pre-existing human concepts. Preliminary questions have been raised regarding whether human players have been adopting AZ's ideas [(González-Díaz and Palacios-Huerta, 2022)](#b44), as some prominent motifs had been analysed in detail in Game Changer [(Sadler and Regan, 2019)](#b90). Recently, it was also shown that AZ may be susceptible to adversarial perturbations [(Lan et al., 2022)](#b64), underscoring the need for a better understanding of the learned representations.

## What are concepts?

There are several possible definitions of a concept -varying from a human-understandable high-level feature to an abstract idea. In this work, we define concepts as a unit of knowledge. There are two key properties we focus on. The first is that a concept contains knowledge: information that is useful; in the context of machine learning, we take this to mean that it can be used to solve a task. For example, consider the concept of a beak. We can teach an algorithm or person (transfer of the knowledge) what a beak is. If the person grasps the beak concept, they can use it to identify birds. Second, a unit implies minimality; it is concise and irrelevant information has been removed.

There are many ways to operationalise this definition and properties, and we choose one of them: showing a concept can be transferred to another agent to help them solve a task (e.g., follow the strategy represented in a concept). Being able to do so implies that the concept is self-contained and useful for the task.

How do we represent concepts? We leverage rich literature that assumes concepts are linearly encoded in the latent space of a neural network [(McGrath et al., 2022;](#b73)[Kim et al., 2018;](#b57)[Gurnee et al., 2023;](#b48)[Conneau et al., 2018;](#b20)[Tenney et al., 2019;](#b104)[Nanda, 2023)](#b78). The latent space refers to the space spanned by post-activation features of a neural network. Although our assumption of linearity is a strong assumption, it has a surprising amount of empirical support: linear probing and related techniques have successfully extracted a wide range of complex concepts from neural networks across multiple domains [(McGrath et al., 2022;](#b73)[Kim et al., 2018;](#b57)[Gurnee et al., 2023;](#b48)[Conneau et al., 2018;](#b20)[Tenney et al., 2019;](#b104)[Nanda, 2023)](#b78). Although we may miss concepts with nonlinear representations, we nevertheless show that we can find useful concepts for our goal using purely linear representations.

What types of concepts do we aim to discover in the RL setting? We aim to discover concepts that give rise to a plan, where a plan is a deliberate sequence of actions optimizing for one or more relevant concepts. We take deliberate to mean that there is an underlying reason. More specifically, we assume a plan is motivated by one or more concepts. Although the terminal goal of a plan the same across states -maximizing the outcome (win or draw) -plans in a specific state will have more context-specific instrumental goals along the way, for instance, capturing a particular piece in an advantageous position, or maximising one's board control. We assume that plans in similar contexts will share similar instrumental goals, and thus give rise to similar concepts.

## Discovering concepts

Our method can be summarised into (1) excavating vectors that represent concepts in AZ using convex optimisation, (2) filtering the concepts based on teachability (whether it is transferable to another AI agent) and novelty (whether it contains some information that is not present in human games). The resulting set of concept vectors is then used to generate chess puzzles (chess positions and solutions), which are presented to human experts (top chess grandmasters) for final validation.

## Excavating concept vectors

To find concepts, we develop a new method since (1) the model input is a mix of binary and realvalued inputs (e.g., saliency maps typically take as input continuous values and are generally not suitable for binary values) and (2) we want to develop an interpretability tool to analyse both parts of AZ machinery -the policy-value network and MCTS. Leveraging both the network and MCTS is crucial, since each component plays a different yet important role in deciding the move (see §8.3 for more detail). We formulate concept discovery as a convex optimisation problem. Using a convex optimisation framework is not new; many existing methods for finding concept vectors, such as non-negative matrix formulation, can often approximated as a convex optimisation problem [(Ding et al., 2008)](#b30).

For each concept vector we want to find, we formulate a separate convex optimisation problem. As mentioned in §3, we define a concept as a unit of knowledge. Minimality is achieved by encouraging sparsity [(Tibshirani, 1996)](#b105)

$through the L 1 norm min ∥v c,l ∥ 1 such that concept constraints hold,(1)$where v c,l ∈ R d l is a vector that lives in latent space of layer l to represent concept c, and d l is the dimension of layer l.

We outline the concept constraints used for two different types of concepts: static and dynamic concepts. Static concepts are defined to be found in a single state, whereas dynamic concepts are found in a sequence of states. An example of a static concept in autonomous driving is that the car is located on the highway. A dynamic concept is that the car is accelerating. While our framework only aims to discover dynamic concepts, we use static concepts to validate our method.

## Concept constraints for static concepts

Static concepts are defined as concepts that only involve a single state. We use supervised data (labels indicate whether a state contains a concept c) to learn static concept vectors. These concepts encode human knowledge, and therefore, we can use them to validate our approach. One example of a static concept is the concept of 'space', which we can infer from a single state. For now, assume we have binary concepts[foot_1](#foot_1) and denote the presence of concept c (concept score) in chess position x by c(x) = 1, and c(x) = 0 otherwise. For each concept c, we can split a general set of chess positions X into positive examples X + , where the concept is present, and X -, where it is absent

$X + = {x ∈ X : c(x) = 1} X -= {x ∈ X : c(x) = 0}.$
## These positive and negative examples allow us to generate corresponding positive and negative examples of latent representations (intermediate post-activation representations in the network).

The function f l (x) generates an activation for layer l given an input x:

$Z + l = {f l (x) : x ∈ X + } Z - l = {f l (x) : x ∈ X -},$where z l = f l (x) denotes the latent representation obtained at layer l by passing input x through the network. See §8.2 for further details on how z l is extracted.

The convex optimisation goal is to learn a sparse vector v c,l that represents a concept c. We hypothesise that the inner product v c,l • z l is higher[foot_2](#foot_2) for activations from Z + l (the set where the concept is present) than for activations from Z - l (the set where the concept is absent). Thus, the formulation becomes

$min ∥v c,l ∥ 1 such that v c,l • z + l ≥ v c,l • z - l ∀ z + l ∈ Z + l , z - l ∈ Z - l .(2)$We can evaluate how well a concept is represented by v c,l in the supervised setting by splitting X into two sets: X train and X test and then v c,l only using X train . We then measure the fraction of elements in X test on which the concept constraints hold. If v c,l represents the concept c well, we expect the concept constraint to hold on previously unseen activations derived from X test .

## Concept constraints for dynamic concepts

Dynamic concepts are defined as concepts found in a sequence of states. While v c,l is found in the activation space of the policy-value network, we use the Monte Carlo Tree Search (MCTS) statistics to find candidates for meaningful sequences of states. MCTS generates a tree of possible moves and subsequent responses from the current chess position x 0 (for details on the implementation of MCTS see [Schrittwieser et al. (2019)](#b91)). The exact details are not essential to understand for our procedure; what matters is that AZ chooses a rollout X + ≤T = (x + 1 , x + 2 , x + 3 , . . . , x + T ), where T is the maximum depth of the rollout, which terminates in the most favourable state according to AZ. We contrast this optimal rollout X + ≤T with a sub-par rollout X - ≤T , which is defined as a path in the MCTS search tree that is suboptimal, according to the value estimate or visit count in MCTS.

The intuition behind our procedure is that X + ≤T is chosen over X - ≤T because of a concept c, and we assume the concept c is detectable by a linear probe at some layer l. The concept presence may affect planning in different ways. Consider two rollouts in MCTS, one chosen by AZ (X + ≤T ), and one not chosen by AZ, (X - ≤T ). There are three different possible explanations for why AZ chooses X + ≤T over X - ≤T :

1. Active planning X + ≤T increases the presence of a concept c. For example, the rollout X + ≤T may increase the concept of piece activity.

2. Prophylactic planning X + ≤T avoids increasing the presence of a concept c. An example may be that the plan in X + ≤T avoids losing a piece.

3. Random X + ≤T is arbitrarily chosen above the X - ≤T , as all concepts are equally present in both rollouts and the value estimates of the final states are approximately equal.

We are interested in scenarios 1 and 2 but not scenario 3. Scenario 3 can be filtered out by leveraging the fact that the two rollouts will have similar value estimates and visit counts in the MCTS statistics.

Using a similar approach to static concepts, we derive our concept constraints on the vector v c,l by contrasting the positive and negative examples, except that this time our contrasting examples are pairs from the chosen rollout X + ≤T and the subpar rollout X - ≤T . We denote the activations at layer l at depth t by z + t,l and z - t,l for positive and negative examples, respectively. A single pair of positive and negative rollouts gives rise to the following convex optimisation problem min ∥v c,l ∥ 1

(3)

$such that v c,l • z + t,l ≥ v c,l • z - t,l ∀ t ≤ T,(4)$for scenario 1, with the inequality reversed for scenario 2.

Figure [3](#): Contrasting the optimal rollout with subpar MCTS rollouts at different time steps. The green rollout shows the optimal rollout, and the red rollouts depict subpar trajectories. At each time step, MCTS finds subpar trajectories. We include each of these pairs in the concept constraints.

We extend this idea by contrasting the optimal trajectory with multiple subpar trajectories across different MCTS depths. Figure [3](#) shows this idea. On the left side of Figure [3](#), we find the optimal and subpar trajectory at the initial chess position, t = 1. However, we can also use the MCTS statistics (value estimate and visit count) to find subpar trajectories at t = 2 (shown in the middle) and t = 3 (shown on the right). The idea behind using multiple subpar trajectories is to further reduce the solution space with the goal of reducing noise (thereby increasing the likelihood that the concept vector is meaningful) and decreasing the likelihood of learning a polysemantic vector.

## Let Z +

≤T,l denote the latent representations in layer l corresponding to the optimal rollout X + ≤T , and Z - ≤T,l,j denote the latent representations in layer l corresponding to the subpar rollout X - ≤T,j selected at time step j. We find the dynamic concept as follows:

$min ||v c,l || 1 (5) such that v c,l • z + t,l ≥ v c,l • z - t,l,j ∀t ≤ T, j ≤ T ,$where T denotes the maximum depth at which we find suboptimal rollouts. T = 3 in Figure [3](#). In general, we set T = T -5 to ensure the rollout is sufficiently deep. Details on how T is set can be found in §8.4.1.

## Filtering concepts

Our approach (described in §4.1) provides many concept vectors, some or many of which represent known concepts or non-generalisable concepts (i.e., only applicable to a single chess position). In this section, we describe how we further filter concepts to ensure that they are useful (transferable) and novel. Our first filtering for usefulness is to see if we can teach a concept to a student network such that it leads to an improved performance on concept test positions. We describe this process of selecting only teachable concepts in §4.2.1. We further filter concepts based on novelty ( §4.2.2) by finding representations in AZ's self-play games that do not occur in a top-level human play dataset.

## Teachability

Recall that we defined a concept as a unit of knowledge -a key aspect is that it is teachable to another AI agent or person, who can apply the concept to solve an unseen task ( §3). To ensure our concepts are teachable, we use teachability as a selection criterion for the final concepts. The idea is simple:

1. Phase 1: Baseline find an agent that does not know the concept. We can estimate the agent's knowledge by evaluating its performance on a concept-related task.

2. Phase 2: Teach the agent the concept using concept prototypes.

3. Phase 3: Evaluate the agent's performance on a concept-related task If a concept is teachable, we expect the agent's performance to improve between the first and third steps. We use a similar process to evaluate when we evaluate our approach with humans ( §6).

Selecting prototypes. In Phase 2, we use AZ as a teacher to supervise a student network on a set of chess positions called 'prototypes' -chess positions that exemplify the use of a concept.

For each candidate concept c, we have a concept vector v c,l . We want chess positions x from a dataset X that epitomises the concept c. For static concepts, we do this by computing a concept score c(x) = v c,l • f l (x) for every x ∈ X and then selecting the top 2.5% of X according to the concept score c(x). We use 2.5%, as we found the concept score c(x) to be comparable to v c,l • z + l , where z + l ∈ Z + l is the training set used to find v c,l . This procedure gives us a prototype set X proto = {x ∈ X : c(x) in 2.5th percentile of c(x)}. For dynamic concepts, we find chess positions using the MCTS statistics. For each chess position x ∈ X, we ran MCTS. Next, we found the chosen rollout X + (and the corresponding latent representations Z + ) and subpar rollout X -(and the corresponding latent representations Z -) as in §4.1.2. For a prototype

$x i ∈ X proto , we require that v c,l • z + i,t ≥ v c,l • z - i,t ∀ t ≤ T .$Teaching and measuring learning. Intuitively, we can interpret the set of prototypes as a curriculum. We can split X proto into a train set X train and a test set X test . The teacher (AZ) teaches the student by minimizing the KL divergence between the policies of the teacher (π t ) and the student (π s ) on the training prototypes:

$xi∈Xtrain KL[π t (x i ), π s (x i )].$Then, to determine whether the student has acquired the new knowledge, we evaluate the student's performance on the test set prototypes by estimating how often the student and teacher select the same top-1 move

$T = xi∈Xtest 1[argmax(π s (x i )), argmax(π t (x i ))]. (6)$Teaching using any curriculum may improve students' performance on the task. To distill general learning from concept-specific learning, we compare the student's performance when taught using concept prototypes versus random chess positions sampled from AZ's games (but with meaningful plans). We sample the chess positions from AZ's games instead of human games for two reasons:

(1) AZ's games tend to be of a higher quality than human games (as AZ has a higher Elo rating), and (2) the data is closer to AZ's natural training data (avoiding any confounding effects due out of distribution data). Figure [4](#fig_1) shows the student's performance in four settings: (1) student trained on concept c and evaluated on concept c (dark blue line); (2) student trained on concept c and evaluated on random data from AZ's games (dark green line); (3) student trained on random data and evaluated on concept c (light blue line); and (4) student trained on random data and evaluated on random data (light green line). When teaching a student with concept-specific prototypes, the student also improves its performance on a random set of prototypes (dark green line) but less than on conceptspecific prototypes (dark blue line). When a student was taught with randomly sampled chess positions, labelled with optimal play, it improves their performance significantly less (light lines) than when it was taught with concept-specific prototypes (dark lines). Naturally, the student learns quicker when taught with concept-specific prototypes (dark blue line) than random prototypes (light blue line). We also observe that concepts can be taught efficiently. The student's performance after training for 50 epochs on a small set of prototypes would have taken 10K -100K epochs using self-play. Recall that the student is evaluated on a holdout test set, ruling out the possibility that the student network memorised the chess positions.

We select the student to be the latest checkpoint for which the top-1 policy overlap is less than 0.2, resulting in 97.6% of the concepts being filtered out.

## Novelty

There are many different ways to ensure the novelty of concepts. We take two simple approaches:

(1) ensure concepts are learnt during the later stages of AZ's training and (2) filter concepts based on a novelty metric.

Concepts are learnt during a late stage in AZ's training. To find chess positions that are complex, we leverage AZ's training history. We use two versions of AZ that differ by 75 Elo points. [4](#foot_3)To find interesting chess positions, we run through each game and select chess positions where the two versions of AZ disagree on the best move according to their policies. We only use these chess positions to discover concepts.

Setup to measure novelty. While the previous section ensures that concepts that emerge in the final stages of training in AZ are complex by construction, it does not ensure that the concepts are novel to humans. One way to validate novelty is to determine whether the concepts arise in AZ's games but not in human games. Leveraging the fact that concepts are represented in the latent space as a vector, we can compare the vector space of AZ games to that of human games. Specifically, let Z a l denote a matrix where we stack the latent representations in layer l of 17, 184 chess positions sampled from AZ's games. Each row represents a chess position, and each column represents a dimension in latent space in layer l. Similarly, we define Z h l as a matrix of 17, 184 chess positions sampled from human games. Using the latent representations, (1) we first get evidence that AZ's game is likely to contain new concepts using a rank experiment, and then (2) measure the novelty scores by regressing concepts onto AZ's games vector space and human games vector space on which we filter concepts based.

AZ games likely to contain new concepts compared to human games. First, we aim to establish whether AZ games contain something new compared to human games. We approach the question by contrasting the number of concepts encoded in both. The number of the basis vectors (or ranks of Z h l and Z a l ) estimates the size of the span of the latent representations of the gamesinformally, we can think of it as a proxy for the number of concepts.

As shown in Table [1](#tab_0), the ranks for Z h l and Z a l vary across different layers. We focus on the final layers in the architecture as these more directly impact the output. Note that while the rank of inputs for both humans and AZ games are similar, AZ games' rank is higher than the humans games' rank at layers 19 (final layer in main bottleneck) and 23 (policy layer), hinting that there might be concepts present in AZ games that are not in human games. Therefore, we focus on finding new concepts in these layers. Novelty Scores for filtering We define the novelty scores based on how well a concept vector can be reconstructed using a set of basis vectors that arise in AZ's games. Naturally, a lower reconstruction loss means that the concept is better represented using a set of given basis vectors.

In other words, we look for concepts that are better explained using AZ's language (basis vectors) than humans'. We define novelty score as the difference between concept's reconstruction loss (see Equation [7](#formula_10)) when using basis vectors from humans' game and AZ games. A higher score means a closer alignment with the basis vectors arising from AZ's games.

Specifically, for Z h l and Z a l , we find the singular value decomposition to find the basis of the space spanned by AZ's and human games

$Z h l = U h l Σ h l V h⊤ l , Z a l = U a l Σ a l V a⊤ l ,$where the columns of U h l and U a l form an orthonormal basis for the rows of Z h l and Z a l , respectively; Σ h l and Σ a l are the singular value matrices; and the columns of V h⊤ l and V a⊤ l form the orthonormal basis for the columns of Z h l and Z a l respectively.

The novelty score of concept vector v c,l is defined as min

$β i,l v c,l - k i=1 β i,l u h i,l 2 -min γ i,l v c,l - k i=1 γ i,l u a i,l 2 , (7$$)$where β i,l , γ i,l are coefficients estimated using linear regression, u a i,l and u h i,l are columns of U a l and U h l , respectively, and k is the number of basis vectors used. We do not set k as the rank of the matrix because they differ for Z a l and Z h l , and doing so would favour the matrix with the largest rank. Instead, we estimate Equation [7](#formula_10)for various values for k.

Figure [5](#) shows the novelty scores for concepts in layer 19 for 120 concepts. We accept concepts for which the reconstruction error using AZ's basis vectors is less than that of the human game's basis vectors for every k. The light green lines denote the novelty scores for the concepts we accept, and the light blue lines denote the novelty scores for the concepts we reject.

Figure [5](#): Filtering concepts based on novelty scores. Concepts for which the reconstruction error using AZ's basis vectors is less than the reconstruction error using human game's basis vectors for every k are accepted (not filtered). The darker green and blue lines show the average over the accepted and rejected concepts.

Of the remaining concepts after teachability-based filtering, we remove a further 27.1% using the novelty metric.

## Method evaluation

This section includes algorithmic evaluations of our proposed concept discovery method. Table [2](#tab_1) summarises the datasets used; further details on each dataset can be found in §8.4.2, and implementation details for each dataset can be found in §8.4.3. 

## Evaluation of the proposed convex optimisation framework for finding concept vectors

Using the datasets mentioned above, we find concept vectors using the approaches described in §4.1.1 and §4.1.2. While we use layer 19 for all other sections due to its potential novelty according to spectral analysis in §4.2.2, we conduct our evaluation on a few additional layers here: the first latent representation in the policy head (layer 23); and the latent representations in the value head (layer 20 and 21) (See §8.2). These layers are selected due to their proximity to the network outputs -the policy and value estimate.

We first validate our approach by showing the convex optimisation formulation can be used to find the vector representations of a concept using labelled data ( §5.1.1) and show that this can be done efficiently with a small number of labels ( §5.1.2). Next, we further validate our approach by showing that amplifying the concept vector in the latent representation makes AZ's moves to be more similar to the concept ( §5.1.3). This section focuses on validating the convex optimisation framework.

## Do the concept constraints hold for a test dataset?

To evaluate our framework and thus the quality of the vector representations of the supervised concept, we measure the percentage of times the concept constraints hold on the test set (80/20 train/test split), as shown in Table [3](#tab_2). We find that most datasets led to a high accuracy, which is an indication of quality concept vectors. A concept-level breakdown of the accuracy for each dataset can be found in §8.9.1. 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) Openings (per line) 0.99 (0.13) 0.99 (0.14) 0.99 (0.14) 0.99 (0.13) 5.1.2 How many data points do we need to learn a concept?

We can use labelled examples to evaluate the concept vector quality (as in the previous section), but they can be hard to come by in practice. This section shows that our formulation can find concept vectors efficiently using a few examples (for the concept constraints). We measure the test set accuracy (as in §5.1.1) while varying sizes of the training set for two datasets: pieces and the strategic test suite (STS). Here, we discuss the result for the piece dataset. The results for the STS dataset are similar and can be found in §8.10.

As shown in Figure [6](#fig_2), we find that the method reaches close to full-set accuracy with only a few samples -often as little as 10 data points on the pieces dataset. Interestingly, we observe relatively lower performance in the value head (layer 20) than the policy head (layer 23). One potential explanation is that the concept of a specific piece no longer has to exist when estimating the value, which is a scalar -a highly compressed representation of the state of the game. We speculate that it is possible that simple concepts are combined with other concepts. For example, the network may encode the presence of light pieces, i.e., bishops and knights, rather than the presence of bishops. This may explain the relatively low, but not significantly lower performance in layer 20. 

## Does amplifying concept vectors increase concept-related behavior?

We want to (1) determine whether the concept vector captures the intended concept and (2) understand whether the concept influences AZ's output (policy). ( [2](#formula_3)) is important as a concept may exist in a latent representation but not be used by the network for its predictions. To investigate these properties, we use concept amplification. Let z l denote the latent representation in layer l of a chess position x. To amplify the presence of a concept, we nudge the latent representation in the direction of the concept vector v c,l

$zl = (1 -α)z l + αβ ∥z l ∥ ∥v c,l ∥ v c,l ,(8)$where α ∈ [0, 1] and β are hyper-parameters for the size of the perturbation; and || • || is the ℓ 2 norm. We use cross-validation to determine β, and find that β = 0.01 is the best overall (see §8.5).

We report our results for various α in Figure [7](#fig_3) using the STS dataset. The STS dataset includes different types of puzzles grouped according to a strategic theme (such as 'square vacancy'). For each concept, there are 100 chess positions (X) and a solution set S i for each chess position x i ∈ X.

The solution moves require applying the concept given a chess position. As a baseline, we first evaluate AZ's performance by recording the percentage of times the move selected by AZ under the policy is in the solution set:

$A = i 1[argmax π l (z i,l ) ∈ S i ],(9)$where π l (z i,l ) is AZ's policy on the latent representation z i,l , and 1[•] is an indicator function that is equal to 1 if the move selected by AZ is in the solution set S i . We compare the performance difference between with and without concept amplification and report the normalised values ( Ã -A)/A in Figure [7](#fig_3).

As this experiment focuses on the impact of the concept on the predicted move (only the policy output, not the value), we analyse the performance of concepts found in layers 18 and 19 (layers before policy and value head split, see §8.2), and layer 23 (policy head). Each line in Figure [7](#fig_3) represents the results for a different concept quality, where concept quality is measured by test accuracy as in §5.1.1. We observe that amplifying the concept can improve AZ's performance on the puzzles of the concept. Naturally, the quality of concepts influences this; concepts with higher test accuracy lead to a larger performance improvement, suggesting a higher test score is a good proxy for how well the concept vector captures the semantic meaning of the concept.

## Human evaluation

We investigate whether top chess grandmasters can successfully learn and subsequently apply the concepts we discovered (in §4). In particular, we investigate if it is possible to learn these concepts via exposure to a small set of prototypes of each concept. These prototypes are found using a concept vector as in §4.2.1, and then filtered based on a set of criteria aimed at identifying the most relevant high-quality prototypes (see §8.8 for more information). Learning from prototypes is similar to the established approaches to teaching chess. Chess students are often presented with puzzles sampled according to a theme (opening, chess position type, piece sacrifices, etc.), and practicing puzzle solving (i.e., finding the correct next moves) is one way of improving their overall strength and ability (Alisa [Melekhina, 2014)](#b4).

Henceforth, we use puzzles to denote prototypes and their 'solutions' (AZ's selected move). Human evaluation with grandmasters follows three phases, similar to how teachability is measured §4.2.1:

• Phase 1: Measuring baseline performance. Each grandmaster provides solutions for a set of provided puzzles corresponding to a set of concepts. This phase determines the baseline performance: the number of puzzles in which the chess grandmaster gets the continuation correct before the learning phase.

• Phase 2: Learning from AZ's calculations. The same puzzles as in Phase 1 are shown to chess grandmasters alongside the associated AZ's suggested top line based on MCTS calculations for each puzzle. This serves as the simplest way of teaching.

• Phase 3: Measuring final performance. Grandmasters are tasked with providing solutions for a test set of unseen puzzles sampled from the same concepts they have seen in Phase 1. We compute the grandmasters' accuracy on the puzzle test set and compare it to their performance on the puzzle training set in Phase 1 to measure whether their performance changes.

We work with four players, all of whom hold the grandmaster title; one of our participants is rated 2600-2700, and three are rated 2700-2800. At each stage, we also asked the grandmasters to provide a summary of their thought process in free form. As each puzzle is complex, the study participants would likely spend considerable time on each puzzle to analyse the chess positions in great depth. Therefore, to avoid over-burdening the study participants, they were presented with four puzzles (per concept) for 3-4 discovered concepts at each stage of the study. In total, each grandmaster saw 36 to 48 chess puzzles. While there was some overlap between study participants in terms of puzzles shown, different participants were shown different concepts. Given that the participants had a limited amount of time and the high time investment per puzzle, this randomisation allows us to explore a more extensive concept set (compared to showing every grandmaster the same concepts and puzzles). While some chess concepts may be more teachable than others, it is difficult to establish this beforehand without inserting human bias. Overall, we find that all study participants improve notably between phases 1 and 3, as shown in Table [4](#tab_3), suggesting that the chess grandmasters were able to learn and apply their understanding of the represented AZ chess concepts. The magnitude of improvement does not correlate with the chess player's strength (i.e., Elo rating). Below, we discuss factors that may have influenced performance:

## Grandmaster performance

• Variability in difficulty and quality. We filter prototypes (as described in §4.2) to ensure quality and complexity. However, the difficulty and quality of puzzles players received may vary across puzzles.

• Variability in teachability. While we filter based on teachability (as described in §4.2.1), the teachability metric is based on teaching the concept to another AI agent, which may be inherently different from teaching humans.

• Overthinking. We observed that grandmasters often mention AZ's move in Phase 3 in their free-form comments but ultimately did not choose the move (which was not counted as 'correct'). We speculate that this may be because players are more familiar with existing strategies in their decision process, despite having learnt the concept.

## Qualitative analysis of the concepts and illustrative examples

In this section, we provide a qualitative analysis of some concepts with the associated puzzles (plotted using the chess python package [(Fiekas, 2023](#b35))), along with the grandmasters' analysis of the puzzles and the provided AZ suggestions. Any text or notation referring to the chess board or moves is written in this font. When giving direct quotes, we refrain from providing player's names to preserve anonymity. These examples will cover some cases of successful and some cases of unsuccessful learning. We discuss potential sources of differences between humans and AZ in §6.2.3.

Overall, the grandmasters appreciated the concepts, describing them as 'clever' (Figure [8](#fig_4)), 'very interesting' (Figure [16](#fig_2)), and 'very nice' (Figure [18](#fig_12)). Further, they found that the ideas often contained novel elements, commenting that the moves were 'something new' and even 'not natural' (Figures 16) and 10). Often, the grandmasters found the positions were very complex -making remarks such as that it was "very complicated -not easy to understand what to do". Even when seeing AZ's solutions, they remarked that it was a 'very nice idea which is hard to spot' (Figure [22](#fig_0)).

## Concept example: positive knowledge transfer

Here, we explore a concept that possesses both strategic and prophylactic characteristics, involving plans that improve the player's piece placement while restricting the opponent's piece activity. This concept contains an additional element of exploiting tactical motifs and weaknesses, combining strategic and tactical play. We speculate that the concept is learnable to humans, as the grandmaster who trained on the concept puzzles improved their performance between Phases 1 and 3. In Phase 1, they were not able to identify AZ's plan in any concept puzzle (0/4), whereas they successfully identified the correct AZ plan in 2/4 of the concept puzzles in Phase 3.

Figures 8 and 10 show two of the puzzles provided to a grandmaster in Phase 1. In Figure [8](#fig_4), AZ plays the move 9.Bg5; the idea is to provoke 9...h6 before retreating to the square e3, thereby inducing a structural weakness. Instead, the grandmaster chose 9.Be3, a natural move to develop the bishop. After seeing AZ's calculations, the grandmaster acknowledged the strength of provoking this weakness: "9.Be3 allows Black the clever option of playing [9...Nxf3 10.Qxf3] Nh5 [as provided by AZ, followed by] f5 ... but 9.Bg5 is clever as it provokes h6 after which f5 is not great and also [the pawn on] h6 serves as a hook for the pawn advance g4-g5. Blacks' plan of stopping c5 with b6 and playing h4-h5 is interesting too but with g4 anyways White manages to open lines so I would prefer White there."

The idea of playing provocative bishop moves to induce pawn weaknesses is not new and arises in human play. However, the interesting and potentially novel element here lies in the planned strategic queen sacrifice that emerges in one of the critical lines in the MCTS calculations. Consider one such critical continuation: 9.Bg5 h6 10.Be3 O-O 11.Nxd4 exd4 12.Qxd4 Ng4 13.hxg4! Queen sacrifices are among the most beautiful (and rare) tactical motifs in chess, as they go against the established chess principles -do not trade more valuable pieces (i.e., the queen) for less valuable pieces (a knight and bishop). However, here AZ's queen sacrifice is strategicafter sacrificing the queen, White continues developing their pieces. The line continues 13...Bxd4 14.Bxd4, as shown in the left of Figure [9](#). Here, due to the pawn on h6, Black's king is vulnerable, and White is better. Therefore, it was critical to play 9.Bg5, rather than 9.Be3, to make the queen sacrifice feasible by creating this weakness. The puzzle on the right of Figure [9](#) arises in the same line if one opts for immediate 9.Be3 instead, failing to induce h6 first. Without the pawn on h6, White is lost, highlighting this is the critical positional element.

In general, some moves in the AZ's calculations are more important for the concept than others. In our convex optimisation formulation, we do not require the concept to be equally important for every chess position (see Equation [5](#)). This is illustrated in the previous example, where Bg5 is more important for the concept. The next puzzle from the same concept, also given in Phase 1, is shown in Figure [10](#fig_5). This puzzle is of particular interest due to the unconventional plan of AZ -it increases space on both sides of the board, expanding with the pawn move b4 while the king is still on b1. To this end, AZ initiates this plan with the prophylactic move 21.Qd2, preventing the immediate 21...Ne4 by Black.

Unlike AZ, the grandmaster's suggestion in this puzzle was 21.g5, with the intention of following up with 21..Nh5 22.Bxh5 Rxh5 23.Rh3 Rdh8 24.Rdh1. Upon seeing the suggested AZ line starting with 21.Qd2, however, the grandmaster study participant remarked "21.Qd2 is a useful move as it stops Ne4 and protects f4 and can be better placed in case of b4 in the future. One curious line [given by AZ] is 21...Rh7 [22.h5 Rdh8] 23.Rh3 gxh5 24.g5 Ng4 White can just play 25.Rf1 and then focus on getting the b4 [pawn] break, which is not natural."

The unconventional plan of pushing the pawn to b4 with the king potentially exposed on b1 is particularly strong in this chess position (shown in Figure [11](#fig_6)) as it allows White to gain space and open up the chess position under unfavourable circumstances for Black, and claim an advantage. Therefore, the more general rules here are discarded based on concrete analysis. For example 21.Qd2 Rh7 22.h5 Rdh8 23.Rh3 gxh5 24.g5 Ng4 25.Rf1!? Nf8 26.b4!! Qxb4+ 27.Ka1 In this line, we see the dynamic play of AZ: the rooks from f1 and h3, switch over to the b-file to attack Black's king.

So far, the ideas in both positions were missed by the grandmaster. AZ's ideas require unconventional continuations that go against common human chess principles. Both of these observations hint at the existence of super-human knowledge (M -H). Figure [12](#fig_7) shows the puzzle from the same concept provided to the same grandmaster in Phase 3; Phase 3 tested whether the grandmasters had learnt the concept. As before, the puzzles underscore the concept's multifaceted attributes, encompassing its prophylactic characteristics and its integration of tactical and strategic elements.

In the puzzle (from Phase 3) shown on the left of Figure [12](#fig_7), the grandmaster correctly found the move suggested by AZ: 24.Bb3, with the idea of forcing the Black rook on e8 into a more passive position (d8 to defend the pawn) prior to commencing activity on the other side of the board. The idea can be seen in a possible continuation: 24...Rd8 25.Ba4 Na5 26.Rdh1 Nc4? 27.Bh6 and White picks up the pawn on f7.

As in the previous puzzles, AZ uses both sides of the board to optimise piece activity.

In the puzzle, shown on the right in Figure [12](#fig_7), the grandmaster again found the correct idea: 22...Bc6, with the idea of preventing the critical pawn advance 23.f4 because of: 23.f4 Qb7 24.Nc3 exf4 25.Rxf4, resulting in a weakened pawn structure, where White cannot recapture with 25.gxf4 as e4 is hanging. The move 22...Bc6 is both prophylactic and tactical; it prevents White from executing their plan to advance the kingside pawns while improving Black's position by activating the bishop and rook.

The overall improvement of the grandmaster on this concept suggests that they may have learnt AZ's concept, thereby expanding H with (M -H). Other examples of concept learning can be found in §8.1. Understanding AZ's concept using graph analysis and human-labelled concepts. Using a simple way to learn a graph (see §8.7) between concept vectors, we discover strong relationships between existing and discovered concepts to gain further insight into the concept meaning (shown in Figure [13](#fig_8)). Edge weight is influenced by (1) the strength of the relation between two concepts and ( [2](#formula_3)) the frequencies at which concepts co-occur. Below, we further discuss the two concepts with the largest edge weights.

Space. AZ's concept has a strong positive weight on the outgoing edge with the (White-side) space concept. In puzzles where White is to move [(Figures 8,](#)[10 and 12)](#), an important component of the plan is to increase space. In a similar vein, given that the idea is to increase space, which is 'easier'/more likely if the initial value is lower, AZ has a negatively weighted incoming edge with the concept space.

Recapture. We observe positive incoming and outgoing edge weights with the recapture concept. Recall that we have dynamic concepts, which refer to a sequence of states. As such, we postulate that this connection is because the plan may be to recapture/gain material in the subsequent chess positions, as in the puzzles in Figures [8](#fig_4) and [12](#fig_7) (left side).

## Concept example: unsuccessful learning

This concept is related to gaining and playing with a space advantage with positional advantage despite less material. In this section, we provide an example of when a grandmaster found the correct move in Phase 1, but provided an incorrect (i.e., not AZ's choice) move in Phase 3. The puzzle on the left side in Figure [14](#fig_9) was provided to the grandmaster in Phase 1, and they correctly chose the same move as AZ: g4. However, while the grandmaster found the correct move, there were further finesses in AZ's calculations that the grandmaster missed during their time-constrained analysis:

White is a pawn down, [and AZ plays the move I suggested] g4 ... The computer plays with h4-h5 [[and]](#) g4 and the queen on f2 or g2. White is playing on the kingside.[foot_4](#foot_4) Black has no active moves, is playing Rd8 or [R]f8, Kh8. Seems convincing to me. On g6 [AZ] goes [Q]g2 which is nice, it didn't occur to me. I was mainly focused on making h4 work for White. White is not in a hurry, will at some point play g5. Compensation, zero counterplay, and AZ is acting on these premises.

As remarked by the grandmaster, AZ is a pawn down, however, Black's pieces, particularly the Bishop on b7, are placed passively. Instead of prioritising regaining material, AZ focuses on improving the kingside position. While the grandmaster understood the general plan, they missed the intricate idea Qg2, and slowly advancing the g and h pawns.

The puzzle on the right in Figure [14](#fig_9) was provided to the grandmaster in Phase 3. Similar to the previous example, AZ focuses on space rather than recapturing material. It continues with the move 18.a4, which was rejected by the grandmaster on account of 18.Rc5, where Black tries to maintain the material advantage. However, AZ finds the rook on c5 misplaced and continues 18.a4 Rc5 19.a5 Be7 20.a6 where after Bf6? White has 21.d4

The idea behind the pawn advance is to weaken Black's pawn structure.

Overall, the evidence suggests that the grandmaster did not learn this concept. Here, AZ selects stronger moves due to its prioritisation of concepts (e.g., focusing on space and activity). Humans tend to prioritise these concepts differently (e.g., prioritising bringing the king to a safe location as soon as possible). This concept may be inherently difficult and require further examples to learn. Understanding AZ's concept using graph analysis and human-labelled concepts. Figure [15](#fig_10) shows the relationship between AZ's concept and high-quality human-labelled concepts. The graph is dense, and we elaborate on the two concepts with the largest edge weight.

Rook. AZ's concept has an incoming positive edge with the rook (activity) concept. In the puzzles in Figure [14](#fig_9), we observe that white White has active rooks (the rooks on a1 and c5, in the left chess position) or plans to activate the rook (the rook on a1, in the chess position on the right).

King. AZ's concept has an outgoing negative edge with king safety. In the chess positions in Figure [14](#fig_9), the king is less safe than usual. In the left chess position in Figure [14](#fig_9), AZ pushes forward the pawn to g4 (and later the pawn to h4) around the king -thereby removing some of the king's defenders. In the right position, White does not castle to improve the king's safety but instead leaves the king in the centre.

Further examples of concept puzzles can be found in Section 8.1.

## Differences between humans and AZ

In this section, we share a few observed differences between the grandmasters and AZ and speculate where they come from. While we do not have definitive answers, the discussion may lead to further research.

The qualitative examples suggest that AZ has different priors over the relevance of concepts in a chess position than humans. Human chess players formulate and adopt heuristic chess principles to inform their analysis, predisposing them to biases that influence which concepts they deem relevant for specific chess positions. An example is the three 'golden rules' of the opening: control the centre, develop your pieces, and bring your king to safety [(Hansen, 2021;](#b49)[Brunia and van Wijgerden, 2021;](#b16)[King, 2000)](#b58). Consequentially, in opening, humans may focus on moves that align with these guidelines. Instead, AZ is self-taught and does not seem to have the same priors over chess concepts as humans. We believe this lack of prior allows AZ to be more flexible -it can apply concepts to various different chess positions and change plans quickly. In essence, AZ formulates its own priors over the relevance of chess concepts for a given chess position. Examples of this behavior are that AZ plays over the entire board, as opposed to focusing on a specific side (see, e.g., [Figures 16,](#)[17,](#)[12,](#)[and 19)](#); places less importance on the material value of pieces, and prioritises space and piece activity (see, e.g., Figures [9 or 14](#fig_9)). This may result in the super-human application of concepts, and new concepts.

One may ask where do the differences between AZ's and humans' play come from? We conjecture that they may arise from differences in objectives and capabilities. AZ learnt to play chess against itself. As such, AZ assumes optimal play and information symmetry. [6](#foot_5) On the other hand, humans play chess against other humans and, therefore, may assume information asymmetry and imperfect play. This leads to a difference in behaviour: while AZ focuses on finding the best move, human chess players often make practical choices. Humans' choices do not always increase the expected outcome against an optimal opponent (their choices may even slightly decrease the expected outcome) but may increase their odds against another human. For example, in drawn chess positions, humans may try to complicate the chess position or opt for continuations where the best moves are less clear-cut in hopes that their opponent makes a mistake (see Figure [23](#fig_16)). However, AZ will try to find the optimal plan, disregarding aspects such as complexity. Therefore, AZ's play may be fundamentally different and better reflect conceptually relevant plans in a chess position.

Another difference between humans and AZ's play is the role of time. Humans have limited energy[foot_6](#foot_6) and time allocation for a game. In chess positions where humans are better, they often simplify the chess position to try to secure the win as quickly as possible and minimise risk (see, e.g., the grandmasters' chosen moves in Figures [22](#fig_0) and [21](#fig_15)). However, AZ does not care about how quickly the game finishes. The training loss function does not have a penalty term to encourage winning as quickly as possible. As a result, it has a different treatment of time. This results in sometimes choosing slow strategic wins (as can be seen in the chess positions in Figure [21](#fig_15)). While the lack of time constraint may lead to super-human concepts, it also may result in complex concepts that are difficult for humans to learn.

Naturally, AZ and humans have different computational capacities. As a result, AZ can opt for more computationally expensive moves and, therefore, defend complicated chess positions where humans might be more hesitant. In terms of playing style, AZ will often opt for what it believes is the optimal move, while humans have a more limited computing budget and may opt for a safer move. In chess, safer is used to describe continuation where there is less probability of making an incorrect move. Sometimes, humans may even play a slightly suboptimal move to minimise the risk. We see this phenomenon in Figures [22](#fig_0) and [23](#fig_16). While computational capacity cannot be transferred, it may still lead to super-human concepts, as AZ can find new ideas that can still be taught to humans.

## Conclusion

Our research represents a first step toward understanding the potential of human learning from Artificial Intelligence (AI). In this work, we focused on AlphaZero (AZ) -an AI model that learned to play chess at a super-human level through self-play without prior knowledge or human bias. Through spectral analysis, we show that AZ's games encode features that are not present in human games, providing evidence for the existence of super-human knowledge. To extract knowledge from AZ's representational space, we developed a framework to uncover new chess concepts in an unsupervised fashion. We discover unsupervised concepts by leveraging AZ's training history to curate a set of complex chess positions. We ensured each concept was informative, by verifying that the concept can be taught to another AI agent, and novel, through a spectral analysis of human and AZ games. Communicating novel concepts requires a common language between humans and AI. We bypass the need for this language by creating puzzles for each concept.

We collaborated with four world-top grandmasters to (1) validate the human capacity to comprehend and apply these concepts by studying AZ's concept prototypes and (2) improve our understanding of the differences between AZ's and humans' chess representation space. All four grandmasters improved their performance after learning concepts compared to baseline performance. We speculate that the differences between AZ and humans may stem from (1) prior biases over concepts, including their perceived applicability, importance, and how they can be combined with other concepts. For example, AZ shows a reduced emphasis on factors such as material value and is more agile in switching between playing on different sides of the board. (2) a difference in the motivation and objectives when playing chess; AZ is trained to accurately evaluate the current chess position, assuming optimal play. There are several aspects of the work that could be further explored. In our work, we found a subset of all possible concepts. For example, we limited our investigation to linear sparse concept vectors. However, other concepts may be discovered in the form of non-linear vectors. Additionally, the current work focuses on finding a single concept to explain a plan. However, a plan may contain multiple concepts. As such, an interesting aspect to further explore is how these concepts relate to each other and influence the plan.

Further work could also explore the optimal conditions for humans to learn novel concepts. We allotted a fixed time budget for grandmasters to assimilate the concepts. However, it is plausible that an unlimited time budget could yield more profound and more intricate insights. In our research, we provided grandmasters with part of AZ's Monte Carlo Tree Search (MCTS), in which the rollouts are motivated by the concept, as an explanation for the concept. We used this approach to keep the explanations as familiar and simple as possible. Nonetheless, it would be interesting to augment this phase with an interactive component: e.g., for each puzzle, humans can actively engage with AZ by playing moves and asking AZ what its response is. This interactive element would allow humans to investigate counterfactual scenarios, allowing for a deeper understanding why AZ did not select their solutions or approaches. 13.Qc2 Rh6 14.0-0-0 b5!?.

The resulting chess position is shown in the right chess position in Figure [17](#fig_11). Both continuations are unorthodox; conventional human-designed chess principles emphasise completing piece development, securing the king's safety and maintaining the bishop pair over trading it for a knight, as outlined in [King (2000)](#b58). However, AZ deviates from these principles favouring a continuation that prioritises a strong control of the centre, space, and piece activity. Another puzzle from the same concept is shown in Figure [18](#fig_12), given in Phase 3. Here, the grandmaster found the best continuation according to AZ: 10.Ndf4 threatening d5. The ideas are 10...a6 11.Qa4 10...Bd7 11.Bc4 gaining control over the square e6 10...d5? 11.Qb3 and the pawn on d5 is lost 10...Bf6 Black's best option 11.Bxf6 exf6 (12.d5? a6 13.Qa4 then Black has the intermediate move 13...Re4!) 12.Ng1 a6 13.Ba4 Bd7 14.Nge2

The knight manoeuvres 10.Ndf4 and 12.Ng1 are against the common rules which advocate for finishing piece development and bringing the king to safety, above further improving a developed piece [(King, 2000;](#b58)[Brunia and van Wijgerden, 2021;](#b16)[Hansen, 2021)](#b49). As in the previous puzzle, AZ prioritises controlling the centre and piece activity.

The grandmaster missed the idea 12.Ng1, although did appreciate it remarking that "Ng1 [is] quite nice actually, [knight] on h3 is gone, and then we probably go for h4 at some point."

## Informative puzzles

In some puzzles, we observe informative manoeuvres from AZ. We provide a few examples here. In the chess position in Figure [19](#fig_13), AZ plays Qc1 with the idea of manoeuvring it to c4. Most human chess players would find this idea unconventional, as White's pieces seem to be active on the kingside. However, there is no way to break through Black's position. AZ's idea is the only way to maintain an advantage. The plan is to re-position the pieces to the queenside, with ideas like Re1, Ba4 and e5.

When analysing this position (and only spending a fixed amount of time), the grandmaster misted the idea and opted for Rxh5, which was the only way for White to equalise according to the grandmaster. We speculate that the difference between AZ and humans is because AZ is more flexible in changing its plan. In this position, humans are likely primed to continue playing on the kingside. The puzzles in Figure [20](#fig_14) correspond to the same concept. In both positions, AZ uses tactics to obtain a positional advantage by maintaining a space advantage. In the left puzzle, the best move is 21.Nc5, which stops Black from advancing their c5 pawn to control the center. The tactic behind the idea is 21.Nc5 dxc5 22.Bxf6 Bxf6 23.Rxd7.

In the puzzle on the right of Figure [20](#fig_14), Black plays 18.Rad1 which is prophylactic against 18...b5 as White has tactic 19.c5 dxc5 20.Nc6. Both of these continuations were found by the grandmaster, who appreciated the importance of Nc5. The grandmaster explained that they took a long time to analyse this position as they found it complicated. While recognising Black's threat of c5 (followed by Bc6 or Nc6), they first explored several other options, including moves such as a3, c5, Bf2 or Na4. However, after exploring other moves, they found Nc5. The grandmaster commented that the idea was very strong and 'by far the best move'.

In this puzzle, the grandmaster opted for the tactical move Bxh7 but also considered quieter moves such as Re3, Be4 or Be1. When analysing this position, they commented "This is tricky, if White decides to protect the pawn, it's clearly better due to the weakness in Black's structure, but somehow getting addicted to more forced attacking lines. Bxh7 is hard to figure out, maybe Kxh7 [followed by] Qh5-Qf7 then Rd3, Bxg2 ... [I] didn't calculate until clear much better position, but thought even with some play, h-file the long problem, maybe certain chances [to win]." However, this sequence ends in a draw, and AZ instead opts for f3, maintaining the advantage for White. When reading AZ's analysis, the grandmaster commented: "Wow, it's a completely positional play. Well, my decision to [sacrifice] is too emotional. To be honest, this choice f3 ... [makes] sense as Black does not have any breakthrough idea, so if White successful [in] controlling both c5 and e5 square then its clearly much better. [My conclusion is] technically strong but again within [the] human perspective."

Here, we see a difference in style between humans and AZ. AZ opts for a slower, longer-dominance play in chess positions where grandmasters tend to consider more forcing sequences.

Human vs AI Play: AZ opts for less forced lines than humans 

## Difficult or non-instructive puzzles

There may exist concepts that are intrinsically hard to understand and learn for human chess players due to differences in ways of abstract thinking, overall capabilities, and their computational Figure [22](#fig_0): AZ takes on more risk than humans due to computational capacity. White is to move.

AZ's calculations: 31.Qa1 f4 (31...Qe7 32.Re8 Qf6 33.Qxf6 Rxf6 34.Rd8 Ne5 35.Ne8) (31...h4 32.Re8 f4 33.exf4 gxf4 34.Bxe4 Nxe4 35.Rxe4 Qf5 36.Qe1 Ne5 37.Qe2 hxg3 38.hxg3 fxg3 39.fxg3) 32.exf4 gxf4 33.Ne6 Nxe6 34.Bh3 Qb5 35.dxe6 Rg7 36.Ra5 White is slightly better budgets. The example in Figure [22](#fig_0) highlights that humans and AI have different computational capacity, allowing AZ to make moves that appear risky to humans.

In the puzzle in Figure [22](#fig_0), AZ plays Qa1 to activate the queen. This move requires calculating carefully to ensure that Black has no counterplay due to an attack on the kingside. As such, humans may perceive this move as risky, and it was not chosen by the grandmaster. When seeing AZ's calculations, they remarked "I would be really worrie [[d]](#) to keep the queens on the board because of the threat with f4 but AZ has a tactical solution. 33.Ne6 Nxe6 [34.]Bh3 is a very nice idea which is quite hard to spot. Black should probably stay still and try to hold with 31...h4 [[32]](#).Re8 Re7."

Here, we see that humans are more risk-averse than AZ. This is logical, given that AZ has a much larger computational capacity and can calculate more/deeper than humans can to more accurately assess the chess position (and thereby take on less risk).

## Differences in motivation in human and AI play

The next example shows that AZ and humans play chess with different motivations. AZ forces the draw with g4. Upon seeing AZ's calculations, the grandmaster commented: "... this is a very clear and important theme to understand. So, g4, the move it proposes, in a practical sense it's a very big move, because you see, in such situations, the engine already knows the final result. For engine it doesn't matter which move it plays because it calculated it's a draw, but g4 is basically forcing it. After g4 Black has no winning chances, but otherwise I have a feeling that after Black plays let's say Qd2, it's not ... easy practically for White to make the draw. For an engine it's ok, but practically no one would play it because g4 is basically offering a draw -and with other moves Black is running zero risk, yet has practical chances to win the game if White makes a mistake. An engine doesn't understand the concept of practical play -while this is a draw, it's not an easy draw for White. g4 is one of many moves leading to a draw, but in a practical sense the worst one as it gives Black zero chances to win. So that is my understanding, that it's not the objective best move. Practically definitely a wrong move."

This underscores a fundamental difference between AZ's playing style and human's playing style. AZ was trained to obtain the expected outcome without an explicit term in the loss function, encouraging it to win. The incentive to find the best move comes from the exploration and move selection criteria in MCTS. Further, AZ assumes an equally strong opponent. For AZ, there is no difference between different equalising moves, even if one move requires a much more precise sequence of moves to equalise. In contrast, humans assume that their opponent may make suboptimal moves. As human chess players play competitively (i.e., their goal is to maximise the outcome), they will try to leverage these chances. This example highlights how the difference in objectives and assumptions may lead to different behaviour of AZ and humans when playing chess.

## Shortcomings of method for generating prototypes

In this puzzle, AZ chooses Rd1 whereas the grandmaster wanted to play Re1 or Ke1. When seeing AZ's calculations, the grandmaster commented: "Drawing position? At first trying to find winning moves for White, but really didn't see any plan to make improvements. In the meantime, considering the possibility for Black to push h pawn to h3, maybe tiny chances, its better to plan Ke1-Qf1-Qf3 at the beginning, or moving the rook to e1 with the idea Re7, forcing ... Ra1 check then White rook retreat to e1, ... [draw by] repetition." This puzzle can be seen as a shortcoming of our method for finding prototypes. We only filter positions based on the criteria described in §8.8; however, this position does not fall under one of our categories. This puzzle is not informative for humans as there are many other viable options. As such, it is more difficult to understand the concept from the sequence of moves.

## Background: how humans and AZ play chess

In this section, we provide further intuition on how humans play chess, and how this relates to AZ's system. This section provides context as to why concepts should explain the policy value network and MCTS to provide a holistic view of chess. 2. Based on step 1, a chess player will find a couple candidate moves -actions they could play in the chess position.

3. For each move, they may calculate a likely continuation -i.e., what is the likely sequence of moves to follow?

This process loops until the player has considered all candidate moves, calculated the relevant move sequences, and determined the optimal trajectory.[foot_7](#foot_7) To play well, chess players must understand the important features of a chess position and calculate move sequences to understand the correct evaluation of the chess position. However, there are several different types of chess positions (e.g., endgames or attacking chess positions) where principles alone are insufficient to determine the optimal continuation, and calculation is necessary.

AZ uses a similar approach. In a given chess position, AZ extracts features using the layers in the policy-value network and outputs a policy and value estimate. The policy weighs the different possible actions, and the moves with the most probability mass can be interpreted as the candidate moves. Next, the policy is passed on as an input to MCTS, a search algorithm that calculates the optimal move. By drawing parallels between AZ's system and how humans play chess, we highlight 

## Further details: convex optimisation formulation for concepts

This section provides further details on our convex optimisation framework to find concepts. Subsection 8.4.1 describes how we set the dynamic concept hyperparameters. §8.4.3 explains how the convex optimisation formulations were implemented for the different datasets.

## Dynamic concept hyperparameters

As AZ learnt to play chess through self-play, the latent representations alternates between the player's and the opponent's perspective within a rollout. For concepts, we may want to find a concept that influences a single player or both players. Therefore, we consider two different ways of using rollouts. For a rollout {z t } T t=0 , we use every other latent representation, i.e., {z 2t } floor(T /2) t=0

, to find concepts for a single player (i.e., for the player to move, or their opponent). These concepts are referred to as 'single'. We use every latent representation to find concepts for both players, i.e. {z t } T t=0 . These concepts will be referred to as 'both'. For the rollout depth used in our dynamic concept formulations, we consider T = 5 and T = 10. For the subpar variations, we required AZ to estimate a minimum value difference of 0.20 and/or a visit count difference of 10% (of the most visited move).

## Datasets

We use labelled and unlabelled datasets to construct concept constraints for the convex optimisation formulation. We use a different convex optimisation formulation for each concept we want to discover. Therefore, for each concept, we need a set of chess positions X + that contains a concept. We leverage educational resources designed to teach humans chess, which contains themed chess puzzles: positions designed to encapsulate single important concepts and test the degree to which a chess player can deploy them in realistic situations. We go beyond chess puzzles, delving into chess positions arising from different openings and searching for AZ-specific concepts to find new ones.

Factors in datasets. The datasets we use vary in the following ways:

• Concept type e.g., the concepts can be strategic or tactical (see Wikipedia contributors (2023b) and Wikipedia contributors (2023c) for a further explanation on strategy and tactics), or correspond to different periods of the game (opening/middle game/endgame).

• Degree of human knowledge some datasets contain human games while others contain AZ's games. The degree of human knowledge (or style of play) may vary across the datasets.

• Complexity some concepts are elementary (i.e., can be learned by beginner-level chess players), whereas others are highly complex (i.e., can only be understood by top-level grandmasters).

Below, we briefly describe the different data sets used 1. Piece dataset We construct a labelled dataset that contains paired chess positions that either contain or do not contain a chess piece. These simple concepts indicate the presence of a piece -queen, rook, bishop, knight, or pawn. We exclude the concept of 'king' as this piece is always present in a chess game.

## Stockfish dataset

We construct a labelled dataset for each concept in the Stockfish engine [(Stockfish Community, 2018)](#). Each dataset contains two sets of chess positions -with and without the concept. Humans use Stockfish concepts in chess position evaluation (such as piece placement, open files, etc.)

3. STS puzzles dataset This is a labelled dataset containing 15 different categories (see [Corbit et al. (2014)](#b22) for further details). These puzzles capture different types of strategic themes.

4. Chess openings (e.g., concepts in Grünfeld vs. Najdorf vs. Queens Gambit) Openings (i.e., the first couple of moves) determine the pawn structure that acts as the backbone of a chess position -it determines crucial aspects of the game, such as optimal piece placement, square weaknesses and strengths, and more generally, plans. We use the Encyclopedia of Chess Openings (ECO) to create a set of labelled chess opening positions (LiChess, 2023). For each chess position, we use AZ's MCTS rollouts to construct sequences of the opening moves.

## AZ self-play games

We construct an unlabelled dataset. We sample 30, 000 chess positions from AZ's games. To ensure that the chess positions we analyze contain complex concepts, we only select chess positions where two versions of AZ at different points in training select a different move. These versions differ by 75 Elo points.

Table [5](#) briefly summarises the different datasets.

each concept. Then, using the formulation for static concepts, we found the concept vector using the following formulation

$min ||v c,l || 1 (13) such that v c,l • z + i,l ≥ v c,l • z - j,l ∀i : x i ∈ X + , j : x j ∈ X -.(14)$Strategic. We use the strategic test suite to extract strategic concepts [(Corbit et al., 2014)](#b22). In this dataset, there are 15 different concepts. We omit the 12th concept due to irregular data formatting.

The remaining concepts are undermine, open files, knight outposts, square vacancy, bishop vs. knight, recapture, offer of simplification, fgh-pawn, abc-pawn, simplification, king activity, pawn push center, 7th rank, avoiding an exchange (see [Corbit et al., 2014, for further details)](#).

Each concept has a set of 100 chess positions, X, and the solution (move) requires applying a strategic concept in each chess position. In our analysis, we run MCTS on the chess position and store the search statistics. We store the optimal trajectory for each chess position x i , denoted as X + i,≤T , where T is the maximum rollout depth. Similarly we select a subpar rollout X - i,≤T . To find the subpar rollout, we find a rollout in the MCTS tree with the most visits where (1) the estimated difference in value is at least 0.2, and (2) the visit difference is at least 10%. As in the main text, for X + i,≤T and X - i,≤T , we find the corresponding latent representations in layer l: Z + i,≤T and Z - i,≤T , respectively. Then, we can find the concept vector using the dynamic concept formulations (see

$§4.1.2) as follows min ||v c,l || 1 (15) such that v c,l • z + i,t ≥ v c,l • z - i,t ∀t ≤ T, i : x i ∈ X.(16)$In our analysis, we use a maximum depth of T = 5.

Openings. For the openings, we focus on the English, Dutch, Scandinavian, Najdorf, French, Tarrasch, Winawer, Ruy Lopez, Grünfeld, King's Indian, Queen's Gambit Declined and Queen's Gambit Accepted. We consider a subset of all openings due to computational costs. For each opening, we use the encyclopedia of chess openings to find relevant starting chess positions to construct X + (see the encyclopedia of chess openings). For each chess position, we ran MCTS to obtain the search statistics and used the formula in Equation 5 to find a concept for each opening. Further, we create a concept set X + for each ECO index belonging to one of the aforementioned openings.

AZ Games. We simulated 1, 308 games. Conditional on hardware, AZ's play is deterministic (after training). To create diverse games, we sample different starting chess positions. We use the ECO to find starting chess positions (see the encyclopedia of chess openings) and we simulate games from these initial chess positions. For each chess position, we ran MCTS to obtain the search statistics.

We leverage AZ's training history to find interesting chess positions. We select a version of AZ that is 75 Elo points weaker than the final model. To construct X + , we run through each game and select chess positions where the two AZ versions choose a different move. Using this approach, we constructed a dataset with 3, 974 chess positions and used the formulation provided in Equation [5](#)to find concept vectors. Other Implementation Details. We solve the convex optimisation problem using a standard solver in the package cvxpy [(Diamond and Boyd, 2016;](#b29)[Agrawal et al., 2018)](#b2).

## Beta hyperparameter tuning

In this section, we provide the validation values for the concept amplification experiments in §5.1.3.

For the values in Table [6](#tab_5), we randomly chose 2 concept sets from the STS dataset and estimated the amplification results for different values of β. Overall, we observe that the results are not very sensitive to the value of β.

## Teachability implementation

In this section, we provide further details on the implementation of teachability. We assume that we have a concept vector v c,l that was found in layer l. To construct prototypes, we sample 30, 000 chess positions from AZ games. For each concept, we find chess positions for which v c,l • z i,l is in the top 2.5%, and store these as prototypes {x 1,l , . . . x n,l }. For dynamic concepts, we found prototypes using MCTS statistics. For each chess position, we ran MCTS. Next, we found the optimal and subpar line (similar to the convex optimisation formulation constraint). For a prototype x i , we required that v c,l • z + i,t,l ≥ v c,l z - i,t,l , ∀t.

We use AZ as the teacher network. For the student network, we want to find an agent that does not know the concept but does understand chess. As chess is a complex game, we cannot train an agent from scratch (using only the curriculum). Instead, we take a training checkpoint of AZ and estimate its knowledge of the concept using

$T = xi∈X 1[argmax(π s (x i )), argmax(π t (x i ))],(17)$where π s () is the student policy and π t () is the teacher policy. This measures how often the teacher and student agree on the best move. We select the student as the latest checkpoint for which the top-1 policy overlap is less than 0.2. We use the prototypes as a curriculum. We train the student network by minimizing the KL divergence between the policies, xi∈Xtrain KL[π t (x i ), π s (x i )]. We use the Adam optimiser [(Kingma and Ba, 2014)](#b59) with learning rate 1 × 10 -4 . As we have several concepts, we train each student for 5 epochs, as we find that this is sufficiently indicative of performance if we train for longer.

We benchmark the performance by comparing it to a student network trained on a random concept and evaluated on (1) the concept data and (2) the random data. A random concept is a vector with the same shape as the latent dimension and sampled from a standard normal distribution.

Graph Verification. To verify the graph, we run an experiment to test whether two concepts with an edge contain related knowledge. If a model learns a concept c, this should improve the model's performance on another related concept c e more than on an unrelated concept c n . For a concept c, let C e denote the set of concepts with an edge and C n denote the set of concepts with no edge in the graph. Following the teachability procedure in §4.2.1, we train a student model using prototypes of concept c. Next, using Equation [6](#), we evaluated student's performance on:

• concepts with an edge C e ; we denote the performance by T c,ce

• concepts without an edge (to v c,l ) C n ; we denote the performance by T c,cn .

If the graph correctly captures the relationships between concepts, then we expect that a model trained on c performs better on C e than C n , i.e. T c,ce > T c,cn . However, we must consider that the concepts in C e may be inherently easier to learn than those in C n . To account for this, we train a model on a random set of data (as in §4.2.1), which we use as a benchmark. We estimate the performance of this model on C e and C n , which we denote by T r,ce and T r,cn , respectively.

If our graph accurately captures the underlying relationships, we expect that T c,ce -T r,ce > T c,cn -T r,cn . Figure [28](#fig_20) shows training curves for two concepts each of which with 5 related concepts and 5 unrelated concepts. We find that the performance on related concepts is significantly better than unrelated concepts at a 5% significance level. This suggests that the graph structure may accurately capture the relationship between concepts.

## Human experiments

Recruitment. We recruited four chess players based on their Elo rating. All the participants hold the grandmaster title; one of our participants is rated 2600-2700, and three are rated 2700-2800.

Experiment Instructions. Each grandmaster was asked to spend two hours on Phase 1, one hour on Phase 2 and two hours on Phase 3. We ask the grandmasters to provide (1) the move they would play or their ranked candidate moves and (2) a thought record -the idea is to capture any thoughts about the chess position. The grandmasters were sent the chess positions to solve at home, in their own time. We explained that the chess positions could vary in nature. The chess positions could be better, equal or worse for the player to move. Similarly, the continuation may require calculation or finding a general plan.

Evaluation. We evaluated how often the grandmasters find the move selected by AZ. Note that if grandmasters made the right move but incorrect reasoning appeared in their free-form comments, we counted this as an incorrect answer.

Prototype Filtering. To ensure the quality of the prototype selection, we filter them according to the following criteria:

• Quality of the value estimate. We ensure that the AZ value estimate is close to the correct assessment of the prototype by running self-play and computing the expected score. If the expected score and the value estimate are in concordance, the prototype chess position is kept, otherwise, it is discarded from consideration for the human study.

• Chess position complexity. For the concepts to be sufficiently complex to be of interest to the top grandmasters, we use prototypes where the policies of the 512K step checkpoint and fully trained models disagree on their top move. The 512K checkpoint model is 75 Elo points weaker than the final model, and therefore, if the policies differ, AZ learned the continuation during a late stage of training when it was already strong.

• Solution complexity. We manually remove trivial chess positions where the solution is theoretically known (e.g., present as an entry in pre-computed tablebases such as the Syzygy tablebase (Bojun Guo, 2023)). Tablebases are sets of chess positions where the ground truth evaluation (outcome with ideal play) is known.

• Reliability. We reject chess positions where AZ's limited compute budget may lead to an unreliable chess position evaluation (i.e., where we observe abrupt changes in the predicted outcome). Therefore, we require that the evaluation stays approximately consistent (i.e., the predicted outcome (win/loss/draw) does not change) throughout the provided lines.

We did not filter based on the difference in the value or the policy probability mass of the optimal move compared to other moves. The reasons are that (1) AZ's value estimate is noisy, and (2) either filter could remove potentially interesting chess positions. For example, requiring a small entropy and large value estimate difference (between moves) would result in predominantly tactical chess positions, thereby omitting interesting strategic puzzles.

AZ's calculations. In the second stage of the human experiment, we provide part of the MCTS statistics. We ran MCTS without a depth limit for a maximum of 10, 000 simulations. We pruned the MCTS tree to avoid providing too many lines, or lines that were insufficiently explored. We provided the main line (most frequently visited), second and third moves, ranked according to visits. We did this for depth t ≤ 2.

![Figure 2: Example of a concept prototype. Most chess players would opt for Rxh5, however, AZ plays Qc1, with the idea of regrouping the pieces to the queenside. Further details can be found in §8.1.]()

![Figure 4: Teachability: AZ Concepts. The y-axis shows how often the student and teacher select the same move (normalised version of Equation 6), and the x-axis shows the training time step. The dark dotted lines show the level of a training checkpoint at which AZ obtains the same level on the concept set as our student. Each plot is a different concept found in layer 19 (top) and in layer 23 (bottom).]()

![Figure 6: Sample efficiency of convex optimisation framework, averaged across 10 seeds, for the bottleneck layer (19), value head (20) and policy head (23).]()

![Figure 7: Performance improvement in solving puzzles using STS dataset across different α values (xaxis). Layers 18 (left) and 19 (centre) are before value/policy head split, and the policy head (layer 23) (right). Each line indicates a set of concepts with different quality (measured by test accuracy as done in §5.1.1). Higher quality (high threshold, orange line) achieves the highest improvement.]()

![Figure 8: Puzzle of shown in Phase 1. White is to play.]()

![Figure 10: Second puzzle shown in Phase 1. White is to move.]()

![Figure 11: Expanding on the critical line in the puzzle shown in Figure 10. White has just played b4.]()

![Figure 12: In the left puzzle, White is to move. In the right puzzle, Black is to move.]()

![Figure 13: Graph of AZ's concept in Figures 8, 10 9 and 12 between AZ's (white), strategic (green) and Stockfish concepts (purple). The information in the parentheses means the layer in which the concept is found, and w = white, b=black, eg= endgame, mg = middlegame, ph=phased. The edge color denotes the edge weight.]()

![Figure 14: In both puzzles, White is to move.]()

![Figure 15: Graph of AZ Concept in Figure 14 between AZ's (white), strategic (green) and Stockfish concepts (purple). The information in the parentheses is the layer number in which the concept is found. w = white, b=black, eg= endgame, mg = middlegame, ph=phased. The edge color denotes the edge weight.]()

![Figure 17: Digging Deeper into the Concept in Figure 16. In both positions, White is to play.]()

![Figure 18: Concept Puzzle 2: White is to play.]()

![Figure 19: Queen Manoeuvre. White is to move.]()

![Figure 20: Positional Tactics. White is to move in both positions.]()

![Figure 21: White is to move.]()

![Figure 23: AZ simplifies the chess position for the draw whereas humans would continue to try to win. Black is to move.]()

![Figure 24: White is to move]()

![Figure 26: Simplified Summary of How Humans Play Chess]()

![Figure 27: Simplified Summary of How AZ Plays Chess]()

![Figure 28: Teachability score on related and unrelated concepts]()

![Rank of latent representation of Human Games and AZ's Games]()

![Datasets Summaries: from concepts more known to humans (top rows) to AZ (bottom rows). S denotes strategic and T denotes tactical.]()

![Evaluation: % the concept constraints hold on test data. The standard error is shown in parentheses.]()

![Improvements in grandmasters' performance. The percentage scores are the % of puzzles that the grandmaster solved correctly (according to AZ's solution). # Puzzles is the number of puzzles shown to the grandmaster in total.]()

![Hyperparameter selection for β Layer 1858.42 58.58 58.50 58.42 58.17 58.17 57.42 56.75 55.00 Layer 19 60.17 60.42 60.42 60.17 60.08 60.17 59.75 58.00 55.83 Layer 23 58.83 59.42 59.33 58.83 58.58 56.25 53.92 47.75 40.33]()

This difference is not due to variance, as the human games have a larger span in input space.

We show how to handle non-binary concepts in §8.4.3.

A larger inner product corresponds to a higher cosine similarly.

For reference, the expected score of a player rated 75 Elo points higher is 0.68, where 1 point is given for a win, 0.5 for a draw, and 0 for a loss. 75 Elo points is a large Elo difference, particularly at AZ's playing strength.

For readers -the queenside refers to the left side marked a-d and the kingside refers to the right side of the board marked e-h.

With information symmetry, we mean that Black and White have the same general knowledge and perform the same calculations in a given chess position.

Classical time control (see match time controls in FIDE, 2019) chess games take hours.

This is a simplified model -in practice, other factors such as time are important.

