# Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence

## Abstract

## 

In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning data set construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured to structured data set, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8×H100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B) and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.

## Introduction

In the highly regulated domain of bio-manufacturing quality, there is a growing need to convert legacy production records into structured digital formats for compliance and analysis. Biomanufacturing has historically been 'steeped in a paper culture', and even incremental moves toward electronic batch records are significant steps in industry digitalization [[Lab25]](#b15). A key prerequisite of this digital migration is schema adherence: AI systems, such as large language models (LLMs) used to transcribe or summarize production logs, must output data that fit a predefined schema exactly. Any deviation (missing fields, incorrect format) could violate data integrity standards and render the generated records unusable for regulatory compliance [[ea24c]](#). This introduces a critical challenge: While modern LLMs are extraordinarily powerful in free-form text generation, ensuring that they produce strictly structured, schema-valid outputs is not trivial.

LLMs by default generate text probabilistically, with no built-in guarantee of conforming to a given format [[ea25b]](#). This unpredictability poses risks when structured output is required for machine consumption or auditing. Empirical studies have found that even state-of-the-art models can fail to consistently follow format instructions -success rates in producing correct JSON, for example, can vary widely from 0% to 100% depending on the task complexity and model used [[ea24a]](#). Such inconsistency is problematic in any setting, but in regulated bio-manufacturing, an output that does not exactly match the schema (e.g., a misformatted timestamp or an extra delimiter) might lead to compliance issues or require costly manual correction. Developers report that substantial effort is spent on prompt tuning and post-processing to coerce LLMs into the desired format [[Sou24]](#b16). From a user perspective, unreliable formatting undermines trustconstraints help prevent nonsense or hallucinated fields, thereby ensuring the output remains credible and useful [[Sou24]](#b16). In short, structured output generation is both a technical and a governance challenge: the model must be reliable in content as well as form.

## Relevant Work

Researchers and practitioners are exploring several approaches to address these challenges and enforce schema adherence in LLM outputs. Key strategies include:

## Supervised Fine-Tuning

An LLM can be fine-tuned on domain-specific data with the required output schema, so it learns to produce the correct structure. Fine-tuning on curated input-output pairs (e.g., historical records mapped to structured entries) can significantly improve format fidelity [[ea24b]](#). However, this approach is resource-intensive -training large models on specialized data is complex and costly, often requiring techniques like low-rank adaptation to be feasible [[ea24b]](#). Fine-tuning also risks making the model too domain-specific or rigid outside the training distribution.

## Reinforcement Learning with Human Feedback (RLHF)

RLHF has proven effective in aligning LLMs with human instructions and preferences [[ea25c]](#). By training a model with feedback signals that reward correct adherence to the desired format, one can encourage structured outputs. Notably, the instruction-following abilities of models like ChatGPT/GPT-4 are largely attributed to such alignment techniques [[ea25c]](#), enabling them to obey fine-grained formatting requests (e.g. "output as JSON"). In regulated settings, RLHF could incorporate compliance-specific criteria into the reward model. The downside is that RLHF requires extensive high-quality feedback data and careful reward design; even then, smaller open-source models often still lag behind in format obedience despite alignment efforts [[ea25c]](#).

## Constraint-Based Decoding

Rather than relying on the model to choose the right format, constraint-based methods force compliance by integrating schema rules into the generation process. Techniques like grammar-or regex-guided decoding intercept the model's token output, only allowing continuations that keep the output valid according to a formal schema [[ea25b]](#), [[ea24d]](#). This guarantees 100% schema adherence by construction. Recent frameworks implement fast, non-invasive constrained decoding that can guide LLMs to produce, for example, JSON that matches a given schema exactly [[ea24b]](#). Industry adoption of these ideas is rising; for instance, OpenAI's API now accepts developer-provided JSON schemas and assures that the model's response will conform to them [[ea25b]](#). The trade-off here is potential complexity in setup and slight inference latency overhead, as well as the challenge of designing schemas that are neither over-nor under-constraining. Nonetheless, when correctness is paramount, constrained decoding is a powerful approach.

## Prompt Engineering

The most accessible technique is to craft the input prompt in a way that strongly cues the desired structure. This can involve giving the model explicit formatting instructions, examples of correctly formatted output, or even "layout hints" in the prompt. A well-designed prompt can often induce a model to produce a nearly perfect structured output [[ea24b]](#). Prompt engineering requires no model training and can be iteratively refined. However, it demands significant manual effort and expertise, and even then does not guarantee consistency [[ea24b]](#). Models may still err on edge cases or as the prompt complexity grows, and maintaining long, complex prompts (especially across different models or updates) can be cumbersome. In practice, prompt-based solutions might be combined with lightweight validation or post-processing in high-stakes applications.

## Hybrid Constraint-Based Decoding and Prompt Engineering

By embedding knowledge of the schema at the prompt level and using a specialized procedure to keep the generation on track (via tagging, iterative re-checks, or extra control tokens), hybrid systems achieve schema adherence more reliably than a vanilla LLM approach [[BTW23]](#b1). This structured, schema-first method is key to guaranteeing the outputs are valid, parseable, and aligned with downstream consumption requirements. Schema acts as a blueprint for how the final text must be organized while controllable generation mechanism conditions the model's decoding process on these schema constraints. Instead of free-form text generation, the model is guided to fill in required slots, adhere to the correct format, and avoid extraneous or malformed outputs [[BTW23]](#b1).

Each of these approaches comes with effectiveness trade-offs, especially under the stringent demands of regulated industries. Fine-tuning and RLHF can deeply instill format compliance into a model but at high development cost and with less transparency. Prompt engineering is more flexible and avoid retraining, but it relies on the base model's capacity to follow instructions. Constraint-based decoding offers hard guarantees on structure, appealing for compliance, though it requires integrating external constraint logic with the model's output stream. The choice often depends on the specific use case and constraints -for instance, biomanufacturers must consider not only technical accuracy but also validation, auditing, and data governance. Ensuring that LLM-generated records are both accurate in content and precise in format is vital to meet quality and regulatory standards. Recent work underlines that reliable structured generation remains an open challenge, calling for continued research into methods that can robustly align LLM outputs with predefined schemas [[ea24a]](#).

## Method

Although the strategies outlined above-ranging from prompt engineering to constraint-based decoding-can improve structured output, they often require specialized tooling or large-scale fine-tuning. In regulated domains such as bio-manufacturing, these approaches must also be cost-effective and robust. In this section, we describe a reasoning-driven methodology that leverages synthetic data construction and iterative LLM reasoning to ensure schema adherence with minimal overhead. Specifically, we demonstrate how to:

• Build RL reasoning dataset Create synthetic unstructured and structured data [[ea23a]](#), [[ea22b]](#) in tandem using controlled prompts and Qwen 14B/32B [[Tea24]](#b17),

Reverse-engineer how unstructured text can map onto an empty JSON schema by engaging a distilled DeepSeek R1 Qwen 32B [[DA25]](#b2) to explain-step by step-how each schema field is populated. Develop custom reward mechanisms that directly evaluate how well the outputs adhere to a predefined schema while balancing fluency, completeness, and correctness.

Train R1-Zero reasoning model from Qwen 2.5 1.5B base model using RL [[aa24]](#), [[ea23c]](#) and synthetic unstructured-structured pair dataset, integrate custom rewards into GRPO [[aa24]](#) without altering the core policy optimization loop. The combined reward drives the training so that the model produces outputs that score highly on all relevant criteria.

Fine tune R1-Zero model into R1 with supervised fine-tuning using reasoning dataset.

## Generating Structured and Unstructured Data

We begin by prompting a language model (Qwen 14B and 32B) to produce diverse, fully populated JSON schemas (including nested and complex fields). These filled schemas emulate real-world documentation (e.g., QA checklists, batch records) while showcasing variations in schema hardness and domain.

You are an expert in building a hierarchical JSON schema and object for the domain { DOMAIN }.

Your task is to create :

1. A multi -level JSON Schema describing :

-ROOT ( level 0) ,

$-SECTION ( level 1) , -SUBSECTION ( level 2) , -$ DETAIL _{ N }$ ( level 3+) .$Each level may contain tables (2 D data layouts ) and checkbox elements ( MCQs , confirmations ) , with nested components reflecting complex structures .

2. A JSON Object that strictly matches this schema , including :

-\" id \" and \" title \" -\" level \" and \"$ level _{ type }$\" -An array of \" component \" objects ( paragraphs , tables , or checkboxes ) -A recursive \" children \" array -Special \" properties \" ( e . g . , \" variables \" , \" content \") for data , logs , metrics , or formulas Formatting Requirements :

-Escape all quotes (\") , replace newlines with \\ n -No trailing commas , single quotes , or extra data -Enclose the final output with no extra explanations :

In parallel, we generate corresponding blank schemas-retaining structural outlines but omitting values. This gives us a "before and after" pair for each schema: an empty template and a filled instance. Such pairs are crucial for teaching LLMs how unstructured text should be systematically transformed into the exact JSON schema. We then produce unstructured text reflecting the same content as the filled schema-but presented in varying layouts (e.g., sequential paragraphs, parallel sections, combined strategies) and table formats (ASCII art, XML/HTML-like snippets, simulated PDF extraction, etc.). These multi-format "narratives" mimic the real challenge of reading and interpreting inconsistent legacy documents. Table styles : \ n { RANDOM _ TABLE _ STYLE } -Checkbox styles : ' ' '[ ] , YES , NO , N /A , etc . ' ' ' ** RULES **: 1. Map every JSON level , component , and attribute to the correct layout / style . 2. Surround JSON data points with additional words / sentences to obscure parsing . 3. Include all data ( title , variables , metadata , content ) ; no extra sections . 4. End each data point with a brief , unrelated remark . 5. Add filler paragraphs ( definitions , domain info , etc .) not directly tied to the JSON content .

In doing so, we create a synthetic corpus that covers a broad range of domain contexts, from general manufacturing logs to specialized quality assurance frameworks. Each piece of unstructured text is logically equivalent to a filled JSON schema, yet differs in structure, style, and formatting.

## Reasoning Dataset: Reverse-Engineering from Text to Schema

We employ Distilled DeepSeekR1 Qwen 32B with the following prompt:

You are an AI assistant tasked with extracting structured data from a piece of text .

Inputs :

1. Text ( source of information )

2. Blank Schema ( unfilled JSON schema )

## Filled Schema ( final populated JSON )

Goals :

1. Compare Text + Blank Schema to the Filled Schema .

2. Explain step by step ( chain -of -thought ) how the text populates the blank schema .

3. Output only the reasoning steps ( thought process ) .

4. Cross -verify that this reasoning exactly produces the Filled Schema .

Format your final response as :

Chain of Thought Explanation : """

The LLM is instructed to output only its chain-of-thought reasoning, explicitly describing the mapping from text to schema. Such self-explaining prompts push the model to maintain strict schema fidelity while revealing the logic behind each structural decision. Because the prompt demands an explicit reasoning path, the LLM self-checks how each field is filled, minimizing random or malformed output. The chain-of-thought not only ensures correctness but also documents how the text was interpreted which is vital for regulated environments. By varying the domain (e.g., different types of QA reports) and text layout styles, we create a dataset that fosters LLM resilience to formatting quirks.

## GRPO Training on a Small Foundation Model

Once we finalize the reasoning dataset, we proceed to train a small foundation model-mirroring the minimalistic DeepSeek R1 Zero approach-using GRPO [[DA25]](#b2). We employ a 1.5B-parameter base model "to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process" [[DA25]](#b2). By leveraging a group-based advantage calculation and carefully designed reward signals (e.g., schema compliance, correctness), we efficiently instill structured reasoning capabilities within a resource-constrained pipeline. By incorporating multiple reward functions [[ea23b]](#) into the GRPO framework, we can simultaneously encourage format correctness (via r format) and content/domain correctness (via r equation). The combined reward drives training so that the model produces outputs that score highly on all relevant criteria. The entire process remains computationally light (e.g., 20 hours on an 8×H100 cluster), demonstrating that strict schema adherence can be achieved even with compact, low-overhead foundation models.

## JSON-Based Reward

This reward algorithm balances two aspects: (1) schema faithfulness via the key-value matching fraction, and (2) structural completeness via JSON length similarity. A high final reward indicates that the predicted JSON object closely matches the ground truth both in field contents and overall size.

## Format Verification Reward

The format check enforces correct usage of specialized tags, crucial for downstream tasks that rely on clearly separated reasoning (<think> block) and final answers (<answer> block). The binary reward (0 or 1) simplifies reinforcement signals, focusing exclusively on structural correctness rather than content fidelity. The optional logging step enables sampling a small fraction of completions for qualitative inspection, aiding diagnostic or future training data curation.

Algorithm 1 JSON-Based Reward Computation 1: Given: A list of completions C = {c 1 , . . . , c n } from the model. A list of ground-truth JSON objects G = {g 1 , . . . , g n }.

Each g i is a valid JSON string. for each pair (c i , g i ) in (C, G) do 5:

$c ′ i ← "<think>" ∥ c i ▷ Insert <think> prefix 6:$ans ← substring(c ′ i , ''<answer>'', ''</answer>'')

7:

if ans is empty then 8:

$r i ← 0 9:$append r i to R; continue

10: end if ▷ Parse as JSON 11: parse ans into answer json; parse g i into gt json 12: if either parse fails then 13: r i ← 0 14: append r i to R; continue 15: end if ▷ Compute field overlap 16: K a ← keys(answer json) 17: K g ← keys(gt json) 18: total fields ← |K a ∪ K g | 19: matching fields ← k∈(Ka∩Kg) 1[answer json[k] = gt json[k]] 20: if total fields > 0 then 21: key match score ← matching fields total fields 22: else 23: key match score ← 0 24: end if ▷ Compare JSON lengths 25: ℓ a ← length(answer json) or 1 26: ℓ g ← length(gt json) or 1 27: length ratio ← min(ℓa,ℓg) max(ℓa,ℓg) ▷ Calculate final reward 28: r i ← key match score+length ratio 2 29: clamp r i to [0, 1]; round to 1 decimal place 30: if r i ≥ 0.6 then 31: log c ′ i with 60% probability 32: end if 33: append r i to R 34: end for 35:

return R 36: end procedure Algorithm 2 Format Verification Reward 1: Goal: Assign a reward of 0 or 1 depending on whether a generated completion follows an expected structure using <think>...</think> and <answer>...</answer>.

## 2: Inputs:

A list of completions C = {c 1 , . . . , c n } (model-generated).

A list of ground-truth objects G = {g 1 , . . . , g n } (not directly used here, but included for extensibility). A small probability p (e.g., 0.1) for selectively logging completions. 3: Output: A list of scalar rewards R = {r 1 , . . . , r n }, with r i ∈ {0, 1}. Synthesize prompt format:

$c ′ i ← "<think>" ∥ c i ▷ Prepend "<think>" 7:$Probabilistic logging:

$Draw x ∼ U [0, 1]. 8: if x < p then 9:$Log c ′ i to file for future analysis. Format check via regex: Define R = "^<think>([^<]*(?:<(?!/?think>)[^<]*)*)</think>\n<answer>([\\s\\S]*?)</answer>$" Match R against c ′ i .

12:

if match fails (no correct grouping) then 13:

$r i ← 0 14: else 15: r i ← 1 16: end if 17:$Append r i to R. 18: end for 19: return R Algorithm 3 GRPO with Multiple Reward Functions Notation and Setup Define a combined reward:

$R comb (c) = f r 1 (c), r 2 (c), . . . , r K (c) ,$where f can be a weighted sum, mean, or any aggregator, π θ be the current policy (a language model parameterized by θ), {r k } K k=1 be K reward functions (e.g., r format , r equation )

## Group-Based Relative Advantage

Let G = { c 1 , . . . , c M } be a group of M outputs sampled from π θ .

For each c i , compute a combined reward R i = R comb (c i ).

Define the relative (rank-based) advantage:

$A (rel) (c i ) = 1 M -1 j̸ =i 1 R i > R j ,$which is the fraction of samples in G that have lower reward than c i .

## GRPO Update

Update θ to favor completions with higher relative advantage.

The GRPO loss for group G is:

$L GRPO (θ) = - ci∈G A (rel) (c i ) log π θ (c i ) + Reg(θ),$where Reg(θ) includes regularization terms (e.g., entropy bonus, KL-divergence). 

## Supervised Fine-Tuning

While reinforcement learning confers advanced reasoning capacities, but supervised fine-tuning provides the final task-and schema-specific "polish" that ensures outputs are both logically grounded and robustly aligned with real-world standards. [[DA25]](#b2). Reinforcement learning (RL) optimizes a policy for broad correctness or format adherence but can overlook rare or domain-specific intricacies (e.g., specialized field naming conventions, unusual data types). SFT exposes the model to explicit examples that emphasize precisely how to handle real-world edge cases, ensuring no field or condition is left under-represented. Although RL fosters adaptability, the learned policy may still exhibit variability in ambiguous contexts or unrepresented task scenarios [[DA25]](#b2). SFT, by contrast, anchors the final policy to concrete labeled examples, reducing output drift. By overlaying a final SFT stage, ThinkJSON tightly aligns its already-developed reasoning to the strict output requirements (e.g., correct JSON keys, mandatory fields), producing outputs suitable for audit or compliance. For SFT (and SFT+LoRA) we used the Unsloth training framework on an A100 GPU, completing the process in about 3 hours.

### Role :

You are an expert data extractor mapping hierarchical text to a given JSON Schema .

### DATA INPUT : Text ; Blank JSON Schema ### TASK REQUIREMENT :

1. Map all relevant text to the JSON Schema .

2. Output in two sections :

-< think >: Reasoning -< answer >: Filled JSON ### STRICT RULES :

1. Provide both < think > and < answer >.

-If minimal reasoning , say : \" Direct mapping from text to schema .\" 2. Map text exactly to the JSON Schema ( no omission / alteration ) .

## Preserve hierarchy ( ROOT

$$\ to $ SECTION $\ to $ SUBSECTION $\ to $ DETAIL \_ N )$4. Correctly set attributes ( id , idc , idx , level \_ type , component \_ type , etc .) .

## JSON Format :

-Escape quotes as \"

-Replace newlines with \\ n -No trailing commas -Only double quotes 6. Explain key decisions in < think >.

### IMPORTANT :

If < think > or < answer > is missing , response is incomplete .\") , axis =1) 

## Evaluation

We evaluated five models: ThinkJSON, Original DeepSeek R1 (671B), Distilled DeepSeek R1 (Qwen-1.5B / Qwen-7B) and Gemini 2.0 Flash (70B) which specializes on structured output generation [[tea25]](#), on a structured data extraction benchmark involving 6.5K rows. Each row was processed to produce or omit a valid JSON object, and we measured metrics including:

• Rows With No Output: Number of rows for which the model produced no structured output.

• Rows With Valid JSON: Number of rows resulting in syntactically valid JSON objects.

• Mean Match Percentage: Average proportion of fields correctly mapped.

• Mean Noise Percentage: Average proportion of extraneous or malformed tokens within the extracted JSON. As illustrated, ThinkJSON yields strong results, with a 62.41% mean match (highest of all five models) and the lowest 0.27% mean noise, indicating minimal extraneous output. The Original DeepSeek R1 also achieves relatively high valid-JSON coverage but shows a lower mean match (41.43%) and higher noise (11.14%). The two distilled variants of DeepSeek R1-Qwen 1.5B and Qwen 7B-exhibit weaker performance overall, with high rates of no extracted JSON or large amounts of noise. Meanwhile, Gemini 2.0 Flash achieves a midrange mean match of 42.88% but suffers from significant noise at 10.86%. These findings underscore the effectiveness of our structured reasoning approach in producing concise, schema-valid outputs.

## Discussion and Future Direction

Our experimental findings confirm that the reasoning-driven, schema-constrained generation pipeline is both broadly applicable-capable of handling diverse reasoning tasks beyond purely mathematical or scientific domains-and budget-conscious, as it requires comparatively moderate GPU resources and a modest dataset of reasoning examples. This balanced approach addresses a critical need in bio-manufacturing compliance , where AI systems must deliver not only correct structure but also reliable, domain-specific reasoning to meet regulatory standards [[ea22a]](#), [[ea25a]](#).

The hallmark of our framework is integrating compliance considerations at the core of the generation process. Rather than relying on prompt-based or post-hoc solutions, our pipeline combines schema adherence objectives with iterative reasoning loops, thus reducing the need for manual oversight. This focus on strict output validation resonates with bio-manufacturing's regulatory requirements-where precise field mappings and hierarchical consistency are crucial for electronic batch records and industry audits.

While we have employed a 1.5B-parameter foundation model, our method is readily scalable to bigger backbones (e.g., 7B parameters). Larger models could potentially yield richer context interpretation and more robust handling of rare or domain-specific phenomena. In future work, we plan to explore how increased capacity further expands the set of reasoning scenarios the model can tackle while maintaining resource efficiency-a pivotal benefit in industrial adoption.

Overall, this reinforcement + fine-tuning pipeline for structured text generation offers a flexible, compliance-aware approach that applies universal reasoning principles-spanning regulated biomanufacturing tasks and broader domains-without incurring prohibitive computational overhead. This synergy of versatility and cost-effectiveness positions our method as a significant step forward in delivering reliable, schema-adherent AI-driven solutions.

![Figure 1: "Think inside the JSON" pipeline]()

![You are an expert in generating hierarchical text documents from JSON Object data points . ** Task **: Convert the JSON Object into an unstructured , paragraph -based document . ** Given Data **: ** Domain **; ** JSON Schema **; ** JSON Object ** ** OUTPUT FORMAT ** ( enclosed strictly within < text >) :for components / levels : \ n { RANDOM _ LAYOUT } -]()

![Initialize an empty rewards list: R ← [ ]. 5: for each pair (c i , g i ) in (C, G) do 6:]()

![Figure 2: GRPO Training Metrics]()

![Figure 3: SFT Training Metrics]()

![Figure 4: Performance Comparison]()

