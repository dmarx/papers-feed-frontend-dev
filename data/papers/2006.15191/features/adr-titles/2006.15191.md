- Decision to investigate the relationship between SGD and Bayesian sampling.
- Choice of architectures and datasets for empirical analysis.
- Methodology for calculating P SGD(f |S) and P B(f |S).
- Decision to use Gaussian processes for estimating Bayesian posterior probabilities.
- Assumption that SGD provides an inductive bias towards low-error functions.
- Choice of hyperparameters (batch size, learning rate, optimizer) for experiments.
- Decision to compare SGD-trained DNNs with Bayesian DNNs.
- Choice of performance metrics for evaluating generalization.
- Decision to analyze the effect of different optimizers on function probability distributions.
- Assumption regarding the bias in the prior affecting the posterior in Bayesian inference.
- Decision to focus on the overparameterized regime in DNNs.
- Choice to explore the implications of function complexity on generalization.
- Decision to use zero training error as a condition for evaluating function probabilities.
- Assumption that untrained DNNs exhibit a bias towards simple functions.
- Decision to frame the problem in terms of the parameter-function map.