<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is SGD a Bayesian sampler? Well, almost</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-10-24">24 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chris</forename><surname>Mingard</surname></persName>
							<email>christopher.mingard@chem.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Chemistry</orgName>
								<orgName type="department" key="dep2">Department of Physics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Physics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillermo</forename><surname>Valle-PÃ©rez</surname></persName>
							<email>guillermo.valle@dtc.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Chemistry</orgName>
								<orgName type="department" key="dep2">Department of Physics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Physics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joar</forename><surname>Skalse</surname></persName>
							<email>joar.skalse@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Chemistry</orgName>
								<orgName type="department" key="dep2">Department of Physics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Physics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ard</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
							<email>ard.louis@physics.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Chemistry</orgName>
								<orgName type="department" key="dep2">Department of Physics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Physics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is SGD a Bayesian sampler? Well, almost</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-24">24 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">43B5D66DA7E3C0C159E6D08FD8EBE74E</idno>
					<idno type="arXiv">arXiv:2006.15191v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>stochastic gradient descent</term>
					<term>Bayesian neural networks</term>
					<term>deep learning</term>
					<term>Gaussian processes</term>
					<term>generalisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) generalise remarkably well in the overparameterised regime, suggesting a strong inductive bias towards functions with low generalisation error. We empirically investigate this bias by calculating, for a range of architectures and datasets, the probability P SGD (f |S) that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function f consistent with a training set S. We also use Gaussian processes to estimate the Bayesian posterior probability P B (f |S) that the DNN expresses f upon random sampling of its parameters, conditioned on S.</p><p>Our main findings are that P SGD (f |S) correlates remarkably well with P B (f |S) and that P B (f |S) is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines P B (f |S)), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime.</p><p>While our results suggest that the Bayesian posterior P B (f |S) is the first order determinant of P SGD (f |S), there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on P SGD (f |S) and/or P B (f |S), can shed light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While deep neural networks (DNNs) have revolutionised modern machine learning <ref type="bibr" target="#b46">(LeCun et al., 2015;</ref><ref type="bibr" target="#b78">Schmidhuber, 2015)</ref>, a solid theoretical understanding of why they work so well is still lacking. One surprising property is that they typically perform best in the overparameterised regime, with many more parameters than data points. Standard learning theory approaches <ref type="bibr" target="#b79">(Shalev-Shwartz and Ben-David, 2014)</ref>, based for example on model capacity, suggest that such highly expressive <ref type="bibr" target="#b15">(Cybenko, 1989;</ref><ref type="bibr" target="#b37">Hornik, 1991;</ref><ref type="bibr" target="#b30">Hanin, 2019)</ref> DNNs should heavily over-fit in this regime, and therefore not generalise at all.</p><p>Stochastic gradient descent (SGD) <ref type="bibr" target="#b9">(Bottou et al., 2018)</ref> is one of the key technical innovations allowing large DNNs to be efficiently trained in the highly overparameterised regime. In supervised learning, SGD allows the user to efficiently find sets of parameters that lead to zero training error. The power of SGD as an optimiser for DNNs was demonstrated in an influential paper <ref type="bibr" target="#b105">(Zhang et al., 2016)</ref>, which showed that zero training error solutions for CIFAR-10 image data with randomised labels can be found with a relatively moderate increase in computational effort over that needed for a correctly labelled dataset. These experiments also framed the conundrum of generalisation in the overparameterised regime as follows: Given that DNNs can memorise randomly labelled image datasets, which leads to poor generalisation, why do they behave so differently on correctly labelled datasets and select for functions that generalise well? The solution to this conundrum must be that SGD-trained DNNs have an inductive bias towards functions that generalise well (on structured data).</p><p>The possibility that SGD is not just good for optimisation, but is also a key source of inductive bias, has generated an extensive literature. One major theme concerns the effect of SGD on the flatness of the minima found, typically expressed in terms of eigenvalues of a local Hessian or related measures. A link between better generalisation and flatter minima has been widely reported <ref type="bibr">(Hochreiter and Schmidhuber, 1997a;</ref><ref type="bibr" target="#b42">Keskar et al., 2016;</ref><ref type="bibr" target="#b39">Jastrzebski et al., 2018;</ref><ref type="bibr" target="#b99">Wu et al., 2017;</ref><ref type="bibr" target="#b106">Zhang et al., 2018;</ref><ref type="bibr" target="#b94">Wei and Schwab, 2019)</ref>, but see also <ref type="bibr" target="#b22">(Dinh et al., 2017)</ref>. Theoretical work on SGD has also generated a large and sophisticated literature. For example, in <ref type="bibr" target="#b86">(Soudry et al., 2018)</ref> it was demonstrated that SGD finds the max-margin solution in unregularised logistic regression, whilst it was shown in <ref type="bibr" target="#b11">(Brutzkus et al., 2017)</ref> that overparameterised DNNs trained with SGD avoid over-fitting on linearly separable data. Recently, <ref type="bibr" target="#b0">(Allen-Zhu et al., 2019)</ref> proved agnostic generalisation bounds of SGD-trained neural networks. Other recent work <ref type="bibr" target="#b69">(Poggio et al., 2020)</ref> suggests that gradient descent performs a hidden regularisation in normalised weights, but a different analysis suggests that such implicit regularisation may well be very hard to prove in a more general setting for SGD <ref type="bibr" target="#b16">(Dauber et al., 2020)</ref>. Overall, while SGD and its related algorithms are excellent optimisers, there is as yet no consensus on what inductive bias SGD provides for DNNs. For a more detailed discussion of this SGD-related literature see Section 7.2.</p><p>An alternative approach is to consider the inductive properties of random neural networks, that is untrained DNNs with weights sampled from a (typically i.i.d.) distribution. Recent theoretical and empirical work <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018;</ref><ref type="bibr" target="#b18">De Palma et al., 2018;</ref><ref type="bibr" target="#b61">Mingard et al., 2019)</ref> suggests that the (prior) probability P (f ) that an untrained DNN outputs a function f upon random sampling of its parameters (typically the weights and biases) is strongly biased towards "simple" functions with low Kolmogorov complexity (see also Section 7.3). A widely held assumption is that such simple hypotheses will generalise well -think Occam's razor. Indeed, many processes modelled by DNNs are simple <ref type="bibr" target="#b52">(Lin et al., 2017;</ref><ref type="bibr" target="#b28">Goldt et al., 2019;</ref><ref type="bibr" target="#b87">Spigler et al., 2019)</ref>. For more on these topics see Section 7.3 and Section 7.5.</p><p>If the inductive bias towards simplicity described above for untrained networks is preserved throughout training, then this could help explain the DNN generalisation conundrum. Again, there is an extensive literature relevant to this topic. For example, a number of papers <ref type="bibr" target="#b70">(Poole et al., 2016;</ref><ref type="bibr" target="#b48">Lee et al., 2018;</ref><ref type="bibr" target="#b92">Valle-PÃ©rez et al., 2018;</ref><ref type="bibr">Yang, 2019a;</ref><ref type="bibr" target="#b61">Mingard et al., 2019;</ref><ref type="bibr" target="#b14">Cohen et al., 2019;</ref><ref type="bibr" target="#b97">Wilson and Izmailov, 2020)</ref> employ arguments on heuristic grounds that the bias in untrained random neural networks could be used to study the inductive bias of optimiser-trained DNNs. Optimiser-trained DNNs have also been directly compared to their Bayesian counterparts (c.f. Sections 6 and 7.4 for more detailed discussions). In an important development, <ref type="bibr" target="#b47">Lee et al. (2017)</ref>; <ref type="bibr">Matthews et al. (2018)</ref>; <ref type="bibr" target="#b67">Novak et al. (2018b)</ref> used the Gaussian process (GP) approximation to Bayesian DNNs, which is exact in the limit of infinite width, and found that the generalisation performance of Bayesian DNNs and SGD-trained DNNs was relatively similar for standard deep learning datasets such as CIFAR-10, though <ref type="bibr" target="#b96">Wenzel et al. (2020)</ref> found more significant differences when using Monte Carlo to approximate finite-width Bayesian DNNs. Others have used either Monte Carlo methods <ref type="bibr" target="#b56">(Mandt et al., 2017)</ref> or the GP approximation <ref type="bibr" target="#b58">(Matthews et al., 2017;</ref><ref type="bibr" target="#b17">de G. Matthews et al., 2018;</ref><ref type="bibr" target="#b49">Lee et al., 2019;</ref><ref type="bibr" target="#b97">Wilson and Izmailov, 2020)</ref> to examine how similar the Bayesian posterior is to the sampling distribution of SGD (whether in parameter or function space), albeit on relatively low dimensional systems compared to conventional DNNs.</p><p>In this paper we perform extensive computations, for a series of standard DNNs and datasets, of the probability P SGD (f |S) that a DNN trained with SGD (or one of its variants) to zero error on training set S, converges on a function f . We then compare these results to the Bayesian posterior probability P B (f |S), for these same functions, conditioned on achieving zero training error on S.</p><p>The main question we explore here is: How similar is P B (f |S) to P SGD (f |S)? If the two are significantly different, then we may conclude that SGD provides an important source of inductive bias. If the two are broadly similar over a wide range of architectures, datasets, and optimisers, then the inductive bias is primarily determined by the prior P (f ) of the untrained DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Main results summary</head><p>We carried out extensive sampling experiments to estimate P SGD (f |S). Functions are distinguished by the way they classify elements on a test set E. We use the Gaussian process (GP) approximation to estimate P B (f |S) for the same systems. Our main findings are:</p><p>(1) P SGD (f |S) â P B (f |S) for a range of architectures, including FCNs, CNNs and LSTMs, applied to datasets such as MNIST, Fashion-MNIST, an IMDb movie review database and an ionosphere dataset. This agreement also holds for variants of SGD, including Adam (Kingma and Ba, 2014), Adagrad <ref type="bibr" target="#b25">(Duchi et al., 2011)</ref>, <ref type="bibr">Adadelta (Zeiler, 2012)</ref> and RMSprop <ref type="bibr" target="#b90">(Tieleman and Hinton, 2012)</ref>.</p><p>(2) The P B (f |S) of functions f that achieve zero-error on the training set S can vary over hundreds of orders of magnitude, with a strong bias towards a set of low generalisation/low complexity functions. This tiny fraction of high probability functions also dominate what is found by DNNs trained with SGD. It is striking that even within this subset of functions, P SGD (f |S) and P B (f |S) correlate so well. Our empirical results suggest that, for DNNs with large bias in P B (f |S), SGD behaves to first order like a Bayesian optimiser and is therefore exponentially biased towards simple functions with better generalisation. Thus, SGD is not itself the primary source of inductive bias for DNNs.</p><p>(3) A function-based picture can also be fruitful for illustrating second order effects where an optimiser-trained DNN differs from the Bayesian prediction. For example, training an FCN with different optimisers (OPT) such as Adam, Adagrad, Adadelta and RMSprop on MNIST generates slight but measurable variations in the distributions of P OPT (f |S). Such information can be used to analyse differences in performance. For instance, we find that changing batch size affects P Adam (f |S) but, as was found for generalisation error <ref type="bibr" target="#b42">(Keskar et al., 2016;</ref><ref type="bibr" target="#b29">Goyal et al., 2017;</ref><ref type="bibr" target="#b36">Hoffer et al., 2017;</ref><ref type="bibr">Smith et al., 2017)</ref>, this effect can be compensated by changes in learning rate. Architecture changes can also be examined in this picture. For example, adding max-pooling to a CNN trained with Adam on Fashion-MNIST increases both P B (f |S) and P Adam (f |S) for the lowest-error function f found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We first introduce a key definition from <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> needed to specify P (f ) and P B (f |S).</p><p>Definition 2.1 (Parameter-function map). Consider a parameterised supervised model, and let the input space be X and the output space be Y. The space of functions the model can express is a set F â Y X . If the model has some number of parameters, taking values within a set Î â R p , then the parameter-function map M is defined by</p><formula xml:id="formula_0">M : Î â F Î¸ â f Î¸ where f Î¸ is the function corresponding to parameters Î¸ â Î.</formula><p>The function space F of a DNN N could in principle be considered to be the entire space of functions that N can express on the input vector space X , but it could also be taken to be the set of partial functions N can express on some subset of X . For example, F could be taken to be the set of possible classifications of images in MNIST. In this paper we always take F to be the set of possible outputs of N for the instances in some dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Bayesian prior probability, P (f )</head><p>Given a distribution P par (Î¸) over the parameters, we define the P (f ) over functions as</p><formula xml:id="formula_1">P (f ) = 1[M(Î¸) = f ]P par (Î¸)dÎ¸, (1)</formula><p>where 1 is an indicator function (1 if its argument is true, and 0 otherwise). This is the probability that the model expresses f upon random sampling of parameters over a parameter initialisation distribution P par (Î¸), which is typically taken to have a simple form such as a (truncated) Gaussian. P (f ) can also be interpreted as the probability that the DNN expresses f upon initialisation before an optimisation process. It was shown in <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> that the exact form of P par (Î¸) (for reasonable choices) does not affect P (f ) much (at least for ReLU networks). If we condition on functions that obtain zero generalisation error on a dataset S, then the procedure above can also be used to generate the posterior P B (f |S) which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Bayesian posterior probability, P B (f |S)</head><p>Here, we describe the Bayesian formalism we use, and show how bias in the prior affects the posterior. Consider a supervised learning problem with training data S corresponding to the exact values of the function which we wish to infer (i.e. no noise). This formulation corresponds to a 0-1 likelihood P (S|f ), indicating whether the data is consistent with the function. Formally, if S = {(x i , y i )} m i=1 corresponds to the set of training pairs, then we let</p><formula xml:id="formula_2">P (S|f ) = 1 if âi, f (x i ) = y i 0 otherwise .</formula><p>Note that in our calculations, this quantity is technically P (S|f ; {x i }), but we denote it as P (S|f ) to simplify notation. We will use a similar convention throughout, whereby the input points are (implicitly) conditioned over. We then assume the prior P (f ) corresponds to the one defined in Section 2.1. Bayesian inference then assigns a Bayesian posterior probability P B (f |S) to each f by conditioning on the data according to Bayes rule</p><formula xml:id="formula_3">P B (f |S) := P (S|f )P (f ) P (S) ,<label>(2)</label></formula><p>where P (S) is also called the marginal likelihood or Bayesian evidence. It is the total probability of all functions compatible with the training set. For discrete functions, P (S) = f P (S|f )P (f ) = f âC(S) P (f ), with C(S) the set of all functions compatible with the training set. For a fixed training set, all the variation in P B (f |S) for f â C(S) comes from the prior P (f ) of the untrained network since P (S) is constant. Thus, the bias in the prior is essentially translated over to the posterior.</p><p>Thus, P B (f |S) is the distribution over functions that would be obtained by randomly sampling parameters according to P par (Î¸) and selecting only those that are compatible with S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head><p>The optimiser probability, P OPT (f |S) But DNNs are not normally trained by randomly sampling parameters: They are trained by an optimiser. The probability that the optimiser OPT (e.g. SGD) finds a function f with zero error on S can be defined as:</p><formula xml:id="formula_4">P OPT (f |S) := 1[M(Î¸ f ) = f ]P OPT (Î¸ f |Î¸ i , S) Ppar (Î¸ i )dÎ¸ i dÎ¸ f (3)</formula><p>where P OPT (Î¸ t |Î¸ i , S) denotes the probability that OPT, initialised with parameters Î¸ i on a DNN, converges to parameters Î¸ f when training is halted after the first epoch where zero classification error is achieved on S<ref type="foot" target="#foot_0">foot_0</ref> , if such a condition is achieved in a number of iterations less than the maximum number which we allow for the experiments. The initialisation distribution Ppar (Î¸ i ) is defined analogously to P par (Î¸) in Equation (1) (though it need not be exactly the same). P OPT (f |S) is, therefore, a measure of the 'size' of f 's 'basin of attraction', which intuitively refers to the set of initial parameters that converge to f upon training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology, Datasets and DNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Definition of functions</head><formula xml:id="formula_5">For a specific DNN, training set S = {(x i , y i )} |S| i=1 and test set E = {(x i , y i )} |E| i=1</formula><p>, we define a function f as a labelling of the inputs in S concatenated with the inputs in E. 2 We will only look at functions which have 0 error on S, so that, for a particular experiment with fixed S and E, the functions are distinguished only by the predictions they make on E. Furthermore we only consider binary classification tasks (c.f. Section 3.2), so that our output space 3 is Y = {0, 1}. Therefore, we will represent functions by a binary string of length |E| representing the labels on E; the ith character representing the label on the ith input of E, x i . For the sake of simplicity, we will not make a distinction between this representation of f and the function f itself, as they are related one-to-one for any particular experiment (with fixed S and E).</p><p>Restricting the input space where functions are defined can be thought of as a coarsegraining of the functions on the full input space (e.g. the space of all possible images for image classification), which allows us to estimate their probabilities from sampling. In the following subsections we explain how the main experimental quantities are computed. Further detail can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Calculating P OPT (f |S)</head><p>For a given optimiser OPT (SGD or one of its variants), a DNN architecture, loss function (cross-entropy (CE) or mean-square error (MSE)), a training set S, and test set E, we repeat the following procedure n times: We sample initial parameters Î¸ i , from an i.i.d. truncated Gaussian distribution Ppar (Î¸ i ), and train with the optimiser until the first epoch where the network has 100% training classification accuracy (except for experiments labelled "overtraining," where we halt training after p further epochs with 0 training error have occured, for some specified p) 4 . We then compute the function f found by evaluating the network on the inputs in E, as described before.</p><p>Note that during training, the network outputs are taken to be the pre-activations of the output layer, which are fed to either the MSE loss, or as logits for the CE loss. At evaluation (to compute f ), the pre-activations are passed through a threshold function so that positive pre-activations output 1 and non-positive pre-activations output 0.</p><p>We chose sample sizes between n = 10 4 and n = 10 6 . In other words, we typically sample over n = 10 4 to n = 10 6 different trained DNNS, and count how many times each function f appears in the sample to generate the estimates of P OPT (f |S). We leave the dependence of P OPT (f |S) on E implicit. This method of estimating P OPT (f |S) is described more formally in Appendix A.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Formally then, our space of functions is</head><formula xml:id="formula_6">F = Y X , where X = {xi} |S| i=1 âª {x i } |E| 3.</formula><p>Note that for training with MSE loss, we centered the output so the loss is measured with respect to target values in {-1, 1}. This is so thresholding can occur at a value of the last layer preactivation of 0, which is the same as for cross-entropy loss on logits. 4. If SGD fails to achieve 100% accuracy on S in a maximum number of iterations, we discard the run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Calculating P B (f |S) with Gaussian Processes</head><p>We use neural network Gaussian processes (NNGPs) <ref type="bibr" target="#b47">(Lee et al., 2017;</ref><ref type="bibr">Matthews et al., 2018;</ref><ref type="bibr" target="#b27">Garriga-Alonso et al., 2019;</ref><ref type="bibr" target="#b67">Novak et al., 2018b)</ref> to approximate P B (f |S), for some training set S and test set E. NNGPs have been shown to accurately approximate the prior over functions P (f ) of finite-width Bayesian DNNs <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018;</ref><ref type="bibr" target="#b17">de G. Matthews et al., 2018)</ref>. We use DNNs with relatively wide intermediate layers, relative to the input dimension, to ensure that we are close to the infinite layer-width NNGP limit. Depending on the loss function, we estimate the posterior P B (f |S) as follows:</p><p>â¢ Classifiation as regression with MSE loss. As has been done in previous work on NNGPs, we consider the classification labels as regression targets with an MSE loss 5 . We compute the analytical posterior for the NNGP with Gaussian likelihood. This is the posterior over the real-valued outputs at the test points on E, which correspond to the pre-activations of the output layer of the DNN. We sample from this posterior, and threshold the real-values like we do for DNNs (positive becomes 1 and otherwise it becomes 0) to obtain labels on E, and thus a function f . We then estimate P B (f |S) by counting how many times each f is obtained from a set of n independent samples from the posterior, similar to what we did for P OPT (f |S). For more details on GP computations with MSE loss, see Appendix A.2.1. We describe this method more formally in Appendix A.1.2. We use this technique when comparing P B (f |S) with P OPT (f |S) for MSE loss (e.g. Figure <ref type="figure" target="#fig_0">1a</ref>).</p><p>â¢ Classification with CE loss. In several experiments, we approximate the NNGP posterior using a 0-1 misclassification loss, which is more justified for classification, and can be thought of as a "low temperature" version of the CE loss. Since, in contrast to the MSE case, this posterior is not analytically tractable, we use the expectation propagation (EP) approximation to estimate probabilities <ref type="bibr" target="#b72">(Rasmussen, 2004)</ref>. In particular, we estimate P B (f |S) via ratio of EP-approximated likelihoods.</p><p>The EP approximation can be used to estimate the marginal likelihood of any labelling over any set of inputs, given a GP prior. As shown in Equation (2), we can use Bayes theorem to express P B (f |S) as a ratio of P (f ) and P (S) (which is valid for functions with 0 error on S, and the 0 -1 likelihood), and then use EP approximation to obtain both of these probabilities. In the text, when we refer to the EP approximation for calculating P B (f |S), we are using it as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Comparing</head><formula xml:id="formula_7">P OPT (f |S) to P B (f |S)</formula><p>We note that for MSE loss, we can sample to accurately estimate function probabilities, whereas for the CE loss, we must use the EP approximation to calculate the probability of functions. 6 When we compare P B (f |S) to P OPT (f |S) for CE loss, we take the functions found by the optimiser, which are obtained as described in Section 3.1.2, and calculate their P B (f |S) using the EP approximation. For MSE loss, both the P B (f |S) and the P OPT (f |S) are sampled independently, and probabilities are compared for functions found by both methods.</p><p>5. As for POPT(f |S) with MSE loss, we take the regression targets to be {-1, 1}, so thresholding occurs at 0 6. See Appendix A.2.2 for more details.</p><p>3.1.5 Calculating P B (f |S) for functions with generalisation error from 0% to 100%</p><p>For the zero training error case studied here, we define functions by their particular labelling on the test set E (as described in Section 3.1.1). A function can be generated by picking a certain labelling. Subsequently P B (f |S) for CE loss can be calculated using the EP approximation as described above. To study how P B (f |S) varies with generalisation error</p><p>G on E (the fraction of missclasified inputs on E), we perform the following procedure. For each value of G chosen, typically 10 functions are uniformly sampled by randomly selecting G |E| bits in E and flipping them. EP is then used to calculate P B (f |S) for those functions.</p><p>The probabilities P B (f |S) can range over many orders of magnitude. The low probability functions cannot be obtained by direct sampling, so that a full comparison with P OPT (f |S) is not feasible. This is more formally described in Appendix A.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">CSR complexity</head><p>The critical sample ratio (CSR) is a measure of complexity of functions expressed by DNNs <ref type="bibr" target="#b44">(Krueger et al., 2017)</ref>. It is defined with respect to a sample of inputs as the fraction of those samples which are critical samples. A critical sample is defined to be an input such that there is another input within a box of side 2r centred around the input, producing a different output (for discrete classification outputs). See Appendix E for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data sets</head><p>To efficiently sample functions, we use relatively small test sets (typically |E| = 100) and, as is often done in the theoretical literature, binarise our classification datasets. We define the datasets used below: MNIST: The MNIST database of handwritten numbers <ref type="bibr" target="#b45">(LeCun et al., 1999)</ref> was binarised with even numbers classified as 0 and odd numbers as 1. Unless otherwise specified, we used |S| = 10000 and |E| = 100. Fashion-MNIST: The Fashion-MNIST database <ref type="bibr" target="#b100">(Xiao et al., 2017)</ref> was binarised with T-shirts, coats, pullovers, shirts and bags classified as 0 and trousers, dresses, sandals, trainers and ankle boots classified as 1. Unless otherwise specified, we used |S| = 10000 and |E| = 100.</p><p>IMDb movie review dataset: We take the IMDb movie review dataset from Keras. The task is to correctly classify each review as positive or negative given the text of the review. We preprocess the set by removing the most common words and normalising. 7 This procedure was employed to make sure there are functions with high enough probability to be sampled multiple times with Experiments 1 and 2 above. Used with |S| = 45000 and |E| = 50.</p><p>Ionosphere Dataset: This is a small non-image dataset with 34 features 8 aimed at identifying structure in the ionosphere <ref type="bibr" target="#b81">(Sigillito et al., 1989)</ref>. Used with |S| = 301 and |E| = 50.</p><p>For image datasets, we will typically use normalised data (pixel values in range [0,1]) for MSE loss, and unnormalised data for CE loss (pixel values in range [0,255]).</p><p>7. We used the version of the dataset and preprocessing technique given here: <ref type="url" target="https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow">https://www.kaggle.com/ drscarlat/imdb-sentiment-analysis-keras-and-tensorflow</ref> 8. <ref type="url" target="https://archive.ics.uci.edu/ml/datasets/Ionosphere">https://archive.ics.uci.edu/ml/datasets/Ionosphere</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architectures</head><p>We used the following standard architectures. FCN: 2 hidden layer, 1024 node vanilla fully connected network <ref type="bibr">(FCN)</ref> with ReLU activations. CNN + (Max Pooling) + [BatchNorm]: Layer 1: Convolutional Layer with 32 features size 3 Ã 3. (Layer 1a: Max Pool 2 Ã 2). [Layer 1b: Batch Norm]. Layer 2: Flatten. Layer 3: FCN with width 1024. [Layer 3a: Batch Norm]. Layer 4: FCN, 1 output with ReLU activations. LSTM: Layer 1: Embedding layer. Layer 2: LSTM, 256 outputs. Layer 3: FCN, 512 outputs. Layer 4: Fully-Connected, 1 output with ReLU activations for the fully connected layers. Hyperparameters are, unless otherwise specified, the default values in Keras 2.3.0. See Appendix A.1.1 for details on the parameter initialisation. 4. Empirical results for P B (f |S) v.s. P OPT (f |S) for different architectures and datasets In this first of two main results sections, we focus on testing our hypothesis that P B (f |S) â P OPT (f |S) for FCN, CNN and LSTM architectures on MNIST, Fashion-MNIST, the IMDb review, and the Ionosphere datasets, using several variants of the SGD optimiser. In the following subsection will describe the main results in detail for an FCN on MNIST. The experiments in the next sections will be the same except for the architecture, dataset, or optimiser which will be varied. 4.1 Comparing P B (f |S) to P OPT (f |S) for an FCN on MNIST In Figure 1 we present a series of results for a standard DNN setup: an FCN (2 hidden layers, each 1024 node wide with ReLU activations), trained on (binarised) MNIST to zero training error with a training set size of |S| = 10, 000 and a test set size of |E| = 100. Note that even for this small test set, there are 2 100 â 1.3 Ã 10 30 functions with zero error on S, all of which an overparametrized DNN could express (Zhang et al., 2016) 9 . We chose standard values for batch size, learning rate, etc., if given by the default values in Keras 2.3.0 (e.g. batch size of 32 and learning rate of 0.01 for SGD). Our experiments in Section 5 and the appendices will show that our results are robust to the choice of these hyperparameters. Figure 1a compares the value of P B (f |S) and P SGD (f |S) for the highest probability functions of each distribution, for MSE loss. Each data point in the plot corresponds to a unique function (a unique classification of images in the test set E). The functions are obtained by sampling both P B (f |S) and P SGD (f |S) and taking the union of the set of functions obtained. P B (f |S) and P SGD (f |S) were estimated as frequencies from the corresponding sample as explained in Sections 3.1.2 and 3.1.3. If a function does not appear in one of the samples, we set its frequency to take the minimum value so that it would appear on top of one of the axes. For example, a function that appears in the SGD runs, but not in the sampling for P B (f |S), will appear on x-axis at the value obtained for P SGD (f |S). Here we used MSE loss rather than the more popular (and typically more computationally efficient) CE loss because 9. We also find in Figure 20a and Figure 20b that our 2-layer FCN is capable of expressing functions on MNIST with the full range of training and generalisation errors (e) P B (f |S) v.s. P Adagrad (f |S) for MSE loss; P Adagrad (f |S) was sampled n = 10 5 times (while the GP sample was the same as in (a)). Adagrad was overtrained until 64 epochs had passed with zero error. The average error is G = 1.53%. (f) is as (e) but with CE loss, so that the EP approximation was used for P B (f |S), making the estimate of P B (f |S) slightly less accurate. G = 2.63%.</p><p>for MSE, the posterior can be sampled from without further approximations, while for CE loss, the expectation propagation (EP) approximation needs to be used making P B (f |S) less accurate (see Appendix A.2 for further details).</p><p>Figure <ref type="figure" target="#fig_0">1a</ref> also demonstrates that P SGD (f |S) and P B (f |S) are remarkably closely correlated for MSE loss, and that a remarkably small number of functions account for most of the probability mass for both P B (f |S) and P SGD (f |S). To appreciate how remarkably tight this agreement is, consider the full scale of probabilities for functions f that achieve zero error on the MNIST training set. The average P B (f |S) of all these functions is 2 -100 â 10 -30 . Therefore the functions in Figure <ref type="figure" target="#fig_0">1a</ref> have probabilities that are many orders of magnitude higher than average. At the same time, P B (f |S) and P SGD (f |S) for these functions typically agree within less than one order of magnitude. Another way of quantifying the agreement is that 90% of the cumulative probability weight from both P SGD (f |S) and P B (f |S) for the test set E in Figure <ref type="figure" target="#fig_0">1a</ref> is made up from the contributions of only a few tens of functions with zero training error out of â 10 30 such possible functions (see vertical doted line in Figure <ref type="figure" target="#fig_0">1a</ref>). Moreover, these particular functions are the same for both P B (f |S) and P SGD (f |S). The agreement between the two methods is remarkable. Overall, the observations in Figure <ref type="figure" target="#fig_0">1a</ref> suggest that the main inductive bias of this DNN is present prior to training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1b plots the mean probability for obtaining a generalisation error of</head><formula xml:id="formula_8">G in the training set E, which is estimated as Ï( G ) P B (f |S) G where Ï( G ) = |E|!/((|E| -G |E|)!( G |E|)!)</formula><p>denotes the number of functions with G |E| errors on E, and P B (f |S) G denotes the expected value of P B (f |S), where the expectation is with respect to uniformly sampling from the set of functions with fixed G . As explained in Section 3.1.5, we estimate the average â¢ G by sampling, and we estimated P B (f |S) for each f in the sample using the EP approximation.</p><p>Figure <ref type="figure" target="#fig_0">1b</ref> can be interpreted as showing that the inductive bias encoded in P B (f |S) gives good generalisation. More precisely, we find that P B (f |S) is exponentially biased towards functions with low generalisation error. To illustrate how strong the bias is, we can look at Ï( G ). Over 50% of functions are in the range of G = 50 Â± 3 errors, while only 10 -23 % have G â¤ 3. Therefore for P B (f |S) to overcome the 'entropic' factor Ï( G ) and show the behaviour in Figure <ref type="figure" target="#fig_0">1b</ref>, it must in average give a probability many orders of magnitude higher to low error functions than to high error functions. In Appendix A.1.3, we also observed that the probability p i of misclassifying an image in the test set varies a lot between images, and that these probabilities are to first order independent. As a corollary, in Figure <ref type="figure" target="#fig_14">8</ref> we show for P B (f |S) and P SGD (f |S) that the probabilities of multiple images being misclassified can be accurately estimated from the products of the probabilities p i for misclassifying individual images. Thus this system appears to behave like a Poisson-Binomial distribution with independent and non-identically distributed random p i , which most likely also explains why log P B (f |S) G scales nearly linearly with G .</p><p>Although we cannot measure P SGD (f |S) for the high generalisation error functions, the agreement in Figure <ref type="figure" target="#fig_0">1a</ref> (and elsewhere in this paper) implies that P SGD (f |S) must also be on average orders of magnitude lower for high error functions than low error functions. However, at the moment we can only conjecture that P SGD (f |S) follows the same exponential behaviour as P B (f |S) over the whole range of G . Finally, in Appendix A.1.3, we make some further remarks and caveats about this experiment, and other similar experiments.</p><p>Figure <ref type="figure" target="#fig_0">1c</ref> shows the correlation between the complexity of the functions obtained to create Figure <ref type="figure" target="#fig_0">1b</ref>, and their generalisation error, as well as their P B (f |S) (from EP approximation) represented in their color. The complexity measure we used is the critical sample ratio (CSR) complexity <ref type="bibr" target="#b44">(Krueger et al., 2017)</ref> computed on the inputs in E, which measures what fraction of inputs are near the decision boundary (see Section 3.1.6).</p><p>Figure <ref type="figure" target="#fig_0">1c</ref> also shows that there is a inverse correlation between the generalisation of a function and its CSR complexity, as well as between P B (f |S) and CSR. In Section 2.2, we showed that P B (f |S) is proportional to the prior probability of a function P (f ) for functions that have zero error on the training set S. We can thus understand the inverse correlation between P B (f |S) and CSR in the light of previous simplicity bias results showing that the prior P (f ) of Bayesian DNNs is exponentially biased towards functions with low Kolmogorov complexity (simple functions) <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018;</ref><ref type="bibr" target="#b61">Mingard et al., 2019)</ref>. In <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref>, it was further shown for an FCN on a subsample of MNIST that P (f ) correlated remarkably well with CSR 10 , and our results are in agreement with that finding. The results in this figure extend those of Figure <ref type="figure" target="#fig_0">1b</ref> to show that P B (f |S) is biased both towards low error and simple functions, and that simple functions are the ones that tend to have good generalisation on MNIST.</p><p>Figure <ref type="figure" target="#fig_0">1d</ref> shows the correlation between P B (f |S) and G for functions used for Figure <ref type="figure" target="#fig_0">1a</ref>. We note that, as can also be observed in Figure <ref type="figure" target="#fig_0">1a</ref>, values of P B (f |S) are high for low error functions, and high error functions have relatively lower values of P B (f |S). This figure also uses colour to show which functions were not found in the sampling of P SGD (f |S). It shows clearly that SGD finds all the high P B (f |S) functions.</p><p>Figure <ref type="figure" target="#fig_0">1e</ref> shows the same type of experiment as in Figure <ref type="figure" target="#fig_0">1b</ref>, but using a different SGDbased optimiser, Adagrad <ref type="bibr" target="#b25">(Duchi et al., 2011)</ref> with overtraining (where training was halted after 64 epochs had passed with 100% training accuracy). We see that it exhibits similar correlation between P B (f |S) and P OPT (f |S) to vanilla SGD (and very similar agreement was observed without overtraining). We will see throughout the paper remarkably good correlations between P B (f |S) and P OPT (f |S) holds for a large range of optimisers and hyperaparameters Figure <ref type="figure" target="#fig_0">1f</ref> shows the same type of experiment as in Figure <ref type="figure" target="#fig_0">1a</ref>, but using CE loss, the Adagrad optimiser, and overtraining (also to 64 epochs). See Figure <ref type="figure" target="#fig_0">11b</ref> for the equivalent plot but without overtraining. As we are using CE loss (see Section 3.1.3 and Section 3.1.4), we sample functions from P OPT (f |S), and then use the EP to estimate P B (f |S) for the functions obtained. We find similar results to Figure <ref type="figure" target="#fig_0">1e</ref>, where we used MSE loss (and direct sampling for P B (f |S)). The errors introduced by the EP approximation may explain why the correlation does not follow the x=y line as closely as it does for the MSE calculations. Nevertheless, the correlation between P B (f |S) and P Adagrad (f |S) is strong, providing evidence that our results for an FCN on MNIST are not an artefact of the exact optimiser or loss function used.</p><p>10. Furthermore in <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref>, it was shown that this was not an exclusive property of CSR and that any measure that could approximate Kolmogorov complexity seems to also correlate well with P (f ). We next turn to a more complex dataset, namely Fashion-MNIST <ref type="bibr" target="#b100">(Xiao et al., 2017)</ref> which consists of images of clothing, as well as a more complex network architecture, the <ref type="bibr">CNN (LeCun et al., 1999)</ref> which was designed in part to have a better inductive bias for images. See Section 3.2 and Section 3.3 for details on dataset and architecture. We can see in Figure <ref type="figure">2</ref> a strong correlation between P B (f |S) and the probabilities found by the Adam optimiser (Kingma and Ba, 2014), a variant of SGD. Note that instead of MSE loss we used CE loss because it is more efficient. A downside of this choice is that we need to use the EP approximation for the GP calculations (see Appendix A.2.2). Although the correlation is strong, it does not follow x=y as closely as we generally find for MSE loss, which is quite possibly an effect of the EP approximation. See Figure <ref type="figure" target="#fig_4">13</ref> for an example with MSE loss where the correlation does follow x=y more closely. Both the FCN and the CNNs exhibit a strong bias towards low error functions on Fashion-MNIST as we can see in Figure <ref type="figure" target="#fig_4">13c</ref> and Figure <ref type="figure" target="#fig_4">13d</ref>.</p><p>For an example of how the effects of architecture modifications can be observed in the function probabilities, compare results in Figure <ref type="figure">2b</ref> for the vanilla CNN to those in Figure <ref type="figure">2c</ref> for a CNN with max-pooling <ref type="bibr" target="#b31">(He et al., 2016)</ref>, a method designed to improve the inductive bias of the CNN. As expected, the generalisation performance of the CNN improves, and an important contributor is the increase in the probability of the highest probability 1-error function in both P B (f |S) and P Adam (f |S), directly demonstrating an enhancement of the inductive bias. See Figure <ref type="figure" target="#fig_4">13</ref> for related results. This example demonstrates how a function based picture as well as analysis of the Bayesian P B (f |S) sheds light on the inductive bias of a DNN. Such insights could help with architecture search, or more generally with developing new architectures with improved implicit bias toward desired low error functions. In Figure <ref type="figure" target="#fig_2">3</ref> we compare P B (f |S) to the output of the neural tangent kernel (NTK) <ref type="bibr" target="#b38">(Jacot et al., 2018)</ref>, which approximates gradient descent in the limit of infinite width and infinitesimal learning rate. The generalisation error of NTK and NNGPs have been shown to be relatively close, and they produce similar functions on simple 1D regression <ref type="bibr" target="#b49">(Lee et al., 2019;</ref><ref type="bibr" target="#b68">Novak et al., 2020)</ref>. Here we show that this similarity also holds for the function probabilities for a more complex classification task. However, we also find the NTK misses many relatively high probability functions that both SGD and the GP find. We are currently investigating this surprising behaviour, which may arise from the infinitesimal learning rate. Their low probability may also be exacerbated by the fact that in Figure <ref type="figure" target="#fig_2">3</ref> the NTK is very highly biased towards one 2-error function, forcing other functions to have low cumulative probability. Again, this example demonstrates how a function based picture picks up rich details of the behaviour that would be missed when simply comparing generalisation error.   We test a more complex DNN with a LSTM layer <ref type="bibr">(Hochreiter and Schmidhuber, 1997b)</ref>, applied to a problem of sentiment analysis on the IMDb movie database. We used a smaller test set |E| = 50 and a larger training set |S| =45,000 to ensure that generalisation was good enough to ensure that functions are found with sufficient frequency to be able to extract probabilities. As can be seen in Figure <ref type="figure" target="#fig_10">4a</ref> we again observe a reasonable correlation between the functions found by Bayesian sampling, and those found by the optimiser. Figure <ref type="figure" target="#fig_10">4b</ref> also shows that, as observed for other datasets, this system is highly biased towards low error functions. We show some further experiments with the LSTM in Figure <ref type="figure" target="#fig_17">14</ref> in Appendix D, including an experiment with MSE loss to avoid the EP approximation. As another non-image classification example, we use the small non-image Ionosphere dataset (with a training set of size 301), using an FCN with 3 hidden layers of width 256. As can be seen in Figure <ref type="figure" target="#fig_10">4c</ref>, for MSE loss we find a fairly good correlation. Further details and an example with CE loss can be found in Figure <ref type="figure" target="#fig_7">15</ref>.</p><formula xml:id="formula_9">(a) P B (f |S) v.s. P Adam (f |S) (b) P B (f |S) v.s. G (c) P B (f |S) v.s. P Adam (f |S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effects of training set size</head><p>We performed experiments comparing P B (f |S) and P OPT (f |S) for different training set sizes for the FCN on MINST. We observe that increasing the amount of training data from |S| = 1000 to |S| = 20000 increases the bias towards low error functions. This increase has the following effects: 1) An increase in the value of P B (f |S) and P SGD (f |S) for functions with low G by several orders of magnitude, 2) an increase by several orders of magnitude of P B (f |S) and P OPT (f |S) for the mode functions (the ones with highest probability), 3) A decrease in the number of functions that cumulatively take up 90% of the observed probability weight, and 4) a significant increase in the tightness of correlation between P B (f |S) and P OPT (f |S). See Appendix C in Appendix C for detailed results and plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Results for other test sets</head><p>For the experiments shown in this section, sampling efficiency considerations means that we have limited ourselves to a relatively small test sets (|E| â¤ 100). In Figure <ref type="figure" target="#fig_0">12</ref>, we have checked that other test sets also show close agreement between P B (f |S) and P SGD (f |S). For larger |E|, P OPT (f |S) quickly becomes impossible to directly measure empirically -doubling the test set roughly means squaring the number of samples to obtain qualitatively similar results, as the values for P B (f |S) decrease exponentially with test set size. However, if we assume that the images are approximately independently distributed throughout the larger test set, as Appendix B suggests, then we can estimate the highest probabilities from products of P B (f |S) or P SGD (f |S) on the smaller sets.</p><p>5. The effect of hyperparameter changes and optimisers on P B (f |S) and P OPT (f |S)</p><p>In the first section we focussed on the first-order similarity between P B (f |S) and P OPT (f |S).</p><p>In this second main results section, we focus on second-order effects that affect P OPT (f |S) differently from P B (f |S). These include the effects of hyperparameter settings and optimiser choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Changing batch size and learning rate</head><p>In a well-known study, <ref type="bibr" target="#b42">(Keskar et al., 2016)</ref> showed that, for a fixed learning rate, using smaller batch sizes could lead to better generalisation. In Figure <ref type="figure" target="#fig_7">5</ref> (a)-(c) we observe this same effect but reflected in the more finely grained spectrum of function probabilities. For batch size 512, we also reproduce in Figure <ref type="figure" target="#fig_7">5d</ref> the effect observed in <ref type="bibr" target="#b29">(Goyal et al., 2017;</ref><ref type="bibr" target="#b36">Hoffer et al., 2017;</ref><ref type="bibr">Smith et al., 2017)</ref>, that speeding up the learning rate for a fixed batch size can mimic the improvement in G for smaller batches. Interestingly, as can be seen by comparing Figures 5d to 5f, the overall correlation of the function probability spectrum appears tighter for the 128 and 512 batch size with the same learning rates, even though the generalisation errors are different. However, if the learning rate is increased 4Ã for the the 512 batch size system, then there is a closer correlation with batch size 128 for the higher probability functions. It is these latter functions that dominate the average for G and so the closer correlation for those functions, rather than the less good correlation for low probability functions, explains the better agreement seen in generalisation error for the two systems. Finally, in Figure <ref type="figure" target="#fig_14">18</ref> of Appendix D, we vary batch size for MSE, finding different trends to CE loss. For MSE, increasing batch size leads to better generalisation due to second order effects where P SGD (f |S) preferentially converges on a few key higher probability/lower error functions. The batch size can be correlated with the noise spectrum of the underlying Langevin equation that describes SGD <ref type="bibr" target="#b9">(Bottou et al., 2018;</ref><ref type="bibr" target="#b39">Jastrzebski et al., 2018;</ref><ref type="bibr" target="#b106">Zhang et al., 2018)</ref>. What our function based results demonstrate is that the behaviour of the optimiser on the loss-landscape is affected in subtle ways by the form of the loss function, as well as the amount noise, and possibly also by correlations in the noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Changing optimisers</head><p>We trained the FCN on MNIST with different optimisers (Adam, Adagrad, RMSprop, Adadelta), and found that to first order P B (f |S) correlated well with P OPT (f |S) for all four optimisers. We also observed some second order effects, including that the distribution of P Adam (f |S) and P Adagrad (f |S) were very similar to one another, as were P RMSprop (f |S) and P Adadelta (f |S), but there was noticeable variation between the two groups. We find that P Adam (f |S) with batch size of 32 is very similar to P RMSprop (f |S) with a batch size of 128. The effect of optimiser choice, batch size, learning rate, and other hyperameters is complex, and the parameter space is large. Analysing optimisers in function-space could be a way to better understand the interaction of these choices with the loss landscape, and understanding the effects of hyperparameter tuning. See Appendix C.1 for further detail and the plots.</p><p>6. Heuristic arguments for the correlation between P B (f |S) and P SGD (f |S)</p><p>At first sight it may seem rather surprising that SGD, which follows gradients down a complex loss-landscape, should converge on a function f with a probability anything like the Bayesian posterior P B (f |S) that upon random sampling of parameters, a DNN generates functions f conditioned on S. Indeed, in the general case of an arbitrary learner we don't expect this correspondence to hold. However, as shown e.g. in Fig <ref type="figure" target="#fig_0">1</ref>, P B (f |S) is orders of magnitude larger for functions with small generalisation error than it is for functions with poor generalisation. As explained in Sections 7.3 and 7.5, such an exponential bias towards low complexity/low error functions can be expected on fairly general grounds <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018;</ref><ref type="bibr" target="#b61">Mingard et al., 2019;</ref><ref type="bibr" target="#b20">Dingle et al., 2018</ref><ref type="bibr" target="#b21">Dingle et al., , 2020))</ref>. If our null expectation is of a large variation in the prior probabilities, then the good correlation can be heuristically justified by a landscape picture <ref type="bibr" target="#b93">(Wales et al., 2003)</ref>, where P B (f |S) is interpreted as the "basin volume" V B (f ) (with measure p par (Î¸)) of function f ), while P SGD (f |S) is interpreted as the "basin of attraction" V SGD (f ), which is loosely defined as a measure of the set of initial parameters Î¸ i for which the optimiser converges to f with high probability (this concept also found in related form in the dynamical systems literature <ref type="bibr" target="#b89">(Strogatz, 2018)</ref>). If V B (f ) varies over many orders of magnitude, then it seems reasonable to expect that V SGD (f ) should correlate with V B (f ), as illustrated schematically in Figure <ref type="figure" target="#fig_8">6a</ref>. Such general intuitions about landscapes are widely held <ref type="bibr" target="#b93">(Wales et al., 2003;</ref><ref type="bibr" target="#b57">Massen and Doye, 2007;</ref><ref type="bibr" target="#b5">Ballard et al., 2017)</ref>, and have also been put forward for the particular landscapes of deep learning; see in particular <ref type="bibr" target="#b99">Wu et al. (2017)</ref> who also argue that functions with good generalisation have larger basins of attraction.</p><p>Another source of intuition follows form a well trodden path linking basic concepts from statistical mechanics to optimisation and learning theory. For example, simple gradient descent (GD) with a small amount of white noise can be described by an over-damped Langevin equation <ref type="bibr" target="#b95">(Welling and Teh, 2011;</ref><ref type="bibr">Smith and Le, 2017;</ref><ref type="bibr" target="#b62">Naveh et al., 2020)</ref> that converges (under some light further conditions) to the Boltzmann distribution The Boltzmann distribution can, in turn, be interpreted as being equivalent to a Bayesian posterior P B (f |S) â e S(f )-Î²E(f ) <ref type="bibr" target="#b55">(MacKay, 2003)</ref> where S(f ) is configurational "entropy" that counts the number of states that generate f and encodes the prior, and E(f ) represents the energy, encoding the log likelihood or loss function. For SGD the equivalent coarse-grained differential equation reduces to Langevin equation with anisotropic noise (Smith and Le, 2017; <ref type="bibr" target="#b106">Zhang et al., 2018)</ref>  Cartoon of a biased loss-landscape. The three functions f 1 , f 2 and f 3 all reach zero classification error (dashed red line), but due to bias in the parameter-function map, the "basin size" V B (f 1 ) V B (f 2 ), V B (f 3 ), which typically implies that for the "basins of attraction" Here functions with frequency &lt; 10 are also shown on the plot. The correlation is much less pronounced than for the unrandomised case shown in Figure <ref type="figure" target="#fig_0">1a</ref>. Dots on the axes denote functions found by just one of the two methods. Let F be the set of functions found by both the optimiser and under GP sampling. Then f âF P B (f |S) = 99.3%, and and doesn't exactly converge to the Bayesian posterior <ref type="bibr" target="#b56">(Mandt et al., 2017;</ref><ref type="bibr" target="#b10">Brosse et al., 2018)</ref>. Nevertheless, it has been conjectured that with small step size, SGD may approximate the Bayesian posterior <ref type="bibr" target="#b62">(Naveh et al., 2020;</ref><ref type="bibr" target="#b14">Cohen et al., 2019)</ref>, as we empirically find in our experiments. These connections are rich and worth exploring further in this context. Nevertheless, some caution is needed with these analogies to statistical mechanics because they depend on assumptions which may only to hold on prohibitively long time-scales.</p><formula xml:id="formula_10">V SGD (f 1 ) V SGD (f 2 ), V SGD (f 3 ). P B (f |S) is proportional to V B (f</formula><p>A better analogy may be to the "arrival of the frequent" phenomenon in evolutionary dynamics <ref type="bibr" target="#b77">(Schaper and Louis, 2014)</ref>, which, like the "basin of attraction" arguments, does not require steady state. Instead it predicts which structures are likely to be reached first by an evolutionary process. For RNA secondary structures, for example, it predicts that a stochastic evolutionary process will reach structures with a probability that to first order is proportional to the likelihood that uniform random sampling of genotypes produces the structure. Indeed, this phenomenon -where the probability upon random sampling predicts the outcomes of a complex search process -can be observed in naturally occurring RNA <ref type="bibr" target="#b19">(Dingle et al., 2015)</ref>, the result of evolutionary dynamics. This type of non-equilibrium analysis may be more relevant for the way we train most of the DNNs in this paper, since we stop the first time 0 training error is reached. The analogy between these evolutionary results with what we observe for SGD is intriguing, but needs further exploration.</p><p>To illustrate the effect of the amount of bias in the posterior, we randomise labels for MNIST and calculate the P B (f |S). As we can see in Figure <ref type="figure" target="#fig_8">6b</ref>, this results in a less strongly biased posterior. The mean log-probability log(P B (f |S)) v.s. G curve becomes less steep with increasing corruption For a relatively small fraction of low error functions to dominate, as they do for zero corruptions in Figure <ref type="figure" target="#fig_0">1a</ref>, the bias must be strong enough here to overcome the "entropic" factor Ï( G ). For the 20% and 50% corruption this is clearly not the case, and a huge number of functions with larger error will dominate P B (f |S) and P SGD (f |S). As can be seen in Figure <ref type="figure" target="#fig_8">6c</ref>, one effect of weaker bias is that the correlation between the optimiser and the Bayesian sampling is much less strong. This behaviour is consistent with the heuristic arguments above, which should only work if the differences in basin volumes are large enough to overcome the myriad other factors that can affect P OPT (f |S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related work on inductive bias on neural networks</head><p>In this section we summarise some key aspects of the literature related to why DNNs exhibit good generalisation while overparameterised, expanding on some briefer remarks in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">The link between inductive bias and generalisation</head><p>Much of the work on inductive biases in stochastic gradient descent (SGD) is framed as a discussion about generalisation. The two concepts are of course intimately related. Before discussing related work on inductive bias DNNs, it may be helpful to distinguish two different questions about generalisation:</p><p>1) Question of over-parameterised generalisation: Why do DNNs generalise at all in the overparameterised regime, where classical learning theory doesn't guarantee generalisation?</p><p>2) Question of fine-tuned generalisation: Given that vanilla DNNs already generalise reasonably well, how can architecture choice and hyperparameter tuning further improve generalisation?</p><p>The first question arises because among the functions that an overparameterised DNN can express, the number that can fit a training data set S, but generalise poorly, is typically many orders of magnitude larger than the number that achieve good generalisation. From classical learning theory we would therefore expect extremely poor generalisation. However, in practice it is often found that many DNN architectures, as long as they are expressive enough to fit the data, generalise sufficiently well to imply a significant inductive bias towards a small fraction of functions that generalise well. This question is also related to the conundrum of why DNNs avoid the "curse of dimensionality", which relates to the poor generalisation that certain highly expressive non-parametric models have in high dimensions <ref type="bibr" target="#b24">(Donoho et al., 2000)</ref>. <ref type="bibr" target="#b92">Valle-PÃ©rez et al. (2018)</ref> argue that the curse of dimensionality is linked to a prior which is not sufficiently biased and that DNNs may avoid this problem by virtue of the strong bias in the prior.</p><p>The second question arises from two common experiences in DNN research. Firstly, changes in architecture can lead to important improvements in generalisation. For example, a CNN with max-pooling typically performs better than a vanilla FCN on image data. Secondly, hyperparameter tuning within a fixed architecture can lead to further improvements of generalisation. While these methods of improving generalisation are important in practice, the starting point is normally a DNN that already has enough inductive bias to raise question 1) above. It is therefore important not to conflate the study of question 2) -as vital as this may be to successful practical implementations -with the more general question of why DNNs generalise in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Related work on implicit bias in optimiser-trained networks</head><p>As mentioned in the introduction, there is an extensive literature on inductive biases in SGD. Much of this literature is empirical: improvements are observed when using particular tuned hyperparameters with variants of SGD. One of the most common rationalisation is in terms of "flatness" which is inspired by early work <ref type="bibr">(Hochreiter and Schmidhuber, 1997a)</ref> who predicted that flatter minima would generalise better. Flatness is often measured using some combination of the eigenvalues of the Hessian matrix for a trained DNN. <ref type="bibr" target="#b42">(Keskar et al., 2016)</ref> showed that DNNs trained with small batch SGD generalise better than identical models trained with large batch SGD (by up to 5%), and also found a correlation between small batch size and minima that are less "sharp" (using not the eigenvalues of the Hessian but a more computationally tractable sensitivity measure). While these results are genuinely interesting, they are mainly relevant to issues raised by question 2 above. For example in <ref type="bibr" target="#b42">(Keskar et al., 2016)</ref> the authors explicitly point out that their results are not about "overfitting" (e.g. question 1 above).</p><p>The effects of changing hyperparameters can be subtle. For example, another series of recent papers <ref type="bibr" target="#b29">(Goyal et al., 2017;</ref><ref type="bibr" target="#b36">Hoffer et al., 2017;</ref><ref type="bibr">Smith et al., 2017)</ref> suggest that better generalisation with small batch SGD may be caused by the fact that the number of optimisation steps per epoch decreases when the batch size increases. These studies showed that a similar improvement in generalisation performance to that found by reducing batch size can be created by increasing the learning rate, or by overtraining (i.e. by continuing to train after 100% accuracy has been reached). In particular, in <ref type="bibr" target="#b36">(Hoffer et al., 2017)</ref> it was argued that overtraining does not generally negatively impact generalisation, as naive expectations based on overfitting might suggest. These results also challenge some theoretical studies that suggested that SGD may control the capacity of the models by limiting the number of parameter updates <ref type="bibr" target="#b11">(Brutzkus et al., 2017)</ref>.</p><p>In another interesting paper, <ref type="bibr" target="#b106">Zhang et al. (2018)</ref> derive a Langevin type equation for both SGD. And argue that in contrast to GD, the noise is anisotropic, and that this may explain why SGD is more likely to find "flatter minima". Similarly, <ref type="bibr" target="#b39">Jastrzebski et al. (2018)</ref> argue that isotropic SGD-induced noise also helps push the optimiser away from sharper minima. An important caveat to the work on sharpness can be found in the work of Dinh et al. <ref type="bibr" target="#b22">(Dinh et al., 2017)</ref> who use the non-negative homogeneity of the ReLU activation function to show that for a number of the measures used in the papers cited above, the "flatness" can be made arbitrarily large (or sharp) without changing the function (and therefore the generalisation performance) that the DNN expresses. This result suggests that care must be used when interpreting local measures of flatness. Finally in this vein, generalisation has also been linked to related concepts including low frequency <ref type="bibr" target="#b71">(Rahaman et al., 2018)</ref>, and to sensitivity to changes in the inputs <ref type="bibr" target="#b3">(Arpit et al., 2017;</ref><ref type="bibr">Novak et al., 2018a)</ref>.</p><p>There is much more literature on SGD induced inductive bias, but the upshot is that while fine-tuning optimiser hyperparameters can be very important for improving generalisation, and by implication, the inductive bias of a DNN, a complete understanding remains elusive. Moreover, where improvements are found, these tend to be in the class of answers to question 2) above. An important example of a paper on flatness that does explicitly address question 1 above is <ref type="bibr" target="#b99">(Wu et al., 2017)</ref>, who show that generalisation trends for data with different levels of corruption correlates with the log of the product of the top 50 eigenvalues of the Hessian both for SGD and for GD trained networks. By heuristically linking their local flatness measure to the global basin volume, they make a very similar argument to the one we flesh out in more detail here, namely that the basin of attraction volume of "good" solutions is much larger than that of "bad" solutions that do not generalise well.</p><p>Significant theoretical effort has been spent on extracting properties of a trained neural network that could be used to explain generalisation. By implication, these investigations should also help illuminate the nature of the implicit bias of trained networks. For example, investigators have attempted to use sensitivity to perturbations (whether in inputs or weights) to explain the generalisation performance either using a PAC-Bayesian analysis <ref type="bibr" target="#b6">(Bartlett et al., 2017;</ref><ref type="bibr" target="#b26">Dziugaite and Roy, 2017;</ref><ref type="bibr">Neyshabur et al., 2018)</ref>, or a compression approach <ref type="bibr" target="#b1">(Arora et al., 2018;</ref><ref type="bibr" target="#b107">Zhou et al., 2019)</ref>. In contrast to the work described above that studies the specific effect of hyperparameter tuning on SGD, much of the work listed in this paragraph is directly applicable to question 1. A very comprehensive review of this line of work empirically finds that the PAC-Bayesian sensitivity approaches seem the most promising <ref type="bibr" target="#b40">(Jiang et al., 2019)</ref>, but no clear answer to the question 1 has emerged.</p><p>The more theoretical side of the study of SGD has also seen recent progress. For example, <ref type="bibr" target="#b86">(Soudry et al., 2018)</ref> showed that SGD finds the max-margin solution in unregularised logistic regression, whilst it was shown in <ref type="bibr" target="#b11">(Brutzkus et al., 2017)</ref> that overparameterised DNNs trained with SGD avoid over-fitting on linearly separable data. More recently, <ref type="bibr" target="#b0">(Allen-Zhu et al., 2019)</ref> proved agnostic generalisation bounds for SGD-trained DNNs (up to three layers), which impose less restrictive assumptions (on the data, architecture, and optimiser) than previous works. Such theoretical analyses may be a potentially fruitful source of new ideas to explain generalisation.</p><p>Another interesting direction is to investigate properties of the loss-landscape itself. Several studies have shown interesting parallels between the loss landscape of DNNs and the energy landscape of spin glasses <ref type="bibr" target="#b13">(Choromanska et al., 2015;</ref><ref type="bibr" target="#b4">Baity-Jesi et al., 2019;</ref><ref type="bibr" target="#b7">Becker et al., 2020)</ref>. While such insights may help explain why SGD works so well as an optimiser in these high dimensional spaces, it is at present less clear how these studies help explain question 1) above.</p><p>A completely different theme builds on the concept of an information bottleneck <ref type="bibr" target="#b91">(Tishby and Zaslavsky, 2015;</ref><ref type="bibr" target="#b80">Shwartz-Ziv and Tishby, 2017)</ref> which suggest that generalisation arises from information compression in deeper layers, aided by SGD. However, recent work <ref type="bibr" target="#b76">(Saxe et al., 2019)</ref> suggests that the compression is strongly affected by activation functions used, suggesting again that this approach is not general enough to capture the implicit bias needed to answer question 1. We note that the debate about this theme is ongoing.</p><p>Finally, it is important to note that simple vanilla gradient descent (GD), when it can be made to converge, does not differ that much (on the scale of question 1 above) from SGD and its variants in generalisation performance <ref type="bibr" target="#b42">(Keskar et al., 2016;</ref><ref type="bibr" target="#b99">Wu et al., 2017;</ref><ref type="bibr" target="#b106">Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Choi et al., 2019)</ref>. Therefore if training with an optimiser itself generates the inductive bias needed to answer question 1, that bias must already largely be present in simple GD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Related work on implicit bias in random neural networks</head><p>We briefly review work inspired by a powerful result from algorithmic information theory (AIT) called the coding theorem <ref type="bibr" target="#b51">(Li and Vitanyi, 2008)</ref>. First derived by Levin <ref type="bibr" target="#b50">(Levin, 1974)</ref>, and building on concepts pioneered by Solomonoff <ref type="bibr" target="#b85">(Solomonoff, 1964)</ref>, it is closely related to more recent bound applicable to a wider range of input-output maps <ref type="bibr" target="#b20">(Dingle et al., 2018</ref><ref type="bibr" target="#b21">(Dingle et al., , 2020))</ref>. This bound predicts (under certain fairly general conditions that the maps must fulfil) that upon randomly sampling the parameters of an input-output map M , the probability P (f ) of obtaining output f can be bounded as</p><formula xml:id="formula_11">P (f ) â¤ 2 -K(f |M )+O(1) â 2 -a K(f )+b (4)</formula><p>where K(f ) is the Kolmogorov complexity of f , the O(1) terms do not depend on the outputs (at least asymptotically), K(f ) is a suitable approximation to K(f ) and a and b are parameters that depend on the map, but not on f . The computable bound was empirically shown to work remarkably well for a wide range of input-output maps from across science and engineering <ref type="bibr" target="#b20">(Dingle et al., 2018)</ref>, giving confidence that it should be widely applicable, at least for maps that satisfy the conditions needed for it to apply. In addition, a statistical lower-bound can be derived that predicts that most of the probability weight will lie relatively close to the bound <ref type="bibr" target="#b21">(Dingle et al., 2020)</ref>. The application of this bound to DNNs was first shown in <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref>. We note that the input-output map of interest is not the map from inputs to DNN outputs, but rather the map from the network parameters to the function f it produces on inputs X which was described in Definition 2.1. The prediction of Equation ( <ref type="formula">4</ref>) for a DNN with parameters sampled randomly (from, for example, truncated i.i.d. Gaussians) is that, if the parameter-function map is sufficiently biased, then the probability of the DNN producing a function f on input data x n i=0 drops exponentially with increasing complexity of the function f . Note that technically we should write f as f |X to indicate the dependence of the function modelled by the DNN on the inputs X . We also note that the AIT bound of Equation ( <ref type="formula">4</ref>) on its own does not force a map to be biased. It still holds for a uniform distribution. But if the map is biased, then it will be biased according to Equation (4).</p><p>In <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> it was shown empirically that this very general prediction of Equation ( <ref type="formula">4</ref>) holds for the P (f ) of a number of different DNNs. This testing was achieved both via direct sampling of the parameters of a small DNN on Boolean inputs and with NNGP calculations for more complex systems. In a complementary approach <ref type="bibr" target="#b61">(Mingard et al., 2019</ref>) some exact results were proven for simplified networks, that are also consistent with the bound of Equation ( <ref type="formula">4</ref>). In particular, they proved that for a perceptron with no bias term, upon randomly sampling the parameters (with a distribution satisfying certain weak assumptions), any value of class-imbalance was equally likely. There are many fewer functions with high class imbalance (low "entropy") than low class imbalance. Low entropy implies low K(f ) (but not the other way around). Thus, these results imply a bias of P (f ) towards certain simple functions. They also proved that for infinite-width ReLU DNNs, this bias becomes monotonically stronger as the number of layers grows. A different direction was pursued in <ref type="bibr" target="#b18">(De Palma et al., 2018)</ref>, who showed that, upon randomly sampling the parameters of a ReLU DNN acting on Boolean inputs, the functions obtained had an average sensitivity to inputs which is much lower than if randomly sampling functions. Functions with low input sensitivity are also simple, thus proving another manifestation of simplicity bias present in these systems.</p><p>On the other hand, in a recent paper <ref type="bibr" target="#b103">(Yang and Salman, 2019)</ref>, it was shown that for DNNs with activation functions such as Erf and Tanh, the bias starts to disappear as the system enters the "chaotic regime", which happens for weight variances above a certain threshold, as the depth grows <ref type="bibr" target="#b70">(Poole et al., 2016)</ref> (note that ReLU networks don't have such a chaotic regime). While these hyperparameters are not typically used for DNNs, they do show that there exist regimes where there is no simplicity bias. Note that the AIT coding theorem bound Equation (4) still holds, but P (f ) is simply approaching a uniform distribution, and the bound becomes loose for small complexity. These results are also interesting because, if the bias becomes weaker, then it may also be the case that the correlation between P B (f |S) and P SGD (f |S) starts to disappear, an effect we are currently investigating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Related work comparing optimiser-trained and Bayesian neural networks</head><p>Another set of investigations studying random neural networks use important recent extensions of Neal's seminal proof <ref type="bibr" target="#b63">(Neal, 1994</ref><ref type="bibr" target="#b64">(Neal, , 2012) )</ref> -that a single-layer DNN with random i.i.d. weights is equivalent to a Gaussian process (GP) <ref type="bibr" target="#b54">(Mackay, 1998)</ref> in the infinite width limit -to multiple layers and architectures <ref type="bibr" target="#b47">(Lee et al., 2017;</ref><ref type="bibr">Matthews et al., 2018;</ref><ref type="bibr" target="#b67">Novak et al., 2018b;</ref><ref type="bibr" target="#b27">Garriga-Alonso et al., 2019;</ref><ref type="bibr">Yang, 2019b)</ref>. These studies have used this correspondence to effectively perform a very good approximation to exact Bayesian inference in DNNs. When they have compared them to SGD-trained DNNs <ref type="bibr" target="#b47">(Lee et al., 2017;</ref><ref type="bibr">Matthews et al., 2018;</ref><ref type="bibr" target="#b67">Novak et al., 2018b)</ref>, the results have generally shown a close agreement between the generalisation performance of optimiser-trained DNNs and their corresponding Bayesian neural network Gaussian process (NNGP).</p><p>In this context another significant development is the introduction of the neural tangent kernel (NTK) <ref type="bibr" target="#b38">(Jacot et al., 2018)</ref> which approximates the dynamics of an infinite width DNN with parameters that are trained by gradient descent in the limit of an infinitesimal learning rate. Recent comparisons to NNGPs show relatively similar performance of the NTK, see for example <ref type="bibr" target="#b2">(Arora et al., 2019;</ref><ref type="bibr" target="#b49">Lee et al., 2019;</ref><ref type="bibr" target="#b68">Novak et al., 2020)</ref>. While there are small performance differences, the overall agreement between NNGPs and the NTK or optimiser trained DNNs is close enough to suggest that the primary source of inductive bias needed for question 1 above is already present in the untrained network, and is essentially maintained under training dynamics.</p><p>The linearisation of DNNs offered by NTK can also be used to prove that, in this regime, GD samples from the Bayesian posterior in a sample-then-optimise fashion. For linear regression models, <ref type="bibr" target="#b58">Matthews et al. (2017)</ref> showed that solutions after training GD with a Gaussian initialisation correspond to exact posterior samples. This idea is also related to Deep Ensembles which has been proposed to be "approximately Bayesian" in <ref type="bibr" target="#b97">Wilson and Izmailov (2020)</ref>.</p><p>In this context, further indirect evidence comes from Valle-PÃ©rez et al. ( <ref type="formula">2018</ref>) who used a simple PAC-Bayesian bound <ref type="bibr" target="#b60">(McAllester, 1999</ref>) that applies to exact Bayesian inference, to predict the generalisation error of SGD-trained DNNs. The bound was shown to provide relatively tight predictions for optimiser-trained DNNs for an FCN and CNNs on MNIST, Fashion-MNIST and CIFAR-10. Moreover, this bound, which takes the Bayesian marginal likelihood as input, reproduced trends such as the increase in the generalisation error upon an increased fraction of randomised labels.</p><p>These lines of work serve as independent evidence to suggest that optimiser-trained DNNs behave very similarly to the same DNNs trained with Bayesian inference, and helped inspire the work in this paper, where we directly tackle this question. These studies also suggest that the infinite-width limit may be enough to answer question 1, as the number of parameters in a DNN typically doesn't have a drastic effect on generalisation (as long as the network is expressive enough to fit the data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Related work on complexity of data, simplicity bias and generalisation</head><p>In Section 7.3, we discussed work showing that DNNs may have an inductive bias towards simple functions in their parameter-function map. Here, we briefly discuss how this "simplicity bias" concept may connect to generalisation. As implied by the no free lunch theorem <ref type="bibr" target="#b98">(Wolpert and Waters, 1994)</ref>, a bias towards simplicity does not automatically imply good generalisation. Instead certain key hypotheses about the data are needed, in particular that it is described by functions that are simple (in a similar sense to the inductive bias). Now the assumption that a more parsimonious hypothesis is more likely to be true has been influential since antiquity and is often articulated by invoking Occam's razor. However, the fundamental justification for this heuristic is disputed, see e.g. <ref type="bibr" target="#b84">(Sober, 2015)</ref> for an overview of the philosophical literature, e.g. <ref type="bibr" target="#b53">(MacKay, 1992;</ref><ref type="bibr" target="#b8">Blumer et al., 1987;</ref><ref type="bibr" target="#b73">Rasmussen and Ghahramani, 2001;</ref><ref type="bibr" target="#b23">Domingos, 1999)</ref> for a set of different perspectives from the machine learning literature, and e.g. <ref type="bibr" target="#b74">(Rathmanner and Hutter, 2011;</ref><ref type="bibr" target="#b88">Sterkenburg, 2016)</ref> for a spirited discussion of the links between the razor and concepts from AIT (pioneered in particular by Solomonoff).</p><p>Studies which imply that data typically studied with DNNs is somehow "simple" include an influential paper <ref type="bibr" target="#b52">(Lin et al., 2017)</ref> invoking arguments, mainly from statistical mechanics, to argue that deep learning works well because the laws of physics typically select for function classes that are "mathematically simple", and so easy to learn. More direct studies have also demonstrated certain types of simplicity. For example, following on previous work in this vein, <ref type="bibr" target="#b87">(Spigler et al., 2019</ref>) calculated an effective dimension d ef f â 15 for MNIST, which is much lower than the 28 2 = 784 dimensional manifold in which the data is embedded. Individual numbers can have effective dimensions that are even lower, ranging from 7 to 13 <ref type="bibr" target="#b32">(Hein and Audibert, 2005)</ref>. So the functions that fit MNIST data are much simpler than those that fit random data <ref type="bibr" target="#b28">(Goldt et al., 2019</ref>). An implicit bias towards simplicity may therefore improve generalisation for structured data, but it will likely have the opposite effect for more random data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>We argue here that the inductive bias found in DNNs trained by SGD or related optimisers, is, to first order, determined by the parameter-function map of an untrained DNN. While on a log scale we find P SGD (f |S) â P B (f |S) there are also measurable second order deviations that are sensitive to hyperparameter tuning and optimiser choice.</p><p>For the conundrum of why DNNs generalise at all in the overparameterised regime, our results strongly suggest that the solution must be found in the properties of P B (f |S), and not in further biases introduced by SGD. Arguments that DNN priors are exponentially biased towards simple functions <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018;</ref><ref type="bibr" target="#b61">Mingard et al., 2019;</ref><ref type="bibr" target="#b18">De Palma et al., 2018)</ref> may help explain the inductive bias of P B (f |S), but more work needs to be done to explore the complex interplay between bias in the prior, the data, and generalisation. While they may not explain the fundamental conundrum above, second order deviations from P B (f |S) are important in practice for further fine-tuning the generalisation performance.</p><p>Our function probability perspective also provides more fine-grained tools for the analysis of DNNs than simply comparing the average test error. This picture can facilitate the investigation of hyperparameter changes, or potentially also the study of techniques such as batch normalisation or dropout. It could assist in the design of new architectures or optimisers.</p><p>It is not obvious how to determine the uncertainty in a prediction of a DNN model. However, if, as we argue here, SGD behaves like a Bayesian sampler, then this offers additional justification for using Deep Ensembles to measure this uncertainty in the case of DNNs (Wilson and Izmailov, 2020). Our results could therefore make it easier to use neural networks in applications where it is important to be able to quantify prediction uncertainty Most of our examples are for image classification. It would be interesting to study the related problem of using DNNs for regression. Sampling considerations means that it is easier to study P SGD (f |S) for smaller generalisation errors. It would be interesting to study systems with intrinsically larger G within this picture as well. There the biasing effect of the optimiser may be larger.</p><p>Finally, to study the correlation between P B (f |S) and P SGD (f |S), we mainly used a fixed test and training set. While we did examine other test and training sets (see Appendices), this was mainly to confirm that our results were not an artefact of our particular choices. A promising future direction would be a Bayesian approach that includes averaging over training sets.</p><p>We typically run between n = 10 4 and n = 10 7 runs (depending on the system). Once the runs are finished, we compile all the empirical frequencies for the functions that are found.</p><p>A.1.2 Bayesian sampling for P B (f |S)</p><p>We use the GP approximation to estimate the Bayesian posterior P B (f |S). Here we follow <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> where this technique is explained (see also Appendix A.2 for more details). We need to define a distribution P par (Î¸) (the definition of the prior from which we calculate P B (f |S), see Equation ( <ref type="formula">1</ref>)). While there are some subtleties in how the prior distribution P par (Î¸) relates to the initialisation distribution Ppar (Î¸), we took a simple approach and defined P par to be the same as the corresponding Ppar (Î¸), except we set Ï b to be a small constant, typically 0.1 Ã Ï w .</p><p>To estimate P B (f |S), there is a small compromise that must be made here. For a number of reasons, MSE loss is less popular for the kinds of classification problems we mainly study in this paper. We also find that it typically takes significantly longer to train using an optimiser so that P OPT (f |S) is more expensive to evaluate for MSE loss on the problems we study. On the other hand, P B (f |S) can be directly sampled n times from the exact posterior (described in Appendix A.2.1) using Algorithm 2), and so is relatively accurate and simple to evaluate.</p><p>For CE loss, which is more frequently used for classification, and is also typically quicker to train than MSE for SGD and its variants, we need to use a further approximation. Here we follow <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> and use the expectation-propagation (EP) approximation for P B (f |S). We then estimate the posterior log probabilities using the estimations of the log marginal likelihoods log P (S) (see Appendix A.2.2 for explanation) in Algorithm 3, or we sample from the approximate EP posterior and use Algorithm 2. These two methods give very similar answers (Figure <ref type="figure" target="#fig_12">7c</ref>).</p><p>Algorithm 2 Calculating P B (f |S) (via sampling) input: DNN N , training data S, test data E. F â â {functions sampled from the GP or GP/EP posterior} do n times: sample a function f from the GP or GP/EP posterior when conditioning on S find f on E save f to F R â â {function probabilities} for each distinct f â F do let Ï f be the frequency of f in F calculate P B (f |S) = Ï f /n save P B (f |S) to R end for return R Algorithm 3 Calculating P B (f |S) for specific f via the ratio of likelihoods approximation input: DNN N , training data S, test data E, optimiser OP T , set of functions F (from Algorithm 1) for each distinct f â F do let Ï f be the frequency of f in F use GP or GP/EP approximation to estimate P B (f |S) of f using the ratio of likelihoods approximation. save P B (f |S) to A end for return A A.1.3 Calculating P B (f |S) for functions with a wider range of G Given a training dataset S and test dataset E we can generate a random sample of different partial functions with varying levels of error on E (by taking the test set classification</p><p>and corrupting some percentage of labels). We can then use the GP/EP approximation to estimate P B (f |S). We typically sample 20 examples for each number of errors. The averages are taken on the logs of the probabilities, and error bars on plots are 2Ï, where Ï is the standard deviation. Note that the vast majority of functions have such small probabilities, that it is not feasible to estimate their P OPT (f |S) (nor use Algorithm 2)</p><p>While the experiments will be informative for how the space is biased, it does not guarantee that all high-probability functions will be found. Since these functions affect generalisation the most, we rely on the results from Algorithm 2 to check that there are no high-probability functions that are missed. </p><formula xml:id="formula_12">B (f |S) of f to V end for return V 0.0 . . . V 1.0</formula><p>We here make a few more remarks for interpreting the results in experiments where we generate functions for a wide range of errors, such as those in Figure <ref type="figure" target="#fig_0">1b</ref>. Firstly, as shown in Figure <ref type="figure" target="#fig_14">8a</ref> in Appendix B, there can be a wide variation in the probabilities p i of misclassifying each of the 100 images in this test set. The generalisation error is therefore dominated by a small number of harder to classify images. This means that the probabilities of functions for a fixed G can vary a lot. This explains why we find that even though the highest probability function in Figure <ref type="figure" target="#fig_0">1a</ref> is a 1-error function, on average the probability for 1-error functions in Figure <ref type="figure" target="#fig_0">1b</ref> is lower than that of the 0-error function. The high variance in P B (f |S) within the functions of fixed G , as well as the EP approximation also means that the estimates of P B (f |S) G may be less accurate. For Figure <ref type="figure" target="#fig_0">1b</ref>, Ï( ) P B (f |S) G â 0.1, which is not far off the correct value of 1. Keeping in mind that we may be missing some higher probability outliers in the average due to finite sampling, this agreement is encouraging. In short, although the quantitative values may not be fully accurate, the results in Figure <ref type="figure" target="#fig_0">1b</ref> are indicative of the strong exponential trend towards low probability with increasing error.</p><p>A third point of clarification is that in Figure <ref type="figure" target="#fig_0">1b</ref> we used CE loss rather than MSE loss. We made this choice because we need to estimate very small P B (f |S) values, for which we need to use the "ratio of likelihoods" approximation. While we only described how to use EP for CE loss, we could also use the EP approximation to estimate P B (f |S) for the analytical posterior of the GP with MSE loss. However, from some preliminary tests, the EP approximation introduced significant systematic errors and in particular it didn't show better results than the EP/"ratio of likelihoods" method for CE loss. In Appendix A.2.3, we can also see that the differences between P B (f |S) for MSE and CE loss are probably of less than a few orders of magnitude, and would not significantly affect the results in Figure <ref type="figure" target="#fig_0">1b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Further notes on methods</head><p>When P B (f |S) and P OPT (f |S) are obtained by sampling, we typically sample between 10 5 and 10 7 times. To avoid finite sampling effects, we place any functions found with frequencies &lt; 10 on the axes of our graphs. However, those functions are included in calculations involving generalisation errors, although typically they contribute very little because they are by definition low probability.</p><p>We will also regularly provide values for f âF P B (f |S) and f âF P OPT (f |S) in our figures comparing P OPT (f |S) with P B (f |S) (only when P B (f |S) is obtained by direct sampling), where F is the set of functions found by both GP sampling and by the optimiser (within a finite number of samples from the GP and optimiser-trained DNNs, typically 10 4 -10 6 ). A value of P B (f |S) close to 1 indicates that almost all functions with high P B (f |S) are found by the optimiser. Similarly, a high value of P OPT (f |S) implies that GP sampling finds almost all functions with high P OPT (f |S) found by the optimiser. Note that for the EP approximation needed for CE loss, we directly calculate the P B (f |S) for functions found by the optimiser, and so a P OPT (f |S) is not defined. Finally, when values for the generalisation error G are depicted in the graphs showing P B (f |S) v.s. P OPT (f |S), G refers to the generalisation error of the optimiser. When G is presented in graphs with only P B (f |S) it refers to the G from GP sampling, or alternatively we use the symbol G GP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Description of Gaussian process calculations</head><p>In this section, we provide more details on the Gaussian process (GP) methods used in the paper. We follow the general Bayesian formalism introduced in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 GP with Mean Squared-Error (MSE) loss</head><p>For this formulation, we consider the space of functions to be the space of real-valued functions on X . These functions correspond to the real-valued pre-activations of the last layer of the neural network, before a final non-linearity (like softmax or a step function) is applied. In the GP limit of DNNs, these functions have a GP prior (the NNGP). We then use a Gaussian likelihood defined as</p><formula xml:id="formula_13">P (S|f ) = m i=1 1 â 2ÏÏ 2 exp 1 2Ï 2 (f (x i ) -y i ) 2 ,<label>(5)</label></formula><p>where Ï 2 is the variance. This likelihood allows us to analytically compute the exact posterior <ref type="bibr" target="#b72">(Rasmussen, 2004</ref>). In the experiments in the paper we therefore sampled from this exact posterior, to get values of f (x) â R at the test points, which were then thresholded at 0 to find the predicted class label. We have chosen a small value of the variance Ï 2 = 0.002, to simulate SGD achieving a small value of the MSE loss.</p><p>Note that under the standard assumption that training and test instances come from the same distribution, this algorithm may be considered to be not fully Bayesian in the sense that the training and test labels are treated differently (Gaussian likelihood in training points versus Bernoulli likelihood at test points). Nevertheless, we believe that the differences are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 GP with 0-1 likelihood, EP, and ratio of likelihoods approximation</head><p>While MSE loss has the advantage that the GP is analytically tractable, it has the disadvantage that it is less principled and less commonly used for classification than the much more popular cross-entropy (CE) loss function. Unfortunately the GP approximation to CE loss is more complex. Here our formulation uses a Gaussian process prior over a space of real-valued "latent" functions f on X . We call these functions "latent" because the true space of functions we are interested in is the space of Boolean functions on X , obtained after applying the threshold nonlinearity to the last layer pre-activations. Formally, f is related to f via a Bernoulli distribution with Heaviside linking function, as</p><formula xml:id="formula_14">P (f (x) = 1) = 1 if f (x) &gt; 0 0 otherwise .</formula><p>We then use the previously defined 0-1 likelihood (Section 2.2) applied to the a binary-valued function f (taking values in {0, 1}). We use this likelihood, arguing that it best approximates the behaviour of an optimiser which is trained (using cross-entropy loss) until it first reaches 0 training error, which is the case in all the experiments in the paper (except those labelled as "overtraining"). Because of this we informally refer to this method as "using the CE loss" throughout the paper. Unfortunately, this likelihood makes the posterior of the GP analytically intractable. We therefore use a standard approximation technique known as expectation propagation (EP) <ref type="bibr" target="#b72">(Rasmussen, 2004)</ref>, which approximates the posterior over the latent function as a Gaussian, which we can sample from and then use the Heaviside function to predict the binary labels at the test points. We use this technique to approximate posterior probabilities of the GP with 0-1 likelihood, by sampling. The marginal likelihood can also be estimated with the EP algorithm <ref type="bibr" target="#b72">(Rasmussen, 2004)</ref>. This gives the probability of a labelling of a set of points. Remember that in the Bayesian formalism S is essentially defined as the event that the input points x i in the training set have labels y i . We can similarly identify a function f with the event that the set of input points x in the whole domain X have labels f (x), which is analogous, and can thus be computed in the same manner as the marginal likelihood! The posterior in Equation (2) for a function f which is compatible with S, can then be simply expressed as</p><formula xml:id="formula_15">P (f |S) = P (f ) P (S) ,</formula><p>where both P (f ) and P (S) are readily computed using the EP algorithm to approximate marginal likelihood. This will be referred to as the "ratio of likelihoods" approximation in the appendices. We have found that this method gives very similar results to the estimates using sampling from the approximate posterior obtained from EP (see Figure <ref type="figure" target="#fig_12">7c</ref>). When we refer to the EP approximation, we imply that we are using the ratio of P (f ) and P (S), each determined using the EP approximation, unless stated otherwise.</p><p>For the LSTM experiments, we used a smooth version of the 0-1 likelihood, analogous to using standard cross-entropy loss rather than miss-classification error. This was because the EP approximation was numerically unstable with the 0-1 likelihood for this system. The smooth version is described in Appendix A.2.3, and we empirically found that the two gave very similar estimates of probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Empirical results concerning the GP approximations</head><p>In this section we compare the behaviour of the GP approximations (and SGD) with different loss functions. We have argued that the GP/EP approximation underestimates probabilities by a power law (that is approximately linear in log-log; see <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> for more details). Our results with the EP approximation are consistent with this expectation. As detailed in Appendices A.2.1 and A.2.2, there are subtle differences in the way the GP approximation with MSE loss and the GP/EP approximation with CE loss calculate their respective estimates for P B (f |S). However, the (latent) function has the same prior in both cases, so we may expect the posterior P B (f |S) to correlate. And it is clear from Figure <ref type="figure" target="#fig_12">7a</ref> that they do indeed correlate. We believe that the correlation not being centred around y = x is predominately due to the EP approximation because apart from scatter, the behaviour of SGD with the two loss functions is centred around y = x. In Appendix F, we also compare the EP and MSE approximations with a estimation via direct sampling (and thus with controlled error) of the posterior probabilities for 0-1 likelihood for small Boolean function datasets, where these computations are feasible. We indeed find that EP tends to underestimate posterior probabilities, specially for complex target functions. Overall what we find is that the EP approximation does reasonably well on relative probabilities, but less well on absolute probabilities.</p><p>To mitigate the effect that the EP approximation underestimates the probabilities, we perform a simple empirical regularisation. For systems where we find that P B (f |S) â 1 for the MSE approximation, we renormalise the P B (f |S) from the EP approximation by a constant factor such that P B (f |S) = 1. For most systems we study the effect of this regularisation procedure is relatively small on a log scale. This method facilitates the comparison with P SGD (f |S) because the errors in the absolute values are regularised in the same way for all systems. Note also that because we sample to obtain P SGD (f |S) its empirical frequencies automatically sum to 1. If it were the case that the Bayesian sampling found a significant number of different high probability functions, then this would be observed in a lack of correlation in the comparison with P SGD (f |S). Instead, we find a strong correlation between P SGD (f |S) and P B (f |S) calculated with the EP approximation, suggesting that the functions found are the dominant ones found by both methods, as is explicitly found to be the case for most instances of MSE loss that we studied. This regularisation is applied to all experiments (for ease of comparison), unless otherwise specified.  This implies that the loss function does not substantially affect P SGD (f |S) on average. Note that in (a) the two methods correlate, but that 1) the GP-EP is systematically lower than the MSE, and 2) that the slope is below x = y. These two trends are, we believe, more general for the GP-EP approximation on CE loss. (c) Here we compare the GP/EP log(p) approximation with GP/EP sampling. For this figure, we use only functions found by Adam in 10 6 samples, and compare probabilities found by the GP/EP log(p) approximation to those found by GP/EP sampling. We use both methods in this paper, but as is clear from the above figure, there is not much difference between them for functions with high P B (f |S).</p><p>Specifically, for Figure <ref type="figure" target="#fig_16">9</ref> the renormalisation constant was calculated for Adam without overtraining and batch size 128 (as it had the highest raw value for P B (f |S)), and the probabilities in the other plots with FCN on MNIST are all adjusted by the multiplicative constant of 3.59, which is modest on the full log scale of the graphs. For other plots, the probabilities were normalised. The two systems for which this renormalisation has a larger effect are the LSTM and the ionosphere dataset. While for both systems the MSE sampling looks relatively close to y = x, the raw EP approximation has significantly lower probabilities. The renormalisation factors were 1.15 Ã 10 5 and 97 respectively. Smaller MSE experiments verify that there is a strong correlation between P SGD (f |S) and P B (f |S) for these systems.  which indicates an exponential like drop-off for â¥ G (similar to what is observed in Figure <ref type="figure" target="#fig_0">1a</ref>). Of course this is an upper bound and the actual distribution can strongly depend on the full spectrum of p i values. We are currently exploring these issues in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Effects of training set size</head><p>It is also instructive to study the correlation between P B (f |S) and P OPT (f |S) for different training set sizes |S|. As can be seen clearly in Figure <ref type="figure" target="#fig_16">9</ref>, as the training set increases in size, the functions with zero training error are more strongly biased towards low generalisation error, as expected. Figures 9c and 9f also illustrate how the stronger bias with increasing |S| means that the entropic factor Ï( G ) plays a smaller role. Thus, for larger training set size, but for the same amount of sampling n, fewer functions are found, but on average they have higher probability. An important question in deep learning is: How does the error reduce with increasing the training set size? There is intriguing evidence that such "learning curves" follow a power law that depends on data complexity, and only weakly on the architecture <ref type="bibr" target="#b33">(Hestness et al., 2017;</ref><ref type="bibr" target="#b87">Spigler et al., 2019;</ref><ref type="bibr" target="#b75">Rosenfeld et al., 2019;</ref><ref type="bibr" target="#b41">Kaplan et al., 2020)</ref>. Figure <ref type="figure" target="#fig_16">9</ref> shows how the spectrum of function probabilities changes with increasing |S|. Investigations based on this more fine-grained picture may help improve our understanding of learning curves.  Much research effort has gone into adaptations of SGD. One goal is to achieve more efficient optimisation, but another is to achieve better generalisation. Figure <ref type="figure" target="#fig_0">11</ref> illustrates the effect of changing the optimiser on the correlation between P B (f |S) and P OPT (f |S). (See also Figures 1f, 5b and 16 to complete the set of optimisers with and without overtraining). To first order this figure shows that P B (f |S) and P OPT (f |S) are remarkably closely correlated for all these optimisers, even taking into account that the EP approximation introduces errors, and likely leads to a slightly too small slope in P B (f |S) v.s. P OPT (f |S).</p><p>What is perhaps more interesting here are second-order effects, since P B (f |S) is identical in each plot. For example, RMSprop has the best generalisation performance, which is reflected in a stronger inductive bias towards a few key low error functions. Note also the similarity of batch size 128 RMSprop to Adam with smaller batch size of 32. We emphasise that this performance here doesn't mean that RMSprop is in general superior to the other SGD variants for FCNs on MNIST. To investigate that question, we would need to study other test and training sets, and need to do further hyperparameter tuning <ref type="bibr" target="#b12">(Choi et al., 2019)</ref>.</p><p>We can also compare the effect of overtraining. In each case shown in Figures 1f, 5b and 16, overtraining brings a modest improvement in generalisation error on this test set (Adam from G = 2.2% to G = 1.74% and Adagrad from G = 2.63% to G = 2.19%), for example. From the graphs one can see a slight increase in the optimiser probability of the lowest error function with overtraining, but also a clear reduction in the scatter of the data for the whole range of probabilities, suggesting that for CE loss, on average, overtraining brings P OPT (f |S) closer to the Bayesian prediction. This behaviour can possibly be rationalised in that overtraining allows the optimiser to sample functions with probabilities closer to the steady-state average (see also Section 6).</p><p>Finally, in Figures 11e, 11f and 17 we directly compare the P OPT (f |S) to one another, in other words, without using P B (f |S). A number of clear trends are visible, for example, with batch size 128, Adam and Adagrad are very similar to one another, as are RMSprop and Adadelta. However, as can be seen in Figure <ref type="figure" target="#fig_0">11f</ref>, Adam with a smaller batch size of 32 is very similar to RMSprop with batch size of 128. What these correlation plots show is that the behaviour of the different optimisers can depend on batch size in subtle ways that may not necessarily be picked up by the generalisation error. Further work (and significant computational resources) would be needed to completely compare these methods.</p><p>These examples show that studying the spectrum of function probabilities provides more fine-grained data than simply comparing generalisation error does. Future studies on problems such as optimiser choice or hyperparameter tuning could exploit this fuller set of information to increase understanding and to improve DNN performance.   Adagrad and Adam correlate very well as do Adadelta and RMSprop for these hyperparameters. The other combinations do not correlate as well, suggesting that they sample the loss-function differently from one another. These plots do not rely on the GP or GP/EP approximation. We believe that this sort of experiment may prove useful for understanding differences in the behaviour of the optimisers.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing the Bayesian prediction P B (f |S) to P OPT (f |S) for SGD and Adagrad, for an FCN on MNIST [We use training/test set size of 10,000/100; For (a,e,f), the vertical dotted blue lines are drawn at the highest value of P OPT (f |S) such that the sum of P OPT (f |S) for all functions above the line is &gt; 90% (90% probability boundary); dashed grey line denotes P B (f |S) = P OPT (f |S).] (a) P B (f |S) v.s. P SGD (f |S) for MSE loss; Both P B (f |S) and P SGD (f |S) were sampled n = 10 6 times. The color shows the number of errors in the test set. The GP has average error G GP = 1.61%, while SGD has average error G = 1.88%. (b) P B (f |S) (with CE loss) v.s. G for the full range of possible errors on E. We use the methods from Section 3.1.5 with 20 random functions sampled per value of error. The solid blue line shows log(P B (f |S)) G , where the average is over the functions for a fixed G ; error bars are 2 standard deviations. The dashed blue line shows the weighted Ï( G ) P B (f |S) G , where Ï( G ) is the number of functions with error G . The small red box and dashed red lines illustrate the range of probability and error found in (a). (c) CSR complexity versus generalisation error for the same functions as in fig (b). Color represents P B (f |S), computed as in (b). (d) Functions from (a) found by the sample of P B (f |S), versus error. 913 functions of the functions are also found by SGD, taking up 97.70% of the probability for P SGD (f |S), and 99.96% for P B (f |S).(e) P B (f |S) v.s. P Adagrad (f |S) for MSE loss; P Adagrad (f |S) was sampled n = 10 5 times (while the GP sample was the same as in (a)). Adagrad was overtrained until 64 epochs had passed with zero error. The average error is G = 1.53%. (f) is as (e) but with CE loss, so that the EP approximation was used for P B (f |S), making the estimate of P B (f |S) slightly less accurate. G = 2.63%. 10</figDesc><graphic coords="10,96.08,254.35,133.92,131.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure 2: Comparing P B (f |S) to P Adam (f |S) for CNNs and the FCN on Fashion-MNIST [We use a training/test set size of 10,000/100; vertical dotted blue lines denote 90% probability boundary; dashed grey line is P B (f |S) = P OPT (f |S).] (a) FCN on Fashion-MNIST; G = 2.11% for Adam with CE loss. (b) Vanilla CNN on Fashion-MNIST; G = 2.25% for Adam with CE loss. (c) CNN with max-pooling on Fashion-MNIST; G = 1.96% for Adam with CE loss. Note that when max-pooling is added, the probability of the lowest-error function increases notably for both P Adam (f |S) and P B (f |S). There is a strong correlation between P B (f |S) and P SGD (f |S) in all three plots. See Figure 13 for related results, including P B (f |S) vs G , a CNN with batch normalisation, and a CNN with MSE loss.</figDesc><graphic coords="13,239.04,90.86,133.92,139.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 3</head><label>3</label><figDesc>Comparing P B (f |S) and P SGD (f |S) to Neural Tangent Kernel results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>(a) P B (f |S) v.s. P NTK (f |S) (b) P SGD (f |S) v.s. P NTK (f |S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing P B (f |S) and P SGD (f |S) to P NTK (f |S) for an FCN on MNIST.[The functions to the right of the blue dotted lines make up 90% of the total probability. We did 10 7 samples for NTK and GP, and 10 6 for SGD]. In (a) we show the correlation between P N T K (f |D) and P B (f |S). Weighted by probability, 77.5% of functions found by sampling from the GP are found by NTK; all functions found by NTK are found by sampling from the GP. In (b), we show the correlation between the P N T K (f |D) and P SGD (f |S). Weighted by probability, 65.8% of functions found by SGD are found by NTK; all functions found by NTK are found by SGD. G = 1.69% (NTK), G = 1.61% (GP), G = 1.88% (SGD).</figDesc><graphic coords="14,125.97,396.64,172.78,169.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 4</head><label>4</label><figDesc>Comparing P B (f |S) to P Adam (f |S) for LSTM on IMDb sentiment analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparing P B (f |S) to P Adam (f |S) for a LSTM on the IMDb movie review dataset, and an FCN on the ionosphere dataset. (a) P B (f |S) v.s. P Adam (f |S) for LSTM on IMDb dataset, ( G =4.28%, 10 4 samples). Because of the computational cost of the problem, we used a training set size of 45000 and a test set of size 50. (b) P B (f |S) v.s. G for the LSTM on IMDb shows that the functions found by the Adam optimiser are in the small fraction of high P B (f |S) probability/low error functions. (c) P B (f |S) v.s. P Adam (f |S) for an FCN with 3 hidden layers of width 256 on the Ionosphere dataset. Training set size is 301 and the test set size is 50. ( G = 4.59% for Adam, G = 5.41% for the GP). See Figures 14 and 15 for further results for these systems.</figDesc><graphic coords="15,90.65,252.27,133.91,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of changing batch size and learning rate on P B (f |S) and P Adam (f |S) for FCN on MNIST with CE loss [We use training/test set size 10,000/100. Vertical dotted blue lines denote 90% probability boundary; dashed grey line is x = y.] (a) Batch size = 32, G = 1.13%. (b) Batch size= 128, G = 2.20%. (c) Batch size = 512, G = 2.67%. (d) Batch size =512 and faster learning rate (4x the others), G = 2.14%. (e) Direct comparison of P Adam (f |S) for batch size 128 and 512. (f) Direct comparison of P Adam (f |S) for batch size 128 and 512 with a 4Ã faster learning rate. The P Adam (f |S) probabilities for the dominant functions in (d) and (b) are remarkably similar, as can be seen by comparing (e) and (f). It is these higher probability functions that explain the similarity in G for batch size 128 and batch size 512 with a faster learning rate. See Figure 18 for related batch size results for MSE loss.</figDesc><graphic coords="17,90.65,346.55,133.92,138.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Schematic landscape and effects of randomising training labels. (a)Cartoon of a biased loss-landscape. The three functions f 1 , f 2 and f 3 all reach zero classification error (dashed red line), but due to bias in the parameter-function map, the "basin size" V B (f 1 )V B (f 2 ), V B (f 3 ), which typically implies that for the "basins of attraction"V SGD (f 1 ) V SGD (f 2 ), V SGD (f 3 ). P B (f |S) is proportional to V B (f ), and P SGD (f |S) is proportional to V SGD (f ). (b) P B (f |S) (solid) and Ï( G )P B (f |S) (dashed) v.s. G , for test set of size 100 and CE loss (as in Figure 1b) but including label corruption c. (b) P SGD (f |S) v.s. P B (f |S) on MNIST with a 2-layer 1024 node wide FCN with MSE loss, test set size 50, and 20% of the training labels randomised ( G SGD = 13.4% and G GP = 5.80%).Here functions with frequency &lt; 10 are also shown on the plot. The correlation is much less pronounced than for the unrandomised case shown in Figure1a. Dots on the axes denote functions found by just one of the two methods. Let F be the set of functions found by both the optimiser and under GP sampling. Then f âF P B (f |S) = 99.3%, and</figDesc><graphic coords="19,90.65,218.38,133.91,115.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>Figure 6: Schematic landscape and effects of randomising training labels. (a)Cartoon of a biased loss-landscape. The three functions f 1 , f 2 and f 3 all reach zero classification error (dashed red line), but due to bias in the parameter-function map, the "basin size" V B (f 1 )V B (f 2 ), V B (f 3 ), which typically implies that for the "basins of attraction"V SGD (f 1 ) V SGD (f 2 ), V SGD (f 3 ). P B (f |S) is proportional to V B (f ), and P SGD (f |S) is proportional to V SGD (f ). (b) P B (f |S) (solid) and Ï( G )P B (f |S) (dashed) v.s. G , for test set of size 100 and CE loss (as in Figure 1b) but including label corruption c. (b) P SGD (f |S) v.s. P B (f |S) on MNIST with a 2-layer 1024 node wide FCN with MSE loss, test set size 50, and 20% of the training labels randomised ( G SGD = 13.4% and G GP = 5.80%).Here functions with frequency &lt; 10 are also shown on the plot. The correlation is much less pronounced than for the unrandomised case shown in Figure1a. Dots on the axes denote functions found by just one of the two methods. Let F be the set of functions found by both the optimiser and under GP sampling. Then f âF P B (f |S) = 99.3%, and</figDesc><graphic coords="19,239.04,207.17,133.91,126.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 4</head><label>4</label><figDesc>Calculating P B (f |S) for larger range of G input: DNN N , training data S, test data E. for â {0.0, 0.5, . . . 1.0} do V â generate classification c with error on E (by randomly choosing |E| Ã distinct labels in the correct function (restricted to the test set) to switch to incorrect). use GP/EP and "ratio of likelihoods" approximation to estimate the P B (f |S) of c save the relative volume P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>(a) P B (f |S)(CE/EP) versus P B (f |S) (MSE) (b) P SGD (f |S)(CE) versus P SGD (f |S) (MSE) (c) P B (f |S)(CE/EP) sampled v.s. ratio of likelihoods estimate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparing GP approximations with CE and MSE loss for FCN on MNIST In (a) we compare the behaviour of the GP approximations for P B (f |S) with MSE loss to the GP/EP approximation with CE loss. We sampled from the GP MSE posterior, and the GP/EP CE posterior distribution 10 6 times. It is expected that the two measures should diverge somewhat due to details of the loss function on the training data. (b) compares P SGD (f |S) with MSE loss and P SGD (f |S) with CE loss. Functions with high P SGD (f |S) are scattered around y = x.This implies that the loss function does not substantially affect P SGD (f |S) on average. Note that in (a) the two methods correlate, but that 1) the GP-EP is systematically lower than the MSE, and 2) that the slope is below x = y. These two trends are, we believe, more general for the GP-EP approximation on CE loss. (c) Here we compare the GP/EP log(p) approximation with GP/EP sampling. For this figure, we use only functions found by Adam in 10 6 samples, and compare probabilities found by the GP/EP log(p) approximation to those found by GP/EP sampling. We use both methods in this paper, but as is clear from the above figure, there is not much difference between them for functions with high P B (f |S).</figDesc><graphic coords="41,90.65,181.05,133.91,132.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>(a) p i for GP (b) P B (f |S) v.s. p i prediction. (c) p i for SGD (d) P B (f |S) v.s. p i prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: (a) shows the NNGP estimate for the values of p i for an FCN with MSE loss on MNIST, for the training set of size 10000 and test set of size 100 that we use in the main text. Clearly these vary over many orders of magnitude. The sample size is 10 7 , so frequencies were cut off at 10 -7 (so functions in that bin have p i â¤ 10 -7 ). 20 bins in total. (b) calculates the probability of other error functions using the values of p i from Figure1dusing the assumption that the images from (a) are independently distributed, and compares this prediction for P B (f |S) to the value of P B (f |S) obtained by direct GP MSE sampling (see Figure1dfor the data). Each datapoint is for a specific function f , and clearly they are close to the y = x line, implying that, at least for these higher probability functions found by direct sampling, the images in this small test set are classified by the GP in a (close to) independent fashion. Figures(c) and (d) are the equivalent of (a) and (b), but for SGD (see Figure1afor the data). There were only 10 6 samples, so (c) and (d) are cut off at one order of magnitude lower than (a) and (b). (c) includes an inset comparing the values for p i with GP MSE sampling from (a) and p i with SGD from the main part of (c). The correlation is fairly tight for the highest p i which dominate the total probability mass, and this correlation helps explain the strong correlation seen for other numbers of errors throughout this paper.</figDesc><graphic coords="43,125.97,304.76,172.79,140.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><figDesc>(a) |S| = 1000 (b) |S| = 5000 (c) |S| = 1000, 5000 (d) |S| = 10000 (e) |S| = 20000 (f) |S| = 10000, 20000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparing P B (f |S) to P Adam (f |S) for an FCN on MNIST with CE loss for different training set sizes. [We use test set size of |E| = 100; vertical dotted blue lines denote 90% probability boundary; solid blue line is a guide to the eye, dashed grey line is x = y.] (a) 1000 training examples. G = 6.65% for Adam. (b) 5000 training examples. G = 3.33% for Adam. (c) P B (f |S) v.s. G for 1000 and 5000 training examples. (d) 10000 training examples. G = 2.20% for Adam. (e) 20000 training examples. G = 0.89% for Adam. (f) P B (f |S) v.s. G for 5000 and 10,000 training examples. A trend of increasing bias towards lower error functions with increasing training set size can be clearly observed. See Figure 10 for related results with MSE loss.</figDesc><graphic coords="45,90.65,242.03,133.90,132.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Comparing P B (f |S) to P OPT (f |S) for a LSTM on the IMDb movie review dataset. Further results for Figure 4. [We use training/test set size 45,000/50 and batch size=192. Vertical dotted blue lines denote 90% probability boundary; solid blue lines are fit to guide the eye; dashed grey line is x = y.] (a) P B (f |S) v.s. P Adadelta (f |S) for MSE loss. n = 2200 for the optimiser and n = 2.6 * 10 4 for the GP. G = 5.22% and G GP = 5.04% (b) Probability of all functions found by NNGP compared to those also found by the optimiser. Green points are the set jointly found functions F . Red denotes functions found only by the GP, f âF P B (f |S) = 54.1% and f âF P OPT (f |S) = 77.0%. (c) Compares the P Adam (f |S) with CE loss from Figure 4a, to the sampled P B (f |S) using MSE loss for n = 10 4 samples. While these results are for very limited sample numbers, they provide evidence that for the LSTM, P B (f |S) has values on the same order of magnitude as P SGD (f |S). The low values we find for the raw EP approximation estimates of P B (f |S) are likely to be due to errors in the EP absolute values. The fact that we still see correlations for the CE-trained LSTM with the renormalised EP approximations for P B (f |S) suggests that the EP still does reasonably on relative errors. CE has the advantage that it is much faster to use than MSE.</figDesc><graphic coords="51,239.04,90.86,133.91,137.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><figDesc>Figure 15: Comparing P B (f |S) to P Adam (f |S) for an FCN on the Ionosphere dataset with CE loss. Further results for Figure 4. [We use training/test set size 301/50 and batch size=192. Vertical dotted blue lines denote 90% probability boundary; solid blue lines are fit to guide the eye; dashed grey line is x = y.] (a) P B (f |S) v.s. P Adam (f |S) for MSE (same as in Figure4), put here ease of comparison). (b) Probability of all functions found by NNGP compared to those also found by the optimiser. Green points are the set jointly found functions F .f âF P B (f |S) = 43.1% and f âF P Adam (f |S) = 99.8% (in other words nearly all functions found by Adam are also found by the GP, but the GP also finds functions that Adam doesn't for this level of sampling). For the optimiser, G = 4.59% and for the GP MSE sampling, G GP = 5.41%. (c) P B (f |S) v.s. P Adam (f |S) for CE. G = 5.88%. (d) &lt; P B (f |S) &gt; versus G with CE, 20 samples per G . The bias towards low error functions is less strong than what is found for MNIST or Fashion-MNIST.</figDesc><graphic coords="52,313.23,284.41,172.78,168.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Comparing P OPT (f |S) with P OPT (f |S) for different optimisers. Further results for Figure 11. [We use the FCN architecture on MNIST, CE loss and a batch size of 128, training/test set size = 10,000/100, y = x is denoted by a dashed line]Adagrad and Adam correlate very well as do Adadelta and RMSprop for these hyperparameters. The other combinations do not correlate as well, suggesting that they sample the loss-function differently from one another. These plots do not rely on the GP or GP/EP approximation. We believe that this sort of experiment may prove useful for understanding differences in the behaviour of the optimisers.</figDesc><graphic coords="54,90.00,90.87,431.98,270.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Results for Appendix F for target function with LZ complexity 35.0. Refer to text for detailed description. 60</figDesc><graphic coords="60,239.04,569.73,133.91,100.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Results for Appendix F for target function with LZ complexity 28.0. Refer to text for detailed description. 61</figDesc><graphic coords="61,239.04,569.73,133.91,100.44" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the special case where we specify the experiment as 'overtraining', then we take the parameters after p epochs with 0 classification error.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>f âF P SGD (f |S) = 24.3%. In other words, while the Adam optimiser finds almost all functions with high P B (f |S), it also finds many functions with low P B (f |S). The much weaker bias under label corruption observed in (b) likely explains the weaker correlation between the Bayesian results and that of the optimiser found here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_2"><p>CNN: https://keras.io/api/layers/convolution_layers/convolution2d/ LSTM: https://keras.io/api/layers/recurrent_layers/lstm/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Further detail for methods <ref type="bibr">(Section 3)</ref> In this Appendix we provide further details and explanation of the methodology outlined in Section 3. In particular, we discuss our experiments in Appendix A.1 and the GP approximation for P B (f |S) in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Methodology in detail</head><p>For each experiment performed in the main text, we pick a DNN N (either FCN, CNN, or an LSTM) and a dataset D (MNIST, Fashion-MNIST or the IMDb dataset). We also pick a fixed training set S â D, and a fixed test set E â D. Training sets are typically of size 10, 000 for FCN and CNN and 45, 000 for the LSTM. Test sets are typically small, 100 for the FCN and CNN, and 50 for the LSTM.</p><p>A.1.1 Using an optimiser to calculate P OPT (f |S)</p><p>When calculating P OPT (f |S) we first pick an optimiser OPT which is either plain SGD, or one of its derivatives: Adam, Adagrad, RMSprop, or Adadelta. Next we pick a loss-function, either mean-square error (MSE) or cross-entropy (CE). We also need to pick an initial parameter distribution Ppar (Î¸) which we take to be from a truncated i.i.d. Gaussian distribution (the distribution from which the DNN N is randomly initialised, see Equation ( <ref type="formula">3</ref>)). For fully-connected layers we used Ï b = 0 and Ï w = 1/ â w where w is the width of the layer. For convolutional and LSTM layers we used the default initialisation provided by Keras 2.3.0 11 . This specifies Ppar (Î¸). As we see in Algorithm 1, we then sample n times from Ppar (Î¸), training each time with OP T until the training error on S is zero, at which point we record the function by what errors it makes on E. Note that if for some reason SGD does not converge, we don't count that run in order to have normalised distributions over functions.</p><p>In Appendix A.2.2, we described using a Heaviside linking function when using the 0-1 loss function in the EP approximation. To test this, we have also sampled f following a Bernoulli distribution with a Probit linking functionto the latent f (which is analogous to using a cross-entropy loss). To test the differences between the results (which we assume to be small), we tested 100 randomly selected functions (on MNIST) with G ranging from 0% to 100%. Of these, the average difference between the results as a percentage of the magnitude of the log probabilities was 0.013%, and the maximum was 0.58% for the FCN architecture. We also compare the GP/EP sampling with GP/EP log P (S) approximation in Figure <ref type="figure">7c</ref>. The log P (S) approximation is a further approximation that allows log P (f ) to be extracted without requiring sampling. Unless otherwise specified, all results in the main text use the GP/EP log P (S) approximation rather than sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Notes on the distribution of MNIST data</head><p>Here, we show that p i , the mean probability, for an FCN on MNIST with MSE loss function, that either SGD or the NNGP classifies the i'th image in the test set E (used throughout this paper, with |E| = 100) incorrectly, varies by many orders of magnitude. We also show that images are classified in an approximately independent manner.</p><p>The values of p i for the MNIST test set which is used in the majority of this paper are given in Figure <ref type="figure">8a</ref>. Note that (1/n) i p i = G ). As can be seen, there are a small number of images that have a much higher probability of being miss-classified. It should be kept in mind that the exact spectrum of p i will depend on which images are in the test set. For example, in Figure <ref type="figure">12</ref> we compare two test sets (albeit with CE-loss and the EP approximation). As can be observed, the zero error function has significantly higher probability in the second test set, and the highest 1-error function has lower probability than for the test set we use in the paper. For test sets of this size such fluctuations are not unexpected.</p><p>Next we use the p i to calculate probabilities for functions with other sets of errors. More specifically, consider test sets E i containing only one image I i (where i takes values in N). Then calculating the probability of a function on E = {E i } i=|E| i=0 for a test set of size |E| can be done by multiplying the appropriate probabilities for functions on E i . Applying the above method to this data is shown in Figure <ref type="figure">8b</ref>. All the functions shown are very close to the y = x line, indicating that the images are classified in an approximately independent fashion.</p><p>In all our examples we observe a clear linear decay of the mean of log(P B (f |S)) v.s. G . If the p i were identical, then this would simply be what is expected for a Binomial distribution. For independent but different p i over images in a finite test set the distribution is called the Poisson Bionomial distribution. Obviously its values depend on the exact distribution of p i . However, there exists a Chernoff bound</p><p>where p( ) is the pmf, P [E &gt; ] is the cmf, and G = (1/n) i p i is the mean. On a log scale, this means      Comparing the 0% and 20% label corruption shows that the weaker bias in the latter leads to less strong correlation between P B (f |S) and P SGD (f |S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Critical Sample Ratio</head><p>A measure of the complexity of a function, the critical sample ratio (CSR), was introduced in <ref type="bibr" target="#b44">(Krueger et al., 2017)</ref>. It is defined with respect to a sample of inputs as the fraction of those samples which are critical samples, defined to be an input such that there is another input within a box of side 2r centred around the input, producing a different output (for discrete outputs). Following <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref>, we use CSR as a rough estimate of the complexity of functions found in the posterior (conditioning on S), and the prior (i.e. functions on S). In our experiments, we used binarised MNIST with a training set size of 10000 and a test set of size 100 (analogously to the majority of our other experiments). For the prior, Figure <ref type="figure">20a</ref>, we randomly generated 100 functions with errors ranging between 0 and 10000 on S. For each function, we recorded the error, P B (f |S) and the CSR. For the posterior, we generated 500 functions with a range of errors on the test set and concatenated them with the function correct on the training set. We then proceeded as with the prior.</p><p>To calculate the CSR, we trained a DNN to model the function in question. Clearly this induces effects not purely due to the parameter-function map -although as we have seen in Appendix A, the functions found by SGD are likely to be similar to those that would be found by training by random sampling of parameters. Therefore, we may expect this process to approximately give the average CSR of parameters producing the function of interest. In Figure <ref type="figure">2a</ref>. of <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref>, an example of a very similar experiment with CIFAR10 can be found, where the network was not trained. This produces very similar results to our experiments with network training. Appendix F. Bayesian inference by direct sampling on Boolean system.</p><p>In this section we consider a much simpler system, with a smaller input space, and thus a much smaller space of functions. This allows us to approximate Bayesian inference in the case of 0-1 likelihood (see Appendix A.2.2) much more accurately, via direct sampling. We use the same DNN architecture and synthetic data studied in <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref>. It consists of a two layer neural network with 7 Boolean inputs, two hidden layers of 40 ReLU-activated neurons, and a single Boolean output. The space of functions is thus the space of 2 128 â 3.4 Ã 10 38 Boolean functions of 7 inputs. We perform 'approximate Bayesian inference' (ABI) by sampling the parameters of the neural network i.i.d. from a Gaussian distribution with distribution parameters 12 Ï w = Ï b = 1.0, evaluating the neural network on the training set, and saving the samples for which the neural network achieves 100% training accuracy. Each of these samples corresponds to a function, sampled from the exact Bayesian posterior. We estimate the posterior probabilities by the empirical frequencies of individual functions (defined on all 2 7 = 128 inputs), after sampling the parameters 10 10 times. We used a random but fixed training set consisting of 32 out of the 128 inputs for a given target function. Target functions were chosen among functions that appeared with reasonably high frequency, in a large sample obtained by randomly sampling the weights of the neural network, so as to ensure ABI would give enough samples. They were chosen to have a range of values of Lempel-Ziv complexity. See <ref type="bibr" target="#b92">(Valle-PÃ©rez et al., 2018)</ref> for the definition of Lempel-Ziv complexity of Boolean functions used here.</p><p>Representative results are shown in <ref type="bibr">Figures 21,</ref><ref type="bibr">22,</ref><ref type="bibr">23 (results</ref> for the other 4 functions we tested look qualitatively similar). We empirically found that the ABI probabilities correlated and are of a similar order of magnitude to the SGD probabilities over the whole range of Boolean functions tried. SGD is consistently more biased towards the most likely functions for CE loss, although it only increased their probability by about a factor of two (a small amount relative to the whole range of probabilities, but because for this system this is the dominant function, this secondary effect can still have a significant effect on the average generalisation error). We also performed sampling using the EP approximation to the posterior with 0-1 loss, and the exact posterior using MSE loss to directly see the effects coming from the EP approximation. We found that for some functions (the simplest ones) GP/EP gave probabilities which were close to those found by ABI. However, for more complex functions 13 GP/EP highly underestimates the probabilities In fact, for the most complex functions we studied, GP/EP didn't find a single function more than once in our sampling. This is in contrast to the GP/MSE sampling probabilities, which shows reasonable correlation with the ABI probabilities, as well as with the probabilities of SGD trained with MSE loss.</p><p>These results support the main hypothesis of the paper that the Bayesian posterior probabilities correlate with the SGD probabilities. They also suggest that the EP approximation can sometimes heavily underestimate the probabilities. This agrees with the conjecture that the EP approximation is the main cause of the discrepancy in the magnitudes of the probabilities between GP/EP and SGD observed in the rest of the experiments in this paper, 12. Remember that, following standard convention, the actual weight variance is dividing by the number of input neurons 13. These are still rather simple w.r.t. the full range of possibilities, necessary to ensure that ABI sampling is feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>58</head><p>and that for CE loss, the true Bayesian prior probabilities may in fact match the SGD probabilities a lot more closely. Furthermore, the MSE results suggest that MSE may give a good approximation to the Bayesian posterior probabilities (even the ones based on 0-1 likelihood). However, we note that this is a small toy model, and so our analysis leaves as an open question how to understand the error induced by the EP approximation in more realistic systems. One approach could be to cross-validate some of our results with state-of-the-art Monte Carlo sampling techniques for Bayesian inference <ref type="bibr" target="#b72">(Rasmussen, 2004)</ref>. Description of figures. In Figures 21,22 and 23 in this section, we show, for three representative target Boolean functions, data comparing P SGD (f |S) for CE or MSE loss versus different ways to estimate P B (f |S), namely ABI, GP/EP for CE loss and GP/MSE for MSE loss.</p><p>In the first column, we show scatter plots comparing sampled probabilities (where functions not found in the sample are shown as if having a frequency of one). The colours denote the number of errors for each function on the test set.</p><p>The second column shows probability versus rank of the different test-set functions (when ranked by probability) for the two sampling methods.</p><p>The third column shows test accuracy histograms for the two sampling methods.</p><p>In the first row, we use sampling of parameters (and 0-1 loss) to estimate P B (f |S), which we use as the gold standard method as it has controlled small errors.</p><p>In the second row, we estimate P B (f |S) with the GP/EP approximation introduced in Appendix A.2.2.</p><p>In the third row, we compare P B (f |S) estimated from sampling of the exact MSE posterior (explained in Appendix A.2.1) versus the ABI sampling (for 0-1 loss).</p><p>In the fourth row, we compare P SGD (f |S) when training with MSE loss versus ABI sampling (for 0-1 loss).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6158" to="6169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/arora18b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8139" to="8148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StanisÅaw</forename><surname>JastrzÄbski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05394</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparing dynamics: Deep neural networks versus glassy systems</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baity-Jesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Spigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GÃ©rard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Biroli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124013</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Energy landscapes for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritankar</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhagash</forename><surname>Martiniani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">D</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><surname>Wales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Chemistry Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="12585" to="12603" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Peter L Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6240" to="6249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometry of energy landscapes and the optimizability of deep neural networks</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">108301</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor</title>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="377" to="380" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="311" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The promises and pitfalls of stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Brosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8268" to="8278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10174</idno>
		<title level="m">Sgd learns overparameterized networks that provably generalize on linearly separable data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05446</idno>
		<title level="m">On empirical comparisons of optimizers for deep learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GÃ©rard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Omry</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Malka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Ringel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05301</idno>
		<title level="m">Learning curves for deep neural networks: a gaussian field theory perspective</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Can implicit bias explain generalization? stochastic convex optimization as a case study</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Dauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06152</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gaussian process behaviour in wide deep neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1-nGgWC-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Palma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Toussi Kiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Lloyd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10156</idno>
		<title level="m">Random deep neural networks are biased towards simple functions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The structure of the genotypephenotype map strongly constrains the evolution of non-coding rna</title>
		<author>
			<persName><forename type="first">Kamaludin</forename><surname>Dingle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schaper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ard A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interface focus</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">20150053. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Input-output maps are strongly biased towards simple outputs</title>
		<author>
			<persName><forename type="first">Kamaludin</forename><surname>Dingle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chico</forename><forename type="middle">Q</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ard A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generic predictions of output probability based on complexities of inputs and outputs</title>
		<author>
			<persName><forename type="first">Kamaludin</forename><surname>Dingle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Valle PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ard A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of occam&apos;s razor in knowledge discovery</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="425" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-dimensional data analysis: The curses and blessings of dimensionality</title>
		<author>
			<persName><surname>David L Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMS math challenges lecture</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data</title>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><surname>Roy</surname></persName>
		</author>
		<ptr target="http://auai.org/uai2017/proceedings/papers/173.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017</title>
		<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 11-15, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep convolutional networks as shallow gaussian processes</title>
		<author>
			<persName><forename type="first">AdriÃ </forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Aitchison</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bklfsi0cKm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Modelling the influence of data structure on learning in neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>MÃ©zard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>ZdeborovÃ¡</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11500</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal function approximation by deep neural nets with bounded width and relu activations</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">992</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality estimation of submanifolds in rd</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00409</idno>
		<title level="m">Deep learning scaling is predictable, empirically</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ClÃ©ment</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Finding flatter minima with sgd</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fantastic generalization measures and where to find them</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02178</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/1609.04836</idno>
		<ptr target="http://arxiv.org/abs/1609.04836" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Amethod for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Representations</title>
		<meeting>3rd Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep nets don&apos;t learn via memorization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJC2SzZCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object recognition with gradient-based learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape, contour and grouping in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00165</idno>
		<title level="m">Deep neural networks as gaussian processes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep neural networks as gaussian processes</title>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1EA-M-0Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8572" to="8583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Laws of information conservation (nongrowth) and aspects of the foundation of probability theory</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Peredachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="30" to="35" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">An introduction to Kolmogorov complexity and its applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M B</forename><surname>Vitanyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag New York Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Why does deep and cheap learning work so well</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Henry W Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1223" to="1247" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Introduction to gaussian processes. NATO ASI series. Series F: computer and system sciences</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="133" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent as approximate bayesian inference</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">134</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Power-law distributions for the areas of the basins of attraction on a potential energy landscape</title>
		<author>
			<persName><forename type="first">Claire</forename><forename type="middle">P</forename><surname>Massen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan Pk Doye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37101</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Samplethen-optimize posterior sampling for bayesian linear models</title>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Alexander G De G Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Alexander G De G Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11271</idno>
		<title level="m">Gaussian process behaviour in wide deep neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pac-bayesian model averaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth annual conference on Computational learning theory</title>
		<meeting>the twelfth annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="164" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mingard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joar</forename><surname>Skalse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Valle-PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>MartÃ­nez-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ard A</forename><surname>Louis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11522</idno>
		<title level="m">Neural networks are a priori biased towards boolean functions with low entropy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Predicting the outputs of finite networks trained with noisy gradients</title>
		<author>
			<persName><forename type="first">Gadi</forename><surname>Naveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haim</forename><surname>Sompolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Ringel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01190</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Priors for infinite networks (tech. rep. no. crg-tr-94-1)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Skz_WfbCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sensitivity and generalization in neural networks: an empirical study</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJC2SzZCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Daniel A Abolafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05148</idno>
		<title level="m">Bayesian convolutional neural networks with many channels are gaussian processes</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural tangents: Fast and easy infinite neural networks in python</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SklD9yrFPS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Complexity control by gradient descent in deep networks</title>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Banburski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhaneil</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3360" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Nasim Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08734</idno>
		<title level="m">On the spectral bias of neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmussen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced lectures on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmussen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="294" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A philosophical treatise of universal induction</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Rathmanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1076" to="1136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">A constructive prediction of the generalization error across scales</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12673</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the information bottleneck theory of deep learning</title>
		<author>
			<persName><forename type="first">Yamini</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhu</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artemy</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">D</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124020</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The arrival of the frequent: how bias in genotype-phenotype maps can steer populations to local optima</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schaper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ard</surname></persName>
		</author>
		<author>
			<persName><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">86635</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Shwartz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Classification of radar returns from the ionosphere using neural networks</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">P</forename><surname>Vincent G Sigillito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larrie</forename><forename type="middle">V</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kile B</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Johns Hopkins APL Technical Digest</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="266" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">A bayesian perspective on generalization and stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1710.06451</idno>
		<ptr target="http://arxiv.org/abs/1710.06451" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<title level="m">Don&apos;t decay the learning rate, increase the batch size</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Ockham&apos;s razors</title>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Sober</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A formal theory of inductive inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">part i. Information and control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Asymptotic learning curves of kernel methods: empirical data vs teacher-student paradigm</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Spigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Wyart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10843</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Solomonoff prediction and occam&apos;s razor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><surname>Sterkenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="479" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Strogatz</surname></persName>
		</author>
		<title level="m">Nonlinear dynamics and chaos with student solutions manual: With applications to physics, biology, chemistry, and engineering</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Deep learning generalizes because the parameter-function map is biased towards simple functions</title>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Valle-PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chico</forename><forename type="middle">Q</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ard A</forename><surname>Louis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08522</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Energy landscapes: Applications to clusters, biomolecules and glasses</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">How noise affects the hessian spectrum in overparameterized neural networks</title>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00195</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><surname>ÅwiÄtkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02405</idno>
		<title level="m">How good is the bayes posterior in deep neural networks really? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Bayesian deep learning and a probabilistic perspective of generalization</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08791</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">The relationship between PAC, the statistical physics framework, the bayesian framework, and the vc framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Towards understanding generalization of deep learning: Perspective of loss landscapes</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.10239</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>CoRR, abs/1708.07747</idno>
		<ptr target="http://arxiv.org/abs/1708.07747" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04760</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Wide feedforward or recurrent neural networks of any architecture are gaussian processes</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9951" to="9960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">A fine-grained spectral perspective on neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Salman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10599</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Energy-entropy competition and the effectiveness of stochastic gradient descent in machine learning</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhu</forename><forename type="middle">S</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alpha</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Physics</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">21-22</biblScope>
			<biblScope unit="page" from="3214" to="3223" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Nonvacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach</title>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Orbanz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJgqqsAct7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
