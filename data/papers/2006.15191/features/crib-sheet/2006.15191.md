- **Key Concept**: Stochastic Gradient Descent (SGD) as an Optimizer
  - SGD allows efficient training of overparameterized DNNs, achieving zero training error.

- **Main Question**: Similarity of Probabilities
  - Investigates how similar \( P_{SGD}(f | S) \) (probability of function \( f \) given training set \( S \) under SGD) is to \( P_B(f | S) \) (Bayesian posterior probability).

- **Main Findings**:
  1. **Correlation**: \( P_{SGD}(f | S) \approx P_B(f | S) \) across various architectures (FCNs, CNNs, LSTMs) and datasets (MNIST, Fashion-MNIST, IMDb).
  2. **Bias Towards Simplicity**: \( P_B(f | S) \) is biased towards low-error, low-complexity functions, which also dominate \( P_{SGD}(f | S) \).
  3. **Second Order Effects**: Variations in optimizer choice (e.g., Adam, Adagrad) and hyperparameters (batch size, learning rate) affect \( P_{OPT}(f | S) \).

- **Parameter-Function Map**:
  - Definition: \( M: \Theta \to F \) where \( \Theta \) is the parameter space and \( F \) is the function space.
  - Expresses how model parameters correspond to functions.

- **Bayesian Prior Probability**:
  - Defined as:
    \[
    P(f) = 1[M(\theta) = f] P_{par}(\theta) d\theta
    \]
  - Indicates the probability of a DNN expressing function \( f \) upon random parameter sampling.

- **Bayesian Posterior Probability**:
  - Given training data \( S \):
    \[
    P_B(f | S) := \frac{P(S|f) P(f)}{P(S)}
    \]
  - \( P(S|f) \) is a 0-1 likelihood indicating consistency of data with function \( f \).

- **Inductive Bias**:
  - Strong inductive bias in the parameter-function map explains DNN generalization in the overparameterized regime, rather than SGD being the primary source of bias.

- **Empirical Results**:
  - Extensive sampling experiments show that SGD behaves like a Bayesian optimizer for DNNs with large bias in \( P_B(f | S) \).

- **Hyperparameter Sensitivity**:
  - Changes in batch size and learning rate can compensate for variations in \( P_{OPT}(f | S) \).

- **Diagrammatic Note**: (If needed for clarity)
```mermaid
flowchart TD
    A[Training Set S] -->|Zero Error| B[Function f]
    B --> C{P(f)}
    C -->|Prior| D[P(f)]
    C -->|Likelihood| E[P(S|f)]
    E --> F[Posterior P_B(f|S)]
    F --> G[Inductive Bias]
```

- **Conclusion**: SGD's role is to find functions that generalize well, with the inductive bias primarily stemming from the prior distribution of untrained DNNs.