# Is SGD a Bayesian sampler? Well, almost

## Abstract

## 

Deep neural networks (DNNs) generalise remarkably well in the overparameterised regime, suggesting a strong inductive bias towards functions with low generalisation error. We empirically investigate this bias by calculating, for a range of architectures and datasets, the probability P SGD (f |S) that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function f consistent with a training set S. We also use Gaussian processes to estimate the Bayesian posterior probability P B (f |S) that the DNN expresses f upon random sampling of its parameters, conditioned on S.

Our main findings are that P SGD (f |S) correlates remarkably well with P B (f |S) and that P B (f |S) is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines P B (f |S)), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime.

While our results suggest that the Bayesian posterior P B (f |S) is the first order determinant of P SGD (f |S), there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on P SGD (f |S) and/or P B (f |S), can shed light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.

## Introduction

While deep neural networks (DNNs) have revolutionised modern machine learning [(LeCun et al., 2015;](#b46)[Schmidhuber, 2015)](#b78), a solid theoretical understanding of why they work so well is still lacking. One surprising property is that they typically perform best in the overparameterised regime, with many more parameters than data points. Standard learning theory approaches [(Shalev-Shwartz and Ben-David, 2014)](#b79), based for example on model capacity, suggest that such highly expressive [(Cybenko, 1989;](#b15)[Hornik, 1991;](#b37)[Hanin, 2019)](#b30) DNNs should heavily over-fit in this regime, and therefore not generalise at all.

Stochastic gradient descent (SGD) [(Bottou et al., 2018)](#b9) is one of the key technical innovations allowing large DNNs to be efficiently trained in the highly overparameterised regime. In supervised learning, SGD allows the user to efficiently find sets of parameters that lead to zero training error. The power of SGD as an optimiser for DNNs was demonstrated in an influential paper [(Zhang et al., 2016)](#b105), which showed that zero training error solutions for CIFAR-10 image data with randomised labels can be found with a relatively moderate increase in computational effort over that needed for a correctly labelled dataset. These experiments also framed the conundrum of generalisation in the overparameterised regime as follows: Given that DNNs can memorise randomly labelled image datasets, which leads to poor generalisation, why do they behave so differently on correctly labelled datasets and select for functions that generalise well? The solution to this conundrum must be that SGD-trained DNNs have an inductive bias towards functions that generalise well (on structured data).

The possibility that SGD is not just good for optimisation, but is also a key source of inductive bias, has generated an extensive literature. One major theme concerns the effect of SGD on the flatness of the minima found, typically expressed in terms of eigenvalues of a local Hessian or related measures. A link between better generalisation and flatter minima has been widely reported [(Hochreiter and Schmidhuber, 1997a;](#)[Keskar et al., 2016;](#b42)[Jastrzebski et al., 2018;](#b39)[Wu et al., 2017;](#b99)[Zhang et al., 2018;](#b106)[Wei and Schwab, 2019)](#b94), but see also [(Dinh et al., 2017)](#b22). Theoretical work on SGD has also generated a large and sophisticated literature. For example, in [(Soudry et al., 2018)](#b86) it was demonstrated that SGD finds the max-margin solution in unregularised logistic regression, whilst it was shown in [(Brutzkus et al., 2017)](#b11) that overparameterised DNNs trained with SGD avoid over-fitting on linearly separable data. Recently, [(Allen-Zhu et al., 2019)](#b0) proved agnostic generalisation bounds of SGD-trained neural networks. Other recent work [(Poggio et al., 2020)](#b69) suggests that gradient descent performs a hidden regularisation in normalised weights, but a different analysis suggests that such implicit regularisation may well be very hard to prove in a more general setting for SGD [(Dauber et al., 2020)](#b16). Overall, while SGD and its related algorithms are excellent optimisers, there is as yet no consensus on what inductive bias SGD provides for DNNs. For a more detailed discussion of this SGD-related literature see Section 7.2.

An alternative approach is to consider the inductive properties of random neural networks, that is untrained DNNs with weights sampled from a (typically i.i.d.) distribution. Recent theoretical and empirical work [(Valle-Pérez et al., 2018;](#b92)[De Palma et al., 2018;](#b18)[Mingard et al., 2019)](#b61) suggests that the (prior) probability P (f ) that an untrained DNN outputs a function f upon random sampling of its parameters (typically the weights and biases) is strongly biased towards "simple" functions with low Kolmogorov complexity (see also Section 7.3). A widely held assumption is that such simple hypotheses will generalise well -think Occam's razor. Indeed, many processes modelled by DNNs are simple [(Lin et al., 2017;](#b52)[Goldt et al., 2019;](#b28)[Spigler et al., 2019)](#b87). For more on these topics see Section 7.3 and Section 7.5.

If the inductive bias towards simplicity described above for untrained networks is preserved throughout training, then this could help explain the DNN generalisation conundrum. Again, there is an extensive literature relevant to this topic. For example, a number of papers [(Poole et al., 2016;](#b70)[Lee et al., 2018;](#b48)[Valle-Pérez et al., 2018;](#b92)[Yang, 2019a;](#)[Mingard et al., 2019;](#b61)[Cohen et al., 2019;](#b14)[Wilson and Izmailov, 2020)](#b97) employ arguments on heuristic grounds that the bias in untrained random neural networks could be used to study the inductive bias of optimiser-trained DNNs. Optimiser-trained DNNs have also been directly compared to their Bayesian counterparts (c.f. Sections 6 and 7.4 for more detailed discussions). In an important development, [Lee et al. (2017)](#b47); [Matthews et al. (2018)](#); [Novak et al. (2018b)](#b67) used the Gaussian process (GP) approximation to Bayesian DNNs, which is exact in the limit of infinite width, and found that the generalisation performance of Bayesian DNNs and SGD-trained DNNs was relatively similar for standard deep learning datasets such as CIFAR-10, though [Wenzel et al. (2020)](#b96) found more significant differences when using Monte Carlo to approximate finite-width Bayesian DNNs. Others have used either Monte Carlo methods [(Mandt et al., 2017)](#b56) or the GP approximation [(Matthews et al., 2017;](#b58)[de G. Matthews et al., 2018;](#b17)[Lee et al., 2019;](#b49)[Wilson and Izmailov, 2020)](#b97) to examine how similar the Bayesian posterior is to the sampling distribution of SGD (whether in parameter or function space), albeit on relatively low dimensional systems compared to conventional DNNs.

In this paper we perform extensive computations, for a series of standard DNNs and datasets, of the probability P SGD (f |S) that a DNN trained with SGD (or one of its variants) to zero error on training set S, converges on a function f . We then compare these results to the Bayesian posterior probability P B (f |S), for these same functions, conditioned on achieving zero training error on S.

The main question we explore here is: How similar is P B (f |S) to P SGD (f |S)? If the two are significantly different, then we may conclude that SGD provides an important source of inductive bias. If the two are broadly similar over a wide range of architectures, datasets, and optimisers, then the inductive bias is primarily determined by the prior P (f ) of the untrained DNN.

## Main results summary

We carried out extensive sampling experiments to estimate P SGD (f |S). Functions are distinguished by the way they classify elements on a test set E. We use the Gaussian process (GP) approximation to estimate P B (f |S) for the same systems. Our main findings are:

(1) P SGD (f |S) ≈ P B (f |S) for a range of architectures, including FCNs, CNNs and LSTMs, applied to datasets such as MNIST, Fashion-MNIST, an IMDb movie review database and an ionosphere dataset. This agreement also holds for variants of SGD, including Adam (Kingma and Ba, 2014), Adagrad [(Duchi et al., 2011)](#b25), [Adadelta (Zeiler, 2012)](#) and RMSprop [(Tieleman and Hinton, 2012)](#b90).

(2) The P B (f |S) of functions f that achieve zero-error on the training set S can vary over hundreds of orders of magnitude, with a strong bias towards a set of low generalisation/low complexity functions. This tiny fraction of high probability functions also dominate what is found by DNNs trained with SGD. It is striking that even within this subset of functions, P SGD (f |S) and P B (f |S) correlate so well. Our empirical results suggest that, for DNNs with large bias in P B (f |S), SGD behaves to first order like a Bayesian optimiser and is therefore exponentially biased towards simple functions with better generalisation. Thus, SGD is not itself the primary source of inductive bias for DNNs.

(3) A function-based picture can also be fruitful for illustrating second order effects where an optimiser-trained DNN differs from the Bayesian prediction. For example, training an FCN with different optimisers (OPT) such as Adam, Adagrad, Adadelta and RMSprop on MNIST generates slight but measurable variations in the distributions of P OPT (f |S). Such information can be used to analyse differences in performance. For instance, we find that changing batch size affects P Adam (f |S) but, as was found for generalisation error [(Keskar et al., 2016;](#b42)[Goyal et al., 2017;](#b29)[Hoffer et al., 2017;](#b36)[Smith et al., 2017)](#), this effect can be compensated by changes in learning rate. Architecture changes can also be examined in this picture. For example, adding max-pooling to a CNN trained with Adam on Fashion-MNIST increases both P B (f |S) and P Adam (f |S) for the lowest-error function f found.

## Preliminaries

We first introduce a key definition from [(Valle-Pérez et al., 2018)](#b92) needed to specify P (f ) and P B (f |S).

Definition 2.1 (Parameter-function map). Consider a parameterised supervised model, and let the input space be X and the output space be Y. The space of functions the model can express is a set F ⊆ Y X . If the model has some number of parameters, taking values within a set Θ ⊆ R p , then the parameter-function map M is defined by

$M : Θ → F θ → f θ where f θ is the function corresponding to parameters θ ∈ Θ.$The function space F of a DNN N could in principle be considered to be the entire space of functions that N can express on the input vector space X , but it could also be taken to be the set of partial functions N can express on some subset of X . For example, F could be taken to be the set of possible classifications of images in MNIST. In this paper we always take F to be the set of possible outputs of N for the instances in some dataset.

## The Bayesian prior probability, P (f )

Given a distribution P par (θ) over the parameters, we define the P (f ) over functions as

$P (f ) = 1[M(θ) = f ]P par (θ)dθ, (1)$where 1 is an indicator function (1 if its argument is true, and 0 otherwise). This is the probability that the model expresses f upon random sampling of parameters over a parameter initialisation distribution P par (θ), which is typically taken to have a simple form such as a (truncated) Gaussian. P (f ) can also be interpreted as the probability that the DNN expresses f upon initialisation before an optimisation process. It was shown in [(Valle-Pérez et al., 2018)](#b92) that the exact form of P par (θ) (for reasonable choices) does not affect P (f ) much (at least for ReLU networks). If we condition on functions that obtain zero generalisation error on a dataset S, then the procedure above can also be used to generate the posterior P B (f |S) which we describe next.

## The Bayesian posterior probability, P B (f |S)

Here, we describe the Bayesian formalism we use, and show how bias in the prior affects the posterior. Consider a supervised learning problem with training data S corresponding to the exact values of the function which we wish to infer (i.e. no noise). This formulation corresponds to a 0-1 likelihood P (S|f ), indicating whether the data is consistent with the function. Formally, if S = {(x i , y i )} m i=1 corresponds to the set of training pairs, then we let

$P (S|f ) = 1 if ∀i, f (x i ) = y i 0 otherwise .$Note that in our calculations, this quantity is technically P (S|f ; {x i }), but we denote it as P (S|f ) to simplify notation. We will use a similar convention throughout, whereby the input points are (implicitly) conditioned over. We then assume the prior P (f ) corresponds to the one defined in Section 2.1. Bayesian inference then assigns a Bayesian posterior probability P B (f |S) to each f by conditioning on the data according to Bayes rule

$P B (f |S) := P (S|f )P (f ) P (S) ,(2)$where P (S) is also called the marginal likelihood or Bayesian evidence. It is the total probability of all functions compatible with the training set. For discrete functions, P (S) = f P (S|f )P (f ) = f ∈C(S) P (f ), with C(S) the set of all functions compatible with the training set. For a fixed training set, all the variation in P B (f |S) for f ∈ C(S) comes from the prior P (f ) of the untrained network since P (S) is constant. Thus, the bias in the prior is essentially translated over to the posterior.

Thus, P B (f |S) is the distribution over functions that would be obtained by randomly sampling parameters according to P par (θ) and selecting only those that are compatible with S.

## 2.3

The optimiser probability, P OPT (f |S) But DNNs are not normally trained by randomly sampling parameters: They are trained by an optimiser. The probability that the optimiser OPT (e.g. SGD) finds a function f with zero error on S can be defined as:

$P OPT (f |S) := 1[M(θ f ) = f ]P OPT (θ f |θ i , S) Ppar (θ i )dθ i dθ f (3)$where P OPT (θ t |θ i , S) denotes the probability that OPT, initialised with parameters θ i on a DNN, converges to parameters θ f when training is halted after the first epoch where zero classification error is achieved on S[foot_0](#foot_0) , if such a condition is achieved in a number of iterations less than the maximum number which we allow for the experiments. The initialisation distribution Ppar (θ i ) is defined analogously to P par (θ) in Equation (1) (though it need not be exactly the same). P OPT (f |S) is, therefore, a measure of the 'size' of f 's 'basin of attraction', which intuitively refers to the set of initial parameters that converge to f upon training.

## Methodology, Datasets and DNNs

## Methodology

## Definition of functions

$For a specific DNN, training set S = {(x i , y i )} |S| i=1 and test set E = {(x i , y i )} |E| i=1$, we define a function f as a labelling of the inputs in S concatenated with the inputs in E. 2 We will only look at functions which have 0 error on S, so that, for a particular experiment with fixed S and E, the functions are distinguished only by the predictions they make on E. Furthermore we only consider binary classification tasks (c.f. Section 3.2), so that our output space 3 is Y = {0, 1}. Therefore, we will represent functions by a binary string of length |E| representing the labels on E; the ith character representing the label on the ith input of E, x i . For the sake of simplicity, we will not make a distinction between this representation of f and the function f itself, as they are related one-to-one for any particular experiment (with fixed S and E).

Restricting the input space where functions are defined can be thought of as a coarsegraining of the functions on the full input space (e.g. the space of all possible images for image classification), which allows us to estimate their probabilities from sampling. In the following subsections we explain how the main experimental quantities are computed. Further detail can be found in Appendix A.

## Calculating P OPT (f |S)

For a given optimiser OPT (SGD or one of its variants), a DNN architecture, loss function (cross-entropy (CE) or mean-square error (MSE)), a training set S, and test set E, we repeat the following procedure n times: We sample initial parameters θ i , from an i.i.d. truncated Gaussian distribution Ppar (θ i ), and train with the optimiser until the first epoch where the network has 100% training classification accuracy (except for experiments labelled "overtraining," where we halt training after p further epochs with 0 training error have occured, for some specified p) 4 . We then compute the function f found by evaluating the network on the inputs in E, as described before.

Note that during training, the network outputs are taken to be the pre-activations of the output layer, which are fed to either the MSE loss, or as logits for the CE loss. At evaluation (to compute f ), the pre-activations are passed through a threshold function so that positive pre-activations output 1 and non-positive pre-activations output 0.

We chose sample sizes between n = 10 4 and n = 10 6 . In other words, we typically sample over n = 10 4 to n = 10 6 different trained DNNS, and count how many times each function f appears in the sample to generate the estimates of P OPT (f |S). We leave the dependence of P OPT (f |S) on E implicit. This method of estimating P OPT (f |S) is described more formally in Appendix A.1.1.

## Formally then, our space of functions is

$F = Y X , where X = {xi} |S| i=1 ∪ {x i } |E| 3.$Note that for training with MSE loss, we centered the output so the loss is measured with respect to target values in {-1, 1}. This is so thresholding can occur at a value of the last layer preactivation of 0, which is the same as for cross-entropy loss on logits. 4. If SGD fails to achieve 100% accuracy on S in a maximum number of iterations, we discard the run.

## Calculating P B (f |S) with Gaussian Processes

We use neural network Gaussian processes (NNGPs) [(Lee et al., 2017;](#b47)[Matthews et al., 2018;](#)[Garriga-Alonso et al., 2019;](#b27)[Novak et al., 2018b)](#b67) to approximate P B (f |S), for some training set S and test set E. NNGPs have been shown to accurately approximate the prior over functions P (f ) of finite-width Bayesian DNNs [(Valle-Pérez et al., 2018;](#b92)[de G. Matthews et al., 2018)](#b17). We use DNNs with relatively wide intermediate layers, relative to the input dimension, to ensure that we are close to the infinite layer-width NNGP limit. Depending on the loss function, we estimate the posterior P B (f |S) as follows:

• Classifiation as regression with MSE loss. As has been done in previous work on NNGPs, we consider the classification labels as regression targets with an MSE loss 5 . We compute the analytical posterior for the NNGP with Gaussian likelihood. This is the posterior over the real-valued outputs at the test points on E, which correspond to the pre-activations of the output layer of the DNN. We sample from this posterior, and threshold the real-values like we do for DNNs (positive becomes 1 and otherwise it becomes 0) to obtain labels on E, and thus a function f . We then estimate P B (f |S) by counting how many times each f is obtained from a set of n independent samples from the posterior, similar to what we did for P OPT (f |S). For more details on GP computations with MSE loss, see Appendix A.2.1. We describe this method more formally in Appendix A.1.2. We use this technique when comparing P B (f |S) with P OPT (f |S) for MSE loss (e.g. Figure [1a](#fig_0)).

• Classification with CE loss. In several experiments, we approximate the NNGP posterior using a 0-1 misclassification loss, which is more justified for classification, and can be thought of as a "low temperature" version of the CE loss. Since, in contrast to the MSE case, this posterior is not analytically tractable, we use the expectation propagation (EP) approximation to estimate probabilities [(Rasmussen, 2004)](#b72). In particular, we estimate P B (f |S) via ratio of EP-approximated likelihoods.

The EP approximation can be used to estimate the marginal likelihood of any labelling over any set of inputs, given a GP prior. As shown in Equation (2), we can use Bayes theorem to express P B (f |S) as a ratio of P (f ) and P (S) (which is valid for functions with 0 error on S, and the 0 -1 likelihood), and then use EP approximation to obtain both of these probabilities. In the text, when we refer to the EP approximation for calculating P B (f |S), we are using it as described above.

## Comparing

$P OPT (f |S) to P B (f |S)$We note that for MSE loss, we can sample to accurately estimate function probabilities, whereas for the CE loss, we must use the EP approximation to calculate the probability of functions. 6 When we compare P B (f |S) to P OPT (f |S) for CE loss, we take the functions found by the optimiser, which are obtained as described in Section 3.1.2, and calculate their P B (f |S) using the EP approximation. For MSE loss, both the P B (f |S) and the P OPT (f |S) are sampled independently, and probabilities are compared for functions found by both methods.

5. As for POPT(f |S) with MSE loss, we take the regression targets to be {-1, 1}, so thresholding occurs at 0 6. See Appendix A.2.2 for more details.

3.1.5 Calculating P B (f |S) for functions with generalisation error from 0% to 100%

For the zero training error case studied here, we define functions by their particular labelling on the test set E (as described in Section 3.1.1). A function can be generated by picking a certain labelling. Subsequently P B (f |S) for CE loss can be calculated using the EP approximation as described above. To study how P B (f |S) varies with generalisation error

G on E (the fraction of missclasified inputs on E), we perform the following procedure. For each value of G chosen, typically 10 functions are uniformly sampled by randomly selecting G |E| bits in E and flipping them. EP is then used to calculate P B (f |S) for those functions.

The probabilities P B (f |S) can range over many orders of magnitude. The low probability functions cannot be obtained by direct sampling, so that a full comparison with P OPT (f |S) is not feasible. This is more formally described in Appendix A.1.3.

## CSR complexity

The critical sample ratio (CSR) is a measure of complexity of functions expressed by DNNs [(Krueger et al., 2017)](#b44). It is defined with respect to a sample of inputs as the fraction of those samples which are critical samples. A critical sample is defined to be an input such that there is another input within a box of side 2r centred around the input, producing a different output (for discrete classification outputs). See Appendix E for further details.

## Data sets

To efficiently sample functions, we use relatively small test sets (typically |E| = 100) and, as is often done in the theoretical literature, binarise our classification datasets. We define the datasets used below: MNIST: The MNIST database of handwritten numbers [(LeCun et al., 1999)](#b45) was binarised with even numbers classified as 0 and odd numbers as 1. Unless otherwise specified, we used |S| = 10000 and |E| = 100. Fashion-MNIST: The Fashion-MNIST database [(Xiao et al., 2017)](#b100) was binarised with T-shirts, coats, pullovers, shirts and bags classified as 0 and trousers, dresses, sandals, trainers and ankle boots classified as 1. Unless otherwise specified, we used |S| = 10000 and |E| = 100.

IMDb movie review dataset: We take the IMDb movie review dataset from Keras. The task is to correctly classify each review as positive or negative given the text of the review. We preprocess the set by removing the most common words and normalising. 7 This procedure was employed to make sure there are functions with high enough probability to be sampled multiple times with Experiments 1 and 2 above. Used with |S| = 45000 and |E| = 50.

Ionosphere Dataset: This is a small non-image dataset with 34 features 8 aimed at identifying structure in the ionosphere [(Sigillito et al., 1989)](#b81). Used with |S| = 301 and |E| = 50.

For image datasets, we will typically use normalised data (pixel values in range [0,1]) for MSE loss, and unnormalised data for CE loss (pixel values in range [0,255]).

7. We used the version of the dataset and preprocessing technique given here: [https://www.kaggle.com/ drscarlat/imdb-sentiment-analysis-keras-and-tensorflow](https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow) 8. [https://archive.ics.uci.edu/ml/datasets/Ionosphere](https://archive.ics.uci.edu/ml/datasets/Ionosphere)

## Architectures

We used the following standard architectures. FCN: 2 hidden layer, 1024 node vanilla fully connected network [(FCN)](#) with ReLU activations. CNN + (Max Pooling) + [BatchNorm]: Layer 1: Convolutional Layer with 32 features size 3 × 3. (Layer 1a: Max Pool 2 × 2). [Layer 1b: Batch Norm]. Layer 2: Flatten. Layer 3: FCN with width 1024. [Layer 3a: Batch Norm]. Layer 4: FCN, 1 output with ReLU activations. LSTM: Layer 1: Embedding layer. Layer 2: LSTM, 256 outputs. Layer 3: FCN, 512 outputs. Layer 4: Fully-Connected, 1 output with ReLU activations for the fully connected layers. Hyperparameters are, unless otherwise specified, the default values in Keras 2.3.0. See Appendix A.1.1 for details on the parameter initialisation. 4. Empirical results for P B (f |S) v.s. P OPT (f |S) for different architectures and datasets In this first of two main results sections, we focus on testing our hypothesis that P B (f |S) ≈ P OPT (f |S) for FCN, CNN and LSTM architectures on MNIST, Fashion-MNIST, the IMDb review, and the Ionosphere datasets, using several variants of the SGD optimiser. In the following subsection will describe the main results in detail for an FCN on MNIST. The experiments in the next sections will be the same except for the architecture, dataset, or optimiser which will be varied. 4.1 Comparing P B (f |S) to P OPT (f |S) for an FCN on MNIST In Figure 1 we present a series of results for a standard DNN setup: an FCN (2 hidden layers, each 1024 node wide with ReLU activations), trained on (binarised) MNIST to zero training error with a training set size of |S| = 10, 000 and a test set size of |E| = 100. Note that even for this small test set, there are 2 100 ≈ 1.3 × 10 30 functions with zero error on S, all of which an overparametrized DNN could express (Zhang et al., 2016) 9 . We chose standard values for batch size, learning rate, etc., if given by the default values in Keras 2.3.0 (e.g. batch size of 32 and learning rate of 0.01 for SGD). Our experiments in Section 5 and the appendices will show that our results are robust to the choice of these hyperparameters. Figure 1a compares the value of P B (f |S) and P SGD (f |S) for the highest probability functions of each distribution, for MSE loss. Each data point in the plot corresponds to a unique function (a unique classification of images in the test set E). The functions are obtained by sampling both P B (f |S) and P SGD (f |S) and taking the union of the set of functions obtained. P B (f |S) and P SGD (f |S) were estimated as frequencies from the corresponding sample as explained in Sections 3.1.2 and 3.1.3. If a function does not appear in one of the samples, we set its frequency to take the minimum value so that it would appear on top of one of the axes. For example, a function that appears in the SGD runs, but not in the sampling for P B (f |S), will appear on x-axis at the value obtained for P SGD (f |S). Here we used MSE loss rather than the more popular (and typically more computationally efficient) CE loss because 9. We also find in Figure 20a and Figure 20b that our 2-layer FCN is capable of expressing functions on MNIST with the full range of training and generalisation errors (e) P B (f |S) v.s. P Adagrad (f |S) for MSE loss; P Adagrad (f |S) was sampled n = 10 5 times (while the GP sample was the same as in (a)). Adagrad was overtrained until 64 epochs had passed with zero error. The average error is G = 1.53%. (f) is as (e) but with CE loss, so that the EP approximation was used for P B (f |S), making the estimate of P B (f |S) slightly less accurate. G = 2.63%.

for MSE, the posterior can be sampled from without further approximations, while for CE loss, the expectation propagation (EP) approximation needs to be used making P B (f |S) less accurate (see Appendix A.2 for further details).

Figure [1a](#fig_0) also demonstrates that P SGD (f |S) and P B (f |S) are remarkably closely correlated for MSE loss, and that a remarkably small number of functions account for most of the probability mass for both P B (f |S) and P SGD (f |S). To appreciate how remarkably tight this agreement is, consider the full scale of probabilities for functions f that achieve zero error on the MNIST training set. The average P B (f |S) of all these functions is 2 -100 ≈ 10 -30 . Therefore the functions in Figure [1a](#fig_0) have probabilities that are many orders of magnitude higher than average. At the same time, P B (f |S) and P SGD (f |S) for these functions typically agree within less than one order of magnitude. Another way of quantifying the agreement is that 90% of the cumulative probability weight from both P SGD (f |S) and P B (f |S) for the test set E in Figure [1a](#fig_0) is made up from the contributions of only a few tens of functions with zero training error out of ≈ 10 30 such possible functions (see vertical doted line in Figure [1a](#fig_0)). Moreover, these particular functions are the same for both P B (f |S) and P SGD (f |S). The agreement between the two methods is remarkable. Overall, the observations in Figure [1a](#fig_0) suggest that the main inductive bias of this DNN is present prior to training.

## Figure 1b plots the mean probability for obtaining a generalisation error of

$G in the training set E, which is estimated as ρ( G ) P B (f |S) G where ρ( G ) = |E|!/((|E| -G |E|)!( G |E|)!)$denotes the number of functions with G |E| errors on E, and P B (f |S) G denotes the expected value of P B (f |S), where the expectation is with respect to uniformly sampling from the set of functions with fixed G . As explained in Section 3.1.5, we estimate the average • G by sampling, and we estimated P B (f |S) for each f in the sample using the EP approximation.

Figure [1b](#fig_0) can be interpreted as showing that the inductive bias encoded in P B (f |S) gives good generalisation. More precisely, we find that P B (f |S) is exponentially biased towards functions with low generalisation error. To illustrate how strong the bias is, we can look at ρ( G ). Over 50% of functions are in the range of G = 50 ± 3 errors, while only 10 -23 % have G ≤ 3. Therefore for P B (f |S) to overcome the 'entropic' factor ρ( G ) and show the behaviour in Figure [1b](#fig_0), it must in average give a probability many orders of magnitude higher to low error functions than to high error functions. In Appendix A.1.3, we also observed that the probability p i of misclassifying an image in the test set varies a lot between images, and that these probabilities are to first order independent. As a corollary, in Figure [8](#fig_14) we show for P B (f |S) and P SGD (f |S) that the probabilities of multiple images being misclassified can be accurately estimated from the products of the probabilities p i for misclassifying individual images. Thus this system appears to behave like a Poisson-Binomial distribution with independent and non-identically distributed random p i , which most likely also explains why log P B (f |S) G scales nearly linearly with G .

Although we cannot measure P SGD (f |S) for the high generalisation error functions, the agreement in Figure [1a](#fig_0) (and elsewhere in this paper) implies that P SGD (f |S) must also be on average orders of magnitude lower for high error functions than low error functions. However, at the moment we can only conjecture that P SGD (f |S) follows the same exponential behaviour as P B (f |S) over the whole range of G . Finally, in Appendix A.1.3, we make some further remarks and caveats about this experiment, and other similar experiments.

Figure [1c](#fig_0) shows the correlation between the complexity of the functions obtained to create Figure [1b](#fig_0), and their generalisation error, as well as their P B (f |S) (from EP approximation) represented in their color. The complexity measure we used is the critical sample ratio (CSR) complexity [(Krueger et al., 2017)](#b44) computed on the inputs in E, which measures what fraction of inputs are near the decision boundary (see Section 3.1.6).

Figure [1c](#fig_0) also shows that there is a inverse correlation between the generalisation of a function and its CSR complexity, as well as between P B (f |S) and CSR. In Section 2.2, we showed that P B (f |S) is proportional to the prior probability of a function P (f ) for functions that have zero error on the training set S. We can thus understand the inverse correlation between P B (f |S) and CSR in the light of previous simplicity bias results showing that the prior P (f ) of Bayesian DNNs is exponentially biased towards functions with low Kolmogorov complexity (simple functions) [(Valle-Pérez et al., 2018;](#b92)[Mingard et al., 2019)](#b61). In [(Valle-Pérez et al., 2018)](#b92), it was further shown for an FCN on a subsample of MNIST that P (f ) correlated remarkably well with CSR 10 , and our results are in agreement with that finding. The results in this figure extend those of Figure [1b](#fig_0) to show that P B (f |S) is biased both towards low error and simple functions, and that simple functions are the ones that tend to have good generalisation on MNIST.

Figure [1d](#fig_0) shows the correlation between P B (f |S) and G for functions used for Figure [1a](#fig_0). We note that, as can also be observed in Figure [1a](#fig_0), values of P B (f |S) are high for low error functions, and high error functions have relatively lower values of P B (f |S). This figure also uses colour to show which functions were not found in the sampling of P SGD (f |S). It shows clearly that SGD finds all the high P B (f |S) functions.

Figure [1e](#fig_0) shows the same type of experiment as in Figure [1b](#fig_0), but using a different SGDbased optimiser, Adagrad [(Duchi et al., 2011)](#b25) with overtraining (where training was halted after 64 epochs had passed with 100% training accuracy). We see that it exhibits similar correlation between P B (f |S) and P OPT (f |S) to vanilla SGD (and very similar agreement was observed without overtraining). We will see throughout the paper remarkably good correlations between P B (f |S) and P OPT (f |S) holds for a large range of optimisers and hyperaparameters Figure [1f](#fig_0) shows the same type of experiment as in Figure [1a](#fig_0), but using CE loss, the Adagrad optimiser, and overtraining (also to 64 epochs). See Figure [11b](#fig_0) for the equivalent plot but without overtraining. As we are using CE loss (see Section 3.1.3 and Section 3.1.4), we sample functions from P OPT (f |S), and then use the EP to estimate P B (f |S) for the functions obtained. We find similar results to Figure [1e](#fig_0), where we used MSE loss (and direct sampling for P B (f |S)). The errors introduced by the EP approximation may explain why the correlation does not follow the x=y line as closely as it does for the MSE calculations. Nevertheless, the correlation between P B (f |S) and P Adagrad (f |S) is strong, providing evidence that our results for an FCN on MNIST are not an artefact of the exact optimiser or loss function used.

10. Furthermore in [(Valle-Pérez et al., 2018)](#b92), it was shown that this was not an exclusive property of CSR and that any measure that could approximate Kolmogorov complexity seems to also correlate well with P (f ). We next turn to a more complex dataset, namely Fashion-MNIST [(Xiao et al., 2017)](#b100) which consists of images of clothing, as well as a more complex network architecture, the [CNN (LeCun et al., 1999)](#) which was designed in part to have a better inductive bias for images. See Section 3.2 and Section 3.3 for details on dataset and architecture. We can see in Figure [2](#) a strong correlation between P B (f |S) and the probabilities found by the Adam optimiser (Kingma and Ba, 2014), a variant of SGD. Note that instead of MSE loss we used CE loss because it is more efficient. A downside of this choice is that we need to use the EP approximation for the GP calculations (see Appendix A.2.2). Although the correlation is strong, it does not follow x=y as closely as we generally find for MSE loss, which is quite possibly an effect of the EP approximation. See Figure [13](#fig_4) for an example with MSE loss where the correlation does follow x=y more closely. Both the FCN and the CNNs exhibit a strong bias towards low error functions on Fashion-MNIST as we can see in Figure [13c](#fig_4) and Figure [13d](#fig_4).

For an example of how the effects of architecture modifications can be observed in the function probabilities, compare results in Figure [2b](#) for the vanilla CNN to those in Figure [2c](#) for a CNN with max-pooling [(He et al., 2016)](#b31), a method designed to improve the inductive bias of the CNN. As expected, the generalisation performance of the CNN improves, and an important contributor is the increase in the probability of the highest probability 1-error function in both P B (f |S) and P Adam (f |S), directly demonstrating an enhancement of the inductive bias. See Figure [13](#fig_4) for related results. This example demonstrates how a function based picture as well as analysis of the Bayesian P B (f |S) sheds light on the inductive bias of a DNN. Such insights could help with architecture search, or more generally with developing new architectures with improved implicit bias toward desired low error functions. In Figure [3](#fig_2) we compare P B (f |S) to the output of the neural tangent kernel (NTK) [(Jacot et al., 2018)](#b38), which approximates gradient descent in the limit of infinite width and infinitesimal learning rate. The generalisation error of NTK and NNGPs have been shown to be relatively close, and they produce similar functions on simple 1D regression [(Lee et al., 2019;](#b49)[Novak et al., 2020)](#b68). Here we show that this similarity also holds for the function probabilities for a more complex classification task. However, we also find the NTK misses many relatively high probability functions that both SGD and the GP find. We are currently investigating this surprising behaviour, which may arise from the infinitesimal learning rate. Their low probability may also be exacerbated by the fact that in Figure [3](#fig_2) the NTK is very highly biased towards one 2-error function, forcing other functions to have low cumulative probability. Again, this example demonstrates how a function based picture picks up rich details of the behaviour that would be missed when simply comparing generalisation error.   We test a more complex DNN with a LSTM layer [(Hochreiter and Schmidhuber, 1997b)](#), applied to a problem of sentiment analysis on the IMDb movie database. We used a smaller test set |E| = 50 and a larger training set |S| =45,000 to ensure that generalisation was good enough to ensure that functions are found with sufficient frequency to be able to extract probabilities. As can be seen in Figure [4a](#fig_10) we again observe a reasonable correlation between the functions found by Bayesian sampling, and those found by the optimiser. Figure [4b](#fig_10) also shows that, as observed for other datasets, this system is highly biased towards low error functions. We show some further experiments with the LSTM in Figure [14](#fig_17) in Appendix D, including an experiment with MSE loss to avoid the EP approximation. As another non-image classification example, we use the small non-image Ionosphere dataset (with a training set of size 301), using an FCN with 3 hidden layers of width 256. As can be seen in Figure [4c](#fig_10), for MSE loss we find a fairly good correlation. Further details and an example with CE loss can be found in Figure [15](#fig_7).

$(a) P B (f |S) v.s. P Adam (f |S) (b) P B (f |S) v.s. G (c) P B (f |S) v.s. P Adam (f |S)$
## Effects of training set size

We performed experiments comparing P B (f |S) and P OPT (f |S) for different training set sizes for the FCN on MINST. We observe that increasing the amount of training data from |S| = 1000 to |S| = 20000 increases the bias towards low error functions. This increase has the following effects: 1) An increase in the value of P B (f |S) and P SGD (f |S) for functions with low G by several orders of magnitude, 2) an increase by several orders of magnitude of P B (f |S) and P OPT (f |S) for the mode functions (the ones with highest probability), 3) A decrease in the number of functions that cumulatively take up 90% of the observed probability weight, and 4) a significant increase in the tightness of correlation between P B (f |S) and P OPT (f |S). See Appendix C in Appendix C for detailed results and plots.

## Results for other test sets

For the experiments shown in this section, sampling efficiency considerations means that we have limited ourselves to a relatively small test sets (|E| ≤ 100). In Figure [12](#fig_0), we have checked that other test sets also show close agreement between P B (f |S) and P SGD (f |S). For larger |E|, P OPT (f |S) quickly becomes impossible to directly measure empirically -doubling the test set roughly means squaring the number of samples to obtain qualitatively similar results, as the values for P B (f |S) decrease exponentially with test set size. However, if we assume that the images are approximately independently distributed throughout the larger test set, as Appendix B suggests, then we can estimate the highest probabilities from products of P B (f |S) or P SGD (f |S) on the smaller sets.

5. The effect of hyperparameter changes and optimisers on P B (f |S) and P OPT (f |S)

In the first section we focussed on the first-order similarity between P B (f |S) and P OPT (f |S).

In this second main results section, we focus on second-order effects that affect P OPT (f |S) differently from P B (f |S). These include the effects of hyperparameter settings and optimiser choice.

## Changing batch size and learning rate

In a well-known study, [(Keskar et al., 2016)](#b42) showed that, for a fixed learning rate, using smaller batch sizes could lead to better generalisation. In Figure [5](#fig_7) (a)-(c) we observe this same effect but reflected in the more finely grained spectrum of function probabilities. For batch size 512, we also reproduce in Figure [5d](#fig_7) the effect observed in [(Goyal et al., 2017;](#b29)[Hoffer et al., 2017;](#b36)[Smith et al., 2017)](#), that speeding up the learning rate for a fixed batch size can mimic the improvement in G for smaller batches. Interestingly, as can be seen by comparing Figures 5d to 5f, the overall correlation of the function probability spectrum appears tighter for the 128 and 512 batch size with the same learning rates, even though the generalisation errors are different. However, if the learning rate is increased 4× for the the 512 batch size system, then there is a closer correlation with batch size 128 for the higher probability functions. It is these latter functions that dominate the average for G and so the closer correlation for those functions, rather than the less good correlation for low probability functions, explains the better agreement seen in generalisation error for the two systems. Finally, in Figure [18](#fig_14) of Appendix D, we vary batch size for MSE, finding different trends to CE loss. For MSE, increasing batch size leads to better generalisation due to second order effects where P SGD (f |S) preferentially converges on a few key higher probability/lower error functions. The batch size can be correlated with the noise spectrum of the underlying Langevin equation that describes SGD [(Bottou et al., 2018;](#b9)[Jastrzebski et al., 2018;](#b39)[Zhang et al., 2018)](#b106). What our function based results demonstrate is that the behaviour of the optimiser on the loss-landscape is affected in subtle ways by the form of the loss function, as well as the amount noise, and possibly also by correlations in the noise. 

## Changing optimisers

We trained the FCN on MNIST with different optimisers (Adam, Adagrad, RMSprop, Adadelta), and found that to first order P B (f |S) correlated well with P OPT (f |S) for all four optimisers. We also observed some second order effects, including that the distribution of P Adam (f |S) and P Adagrad (f |S) were very similar to one another, as were P RMSprop (f |S) and P Adadelta (f |S), but there was noticeable variation between the two groups. We find that P Adam (f |S) with batch size of 32 is very similar to P RMSprop (f |S) with a batch size of 128. The effect of optimiser choice, batch size, learning rate, and other hyperameters is complex, and the parameter space is large. Analysing optimisers in function-space could be a way to better understand the interaction of these choices with the loss landscape, and understanding the effects of hyperparameter tuning. See Appendix C.1 for further detail and the plots.

6. Heuristic arguments for the correlation between P B (f |S) and P SGD (f |S)

At first sight it may seem rather surprising that SGD, which follows gradients down a complex loss-landscape, should converge on a function f with a probability anything like the Bayesian posterior P B (f |S) that upon random sampling of parameters, a DNN generates functions f conditioned on S. Indeed, in the general case of an arbitrary learner we don't expect this correspondence to hold. However, as shown e.g. in Fig [1](#fig_0), P B (f |S) is orders of magnitude larger for functions with small generalisation error than it is for functions with poor generalisation. As explained in Sections 7.3 and 7.5, such an exponential bias towards low complexity/low error functions can be expected on fairly general grounds [(Valle-Pérez et al., 2018;](#b92)[Mingard et al., 2019;](#b61)[Dingle et al., 2018](#b20)[Dingle et al., , 2020))](#b21). If our null expectation is of a large variation in the prior probabilities, then the good correlation can be heuristically justified by a landscape picture [(Wales et al., 2003)](#b93), where P B (f |S) is interpreted as the "basin volume" V B (f ) (with measure p par (θ)) of function f ), while P SGD (f |S) is interpreted as the "basin of attraction" V SGD (f ), which is loosely defined as a measure of the set of initial parameters θ i for which the optimiser converges to f with high probability (this concept also found in related form in the dynamical systems literature [(Strogatz, 2018)](#b89)). If V B (f ) varies over many orders of magnitude, then it seems reasonable to expect that V SGD (f ) should correlate with V B (f ), as illustrated schematically in Figure [6a](#fig_8). Such general intuitions about landscapes are widely held [(Wales et al., 2003;](#b93)[Massen and Doye, 2007;](#b57)[Ballard et al., 2017)](#b5), and have also been put forward for the particular landscapes of deep learning; see in particular [Wu et al. (2017)](#b99) who also argue that functions with good generalisation have larger basins of attraction.

Another source of intuition follows form a well trodden path linking basic concepts from statistical mechanics to optimisation and learning theory. For example, simple gradient descent (GD) with a small amount of white noise can be described by an over-damped Langevin equation [(Welling and Teh, 2011;](#b95)[Smith and Le, 2017;](#)[Naveh et al., 2020)](#b62) that converges (under some light further conditions) to the Boltzmann distribution The Boltzmann distribution can, in turn, be interpreted as being equivalent to a Bayesian posterior P B (f |S) ∝ e S(f )-βE(f ) [(MacKay, 2003)](#b55) where S(f ) is configurational "entropy" that counts the number of states that generate f and encodes the prior, and E(f ) represents the energy, encoding the log likelihood or loss function. For SGD the equivalent coarse-grained differential equation reduces to Langevin equation with anisotropic noise (Smith and Le, 2017; [Zhang et al., 2018)](#b106)  Cartoon of a biased loss-landscape. The three functions f 1 , f 2 and f 3 all reach zero classification error (dashed red line), but due to bias in the parameter-function map, the "basin size" V B (f 1 ) V B (f 2 ), V B (f 3 ), which typically implies that for the "basins of attraction" Here functions with frequency < 10 are also shown on the plot. The correlation is much less pronounced than for the unrandomised case shown in Figure [1a](#fig_0). Dots on the axes denote functions found by just one of the two methods. Let F be the set of functions found by both the optimiser and under GP sampling. Then f ∈F P B (f |S) = 99.3%, and and doesn't exactly converge to the Bayesian posterior [(Mandt et al., 2017;](#b56)[Brosse et al., 2018)](#b10). Nevertheless, it has been conjectured that with small step size, SGD may approximate the Bayesian posterior [(Naveh et al., 2020;](#b62)[Cohen et al., 2019)](#b14), as we empirically find in our experiments. These connections are rich and worth exploring further in this context. Nevertheless, some caution is needed with these analogies to statistical mechanics because they depend on assumptions which may only to hold on prohibitively long time-scales.

$V SGD (f 1 ) V SGD (f 2 ), V SGD (f 3 ). P B (f |S) is proportional to V B (f$A better analogy may be to the "arrival of the frequent" phenomenon in evolutionary dynamics [(Schaper and Louis, 2014)](#b77), which, like the "basin of attraction" arguments, does not require steady state. Instead it predicts which structures are likely to be reached first by an evolutionary process. For RNA secondary structures, for example, it predicts that a stochastic evolutionary process will reach structures with a probability that to first order is proportional to the likelihood that uniform random sampling of genotypes produces the structure. Indeed, this phenomenon -where the probability upon random sampling predicts the outcomes of a complex search process -can be observed in naturally occurring RNA [(Dingle et al., 2015)](#b19), the result of evolutionary dynamics. This type of non-equilibrium analysis may be more relevant for the way we train most of the DNNs in this paper, since we stop the first time 0 training error is reached. The analogy between these evolutionary results with what we observe for SGD is intriguing, but needs further exploration.

To illustrate the effect of the amount of bias in the posterior, we randomise labels for MNIST and calculate the P B (f |S). As we can see in Figure [6b](#fig_8), this results in a less strongly biased posterior. The mean log-probability log(P B (f |S)) v.s. G curve becomes less steep with increasing corruption For a relatively small fraction of low error functions to dominate, as they do for zero corruptions in Figure [1a](#fig_0), the bias must be strong enough here to overcome the "entropic" factor ρ( G ). For the 20% and 50% corruption this is clearly not the case, and a huge number of functions with larger error will dominate P B (f |S) and P SGD (f |S). As can be seen in Figure [6c](#fig_8), one effect of weaker bias is that the correlation between the optimiser and the Bayesian sampling is much less strong. This behaviour is consistent with the heuristic arguments above, which should only work if the differences in basin volumes are large enough to overcome the myriad other factors that can affect P OPT (f |S).

## Related work on inductive bias on neural networks

In this section we summarise some key aspects of the literature related to why DNNs exhibit good generalisation while overparameterised, expanding on some briefer remarks in Section 1.

## The link between inductive bias and generalisation

Much of the work on inductive biases in stochastic gradient descent (SGD) is framed as a discussion about generalisation. The two concepts are of course intimately related. Before discussing related work on inductive bias DNNs, it may be helpful to distinguish two different questions about generalisation:

1) Question of over-parameterised generalisation: Why do DNNs generalise at all in the overparameterised regime, where classical learning theory doesn't guarantee generalisation?

2) Question of fine-tuned generalisation: Given that vanilla DNNs already generalise reasonably well, how can architecture choice and hyperparameter tuning further improve generalisation?

The first question arises because among the functions that an overparameterised DNN can express, the number that can fit a training data set S, but generalise poorly, is typically many orders of magnitude larger than the number that achieve good generalisation. From classical learning theory we would therefore expect extremely poor generalisation. However, in practice it is often found that many DNN architectures, as long as they are expressive enough to fit the data, generalise sufficiently well to imply a significant inductive bias towards a small fraction of functions that generalise well. This question is also related to the conundrum of why DNNs avoid the "curse of dimensionality", which relates to the poor generalisation that certain highly expressive non-parametric models have in high dimensions [(Donoho et al., 2000)](#b24). [Valle-Pérez et al. (2018)](#b92) argue that the curse of dimensionality is linked to a prior which is not sufficiently biased and that DNNs may avoid this problem by virtue of the strong bias in the prior.

The second question arises from two common experiences in DNN research. Firstly, changes in architecture can lead to important improvements in generalisation. For example, a CNN with max-pooling typically performs better than a vanilla FCN on image data. Secondly, hyperparameter tuning within a fixed architecture can lead to further improvements of generalisation. While these methods of improving generalisation are important in practice, the starting point is normally a DNN that already has enough inductive bias to raise question 1) above. It is therefore important not to conflate the study of question 2) -as vital as this may be to successful practical implementations -with the more general question of why DNNs generalise in the first place.

## Related work on implicit bias in optimiser-trained networks

As mentioned in the introduction, there is an extensive literature on inductive biases in SGD. Much of this literature is empirical: improvements are observed when using particular tuned hyperparameters with variants of SGD. One of the most common rationalisation is in terms of "flatness" which is inspired by early work [(Hochreiter and Schmidhuber, 1997a)](#) who predicted that flatter minima would generalise better. Flatness is often measured using some combination of the eigenvalues of the Hessian matrix for a trained DNN. [(Keskar et al., 2016)](#b42) showed that DNNs trained with small batch SGD generalise better than identical models trained with large batch SGD (by up to 5%), and also found a correlation between small batch size and minima that are less "sharp" (using not the eigenvalues of the Hessian but a more computationally tractable sensitivity measure). While these results are genuinely interesting, they are mainly relevant to issues raised by question 2 above. For example in [(Keskar et al., 2016)](#b42) the authors explicitly point out that their results are not about "overfitting" (e.g. question 1 above).

The effects of changing hyperparameters can be subtle. For example, another series of recent papers [(Goyal et al., 2017;](#b29)[Hoffer et al., 2017;](#b36)[Smith et al., 2017)](#) suggest that better generalisation with small batch SGD may be caused by the fact that the number of optimisation steps per epoch decreases when the batch size increases. These studies showed that a similar improvement in generalisation performance to that found by reducing batch size can be created by increasing the learning rate, or by overtraining (i.e. by continuing to train after 100% accuracy has been reached). In particular, in [(Hoffer et al., 2017)](#b36) it was argued that overtraining does not generally negatively impact generalisation, as naive expectations based on overfitting might suggest. These results also challenge some theoretical studies that suggested that SGD may control the capacity of the models by limiting the number of parameter updates [(Brutzkus et al., 2017)](#b11).

In another interesting paper, [Zhang et al. (2018)](#b106) derive a Langevin type equation for both SGD. And argue that in contrast to GD, the noise is anisotropic, and that this may explain why SGD is more likely to find "flatter minima". Similarly, [Jastrzebski et al. (2018)](#b39) argue that isotropic SGD-induced noise also helps push the optimiser away from sharper minima. An important caveat to the work on sharpness can be found in the work of Dinh et al. [(Dinh et al., 2017)](#b22) who use the non-negative homogeneity of the ReLU activation function to show that for a number of the measures used in the papers cited above, the "flatness" can be made arbitrarily large (or sharp) without changing the function (and therefore the generalisation performance) that the DNN expresses. This result suggests that care must be used when interpreting local measures of flatness. Finally in this vein, generalisation has also been linked to related concepts including low frequency [(Rahaman et al., 2018)](#b71), and to sensitivity to changes in the inputs [(Arpit et al., 2017;](#b3)[Novak et al., 2018a)](#).

There is much more literature on SGD induced inductive bias, but the upshot is that while fine-tuning optimiser hyperparameters can be very important for improving generalisation, and by implication, the inductive bias of a DNN, a complete understanding remains elusive. Moreover, where improvements are found, these tend to be in the class of answers to question 2) above. An important example of a paper on flatness that does explicitly address question 1 above is [(Wu et al., 2017)](#b99), who show that generalisation trends for data with different levels of corruption correlates with the log of the product of the top 50 eigenvalues of the Hessian both for SGD and for GD trained networks. By heuristically linking their local flatness measure to the global basin volume, they make a very similar argument to the one we flesh out in more detail here, namely that the basin of attraction volume of "good" solutions is much larger than that of "bad" solutions that do not generalise well.

Significant theoretical effort has been spent on extracting properties of a trained neural network that could be used to explain generalisation. By implication, these investigations should also help illuminate the nature of the implicit bias of trained networks. For example, investigators have attempted to use sensitivity to perturbations (whether in inputs or weights) to explain the generalisation performance either using a PAC-Bayesian analysis [(Bartlett et al., 2017;](#b6)[Dziugaite and Roy, 2017;](#b26)[Neyshabur et al., 2018)](#), or a compression approach [(Arora et al., 2018;](#b1)[Zhou et al., 2019)](#b107). In contrast to the work described above that studies the specific effect of hyperparameter tuning on SGD, much of the work listed in this paragraph is directly applicable to question 1. A very comprehensive review of this line of work empirically finds that the PAC-Bayesian sensitivity approaches seem the most promising [(Jiang et al., 2019)](#b40), but no clear answer to the question 1 has emerged.

The more theoretical side of the study of SGD has also seen recent progress. For example, [(Soudry et al., 2018)](#b86) showed that SGD finds the max-margin solution in unregularised logistic regression, whilst it was shown in [(Brutzkus et al., 2017)](#b11) that overparameterised DNNs trained with SGD avoid over-fitting on linearly separable data. More recently, [(Allen-Zhu et al., 2019)](#b0) proved agnostic generalisation bounds for SGD-trained DNNs (up to three layers), which impose less restrictive assumptions (on the data, architecture, and optimiser) than previous works. Such theoretical analyses may be a potentially fruitful source of new ideas to explain generalisation.

Another interesting direction is to investigate properties of the loss-landscape itself. Several studies have shown interesting parallels between the loss landscape of DNNs and the energy landscape of spin glasses [(Choromanska et al., 2015;](#b13)[Baity-Jesi et al., 2019;](#b4)[Becker et al., 2020)](#b7). While such insights may help explain why SGD works so well as an optimiser in these high dimensional spaces, it is at present less clear how these studies help explain question 1) above.

A completely different theme builds on the concept of an information bottleneck [(Tishby and Zaslavsky, 2015;](#b91)[Shwartz-Ziv and Tishby, 2017)](#b80) which suggest that generalisation arises from information compression in deeper layers, aided by SGD. However, recent work [(Saxe et al., 2019)](#b76) suggests that the compression is strongly affected by activation functions used, suggesting again that this approach is not general enough to capture the implicit bias needed to answer question 1. We note that the debate about this theme is ongoing.

Finally, it is important to note that simple vanilla gradient descent (GD), when it can be made to converge, does not differ that much (on the scale of question 1 above) from SGD and its variants in generalisation performance [(Keskar et al., 2016;](#b42)[Wu et al., 2017;](#b99)[Zhang et al., 2018;](#b106)[Choi et al., 2019)](#b12). Therefore if training with an optimiser itself generates the inductive bias needed to answer question 1, that bias must already largely be present in simple GD.

## Related work on implicit bias in random neural networks

We briefly review work inspired by a powerful result from algorithmic information theory (AIT) called the coding theorem [(Li and Vitanyi, 2008)](#b51). First derived by Levin [(Levin, 1974)](#b50), and building on concepts pioneered by Solomonoff [(Solomonoff, 1964)](#b85), it is closely related to more recent bound applicable to a wider range of input-output maps [(Dingle et al., 2018](#b20)[(Dingle et al., , 2020))](#b21). This bound predicts (under certain fairly general conditions that the maps must fulfil) that upon randomly sampling the parameters of an input-output map M , the probability P (f ) of obtaining output f can be bounded as

$P (f ) ≤ 2 -K(f |M )+O(1) ≈ 2 -a K(f )+b (4)$where K(f ) is the Kolmogorov complexity of f , the O(1) terms do not depend on the outputs (at least asymptotically), K(f ) is a suitable approximation to K(f ) and a and b are parameters that depend on the map, but not on f . The computable bound was empirically shown to work remarkably well for a wide range of input-output maps from across science and engineering [(Dingle et al., 2018)](#b20), giving confidence that it should be widely applicable, at least for maps that satisfy the conditions needed for it to apply. In addition, a statistical lower-bound can be derived that predicts that most of the probability weight will lie relatively close to the bound [(Dingle et al., 2020)](#b21). The application of this bound to DNNs was first shown in [(Valle-Pérez et al., 2018)](#b92). We note that the input-output map of interest is not the map from inputs to DNN outputs, but rather the map from the network parameters to the function f it produces on inputs X which was described in Definition 2.1. The prediction of Equation ( [4](#)) for a DNN with parameters sampled randomly (from, for example, truncated i.i.d. Gaussians) is that, if the parameter-function map is sufficiently biased, then the probability of the DNN producing a function f on input data x n i=0 drops exponentially with increasing complexity of the function f . Note that technically we should write f as f |X to indicate the dependence of the function modelled by the DNN on the inputs X . We also note that the AIT bound of Equation ( [4](#)) on its own does not force a map to be biased. It still holds for a uniform distribution. But if the map is biased, then it will be biased according to Equation (4).

In [(Valle-Pérez et al., 2018)](#b92) it was shown empirically that this very general prediction of Equation ( [4](#)) holds for the P (f ) of a number of different DNNs. This testing was achieved both via direct sampling of the parameters of a small DNN on Boolean inputs and with NNGP calculations for more complex systems. In a complementary approach [(Mingard et al., 2019](#b61)) some exact results were proven for simplified networks, that are also consistent with the bound of Equation ( [4](#)). In particular, they proved that for a perceptron with no bias term, upon randomly sampling the parameters (with a distribution satisfying certain weak assumptions), any value of class-imbalance was equally likely. There are many fewer functions with high class imbalance (low "entropy") than low class imbalance. Low entropy implies low K(f ) (but not the other way around). Thus, these results imply a bias of P (f ) towards certain simple functions. They also proved that for infinite-width ReLU DNNs, this bias becomes monotonically stronger as the number of layers grows. A different direction was pursued in [(De Palma et al., 2018)](#b18), who showed that, upon randomly sampling the parameters of a ReLU DNN acting on Boolean inputs, the functions obtained had an average sensitivity to inputs which is much lower than if randomly sampling functions. Functions with low input sensitivity are also simple, thus proving another manifestation of simplicity bias present in these systems.

On the other hand, in a recent paper [(Yang and Salman, 2019)](#b103), it was shown that for DNNs with activation functions such as Erf and Tanh, the bias starts to disappear as the system enters the "chaotic regime", which happens for weight variances above a certain threshold, as the depth grows [(Poole et al., 2016)](#b70) (note that ReLU networks don't have such a chaotic regime). While these hyperparameters are not typically used for DNNs, they do show that there exist regimes where there is no simplicity bias. Note that the AIT coding theorem bound Equation (4) still holds, but P (f ) is simply approaching a uniform distribution, and the bound becomes loose for small complexity. These results are also interesting because, if the bias becomes weaker, then it may also be the case that the correlation between P B (f |S) and P SGD (f |S) starts to disappear, an effect we are currently investigating.

## Related work comparing optimiser-trained and Bayesian neural networks

Another set of investigations studying random neural networks use important recent extensions of Neal's seminal proof [(Neal, 1994](#b63)[(Neal, , 2012) )](#b64) -that a single-layer DNN with random i.i.d. weights is equivalent to a Gaussian process (GP) [(Mackay, 1998)](#b54) in the infinite width limit -to multiple layers and architectures [(Lee et al., 2017;](#b47)[Matthews et al., 2018;](#)[Novak et al., 2018b;](#b67)[Garriga-Alonso et al., 2019;](#b27)[Yang, 2019b)](#). These studies have used this correspondence to effectively perform a very good approximation to exact Bayesian inference in DNNs. When they have compared them to SGD-trained DNNs [(Lee et al., 2017;](#b47)[Matthews et al., 2018;](#)[Novak et al., 2018b)](#b67), the results have generally shown a close agreement between the generalisation performance of optimiser-trained DNNs and their corresponding Bayesian neural network Gaussian process (NNGP).

In this context another significant development is the introduction of the neural tangent kernel (NTK) [(Jacot et al., 2018)](#b38) which approximates the dynamics of an infinite width DNN with parameters that are trained by gradient descent in the limit of an infinitesimal learning rate. Recent comparisons to NNGPs show relatively similar performance of the NTK, see for example [(Arora et al., 2019;](#b2)[Lee et al., 2019;](#b49)[Novak et al., 2020)](#b68). While there are small performance differences, the overall agreement between NNGPs and the NTK or optimiser trained DNNs is close enough to suggest that the primary source of inductive bias needed for question 1 above is already present in the untrained network, and is essentially maintained under training dynamics.

The linearisation of DNNs offered by NTK can also be used to prove that, in this regime, GD samples from the Bayesian posterior in a sample-then-optimise fashion. For linear regression models, [Matthews et al. (2017)](#b58) showed that solutions after training GD with a Gaussian initialisation correspond to exact posterior samples. This idea is also related to Deep Ensembles which has been proposed to be "approximately Bayesian" in [Wilson and Izmailov (2020)](#b97).

In this context, further indirect evidence comes from Valle-Pérez et al. ( [2018](#)) who used a simple PAC-Bayesian bound [(McAllester, 1999](#b60)) that applies to exact Bayesian inference, to predict the generalisation error of SGD-trained DNNs. The bound was shown to provide relatively tight predictions for optimiser-trained DNNs for an FCN and CNNs on MNIST, Fashion-MNIST and CIFAR-10. Moreover, this bound, which takes the Bayesian marginal likelihood as input, reproduced trends such as the increase in the generalisation error upon an increased fraction of randomised labels.

These lines of work serve as independent evidence to suggest that optimiser-trained DNNs behave very similarly to the same DNNs trained with Bayesian inference, and helped inspire the work in this paper, where we directly tackle this question. These studies also suggest that the infinite-width limit may be enough to answer question 1, as the number of parameters in a DNN typically doesn't have a drastic effect on generalisation (as long as the network is expressive enough to fit the data).

## Related work on complexity of data, simplicity bias and generalisation

In Section 7.3, we discussed work showing that DNNs may have an inductive bias towards simple functions in their parameter-function map. Here, we briefly discuss how this "simplicity bias" concept may connect to generalisation. As implied by the no free lunch theorem [(Wolpert and Waters, 1994)](#b98), a bias towards simplicity does not automatically imply good generalisation. Instead certain key hypotheses about the data are needed, in particular that it is described by functions that are simple (in a similar sense to the inductive bias). Now the assumption that a more parsimonious hypothesis is more likely to be true has been influential since antiquity and is often articulated by invoking Occam's razor. However, the fundamental justification for this heuristic is disputed, see e.g. [(Sober, 2015)](#b84) for an overview of the philosophical literature, e.g. [(MacKay, 1992;](#b53)[Blumer et al., 1987;](#b8)[Rasmussen and Ghahramani, 2001;](#b73)[Domingos, 1999)](#b23) for a set of different perspectives from the machine learning literature, and e.g. [(Rathmanner and Hutter, 2011;](#b74)[Sterkenburg, 2016)](#b88) for a spirited discussion of the links between the razor and concepts from AIT (pioneered in particular by Solomonoff).

Studies which imply that data typically studied with DNNs is somehow "simple" include an influential paper [(Lin et al., 2017)](#b52) invoking arguments, mainly from statistical mechanics, to argue that deep learning works well because the laws of physics typically select for function classes that are "mathematically simple", and so easy to learn. More direct studies have also demonstrated certain types of simplicity. For example, following on previous work in this vein, [(Spigler et al., 2019](#b87)) calculated an effective dimension d ef f ≈ 15 for MNIST, which is much lower than the 28 2 = 784 dimensional manifold in which the data is embedded. Individual numbers can have effective dimensions that are even lower, ranging from 7 to 13 [(Hein and Audibert, 2005)](#b32). So the functions that fit MNIST data are much simpler than those that fit random data [(Goldt et al., 2019](#b28)). An implicit bias towards simplicity may therefore improve generalisation for structured data, but it will likely have the opposite effect for more random data.

## Discussion

We argue here that the inductive bias found in DNNs trained by SGD or related optimisers, is, to first order, determined by the parameter-function map of an untrained DNN. While on a log scale we find P SGD (f |S) ≈ P B (f |S) there are also measurable second order deviations that are sensitive to hyperparameter tuning and optimiser choice.

For the conundrum of why DNNs generalise at all in the overparameterised regime, our results strongly suggest that the solution must be found in the properties of P B (f |S), and not in further biases introduced by SGD. Arguments that DNN priors are exponentially biased towards simple functions [(Valle-Pérez et al., 2018;](#b92)[Mingard et al., 2019;](#b61)[De Palma et al., 2018)](#b18) may help explain the inductive bias of P B (f |S), but more work needs to be done to explore the complex interplay between bias in the prior, the data, and generalisation. While they may not explain the fundamental conundrum above, second order deviations from P B (f |S) are important in practice for further fine-tuning the generalisation performance.

Our function probability perspective also provides more fine-grained tools for the analysis of DNNs than simply comparing the average test error. This picture can facilitate the investigation of hyperparameter changes, or potentially also the study of techniques such as batch normalisation or dropout. It could assist in the design of new architectures or optimisers.

It is not obvious how to determine the uncertainty in a prediction of a DNN model. However, if, as we argue here, SGD behaves like a Bayesian sampler, then this offers additional justification for using Deep Ensembles to measure this uncertainty in the case of DNNs (Wilson and Izmailov, 2020). Our results could therefore make it easier to use neural networks in applications where it is important to be able to quantify prediction uncertainty Most of our examples are for image classification. It would be interesting to study the related problem of using DNNs for regression. Sampling considerations means that it is easier to study P SGD (f |S) for smaller generalisation errors. It would be interesting to study systems with intrinsically larger G within this picture as well. There the biasing effect of the optimiser may be larger.

Finally, to study the correlation between P B (f |S) and P SGD (f |S), we mainly used a fixed test and training set. While we did examine other test and training sets (see Appendices), this was mainly to confirm that our results were not an artefact of our particular choices. A promising future direction would be a Bayesian approach that includes averaging over training sets.

We typically run between n = 10 4 and n = 10 7 runs (depending on the system). Once the runs are finished, we compile all the empirical frequencies for the functions that are found.

A.1.2 Bayesian sampling for P B (f |S)

We use the GP approximation to estimate the Bayesian posterior P B (f |S). Here we follow [(Valle-Pérez et al., 2018)](#b92) where this technique is explained (see also Appendix A.2 for more details). We need to define a distribution P par (θ) (the definition of the prior from which we calculate P B (f |S), see Equation ( [1](#))). While there are some subtleties in how the prior distribution P par (θ) relates to the initialisation distribution Ppar (θ), we took a simple approach and defined P par to be the same as the corresponding Ppar (θ), except we set σ b to be a small constant, typically 0.1 × σ w .

To estimate P B (f |S), there is a small compromise that must be made here. For a number of reasons, MSE loss is less popular for the kinds of classification problems we mainly study in this paper. We also find that it typically takes significantly longer to train using an optimiser so that P OPT (f |S) is more expensive to evaluate for MSE loss on the problems we study. On the other hand, P B (f |S) can be directly sampled n times from the exact posterior (described in Appendix A.2.1) using Algorithm 2), and so is relatively accurate and simple to evaluate.

For CE loss, which is more frequently used for classification, and is also typically quicker to train than MSE for SGD and its variants, we need to use a further approximation. Here we follow [(Valle-Pérez et al., 2018)](#b92) and use the expectation-propagation (EP) approximation for P B (f |S). We then estimate the posterior log probabilities using the estimations of the log marginal likelihoods log P (S) (see Appendix A.2.2 for explanation) in Algorithm 3, or we sample from the approximate EP posterior and use Algorithm 2. These two methods give very similar answers (Figure [7c](#fig_12)).

Algorithm 2 Calculating P B (f |S) (via sampling) input: DNN N , training data S, test data E. F ← ∅ {functions sampled from the GP or GP/EP posterior} do n times: sample a function f from the GP or GP/EP posterior when conditioning on S find f on E save f to F R ← ∅ {function probabilities} for each distinct f ∈ F do let ρ f be the frequency of f in F calculate P B (f |S) = ρ f /n save P B (f |S) to R end for return R Algorithm 3 Calculating P B (f |S) for specific f via the ratio of likelihoods approximation input: DNN N , training data S, test data E, optimiser OP T , set of functions F (from Algorithm 1) for each distinct f ∈ F do let ρ f be the frequency of f in F use GP or GP/EP approximation to estimate P B (f |S) of f using the ratio of likelihoods approximation. save P B (f |S) to A end for return A A.1.3 Calculating P B (f |S) for functions with a wider range of G Given a training dataset S and test dataset E we can generate a random sample of different partial functions with varying levels of error on E (by taking the test set classification

and corrupting some percentage of labels). We can then use the GP/EP approximation to estimate P B (f |S). We typically sample 20 examples for each number of errors. The averages are taken on the logs of the probabilities, and error bars on plots are 2σ, where σ is the standard deviation. Note that the vast majority of functions have such small probabilities, that it is not feasible to estimate their P OPT (f |S) (nor use Algorithm 2)

While the experiments will be informative for how the space is biased, it does not guarantee that all high-probability functions will be found. Since these functions affect generalisation the most, we rely on the results from Algorithm 2 to check that there are no high-probability functions that are missed. 

$B (f |S) of f to V end for return V 0.0 . . . V 1.0$We here make a few more remarks for interpreting the results in experiments where we generate functions for a wide range of errors, such as those in Figure [1b](#fig_0). Firstly, as shown in Figure [8a](#fig_14) in Appendix B, there can be a wide variation in the probabilities p i of misclassifying each of the 100 images in this test set. The generalisation error is therefore dominated by a small number of harder to classify images. This means that the probabilities of functions for a fixed G can vary a lot. This explains why we find that even though the highest probability function in Figure [1a](#fig_0) is a 1-error function, on average the probability for 1-error functions in Figure [1b](#fig_0) is lower than that of the 0-error function. The high variance in P B (f |S) within the functions of fixed G , as well as the EP approximation also means that the estimates of P B (f |S) G may be less accurate. For Figure [1b](#fig_0), ρ( ) P B (f |S) G ≈ 0.1, which is not far off the correct value of 1. Keeping in mind that we may be missing some higher probability outliers in the average due to finite sampling, this agreement is encouraging. In short, although the quantitative values may not be fully accurate, the results in Figure [1b](#fig_0) are indicative of the strong exponential trend towards low probability with increasing error.

A third point of clarification is that in Figure [1b](#fig_0) we used CE loss rather than MSE loss. We made this choice because we need to estimate very small P B (f |S) values, for which we need to use the "ratio of likelihoods" approximation. While we only described how to use EP for CE loss, we could also use the EP approximation to estimate P B (f |S) for the analytical posterior of the GP with MSE loss. However, from some preliminary tests, the EP approximation introduced significant systematic errors and in particular it didn't show better results than the EP/"ratio of likelihoods" method for CE loss. In Appendix A.2.3, we can also see that the differences between P B (f |S) for MSE and CE loss are probably of less than a few orders of magnitude, and would not significantly affect the results in Figure [1b](#fig_0).

## A.1.4 Further notes on methods

When P B (f |S) and P OPT (f |S) are obtained by sampling, we typically sample between 10 5 and 10 7 times. To avoid finite sampling effects, we place any functions found with frequencies < 10 on the axes of our graphs. However, those functions are included in calculations involving generalisation errors, although typically they contribute very little because they are by definition low probability.

We will also regularly provide values for f ∈F P B (f |S) and f ∈F P OPT (f |S) in our figures comparing P OPT (f |S) with P B (f |S) (only when P B (f |S) is obtained by direct sampling), where F is the set of functions found by both GP sampling and by the optimiser (within a finite number of samples from the GP and optimiser-trained DNNs, typically 10 4 -10 6 ). A value of P B (f |S) close to 1 indicates that almost all functions with high P B (f |S) are found by the optimiser. Similarly, a high value of P OPT (f |S) implies that GP sampling finds almost all functions with high P OPT (f |S) found by the optimiser. Note that for the EP approximation needed for CE loss, we directly calculate the P B (f |S) for functions found by the optimiser, and so a P OPT (f |S) is not defined. Finally, when values for the generalisation error G are depicted in the graphs showing P B (f |S) v.s. P OPT (f |S), G refers to the generalisation error of the optimiser. When G is presented in graphs with only P B (f |S) it refers to the G from GP sampling, or alternatively we use the symbol G GP .

## A.2 Description of Gaussian process calculations

In this section, we provide more details on the Gaussian process (GP) methods used in the paper. We follow the general Bayesian formalism introduced in Section 2.

## A.2.1 GP with Mean Squared-Error (MSE) loss

For this formulation, we consider the space of functions to be the space of real-valued functions on X . These functions correspond to the real-valued pre-activations of the last layer of the neural network, before a final non-linearity (like softmax or a step function) is applied. In the GP limit of DNNs, these functions have a GP prior (the NNGP). We then use a Gaussian likelihood defined as

$P (S|f ) = m i=1 1 √ 2πσ 2 exp 1 2σ 2 (f (x i ) -y i ) 2 ,(5)$where σ 2 is the variance. This likelihood allows us to analytically compute the exact posterior [(Rasmussen, 2004](#b72)). In the experiments in the paper we therefore sampled from this exact posterior, to get values of f (x) ∈ R at the test points, which were then thresholded at 0 to find the predicted class label. We have chosen a small value of the variance σ 2 = 0.002, to simulate SGD achieving a small value of the MSE loss.

Note that under the standard assumption that training and test instances come from the same distribution, this algorithm may be considered to be not fully Bayesian in the sense that the training and test labels are treated differently (Gaussian likelihood in training points versus Bernoulli likelihood at test points). Nevertheless, we believe that the differences are small.

## A.2.2 GP with 0-1 likelihood, EP, and ratio of likelihoods approximation

While MSE loss has the advantage that the GP is analytically tractable, it has the disadvantage that it is less principled and less commonly used for classification than the much more popular cross-entropy (CE) loss function. Unfortunately the GP approximation to CE loss is more complex. Here our formulation uses a Gaussian process prior over a space of real-valued "latent" functions f on X . We call these functions "latent" because the true space of functions we are interested in is the space of Boolean functions on X , obtained after applying the threshold nonlinearity to the last layer pre-activations. Formally, f is related to f via a Bernoulli distribution with Heaviside linking function, as

$P (f (x) = 1) = 1 if f (x) > 0 0 otherwise .$We then use the previously defined 0-1 likelihood (Section 2.2) applied to the a binary-valued function f (taking values in {0, 1}). We use this likelihood, arguing that it best approximates the behaviour of an optimiser which is trained (using cross-entropy loss) until it first reaches 0 training error, which is the case in all the experiments in the paper (except those labelled as "overtraining"). Because of this we informally refer to this method as "using the CE loss" throughout the paper. Unfortunately, this likelihood makes the posterior of the GP analytically intractable. We therefore use a standard approximation technique known as expectation propagation (EP) [(Rasmussen, 2004)](#b72), which approximates the posterior over the latent function as a Gaussian, which we can sample from and then use the Heaviside function to predict the binary labels at the test points. We use this technique to approximate posterior probabilities of the GP with 0-1 likelihood, by sampling. The marginal likelihood can also be estimated with the EP algorithm [(Rasmussen, 2004)](#b72). This gives the probability of a labelling of a set of points. Remember that in the Bayesian formalism S is essentially defined as the event that the input points x i in the training set have labels y i . We can similarly identify a function f with the event that the set of input points x in the whole domain X have labels f (x), which is analogous, and can thus be computed in the same manner as the marginal likelihood! The posterior in Equation (2) for a function f which is compatible with S, can then be simply expressed as

$P (f |S) = P (f ) P (S) ,$where both P (f ) and P (S) are readily computed using the EP algorithm to approximate marginal likelihood. This will be referred to as the "ratio of likelihoods" approximation in the appendices. We have found that this method gives very similar results to the estimates using sampling from the approximate posterior obtained from EP (see Figure [7c](#fig_12)). When we refer to the EP approximation, we imply that we are using the ratio of P (f ) and P (S), each determined using the EP approximation, unless stated otherwise.

For the LSTM experiments, we used a smooth version of the 0-1 likelihood, analogous to using standard cross-entropy loss rather than miss-classification error. This was because the EP approximation was numerically unstable with the 0-1 likelihood for this system. The smooth version is described in Appendix A.2.3, and we empirically found that the two gave very similar estimates of probability.

## A.2.3 Empirical results concerning the GP approximations

In this section we compare the behaviour of the GP approximations (and SGD) with different loss functions. We have argued that the GP/EP approximation underestimates probabilities by a power law (that is approximately linear in log-log; see [(Valle-Pérez et al., 2018)](#b92) for more details). Our results with the EP approximation are consistent with this expectation. As detailed in Appendices A.2.1 and A.2.2, there are subtle differences in the way the GP approximation with MSE loss and the GP/EP approximation with CE loss calculate their respective estimates for P B (f |S). However, the (latent) function has the same prior in both cases, so we may expect the posterior P B (f |S) to correlate. And it is clear from Figure [7a](#fig_12) that they do indeed correlate. We believe that the correlation not being centred around y = x is predominately due to the EP approximation because apart from scatter, the behaviour of SGD with the two loss functions is centred around y = x. In Appendix F, we also compare the EP and MSE approximations with a estimation via direct sampling (and thus with controlled error) of the posterior probabilities for 0-1 likelihood for small Boolean function datasets, where these computations are feasible. We indeed find that EP tends to underestimate posterior probabilities, specially for complex target functions. Overall what we find is that the EP approximation does reasonably well on relative probabilities, but less well on absolute probabilities.

To mitigate the effect that the EP approximation underestimates the probabilities, we perform a simple empirical regularisation. For systems where we find that P B (f |S) ≈ 1 for the MSE approximation, we renormalise the P B (f |S) from the EP approximation by a constant factor such that P B (f |S) = 1. For most systems we study the effect of this regularisation procedure is relatively small on a log scale. This method facilitates the comparison with P SGD (f |S) because the errors in the absolute values are regularised in the same way for all systems. Note also that because we sample to obtain P SGD (f |S) its empirical frequencies automatically sum to 1. If it were the case that the Bayesian sampling found a significant number of different high probability functions, then this would be observed in a lack of correlation in the comparison with P SGD (f |S). Instead, we find a strong correlation between P SGD (f |S) and P B (f |S) calculated with the EP approximation, suggesting that the functions found are the dominant ones found by both methods, as is explicitly found to be the case for most instances of MSE loss that we studied. This regularisation is applied to all experiments (for ease of comparison), unless otherwise specified.  This implies that the loss function does not substantially affect P SGD (f |S) on average. Note that in (a) the two methods correlate, but that 1) the GP-EP is systematically lower than the MSE, and 2) that the slope is below x = y. These two trends are, we believe, more general for the GP-EP approximation on CE loss. (c) Here we compare the GP/EP log(p) approximation with GP/EP sampling. For this figure, we use only functions found by Adam in 10 6 samples, and compare probabilities found by the GP/EP log(p) approximation to those found by GP/EP sampling. We use both methods in this paper, but as is clear from the above figure, there is not much difference between them for functions with high P B (f |S).

Specifically, for Figure [9](#fig_16) the renormalisation constant was calculated for Adam without overtraining and batch size 128 (as it had the highest raw value for P B (f |S)), and the probabilities in the other plots with FCN on MNIST are all adjusted by the multiplicative constant of 3.59, which is modest on the full log scale of the graphs. For other plots, the probabilities were normalised. The two systems for which this renormalisation has a larger effect are the LSTM and the ionosphere dataset. While for both systems the MSE sampling looks relatively close to y = x, the raw EP approximation has significantly lower probabilities. The renormalisation factors were 1.15 × 10 5 and 97 respectively. Smaller MSE experiments verify that there is a strong correlation between P SGD (f |S) and P B (f |S) for these systems.  which indicates an exponential like drop-off for ≥ G (similar to what is observed in Figure [1a](#fig_0)). Of course this is an upper bound and the actual distribution can strongly depend on the full spectrum of p i values. We are currently exploring these issues in more detail.

## Appendix C. Effects of training set size

It is also instructive to study the correlation between P B (f |S) and P OPT (f |S) for different training set sizes |S|. As can be seen clearly in Figure [9](#fig_16), as the training set increases in size, the functions with zero training error are more strongly biased towards low generalisation error, as expected. Figures 9c and 9f also illustrate how the stronger bias with increasing |S| means that the entropic factor ρ( G ) plays a smaller role. Thus, for larger training set size, but for the same amount of sampling n, fewer functions are found, but on average they have higher probability. An important question in deep learning is: How does the error reduce with increasing the training set size? There is intriguing evidence that such "learning curves" follow a power law that depends on data complexity, and only weakly on the architecture [(Hestness et al., 2017;](#b33)[Spigler et al., 2019;](#b87)[Rosenfeld et al., 2019;](#b75)[Kaplan et al., 2020)](#b41). Figure [9](#fig_16) shows how the spectrum of function probabilities changes with increasing |S|. Investigations based on this more fine-grained picture may help improve our understanding of learning curves.  Much research effort has gone into adaptations of SGD. One goal is to achieve more efficient optimisation, but another is to achieve better generalisation. Figure [11](#fig_0) illustrates the effect of changing the optimiser on the correlation between P B (f |S) and P OPT (f |S). (See also Figures 1f, 5b and 16 to complete the set of optimisers with and without overtraining). To first order this figure shows that P B (f |S) and P OPT (f |S) are remarkably closely correlated for all these optimisers, even taking into account that the EP approximation introduces errors, and likely leads to a slightly too small slope in P B (f |S) v.s. P OPT (f |S).

What is perhaps more interesting here are second-order effects, since P B (f |S) is identical in each plot. For example, RMSprop has the best generalisation performance, which is reflected in a stronger inductive bias towards a few key low error functions. Note also the similarity of batch size 128 RMSprop to Adam with smaller batch size of 32. We emphasise that this performance here doesn't mean that RMSprop is in general superior to the other SGD variants for FCNs on MNIST. To investigate that question, we would need to study other test and training sets, and need to do further hyperparameter tuning [(Choi et al., 2019)](#b12).

We can also compare the effect of overtraining. In each case shown in Figures 1f, 5b and 16, overtraining brings a modest improvement in generalisation error on this test set (Adam from G = 2.2% to G = 1.74% and Adagrad from G = 2.63% to G = 2.19%), for example. From the graphs one can see a slight increase in the optimiser probability of the lowest error function with overtraining, but also a clear reduction in the scatter of the data for the whole range of probabilities, suggesting that for CE loss, on average, overtraining brings P OPT (f |S) closer to the Bayesian prediction. This behaviour can possibly be rationalised in that overtraining allows the optimiser to sample functions with probabilities closer to the steady-state average (see also Section 6).

Finally, in Figures 11e, 11f and 17 we directly compare the P OPT (f |S) to one another, in other words, without using P B (f |S). A number of clear trends are visible, for example, with batch size 128, Adam and Adagrad are very similar to one another, as are RMSprop and Adadelta. However, as can be seen in Figure [11f](#fig_0), Adam with a smaller batch size of 32 is very similar to RMSprop with batch size of 128. What these correlation plots show is that the behaviour of the different optimisers can depend on batch size in subtle ways that may not necessarily be picked up by the generalisation error. Further work (and significant computational resources) would be needed to completely compare these methods.

These examples show that studying the spectrum of function probabilities provides more fine-grained data than simply comparing generalisation error does. Future studies on problems such as optimiser choice or hyperparameter tuning could exploit this fuller set of information to increase understanding and to improve DNN performance.   Adagrad and Adam correlate very well as do Adadelta and RMSprop for these hyperparameters. The other combinations do not correlate as well, suggesting that they sample the loss-function differently from one another. These plots do not rely on the GP or GP/EP approximation. We believe that this sort of experiment may prove useful for understanding differences in the behaviour of the optimisers.  

![Figure 1: Comparing the Bayesian prediction P B (f |S) to P OPT (f |S) for SGD and Adagrad, for an FCN on MNIST [We use training/test set size of 10,000/100; For (a,e,f), the vertical dotted blue lines are drawn at the highest value of P OPT (f |S) such that the sum of P OPT (f |S) for all functions above the line is > 90% (90% probability boundary); dashed grey line denotes P B (f |S) = P OPT (f |S).] (a) P B (f |S) v.s. P SGD (f |S) for MSE loss; Both P B (f |S) and P SGD (f |S) were sampled n = 10 6 times. The color shows the number of errors in the test set. The GP has average error G GP = 1.61%, while SGD has average error G = 1.88%. (b) P B (f |S) (with CE loss) v.s. G for the full range of possible errors on E. We use the methods from Section 3.1.5 with 20 random functions sampled per value of error. The solid blue line shows log(P B (f |S)) G , where the average is over the functions for a fixed G ; error bars are 2 standard deviations. The dashed blue line shows the weighted ρ( G ) P B (f |S) G , where ρ( G ) is the number of functions with error G . The small red box and dashed red lines illustrate the range of probability and error found in (a). (c) CSR complexity versus generalisation error for the same functions as in fig (b). Color represents P B (f |S), computed as in (b). (d) Functions from (a) found by the sample of P B (f |S), versus error. 913 functions of the functions are also found by SGD, taking up 97.70% of the probability for P SGD (f |S), and 99.96% for P B (f |S).(e) P B (f |S) v.s. P Adagrad (f |S) for MSE loss; P Adagrad (f |S) was sampled n = 10 5 times (while the GP sample was the same as in (a)). Adagrad was overtrained until 64 epochs had passed with zero error. The average error is G = 1.53%. (f) is as (e) but with CE loss, so that the EP approximation was used for P B (f |S), making the estimate of P B (f |S) slightly less accurate. G = 2.63%. 10]()

![Figure 2: Comparing P B (f |S) to P Adam (f |S) for CNNs and the FCN on Fashion-MNIST [We use a training/test set size of 10,000/100; vertical dotted blue lines denote 90% probability boundary; dashed grey line is P B (f |S) = P OPT (f |S).] (a) FCN on Fashion-MNIST; G = 2.11% for Adam with CE loss. (b) Vanilla CNN on Fashion-MNIST; G = 2.25% for Adam with CE loss. (c) CNN with max-pooling on Fashion-MNIST; G = 1.96% for Adam with CE loss. Note that when max-pooling is added, the probability of the lowest-error function increases notably for both P Adam (f |S) and P B (f |S). There is a strong correlation between P B (f |S) and P SGD (f |S) in all three plots. See Figure 13 for related results, including P B (f |S) vs G , a CNN with batch normalisation, and a CNN with MSE loss.]()

![Comparing P B (f |S) and P SGD (f |S) to Neural Tangent Kernel results]()

![(a) P B (f |S) v.s. P NTK (f |S) (b) P SGD (f |S) v.s. P NTK (f |S)]()

![Figure 3: Comparing P B (f |S) and P SGD (f |S) to P NTK (f |S) for an FCN on MNIST.[The functions to the right of the blue dotted lines make up 90% of the total probability. We did 10 7 samples for NTK and GP, and 10 6 for SGD]. In (a) we show the correlation between P N T K (f |D) and P B (f |S). Weighted by probability, 77.5% of functions found by sampling from the GP are found by NTK; all functions found by NTK are found by sampling from the GP. In (b), we show the correlation between the P N T K (f |D) and P SGD (f |S). Weighted by probability, 65.8% of functions found by SGD are found by NTK; all functions found by NTK are found by SGD. G = 1.69% (NTK), G = 1.61% (GP), G = 1.88% (SGD).]()

![Comparing P B (f |S) to P Adam (f |S) for LSTM on IMDb sentiment analysis]()

![Figure 4: Comparing P B (f |S) to P Adam (f |S) for a LSTM on the IMDb movie review dataset, and an FCN on the ionosphere dataset. (a) P B (f |S) v.s. P Adam (f |S) for LSTM on IMDb dataset, ( G =4.28%, 10 4 samples). Because of the computational cost of the problem, we used a training set size of 45000 and a test set of size 50. (b) P B (f |S) v.s. G for the LSTM on IMDb shows that the functions found by the Adam optimiser are in the small fraction of high P B (f |S) probability/low error functions. (c) P B (f |S) v.s. P Adam (f |S) for an FCN with 3 hidden layers of width 256 on the Ionosphere dataset. Training set size is 301 and the test set size is 50. ( G = 4.59% for Adam, G = 5.41% for the GP). See Figures 14 and 15 for further results for these systems.]()

![Figure 5: Effects of changing batch size and learning rate on P B (f |S) and P Adam (f |S) for FCN on MNIST with CE loss [We use training/test set size 10,000/100. Vertical dotted blue lines denote 90% probability boundary; dashed grey line is x = y.] (a) Batch size = 32, G = 1.13%. (b) Batch size= 128, G = 2.20%. (c) Batch size = 512, G = 2.67%. (d) Batch size =512 and faster learning rate (4x the others), G = 2.14%. (e) Direct comparison of P Adam (f |S) for batch size 128 and 512. (f) Direct comparison of P Adam (f |S) for batch size 128 and 512 with a 4× faster learning rate. The P Adam (f |S) probabilities for the dominant functions in (d) and (b) are remarkably similar, as can be seen by comparing (e) and (f). It is these higher probability functions that explain the similarity in G for batch size 128 and batch size 512 with a faster learning rate. See Figure 18 for related batch size results for MSE loss.]()

![Figure 6: Schematic landscape and effects of randomising training labels. (a)Cartoon of a biased loss-landscape. The three functions f 1 , f 2 and f 3 all reach zero classification error (dashed red line), but due to bias in the parameter-function map, the "basin size" V B (f 1 )V B (f 2 ), V B (f 3 ), which typically implies that for the "basins of attraction"V SGD (f 1 ) V SGD (f 2 ), V SGD (f 3 ). P B (f |S) is proportional to V B (f ), and P SGD (f |S) is proportional to V SGD (f ). (b) P B (f |S) (solid) and ρ( G )P B (f |S) (dashed) v.s. G , for test set of size 100 and CE loss (as in Figure 1b) but including label corruption c. (b) P SGD (f |S) v.s. P B (f |S) on MNIST with a 2-layer 1024 node wide FCN with MSE loss, test set size 50, and 20% of the training labels randomised ( G SGD = 13.4% and G GP = 5.80%).Here functions with frequency < 10 are also shown on the plot. The correlation is much less pronounced than for the unrandomised case shown in Figure1a. Dots on the axes denote functions found by just one of the two methods. Let F be the set of functions found by both the optimiser and under GP sampling. Then f ∈F P B (f |S) = 99.3%, and]()

![Figure 6: Schematic landscape and effects of randomising training labels. (a)Cartoon of a biased loss-landscape. The three functions f 1 , f 2 and f 3 all reach zero classification error (dashed red line), but due to bias in the parameter-function map, the "basin size" V B (f 1 )V B (f 2 ), V B (f 3 ), which typically implies that for the "basins of attraction"V SGD (f 1 ) V SGD (f 2 ), V SGD (f 3 ). P B (f |S) is proportional to V B (f ), and P SGD (f |S) is proportional to V SGD (f ). (b) P B (f |S) (solid) and ρ( G )P B (f |S) (dashed) v.s. G , for test set of size 100 and CE loss (as in Figure 1b) but including label corruption c. (b) P SGD (f |S) v.s. P B (f |S) on MNIST with a 2-layer 1024 node wide FCN with MSE loss, test set size 50, and 20% of the training labels randomised ( G SGD = 13.4% and G GP = 5.80%).Here functions with frequency < 10 are also shown on the plot. The correlation is much less pronounced than for the unrandomised case shown in Figure1a. Dots on the axes denote functions found by just one of the two methods. Let F be the set of functions found by both the optimiser and under GP sampling. Then f ∈F P B (f |S) = 99.3%, and]()

![Calculating P B (f |S) for larger range of G input: DNN N , training data S, test data E. for ∈ {0.0, 0.5, . . . 1.0} do V ← generate classification c with error on E (by randomly choosing |E| × distinct labels in the correct function (restricted to the test set) to switch to incorrect). use GP/EP and "ratio of likelihoods" approximation to estimate the P B (f |S) of c save the relative volume P]()

![(a) P B (f |S)(CE/EP) versus P B (f |S) (MSE) (b) P SGD (f |S)(CE) versus P SGD (f |S) (MSE) (c) P B (f |S)(CE/EP) sampled v.s. ratio of likelihoods estimate]()

![Figure 7: Comparing GP approximations with CE and MSE loss for FCN on MNIST In (a) we compare the behaviour of the GP approximations for P B (f |S) with MSE loss to the GP/EP approximation with CE loss. We sampled from the GP MSE posterior, and the GP/EP CE posterior distribution 10 6 times. It is expected that the two measures should diverge somewhat due to details of the loss function on the training data. (b) compares P SGD (f |S) with MSE loss and P SGD (f |S) with CE loss. Functions with high P SGD (f |S) are scattered around y = x.This implies that the loss function does not substantially affect P SGD (f |S) on average. Note that in (a) the two methods correlate, but that 1) the GP-EP is systematically lower than the MSE, and 2) that the slope is below x = y. These two trends are, we believe, more general for the GP-EP approximation on CE loss. (c) Here we compare the GP/EP log(p) approximation with GP/EP sampling. For this figure, we use only functions found by Adam in 10 6 samples, and compare probabilities found by the GP/EP log(p) approximation to those found by GP/EP sampling. We use both methods in this paper, but as is clear from the above figure, there is not much difference between them for functions with high P B (f |S).]()

![(a) p i for GP (b) P B (f |S) v.s. p i prediction. (c) p i for SGD (d) P B (f |S) v.s. p i prediction.]()

![Figure8: (a) shows the NNGP estimate for the values of p i for an FCN with MSE loss on MNIST, for the training set of size 10000 and test set of size 100 that we use in the main text. Clearly these vary over many orders of magnitude. The sample size is 10 7 , so frequencies were cut off at 10 -7 (so functions in that bin have p i ≤ 10 -7 ). 20 bins in total. (b) calculates the probability of other error functions using the values of p i from Figure1dusing the assumption that the images from (a) are independently distributed, and compares this prediction for P B (f |S) to the value of P B (f |S) obtained by direct GP MSE sampling (see Figure1dfor the data). Each datapoint is for a specific function f , and clearly they are close to the y = x line, implying that, at least for these higher probability functions found by direct sampling, the images in this small test set are classified by the GP in a (close to) independent fashion. Figures(c) and (d) are the equivalent of (a) and (b), but for SGD (see Figure1afor the data). There were only 10 6 samples, so (c) and (d) are cut off at one order of magnitude lower than (a) and (b). (c) includes an inset comparing the values for p i with GP MSE sampling from (a) and p i with SGD from the main part of (c). The correlation is fairly tight for the highest p i which dominate the total probability mass, and this correlation helps explain the strong correlation seen for other numbers of errors throughout this paper.]()

![(a) |S| = 1000 (b) |S| = 5000 (c) |S| = 1000, 5000 (d) |S| = 10000 (e) |S| = 20000 (f) |S| = 10000, 20000]()

![Figure 9: Comparing P B (f |S) to P Adam (f |S) for an FCN on MNIST with CE loss for different training set sizes. [We use test set size of |E| = 100; vertical dotted blue lines denote 90% probability boundary; solid blue line is a guide to the eye, dashed grey line is x = y.] (a) 1000 training examples. G = 6.65% for Adam. (b) 5000 training examples. G = 3.33% for Adam. (c) P B (f |S) v.s. G for 1000 and 5000 training examples. (d) 10000 training examples. G = 2.20% for Adam. (e) 20000 training examples. G = 0.89% for Adam. (f) P B (f |S) v.s. G for 5000 and 10,000 training examples. A trend of increasing bias towards lower error functions with increasing training set size can be clearly observed. See Figure 10 for related results with MSE loss.]()

![Figure 14: Comparing P B (f |S) to P OPT (f |S) for a LSTM on the IMDb movie review dataset. Further results for Figure 4. [We use training/test set size 45,000/50 and batch size=192. Vertical dotted blue lines denote 90% probability boundary; solid blue lines are fit to guide the eye; dashed grey line is x = y.] (a) P B (f |S) v.s. P Adadelta (f |S) for MSE loss. n = 2200 for the optimiser and n = 2.6 * 10 4 for the GP. G = 5.22% and G GP = 5.04% (b) Probability of all functions found by NNGP compared to those also found by the optimiser. Green points are the set jointly found functions F . Red denotes functions found only by the GP, f ∈F P B (f |S) = 54.1% and f ∈F P OPT (f |S) = 77.0%. (c) Compares the P Adam (f |S) with CE loss from Figure 4a, to the sampled P B (f |S) using MSE loss for n = 10 4 samples. While these results are for very limited sample numbers, they provide evidence that for the LSTM, P B (f |S) has values on the same order of magnitude as P SGD (f |S). The low values we find for the raw EP approximation estimates of P B (f |S) are likely to be due to errors in the EP absolute values. The fact that we still see correlations for the CE-trained LSTM with the renormalised EP approximations for P B (f |S) suggests that the EP still does reasonably on relative errors. CE has the advantage that it is much faster to use than MSE.]()

![Figure 15: Comparing P B (f |S) to P Adam (f |S) for an FCN on the Ionosphere dataset with CE loss. Further results for Figure 4. [We use training/test set size 301/50 and batch size=192. Vertical dotted blue lines denote 90% probability boundary; solid blue lines are fit to guide the eye; dashed grey line is x = y.] (a) P B (f |S) v.s. P Adam (f |S) for MSE (same as in Figure4), put here ease of comparison). (b) Probability of all functions found by NNGP compared to those also found by the optimiser. Green points are the set jointly found functions F .f ∈F P B (f |S) = 43.1% and f ∈F P Adam (f |S) = 99.8% (in other words nearly all functions found by Adam are also found by the GP, but the GP also finds functions that Adam doesn't for this level of sampling). For the optimiser, G = 4.59% and for the GP MSE sampling, G GP = 5.41%. (c) P B (f |S) v.s. P Adam (f |S) for CE. G = 5.88%. (d) < P B (f |S) > versus G with CE, 20 samples per G . The bias towards low error functions is less strong than what is found for MNIST or Fashion-MNIST.]()

![Figure 17: Comparing P OPT (f |S) with P OPT (f |S) for different optimisers. Further results for Figure 11. [We use the FCN architecture on MNIST, CE loss and a batch size of 128, training/test set size = 10,000/100, y = x is denoted by a dashed line]Adagrad and Adam correlate very well as do Adadelta and RMSprop for these hyperparameters. The other combinations do not correlate as well, suggesting that they sample the loss-function differently from one another. These plots do not rely on the GP or GP/EP approximation. We believe that this sort of experiment may prove useful for understanding differences in the behaviour of the optimisers.]()

![Figure 21: Results for Appendix F for target function with LZ complexity 35.0. Refer to text for detailed description. 60]()

![Figure 22: Results for Appendix F for target function with LZ complexity 28.0. Refer to text for detailed description. 61]()

In the special case where we specify the experiment as 'overtraining', then we take the parameters after p epochs with 0 classification error.

f ∈F P SGD (f |S) = 24.3%. In other words, while the Adam optimiser finds almost all functions with high P B (f |S), it also finds many functions with low P B (f |S). The much weaker bias under label corruption observed in (b) likely explains the weaker correlation between the Bayesian results and that of the optimiser found here.

CNN: https://keras.io/api/layers/convolution_layers/convolution2d/ LSTM: https://keras.io/api/layers/recurrent_layers/lstm/

