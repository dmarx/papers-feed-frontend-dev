<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-06">6 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southwest University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-06">6 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">DE96DF045A7AA31EB3A4BFB297A1340E</idno>
					<idno type="arXiv">arXiv:2501.02976v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://nju-pcalab.github.io/projects/STAR">https://nju-pcalab.github.io/projects/STAR</ref> Real-world Input (from VideoLQ) Ours Upscale-A-Video RealViformer Ours Upscale-A-Video RealViformer Ours Ours Real-world Input (from VideoLQ) * Equal contributions. Work done during Rui Xie's ByteDance internship. ‚Ä† indicates corresponding author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>. Visualization comparisons on both real-world and synthetic low-resolution videos. Compared to the state-of-the-art VSR models <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b75">75]</ref>, our results demonstrate more natural facial details and better structure of the text. (Zoom-in for best view) Abstract Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce STAR (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate STAR outperforms state-of-the-art methods on both synthetic and realworld datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-world video super-resolution (VSR) aims to generate high-resolution (HR) videos with clear details and strong temporal consistency from low-resolution (LR) inputs with unknown degradations. Most VSR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b60">60]</ref> only focus on simple, known degradations like downsampling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref> or camera-related issues <ref type="bibr" target="#b62">[62]</ref>. However, realworld scenarios often involve unexpected degradations such as noise, blur, and compression, making it difficult for models to capture both spatial and temporal information needed for high-quality, consistent restoration.</p><p>GAN-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b73">73]</ref> are widely used in real-world VSR for improving details through adversarial learning. By incorporating optical flow maps, they also improve temporal consistency, yielding smooth motion across frames. However, their limited generative capacity often results in oversmoothing, as illustrated in Figure <ref type="figure">1</ref>. Recently, image diffusion models <ref type="bibr" target="#b43">[43]</ref> have been applied to real-world VSR for realistic video generation. Methods like <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b75">75]</ref> incorporate temporal blocks or optical flow maps to improve temporal information capture. However, since these models are primarily trained on image data rather than video data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b53">53]</ref>, simply adding temporal layers often fails to ensure high temporal consistency (see Figure <ref type="figure" target="#fig_5">8</ref>). VEnhancer <ref type="bibr" target="#b16">[17]</ref> and LaVie-SR <ref type="bibr" target="#b52">[52]</ref> incorporate T2V models for super-resolving AI-generated videos. However, two key challenges still remain: artifacts introduced by complex degradations in real-world settings, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX).</p><p>To fully leverage the T2V prior <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b72">72]</ref> to enhance practical VSR, we introduce STAR, a novel Spatial-Temporal Augmentation approach for Real-world VSR that achieves realistic spatial details and robust temporal consistency. Specifically, 1) To address artifacts, we introduce a Local Information Enhancement Module (LIEM) before global self-attention to evaluate its impact on T2V models for realworld VSR. This approach stems from our observation that most T2V models rely solely on a global information extraction module (i.e., global self-attention), whereas capturing local details is crucial for video restoration. 2) To improve fidelity, we propose a Dynamic Frequency (DF) Loss, guiding the model to prioritize low-or high-frequency information at different diffusion steps. This is based on our observation that during the reverse diffusion process, our model tends to first recover structure and then refine details. This approach decouples fidelity requirements, reduces learning difficulty, and enhances restoration fidelity.</p><p>In summary, our main contributions are as follows:</p><p>‚Ä¢ We propose STAR, a Spatio-Temporal quality Augmentation framework for Real-world VSR. To our best knowledge, we are the first to integrate diverse, powerful text-to-video diffusion priors into real-world VSR, improv-ing both spatial details and temporal consistency.</p><p>‚Ä¢ We introduce LIEM to enhance local details and ease degradation removal, effectively mitigating artifacts. Moreover, we propose DF loss to guide the model in learning frequency-specific information across diffusion steps, decoupling fidelity requirements and ultimately improving overall fidelity.</p><p>‚Ä¢ Our STAR achieves the highest clarity (DOVER scores) across all datasets compared to state-of-the-art methods, while maintaining robust temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Super-Resolution. Traditional VSR methods can be roughly divided into two categories: recurrent-based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref> and sliding-window-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b65">65]</ref> methods. Recurrent-based methods process LR video frame by frame using recurrent neural networks <ref type="bibr" target="#b34">[34]</ref>. In contrast, sliding-window-based methods divide a video sequence into segments, using each as input to super-resolve the video. However, both approaches suffer from degradation mismatch, leading to significant performance drops in realworld applications. Recently, there has been a growing focus on real-world VSR, targeting complex, unknown degradations. RealBasicVSR <ref type="bibr" target="#b10">[11]</ref>, an extension of BasicVSR <ref type="bibr" target="#b8">[9]</ref>, introduces a pre-cleaning module to mitigate artifacts. Re-alViformer <ref type="bibr" target="#b73">[73]</ref> discovers that channel attention is less sensitive to artifacts and uses squeeze-excite mechanisms and covariance-based rescaling to address these challenges further. While GAN-based and image diffusion models have made substantial progress, they still face issues such as over-smoothing details and temporal inconsistency.</p><p>Text-to-Video Diffusion Model. Large-scale pre-trained text-to-video (T2V) diffusion models have garnered significant attention, particularly with the impressive results from Sora <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">37]</ref>. Numerous T2V models have since emerged, generally divided into: U-Net-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">47]</ref> and DiT-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b64">64]</ref>. I2VGen-XL <ref type="bibr" target="#b72">[72]</ref>, a U-Net-based method, employs a two-stage approach: first generating semantically and content-consistent LR videos, then using these as conditions to produce HR outputs. CogvideoX <ref type="bibr" target="#b64">[64]</ref>, built on DiT <ref type="bibr" target="#b39">[39]</ref>, introduces an adaptive LayerNorm to enhance text-video alignment and employs 3D attention to better integrate spatio-temporal information. Both models have large model capacities and are trained on large-scale datasets, enabling them to capture robust spatiotemporal priors. In this work, we propose STAR to fully leverage T2V model prior for real-world VSR.</p><p>Diffusion Prior for Super-Resolution. Several works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b74">74]</ref> have leveraged generative diffusion priors for image and video super-resolution. StableSR <ref type="bibr" target="#b48">[48]</ref> adds a time-aware encoder and feature warping module to the SD model. DiffBIR <ref type="bibr" target="#b30">[30]</ref> integrates restoration and</p><p>Loss Calculation G-SA LIEM G-SA LIEM G-SA LIEM ‚Ä¶ ‚Ä¶ ‚Ä¶ VAE Encoder v ControlNet T2V Model ùëçùëç ùêªùêª LR video ùëãùëã ùêøùêø HR video ùëãùëã ùêªùêª + ùúñùúñ VAE Encoder VAE Decoder Text Encoder The video captures the dynamic interaction between the ocean and the rocky ... ùëçùëç ùêªùêª ùëãùëã ùêªùêª Local Information Enhancement Module Global Spatial/Temporal Self-Attention Z Z Z H Calculation Loss Calculation Trainable Frozen ^·∫êùëç ùêøùêø ùëêùëê ùë°ùë°ùë°ùë°ùë°ùë°ùë°ùë° ùëêùëê ùëôùëô + Element-wise Addition Total Loss Dynamic Frequency Loss Velocity-Prediction Loss generative modules via ControlNet, while PASD <ref type="bibr" target="#b61">[61]</ref> and SeeSR <ref type="bibr" target="#b57">[57]</ref> embed semantic information in U-Net to guide diffusion. These methods balance fidelity and perceptual quality, achieving high-resolution image details. Methods like Upscale-A-Video <ref type="bibr" target="#b75">[75]</ref>, MGLD-VSR <ref type="bibr" target="#b63">[63]</ref>, Inflating with Diffusion <ref type="bibr" target="#b67">[67]</ref>, and SATeCo <ref type="bibr" target="#b13">[14]</ref> have adapted text-toimage diffusion priors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">43]</ref> for VSR by adding temporal layers. However, rooted in text-to-image models, they often struggle with temporal consistency. More recently, VEnhancer <ref type="bibr" target="#b16">[17]</ref> and LaVie-SR <ref type="bibr" target="#b52">[52]</ref> have incorporated T2V models to super-resolve AI-generated videos but struggle with complex degradations in practical environments. In contrast, we are the first to integrate powerful T2V diffusion priors for real-world VSR, introducing the LIEM module to address spatial artifacts and DF loss to enhance fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Modules. The STAR primarily includes four modules: VAE <ref type="bibr" target="#b24">[24]</ref>, text encoder <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref>, ControlNet <ref type="bibr" target="#b70">[70]</ref> and T2V model <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b72">72]</ref> with Local Information Enhancement Module (LIEM) to alleviate the artifacts (further analysis is provided in Sec. 3.2). As depicted in Figure 2, the VAE encoder takes HR videos X H and LR videos X L as input to generate latent tensors Z H and Z L , respectively. The text encoder is responsible for generating text embeddings c text to provide high-level information. ControlNet takes Z L and c text as input to guide the T2V model output. Finally, the T2V model œï Œ∏ with LIEM receives noisy input Z t = Œ± t Z H +œÉ t œµ (t denotes diffusion step, Œ± t and œÉ t are noise scheduler parameters), c text and the control signal from ControlNet c l to predict the velocity v t ‚â° Œ± t œµ -œÉ t Z H [45].</p><p>Losses. We utilize v-prediction objective in optimization:</p><formula xml:id="formula_0">L v = E[‚à•v t -œï Œ∏ (Z t , c text , c l , t)‚à• 2 2 ].<label>(1)</label></formula><p>Given the strong generalization ability of T2V models, relying solely on the v-prediction objective for optimization may lead to restored outputs with low fidelity, an essential factor in video super-resolution tasks. To address this, we introduce Dynamic Frequency (DF) Loss, which adaptively adjusts the constraint on high-and low-frequency components of the predicted XH across different diffusion steps. The overall optimization objective for STAR is as follows:</p><formula xml:id="formula_1">L total = L v + b(t)L DF ( XH , X H ),<label>(2)</label></formula><p>where b(t) = 1 -t tmax is a weighting function (t max is set to 999) to balance L v and L DF . With the proposed LIEM and DF loss, STAR achieves high spatio-temporal quality, reduced artifacts and enhanced fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Information Enhancement Module</head><p>Motivation. Most T2V models primarily use a global attention mechanism <ref type="bibr" target="#b31">[31]</ref>, which is well-suited to text-tovideo tasks by capturing global information to generate complete videos from scratch. However, this approach may be suboptimal for real-world video super-resolution, where complex degradations occur and local details are crucial <ref type="bibr" target="#b25">[25]</ref>. Relying solely on global attention mechanisms presents two drawbacks for real-world video superresolution: 1) It complicates degradation removal, as it processes the entire degraded video at once (the first and second columns in Figure <ref type="figure" target="#fig_1">3</ref>   </p><formula xml:id="formula_2">L(F I ) = Sigmoid(Conv 3√ó3 (Concat(AP (F I ), M P (F I )))),<label>(3)</label></formula><formula xml:id="formula_3">F O = G(L(F I ) ‚Ä¢ F I ) + F I ,<label>(4)</label></formula><p>where AP (‚Ä¢) and M P (‚Ä¢) denote average pooling and max pooling, respectively. F I and F O represent the input and output features, while G(‚Ä¢) and L(‚Ä¢) refer to the global attention block and LIEM. We adopt the local attention block in CBAM <ref type="bibr" target="#b55">[55]</ref> as LIEM for simplicity. Additional analysis on the impact of adding LIEM is provided in Sec. 4.3.</p><p>Intuitively, as shown in the second row of Figure <ref type="figure" target="#fig_1">3</ref> (left), incorporating LIEM enables the T2V model to address local region degradation first and then aggregate global features. This approach reduces the complexity of degradation removal and mitigates artifacts. Furthermore, the T2V model with LIEM produces clearer, more detailed results due to the enriched local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Frequency Loss</head><p>Motivation. The powerful generative capacity of diffusion models may compromise the fidelity in restored result <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b66">66]</ref>. In Figure <ref type="figure" target="#fig_2">4</ref> (Right), an interesting pattern emerges when examining restored results at each diffusion step during inference. In the early stages, the model primarily reconstructs structure with low frequency, whereas in later stages, after the structure is largely complete, focus shifts to refining details with high frequency. To further illustrate this phenomenon, Figure <ref type="figure" target="#fig_2">4</ref> (Left) presents PSNR curves of low-and high-frequency components against the ground truth across diffusion steps. The low-frequency PSNR rises in the early stages, while the high-frequency PSNR increases later, aligning with the visual results.</p><p>Fidelity can be divided into two types: 1) Lowfrequency fidelity, encompassing large structures and instances. 2) High-frequency fidelity, including edges and textures, aligning with the characteristics of the denoising process. This raises a question: Can we design a loss func-  tion that exploits this characteristic to decouple fidelity and simplify optimization? Specifically, we aim to guide the model to prioritize low-frequency components in the early stages, shifting focus to high-frequency components later.</p><p>Details of DF Loss. Here, we propose Dynamic Frequency Loss. Specifically, in each diffusion step t, we use the following equation to obtain the estimated ·∫êH :</p><formula xml:id="formula_4">·∫êH = œÉ -1 t (Œ± t œµ -œï Œ∏ (Z t , c text , c l , t)).<label>(5)</label></formula><p>Then, we use the decoder to convert the latent ·∫êH back to the pixel space, resulting in XH . After that, we apply Discrete Fourier Transform (DFT) to transform XH into the frequency domain as shown in Figure <ref type="figure" target="#fig_3">5</ref>. We predefine a low-frequency pass filter œà to obtain the low-and highfrequency:</p><formula xml:id="formula_5">fl = F( XH ) ‚äô œà, fh = F( XH ) ‚äô (1 -œà),<label>(6)</label></formula><p>where F(‚Ä¢) is DFT, ‚äô is element-wise multiplication. fl and fh denote the low and high frequency of XH . The pro- posed DF loss can be written as:</p><formula xml:id="formula_6">L LF = ‚à•f l -fl ‚à•, L HF = ‚à•f h -fh ‚à•,<label>(7)</label></formula><formula xml:id="formula_7">L DF = c(t)L LF + (1 -c(t))L HF ,<label>(8)</label></formula><p>where f l / f h stand for low-/ high-frequency of X H , respectively. c(t) = (t/t max ) Œ± is the weighting function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Datasets and Implementation</head><p>Training Datasets. We train STAR using the subset of OpenVid-1M <ref type="bibr" target="#b36">[36]</ref>, containing ‚àº200K text-video pairs. The OpenVid-1M dataset is a high-quality video dataset consisting of over 1 million in-the-wild video clips with detailed captions, where the minimum resolution is 512√ó512 and the average length is 7.2 seconds. Utilizing this largescale high-quality data for training further improves our model's restoration capacity for real-world VSR. More training dataset comparisons can be found in Table <ref type="table" target="#tab_3">2</ref>. We generate the LR-HR video pairs following the degradation strategy in Real-ESRGAN <ref type="bibr" target="#b51">[51]</ref>, combined with video compression operations, resulting in severe degradation similar to the approach used in RealBasicVSR <ref type="bibr" target="#b10">[11]</ref>.</p><p>Testing Datasets. We evaluate our method on both synthetic and real-world datasets. As for synthetic testing datasets, we follow the same degradation pipeline in training to generate LR videos from HR ones to construct three synthetic datasets (i.e., UDM10 <ref type="bibr" target="#b65">[65]</ref>, REDS30 <ref type="bibr" target="#b35">[35]</ref>, and OpenVid30). The OpenVid30 is split from OpenVid-1M <ref type="bibr" target="#b36">[36]</ref> ensuring no overlap with the training dataset and comprises the first approximately 100 frames of 30 videos. For the real-world dataset, we choose VideoLQ <ref type="bibr" target="#b10">[11]</ref> which contains 50 videos, each with 100 frames.</p><p>Training Details. By default, we adopt I2VGen-XL <ref type="bibr" target="#b72">[72]</ref> as our T2V backbone. For fast convergence, we initialize the model using the weights from VEnhancer <ref type="bibr" target="#b16">[17]</ref>. We then train the ControlNet and inserted LIEM to adapt the T2V model for the real-world VSR task. Specifically, we train STAR on 8 NVIDIA A100-80G GPUs with 15K iterations and a batch size of 8. The training data is 720√ó1280 with 32 frames. We use AdamW <ref type="bibr" target="#b33">[33]</ref> as the optimizer with a learning rate of 5e-5.</p><p>Evaluation Metrics. We adopt six metrics to evaluate the VSR outputs from several different perspectives: image fidelity (PSNR), perceptual similarity (SSIM <ref type="bibr" target="#b54">[54]</ref>, LPIPS <ref type="bibr" target="#b71">[71]</ref>), quality (ILNIQE <ref type="bibr" target="#b69">[69]</ref>), video clarity (DOVER <ref type="bibr" target="#b56">[56]</ref>) and temporal consistency (E * warp <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b32">32]</ref>). For synthetic datasets, we calculate PSNR, SSIM and LPIPS between the output and ground-truth frames, along with DOVER and flow warping error (i.e., E * warp ) of output videos. For realworld dataset, because of no ground-truth videos, we use three non-reference metrics: ILNIQE, DOVER, and E * warp . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons</head><p>To verify the effectiveness of our approach, we compare STAR with several state-of-the-art methods, including Real-ESRGAN <ref type="bibr" target="#b51">[51]</ref>, DBVSR <ref type="bibr" target="#b38">[38]</ref>, RealBasicVSR <ref type="bibr" target="#b10">[11]</ref>, RealViformer <ref type="bibr" target="#b73">[73]</ref>, ResShift <ref type="bibr" target="#b68">[68]</ref>, StableSR <ref type="bibr" target="#b48">[48]</ref>, and Upscale-A-Video <ref type="bibr" target="#b75">[75]</ref>. Quantitative Evaluation. As shown in Table <ref type="table" target="#tab_2">1</ref>, we calculate five metrics on each synthetic benchmark. Our STAR achieves the best scores in four out of these five metrics (SSIM, LPIPS, DOVER, and E * warp ) on both UDM10 and OpenVid30 datasets, along with the secondbest PSNR scores. This indicates that STAR can generate realistic details with good fidelity and robust temporal consistency. Moreover, we evaluate three non-reference metrics on a real-world dataset. On this dataset, STAR achieves the best score in DOVER and the second-best scores in IL-NIQE and E * warp . These results demonstrate that STAR can effectively restore real-world videos with high spatial and temporal quality. Additionally, our visual results on both real-world and synthetic datasets are preferred by human evaluators, as detailed in the User Study section (see Appendix). Qualitative Evaluation. To intuitively demonstrate the effectiveness of the proposed STAR, we present visual results on both synthetic and real-world datasets in Figure <ref type="figure">6</ref> and <ref type="figure">7</ref>, respectively. As shown, our STAR generates the most realistic spatial details and exhibits the best degradation removal capability. Specifically, the first example in Figure <ref type="figure">7</ref> illustrates that STAR reconstructs the text structure most effectively, thanks to the T2V prior efficiently capturing temporal information, and the DF loss that improves the fidelity. Furthermore, the T2V model has a strong spatial prior, which helps generate more realistic details and structures, such as the human hand in Figure <ref type="figure">6</ref> and the horse shape and fur in Figure <ref type="figure">7</ref>.</p><p>We also compare the temporal consistency in Figure <ref type="figure" target="#fig_5">8</ref>. As observed in the left of Figure <ref type="figure" target="#fig_5">8</ref>, StableSR demonstrates the most temporal inconsistency, primarily because it is originally designed for image super-resolution. Although RealBasicVSR, Upscale-A-Video, and RealViformer incorporate optical flow maps to enhance temporal consistency, they still face challenges in generating consistent results under complex degraded video conditions, as the optical flow maps may not always be accurate. In contrast, our proposed STAR achieves the best temporal consistency, thanks to the powerful temporal prior inherent in the T2V model, which effectively helps reconstruct temporal information even without the use of optical flow maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Local Information Enhancement Module. We primarily investigate the impact of introducing LIEM in different ways. First, we find that adding LIEM on both spatial and temporal blocks achieves the best results as shown in Table <ref type="table" target="#tab_4">3</ref>. Second, we consider three connection types as shown in Figure <ref type="figure" target="#fig_6">9</ref> (Left). From visual results in Figure <ref type="figure" target="#fig_6">9</ref> (Right) and quantitative results in Table <ref type="table" target="#tab_4">3</ref>, we find that position (i) achieves the best results. This phenomenon can be attributed to the fact that, with most weights frozen to preserve the prior, the newly added blocks can influence the model's mapping process. However, the impact at positions (ii) and (iii) is too large, making it difficult for the model to fine-tune and adapt to this change, resulting in poor performance. Dynamic Frequency Loss. First, we investigate the impact of different variants of frequency loss. As shown in Table <ref type="table" target="#tab_5">4</ref>, "Separate" indicates whether the frequency components are separated into high and low frequency, constraining them individually. "Type" refers to the specific   -XL) Ours (CogvideoX-2B) Ours (CogvideoX-5B) definition of the DF loss: if set to "inverse," a higher weight is given to high frequencies in the early stages and a lower weight to low frequencies; if set to "direct", a higher weight is given to low frequencies initially and a lower weight to high frequencies, which is matching the analysis in Sec. 3.3. As observed, separating the frequency components and prioritizing low-frequency reconstruction early on yield the best perceptual quality while maintaining high fidelity. Second, we explore the optimal settings for b(t) and Œ± in c(t).</p><p>As shown in Table <ref type="table" target="#tab_6">5</ref>, using a linear form for b(t) with Œ± = 2 for c(t) yields the best results. Therefore, we adopt this DF loss configuration for training our model and comparing it with other state-of-the-art methods.</p><p>Scaling up with Larger T2V Models. To further validate the effectiveness of T2V diffusion priors for realworld VSR, we replace I2VGen-XL with larger DiT-based <ref type="bibr" target="#b39">[39]</ref> T2V models (i.e., CogVideoX <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b64">64]</ref>), and evaluate results both quantitatively and qualitatively. Since CogVideoX only supports inputs at 480√ó720 resolution, we created a new test set by cropping 10 videos from OpenVid-1M <ref type="bibr" target="#b36">[36]</ref> to this size. As shown in Table <ref type="table" target="#tab_8">6</ref>, the powerful CogVideoX models yield consistent improvements across all metrics. Notably, SSIM improves from 0.6944 to 0.7400, and DOVER increases from 0.6609 to 0.7350, marking a substantial enhancement in visual quality. The robust spatio-temporal priors in CogVideoX enable realistic details and clear building structures (Figure <ref type="figure" target="#fig_7">10</ref>), while maintaining high temporal consistency (Figure <ref type="figure" target="#fig_5">8</ref> Right).</p><p>Inspired by scaling law <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> and our findings, we believe larger, more powerful T2V models will further advance VSR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present STAR, a real-world VSR framework that leverages T2V diffusion prior to restore videos with fewer artifacts, higher spatial fidelity, and stronger temporal consistency. Specifically, we introduce a Local Information Enhancement Module into the original T2V backbone to improve its ability to handle degradations and reconstruct fine details. Additionally, we propose a Dynamic Frequency Loss that guides the model to focus on restoring different frequency components at each diffusion step, thereby enhancing fidelity. Furthermore, we demonstrate that a powerful T2V model can effectively generate high-quality results in both spatial and temporal dimensions. Extensive experiments show that STAR achieves superior performance in both spatial and temporal quality. We hope our work lays a solid foundation for applying T2V models in realworld VSR and inspires future advancements in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Perception-Distortion Trade-Off</head><p>The trade-off between perception and distortion <ref type="bibr" target="#b5">[6]</ref> is a widely recognized challenge in the super-resolution domain. Thanks to our DF Loss, our method can easily control the model to favor either fidelity or perceptual quality in the generated results. We can adjust the hyper-parameter Œ≤ in the b(t) to achieve this goal. The total loss in our STAR is:</p><formula xml:id="formula_8">L total = L v + b(t)L DF ,<label>(9)</label></formula><p>The b(t) can be written as follows:</p><formula xml:id="formula_9">b(t) = Œ≤ ‚Ä¢ (1 - t t max ),<label>(10)</label></formula><p>Where t is the timestep and Œ≤ is the hyper-parameter that adjusts the weight between L v and L DF , which we set to 1 by default. From equations ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>), we can observe that a larger Œ≤ increases the weight of the DF loss at each timestep, thereby further enhancing the fidelity of the results. In contrast, a smaller Œ≤ reduces the influence of the DF loss at each timestep, allowing the v-prediction loss to have a greater impact and produce more perceptual results. The b(t) -t curves under different Œ≤ are shown in Figure <ref type="figure" target="#fig_8">11</ref>.</p><p>We conduct experiments under these settings to demonstrate the ability to achieve the perception-distortion tradeoff. The quantitative results are shown in Table <ref type="table" target="#tab_9">7</ref>. From Table 7, we can observe that increasing Œ≤ improves the PSNR and E * warp , leading to better fidelity. Conversely, decreasing Œ≤ reduces the LPIPS score, indicating better perceptual quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. User Study</head><p>To find the human-preferred results between our STAR and other state-of-the-art methods, we conduct a user study that evaluate the results on both real-world and synthetic datasets. Specifically, we use the real-world dataset Vide-oLQ <ref type="bibr" target="#b10">[11]</ref> and the synthetic dataset REDS30 <ref type="bibr" target="#b35">[35]</ref>. We select two image-diffusion-model-based methods, Upscale-A-Video <ref type="bibr" target="#b75">[75]</ref> and MGLD-VSR <ref type="bibr" target="#b63">[63]</ref>; and one GAN-based method, RealViformer <ref type="bibr" target="#b73">[73]</ref> for comparison. We invite 12 evaluators to participate in the user study. For each evaluator, we randomly select 10 videos from each dataset and present four results: one from our STAR and three from the compared methods. The evaluators were asked to choose which result had the best visual quality and temporal consistency. The results of the user study are depicted in Figure <ref type="figure" target="#fig_0">12</ref>, indicating that our STAR is preferred by most human evaluators for both visual quality and temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Qualitative Comparisons</head><p>We provide more visual comparisons on synthetic and realworld datasets in Figure <ref type="figure" target="#fig_1">13</ref> and Figure <ref type="figure" target="#fig_2">14</ref> to further highlight our advantages in spatial quality. These results clearly demonstrate that our method preserves richer details and achieves greater realism. To demonstrate the impact of scaling up with larger text-to-video (T2V) models, we present additional results in Figure <ref type="figure" target="#fig_3">15</ref>. It is evident that scaling up the T2V model further improves the restoration effect, indicating that a large and robust T2V model can serve as a strong base model for video super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Video Demo</head><p>We provide a demo video [STAR-demo.mp4] in the supplementary material, showcasing the temporal and spatial advantages of our proposed STAR more intuitively. This video includes additional results and comparisons on synthetic, real-world, and AIGC videos. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the proposed STAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Motivation of LIEM. Left: schematic diagram illustrating the impact of using only global structure versus a combination of local and global structures. Right: visual comparison on real-world and synthetic videos. (Zoom-in for best view)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Motivation of DF Loss. Left: PSNR curves of low-and high-frequency components relative to ground truth across diffusion steps. The low-frequency PSNR increases during the early diffusion steps, while the high-frequency PSNR rises in the later diffusion steps. Right: visual results of low-and high-frequency components at different diffusion stage. (Zoom-in for best view)</figDesc><graphic coords="4,50.82,295.53,241.77,112.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Dynamic Frequency Loss. Left: curves of weighting function c(t) for different Œ±. Right: details of DF loss.</figDesc><graphic coords="5,57.59,303.22,170.05,112.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Qualitative comparisons on synthetic LR videos from OpenVid30 and REDS30[35]. (Zoom-in for best view)</figDesc><graphic coords="6,59.36,252.78,156.24,117.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Qualitative comparisons on temporal consistency in REDS30 [35] and OpenVid dataset. (Zoom-in for best view)</figDesc><graphic coords="7,61.59,93.76,114.16,96.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Ablation study about LIEM. Left: illustration of different insertion positions of LIEM and the structure of LIEM. Right: visual comparison on real-world and synthetic videos with different LIEM positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Illustration on scaling up with larger t2v models on a real-world low-quality video. (Zoom-in for best view)</figDesc><graphic coords="8,62.29,523.96,72.92,63.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Ablation on b(t). Higher hyper-parameter Œ≤ produces results with greater fidelity, while lower Œ≤ emphasizes more perceptual quality.</figDesc><graphic coords="12,58.50,496.36,236.25,171.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>Figure 14. Qualitative comparisons on real-world datasets. Our STAR produces the clearest facial details and the most accurate text structure. (Zoom-in for best view)</figDesc><graphic coords="14,219.75,289.92,79.98,51.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluations on diverse VSR benchmarks from synthetic (UDM10, REDS30, OpenVid30) and real-world (VideoLQ) sources. The best performance is highlighted in bold, and the second-best in underlined. E * warp refers to Ewarp (√ó10 -3 ).</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell cols="2">Real-ESRGAN ICCVW 2021 ICCV 2021 DBVSR</cell><cell></cell><cell cols="2">RealBasicVSR RealViformer CVPR 2022 ECCV 2024</cell><cell cols="2">ResShift NeurIPS 2023 IJCV 2024 StableSR</cell><cell cols="2">Upscale-A-Video MGLDVSR CVPR 2024 ECCV 2024</cell><cell>Ours -</cell></row><row><cell></cell><cell>PSNR‚Üë</cell><cell>22.41</cell><cell>19.65</cell><cell></cell><cell>23.64</cell><cell>24.00</cell><cell>22.90</cell><cell>23.50</cell><cell>21.29</cell><cell>23.74</cell><cell>23.91</cell></row><row><cell></cell><cell>SSIM‚Üë</cell><cell>0.6476</cell><cell>0.4747</cell><cell></cell><cell>0.6842</cell><cell>0.6896</cell><cell>0.5451</cell><cell>0.6599</cell><cell>0.5967</cell><cell>0.6826</cell><cell>0.7164</cell></row><row><cell>UDM10</cell><cell>LPIPS‚Üì</cell><cell>0.2769</cell><cell>0.4566</cell><cell></cell><cell>0.2514</cell><cell>0.2325</cell><cell>0.4036</cell><cell>0.2785</cell><cell>0.3006</cell><cell>0.2195</cell><cell>0.1885</cell></row><row><cell></cell><cell>DOVER‚Üë</cell><cell>0.4831</cell><cell>0.0959</cell><cell></cell><cell>0.5039</cell><cell>0.5055</cell><cell>0.3252</cell><cell>0.3490</cell><cell>0.5309</cell><cell>0.4896</cell><cell>0.5422</cell></row><row><cell></cell><cell>E  *  warp ‚Üì</cell><cell>11.17</cell><cell>12.56</cell><cell></cell><cell>5.14</cell><cell>3.57</cell><cell>12.69</cell><cell>8.89</cell><cell>2.83</cell><cell>6.03</cell><cell>2.68</cell></row><row><cell></cell><cell>PSNR‚Üë</cell><cell>19.56</cell><cell>14.85</cell><cell></cell><cell>20.85</cell><cell>20.86</cell><cell>19.93</cell><cell>20.32</cell><cell>19.71</cell><cell>20.57</cell><cell>20.29</cell></row><row><cell></cell><cell>SSIM‚Üë</cell><cell>0.4862</cell><cell>0.2941</cell><cell></cell><cell>0.5469</cell><cell>0.5377</cell><cell>0.4261</cell><cell>0.5043</cell><cell>0.4315</cell><cell>0.5113</cell><cell>0.5411</cell></row><row><cell>REDS30</cell><cell>LPIPS‚Üì</cell><cell>0.3376</cell><cell>0.5915</cell><cell></cell><cell>0.2899</cell><cell>0.2597</cell><cell>0.4422</cell><cell>0.3857</cell><cell>0.3443</cell><cell>0.2240</cell><cell>0.2804</cell></row><row><cell></cell><cell>DOVER‚Üë</cell><cell>0.3182</cell><cell>0.0600</cell><cell></cell><cell>0.3483</cell><cell>0.3400</cell><cell>0.2221</cell><cell>0.2519</cell><cell>0.2857</cell><cell>0.3857</cell><cell>0.4017</cell></row><row><cell></cell><cell>E  *  warp ‚Üì</cell><cell>19.1</cell><cell>18.00</cell><cell></cell><cell>8.32</cell><cell>6.06</cell><cell>17.40</cell><cell>22.14</cell><cell>15.65</cell><cell>12.28</cell><cell>7.30</cell></row><row><cell></cell><cell>PSNR‚Üë</cell><cell>24.62</cell><cell>21.14</cell><cell></cell><cell>24.63</cell><cell>26.21</cell><cell>24.29</cell><cell>24.91</cell><cell>24.41</cell><cell>24.73</cell><cell>25.30</cell></row><row><cell></cell><cell>SSIM‚Üë</cell><cell>0.7778</cell><cell>0.5887</cell><cell></cell><cell>0.7759</cell><cell>0.8080</cell><cell>0.6070</cell><cell>0.7633</cell><cell>0.7167</cell><cell>0.7686</cell><cell>0.8371</cell></row><row><cell>OpenVid30</cell><cell>LPIPS‚Üì</cell><cell>0.1994</cell><cell>0.4207</cell><cell></cell><cell>0.2297</cell><cell>0.1881</cell><cell>0.3902</cell><cell>0.2102</cell><cell>0.2479</cell><cell>0.2074</cell><cell>0.1011</cell></row><row><cell></cell><cell>DOVER‚Üë</cell><cell>0.6992</cell><cell>0.1819</cell><cell></cell><cell>0.7345</cell><cell>0.7275</cell><cell>0.5435</cell><cell>0.6368</cell><cell>0.7201</cell><cell>0.7191</cell><cell>0.7393</cell></row><row><cell></cell><cell>E  *  warp ‚Üì</cell><cell>8.46</cell><cell>12.11</cell><cell></cell><cell>4.12</cell><cell>2.52</cell><cell>9.78</cell><cell>8.87</cell><cell>4.72</cell><cell>4.82</cell><cell>1.82</cell></row><row><cell></cell><cell>ILNIQE‚Üì</cell><cell>27.95</cell><cell>27.19</cell><cell></cell><cell>26.29</cell><cell>26.11</cell><cell>25.92</cell><cell>29.97</cell><cell>24.49</cell><cell>23.94</cell><cell>25.35</cell></row><row><cell>VideoLQ</cell><cell>DOVER‚Üë E  *  warp ‚Üì</cell><cell>0.4967 8.00</cell><cell>0.3392 7.75</cell><cell></cell><cell>0.5285 6.52</cell><cell>0.4804 5.10</cell><cell>0.4113 8.33</cell><cell>0.4775 9.26</cell><cell>0.4833 10.89</cell><cell>0.5319 7.82</cell><cell>0.5431 6.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ÔøΩ ùëãùëã ùêªùêª</cell><cell></cell><cell></cell><cell>ùëãùëã ùêªùêª</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Discrete Fourier Transform</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 -ùúìùúì</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ùúìùúì</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>fùëì‚Ñé</cell><cell>fùëìùëôùëô</cell><cell>‚Ñí ùêøùêøùêøùêø</cell><cell>ùëìùëì ùëôùëô ùëìùëì ‚Ñé</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>‚Ñí ùêªùêªùêøùêø</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Element-wise Multiplication</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Training dataset comparison.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>Size</cell><cell>#Frames</cell><cell>Resolution</cell></row><row><cell>UAV[75]</cell><cell cols="2">WebVid [2] + YouHQ [75] 335K+37K</cell><cell>-</cell><cell>336√ó596, 1080√ó1920</cell></row><row><cell>RealViformer[73]</cell><cell>REDS [35]</cell><cell>300K</cell><cell>100</cell><cell>720√ó1280</cell></row><row><cell>Ours</cell><cell>OpenVid [36]</cell><cell>200K</cell><cell>32</cell><cell>720√ó1280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation of LIEM position.</figDesc><table><row><cell cols="6">Position Spa-Local Temp-Local PSNR‚Üë LPIPS‚Üì E  *  warp ‚Üì</cell></row><row><cell></cell><cell></cell><cell></cell><cell>23.14</cell><cell>0.2015</cell><cell>2.83</cell></row><row><cell>(i)</cell><cell>‚úì</cell><cell>‚úì</cell><cell>23.61 23.65</cell><cell>0.2013 0.1945</cell><cell>2.82 2.92</cell></row><row><cell></cell><cell></cell><cell></cell><cell>23.69</cell><cell>0.1943</cell><cell>2.74</cell></row><row><cell>(ii)</cell><cell>‚úì</cell><cell>‚úì</cell><cell>23.27</cell><cell>0.2363</cell><cell>3.57</cell></row><row><cell>(iii)</cell><cell></cell><cell></cell><cell>24.51</cell><cell>0.2094</cell><cell>1.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation of different variants of DF loss.</figDesc><table><row><cell>Seperate</cell><cell>Type</cell><cell cols="3">PSNR‚Üë LPIPS‚Üì E  *  warp ‚Üì</cell></row><row><cell cols="2">w/o Frequency Loss</cell><cell>23.69</cell><cell>0.1943</cell><cell>2.74</cell></row><row><cell>-</cell><cell>-</cell><cell>23.72</cell><cell>0.1941</cell><cell>2.71</cell></row><row><cell>‚úì</cell><cell>Inverse</cell><cell>23.67</cell><cell>0.1945</cell><cell>2.83</cell></row><row><cell>‚úì</cell><cell>Direct</cell><cell>23.85</cell><cell>0.1903</cell><cell>2.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation of b(t) and Œ± in c(t).</figDesc><table><row><cell>b(t)</cell><cell>Œ±</cell><cell cols="3">PSNR‚Üë LPIPS‚Üì E  *  warp ‚Üì</cell></row><row><cell></cell><cell>0.25</cell><cell>23.76</cell><cell>0.2030</cell><cell>2.72</cell></row><row><cell></cell><cell>0.5</cell><cell>23.71</cell><cell>0.2010</cell><cell>2.75</cell></row><row><cell>Linear</cell><cell>1</cell><cell>23.85</cell><cell>0.1903</cell><cell>2.69</cell></row><row><cell></cell><cell>1.5</cell><cell>23.53</cell><cell>0.1928</cell><cell>2.81</cell></row><row><cell>Exponential</cell><cell>2</cell><cell>23.91 23.68</cell><cell>0.1885 0.1990</cell><cell>2.61 2.78</cell></row><row><cell>Input</cell><cell></cell><cell>RealViformer</cell><cell cols="2">Upscale-A-Video</cell></row><row><cell>Ours (I2VGen</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Effectiveness of T2V diffusion prior for real-world VSR.</figDesc><table><row><cell>Metrics</cell><cell>UAV</cell><cell>RealViformer</cell><cell cols="3">Ours I2VGen-XL CogX-2B CogX-5B</cell></row><row><cell>PSNR‚Üë</cell><cell>22.46</cell><cell>22.90</cell><cell>21.46</cell><cell>23.18</cell><cell>23.60</cell></row><row><cell>SSIM‚Üë</cell><cell>0.6552</cell><cell>0.6944</cell><cell>0.6715</cell><cell>0.7112</cell><cell>0.7400</cell></row><row><cell>LPIPS‚Üì</cell><cell>0.2035</cell><cell>0.1823</cell><cell>0.1779</cell><cell>0.1571</cell><cell>0.1314</cell></row><row><cell cols="2">DOVER‚Üë 0.6609</cell><cell>0.4286</cell><cell>0.7267</cell><cell>0.6955</cell><cell>0.7350</cell></row><row><cell>E  *  warp ‚Üì</cell><cell>5.424</cell><cell>4.75</cell><cell>5.529</cell><cell>3.68</cell><cell>4.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Qualitative comparison under different Œ≤ of b(t).</figDesc><table><row><cell>Œ≤</cell><cell cols="3">PSNR‚Üë LPIPS‚Üì E  *  warp ‚Üì</cell></row><row><cell>0.25</cell><cell>23.55</cell><cell>0.1825</cell><cell>2.88</cell></row><row><cell>0.75</cell><cell>23.76</cell><cell>0.1842</cell><cell>2.74</cell></row><row><cell>1.0</cell><cell>23.91</cell><cell>0.1885</cell><cell>2.68</cell></row><row><cell>1.5</cell><cell>24.08</cell><cell>0.2272</cell><cell>2.53</cell></row><row><cell>2.0</cell><cell>24.41</cell><cell>0.3339</cell><cell>2.21</cell></row><row><cell cols="2">B. More Results</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://huggingface.co/THUDM/CogVideoX-5b.8" />
		<title level="m">Cogvideox-5b</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">G√ºl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chendong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guande</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaole</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04233</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align your latents: High-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22563" to="22575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Kelvin Ck Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4947" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Basicvsr++: Improving video super-resolution with enhanced propagation and alignment</title>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Kelvin Ck Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5972" to="5981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating tradeoffs in real-world video super-resolution</title>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Kelvin Ck Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022. 2, 5, 6, 7, 12</date>
			<biblScope unit="page" from="5962" to="5971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gentron: Diffusion transformers for image and video generation</title>
		<author>
			<persName><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuren</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6441" to="6451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Panda-70m: Captioning 70m videos with multiple cross-modality teachers</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Tsai-Shien Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Wei</forename><surname>Deyneka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Eun Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13320" to="13331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning spatial adaptation and temporal coherence in diffusion models for video super-resolution</title>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient video super-resolution through recurrent latent space propagation</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Fuoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3476" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Venhancer: Generative space-time enhancement for video generation</title>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.07667</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scaling laws for autoregressive generative modeling</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video superresolution via bidirectional recurrent convolutional networks</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video super-resolution with recurrent structure-detail network</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="645" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seoung</forename><forename type="middle">Wug</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeyeon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<imprint>
			<pubPlace>Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Residual local feature network for efficient super-resolution</title>
		<author>
			<persName><forename type="first">Fangyuan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lean</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="766" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mucan: Multi-correspondence aggregation network for video super-resolution</title>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent video restoration transformer with guided deformable attention</title>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="378" to="393" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vrt: A video restoration transformer</title>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Xinqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanghua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.15070</idno>
		<title level="m">Diffbir: Towards blind image restoration with generative diffusion prior</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Global attention mechanism: Retain information to enhance channelspatial interactions</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05561,2021.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evalcrafter: Benchmarking and evaluating large video generation models</title>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="22139" to="22149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafi√°t</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cernock·ª≥</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<publisher>Makuhari</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring and superresolution: Dataset and study</title>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Openvid-1m: A large-scale high-quality dataset for text-tovideo generation</title>
		<author>
			<persName><forename type="first">Kepan</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiehan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.02371</idno>
		<imprint>
			<date type="published" when="2008">2024. 2, 5, 6, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Sora</surname></persName>
		</author>
		<ptr target="https://openai.com/index/sora.2" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep blind video super-resolution</title>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangxin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4811" to="4820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008">2023. 2, 8</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Movie gen: A cast of media foundation models</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13720</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking alignment in video superresolution transformers</title>
		<author>
			<persName><forename type="first">Shuwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="36081" to="36093" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Exploiting diffusion prior for real-world image super-resolution. IJCV</title>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongsheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin Ck</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusion models</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06098</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin Ck</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Real-esrgan: Training real-world blind super-resolution with pure synthetic data</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Lavie: High-quality video generation with cascaded latent diffusion models</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqing</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15103</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Internvid: A large-scale video-text dataset for multimodal understanding and generation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06942</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Exploring video quality assessment on user generated contents from aesthetic and technical perspectives</title>
		<author>
			<persName><forename type="first">Haoning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Hou Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Seesr: Towards semantics-aware real-world image super-resolution</title>
		<author>
			<persName><forename type="first">Rongyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingchen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Animesr: Learning real-world super-resolution models for animation videos</title>
		<author>
			<persName><forename type="first">Yanze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11241" to="11252" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal modulation network for controllable space-time video super-resolution</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6388" to="6397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14469</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Realworld video super-resolution: A benchmark dataset and a decomposition based learning scheme</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4781" to="4790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Motionguided latent diffusion for temporally consistent real-world video super-resolution</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
			<biblScope unit="page" from="3106" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scaling up to excellence: Practicing model scaling for photorealistic image restoration in the wild</title>
		<author>
			<persName><forename type="first">Fanghua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangtao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="25669" to="25680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Inflation with diffusion: Efficient temporal adaptation for text-to-video super-resolution</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinoo</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Resshift: Efficient diffusion model for image superresolution by residual shifting</title>
		<author>
			<persName><forename type="first">Zongsheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04145</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Realviformer: Investigating attention for real-world video super-resolution</title>
		<author>
			<persName><forename type="first">Yuehan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 5, 7, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8281" to="8291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Upscale-a-video: Temporalconsistent diffusion model for real-world video superresolution</title>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 3, 5, 7, 12</date>
			<biblScope unit="page" from="2535" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
