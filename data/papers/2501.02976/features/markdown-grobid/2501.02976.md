# STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution

## Abstract

## 

[https://nju-pcalab.github.io/projects/STAR](https://nju-pcalab.github.io/projects/STAR) Real-world Input (from VideoLQ) Ours Upscale-A-Video RealViformer Ours Upscale-A-Video RealViformer Ours Ours Real-world Input (from VideoLQ) * Equal contributions. Work done during Rui Xie's ByteDance internship. † indicates corresponding author.

## 

Figure [1](#). Visualization comparisons on both real-world and synthetic low-resolution videos. Compared to the state-of-the-art VSR models [[73,](#b73)[75]](#b75), our results demonstrate more natural facial details and better structure of the text. (Zoom-in for best view) Abstract Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce STAR (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate STAR outperforms state-of-the-art methods on both synthetic and realworld datasets.

## Introduction

Real-world video super-resolution (VSR) aims to generate high-resolution (HR) videos with clear details and strong temporal consistency from low-resolution (LR) inputs with unknown degradations. Most VSR methods [[10,](#b9)[22,](#b21)[50,](#b50)[60]](#b60) only focus on simple, known degradations like downsampling [[15,](#b14)[21]](#b20) or camera-related issues [[62]](#b62). However, realworld scenarios often involve unexpected degradations such as noise, blur, and compression, making it difficult for models to capture both spatial and temporal information needed for high-quality, consistent restoration.

GAN-based methods [[11,](#b10)[51,](#b51)[58,](#b58)[62,](#b62)[73]](#b73) are widely used in real-world VSR for improving details through adversarial learning. By incorporating optical flow maps, they also improve temporal consistency, yielding smooth motion across frames. However, their limited generative capacity often results in oversmoothing, as illustrated in Figure [1](#). Recently, image diffusion models [[43]](#b43) have been applied to real-world VSR for realistic video generation. Methods like [[14,](#b13)[63,](#b63)[67,](#b67)[75]](#b75) incorporate temporal blocks or optical flow maps to improve temporal information capture. However, since these models are primarily trained on image data rather than video data [[13,](#b12)[36,](#b36)[49,](#b49)[53]](#b53), simply adding temporal layers often fails to ensure high temporal consistency (see Figure [8](#fig_5)). VEnhancer [[17]](#b16) and LaVie-SR [[52]](#b52) incorporate T2V models for super-resolving AI-generated videos. However, two key challenges still remain: artifacts introduced by complex degradations in real-world settings, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX).

To fully leverage the T2V prior [[64,](#b64)[72]](#b72) to enhance practical VSR, we introduce STAR, a novel Spatial-Temporal Augmentation approach for Real-world VSR that achieves realistic spatial details and robust temporal consistency. Specifically, 1) To address artifacts, we introduce a Local Information Enhancement Module (LIEM) before global self-attention to evaluate its impact on T2V models for realworld VSR. This approach stems from our observation that most T2V models rely solely on a global information extraction module (i.e., global self-attention), whereas capturing local details is crucial for video restoration. 2) To improve fidelity, we propose a Dynamic Frequency (DF) Loss, guiding the model to prioritize low-or high-frequency information at different diffusion steps. This is based on our observation that during the reverse diffusion process, our model tends to first recover structure and then refine details. This approach decouples fidelity requirements, reduces learning difficulty, and enhances restoration fidelity.

In summary, our main contributions are as follows:

• We propose STAR, a Spatio-Temporal quality Augmentation framework for Real-world VSR. To our best knowledge, we are the first to integrate diverse, powerful text-to-video diffusion priors into real-world VSR, improv-ing both spatial details and temporal consistency.

• We introduce LIEM to enhance local details and ease degradation removal, effectively mitigating artifacts. Moreover, we propose DF loss to guide the model in learning frequency-specific information across diffusion steps, decoupling fidelity requirements and ultimately improving overall fidelity.

• Our STAR achieves the highest clarity (DOVER scores) across all datasets compared to state-of-the-art methods, while maintaining robust temporal consistency.

## Related Work

Video Super-Resolution. Traditional VSR methods can be roughly divided into two categories: recurrent-based [[16,](#b15)[20,](#b19)[28,](#b28)[44,](#b44)[46]](#b46) and sliding-window-based [[8,](#b7)[27,](#b27)[29,](#b29)[59,](#b59)[65]](#b65) methods. Recurrent-based methods process LR video frame by frame using recurrent neural networks [[34]](#b34). In contrast, sliding-window-based methods divide a video sequence into segments, using each as input to super-resolve the video. However, both approaches suffer from degradation mismatch, leading to significant performance drops in realworld applications. Recently, there has been a growing focus on real-world VSR, targeting complex, unknown degradations. RealBasicVSR [[11]](#b10), an extension of BasicVSR [[9]](#b8), introduces a pre-cleaning module to mitigate artifacts. Re-alViformer [[73]](#b73) discovers that channel attention is less sensitive to artifacts and uses squeeze-excite mechanisms and covariance-based rescaling to address these challenges further. While GAN-based and image diffusion models have made substantial progress, they still face issues such as over-smoothing details and temporal inconsistency.

Text-to-Video Diffusion Model. Large-scale pre-trained text-to-video (T2V) diffusion models have garnered significant attention, particularly with the impressive results from Sora [[7,](#b6)[37]](#b37). Numerous T2V models have since emerged, generally divided into: U-Net-based methods [[4,](#b3)[5,](#b4)[19,](#b18)[47]](#b47) and DiT-based methods [[3,](#b2)[12,](#b11)[40,](#b40)[64]](#b64). I2VGen-XL [[72]](#b72), a U-Net-based method, employs a two-stage approach: first generating semantically and content-consistent LR videos, then using these as conditions to produce HR outputs. CogvideoX [[64]](#b64), built on DiT [[39]](#b39), introduces an adaptive LayerNorm to enhance text-video alignment and employs 3D attention to better integrate spatio-temporal information. Both models have large model capacities and are trained on large-scale datasets, enabling them to capture robust spatiotemporal priors. In this work, we propose STAR to fully leverage T2V model prior for real-world VSR.

Diffusion Prior for Super-Resolution. Several works [[30,](#b30)[48,](#b48)[57,](#b57)[61,](#b61)[74]](#b74) have leveraged generative diffusion priors for image and video super-resolution. StableSR [[48]](#b48) adds a time-aware encoder and feature warping module to the SD model. DiffBIR [[30]](#b30) integrates restoration and

Loss Calculation G-SA LIEM G-SA LIEM G-SA LIEM … … … VAE Encoder v ControlNet T2V Model 𝑍𝑍 𝐻𝐻 LR video 𝑋𝑋 𝐿𝐿 HR video 𝑋𝑋 𝐻𝐻 + 𝜖𝜖 VAE Encoder VAE Decoder Text Encoder The video captures the dynamic interaction between the ocean and the rocky ... 𝑍𝑍 𝐻𝐻 𝑋𝑋 𝐻𝐻 Local Information Enhancement Module Global Spatial/Temporal Self-Attention Z Z Z H Calculation Loss Calculation Trainable Frozen ^Ẑ𝑍 𝐿𝐿 𝑐𝑐 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 𝑐𝑐 𝑙𝑙 + Element-wise Addition Total Loss Dynamic Frequency Loss Velocity-Prediction Loss generative modules via ControlNet, while PASD [[61]](#b61) and SeeSR [[57]](#b57) embed semantic information in U-Net to guide diffusion. These methods balance fidelity and perceptual quality, achieving high-resolution image details. Methods like Upscale-A-Video [[75]](#b75), MGLD-VSR [[63]](#b63), Inflating with Diffusion [[67]](#b67), and SATeCo [[14]](#b13) have adapted text-toimage diffusion priors [[19,](#b18)[43]](#b43) for VSR by adding temporal layers. However, rooted in text-to-image models, they often struggle with temporal consistency. More recently, VEnhancer [[17]](#b16) and LaVie-SR [[52]](#b52) have incorporated T2V models to super-resolve AI-generated videos but struggle with complex degradations in practical environments. In contrast, we are the first to integrate powerful T2V diffusion priors for real-world VSR, introducing the LIEM module to address spatial artifacts and DF loss to enhance fidelity.

## Methodology

## Overview

Modules. The STAR primarily includes four modules: VAE [[24]](#b24), text encoder [[41,](#b41)[42]](#b42), ControlNet [[70]](#b70) and T2V model [[64,](#b64)[72]](#b72) with Local Information Enhancement Module (LIEM) to alleviate the artifacts (further analysis is provided in Sec. 3.2). As depicted in Figure 2, the VAE encoder takes HR videos X H and LR videos X L as input to generate latent tensors Z H and Z L , respectively. The text encoder is responsible for generating text embeddings c text to provide high-level information. ControlNet takes Z L and c text as input to guide the T2V model output. Finally, the T2V model ϕ θ with LIEM receives noisy input Z t = α t Z H +σ t ϵ (t denotes diffusion step, α t and σ t are noise scheduler parameters), c text and the control signal from ControlNet c l to predict the velocity v t ≡ α t ϵ -σ t Z H [45].

Losses. We utilize v-prediction objective in optimization:

$L v = E[∥v t -ϕ θ (Z t , c text , c l , t)∥ 2 2 ].(1)$Given the strong generalization ability of T2V models, relying solely on the v-prediction objective for optimization may lead to restored outputs with low fidelity, an essential factor in video super-resolution tasks. To address this, we introduce Dynamic Frequency (DF) Loss, which adaptively adjusts the constraint on high-and low-frequency components of the predicted XH across different diffusion steps. The overall optimization objective for STAR is as follows:

$L total = L v + b(t)L DF ( XH , X H ),(2)$where b(t) = 1 -t tmax is a weighting function (t max is set to 999) to balance L v and L DF . With the proposed LIEM and DF loss, STAR achieves high spatio-temporal quality, reduced artifacts and enhanced fidelity.

## Local Information Enhancement Module

Motivation. Most T2V models primarily use a global attention mechanism [[31]](#b31), which is well-suited to text-tovideo tasks by capturing global information to generate complete videos from scratch. However, this approach may be suboptimal for real-world video super-resolution, where complex degradations occur and local details are crucial [[25]](#b25). Relying solely on global attention mechanisms presents two drawbacks for real-world video superresolution: 1) It complicates degradation removal, as it processes the entire degraded video at once (the first and second columns in Figure [3](#fig_1)

$L(F I ) = Sigmoid(Conv 3×3 (Concat(AP (F I ), M P (F I )))),(3)$$F O = G(L(F I ) • F I ) + F I ,(4)$where AP (•) and M P (•) denote average pooling and max pooling, respectively. F I and F O represent the input and output features, while G(•) and L(•) refer to the global attention block and LIEM. We adopt the local attention block in CBAM [[55]](#b55) as LIEM for simplicity. Additional analysis on the impact of adding LIEM is provided in Sec. 4.3.

Intuitively, as shown in the second row of Figure [3](#fig_1) (left), incorporating LIEM enables the T2V model to address local region degradation first and then aggregate global features. This approach reduces the complexity of degradation removal and mitigates artifacts. Furthermore, the T2V model with LIEM produces clearer, more detailed results due to the enriched local information.

## Dynamic Frequency Loss

Motivation. The powerful generative capacity of diffusion models may compromise the fidelity in restored result [[57,](#b57)[66]](#b66). In Figure [4](#fig_2) (Right), an interesting pattern emerges when examining restored results at each diffusion step during inference. In the early stages, the model primarily reconstructs structure with low frequency, whereas in later stages, after the structure is largely complete, focus shifts to refining details with high frequency. To further illustrate this phenomenon, Figure [4](#fig_2) (Left) presents PSNR curves of low-and high-frequency components against the ground truth across diffusion steps. The low-frequency PSNR rises in the early stages, while the high-frequency PSNR increases later, aligning with the visual results.

Fidelity can be divided into two types: 1) Lowfrequency fidelity, encompassing large structures and instances. 2) High-frequency fidelity, including edges and textures, aligning with the characteristics of the denoising process. This raises a question: Can we design a loss func-  tion that exploits this characteristic to decouple fidelity and simplify optimization? Specifically, we aim to guide the model to prioritize low-frequency components in the early stages, shifting focus to high-frequency components later.

Details of DF Loss. Here, we propose Dynamic Frequency Loss. Specifically, in each diffusion step t, we use the following equation to obtain the estimated ẐH :

$ẐH = σ -1 t (α t ϵ -ϕ θ (Z t , c text , c l , t)).(5)$Then, we use the decoder to convert the latent ẐH back to the pixel space, resulting in XH . After that, we apply Discrete Fourier Transform (DFT) to transform XH into the frequency domain as shown in Figure [5](#fig_3). We predefine a low-frequency pass filter ψ to obtain the low-and highfrequency:

$fl = F( XH ) ⊙ ψ, fh = F( XH ) ⊙ (1 -ψ),(6)$where F(•) is DFT, ⊙ is element-wise multiplication. fl and fh denote the low and high frequency of XH . The pro- posed DF loss can be written as:

$L LF = ∥f l -fl ∥, L HF = ∥f h -fh ∥,(7)$$L DF = c(t)L LF + (1 -c(t))L HF ,(8)$where f l / f h stand for low-/ high-frequency of X H , respectively. c(t) = (t/t max ) α is the weighting function.

## Experiments 4.1. Datasets and Implementation

Training Datasets. We train STAR using the subset of OpenVid-1M [[36]](#b36), containing ∼200K text-video pairs. The OpenVid-1M dataset is a high-quality video dataset consisting of over 1 million in-the-wild video clips with detailed captions, where the minimum resolution is 512×512 and the average length is 7.2 seconds. Utilizing this largescale high-quality data for training further improves our model's restoration capacity for real-world VSR. More training dataset comparisons can be found in Table [2](#tab_3). We generate the LR-HR video pairs following the degradation strategy in Real-ESRGAN [[51]](#b51), combined with video compression operations, resulting in severe degradation similar to the approach used in RealBasicVSR [[11]](#b10).

Testing Datasets. We evaluate our method on both synthetic and real-world datasets. As for synthetic testing datasets, we follow the same degradation pipeline in training to generate LR videos from HR ones to construct three synthetic datasets (i.e., UDM10 [[65]](#b65), REDS30 [[35]](#b35), and OpenVid30). The OpenVid30 is split from OpenVid-1M [[36]](#b36) ensuring no overlap with the training dataset and comprises the first approximately 100 frames of 30 videos. For the real-world dataset, we choose VideoLQ [[11]](#b10) which contains 50 videos, each with 100 frames.

Training Details. By default, we adopt I2VGen-XL [[72]](#b72) as our T2V backbone. For fast convergence, we initialize the model using the weights from VEnhancer [[17]](#b16). We then train the ControlNet and inserted LIEM to adapt the T2V model for the real-world VSR task. Specifically, we train STAR on 8 NVIDIA A100-80G GPUs with 15K iterations and a batch size of 8. The training data is 720×1280 with 32 frames. We use AdamW [[33]](#b33) as the optimizer with a learning rate of 5e-5.

Evaluation Metrics. We adopt six metrics to evaluate the VSR outputs from several different perspectives: image fidelity (PSNR), perceptual similarity (SSIM [[54]](#b54), LPIPS [[71]](#b71)), quality (ILNIQE [[69]](#b69)), video clarity (DOVER [[56]](#b56)) and temporal consistency (E * warp [[26,](#b26)[32]](#b32)). For synthetic datasets, we calculate PSNR, SSIM and LPIPS between the output and ground-truth frames, along with DOVER and flow warping error (i.e., E * warp ) of output videos. For realworld dataset, because of no ground-truth videos, we use three non-reference metrics: ILNIQE, DOVER, and E * warp . 

## Comparisons

To verify the effectiveness of our approach, we compare STAR with several state-of-the-art methods, including Real-ESRGAN [[51]](#b51), DBVSR [[38]](#b38), RealBasicVSR [[11]](#b10), RealViformer [[73]](#b73), ResShift [[68]](#b68), StableSR [[48]](#b48), and Upscale-A-Video [[75]](#b75). Quantitative Evaluation. As shown in Table [1](#tab_2), we calculate five metrics on each synthetic benchmark. Our STAR achieves the best scores in four out of these five metrics (SSIM, LPIPS, DOVER, and E * warp ) on both UDM10 and OpenVid30 datasets, along with the secondbest PSNR scores. This indicates that STAR can generate realistic details with good fidelity and robust temporal consistency. Moreover, we evaluate three non-reference metrics on a real-world dataset. On this dataset, STAR achieves the best score in DOVER and the second-best scores in IL-NIQE and E * warp . These results demonstrate that STAR can effectively restore real-world videos with high spatial and temporal quality. Additionally, our visual results on both real-world and synthetic datasets are preferred by human evaluators, as detailed in the User Study section (see Appendix). Qualitative Evaluation. To intuitively demonstrate the effectiveness of the proposed STAR, we present visual results on both synthetic and real-world datasets in Figure [6](#) and [7](#), respectively. As shown, our STAR generates the most realistic spatial details and exhibits the best degradation removal capability. Specifically, the first example in Figure [7](#) illustrates that STAR reconstructs the text structure most effectively, thanks to the T2V prior efficiently capturing temporal information, and the DF loss that improves the fidelity. Furthermore, the T2V model has a strong spatial prior, which helps generate more realistic details and structures, such as the human hand in Figure [6](#) and the horse shape and fur in Figure [7](#).

We also compare the temporal consistency in Figure [8](#fig_5). As observed in the left of Figure [8](#fig_5), StableSR demonstrates the most temporal inconsistency, primarily because it is originally designed for image super-resolution. Although RealBasicVSR, Upscale-A-Video, and RealViformer incorporate optical flow maps to enhance temporal consistency, they still face challenges in generating consistent results under complex degraded video conditions, as the optical flow maps may not always be accurate. In contrast, our proposed STAR achieves the best temporal consistency, thanks to the powerful temporal prior inherent in the T2V model, which effectively helps reconstruct temporal information even without the use of optical flow maps.

## Ablation Study

Local Information Enhancement Module. We primarily investigate the impact of introducing LIEM in different ways. First, we find that adding LIEM on both spatial and temporal blocks achieves the best results as shown in Table [3](#tab_4). Second, we consider three connection types as shown in Figure [9](#fig_6) (Left). From visual results in Figure [9](#fig_6) (Right) and quantitative results in Table [3](#tab_4), we find that position (i) achieves the best results. This phenomenon can be attributed to the fact that, with most weights frozen to preserve the prior, the newly added blocks can influence the model's mapping process. However, the impact at positions (ii) and (iii) is too large, making it difficult for the model to fine-tune and adapt to this change, resulting in poor performance. Dynamic Frequency Loss. First, we investigate the impact of different variants of frequency loss. As shown in Table [4](#tab_5), "Separate" indicates whether the frequency components are separated into high and low frequency, constraining them individually. "Type" refers to the specific   -XL) Ours (CogvideoX-2B) Ours (CogvideoX-5B) definition of the DF loss: if set to "inverse," a higher weight is given to high frequencies in the early stages and a lower weight to low frequencies; if set to "direct", a higher weight is given to low frequencies initially and a lower weight to high frequencies, which is matching the analysis in Sec. 3.3. As observed, separating the frequency components and prioritizing low-frequency reconstruction early on yield the best perceptual quality while maintaining high fidelity. Second, we explore the optimal settings for b(t) and α in c(t).

As shown in Table [5](#tab_6), using a linear form for b(t) with α = 2 for c(t) yields the best results. Therefore, we adopt this DF loss configuration for training our model and comparing it with other state-of-the-art methods.

Scaling up with Larger T2V Models. To further validate the effectiveness of T2V diffusion priors for realworld VSR, we replace I2VGen-XL with larger DiT-based [[39]](#b39) T2V models (i.e., CogVideoX [[1,](#b0)[64]](#b64)), and evaluate results both quantitatively and qualitatively. Since CogVideoX only supports inputs at 480×720 resolution, we created a new test set by cropping 10 videos from OpenVid-1M [[36]](#b36) to this size. As shown in Table [6](#tab_8), the powerful CogVideoX models yield consistent improvements across all metrics. Notably, SSIM improves from 0.6944 to 0.7400, and DOVER increases from 0.6609 to 0.7350, marking a substantial enhancement in visual quality. The robust spatio-temporal priors in CogVideoX enable realistic details and clear building structures (Figure [10](#fig_7)), while maintaining high temporal consistency (Figure [8](#fig_5) Right).

Inspired by scaling law [[18,](#b17)[23]](#b22) and our findings, we believe larger, more powerful T2V models will further advance VSR tasks.

## Conclusion

In this paper, we present STAR, a real-world VSR framework that leverages T2V diffusion prior to restore videos with fewer artifacts, higher spatial fidelity, and stronger temporal consistency. Specifically, we introduce a Local Information Enhancement Module into the original T2V backbone to improve its ability to handle degradations and reconstruct fine details. Additionally, we propose a Dynamic Frequency Loss that guides the model to focus on restoring different frequency components at each diffusion step, thereby enhancing fidelity. Furthermore, we demonstrate that a powerful T2V model can effectively generate high-quality results in both spatial and temporal dimensions. Extensive experiments show that STAR achieves superior performance in both spatial and temporal quality. We hope our work lays a solid foundation for applying T2V models in realworld VSR and inspires future advancements in the field.

## A. Perception-Distortion Trade-Off

The trade-off between perception and distortion [[6]](#b5) is a widely recognized challenge in the super-resolution domain. Thanks to our DF Loss, our method can easily control the model to favor either fidelity or perceptual quality in the generated results. We can adjust the hyper-parameter β in the b(t) to achieve this goal. The total loss in our STAR is:

$L total = L v + b(t)L DF ,(9)$The b(t) can be written as follows:

$b(t) = β • (1 - t t max ),(10)$Where t is the timestep and β is the hyper-parameter that adjusts the weight between L v and L DF , which we set to 1 by default. From equations ( [1](#formula_0)) and ( [2](#formula_1)), we can observe that a larger β increases the weight of the DF loss at each timestep, thereby further enhancing the fidelity of the results. In contrast, a smaller β reduces the influence of the DF loss at each timestep, allowing the v-prediction loss to have a greater impact and produce more perceptual results. The b(t) -t curves under different β are shown in Figure [11](#fig_8).

We conduct experiments under these settings to demonstrate the ability to achieve the perception-distortion tradeoff. The quantitative results are shown in Table [7](#tab_9). From Table 7, we can observe that increasing β improves the PSNR and E * warp , leading to better fidelity. Conversely, decreasing β reduces the LPIPS score, indicating better perceptual quality.  

## B.1. User Study

To find the human-preferred results between our STAR and other state-of-the-art methods, we conduct a user study that evaluate the results on both real-world and synthetic datasets. Specifically, we use the real-world dataset Vide-oLQ [[11]](#b10) and the synthetic dataset REDS30 [[35]](#b35). We select two image-diffusion-model-based methods, Upscale-A-Video [[75]](#b75) and MGLD-VSR [[63]](#b63); and one GAN-based method, RealViformer [[73]](#b73) for comparison. We invite 12 evaluators to participate in the user study. For each evaluator, we randomly select 10 videos from each dataset and present four results: one from our STAR and three from the compared methods. The evaluators were asked to choose which result had the best visual quality and temporal consistency. The results of the user study are depicted in Figure [12](#fig_0), indicating that our STAR is preferred by most human evaluators for both visual quality and temporal consistency.

## B.2. Qualitative Comparisons

We provide more visual comparisons on synthetic and realworld datasets in Figure [13](#fig_1) and Figure [14](#fig_2) to further highlight our advantages in spatial quality. These results clearly demonstrate that our method preserves richer details and achieves greater realism. To demonstrate the impact of scaling up with larger text-to-video (T2V) models, we present additional results in Figure [15](#fig_3). It is evident that scaling up the T2V model further improves the restoration effect, indicating that a large and robust T2V model can serve as a strong base model for video super-resolution.

## B.3. Video Demo

We provide a demo video [STAR-demo.mp4] in the supplementary material, showcasing the temporal and spatial advantages of our proposed STAR more intuitively. This video includes additional results and comparisons on synthetic, real-world, and AIGC videos. 

![Figure 2. Overview of the proposed STAR.]()

![Figure 3. Motivation of LIEM. Left: schematic diagram illustrating the impact of using only global structure versus a combination of local and global structures. Right: visual comparison on real-world and synthetic videos. (Zoom-in for best view)]()

![Figure 4. Motivation of DF Loss. Left: PSNR curves of low-and high-frequency components relative to ground truth across diffusion steps. The low-frequency PSNR increases during the early diffusion steps, while the high-frequency PSNR rises in the later diffusion steps. Right: visual results of low-and high-frequency components at different diffusion stage. (Zoom-in for best view)]()

![Figure 5. Dynamic Frequency Loss. Left: curves of weighting function c(t) for different α. Right: details of DF loss.]()

![Figure 6. Qualitative comparisons on synthetic LR videos from OpenVid30 and REDS30[35]. (Zoom-in for best view)]()

![Figure 8. Qualitative comparisons on temporal consistency in REDS30 [35] and OpenVid dataset. (Zoom-in for best view)]()

![Figure 9. Ablation study about LIEM. Left: illustration of different insertion positions of LIEM and the structure of LIEM. Right: visual comparison on real-world and synthetic videos with different LIEM positions.]()

![Figure 10. Illustration on scaling up with larger t2v models on a real-world low-quality video. (Zoom-in for best view)]()

![Figure 11. Ablation on b(t). Higher hyper-parameter β produces results with greater fidelity, while lower β emphasizes more perceptual quality.]()

![Figure 14. Qualitative comparisons on real-world datasets. Our STAR produces the clearest facial details and the most accurate text structure. (Zoom-in for best view)]()

![Quantitative evaluations on diverse VSR benchmarks from synthetic (UDM10, REDS30, OpenVid30) and real-world (VideoLQ) sources. The best performance is highlighted in bold, and the second-best in underlined. E * warp refers to Ewarp (×10 -3 ).]()

![Training dataset comparison.]()

![Ablation of LIEM position.]()

![Ablation of different variants of DF loss.]()

![Ablation of b(t) and α in c(t).]()

![Effectiveness of T2V diffusion prior for real-world VSR.]()

![Qualitative comparison under different β of b(t).]()

