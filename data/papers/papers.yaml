'1310.6753':
  abstract: 'A crucial task in the analysis of on-line social-networking systems is
    to

    identify important people --- those linked by strong social ties --- within an

    individual''s network neighborhood. Here we investigate this question for a

    particular category of strong ties, those involving spouses or romantic

    partners. We organize our analysis around a basic question: given all the

    connections among a person''s friends, can you recognize his or her romantic

    partner from the network structure alone? Using data from a large sample of

    Facebook users, we find that this task can be accomplished with high accuracy,

    but doing so requires the development of a new measure of tie strength that we

    term `dispersion'' --- the extent to which two people''s mutual friends are not

    themselves well-connected. The results offer methods for identifying types of

    structurally significant people in on-line applications, and suggest a

    potential expansion of existing theories of tie strength.'
  arxivId: '1310.6753'
  arxiv_tags:
  - cs.SI
  - physics.soc-ph
  - H.2.8
  authors: Lars Backstrom, Jon Kleinberg
  created_at: '2025-01-04T06:52:00.637611'
  issue_number: 770
  issue_url: https://github.com/dmarx/papers-feed/issues/770
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:00.641659'
  last_visited: '2025-01-03T18:28:19.059Z'
  main_tex_file: null
  published_date: '2013-10-24T20:00:18Z'
  state: open
  title: "Romantic Partnerships and the Dispersion of Social Ties: A Network\n  Analysis\
    \ of Relationship Status on Facebook"
  total_reading_time_seconds: 79
  url: https://arxiv.org/abs/1310.6753
'1411.1792':
  abstract: 'Many deep neural networks trained on natural images exhibit a curious

    phenomenon in common: on the first layer they learn features similar to Gabor

    filters and color blobs. Such first-layer features appear not to be specific to

    a particular dataset or task, but general in that they are applicable to many

    datasets and tasks. Features must eventually transition from general to

    specific by the last layer of the network, but this transition has not been

    studied extensively. In this paper we experimentally quantify the generality

    versus specificity of neurons in each layer of a deep convolutional neural

    network and report a few surprising results. Transferability is negatively

    affected by two distinct issues: (1) the specialization of higher layer neurons

    to their original task at the expense of performance on the target task, which

    was expected, and (2) optimization difficulties related to splitting networks

    between co-adapted neurons, which was not expected. In an example network

    trained on ImageNet, we demonstrate that either of these two issues may

    dominate, depending on whether features are transferred from the bottom,

    middle, or top of the network. We also document that the transferability of

    features decreases as the distance between the base task and target task

    increases, but that transferring features even from distant tasks can be better

    than using random features. A final surprising result is that initializing a

    network with transferred features from almost any number of layers can produce

    a boost to generalization that lingers even after fine-tuning to the target

    dataset.'
  arxivId: '1411.1792'
  arxiv_tags:
  - cs.LG
  - cs.NE
  authors: Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson
  created_at: '2025-01-05T08:23:38.525164'
  issue_number: 212
  issue_url: https://github.com/dmarx/papers-feed/issues/212
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:43:09.950Z'
  main_tex_file: null
  published_date: '2014-11-06T23:09:37Z'
  state: open
  title: How transferable are features in deep neural networks?
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1411.1792
'1503.02531':
  abstract: 'A very simple way to improve the performance of almost any machine learning

    algorithm is to train many different models on the same data and then to

    average their predictions. Unfortunately, making predictions using a whole

    ensemble of models is cumbersome and may be too computationally expensive to

    allow deployment to a large number of users, especially if the individual

    models are large neural nets. Caruana and his collaborators have shown that it

    is possible to compress the knowledge in an ensemble into a single model which

    is much easier to deploy and we develop this approach further using a different

    compression technique. We achieve some surprising results on MNIST and we show

    that we can significantly improve the acoustic model of a heavily used

    commercial system by distilling the knowledge in an ensemble of models into a

    single model. We also introduce a new type of ensemble composed of one or more

    full models and many specialist models which learn to distinguish fine-grained

    classes that the full models confuse. Unlike a mixture of experts, these

    specialist models can be trained rapidly and in parallel.'
  arxivId: '1503.02531'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - cs.NE
  authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
  created_at: '2025-01-05T08:25:05.518450'
  issue_number: 95
  issue_url: https://github.com/dmarx/papers-feed/issues/95
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T16:06:57.364Z'
  main_tex_file: null
  published_date: '2015-03-09T15:44:49Z'
  state: open
  title: Distilling the Knowledge in a Neural Network
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1503.02531
'1503.03585':
  abstract: 'A central problem in machine learning involves modeling complex data-sets

    using highly flexible families of probability distributions in which learning,

    sampling, inference, and evaluation are still analytically or computationally

    tractable. Here, we develop an approach that simultaneously achieves both

    flexibility and tractability. The essential idea, inspired by non-equilibrium

    statistical physics, is to systematically and slowly destroy structure in a

    data distribution through an iterative forward diffusion process. We then learn

    a reverse diffusion process that restores structure in data, yielding a highly

    flexible and tractable generative model of the data. This approach allows us to

    rapidly learn, sample from, and evaluate probabilities in deep generative

    models with thousands of layers or time steps, as well as to compute

    conditional and posterior probabilities under the learned model. We

    additionally release an open source reference implementation of the algorithm.'
  arxivId: '1503.03585'
  arxiv_tags:
  - cs.LG
  - cond-mat.dis-nn
  - q-bio.NC
  - stat.ML
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2025-01-05T08:23:44.499582'
  issue_number: 118
  issue_url: https://github.com/dmarx/papers-feed/issues/118
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:44.500651'
  last_visited: '2024-12-22T07:09:20.505Z'
  main_tex_file: null
  published_date: '2015-03-12T04:51:37Z'
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/1503.03585
'1506.06579':
  abstract: 'Recent years have produced great advances in training large, deep neural

    networks (DNNs), including notable successes in training convolutional neural

    networks (convnets) to recognize natural images. However, our understanding of

    how these models work, especially what computations they perform at

    intermediate layers, has lagged behind. Progress in the field will be further

    accelerated by the development of better tools for visualizing and interpreting

    neural nets. We introduce two such tools here. The first is a tool that

    visualizes the activations produced on each layer of a trained convnet as it

    processes an image or video (e.g. a live webcam stream). We have found that

    looking at live activations that change in response to user input helps build

    valuable intuitions about how convnets work. The second tool enables

    visualizing features at each layer of a DNN via regularized optimization in

    image space. Because previous versions of this idea produced less recognizable

    images, here we introduce several new regularization methods that combine to

    produce qualitatively clearer, more interpretable visualizations. Both tools

    are open source and work on a pre-trained convnet with minimal setup.'
  arxivId: '1506.06579'
  arxiv_tags:
  - cs.CV
  - cs.LG
  - cs.NE
  authors: Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson
  created_at: '2025-01-05T08:23:35.490852'
  issue_number: 214
  issue_url: https://github.com/dmarx/papers-feed/issues/214
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:47:02.227Z'
  main_tex_file: null
  published_date: '2015-06-22T12:57:15Z'
  state: open
  title: Understanding Neural Networks Through Deep Visualization
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1506.06579
'1602.03483':
  abstract: 'Unsupervised methods for learning distributed representations of words
    are

    ubiquitous in today''s NLP research, but far less is known about the best ways

    to learn distributed phrase or sentence representations from unlabelled data.

    This paper is a systematic comparison of models that learn such

    representations. We find that the optimal approach depends critically on the

    intended application. Deeper, more complex models are preferable for

    representations to be used in supervised systems, but shallow log-linear models

    work best for building representation spaces that can be decoded with simple

    spatial distance metrics. We also propose two new unsupervised

    representation-learning objectives designed to optimise the trade-off between

    training time, domain portability and performance.'
  arxivId: '1602.03483'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Felix Hill, Kyunghyun Cho, Anna Korhonen
  created_at: '2025-01-04T06:51:57.839597'
  issue_number: 777
  issue_url: https://github.com/dmarx/papers-feed/issues/777
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:51:57.840814'
  last_visited: '2025-01-03T20:13:43.540Z'
  main_tex_file: null
  published_date: '2016-02-10T18:49:58Z'
  state: open
  title: Learning Distributed Representations of Sentences from Unlabelled Data
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/1602.03483
'1705.07831':
  abstract: 'Training generative adversarial networks is unstable in high-dimensions
    as

    the true data distribution tends to be concentrated in a small fraction of the

    ambient space. The discriminator is then quickly able to classify nearly all

    generated samples as fake, leaving the generator without meaningful gradients

    and causing it to deteriorate after a point in training. In this work, we

    propose training a single generator simultaneously against an array of

    discriminators, each of which looks at a different random low-dimensional

    projection of the data. Individual discriminators, now provided with restricted

    views of the input, are unable to reject generated samples perfectly and

    continue to provide meaningful gradients to the generator throughout training.

    Meanwhile, the generator learns to produce samples consistent with the full

    data distribution to satisfy all discriminators simultaneously. We demonstrate

    the practical utility of this approach experimentally, and show that it is able

    to produce image samples with higher quality than traditional training with a

    single discriminator.'
  arxivId: '1705.07831'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti
  created_at: '2025-01-11T08:03:52.542936'
  issue_number: 924
  issue_url: https://github.com/dmarx/papers-feed/issues/924
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-11T08:02:03.936Z'
  main_tex_file: null
  published_date: '2017-05-22T16:23:26Z'
  state: open
  title: Stabilizing GAN Training with Multiple Random Projections
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1705.07831
'1705.08039':
  abstract: 'Representation learning has become an invaluable approach for learning
    from

    symbolic data such as text and graphs. However, while complex symbolic datasets

    often exhibit a latent hierarchical structure, state-of-the-art methods

    typically learn embeddings in Euclidean vector spaces, which do not account for

    this property. For this purpose, we introduce a new approach for learning

    hierarchical representations of symbolic data by embedding them into hyperbolic

    space -- or more precisely into an n-dimensional Poincar\''e ball. Due to the

    underlying hyperbolic geometry, this allows us to learn parsimonious

    representations of symbolic data by simultaneously capturing hierarchy and

    similarity. We introduce an efficient algorithm to learn the embeddings based

    on Riemannian optimization and show experimentally that Poincar\''e embeddings

    outperform Euclidean embeddings significantly on data with latent hierarchies,

    both in terms of representation capacity and in terms of generalization

    ability.'
  arxivId: '1705.08039'
  arxiv_tags:
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Maximilian Nickel, Douwe Kiela
  created_at: '2025-01-04T14:49:33.224758'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:42.247780'
  last_visited: '2024-12-29T01:44:01.360000+00:00'
  main_tex_file: null
  published_date: '2017-05-22T23:14:36Z'
  state: open
  title: Poincaré Embeddings for Learning Hierarchical Representations
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1705.08039
'1705.10359':
  abstract: 'Neural embeddings have been used with great success in Natural Language

    Processing (NLP). They provide compact representations that encapsulate word

    similarity and attain state-of-the-art performance in a range of linguistic

    tasks. The success of neural embeddings has prompted significant amounts of

    research into applications in domains other than language. One such domain is

    graph-structured data, where embeddings of vertices can be learned that

    encapsulate vertex similarity and improve performance on tasks including edge

    prediction and vertex labelling. For both NLP and graph based tasks, embeddings

    have been learned in high-dimensional Euclidean spaces. However, recent work

    has shown that the appropriate isometric space for embedding complex networks

    is not the flat Euclidean space, but negatively curved, hyperbolic space. We

    present a new concept that exploits these recent insights and propose learning

    neural embeddings of graphs in hyperbolic space. We provide experimental

    evidence that embedding graphs in their natural geometry significantly improves

    performance on downstream tasks for several real-world public datasets.'
  arxivId: '1705.10359'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth
  created_at: '2025-01-04T14:49:36.224476'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:42.248652'
  last_visited: '2024-12-29T01:43:53.617000+00:00'
  main_tex_file: null
  published_date: '2017-05-29T18:47:30Z'
  state: open
  title: Neural Embeddings of Graphs in Hyperbolic Space
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/1705.10359
'1706.05806':
  abstract: 'We propose a new technique, Singular Vector Canonical Correlation Analysis

    (SVCCA), a tool for quickly comparing two representations in a way that is both

    invariant to affine transform (allowing comparison between different layers and

    networks) and fast to compute (allowing more comparisons to be calculated than

    with previous methods). We deploy this tool to measure the intrinsic

    dimensionality of layers, showing in some cases needless over-parameterization;

    to probe learning dynamics throughout training, finding that networks converge

    to final representations from the bottom up; to show where class-specific

    information in networks is formed; and to suggest new training regimes that

    simultaneously save computation and overfit less. Code:

    https://github.com/google/svcca/'
  arxivId: '1706.05806'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein
  created_at: '2025-01-05T08:23:29.501696'
  issue_number: 218
  issue_url: https://github.com/dmarx/papers-feed/issues/218
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:29.503537'
  last_visited: '2024-12-24T02:53:44.603Z'
  main_tex_file: null
  published_date: '2017-06-19T07:09:20Z'
  state: open
  title: "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning\n\
    \  Dynamics and Interpretability"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1706.05806
'1710.09412':
  abstract: 'Large deep neural networks are powerful, but exhibit undesirable behaviors

    such as memorization and sensitivity to adversarial examples. In this work, we

    propose mixup, a simple learning principle to alleviate these issues. In

    essence, mixup trains a neural network on convex combinations of pairs of

    examples and their labels. By doing so, mixup regularizes the neural network to

    favor simple linear behavior in-between training examples. Our experiments on

    the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show

    that mixup improves the generalization of state-of-the-art neural network

    architectures. We also find that mixup reduces the memorization of corrupt

    labels, increases the robustness to adversarial examples, and stabilizes the

    training of generative adversarial networks.'
  arxivId: '1710.09412'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz
  created_at: '2025-01-05T08:23:56.495087'
  issue_number: 162
  issue_url: https://github.com/dmarx/papers-feed/issues/162
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T17:42:57.473Z'
  main_tex_file: null
  published_date: '2017-10-25T18:30:49Z'
  state: open
  title: 'mixup: Beyond Empirical Risk Minimization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1710.09412
'1711.11586':
  abstract: 'Many image-to-image translation problems are ambiguous, as a single input

    image may correspond to multiple possible outputs. In this work, we aim to

    model a \emph{distribution} of possible outputs in a conditional generative

    modeling setting. The ambiguity of the mapping is distilled in a

    low-dimensional latent vector, which can be randomly sampled at test time. A

    generator learns to map the given input, combined with this latent code, to the

    output. We explicitly encourage the connection between output and the latent

    code to be invertible. This helps prevent a many-to-one mapping from the latent

    code to the output during training, also known as the problem of mode collapse,

    and produces more diverse results. We explore several variants of this approach

    by employing different training objectives, network architectures, and methods

    of injecting the latent code. Our proposed method encourages bijective

    consistency between the latent encoding and output modes. We present a

    systematic comparison of our method and other variants on both perceptual

    realism and diversity.'
  arxivId: '1711.11586'
  arxiv_tags:
  - cs.CV
  - cs.GR
  - stat.ML
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2025-01-05T18:41:10.033770'
  issue_number: 934
  issue_url: https://github.com/dmarx/papers-feed/issues/934
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-12T06:43:53.633334'
  last_visited: '2025-01-12T06:42:01.228Z'
  main_tex_file: null
  published_date: '2017-11-30T18:59:01Z'
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1711.11586
'1802.03426':
  abstract: 'UMAP (Uniform Manifold Approximation and Projection) is a novel manifold

    learning technique for dimension reduction. UMAP is constructed from a

    theoretical framework based in Riemannian geometry and algebraic topology. The

    result is a practical scalable algorithm that applies to real world data. The

    UMAP algorithm is competitive with t-SNE for visualization quality, and

    arguably preserves more of the global structure with superior run time

    performance. Furthermore, UMAP has no computational restrictions on embedding

    dimension, making it viable as a general purpose dimension reduction technique

    for machine learning.'
  arxivId: '1802.03426'
  arxiv_tags:
  - stat.ML
  - cs.CG
  - cs.LG
  authors: Leland McInnes, John Healy, James Melville
  created_at: '2025-01-04T14:49:00.223341'
  issue_number: 533
  issue_url: https://github.com/dmarx/papers-feed/issues/533
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T03:35:58.743Z'
  main_tex_file: null
  published_date: '2018-02-09T19:39:33Z'
  state: open
  title: "UMAP: Uniform Manifold Approximation and Projection for Dimension\n  Reduction"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1802.03426
'1804.03329':
  abstract: 'Hyperbolic embeddings offer excellent quality with few dimensions when

    embedding hierarchical data structures like synonym or type hierarchies. Given

    a tree, we give a combinatorial construction that embeds the tree in hyperbolic

    space with arbitrarily low distortion without using optimization. On WordNet,

    our combinatorial embedding obtains a mean-average-precision of 0.989 with only

    two dimensions, while Nickel et al.''s recent construction obtains 0.87 using

    200 dimensions. We provide upper and lower bounds that allow us to characterize

    the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To

    embed general metric spaces, we propose a hyperbolic generalization of

    multidimensional scaling (h-MDS). We show how to perform exact recovery of

    hyperbolic points from distances, provide a perturbation analysis, and give a

    recovery result that allows us to reduce dimensionality. The h-MDS approach

    offers consistently low distortion even with few dimensions across several

    datasets. Finally, we extract lessons from the algorithms and theory above to

    design a PyTorch-based implementation that can handle incomplete information

    and is scalable.'
  arxivId: '1804.03329'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala
  created_at: '2025-01-04T14:49:30.230276'
  issue_number: 457
  issue_url: https://github.com/dmarx/papers-feed/issues/457
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T02:34:35.575Z'
  main_tex_file: null
  published_date: '2018-04-10T03:39:16Z'
  state: open
  title: Representation Tradeoffs for Hyperbolic Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1804.03329
'1807.00734':
  abstract: "In standard generative adversarial network (SGAN), the discriminator\n\
    estimates the probability that the input data is real. The generator is trained\n\
    to increase the probability that fake data is real. We argue that it should\n\
    also simultaneously decrease the probability that real data is real because 1)\n\
    this would account for a priori knowledge that half of the data in the\nmini-batch\
    \ is fake, 2) this would be observed with divergence minimization, and\n3) in\
    \ optimal settings, SGAN would be equivalent to integral probability metric\n\
    (IPM) GANs.\n  We show that this property can be induced by using a relativistic\n\
    discriminator which estimate the probability that the given real data is more\n\
    realistic than a randomly sampled fake data. We also present a variant in which\n\
    the discriminator estimate the probability that the given real data is more\n\
    realistic than fake data, on average. We generalize both approaches to\nnon-standard\
    \ GAN loss functions and we refer to them respectively as\nRelativistic GANs (RGANs)\
    \ and Relativistic average GANs (RaGANs). We show that\nIPM-based GANs are a subset\
    \ of RGANs which use the identity function.\n  Empirically, we observe that 1)\
    \ RGANs and RaGANs are significantly more\nstable and generate higher quality\
    \ data samples than their non-relativistic\ncounterparts, 2) Standard RaGAN with\
    \ gradient penalty generate data of better\nquality than WGAN-GP while only requiring\
    \ a single discriminator update per\ngenerator update (reducing the time taken\
    \ for reaching the state-of-the-art by\n400%), and 3) RaGANs are able to generate\
    \ plausible high resolutions images\n(256x256) from a very small sample (N=2011),\
    \ while GAN and LSGAN cannot; these\nimages are of significantly better quality\
    \ than the ones generated by WGAN-GP\nand SGAN with spectral normalization."
  arxivId: '1807.00734'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CR
  - stat.ML
  authors: Alexia Jolicoeur-Martineau
  created_at: '2025-01-11T08:07:25.237609'
  issue_number: 939
  issue_url: https://github.com/dmarx/papers-feed/issues/939
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-11T08:07:25.239318'
  last_visited: '2025-01-12T17:59:03.697Z'
  main_tex_file: null
  published_date: '2018-07-02T15:11:23Z'
  state: open
  title: 'The relativistic discriminator: a key element missing from standard GAN'
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/1807.00734
'1810.00363':
  abstract: 'We propose a new point of view for regularizing deep neural networks
    by using

    the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm

    cannot be computed, it admits upper and lower approximations leading to various

    practical strategies. Specifically, this perspective (i) provides a common

    umbrella for many existing regularization principles, including spectral norm

    and gradient penalties, or adversarial training, (ii) leads to new effective

    regularization penalties, and (iii) suggests hybrid strategies combining lower

    and upper bounds to get better approximations of the RKHS norm. We

    experimentally show this approach to be effective when learning on small

    datasets, or to obtain adversarially robust models.'
  arxivId: '1810.00363'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Alberto Bietti, Grégoire Mialon, Dexiong Chen, Julien Mairal
  created_at: '2025-01-04T14:49:18.222858'
  issue_number: 478
  issue_url: https://github.com/dmarx/papers-feed/issues/478
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:18.223709'
  last_visited: '2024-12-29T10:35:11.499Z'
  main_tex_file: null
  published_date: '2018-09-30T11:40:59Z'
  state: open
  title: A Kernel Perspective for Regularizing Deep Neural Networks
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/1810.00363
'1810.01588':
  abstract: 'Interpreting the prediction mechanism of complex models is currently
    one of

    the most important tasks in the machine learning field, especially with layered

    neural networks, which have achieved high predictive performance with various

    practical data sets. To reveal the global structure of a trained neural network

    in an interpretable way, a series of clustering methods have been proposed,

    which decompose the units into clusters according to the similarity of their

    inference roles. The main problems in these studies were that (1) we have no

    prior knowledge about the optimal resolution for the decomposition, or the

    appropriate number of clusters, and (2) there was no method with which to

    acquire knowledge about whether the outputs of each cluster have a positive or

    negative correlation with the input and output dimension values. In this paper,

    to solve these problems, we propose a method for obtaining a hierarchical

    modular representation of a layered neural network. The application of a

    hierarchical clustering method to a trained network reveals a tree-structured

    relationship among hidden layer units, based on their feature vectors defined

    by their correlation with the input and output dimension values.'
  arxivId: '1810.01588'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Chihiro Watanabe
  created_at: '2025-01-04T15:03:30.855575'
  issue_number: 230
  issue_url: https://github.com/dmarx/papers-feed/issues/230
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:03:30.857351'
  last_visited: '2024-12-24T03:07:38.277Z'
  main_tex_file: null
  published_date: '2018-10-03T05:38:26Z'
  state: open
  title: "Interpreting Layered Neural Networks via Hierarchical Modular\n  Representation"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1810.01588
'1904.08779':
  abstract: 'We present SpecAugment, a simple data augmentation method for speech

    recognition. SpecAugment is applied directly to the feature inputs of a neural

    network (i.e., filter bank coefficients). The augmentation policy consists of

    warping the features, masking blocks of frequency channels, and masking blocks

    of time steps. We apply SpecAugment on Listen, Attend and Spell networks for

    end-to-end speech recognition tasks. We achieve state-of-the-art performance on

    the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.

    On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language

    model, and 5.8% WER with shallow fusion with a language model. This compares to

    the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we

    achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5''00 test set

    without the use of a language model, and 6.8%/14.1% with shallow fusion, which

    compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.'
  arxivId: '1904.08779'
  arxiv_tags:
  - eess.AS
  - cs.CL
  - cs.LG
  - cs.SD
  - stat.ML
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2025-01-05T18:41:07.015765'
  issue_number: 60
  issue_url: https://github.com/dmarx/papers-feed/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-17T14:41:05.563Z'
  main_tex_file: null
  published_date: '2019-04-18T17:53:38Z'
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1904.08779
'1906.01563':
  abstract: 'Even though neural networks enjoy widespread use, they still struggle
    to

    learn the basic laws of physics. How might we endow them with better inductive

    biases? In this paper, we draw inspiration from Hamiltonian mechanics to train

    models that learn and respect exact conservation laws in an unsupervised

    manner. We evaluate our models on problems where conservation of energy is

    important, including the two-body problem and pixel observations of a pendulum.

    Our model trains faster and generalizes better than a regular neural network.

    An interesting side effect is that our model is perfectly reversible in time.'
  arxivId: '1906.01563'
  arxiv_tags:
  - cs.NE
  authors: Sam Greydanus, Misko Dzamba, Jason Yosinski
  created_at: '2025-01-05T08:23:32.513834'
  issue_number: 216
  issue_url: https://github.com/dmarx/papers-feed/issues/216
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:49:04.570Z'
  main_tex_file: null
  published_date: '2019-06-04T16:27:55Z'
  state: open
  title: Hamiltonian Neural Networks
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1906.01563
'1906.04358':
  abstract: 'Not all neural network architectures are created equal, some perform
    much

    better than others for certain tasks. But how important are the weight

    parameters of a neural network compared to its architecture? In this work, we

    question to what extent neural network architectures alone, without learning

    any weight parameters, can encode solutions for a given task. We propose a

    search method for neural network architectures that can already perform a task

    without any explicit weight training. To evaluate these networks, we populate

    the connections with a single shared weight parameter sampled from a uniform

    random distribution, and measure the expected performance. We demonstrate that

    our method can find minimal neural network architectures that can perform

    several reinforcement learning tasks without weight training. On a supervised

    learning domain, we find network architectures that achieve much higher than

    chance accuracy on MNIST using random weights. Interactive version of this

    paper at https://weightagnostic.github.io/'
  arxivId: '1906.04358'
  arxiv_tags:
  - cs.LG
  - cs.NE
  - stat.ML
  authors: Adam Gaier, David Ha
  created_at: '2025-01-04T15:03:24.864204'
  issue_number: 235
  issue_url: https://github.com/dmarx/papers-feed/issues/235
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:03:24.866258'
  last_visited: '2024-12-24T03:27:56.957Z'
  main_tex_file: null
  published_date: '2019-06-11T02:40:11Z'
  state: open
  title: Weight Agnostic Neural Networks
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1906.04358
'1906.05433':
  abstract: 'Climate change is one of the greatest challenges facing humanity, and
    we, as

    machine learning experts, may wonder how we can help. Here we describe how

    machine learning can be a powerful tool in reducing greenhouse gas emissions

    and helping society adapt to a changing climate. From smart grids to disaster

    management, we identify high impact problems where existing gaps can be filled

    by machine learning, in collaboration with other fields. Our recommendations

    encompass exciting research questions as well as promising business

    opportunities. We call on the machine learning community to join the global

    effort against climate change.'
  arxivId: '1906.05433'
  arxiv_tags:
  - cs.CY
  - cs.AI
  - cs.LG
  - stat.ML
  authors: David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre
    Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
    Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik
    Mukkavilli, Konrad P. Kording, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John
    C. Platt, Felix Creutzig, Jennifer Chayes, Yoshua Bengio
  created_at: '2025-01-05T08:24:41.512078'
  issue_number: 111
  issue_url: https://github.com/dmarx/papers-feed/issues/111
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:23:13.395Z'
  main_tex_file: null
  published_date: '2019-06-10T17:51:47Z'
  state: open
  title: Tackling Climate Change with Machine Learning
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1906.05433
'1912.02757':
  abstract: 'Deep ensembles have been empirically shown to be a promising approach
    for

    improving accuracy, uncertainty and out-of-distribution robustness of deep

    learning models. While deep ensembles were theoretically motivated by the

    bootstrap, non-bootstrap ensembles trained with just random initialization also

    perform well in practice, which suggests that there could be other explanations

    for why deep ensembles work well. Bayesian neural networks, which learn

    distributions over the parameters of the network, are theoretically

    well-motivated by Bayesian principles, but do not perform as well as deep

    ensembles in practice, particularly under dataset shift. One possible

    explanation for this gap between theory and practice is that popular scalable

    variational Bayesian methods tend to focus on a single mode, whereas deep

    ensembles tend to explore diverse modes in function space. We investigate this

    hypothesis by building on recent work on understanding the loss landscape of

    neural networks and adding our own exploration to measure the similarity of

    functions in the space of predictions. Our results show that random

    initializations explore entirely different modes, while functions along an

    optimization trajectory or sampled from the subspace thereof cluster within a

    single mode predictions-wise, while often deviating significantly in the weight

    space. Developing the concept of the diversity--accuracy plane, we show that

    the decorrelation power of random initializations is unmatched by popular

    subspace sampling methods. Finally, we evaluate the relative effects of

    ensembling, subspace based methods and ensembles of subspace based methods, and

    the experimental results validate our hypothesis.'
  arxivId: '1912.02757'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan
  created_at: '2025-01-05T08:23:23.616987'
  issue_number: 224
  issue_url: https://github.com/dmarx/papers-feed/issues/224
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:23.618098'
  last_visited: '2024-12-24T03:01:38.243Z'
  main_tex_file: null
  published_date: '2019-12-05T17:48:18Z'
  state: open
  title: 'Deep Ensembles: A Loss Landscape Perspective'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1912.02757
'2001.04063':
  abstract: 'This paper presents a new sequence-to-sequence pre-training model called

    ProphetNet, which introduces a novel self-supervised objective named future

    n-gram prediction and the proposed n-stream self-attention mechanism. Instead

    of optimizing one-step-ahead prediction in the traditional sequence-to-sequence

    model, the ProphetNet is optimized by n-step ahead prediction that predicts the

    next n tokens simultaneously based on previous context tokens at each time

    step. The future n-gram prediction explicitly encourages the model to plan for

    the future tokens and prevent overfitting on strong local correlations. We

    pre-train ProphetNet using a base scale dataset (16GB) and a large-scale

    dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail,

    Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question

    generation tasks. Experimental results show that ProphetNet achieves new

    state-of-the-art results on all these datasets compared to the models using the

    same scale pre-training corpus.'
  arxivId: '2001.04063'
  arxiv_tags:
  - cs.CL
  authors: Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,
    Ruofei Zhang, Ming Zhou
  created_at: '2025-01-04T15:03:09.857516'
  issue_number: 286
  issue_url: https://github.com/dmarx/papers-feed/issues/286
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:03:09.858274'
  last_visited: '2024-12-26T17:17:59.219Z'
  main_tex_file: null
  published_date: '2020-01-13T05:12:38Z'
  state: open
  title: "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2001.04063
'2001.04451':
  abstract: 'Large Transformer models routinely achieve state-of-the-art results on
    a

    number of tasks but training these models can be prohibitively costly,

    especially on long sequences. We introduce two techniques to improve the

    efficiency of Transformers. For one, we replace dot-product attention by one

    that uses locality-sensitive hashing, changing its complexity from O($L^2$) to

    O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use

    reversible residual layers instead of the standard residuals, which allows

    storing activations only once in the training process instead of $N$ times,

    where $N$ is the number of layers. The resulting model, the Reformer, performs

    on par with Transformer models while being much more memory-efficient and much

    faster on long sequences.'
  arxivId: '2001.04451'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - stat.ML
  authors: Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya
  created_at: '2025-01-05T18:41:03.688230'
  issue_number: 801
  issue_url: https://github.com/dmarx/papers-feed/issues/801
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T20:09:30.079520'
  last_visited: '2025-01-05T20:08:29.858000+00:00'
  main_tex_file: null
  published_date: '2020-01-13T18:38:28Z'
  state: open
  title: 'Reformer: The Efficient Transformer'
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2001.04451
'2006.11120':
  abstract: 'A basic operation in Convolutional Neural Networks (CNNs) is spatial
    resizing

    of feature maps. This is done either by strided convolution (donwscaling) or

    transposed convolution (upscaling). Such operations are limited to a fixed

    filter moving at predetermined integer steps (strides). Spatial sizes of

    consecutive layers are related by integer scale factors, predetermined at

    architectural design, and remain fixed throughout training and inference time.

    We propose a generalization of the common Conv-layer, from a discrete layer to

    a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by

    representing the filter as a learned continuous function over sub-pixel

    coordinates. This allows learnable and principled resizing of feature maps, to

    any size, dynamically and consistently across scales. Once trained, the CC

    layer can be used to output any scale/size chosen at inference time. The scale

    can be non-integer and differ between the axes. CC gives rise to new freedoms

    for architectural design, such as dynamic layer shapes at inference time, or

    gradual architectures where the size changes by a small factor at each layer.

    This gives rise to many desired CNN properties, new architectural design

    capabilities, and useful applications. We further show that current Conv-layers

    suffer from inherent misalignments, which are ameliorated by CC layers.'
  arxivId: '2006.11120'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - stat.ML
  authors: Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani
  created_at: '2025-01-04T15:03:21.951419'
  issue_number: 264
  issue_url: https://github.com/dmarx/papers-feed/issues/264
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-25T05:29:13.001Z'
  main_tex_file: null
  published_date: '2020-06-19T13:16:06Z'
  state: open
  title: From Discrete to Continuous Convolution Layers
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2006.11120
'2006.14769':
  abstract: 'We present the Supermasks in Superposition (SupSup) model, capable of

    sequentially learning thousands of tasks without catastrophic forgetting. Our

    approach uses a randomly initialized, fixed base network and for each task

    finds a subnetwork (supermask) that achieves good performance. If task identity

    is given at test time, the correct subnetwork can be retrieved with minimal

    memory usage. If not provided, SupSup can infer the task using gradient-based

    optimization to find a linear superposition of learned supermasks which

    minimizes the output entropy. In practice we find that a single gradient step

    is often sufficient to identify the correct mask, even among 2500 tasks. We

    also showcase two promising extensions. First, SupSup models can be trained

    entirely without task identity information, as they may detect when they are

    uncertain about new data and allocate an additional supermask for the new

    training distribution. Finally the entire, growing set of supermasks can be

    stored in a constant-sized reservoir by implicitly storing them as attractors

    in a fixed-sized Hopfield network.'
  arxivId: '2006.14769'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad
    Rastegari, Jason Yosinski, Ali Farhadi
  created_at: '2025-01-05T08:23:41.553406'
  issue_number: 210
  issue_url: https://github.com/dmarx/papers-feed/issues/210
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T02:37:38.778Z'
  main_tex_file: null
  published_date: '2020-06-26T03:16:44Z'
  state: open
  title: Supermasks in Superposition
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2006.14769
'2009.10195':
  abstract: 'Models that perform well on a training domain often fail to generalize
    to

    out-of-domain (OOD) examples. Data augmentation is a common method used to

    prevent overfitting and improve OOD generalization. However, in natural

    language, it is difficult to generate new examples that stay on the underlying

    data manifold. We introduce SSMBA, a data augmentation method for generating

    synthetic training examples by using a pair of corruption and reconstruction

    functions to move randomly on a data manifold. We investigate the use of SSMBA

    in the natural language domain, leveraging the manifold assumption to

    reconstruct corrupted text with masked language models. In experiments on

    robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently

    outperforms existing data augmentation methods and baseline models on both

    in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews,

    1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.'
  arxivId: '2009.10195'
  arxiv_tags:
  - cs.CL
  - cs.LG
  - stat.ML
  authors: Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi
  created_at: '2025-01-05T08:24:59.493973'
  issue_number: 97
  issue_url: https://github.com/dmarx/papers-feed/issues/97
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T18:19:38.104Z'
  main_tex_file: null
  published_date: '2020-09-21T22:02:33Z'
  state: open
  title: "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n\
    \  Out-of-Domain Robustness"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2009.10195
'2010.11929':
  abstract: 'While the Transformer architecture has become the de-facto standard for

    natural language processing tasks, its applications to computer vision remain

    limited. In vision, attention is either applied in conjunction with

    convolutional networks, or used to replace certain components of convolutional

    networks while keeping their overall structure in place. We show that this

    reliance on CNNs is not necessary and a pure transformer applied directly to

    sequences of image patches can perform very well on image classification tasks.

    When pre-trained on large amounts of data and transferred to multiple mid-sized

    or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision

    Transformer (ViT) attains excellent results compared to state-of-the-art

    convolutional networks while requiring substantially fewer computational

    resources to train.'
  arxivId: '2010.11929'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.LG
  authors: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
  created_at: '2025-01-04T15:01:58.103740'
  issue_number: 786
  issue_url: https://github.com/dmarx/papers-feed/issues/786
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:01:58.106137'
  last_visited: '2025-01-04T15:01:19.005Z'
  main_tex_file: null
  published_date: '2020-10-22T17:55:59Z'
  state: open
  title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at\n \
    \ Scale"
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2010.11929
'2012.13255':
  abstract: 'Although pretrained language models can be fine-tuned to produce

    state-of-the-art results for a very wide range of language understanding tasks,

    the dynamics of this process are not well understood, especially in the low

    data regime. Why can we use relatively vanilla gradient descent algorithms

    (e.g., without strong regularization) to tune a model with hundreds of millions

    of parameters on datasets with only hundreds or thousands of labeled examples?

    In this paper, we argue that analyzing fine-tuning through the lens of

    intrinsic dimension provides us with empirical and theoretical intuitions to

    explain this remarkable phenomenon. We empirically show that common pre-trained

    models have a very low intrinsic dimension; in other words, there exists a low

    dimension reparameterization that is as effective for fine-tuning as the full

    parameter space. For example, by optimizing only 200 trainable parameters

    randomly projected back into the full space, we can tune a RoBERTa model to

    achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we

    empirically show that pre-training implicitly minimizes intrinsic dimension

    and, perhaps surprisingly, larger models tend to have lower intrinsic dimension

    after a fixed number of pre-training updates, at least in part explaining their

    extreme effectiveness. Lastly, we connect intrinsic dimensionality with low

    dimensional task representations and compression based generalization bounds to

    provide intrinsic-dimension-based generalization bounds that are independent of

    the full parameter count.'
  arxivId: '2012.13255'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta
  created_at: '2025-01-05T08:23:26.492388'
  issue_number: 222
  issue_url: https://github.com/dmarx/papers-feed/issues/222
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:41.555214'
  last_visited: '2024-12-24T02:33:31.614000+00:00'
  main_tex_file: null
  published_date: '2020-12-22T07:42:30Z'
  state: open
  title: "Intrinsic Dimensionality Explains the Effectiveness of Language Model\n\
    \  Fine-Tuning"
  total_reading_time_seconds: 7
  url: https://arxiv.org/abs/2012.13255
'2103.13413':
  abstract: 'We introduce dense vision transformers, an architecture that leverages
    vision

    transformers in place of convolutional networks as a backbone for dense

    prediction tasks. We assemble tokens from various stages of the vision

    transformer into image-like representations at various resolutions and

    progressively combine them into full-resolution predictions using a

    convolutional decoder. The transformer backbone processes representations at a

    constant and relatively high resolution and has a global receptive field at

    every stage. These properties allow the dense vision transformer to provide

    finer-grained and more globally coherent predictions when compared to

    fully-convolutional networks. Our experiments show that this architecture

    yields substantial improvements on dense prediction tasks, especially when a

    large amount of training data is available. For monocular depth estimation, we

    observe an improvement of up to 28% in relative performance when compared to a

    state-of-the-art fully-convolutional network. When applied to semantic

    segmentation, dense vision transformers set a new state of the art on ADE20K

    with 49.02% mIoU. We further show that the architecture can be fine-tuned on

    smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets

    the new state of the art. Our models are available at

    https://github.com/intel-isl/DPT.'
  arxivId: '2103.13413'
  arxiv_tags:
  - cs.CV
  authors: René Ranftl, Alexey Bochkovskiy, Vladlen Koltun
  created_at: '2025-01-04T14:48:27.532008'
  issue_number: 785
  issue_url: https://github.com/dmarx/papers-feed/issues/785
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:01:58.107814'
  last_visited: '2025-01-04T14:48:49.256000+00:00'
  main_tex_file: null
  published_date: '2021-03-24T18:01:17Z'
  state: open
  title: Vision Transformers for Dense Prediction
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2103.13413
'2105.01601':
  abstract: 'Convolutional Neural Networks (CNNs) are the go-to model for computer
    vision.

    Recently, attention-based networks, such as the Vision Transformer, have also

    become popular. In this paper we show that while convolutions and attention are

    both sufficient for good performance, neither of them are necessary. We present

    MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).

    MLP-Mixer contains two types of layers: one with MLPs applied independently to

    image patches (i.e. "mixing" the per-location features), and one with MLPs

    applied across patches (i.e. "mixing" spatial information). When trained on

    large datasets, or with modern regularization schemes, MLP-Mixer attains

    competitive scores on image classification benchmarks, with pre-training and

    inference cost comparable to state-of-the-art models. We hope that these

    results spark further research beyond the realms of well established CNNs and

    Transformers.'
  arxivId: '2105.01601'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.LG
  authors: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
    Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
    Uszkoreit, Mario Lucic, Alexey Dosovitskiy
  created_at: '2025-01-05T20:13:29.392363'
  issue_number: 822
  issue_url: https://github.com/dmarx/papers-feed/issues/822
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T20:13:29.393146'
  last_visited: '2025-01-05T20:12:13.323Z'
  main_tex_file: null
  published_date: '2021-05-04T16:17:21Z'
  state: open
  title: 'MLP-Mixer: An all-MLP Architecture for Vision'
  total_reading_time_seconds: 22
  url: https://arxiv.org/abs/2105.01601
'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  arxiv_tags:
  - cs.DC
  - cs.LG
  - cs.PL
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2025-01-04T14:48:39.270195'
  issue_number: 49
  issue_url: https://github.com/dmarx/papers-feed/issues/49
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T18:41:15.658266'
  last_visited: '2024-12-16T05:53:22.033Z'
  main_tex_file: null
  published_date: '2021-05-12T15:13:43Z'
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_seconds: 158
  url: https://arxiv.org/abs/2105.05720
'2105.08050':
  abstract: 'Transformers have become one of the most important architectural innovations

    in deep learning and have enabled many breakthroughs over the past few years.

    Here we propose a simple network architecture, gMLP, based on MLPs with gating,

    and show that it can perform as well as Transformers in key language and vision

    applications. Our comparisons show that self-attention is not critical for

    Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model

    achieves parity with Transformers on pretraining perplexity and is better on

    some downstream NLP tasks. On finetuning tasks where gMLP performs worse,

    making the gMLP model substantially larger can close the gap with Transformers.

    In general, our experiments show that gMLP can scale as well as Transformers

    over increased data and compute.'
  arxivId: '2105.08050'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.CV
  authors: Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le
  created_at: '2025-01-05T18:41:00.743341'
  issue_number: 817
  issue_url: https://github.com/dmarx/papers-feed/issues/817
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T23:08:39.880665'
  last_visited: '2025-01-05T23:07:26.844000+00:00'
  main_tex_file: null
  published_date: '2021-05-17T17:55:04Z'
  state: open
  title: Pay Attention to MLPs
  total_reading_time_seconds: 45
  url: https://arxiv.org/abs/2105.08050
'2106.10165':
  abstract: 'This book develops an effective theory approach to understanding deep
    neural

    networks of practical relevance. Beginning from a first-principles

    component-level picture of networks, we explain how to determine an accurate

    description of the output of trained networks by solving layer-to-layer

    iteration equations and nonlinear learning dynamics. A main result is that the

    predictions of networks are described by nearly-Gaussian distributions, with

    the depth-to-width aspect ratio of the network controlling the deviations from

    the infinite-width Gaussian description. We explain how these effectively-deep

    networks learn nontrivial representations from training and more broadly

    analyze the mechanism of representation learning for nonlinear models. From a

    nearly-kernel-methods perspective, we find that the dependence of such models''

    predictions on the underlying learning algorithm can be expressed in a simple

    and universal way. To obtain these results, we develop the notion of

    representation group flow (RG flow) to characterize the propagation of signals

    through the network. By tuning networks to criticality, we give a practical

    solution to the exploding and vanishing gradient problem. We further explain

    how RG flow leads to near-universal behavior and lets us categorize networks

    built from different activation functions into universality classes.

    Altogether, we show that the depth-to-width ratio governs the effective model

    complexity of the ensemble of trained networks. By using information-theoretic

    techniques, we estimate the optimal aspect ratio at which we expect the network

    to be practically most useful and show how residual connections can be used to

    push this scale to arbitrary depths. With these tools, we can learn in detail

    about the inductive bias of architectures, hyperparameters, and optimizers.'
  arxivId: '2106.10165'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - hep-th
  - stat.ML
  authors: Daniel A. Roberts, Sho Yaida, Boris Hanin
  created_at: '2025-01-04T14:49:06.220612'
  issue_number: 523
  issue_url: https://github.com/dmarx/papers-feed/issues/523
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T22:46:13.679Z'
  main_tex_file: null
  published_date: '2021-06-18T15:00:00Z'
  state: open
  title: The Principles of Deep Learning Theory
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2106.10165
'2107.11817':
  abstract: "More transformer blocks with residual connections have recently achieved\n\
    impressive results on various tasks. To achieve better performance with fewer\n\
    trainable parameters, recent methods are proposed to go shallower by parameter\n\
    sharing or model compressing along with the depth. However, weak modeling\ncapacity\
    \ limits their performance. Contrastively, going wider by inducing more\ntrainable\
    \ matrixes and parameters would produce a huge model requiring advanced\nparallelism\
    \ to train and inference.\n  In this paper, we propose a parameter-efficient framework,\
    \ going wider\ninstead of deeper. Specially, following existing works, we adapt\
    \ parameter\nsharing to compress along depth. But, such deployment would limit\
    \ the\nperformance. To maximize modeling capacity, we scale along model width\
    \ by\nreplacing feed-forward network (FFN) with mixture-of-experts (MoE). Across\n\
    transformer blocks, instead of sharing normalization layers, we propose to use\n\
    individual layernorms to transform various semantic representations in a more\n\
    parameter-efficient way. To evaluate our plug-and-run framework, we design\nWideNet\
    \ and conduct comprehensive experiments on popular computer vision and\nnatural\
    \ language processing benchmarks. On ImageNet-1K, our best model\noutperforms\
    \ Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable\nparameters.\
    \ Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can\nstill surpass\
    \ ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four\nnatural language\
    \ processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on\naverage and\
    \ surpass BERT using factorized embedding parameterization by $0.8\\%$\nwith fewer\
    \ parameters."
  arxivId: '2107.11817'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, Yang You
  created_at: '2025-01-05T18:40:55.338171'
  issue_number: 802
  issue_url: https://github.com/dmarx/papers-feed/issues/802
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T18:40:55.338983'
  last_visited: '2025-01-05T18:44:31.871Z'
  main_tex_file: null
  published_date: '2021-07-25T14:44:24Z'
  state: open
  title: Go Wider Instead of Deeper
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2107.11817
'2110.09456':
  abstract: 'During pretraining, the Pre-LayerNorm transformer suffers from a gradient

    magnitude mismatch: gradients at early layers are much larger than at later

    layers. These issues can be alleviated by our proposed NormFormer architecture,

    which adds three normalization operations to each layer: a Layer Norm after

    self attention, head-wise scaling of self-attention outputs, and a Layer Norm

    after the first fully connected layer. The extra operations incur negligible

    compute cost (+0.4% parameter increase), but improve pretraining perplexity and

    downstream task performance for both causal and masked language models ranging

    from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on

    top of our strongest 1.3B parameter baseline can reach equal perplexity 24%

    faster, or converge 0.27 perplexity better in the same compute budget. This

    model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked

    language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on

    average. Code to train NormFormer models is available in fairseq

    https://github.com/pytorch/fairseq/tree/main/examples/normformer .'
  arxivId: '2110.09456'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Sam Shleifer, Jason Weston, Myle Ott
  created_at: '2025-01-07T23:18:44.185821'
  issue_number: 850
  issue_url: https://github.com/dmarx/papers-feed/issues/850
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-07T23:18:44.187174'
  last_visited: '2025-01-07T23:17:21.742Z'
  main_tex_file: null
  published_date: '2021-10-18T16:47:45Z'
  state: open
  title: 'NormFormer: Improved Transformer Pretraining with Extra Normalization'
  total_reading_time_seconds: 36
  url: https://arxiv.org/abs/2110.09456
'2111.11418':
  abstract: 'Transformers have shown great potential in computer vision tasks. A common

    belief is their attention-based token mixer module contributes most to their

    competence. However, recent works show the attention-based module in

    Transformers can be replaced by spatial MLPs and the resulted models still

    perform quite well. Based on this observation, we hypothesize that the general

    architecture of the Transformers, instead of the specific token mixer module,

    is more essential to the model''s performance. To verify this, we deliberately

    replace the attention module in Transformers with an embarrassingly simple

    spatial pooling operator to conduct only basic token mixing. Surprisingly, we

    observe that the derived model, termed as PoolFormer, achieves competitive

    performance on multiple computer vision tasks. For example, on ImageNet-1K,

    PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision

    Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with

    35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of

    PoolFormer verifies our hypothesis and urges us to initiate the concept of

    "MetaFormer", a general architecture abstracted from Transformers without

    specifying the token mixer. Based on the extensive experiments, we argue that

    MetaFormer is the key player in achieving superior results for recent

    Transformer and MLP-like models on vision tasks. This work calls for more

    future research dedicated to improving MetaFormer instead of focusing on the

    token mixer modules. Additionally, our proposed PoolFormer could serve as a

    starting baseline for future MetaFormer architecture design. Code is available

    at https://github.com/sail-sg/poolformer.'
  arxivId: '2111.11418'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.LG
  authors: Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi
    Feng, Shuicheng Yan
  created_at: '2025-01-05T20:26:49.248808'
  issue_number: 829
  issue_url: https://github.com/dmarx/papers-feed/issues/829
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T20:26:49.250252'
  last_visited: '2025-01-05T20:25:32.615Z'
  main_tex_file: null
  published_date: '2021-11-22T18:52:03Z'
  state: open
  title: MetaFormer Is Actually What You Need for Vision
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2111.11418
'2112.04215':
  abstract: 'Self-supervised models have been shown to produce comparable or better
    visual

    representations than their supervised counterparts when trained offline on

    unlabeled data at scale. However, their efficacy is catastrophically reduced in

    a Continual Learning (CL) scenario where data is presented to the model

    sequentially. In this paper, we show that self-supervised loss functions can be

    seamlessly converted into distillation mechanisms for CL by adding a predictor

    network that maps the current state of the representations to their past state.

    This enables us to devise a framework for Continual self-supervised visual

    representation Learning that (i) significantly improves the quality of the

    learned representations, (ii) is compatible with several state-of-the-art

    self-supervised objectives, and (iii) needs little to no hyperparameter tuning.

    We demonstrate the effectiveness of our approach empirically by training six

    popular self-supervised models in various CL settings.'
  arxivId: '2112.04215'
  arxiv_tags:
  - cs.CV
  - cs.LG
  authors: Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci,
    Karteek Alahari, Julien Mairal
  created_at: '2025-01-04T14:49:51.228547'
  issue_number: 407
  issue_url: https://github.com/dmarx/papers-feed/issues/407
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:29:58.974Z'
  main_tex_file: null
  published_date: '2021-12-08T10:39:13Z'
  state: open
  title: Self-Supervised Models are Continual Learners
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2112.04215
'2112.14569':
  abstract: 'Transformers are responsible for the vast majority of recent advances
    in

    natural language processing. The majority of practical natural language

    processing applications of these models are typically enabled through transfer

    learning. This paper studies if corpus-specific tokenization used for

    fine-tuning improves the resulting performance of the model. Through a series

    of experiments, we demonstrate that such tokenization combined with the

    initialization and fine-tuning strategy for the vocabulary tokens speeds up the

    transfer and boosts the performance of the fine-tuned model. We call this

    aspect of transfer facilitation vocabulary transfer.'
  arxivId: '2112.14569'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  - 68T50, 91F20
  - I.2.7
  authors: Vladislav Mosin, Igor Samenko, Alexey Tikhonov, Borislav Kozlovskii, Ivan
    P. Yamshchikov
  created_at: '2025-01-10T01:58:58.948611'
  issue_number: 878
  issue_url: https://github.com/dmarx/papers-feed/issues/878
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T02:01:59.118581'
  last_visited: '2025-01-10T01:59:25.704Z'
  main_tex_file: null
  published_date: '2021-12-29T14:22:42Z'
  state: open
  title: 'Fine-Tuning Transformers: Vocabulary Transfer'
  total_reading_time_seconds: 28
  url: https://arxiv.org/abs/2112.14569
'2203.02155':
  abstract: 'Making language models bigger does not inherently make them better at

    following a user''s intent. For example, large language models can generate

    outputs that are untruthful, toxic, or simply not helpful to the user. In other

    words, these models are not aligned with their users. In this paper, we show an

    avenue for aligning language models with user intent on a wide range of tasks

    by fine-tuning with human feedback. Starting with a set of labeler-written

    prompts and prompts submitted through the OpenAI API, we collect a dataset of

    labeler demonstrations of the desired model behavior, which we use to fine-tune

    GPT-3 using supervised learning. We then collect a dataset of rankings of model

    outputs, which we use to further fine-tune this supervised model using

    reinforcement learning from human feedback. We call the resulting models

    InstructGPT. In human evaluations on our prompt distribution, outputs from the

    1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,

    despite having 100x fewer parameters. Moreover, InstructGPT models show

    improvements in truthfulness and reductions in toxic output generation while

    having minimal performance regressions on public NLP datasets. Even though

    InstructGPT still makes simple mistakes, our results show that fine-tuning with

    human feedback is a promising direction for aligning language models with human

    intent.'
  arxivId: '2203.02155'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela
    Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman,
    Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter
    Welinder, Paul Christiano, Jan Leike, Ryan Lowe
  created_at: '2025-01-06T23:16:42.091505'
  issue_number: 844
  issue_url: https://github.com/dmarx/papers-feed/issues/844
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-06T23:16:42.092303'
  last_visited: '2025-01-06T23:15:51.499Z'
  main_tex_file: null
  published_date: '2022-03-04T07:04:42Z'
  state: open
  title: Training language models to follow instructions with human feedback
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2203.02155
'2204.00595':
  abstract: 'Large neural networks excel in many domains, but they are expensive to
    train

    and fine-tune. A popular approach to reduce their compute or memory

    requirements is to replace dense weight matrices with structured ones (e.g.,

    sparse, low-rank, Fourier transform). These methods have not seen widespread

    adoption (1) in end-to-end training due to unfavorable efficiency--quality

    tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable

    algorithms to approximate a given dense weight matrix. To address these issues,

    we propose a class of matrices (Monarch) that is hardware-efficient (they are

    parameterized as products of two block-diagonal matrices for better hardware

    utilization) and expressive (they can represent many commonly used transforms).

    Surprisingly, the problem of approximating a dense weight matrix with a Monarch

    matrix, though nonconvex, has an analytical optimal solution. These properties

    of Monarch matrices unlock new ways to train and fine-tune sparse and dense

    models. We empirically validate that Monarch can achieve favorable

    accuracy-efficiency tradeoffs in several end-to-end sparse training

    applications: speeding up ViT and GPT-2 training on ImageNet classification and

    Wikitext-103 language modeling by 2x with comparable model quality, and

    reducing the error on PDE solving and MRI reconstruction tasks by 40%. In

    sparse-to-dense training, with a simple technique called "reverse

    sparsification," Monarch matrices serve as a useful intermediate representation

    to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The

    same technique brings 23% faster BERT pretraining than even the very optimized

    implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse

    fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds

    up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.'
  arxivId: '2204.00595'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
    Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré
  created_at: '2025-01-04T15:02:51.854751'
  issue_number: 335
  issue_url: https://github.com/dmarx/papers-feed/issues/335
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:51.856832'
  last_visited: '2024-12-28T06:07:58.885Z'
  main_tex_file: null
  published_date: '2022-04-01T17:37:29Z'
  state: open
  title: "Monarch: Expressive Structured Matrices for Efficient and Accurate\n  Training"
  total_reading_time_seconds: 35
  url: https://arxiv.org/abs/2204.00595
'2205.12381':
  abstract: 'How can we train an assistive human-machine interface (e.g., an

    electromyography-based limb prosthesis) to translate a user''s raw command

    signals into the actions of a robot or computer when there is no prior mapping,

    we cannot ask the user for supervision in the form of action labels or reward

    feedback, and we do not have prior knowledge of the tasks the user is trying to

    accomplish? The key idea in this paper is that, regardless of the task, when an

    interface is more intuitive, the user''s commands are less noisy. We formalize

    this idea as a completely unsupervised objective for optimizing interfaces: the

    mutual information between the user''s command signals and the induced state

    transitions in the environment. To evaluate whether this mutual information

    score can distinguish between effective and ineffective interfaces, we conduct

    an observational study on 540K examples of users operating various keyboard and

    eye gaze interfaces for typing, controlling simulated robots, and playing video

    games. The results show that our mutual information scores are predictive of

    the ground-truth task completion metrics in a variety of domains, with an

    average Spearman''s rank correlation of 0.43. In addition to offline evaluation

    of existing interfaces, we use our unsupervised objective to learn an interface

    from scratch: we randomly initialize the interface, have the user attempt to

    perform their desired tasks using the interface, measure the mutual information

    score, and update the interface to maximize mutual information through

    reinforcement learning. We evaluate our method through a user study with 12

    participants who perform a 2D cursor control task using a perturbed mouse, and

    an experiment with one user playing the Lunar Lander game using hand gestures.

    The results show that we can learn an interface from scratch, without any user

    supervision or prior knowledge of tasks, in under 30 minutes.'
  arxivId: '2205.12381'
  arxiv_tags:
  - cs.LG
  - cs.HC
  - cs.RO
  authors: Siddharth Reddy, Sergey Levine, Anca D. Dragan
  created_at: '2025-01-06T11:07:41.187716'
  issue_number: 838
  issue_url: https://github.com/dmarx/papers-feed/issues/838
  labels:
  - paper
  - rating:upvote
  last_read: '2025-01-06T11:07:41.188548'
  last_visited: '2025-01-06T11:04:10.770Z'
  main_tex_file: null
  published_date: '2022-05-24T21:57:18Z'
  state: open
  title: "First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual\n  Information\
    \ Maximization"
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2205.12381
'2205.13509':
  abstract: 'The presence of obstacles is intuitively expected to hinder the diffusive

    transport of micro-swimmers. However, for chiral micro-swimmers, a low density

    of obstacles near a surface can enhance their diffusive behavior, due to the

    rectification of the chiral motion by the obstacles. Here, we study numerically

    the role that disorder plays in determining the transport dynamics of chiral

    micro-swimmers on surfaces with obstacles. We consider different densities of

    regularly spaced obstacles and distinct types of disorder: noise in the

    dynamics of the micro-swimmer, quenched noise in the positions of the obstacles

    as well as obstacle size polydispersity. We show that, depending on the type

    and strength of the disorder, the presence of obstacles can either enhance or

    hinder transport, and discuss implications for the control of active transport

    in disordered media.'
  arxivId: '2205.13509'
  arxiv_tags:
  - cond-mat.soft
  - cond-mat.stat-mech
  authors: Danne M. van Roon, Giorgio Volpe, Margarida M. Telo da Gama, Nuno A. M.
    Araújo
  created_at: '2025-01-04T14:49:54.237647'
  issue_number: 405
  issue_url: https://github.com/dmarx/papers-feed/issues/405
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:54.238526'
  last_visited: '2024-12-28T07:25:12.900Z'
  main_tex_file: null
  published_date: '2022-05-26T17:22:50Z'
  state: open
  title: "The role of disorder in the motion of chiral swimmers in the presence of\n\
    \  obstacles"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2205.13509
'2207.10342':
  abstract: 'Prompted models have demonstrated impressive few-shot learning abilities.

    Repeated interactions at test-time with a single model, or the composition of

    multiple models together, further expands capabilities. These compositions are

    probabilistic models, and may be expressed in the language of graphical models

    with random variables whose values are complex data types such as strings.

    Cases with control flow and dynamic structure require techniques from

    probabilistic programming, which allow implementing disparate model structures

    and inference strategies in a unified language. We formalize several existing

    techniques from this perspective, including scratchpads / chain of thought,

    verifiers, STaR, selection-inference, and tool use. We refer to the resulting

    programs as language model cascades.'
  arxivId: '2207.10342'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael
    Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein,
    Kevin Murphy, Charles Sutton
  created_at: '2025-01-04T14:49:24.257305'
  issue_number: 415
  issue_url: https://github.com/dmarx/papers-feed/issues/415
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:42.250385'
  last_visited: '2024-12-28T09:09:37.944Z'
  main_tex_file: null
  published_date: '2022-07-21T07:35:18Z'
  state: open
  title: Language Model Cascades
  total_reading_time_seconds: 51
  url: https://arxiv.org/abs/2207.10342
'2208.02554':
  abstract: 'Working within specific NLP subdomains presents significant challenges,

    primarily due to a persistent deficit of data. Stringent privacy concerns and

    limited data accessibility often drive this shortage. Additionally, the medical

    domain demands high accuracy, where even marginal improvements in model

    performance can have profound impacts. In this study, we investigate the

    potential of vocabulary transfer to enhance model performance in biomedical NLP

    tasks. Specifically, we focus on vocabulary extension, a technique that

    involves expanding the target vocabulary to incorporate domain-specific

    biomedical terms. Our findings demonstrate that vocabulary extension, leads to

    measurable improvements in both downstream model performance and inference

    time.'
  arxivId: '2208.02554'
  arxiv_tags:
  - cs.CL
  - I.2.7
  authors: Priyanka Singh, Vladislav D. Mosin, Ivan P. Yamshchikov
  created_at: '2025-01-10T01:59:01.898536'
  issue_number: 873
  issue_url: https://github.com/dmarx/papers-feed/issues/873
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T02:01:59.117896'
  last_visited: '2025-01-10T02:00:26.390000+00:00'
  main_tex_file: null
  published_date: '2022-08-04T09:53:22Z'
  state: open
  title: "Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add\n\
    \  Data"
  total_reading_time_seconds: 59
  url: https://arxiv.org/abs/2208.02554
'2208.11665':
  abstract: 'The Manifold Hypothesis is a widely accepted tenet of Machine Learning
    which

    asserts that nominally high-dimensional data are in fact concentrated near a

    low-dimensional manifold, embedded in high-dimensional space. This phenomenon

    is observed empirically in many real world situations, has led to development

    of a wide range of statistical methods in the last few decades, and has been

    suggested as a key factor in the success of modern AI technologies. We show

    that rich and sometimes intricate manifold structure in data can emerge from a

    generic and remarkably simple statistical model -- the Latent Metric Model --

    via elementary concepts such as latent variables, correlation and stationarity.

    This establishes a general statistical explanation for why the Manifold

    Hypothesis seems to hold in so many situations. Informed by the Latent Metric

    Model we derive procedures to discover and interpret the geometry of

    high-dimensional data, and explore hypotheses about the data generating

    mechanism. These procedures operate under minimal assumptions and make use of

    well known, scaleable graph-analytic algorithms.'
  arxivId: '2208.11665'
  arxiv_tags:
  - stat.ME
  - cs.LG
  - stat.ML
  - 62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30
  authors: Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy
  created_at: '2025-01-04T14:49:42.245960'
  issue_number: 452
  issue_url: https://github.com/dmarx/papers-feed/issues/452
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-29T02:26:31.276Z'
  main_tex_file: null
  published_date: '2022-08-24T17:00:16Z'
  state: open
  title: Statistical exploration of the Manifold Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2208.11665
'2209.02740':
  abstract: 'Networks of weakly coupled oscillators had a profound impact on our

    understanding of complex systems. Studies on model reconstruction from data

    have shown prevalent contributions from hypernetworks with triplet and higher

    interactions among oscillators, in spite that such models were originally

    defined as oscillator networks with pairwise interactions. Here, we show that

    hypernetworks can spontaneously emerge even in the presence of pairwise albeit

    nonlinear coupling given certain triplet frequency resonance conditions. The

    results are demonstrated in experiments with electrochemical oscillators and in

    simulations with integrate-and-fire neurons. By developing a comprehensive

    theory, we uncover the mechanism for emergent hypernetworks by identifying

    appearing and forbidden frequency resonant conditions. Furthermore, it is shown

    that microscopic linear (difference) coupling among units results in coupled

    mean fields, which have sufficient nonlinearity to facilitate hypernetworks.

    Our findings shed light on the apparent abundance of hypernetworks and provide

    a constructive way to predict and engineer their emergence.'
  arxivId: '2209.02740'
  arxiv_tags:
  - math.DS
  authors: Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu, István Z. Kiss,
    Tiago Pereira
  created_at: '2025-01-05T08:24:53.531273'
  issue_number: 103
  issue_url: https://github.com/dmarx/papers-feed/issues/103
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T05:30:04.148Z'
  main_tex_file: null
  published_date: '2022-09-06T18:02:12Z'
  state: open
  title: Emergent hypernetworks in weakly coupled oscillators
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2209.02740
'2211.14453':
  abstract: 'Spectral analysis provides one of the most effective paradigms for

    information-preserving dimensionality reduction, as simple descriptions of

    naturally occurring signals are often obtained via few terms of periodic basis

    functions. In this work, we study deep neural networks designed to harness the

    structure in frequency domain for efficient learning of long-range correlations

    in space or time: frequency-domain models (FDMs). Existing FDMs are based on

    complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform

    computation on the spectrum and input data separately. This design introduces

    considerable computational overhead: for each layer, a forward and inverse FT.

    Instead, this work introduces a blueprint for frequency domain learning through

    a single transform: transform once (T1). To enable efficient, direct learning

    in the frequency domain we derive a variance-preserving weight initialization

    scheme and investigate methods for frequency selection in reduced-order FDMs.

    Our results noticeably streamline the design process of FDMs, pruning redundant

    transforms, and leading to speedups of 3x to 10x that increase with data

    resolution and model size. We perform extensive experiments on learning the

    solution operator of spatio-temporal dynamics, including incompressible

    Navier-Stokes, turbulent flows around airfoils and high-resolution video of

    smoke. T1 models improve on the test performance of FDMs while requiring

    significantly less computation (5 hours instead of 32 for our large-scale

    experiment), with over 20% reduction in average predictive error across tasks.'
  arxivId: '2211.14453'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.SY
  - eess.SY
  authors: Michael Poli, Stefano Massaroli, Federico Berto, Jinykoo Park, Tri Dao,
    Christopher Ré, Stefano Ermon
  created_at: '2025-01-04T15:02:00.948997'
  issue_number: 404
  issue_url: https://github.com/dmarx/papers-feed/issues/404
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:00.950202'
  last_visited: '2024-12-28T07:23:40.569000+00:00'
  main_tex_file: null
  published_date: '2022-11-26T01:56:05Z'
  state: open
  title: 'Transform Once: Efficient Operator Learning in Frequency Domain'
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2211.14453
'2212.07677':
  abstract: 'At present, the mechanisms of in-context learning in Transformers are
    not

    well understood and remain mostly an intuition. In this paper, we suggest that

    training Transformers on auto-regressive objectives is closely related to

    gradient-based meta-learning formulations. We start by providing a simple

    weight construction that shows the equivalence of data transformations induced

    by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on
    a

    regression loss. Motivated by that construction, we show empirically that when

    training self-attention-only Transformers on simple regression tasks either the

    models learned by GD and Transformers show great similarity or, remarkably, the

    weights found by optimization match the construction. Thus we show how trained

    Transformers become mesa-optimizers i.e. learn models by gradient descent in

    their forward pass. This allows us, at least in the domain of regression

    problems, to mechanistically understand the inner workings of in-context

    learning in optimized Transformers. Building on this insight, we furthermore

    identify how Transformers surpass the performance of plain gradient descent by

    learning an iterative curvature correction and learn linear models on deep data

    representations to solve non-linear regression tasks. Finally, we discuss

    intriguing parallels to a mechanism identified to be crucial for in-context

    learning termed induction-head (Olsson et al., 2022) and show how it could be

    understood as a specific case of in-context learning by gradient descent

    learning within Transformers. Code to reproduce the experiments can be found at

    https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
    .'
  arxivId: '2212.07677'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,
    Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
  created_at: '2025-01-05T08:25:08.591396'
  issue_number: 91
  issue_url: https://github.com/dmarx/papers-feed/issues/91
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T08:19:06.235Z'
  main_tex_file: null
  published_date: '2022-12-15T09:21:21Z'
  state: open
  title: Transformers learn in-context by gradient descent
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2212.07677
'2212.14052':
  abstract: 'State space models (SSMs) have demonstrated state-of-the-art sequence

    modeling performance in some modalities, but underperform attention in language

    modeling. Moreover, despite scaling nearly linearly in sequence length instead

    of quadratically, SSMs are still slower than Transformers due to poor hardware

    utilization. In this paper, we make progress on understanding the expressivity

    gap between SSMs and attention in language modeling, and on reducing the

    hardware barrier between SSMs and attention. First, we use synthetic language

    modeling tasks to understand the gap between SSMs and attention. We find that

    existing SSMs struggle with two capabilities: recalling earlier tokens in the

    sequence and comparing tokens across the sequence. To understand the impact on

    language modeling, we propose a new SSM layer, H3, that is explicitly designed

    for these abilities. H3 matches attention on the synthetic languages and comes

    within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid

    125M-parameter H3-attention model that retains two attention layers

    surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to

    improve the efficiency of training SSMs on modern hardware, we propose

    FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on

    sequences up to 8K, and introduces a novel state passing algorithm that

    exploits the recurrent properties of SSMs to scale to longer sequences.

    FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows

    hybrid language models to generate text 2.4$\times$ faster than Transformers.

    Using FlashConv, we scale hybrid H3-attention language models up to 2.7B

    parameters on the Pile and find promising initial results, achieving lower

    perplexity than Transformers and outperforming Transformers in zero- and

    few-shot learning on a majority of tasks in the SuperGLUE benchmark.'
  arxivId: '2212.14052'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher
    Ré
  created_at: '2025-01-04T15:02:03.932890'
  issue_number: 402
  issue_url: https://github.com/dmarx/papers-feed/issues/402
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:22:13.066Z'
  main_tex_file: null
  published_date: '2022-12-28T17:56:03Z'
  state: open
  title: 'Hungry Hungry Hippos: Towards Language Modeling with State Space Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2212.14052
'2302.04222':
  abstract: 'Recent text-to-image diffusion models such as MidJourney and Stable Diffusion

    threaten to displace many in the professional artist community. In particular,

    models can learn to mimic the artistic style of specific artists after

    "fine-tuning" on samples of their art. In this paper, we describe the design,

    implementation and evaluation of Glaze, a tool that enables artists to apply

    "style cloaks" to their art before sharing online. These cloaks apply barely

    perceptible perturbations to images, and when used as training data, mislead

    generative models that try to mimic a specific artist. In coordination with the

    professional artist community, we deploy user studies to more than 1000

    artists, assessing their views of AI art, as well as the efficacy of our tool,

    its usability and tolerability of perturbations, and robustness across

    different scenarios and against adaptive countermeasures. Both surveyed artists

    and empirical CLIP-based scores show that even at low perturbation levels

    (p=0.05), Glaze is highly successful at disrupting mimicry under normal

    conditions (>92%) and against adaptive countermeasures (>85%).'
  arxivId: '2302.04222'
  arxiv_tags:
  - cs.CR
  authors: Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben
    Y. Zhao
  created_at: '2025-01-05T08:24:26.526523'
  issue_number: 149
  issue_url: https://github.com/dmarx/papers-feed/issues/149
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:19:38.213Z'
  main_tex_file: null
  published_date: '2023-02-08T17:45:23Z'
  state: open
  title: 'Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2302.04222
'2302.10866':
  abstract: 'Recent advances in deep learning have relied heavily on the use of large

    Transformers due to their ability to learn at scale. However, the core building

    block of Transformers, the attention operator, exhibits quadratic cost in

    sequence length, limiting the amount of context accessible. Existing

    subquadratic methods based on low-rank and sparse approximations need to be

    combined with dense attention layers to match Transformers, indicating a gap in

    capability. In this work, we propose Hyena, a subquadratic drop-in replacement

    for attention constructed by interleaving implicitly parametrized long

    convolutions and data-controlled gating. In recall and reasoning tasks on

    sequences of thousands to hundreds of thousands of tokens, Hyena improves

    accuracy by more than 50 points over operators relying on state-spaces and

    other implicit and explicit methods, matching attention-based models. We set a

    new state-of-the-art for dense-attention-free architectures on language

    modeling in standard datasets (WikiText103 and The Pile), reaching Transformer

    quality with a 20% reduction in training compute required at sequence length

    2K. Hyena operators are twice as fast as highly optimized attention at sequence

    length 8K, and 100x faster at sequence length 64K.'
  arxivId: '2302.10866'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen
    Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré
  created_at: '2025-01-04T15:02:06.928206'
  issue_number: 401
  issue_url: https://github.com/dmarx/papers-feed/issues/401
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:17:27.699Z'
  main_tex_file: null
  published_date: '2023-02-21T18:29:25Z'
  state: open
  title: 'Hyena Hierarchy: Towards Larger Convolutional Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2302.10866
'2302.11529':
  abstract: 'Transfer learning has recently become the dominant paradigm of machine

    learning. Pre-trained models fine-tuned for downstream tasks achieve better

    performance with fewer labelled examples. Nonetheless, it remains unclear how

    to develop models that specialise towards multiple tasks without incurring

    negative interference and that generalise systematically to non-identically

    distributed tasks. Modular deep learning has emerged as a promising solution to

    these challenges. In this framework, units of computation are often implemented

    as autonomous parameter-efficient modules. Information is conditionally routed

    to a subset of modules and subsequently aggregated. These properties enable

    positive transfer and systematic generalisation by separating computation from

    routing and updating modules locally. We offer a survey of modular

    architectures, providing a unified view over several threads of research that

    evolved independently in the scientific literature. Moreover, we explore

    various additional purposes of modularity, including scaling language models,

    causal inference, programme induction, and planning in reinforcement learning.

    Finally, we report various concrete applications where modularity has been

    successfully deployed such as cross-lingual and cross-modal knowledge transfer.

    Related talks and projects to this survey, are available at

    https://www.modulardeeplearning.com/.'
  arxivId: '2302.11529'
  arxiv_tags:
  - cs.LG
  authors: Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, Edoardo Maria Ponti
  created_at: '2025-01-04T15:03:27.942084'
  issue_number: 227
  issue_url: https://github.com/dmarx/papers-feed/issues/227
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:20.689362'
  last_visited: '2024-12-24T03:05:18.719Z'
  main_tex_file: null
  published_date: '2023-02-22T18:11:25Z'
  state: open
  title: Modular Deep Learning
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2302.11529
'2302.13714':
  abstract: 'In this work, we investigate a challenging problem, which has been considered

    to be an important criterion in designing codewords for DNA computing purposes,

    namely secondary structure avoidance in single-stranded DNA molecules. In

    short, secondary structure refers to the tendency of a single-stranded DNA

    sequence to fold back upon itself, thus becoming inactive in the computation

    process. While some design criteria that reduces the possibility of secondary

    structure formation has been proposed by Milenkovic and Kashyap (2006), the

    main contribution of this work is to provide an explicit construction of DNA

    codes that completely avoid secondary structure of arbitrary stem length.

    Formally, given codeword length n and arbitrary integer m>=2, we provide

    efficient methods to construct DNA codes of length n that avoid secondary

    structure of any stem length more than or equal to m. Particularly, when m = 3,

    our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the

    highest rate found in the prior art was 1.1609 bits/nt. In addition, for

    m>=3log n + 4, we provide an efficient encoder that incurs only one redundant

    symbol.'
  arxivId: '2302.13714'
  arxiv_tags:
  - cs.IT
  - math.CO
  - math.IT
  authors: Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah, Duc Tu Dao, Kees A. Schouhamer
    Immink
  created_at: '2025-01-04T15:02:09.851732'
  issue_number: 400
  issue_url: https://github.com/dmarx/papers-feed/issues/400
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:09.852499'
  last_visited: '2024-12-28T07:13:40.029Z'
  main_tex_file: null
  published_date: '2023-02-27T12:22:07Z'
  state: open
  title: "On the Design of Codes for DNA Computing: Secondary Structure Avoidance\n\
    \  Codes"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2302.13714
'2303.00383':
  abstract: 'We propose a robust and computationally efficient algorithm to generically

    construct first return maps of dynamical systems from time series without the

    need for embedding. Typically, a first return map is constructed using a

    heuristic convenience (maxima or zero-crossings of the time series, for

    example) or a computationally delicate geometric approach (explicitly

    constructing a Poincar\''e section from a hyper-surface normal to the flow and

    then interpolating to determine intersections with trajectories). Our approach

    relies on ordinal partitions of the time series and builds the first return map

    from successive intersections with particular ordinal sequences. Generically,

    we can obtain distinct first return maps for each ordinal sequence. We define

    entropy-based measures to guide our selection of the ordinal sequence for a

    ``good'''' first return map and show that this method can robustly be applied
    to

    time series from classical chaotic systems to extract the underlying first

    return map dynamics. The results are shown on several well-known dynamical

    systems (Lorenz, R{\"o}ssler and Mackey-Glass in chaotic regimes).'
  arxivId: '2303.00383'
  arxiv_tags:
  - math.DS
  authors: Zahra Shahriari, Shannon Dee Algar, David M. Walker, Michael Small
  created_at: '2025-01-04T06:52:03.682689'
  issue_number: 760
  issue_url: https://github.com/dmarx/papers-feed/issues/760
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:03.683895'
  last_visited: '2025-01-03T08:49:43.332Z'
  main_tex_file: null
  published_date: '2023-03-01T10:09:57Z'
  state: open
  title: "Ordinal Poincaré Sections: Reconstructing the First Return Map from an\n\
    \  Ordinal Segmentation of Time Series"
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2303.00383
'2303.03667':
  abstract: 'To design fast neural networks, many works have been focusing on reducing
    the

    number of floating-point operations (FLOPs). We observe that such reduction in

    FLOPs, however, does not necessarily lead to a similar level of reduction in

    latency. This mainly stems from inefficiently low floating-point operations per

    second (FLOPS). To achieve faster networks, we revisit popular operators and

    demonstrate that such low FLOPS is mainly due to frequent memory access of the

    operators, especially the depthwise convolution. We hence propose a novel

    partial convolution (PConv) that extracts spatial features more efficiently, by

    cutting down redundant computation and memory access simultaneously. Building

    upon our PConv, we further propose FasterNet, a new family of neural networks,

    which attains substantially higher running speed than others on a wide range of

    devices, without compromising on accuracy for various vision tasks. For

    example, on ImageNet-1k, our tiny FasterNet-T0 is $2.8\times$, $3.3\times$, and

    $2.4\times$ faster than MobileViT-XXS on GPU, CPU, and ARM processors,

    respectively, while being $2.9\%$ more accurate. Our large FasterNet-L achieves

    impressive $83.5\%$ top-1 accuracy, on par with the emerging Swin-B, while

    having $36\%$ higher inference throughput on GPU, as well as saving $37\%$

    compute time on CPU. Code is available at

    \url{https://github.com/JierunChen/FasterNet}.'
  arxivId: '2303.03667'
  arxiv_tags:
  - cs.CV
  authors: Jierun Chen, Shiu-hong Kao, Hao He, Weipeng Zhuo, Song Wen, Chul-Ho Lee,
    S. -H. Gary Chan
  created_at: '2025-01-05T20:18:59.771049'
  issue_number: 825
  issue_url: https://github.com/dmarx/papers-feed/issues/825
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T20:21:34.744107'
  last_visited: '2025-01-05T20:19:32.266000+00:00'
  main_tex_file: null
  published_date: '2023-03-07T06:05:30Z'
  state: open
  title: 'Run, Don''t Walk: Chasing Higher FLOPS for Faster Neural Networks'
  total_reading_time_seconds: 35
  url: https://arxiv.org/abs/2303.03667
'2303.08500':
  abstract: 'Protecting personal data against exploitation of machine learning models
    is

    crucial. Recently, availability attacks have shown great promise to provide an

    extra layer of protection against the unauthorized use of data to train neural

    networks. These methods aim to add imperceptible noise to clean data so that

    the neural networks cannot extract meaningful patterns from the protected data,

    claiming that they can make personal data "unexploitable." This paper provides

    a strong countermeasure against such approaches, showing that unexploitable

    data might only be an illusion. In particular, we leverage the power of

    diffusion models and show that a carefully designed denoising process can

    counteract the effectiveness of the data-protecting perturbations. We

    rigorously analyze our algorithm, and theoretically prove that the amount of

    required denoising is directly related to the magnitude of the data-protecting

    perturbations. Our approach, called AVATAR, delivers state-of-the-art

    performance against a suite of recent availability attacks in various

    scenarios, outperforming adversarial training even under distribution mismatch

    between the diffusion model and the protected data. Our findings call for more

    research into making personal data unexploitable, showing that this goal is far

    from over. Our implementation is available at this repository:

    https://github.com/hmdolatabadi/AVATAR.'
  arxivId: '2303.08500'
  arxiv_tags:
  - cs.LG
  - cs.CR
  - cs.CV
  authors: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
  created_at: '2025-01-05T08:24:11.508058'
  issue_number: 156
  issue_url: https://github.com/dmarx/papers-feed/issues/156
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:41:41.995Z'
  main_tex_file: null
  published_date: '2023-03-15T10:20:49Z'
  state: open
  title: "The Devil's Advocate: Shattering the Illusion of Unexploitable Data\n  using\
    \ Diffusion Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2303.08500
'2303.09489':
  abstract: 'Time series modeling is a well-established problem, which often requires
    that

    methods (1) expressively represent complicated dependencies, (2) forecast long

    horizons, and (3) efficiently train over long sequences. State-space models

    (SSMs) are classical models for time series, and prior works combine SSMs with

    deep learning layers for efficient sequence modeling. However, we find

    fundamental limitations with these prior approaches, proving their SSM

    representations cannot express autoregressive time series processes. We thus

    introduce SpaceTime, a new state-space time series architecture that improves

    all three criteria. For expressivity, we propose a new SSM parameterization

    based on the companion matrix -- a canonical representation for discrete-time

    processes -- which enables SpaceTime''s SSM layers to learn desirable

    autoregressive processes. For long horizon forecasting, we introduce a

    "closed-loop" variation of the companion SSM, which enables SpaceTime to

    predict many future time-steps by generating its own layer-wise inputs. For

    efficient training and inference, we introduce an algorithm that reduces the

    memory and compute of a forward pass with the companion matrix. With sequence

    length $\ell$ and state-space size $d$, we go from $\tilde{O}(d \ell)$

    na\"ively to $\tilde{O}(d + \ell)$. In experiments, our contributions lead to

    state-of-the-art results on extensive and diverse benchmarks, with best or

    second-best AUROC on 6 / 7 ECG and speech time series classification, and best

    MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1)

    fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more

    accurately on longer horizons than prior state-of-the-art, and (3) speeds up

    training on real-world ETTh1 data by 73% and 80% relative wall-clock time over

    Transformers and LSTMs.'
  arxivId: '2303.09489'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Michael Zhang, Khaled K. Saab, Michael Poli, Tri Dao, Karan Goel, Christopher
    Ré
  created_at: '2025-01-04T15:02:18.932613'
  issue_number: 394
  issue_url: https://github.com/dmarx/papers-feed/issues/394
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:09:58.237Z'
  main_tex_file: null
  published_date: '2023-03-16T17:08:21Z'
  state: open
  title: Effectively Modeling Time Series with Simple Discrete State Spaces
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2303.09489
'2304.02234':
  abstract: 'Recently developed text-to-image diffusion models make it easy to edit
    or

    create high-quality images. Their ease of use has raised concerns about the

    potential for malicious editing or deepfake creation. Imperceptible

    perturbations have been proposed as a means of protecting images from malicious

    editing by preventing diffusion models from generating realistic images.

    However, we find that the aforementioned perturbations are not robust to JPEG

    compression, which poses a major weakness because of the common usage and

    availability of JPEG. We discuss the importance of robustness for additive

    imperceptible perturbations and encourage alternative approaches to protect

    images against editing.'
  arxivId: '2304.02234'
  arxiv_tags:
  - cs.LG
  - cs.CR
  - cs.CV
  authors: Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein
  created_at: '2025-01-05T08:24:08.494533'
  issue_number: 157
  issue_url: https://github.com/dmarx/papers-feed/issues/157
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:44:11.039Z'
  main_tex_file: null
  published_date: '2023-04-05T05:30:09Z'
  state: open
  title: JPEG Compressed Images Can Bypass Protections Against AI Editing
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2304.02234
'2304.15004':
  abstract: 'Recent work claims that large language models display emergent abilities,

    abilities not present in smaller-scale models that are present in larger-scale

    models. What makes emergent abilities intriguing is two-fold: their sharpness,

    transitioning seemingly instantaneously from not present to present, and their

    unpredictability, appearing at seemingly unforeseeable model scales. Here, we

    present an alternative explanation for emergent abilities: that for a

    particular task and model family, when analyzing fixed model outputs, emergent

    abilities appear due to the researcher''s choice of metric rather than due to

    fundamental changes in model behavior with scale. Specifically, nonlinear or

    discontinuous metrics produce apparent emergent abilities, whereas linear or

    continuous metrics produce smooth, continuous predictable changes in model

    performance. We present our alternative explanation in a simple mathematical

    model, then test it in three complementary ways: we (1) make, test and confirm

    three predictions on the effect of metric choice using the InstructGPT/GPT-3

    family on tasks with claimed emergent abilities; (2) make, test and confirm two

    predictions about metric choices in a meta-analysis of emergent abilities on

    BIG-Bench; and (3) show to choose metrics to produce never-before-seen

    seemingly emergent abilities in multiple vision tasks across diverse deep

    networks. Via all three analyses, we provide evidence that alleged emergent

    abilities evaporate with different metrics or with better statistics, and may

    not be a fundamental property of scaling AI models.'
  arxivId: '2304.15004'
  arxiv_tags:
  - cs.AI
  - cs.LG
  authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo
  created_at: '2025-01-05T08:25:11.508634'
  issue_number: 89
  issue_url: https://github.com/dmarx/papers-feed/issues/89
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T06:06:11.226Z'
  main_tex_file: null
  published_date: '2023-04-28T17:52:11Z'
  state: open
  title: Are Emergent Abilities of Large Language Models a Mirage?
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2304.15004
'2305.06161':
  abstract: 'The BigCode community, an open-scientific collaboration working on the

    responsible development of Large Language Models for Code (Code LLMs),

    introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context

    length, infilling capabilities and fast large-batch inference enabled by

    multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced

    from The Stack, a large collection of permissively licensed GitHub repositories

    with inspection tools and an opt-out process. We fine-tuned StarCoderBase on

    35B Python tokens, resulting in the creation of StarCoder. We perform the most

    comprehensive evaluation of Code LLMs to date and show that StarCoderBase

    outperforms every open Code LLM that supports multiple programming languages

    and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,

    StarCoder outperforms every model that is fine-tuned on Python, can be prompted

    to achieve 40\% pass@1 on HumanEval, and still retains its performance on other

    programming languages. We take several important steps towards a safe

    open-access model release, including an improved PII redaction pipeline and a

    novel attribution tracing tool, and make the StarCoder models publicly

    available under a more commercially viable version of the Open Responsible AI

    Model license.'
  arxivId: '2305.06161'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.PL
  - cs.SE
  authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
    Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii
    Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
    Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,
    Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham
    Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry
    Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
    Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor
    Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger,
    Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
    Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel
    Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
    Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
  created_at: '2025-01-04T15:02:21.866440'
  issue_number: 358
  issue_url: https://github.com/dmarx/papers-feed/issues/358
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:21.867203'
  last_visited: '2024-12-28T06:14:43.367Z'
  main_tex_file: null
  published_date: '2023-05-09T08:16:42Z'
  state: open
  title: 'StarCoder: may the source be with you!'
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2305.06161
'2305.13169':
  abstract: 'Pretraining is the preliminary and fundamental step in developing capable

    language models (LM). Despite this, pretraining data design is critically

    under-documented and often guided by empirically unsupported intuitions. To

    address this, we pretrain 28 1.5B parameter decoder-only models, training on

    data curated (1) at different times, (2) with varying toxicity and quality

    filters, and (3) with different domain compositions. First, we quantify the

    effect of pretraining data age. A temporal shift between evaluation data and

    pretraining data leads to performance degradation, which is not overcome by

    finetuning. Second, we explore the effect of quality and toxicity filters,

    showing a trade-off between performance on standard benchmarks and risk of

    toxic generations. Our findings indicate there does not exist a

    one-size-fits-all solution to filtering training data. We also find that the

    effects of different types of filtering are not predictable from text domain

    characteristics. Lastly, we empirically validate that the inclusion of

    heterogeneous data sources, like books and web, is broadly beneficial and

    warrants greater prioritization. These findings constitute the largest set of

    experiments to validate, quantify, and expose many undocumented intuitions

    about text pretraining, which we hope will help support more informed

    data-centric decisions in LM development.'
  arxivId: '2305.13169'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts,
    Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito
  created_at: '2025-01-04T06:53:03.610654'
  issue_number: 657
  issue_url: https://github.com/dmarx/papers-feed/issues/657
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:20:50.556Z'
  main_tex_file: null
  published_date: '2023-05-22T15:57:53Z'
  state: open
  title: "A Pretrainer's Guide to Training Data: Measuring the Effects of Data\n \
    \ Age, Domain Coverage, Quality, & Toxicity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2305.13169
'2307.08691':
  abstract: 'Scaling Transformers to longer sequence lengths has been a major problem
    in

    the last several years, promising to improve performance in language modeling

    and high-resolution image understanding, as well as to unlock new applications

    in code, audio, and video generation. The attention layer is the main

    bottleneck in scaling to longer sequences, as its runtime and memory increase

    quadratically in the sequence length. FlashAttention exploits the asymmetric

    GPU memory hierarchy to bring significant memory saving (linear instead of

    quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines),

    with no approximation. However, FlashAttention is still not nearly as fast as

    optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the

    theoretical maximum FLOPs/s. We observe that the inefficiency is due to

    suboptimal work partitioning between different thread blocks and warps on the

    GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We

    propose FlashAttention-2, with better work partitioning to address these

    issues. In particular, we (1) tweak the algorithm to reduce the number of

    non-matmul FLOPs (2) parallelize the attention computation, even for a single

    head, across different thread blocks to increase occupancy, and (3) within each

    thread block, distribute the work between warps to reduce communication through

    shared memory. These yield around 2$\times$ speedup compared to FlashAttention,

    reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close

    to the efficiency of GEMM operations. We empirically validate that when used

    end-to-end to train GPT-style models, FlashAttention-2 reaches training speed

    of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).'
  arxivId: '2307.08691'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao
  created_at: '2025-01-04T15:02:24.941588'
  issue_number: 356
  issue_url: https://github.com/dmarx/papers-feed/issues/356
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:28.119002'
  last_visited: '2024-12-28T06:14:32.268Z'
  main_tex_file: null
  published_date: '2023-07-17T17:50:36Z'
  state: open
  title: "FlashAttention-2: Faster Attention with Better Parallelism and Work\n  Partitioning"
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2307.08691
'2307.09288':
  abstract: 'In this work, we develop and release Llama 2, a collection of pretrained
    and

    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70

    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for

    dialogue use cases. Our models outperform open-source chat models on most

    benchmarks we tested, and based on our human evaluations for helpfulness and

    safety, may be a suitable substitute for closed-source models. We provide a

    detailed description of our approach to fine-tuning and safety improvements of

    Llama 2-Chat in order to enable the community to build on our work and

    contribute to the responsible development of LLMs.'
  arxivId: '2307.09288'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
  created_at: '2025-01-04T14:49:21.224810'
  issue_number: 472
  issue_url: https://github.com/dmarx/papers-feed/issues/472
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:21.229139'
  last_visited: '2024-12-29T10:07:22.174Z'
  main_tex_file: null
  published_date: '2023-07-18T14:31:57Z'
  state: open
  title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models'
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2307.09288
'2307.12868':
  abstract: 'Despite the success of diffusion models (DMs), we still lack a thorough

    understanding of their latent space. To understand the latent space

    $\mathbf{x}_t \in \mathcal{X}$, we analyze them from a geometrical perspective.

    Our approach involves deriving the local latent basis within $\mathcal{X}$ by

    leveraging the pullback metric associated with their encoding feature maps.

    Remarkably, our discovered local latent basis enables image editing

    capabilities by moving $\mathbf{x}_t$, the latent space of DMs, along the basis

    vector at specific timesteps. We further analyze how the geometric structure of

    DMs evolves over diffusion timesteps and differs across different text

    conditions. This confirms the known phenomenon of coarse-to-fine generation, as

    well as reveals novel insights such as the discrepancy between $\mathbf{x}_t$

    across timesteps, the effect of dataset complexity, and the time-varying

    influence of text prompts. To the best of our knowledge, this paper is the

    first to present image editing through $\mathbf{x}$-space traversal, editing

    only once at specific timestep $t$ without any additional training, and

    providing thorough analyses of the latent structure of DMs. The code to

    reproduce our experiments can be found at

    https://github.com/enkeejunior1/Diffusion-Pullback.'
  arxivId: '2307.12868'
  arxiv_tags:
  - cs.CV
  authors: Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, Youngjung Uh
  created_at: '2025-01-10T20:56:46.516441'
  issue_number: 917
  issue_url: https://github.com/dmarx/papers-feed/issues/917
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T20:56:46.517220'
  last_visited: '2025-01-10T20:55:12.357Z'
  main_tex_file: null
  published_date: '2023-07-24T15:06:42Z'
  state: open
  title: "Understanding the Latent Space of Diffusion Models through the Lens of\n\
    \  Riemannian Geometry"
  total_reading_time_seconds: 18
  url: https://arxiv.org/abs/2307.12868
'2308.06259':
  abstract: 'We present a scalable method to build a high quality instruction following

    language model by automatically labelling human-written text with corresponding

    instructions. Our approach, named instruction backtranslation, starts with a

    language model finetuned on a small amount of seed data, and a given web

    corpus. The seed model is used to construct training examples by generating

    instruction prompts for web documents (self-augmentation), and then selecting

    high quality examples from among these candidates (self-curation). This data is

    then used to finetune a stronger model. Finetuning LLaMa on two iterations of

    our approach yields a model that outperforms all other LLaMa-based models on

    the Alpaca leaderboard not relying on distillation data, demonstrating highly

    effective self-alignment.'
  arxivId: '2308.06259'
  arxiv_tags:
  - cs.CL
  authors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer,
    Jason Weston, Mike Lewis
  created_at: '2025-01-04T14:48:45.526814'
  issue_number: 575
  issue_url: https://github.com/dmarx/papers-feed/issues/575
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T04:57:10.471Z'
  main_tex_file: null
  published_date: '2023-08-11T17:47:54Z'
  state: open
  title: Self-Alignment with Instruction Backtranslation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2308.06259
'2308.10718':
  abstract: "Recent years have witnessed success in AIGC (AI Generated Content). People\n\
    can make use of a pre-trained diffusion model to generate images of high\nquality\
    \ or freely modify existing pictures with only prompts in nature\nlanguage. More\
    \ excitingly, the emerging personalization techniques make it\nfeasible to create\
    \ specific-desired images with only a few images as\nreferences. However, this\
    \ induces severe threats if such advanced techniques\nare misused by malicious\
    \ users, such as spreading fake news or defaming\nindividual reputations. Thus,\
    \ it is necessary to regulate personalization\nmodels (i.e., concept censorship)\
    \ for their development and advancement.\n  In this paper, we focus on the personalization\
    \ technique dubbed Textual\nInversion (TI), which is becoming prevailing for its\
    \ lightweight nature and\nexcellent performance. TI crafts the word embedding\
    \ that contains detailed\ninformation about a specific object. Users can easily\
    \ download the word\nembedding from public websites like Civitai and add it to\
    \ their own stable\ndiffusion model without fine-tuning for personalization. To\
    \ achieve the concept\ncensorship of a TI model, we propose leveraging the backdoor\
    \ technique for good\nby injecting backdoors into the Textual Inversion embeddings.\
    \ Briefly, we\nselect some sensitive words as triggers during the training of\
    \ TI, which will\nbe censored for normal use. In the subsequent generation stage,\
    \ if the triggers\nare combined with personalized embeddings as final prompts,\
    \ the model will\noutput a pre-defined target image rather than images including\
    \ the desired\nmalicious concept.\n  To demonstrate the effectiveness of our approach,\
    \ we conduct extensive\nexperiments on Stable Diffusion, a prevailing open-sourced\
    \ text-to-image model.\nOur code, data, and results are available at\nhttps://concept-censorship.github.io."
  arxivId: '2308.10718'
  arxiv_tags:
  - cs.CR
  - cs.CV
  authors: Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang
  created_at: '2025-01-05T08:24:02.651619'
  issue_number: 160
  issue_url: https://github.com/dmarx/papers-feed/issues/160
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:47:15.076Z'
  main_tex_file: null
  published_date: '2023-08-21T13:39:04Z'
  state: open
  title: Backdooring Textual Inversion for Concept Censorship
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2308.10718
'2308.10792':
  abstract: 'This paper surveys research works in the quickly advancing field of

    instruction tuning (IT), which can also be referred to as supervised

    fine-tuning (SFT)\footnote{In this paper, unless specified otherwise,

    supervised fine-tuning (SFT) and instruction tuning (IT) are used

    interchangeably.}, a crucial technique to enhance the capabilities and

    controllability of large language models (LLMs). Instruction tuning refers to

    the process of further training LLMs on a dataset consisting of

    \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the

    gap between the next-word prediction objective of LLMs and the users'' objective

    of having LLMs adhere to human instructions. In this work, we make a systematic

    review of the literature, including the general methodology of SFT, the

    construction of SFT datasets, the training of SFT models, and applications to

    different modalities, domains and application, along with analysis on aspects

    that influence the outcome of SFT (e.g., generation of instruction outputs,

    size of the instruction dataset, etc). We also review the potential pitfalls of

    SFT along with criticism against it, along with efforts pointing out current

    deficiencies of existing strategies and suggest some avenues for fruitful

    research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey'
  arxivId: '2308.10792'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang,
    Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
  created_at: '2025-01-06T23:16:44.633238'
  issue_number: 841
  issue_url: https://github.com/dmarx/papers-feed/issues/841
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-06T23:26:59.522369'
  last_visited: '2025-01-06T23:25:42.652000+00:00'
  main_tex_file: null
  published_date: '2023-08-21T15:35:16Z'
  state: open
  title: 'Instruction Tuning for Large Language Models: A Survey'
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2308.10792
'2308.13561':
  abstract: 'Egocentric, multi-modal data as available on future augmented reality
    (AR)

    devices provides unique challenges and opportunities for machine perception.

    These future devices will need to be all-day wearable in a socially acceptable

    form-factor to support always available, context-aware and personalized AI

    applications. Our team at Meta Reality Labs Research built the Aria device, an

    egocentric, multi-modal data recording and streaming device with the goal to

    foster and accelerate research in this area. In this paper, we describe the

    Aria device hardware including its sensor configuration and the corresponding

    software tools that enable recording and processing of such data.'
  arxivId: '2308.13561'
  arxiv_tags:
  - cs.HC
  - cs.CV
  authors: Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander
    Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith,
    Cheng Peng, Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso,
    Derek Valleroy, Dinesh Ginjupalli, Duncan Frost, Edward Miller, Elias Mueggler,
    Evgeniy Oleinik, Fan Zhang, Guruprasad Somasundaram, Gustavo Solaira, Harry Lanaras,
    Henry Howard-Jenkins, Huixuan Tang, Hyo Jin Kim, Jaime Rivera, Ji Luo, Jing Dong,
    Julian Straub, Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira, Mark
    Schwesinger, Maurizio Monge, Nan Yang, Nick Charron, Nikhil Raina, Omkar Parkhi,
    Peter Borschowa, Pierre Moulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington,
    Sachin Kulkarni, Sagar Miglani, Santosh Gondi, Saransh Solanki, Sean Diener, Shangyi
    Cheng, Simon Green, Steve Saarinen, Suvam Patra, Tassos Mourikis, Thomas Whelan,
    Tripti Singh, Vasileios Balntas, Vijay Baiyya, Wilson Dreewes, Xiaqing Pan, Yang
    Lou, Yipu Zhao, Yusuf Mansour, Yuyang Zou, Zhaoyang Lv, Zijian Wang, Mingfei Yan,
    Carl Ren, Renzo De Nardi, Richard Newcombe
  created_at: '2025-01-10T06:32:03.789671'
  issue_number: 904
  issue_url: https://github.com/dmarx/papers-feed/issues/904
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T06:32:03.790467'
  last_visited: '2025-01-10T06:30:22.302Z'
  main_tex_file: null
  published_date: '2023-08-24T20:42:21Z'
  state: open
  title: 'Project Aria: A New Tool for Egocentric Multi-Modal AI Research'
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2308.13561
'2309.06180':
  abstract: 'High throughput serving of large language models (LLMs) requires batching

    sufficiently many requests at a time. However, existing systems struggle

    because the key-value cache (KV cache) memory for each request is huge and

    grows and shrinks dynamically. When managed inefficiently, this memory can be

    significantly wasted by fragmentation and redundant duplication, limiting the

    batch size. To address this problem, we propose PagedAttention, an attention

    algorithm inspired by the classical virtual memory and paging techniques in

    operating systems. On top of it, we build vLLM, an LLM serving system that

    achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV

    cache within and across requests to further reduce memory usage. Our

    evaluations show that vLLM improves the throughput of popular LLMs by

    2-4$\times$ with the same level of latency compared to the state-of-the-art

    systems, such as FasterTransformer and Orca. The improvement is more pronounced

    with longer sequences, larger models, and more complex decoding algorithms.

    vLLM''s source code is publicly available at

    https://github.com/vllm-project/vllm'
  arxivId: '2309.06180'
  arxiv_tags:
  - cs.LG
  - cs.DC
  authors: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica
  created_at: '2025-01-15T19:12:38.376361'
  issue_number: 982
  issue_url: https://github.com/dmarx/papers-feed/issues/982
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-15T19:12:38.377163'
  last_visited: '2025-01-15T19:11:27.895Z'
  main_tex_file: null
  published_date: '2023-09-12T12:50:04Z'
  state: open
  title: "Efficient Memory Management for Large Language Model Serving with\n  PagedAttention"
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2309.06180
'2309.07965':
  abstract: 'In this work, we introduce the concept of relative Lipschitz saturation,

    along with its key categorical and algebraic properties, and demonstrate how

    such a structure always gives rise to a radicial algebra.'
  arxivId: '2309.07965'
  arxiv_tags:
  - math.AC
  - 13B22
  authors: Thiago da Silva, Guilherme Schultz Netto
  created_at: '2025-01-04T15:02:13.218276'
  issue_number: 398
  issue_url: https://github.com/dmarx/papers-feed/issues/398
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:13.219648'
  last_visited: '2024-12-28T07:12:00.313000+00:00'
  main_tex_file: null
  published_date: '2023-09-14T18:02:12Z'
  state: open
  title: "A survey on relative Lipschitz saturation of algebras and its relation\n\
    \  with radicial algebras"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2309.07965
'2309.12032':
  abstract: 'Structure learning is the crux of causal inference. Notably, causal discovery

    (CD) algorithms are brittle when data is scarce, possibly inferring imprecise

    causal relations that contradict expert knowledge -- especially when

    considering latent confounders. To aggravate the issue, most CD methods do not

    provide uncertainty estimates, making it hard for users to interpret results

    and improve the inference process. Surprisingly, while CD is a human-centered

    affair, no works have focused on building methods that both 1) output

    uncertainty estimates that can be verified by experts and 2) interact with

    those experts to iteratively refine CD. To solve these issues, we start by

    proposing to sample (causal) ancestral graphs proportionally to a belief

    distribution based on a score function, such as the Bayesian information

    criterion (BIC), using generative flow networks. Then, we leverage the

    diversity in candidate graphs and introduce an optimal experimental design to

    iteratively probe the expert about the relations among variables, effectively

    reducing the uncertainty of our belief over ancestral graphs. Finally, we

    update our samples to incorporate human feedback via importance sampling.

    Importantly, our method does not require causal sufficiency (i.e., unobserved

    confounders may exist). Experiments with synthetic observational data show that

    our method can accurately sample from distributions over ancestral graphs and

    that we can greatly improve inference quality with human aid.'
  arxivId: '2309.12032'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Tiago da Silva, Eliezer Silva, António Góis, Dominik Heider, Samuel Kaski,
    Diego Mesquita, Adèle Ribeiro
  created_at: '2025-01-04T15:02:28.115639'
  issue_number: 354
  issue_url: https://github.com/dmarx/papers-feed/issues/354
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:28.120081'
  last_visited: '2024-12-28T06:14:26.797Z'
  main_tex_file: null
  published_date: '2023-09-21T12:53:45Z'
  state: open
  title: "Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral\
    \ GFlowNets"
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2309.12032
'2309.14556':
  abstract: 'Researchers have argued that large language models (LLMs) exhibit

    high-quality writing capabilities from blogs to stories. However, evaluating

    objectively the creativity of a piece of writing is challenging. Inspired by

    the Torrance Test of Creative Thinking (TTCT), which measures creativity as a

    process, we use the Consensual Assessment Technique [3] and propose the

    Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product.

    TTCW consists of 14 binary tests organized into the original dimensions of

    Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative

    writers and implement a human assessment of 48 stories written either by

    professional authors or LLMs using TTCW. Our analysis shows that LLM-generated

    stories pass 3-10X less TTCW tests than stories written by professionals. In

    addition, we explore the use of LLMs as assessors to automate the TTCW

    evaluation, revealing that none of the LLMs positively correlate with the

    expert assessments.'
  arxivId: '2309.14556'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.HC
  authors: Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan,
    Chien-Sheng Wu
  created_at: '2025-01-04T14:48:36.292112'
  issue_number: 585
  issue_url: https://github.com/dmarx/papers-feed/issues/585
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T14:42:55.576Z'
  main_tex_file: null
  published_date: '2023-09-25T22:02:46Z'
  state: open
  title: "Art or Artifice? Large Language Models and the False Promise of\n  Creativity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2309.14556
'2310.01889':
  abstract: 'Transformers have emerged as the architecture of choice for many

    state-of-the-art AI models, showcasing exceptional performance across a wide

    range of AI applications. However, the memory demands imposed by Transformers

    limit their ability to handle long sequences, thereby posing challenges in

    utilizing videos, actions, and other long-form sequences and modalities in

    complex environments. We present a novel approach, Ring Attention with

    Blockwise Transformers (Ring Attention), which leverages blockwise computation

    of self-attention and feedforward to distribute long sequences across multiple

    devices while fully overlapping the communication of key-value blocks with the

    computation of blockwise attention. Our approach enables training and inference

    of sequences that are up to device count times longer than those achievable by

    prior memory-efficient Transformers, without resorting to approximations or

    incurring additional communication and computation overheads. Extensive

    experiments on language modeling and reinforcement learning tasks demonstrate

    the effectiveness of our approach in allowing millions of tokens context size

    and improving performance.'
  arxivId: '2310.01889'
  arxiv_tags:
  - cs.CL
  authors: Hao Liu, Matei Zaharia, Pieter Abbeel
  created_at: '2025-01-04T06:52:27.607291'
  issue_number: 712
  issue_url: https://github.com/dmarx/papers-feed/issues/712
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:27.608961'
  last_visited: '2025-01-02T07:40:21.565Z'
  main_tex_file: null
  published_date: '2023-10-03T08:44:50Z'
  state: open
  title: Ring Attention with Blockwise Transformers for Near-Infinite Context
  total_reading_time_seconds: 40
  url: https://arxiv.org/abs/2310.01889
'2310.16410':
  abstract: 'Artificial Intelligence (AI) systems have made remarkable progress, attaining

    super-human performance across various domains. This presents us with an

    opportunity to further human knowledge and improve human expert performance by

    leveraging the hidden knowledge encoded within these highly performant AI

    systems. Yet, this knowledge is often hard to extract, and may be hard to

    understand or learn from. Here, we show that this is possible by proposing a

    new method that allows us to extract new chess concepts in AlphaZero, an AI

    system that mastered the game of chess via self-play without human supervision.

    Our analysis indicates that AlphaZero may encode knowledge that extends beyond

    the existing human knowledge, but knowledge that is ultimately not beyond human

    grasp, and can be successfully learned from. In a human study, we show that

    these concepts are learnable by top human experts, as four top chess

    grandmasters show improvements in solving the presented concept prototype

    positions. This marks an important first milestone in advancing the frontier of

    human knowledge by leveraging AI; a development that could bear profound

    implications and help us shape how we interact with AI systems across many AI

    applications.'
  arxivId: '2310.16410'
  arxiv_tags:
  - cs.AI
  - cs.HC
  - cs.LG
  - stat.ML
  authors: Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet,
    Been Kim
  created_at: '2025-01-17T18:10:52.308660'
  issue_number: 991
  issue_url: https://github.com/dmarx/papers-feed/issues/991
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-17T18:10:52.310417'
  last_visited: '2025-01-17T18:09:38.173Z'
  main_tex_file: null
  published_date: '2023-10-25T06:49:26Z'
  state: open
  title: "Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in\n\
    \  AlphaZero"
  total_reading_time_seconds: 56
  url: https://arxiv.org/abs/2310.16410
'2310.17157':
  abstract: 'Large language models (LLMs) with hundreds of billions of parameters
    have

    sparked a new wave of exciting AI applications. However, they are

    computationally expensive at inference time. Sparsity is a natural approach to

    reduce this cost, but existing methods either require costly retraining, have

    to forgo LLM''s in-context learning ability, or do not yield wall-clock time

    speedup on modern hardware. We hypothesize that contextual sparsity, which are

    small, input-dependent sets of attention heads and MLP parameters that yield

    approximately the same output as the dense model for a given input, can address

    these issues. We show that contextual sparsity exists, that it can be

    accurately predicted, and that we can exploit it to speed up LLM inference in

    wall-clock time without compromising LLM''s quality or in-context learning

    ability. Based on these insights, we propose DejaVu, a system that uses a

    low-cost algorithm to predict contextual sparsity on the fly given inputs to

    each layer, along with an asynchronous and hardware-aware implementation that

    speeds up LLM inference. We validate that DejaVu can reduce the inference

    latency of OPT-175B by over 2X compared to the state-of-the-art

    FasterTransformer, and over 6X compared to the widely used Hugging Face

    implementation, without compromising model quality. The code is available at

    https://github.com/FMInference/DejaVu.'
  arxivId: '2310.17157'
  arxiv_tags:
  - cs.LG
  authors: Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
    Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen
  created_at: '2025-01-04T15:02:30.861014'
  issue_number: 352
  issue_url: https://github.com/dmarx/papers-feed/issues/352
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:30.861843'
  last_visited: '2024-12-28T06:14:06.334Z'
  main_tex_file: null
  published_date: '2023-10-26T05:01:09Z'
  state: open
  title: 'Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time'
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2310.17157
'2312.00330':
  abstract: 'Text-to-video (T2V) models have shown remarkable capabilities in generating

    diverse videos. However, they struggle to produce user-desired stylized videos

    due to (i) text''s inherent clumsiness in expressing specific styles and (ii)

    the generally degraded style fidelity. To address these challenges, we

    introduce StyleCrafter, a generic method that enhances pre-trained T2V models

    with a style control adapter, enabling video generation in any style by

    providing a reference image. Considering the scarcity of stylized video

    datasets, we propose to first train a style control adapter using style-rich

    image datasets, then transfer the learned stylization ability to video

    generation through a tailor-made finetuning paradigm. To promote content-style

    disentanglement, we remove style descriptions from the text prompt and extract

    style information solely from the reference image using a decoupling learning

    strategy. Additionally, we design a scale-adaptive fusion module to balance the

    influences of text-based content features and image-based style features, which

    helps generalization across various text and style combinations. StyleCrafter

    efficiently generates high-quality stylized videos that align with the content

    of the texts and resemble the style of the reference images. Experiments

    demonstrate that our approach is more flexible and efficient than existing

    competitors.'
  arxivId: '2312.00330'
  arxiv_tags:
  - cs.CV
  - cs.AI
  authors: Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang,
    Xintao Wang, Yujiu Yang, Ying Shan
  created_at: '2025-01-05T18:41:18.669173'
  issue_number: 38
  issue_url: https://github.com/dmarx/papers-feed/issues/38
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T20:33:49.865Z'
  main_tex_file: null
  published_date: '2023-12-01T03:53:21Z'
  state: open
  title: "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style\n \
    \ Adapter"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2312.00330
'2312.00752':
  abstract: 'Foundation models, now powering most of the exciting applications in
    deep

    learning, are almost universally based on the Transformer architecture and its

    core attention module. Many subquadratic-time architectures such as linear

    attention, gated convolution and recurrent models, and structured state space

    models (SSMs) have been developed to address Transformers'' computational

    inefficiency on long sequences, but they have not performed as well as

    attention on important modalities such as language. We identify that a key

    weakness of such models is their inability to perform content-based reasoning,

    and make several improvements. First, simply letting the SSM parameters be

    functions of the input addresses their weakness with discrete modalities,

    allowing the model to selectively propagate or forget information along the

    sequence length dimension depending on the current token. Second, even though

    this change prevents the use of efficient convolutions, we design a

    hardware-aware parallel algorithm in recurrent mode. We integrate these

    selective SSMs into a simplified end-to-end neural network architecture without

    attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$

    higher throughput than Transformers) and linear scaling in sequence length, and

    its performance improves on real data up to million-length sequences. As a

    general sequence model backbone, Mamba achieves state-of-the-art performance

    across several modalities such as language, audio, and genomics. On language

    modeling, our Mamba-3B model outperforms Transformers of the same size and

    matches Transformers twice its size, both in pretraining and downstream

    evaluation.'
  arxivId: '2312.00752'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Albert Gu, Tri Dao
  created_at: '2025-01-04T15:02:33.936181'
  issue_number: 351
  issue_url: https://github.com/dmarx/papers-feed/issues/351
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T06:13:33.669Z'
  main_tex_file: null
  published_date: '2023-12-01T18:01:34Z'
  state: open
  title: 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2312.00752
'2312.07731':
  abstract: 'Recent work proposed a new mechanism to remove protective perturbation
    added

    by Glaze in order to again enable mimicry of art styles from images protected

    by Glaze. Despite promising results shown in the original paper, our own tests

    with the authors'' code demonstrated several limitations of the proposed

    purification approach. The main limitations are 1) purification has a limited

    effect when tested on artists that are not well-known historical artists

    already embedded in original training data, 2) problems in evaluation metrics,

    and 3) collateral damage on mimicry result for clean images. We believe these

    limitations should be carefully considered in order to understand real world

    usability of the purification attack.'
  arxivId: '2312.07731'
  arxiv_tags:
  - cs.CR
  authors: Shawn Shan, Stanley Wu, Haitao Zheng, Ben Y. Zhao
  created_at: '2025-01-05T08:23:59.519495'
  issue_number: 161
  issue_url: https://github.com/dmarx/papers-feed/issues/161
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:49:35.138Z'
  main_tex_file: null
  published_date: '2023-12-12T20:52:27Z'
  state: open
  title: A Response to Glaze Purification via IMPRESS
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2312.07731
'2312.17127':
  abstract: "We study semantic models of probabilistic programming languages over\
    \ graphs,\nand establish a connection to graphons from graph theory and combinatorics.\
    \ We\nshow that every well-behaved equational theory for our graph probabilistic\n\
    programming language corresponds to a graphon, and conversely, every graphon\n\
    arises in this way.\n  We provide three constructions for showing that every graphon\
    \ arises from an\nequational theory. The first is an abstract construction, using\
    \ Markov\ncategories and monoidal indeterminates. The second and third are more\
    \ concrete.\nThe second is in terms of traditional measure theoretic probability,\
    \ which\ncovers 'black-and-white' graphons. The third is in terms of probability\
    \ monads\non the nominal sets of Gabbay and Pitts. Specifically, we use a variation\
    \ of\nnominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\\
    'enyi\ngraphons. In this way, we build new models of graph probabilistic programming\n\
    from graphons."
  arxivId: '2312.17127'
  arxiv_tags:
  - cs.PL
  - cs.LO
  - math.PR
  authors: Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar, Jacek Karwowski,
    Sean K. Moss, Daniel M. Roy, Sam Staton, Hongseok Yang
  created_at: '2025-01-04T14:49:15.402966'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:15.404627'
  last_visited: '2024-12-29T19:53:34.293000+00:00'
  main_tex_file: null
  published_date: '2023-12-28T17:04:50Z'
  state: open
  title: "Probabilistic programming interfaces for random graphs: Markov\n  categories,\
    \ graphons, and nominal sets"
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2312.17127
'2401.00649':
  abstract: 'I developed the lecture notes based on my ``Linear Model'''' course at
    the

    University of California Berkeley over the past seven years. This book provides

    an intermediate-level introduction to the linear model. It balances rigorous

    proofs and heuristic arguments. This book provides R code to replicate all

    simulation studies and case studies.'
  arxivId: '2401.00649'
  arxiv_tags:
  - stat.ME
  - stat.AP
  authors: Peng Ding
  created_at: '2025-01-14T15:15:43.034838'
  issue_number: 968
  issue_url: https://github.com/dmarx/papers-feed/issues/968
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-14T15:20:47.368469'
  last_visited: '2025-01-14T15:24:20.284000+00:00'
  main_tex_file: null
  published_date: '2024-01-01T03:34:17Z'
  state: open
  title: Linear Model and Extensions
  total_reading_time_seconds: 308
  url: https://arxiv.org/abs/2401.00649
'2401.10774':
  abstract: "Large Language Models (LLMs) employ auto-regressive decoding that requires\n\
    sequential computation, with each step reliant on the previous one's output.\n\
    This creates a bottleneck as each step necessitates moving the full model\nparameters\
    \ from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods\
    \ such as speculative decoding have been suggested to address this issue,\ntheir\
    \ implementation is impeded by the challenges associated with acquiring and\n\
    maintaining a separate draft model. In this paper, we present Medusa, an\nefficient\
    \ method that augments LLM inference by adding extra decoding heads to\npredict\
    \ multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism,\
    \ Medusa constructs multiple candidate continuations and verifies them\nsimultaneously\
    \ in each decoding step. By leveraging parallel processing, Medusa\nsubstantially\
    \ reduces the number of decoding steps required. We present two\nlevels of fine-tuning\
    \ procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa\
    \ is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference\
    \ acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM,\
    \ enabling better prediction accuracy of Medusa\nheads and higher speedup but\
    \ needing a special training recipe that preserves\nthe backbone model's capabilities.\n\
    \  Moreover, we propose several extensions that improve or expand the utility\
    \ of\nMedusa, including a self-distillation to handle situations where no training\n\
    data is available and a typical acceptance scheme to boost the acceptance rate\n\
    while maintaining generation quality. We evaluate Medusa on models of various\n\
    sizes and training procedures. Our experiments demonstrate that Medusa-1 can\n\
    achieve over 2.2x speedup without compromising generation quality, while\nMedusa-2\
    \ further improves the speedup to 2.3-3.6x."
  arxivId: '2401.10774'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming
    Chen, Tri Dao
  created_at: '2025-01-04T15:02:15.845680'
  issue_number: 395
  issue_url: https://github.com/dmarx/papers-feed/issues/395
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:15.846437'
  last_visited: '2024-12-28T07:11:26.727Z'
  main_tex_file: null
  published_date: '2024-01-19T15:48:40Z'
  state: open
  title: "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding\
    \ Heads"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2401.10774
'2402.03239':
  abstract: 'Bluesky is a new social network built upon the AT Protocol, a decentralized

    foundation for public social media. It was launched in private beta in February

    2023, and has grown to over 10 million registered users by October 2024. In

    this paper we introduce the architecture of Bluesky and the AT Protocol, and

    explain how the technical design of Bluesky is informed by our goals: to enable

    decentralization by having multiple interoperable providers for every part of

    the system; to make it easy for users to switch providers; to give users agency

    over the content they see; and to provide a simple user experience that does

    not burden users with complexity arising from the system''s decentralized

    nature. The system''s openness allows anybody to contribute to content

    moderation and community management, and we invite the research community to

    use Bluesky as a dataset and testing ground for new approaches in social media

    moderation.'
  arxivId: '2402.03239'
  arxiv_tags:
  - cs.DC
  - cs.SI
  authors: Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren,
    Devin Ivy, Jeromy Johnson, Bryan Newbold, Jaz Volpert
  created_at: '2025-01-05T08:24:50.498276'
  issue_number: 105
  issue_url: https://github.com/dmarx/papers-feed/issues/105
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T05:41:38.653Z'
  main_tex_file: null
  published_date: '2024-02-05T17:55:51Z'
  state: open
  title: 'Bluesky and the AT Protocol: Usable Decentralized Social Media'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.03239
'2402.09949':
  abstract: 'Large Language Models have proven highly successful at modelling a variety
    of

    tasks. However, this comes at a steep computational cost that hinders wider

    industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that

    goes beyond word boundaries by representing frequent multi-word expressions as

    single tokens. MWTs produce a more compact and efficient tokenization that

    yields two benefits: (1) Increase in performance due to a greater coverage of

    input data given a fixed sequence length budget; (2) Faster and lighter

    inference due to the ability to reduce the sequence length with negligible

    drops in performance. Our results show that MWT is more robust across shorter

    sequence lengths, thus allowing for major speedups via early sequence

    truncation.'
  arxivId: '2402.09949'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini
  created_at: '2025-01-10T01:42:15.861174'
  issue_number: 863
  issue_url: https://github.com/dmarx/papers-feed/issues/863
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T01:42:15.861971'
  last_visited: '2025-01-10T01:40:44.674Z'
  main_tex_file: null
  published_date: '2024-02-15T13:52:23Z'
  state: open
  title: Multi-word Tokenization for Sequence Compression
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/2402.09949
'2402.09977':
  abstract: 'Real-world business applications require a trade-off between language
    model

    performance and size. We propose a new method for model compression that relies

    on vocabulary transfer. We evaluate the method on various vertical domains and

    downstream tasks. Our results indicate that vocabulary transfer can be

    effectively used in combination with other compression techniques, yielding a

    significant reduction in model size and inference time while marginally

    compromising on performance.'
  arxivId: '2402.09977'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni
  created_at: '2025-01-10T01:42:13.038168'
  issue_number: 869
  issue_url: https://github.com/dmarx/papers-feed/issues/869
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T02:01:59.117108'
  last_visited: '2025-01-10T02:00:48.166000+00:00'
  main_tex_file: null
  published_date: '2024-02-15T14:37:07Z'
  state: open
  title: Fast Vocabulary Transfer for Language Model Compression
  total_reading_time_seconds: 65
  url: https://arxiv.org/abs/2402.09977
'2402.10193':
  abstract: 'Large Language Models (LLMs) are typically trained in two phases:

    pre-training on large internet-scale datasets, and fine-tuning for downstream

    tasks. Given the higher computational demand of pre-training, it''s intuitive
    to

    assume that fine-tuning adds less new information to the model, and is thus

    more compressible. We explore this assumption by decomposing the weights of

    fine-tuned models into their pre-trained components and an additional delta. We

    introduce a simple method, BitDelta, which successfully quantizes this delta

    down to 1 bit without compromising performance. This interesting finding not

    only highlights the potential redundancy of information added during

    fine-tuning, but also has significant implications for the multi-tenant serving

    and multi-tenant storage of fine-tuned models. By enabling the use of a single

    high-precision base model accompanied by multiple 1-bit deltas, BitDelta

    dramatically reduces GPU memory requirements by more than 10x, which can also

    be translated to enhanced generation latency in multi-tenant settings. We

    validate BitDelta through experiments across Llama-2 and Mistral model

    families, and on models up to 70B parameters, showcasing minimal performance

    degradation over all tested settings.'
  arxivId: '2402.10193'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle
    Cai
  created_at: '2025-01-04T15:02:36.858483'
  issue_number: 350
  issue_url: https://github.com/dmarx/papers-feed/issues/350
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T06:12:55.272Z'
  main_tex_file: null
  published_date: '2024-02-15T18:50:06Z'
  state: open
  title: 'BitDelta: Your Fine-Tune May Only Be Worth One Bit'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.10193
'2402.16670':
  abstract: 'Over the last 70 years, we, humans, have created an economic market where

    attention is being captured and turned into money thanks to advertising. During

    the last two decades, leveraging research in psychology, sociology,

    neuroscience and other domains, Web platforms have brought the process of

    capturing attention to an unprecedented scale. With the initial commonplace

    goal of making targeted advertising more effective, the generalization of

    attention-capturing techniques and their use of cognitive biases and emotions

    have multiple detrimental side effects such as polarizing opinions, spreading

    false information and threatening public health, economies and democracies.

    This is clearly a case where the Web is not used for the common good and where,

    in fact, all its users become a vulnerable population. This paper brings

    together contributions from a wide range of disciplines to analyze current

    practices and consequences thereof. Through a set of propositions and

    principles that could be used do drive further works, it calls for actions

    against these practices competing to capture our attention on the Web, as it

    would be unsustainable for a civilization to allow attention to be wasted with

    impunity on a world-wide scale.'
  arxivId: '2402.16670'
  arxiv_tags:
  - cs.SI
  authors: Franck Michel, Fabien Gandon
  created_at: '2025-01-04T15:02:57.844414'
  issue_number: 330
  issue_url: https://github.com/dmarx/papers-feed/issues/330
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-27T22:07:23.174Z'
  main_tex_file: null
  published_date: '2024-02-26T15:46:43Z'
  state: open
  title: "Pay Attention: a Call to Regulate the Attention Market and Prevent\n  Algorithmic\
    \ Emotional Governance"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.16670
'2402.19173':
  abstract: 'The BigCode project, an open-scientific collaboration focused on the

    responsible development of Large Language Models for Code (Code LLMs),

    introduces StarCoder2. In partnership with Software Heritage (SWH), we build

    The Stack v2 on top of the digital commons of their source code archive.

    Alongside the SWH repositories spanning 619 programming languages, we carefully

    select other high-quality data sources, such as GitHub pull requests, Kaggle

    notebooks, and code documentation. This results in a training set that is 4x

    larger than the first StarCoder dataset. We train StarCoder2 models with 3B,

    7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate

    them on a comprehensive set of Code LLM benchmarks. We find that our small

    model, StarCoder2-3B, outperforms other Code LLMs of similar size on most

    benchmarks, and also outperforms StarCoderBase-15B. Our large model,

    StarCoder2- 15B, significantly outperforms other models of comparable size. In

    addition, it matches or outperforms CodeLlama-34B, a model more than twice its

    size. Although DeepSeekCoder- 33B is the best-performing model at code

    completion for high-resource languages, we find that StarCoder2-15B outperforms

    it on math and code reasoning benchmarks, as well as several low-resource

    languages. We make the model weights available under an OpenRAIL license and

    ensure full transparency regarding the training data by releasing the SoftWare

    Heritage persistent IDentifiers (SWHIDs) of the source code data.'
  arxivId: '2402.19173'
  arxiv_tags:
  - cs.SE
  - cs.AI
  authors: Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
    Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu,
    Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu,
    Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li,
    Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu,
    Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun
    Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki,
    Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel
    Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten
    Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados,
    Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming
    Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
  created_at: '2025-01-04T15:02:39.854557'
  issue_number: 348
  issue_url: https://github.com/dmarx/papers-feed/issues/348
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:39.855254'
  last_visited: '2024-12-28T06:12:42.484Z'
  main_tex_file: null
  published_date: '2024-02-29T13:53:35Z'
  state: open
  title: 'StarCoder 2 and The Stack v2: The Next Generation'
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2402.19173
'2403.08540':
  abstract: 'Scaling laws are useful guides for derisking expensive training runs,
    as they

    predict performance of large models using cheaper, small-scale experiments.

    However, there remain gaps between current scaling studies and how language

    models are ultimately trained and evaluated. For instance, scaling is usually

    studied in the compute-optimal training regime (i.e., "Chinchilla optimal"

    regime). In contrast, models are often over-trained to reduce inference costs.

    Moreover, scaling laws mostly predict loss on next-token prediction, but models

    are usually compared on downstream task performance. To address both

    shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters

    trained with various numbers of tokens on three data distributions. First, we

    fit scaling laws that extrapolate in both the amount of over-training and the

    number of model parameters. This enables us to predict the validation loss of
    a

    1.4B parameter, 900B token run (i.e., 32$\times$ over-trained) and a 6.9B

    parameter, 138B token run (i.e., a compute-optimal run)$\unicode{x2014}$each

    from experiments that take 300$\times$ less compute. Second, we relate the

    perplexity of a language model to its downstream task performance by proposing

    a power law. We use this law to predict top-1 error averaged over downstream

    tasks for the two aforementioned models, using experiments that take 20$\times$

    less compute. Our experiments are available at

    https://github.com/mlfoundations/scaling.'
  arxivId: '2403.08540'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan,
    Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh,
    Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Luca Soldaini, Alexandros
    G. Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, Yair Carmon,
    Achal Dave, Reinhard Heckel, Niklas Muennighoff, Ludwig Schmidt
  created_at: '2025-01-04T06:53:18.643645'
  issue_number: 599
  issue_url: https://github.com/dmarx/papers-feed/issues/599
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.648664'
  last_visited: '2024-12-30T19:37:31.972Z'
  main_tex_file: null
  published_date: '2024-03-13T13:54:00Z'
  state: open
  title: "Language models scale reliably with over-training and on downstream\n  tasks"
  total_reading_time_seconds: 22
  url: https://arxiv.org/abs/2403.08540
'2403.10304':
  abstract: 'We present a Wikidata-based framework, called KIF, for virtually integrating

    heterogeneous knowledge sources. KIF is written in Python and is released as

    open-source. It leverages Wikidata''s data model and vocabulary plus

    user-defined mappings to construct a unified view of the underlying sources

    while keeping track of the context and provenance of their statements. The

    underlying sources can be triplestores, relational databases, CSV files, etc.,

    which may or may not use the vocabulary and RDF encoding of Wikidata. The end

    result is a virtual knowledge base which behaves like an "extended Wikidata"

    and which can be queried using a simple but expressive pattern language,

    defined in terms of Wikidata''s data model. In this paper, we present the design

    and implementation of KIF, discuss how we have used it to solve a real

    integration problem in the domain of chemistry (involving Wikidata, PubChem,

    and IBM CIRCA), and present experimental results on the performance and

    overhead of KIF'
  arxivId: '2403.10304'
  arxiv_tags:
  - cs.AI
  - cs.DB
  authors: Guilherme Lima, João M. B. Rodrigues, Marcelo Machado, Elton Soares, Sandro
    R. Fiorini, Raphael Thiago, Leonardo G. Azevedo, Viviane T. da Silva, Renato Cerqueira
  created_at: '2025-01-04T15:02:42.884454'
  issue_number: 346
  issue_url: https://github.com/dmarx/papers-feed/issues/346
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:42.885145'
  last_visited: '2024-12-28T06:12:17.716Z'
  main_tex_file: null
  published_date: '2024-03-15T13:46:36Z'
  state: open
  title: "KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge\n\
    \  Sources"
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2403.10304
'2403.14554':
  abstract: 'We propose Gaussian Frosting, a novel mesh-based representation for

    high-quality rendering and editing of complex 3D effects in real-time. Our

    approach builds on the recent 3D Gaussian Splatting framework, which optimizes

    a set of 3D Gaussians to approximate a radiance field from images. We propose

    first extracting a base mesh from Gaussians during optimization, then building

    and refining an adaptive layer of Gaussians with a variable thickness around

    the mesh to better capture the fine details and volumetric effects near the

    surface, such as hair or grass. We call this layer Gaussian Frosting, as it

    resembles a coating of frosting on a cake. The fuzzier the material, the

    thicker the frosting. We also introduce a parameterization of the Gaussians to

    enforce them to stay inside the frosting layer and automatically adjust their

    parameters when deforming, rescaling, editing or animating the mesh. Our

    representation allows for efficient rendering using Gaussian splatting, as well

    as editing and animation by modifying the base mesh. We demonstrate the

    effectiveness of our method on various synthetic and real scenes, and show that

    it outperforms existing surface-based approaches. We will release our code and

    a web-based viewer as additional contributions. Our project page is the

    following: https://anttwo.github.io/frosting/'
  arxivId: '2403.14554'
  arxiv_tags:
  - cs.CV
  - cs.GR
  authors: Antoine Guédon, Vincent Lepetit
  created_at: '2025-01-04T06:52:06.631074'
  issue_number: 758
  issue_url: https://github.com/dmarx/papers-feed/issues/758
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:06.631879'
  last_visited: '2025-01-03T02:52:11.974Z'
  main_tex_file: null
  published_date: '2024-03-21T16:53:03Z'
  state: open
  title: "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2403.14554
'2404.13320':
  abstract: 'Adversarial examples for diffusion models are widely used as solutions
    for

    safety concerns. By adding adversarial perturbations to personal images,

    attackers can not edit or imitate them easily. However, it is essential to note

    that all these protections target the latent diffusion model (LDMs), the

    adversarial examples for diffusion models in the pixel space (PDMs) are largely

    overlooked. This may mislead us to think that the diffusion models are

    vulnerable to adversarial attacks like most deep models. In this paper, we show

    novel findings that: even though gradient-based white-box attacks can be used

    to attack the LDMs, they fail to attack PDMs. This finding is supported by

    extensive experiments of almost a wide range of attacking methods on various

    PDMs and LDMs with different model structures, which means diffusion models are

    indeed much more robust against adversarial attacks. We also find that PDMs can

    be used as an off-the-shelf purifier to effectively remove the adversarial

    patterns that were generated on LDMs to protect the images, which means that

    most protection methods nowadays, to some extent, cannot protect our images

    from malicious attacks. We hope that our insights will inspire the community to

    rethink the adversarial samples for diffusion models as protection methods and

    move forward to more effective protection. Codes are available in

    https://github.com/xavihart/PDM-Pure.'
  arxivId: '2404.13320'
  arxiv_tags:
  - cs.CV
  - cs.AI
  authors: Haotian Xue, Yongxin Chen
  created_at: '2025-01-05T08:24:20.521827'
  issue_number: 151
  issue_url: https://github.com/dmarx/papers-feed/issues/151
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:33:17.796Z'
  main_tex_file: null
  published_date: '2024-04-20T08:28:43Z'
  state: open
  title: "Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than\n\
    \  We Think"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2404.13320
'2404.16698':
  abstract: 'As AI systems pervade human life, ensuring that large language models
    (LLMs)

    make safe decisions remains a significant challenge. We introduce the

    Governance of the Commons Simulation (GovSim), a generative simulation platform

    designed to study strategic interactions and cooperative decision-making in

    LLMs. In GovSim, a society of AI agents must collectively balance exploiting a

    common resource with sustaining it for future use. This environment enables the

    study of how ethical considerations, strategic planning, and negotiation skills

    impact cooperative outcomes. We develop an LLM-based agent architecture and

    test it with the leading open and closed LLMs. We find that all but the most

    powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with

    the highest survival rate below 54%. Ablations reveal that successful

    multi-agent communication between agents is critical for achieving cooperation

    in these cases. Furthermore, our analyses show that the failure to achieve

    sustainable cooperation in most LLMs stems from their inability to formulate

    and analyze hypotheses about the long-term effects of their actions on the

    equilibrium of the group. Finally, we show that agents that leverage

    "Universalization"-based reasoning, a theory of moral thinking, are able to

    achieve significantly better sustainability. Taken together, GovSim enables us

    to study the mechanisms that underlie sustainable self-government with

    specificity and scale. We open source the full suite of our research results,

    including the simulation environment, agent prompts, and a comprehensive web

    interface.'
  arxivId: '2404.16698'
  arxiv_tags:
  - cs.CL
  authors: Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya
    Sachan, Rada Mihalcea
  created_at: '2025-01-05T20:09:30.076359'
  issue_number: 813
  issue_url: https://github.com/dmarx/papers-feed/issues/813
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T20:12:12.476203'
  last_visited: '2025-01-05T20:11:03.137000+00:00'
  main_tex_file: null
  published_date: '2024-04-25T15:59:16Z'
  state: open
  title: "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society\n\
    \  of LLM Agents"
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2404.16698
'2405.04434':
  abstract: 'We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model

    characterized by economical training and efficient inference. It comprises 236B

    total parameters, of which 21B are activated for each token, and supports a

    context length of 128K tokens. DeepSeek-V2 adopts innovative architectures

    including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees

    efficient inference through significantly compressing the Key-Value (KV) cache

    into a latent vector, while DeepSeekMoE enables training strong models at an

    economical cost through sparse computation. Compared with DeepSeek 67B,

    DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves

    42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum

    generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality

    and multi-source corpus consisting of 8.1T tokens, and further perform

    Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock

    its potential. Evaluation results show that, even with only 21B activated

    parameters, DeepSeek-V2 and its chat versions still achieve top-tier

    performance among open-source models.'
  arxivId: '2405.04434'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang
    Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen,
    Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei
    Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo
    Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li,
    Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan,
    Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun
    Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan
    Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R.
    L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan
    Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang
    Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu
    Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao
    Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,
    Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang
    Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan
    Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang,
    Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang,
    Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan
    Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou,
    Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren,
    Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao,
    Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin
    Li, Ziwei Xie
  created_at: '2025-01-04T15:03:12.859314'
  issue_number: 281
  issue_url: https://github.com/dmarx/papers-feed/issues/281
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:03:12.861833'
  last_visited: '2024-12-26T13:01:17.331Z'
  main_tex_file: null
  published_date: '2024-05-07T15:56:43Z'
  state: open
  title: "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\n  Language\
    \ Model"
  total_reading_time_seconds: 46
  url: https://arxiv.org/abs/2405.04434
'2405.04517':
  abstract: 'In the 1990s, the constant error carousel and gating were introduced
    as the

    central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have

    stood the test of time and contributed to numerous deep learning success

    stories, in particular they constituted the first Large Language Models (LLMs).

    However, the advent of the Transformer technology with parallelizable

    self-attention at its core marked the dawn of a new era, outpacing LSTMs at

    scale. We now raise a simple question: How far do we get in language modeling

    when scaling LSTMs to billions of parameters, leveraging the latest techniques

    from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we

    introduce exponential gating with appropriate normalization and stabilization

    techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM

    with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that

    is fully parallelizable with a matrix memory and a covariance update rule.

    Integrating these LSTM extensions into residual block backbones yields xLSTM

    blocks that are then residually stacked into xLSTM architectures. Exponential

    gating and modified memory structures boost xLSTM capabilities to perform

    favorably when compared to state-of-the-art Transformers and State Space

    Models, both in performance and scaling.'
  arxivId: '2405.04517'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra
    Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter
  created_at: '2025-01-04T06:53:15.665066'
  issue_number: 649
  issue_url: https://github.com/dmarx/papers-feed/issues/649
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:15.667071'
  last_visited: '2024-12-30T20:10:41.007000+00:00'
  main_tex_file: null
  published_date: '2024-05-07T17:50:21Z'
  state: open
  title: 'xLSTM: Extended Long Short-Term Memory'
  total_reading_time_seconds: 31
  url: https://arxiv.org/abs/2405.04517
'2405.06865':
  abstract: 'Generative AI models are often used to perform mimicry attacks, where
    a

    pretrained model is fine-tuned on a small sample of images to learn to mimic a

    specific artist of interest. While researchers have introduced multiple

    anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence

    points to a growing trend of mimicry models using videos as sources of training

    data. This paper presents our experiences exploring techniques to disrupt style

    mimicry on video imagery. We first validate that mimicry attacks can succeed by

    training on individual frames extracted from videos. We show that while

    anti-mimicry tools can offer protection when applied to individual frames, this

    approach is vulnerable to an adaptive countermeasure that removes protection by

    exploiting randomness in optimization results of consecutive (nearly-identical)

    frames. We develop a new, tool-agnostic framework that segments videos into

    short scenes based on frame-level similarity, and use a per-scene optimization

    baseline to remove inter-frame randomization while reducing computational cost.

    We show via both image level metrics and an end-to-end user study that the

    resulting protection restores protection against mimicry (including the

    countermeasure). Finally, we develop another adaptive countermeasure and find

    that it falls short against our framework.'
  arxivId: '2405.06865'
  arxiv_tags:
  - cs.CV
  - cs.CR
  authors: Josephine Passananti, Stanley Wu, Shawn Shan, Haitao Zheng, Ben Y. Zhao
  created_at: '2025-01-05T08:24:17.506837'
  issue_number: 153
  issue_url: https://github.com/dmarx/papers-feed/issues/153
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:37:12.908Z'
  main_tex_file: null
  published_date: '2024-05-11T01:40:19Z'
  state: open
  title: Disrupting Style Mimicry Attacks on Video Imagery
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.06865
'2405.07883':
  abstract: 'Language models (LMs) are bound to their tokenizer, which maps raw text
    to a

    sequence of vocabulary items (tokens). This restricts their flexibility: for

    example, LMs trained primarily on English may still perform well in other

    natural and programming languages, but have vastly decreased efficiency due to

    their English-centric tokenizer. To mitigate this, we should be able to swap

    the original LM tokenizer with an arbitrary one, on the fly, without degrading

    performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer

    Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for

    the tokens in the vocabulary of the new tokenizer. Since prior heuristics for

    initializing embeddings often perform at chance level in a ZeTT setting, we

    propose a new solution: we train a hypernetwork taking a tokenizer as input and

    predicting the corresponding embeddings. We empirically demonstrate that the

    hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and

    decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models''

    performance in cross-lingual and coding tasks while markedly reducing the

    length of the tokenized sequence. We also find that the remaining gap can be

    quickly closed by continued training on less than 1B tokens. Finally, we show

    that a ZeTT hypernetwork trained for a base (L)LM can also be applied to

    fine-tuned variants without extra training. Overall, our results make

    substantial strides toward detaching LMs from their tokenizer.'
  arxivId: '2405.07883'
  arxiv_tags:
  - cs.CL
  authors: Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vulić
  created_at: '2025-01-10T01:45:17.778727'
  issue_number: 872
  issue_url: https://github.com/dmarx/papers-feed/issues/872
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T02:01:59.116135'
  last_visited: '2025-01-10T02:01:14.129000+00:00'
  main_tex_file: null
  published_date: '2024-05-13T16:17:10Z'
  state: open
  title: Zero-Shot Tokenizer Transfer
  total_reading_time_seconds: 15
  url: https://arxiv.org/abs/2405.07883
'2405.12399':
  abstract: 'World models constitute a promising approach for training reinforcement

    learning agents in a safe and sample-efficient manner. Recent world models

    predominantly operate on sequences of discrete latent variables to model

    environment dynamics. However, this compression into a compact discrete

    representation may ignore visual details that are important for reinforcement

    learning. Concurrently, diffusion models have become a dominant approach for

    image generation, challenging well-established methods modeling discrete

    latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As
    a

    Model Of eNvironment Dreams), a reinforcement learning agent trained in a

    diffusion world model. We analyze the key design choices that are required to

    make diffusion suitable for world modeling, and demonstrate how improved visual

    details can lead to improved agent performance. DIAMOND achieves a mean human

    normalized score of 1.46 on the competitive Atari 100k benchmark; a new best

    for agents trained entirely within a world model. We further demonstrate that

    DIAMOND''s diffusion world model can stand alone as an interactive neural game

    engine by training on static Counter-Strike: Global Offensive gameplay. To

    foster future research on diffusion for world modeling, we release our code,

    agents, videos and playable world models at https://diamond-wm.github.io.'
  arxivId: '2405.12399'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey,
    Tim Pearce, François Fleuret
  created_at: '2025-01-04T15:03:00.857112'
  issue_number: 296
  issue_url: https://github.com/dmarx/papers-feed/issues/296
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:03:00.857912'
  last_visited: '2024-12-26T22:06:50.951Z'
  main_tex_file: null
  published_date: '2024-05-20T22:51:05Z'
  state: open
  title: 'Diffusion for World Modeling: Visual Details Matter in Atari'
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2405.12399
'2405.16567':
  abstract: 'Recent AI systems have shown extremely powerful performance, even surpassing

    human performance, on various tasks such as information retrieval, language

    generation, and image generation based on large language models (LLMs). At the

    same time, there are diverse safety risks that can cause the generation of

    malicious contents by circumventing the alignment in LLMs, which are often

    referred to as jailbreaking. However, most of the previous works only focused

    on the text-based jailbreaking in LLMs, and the jailbreaking of the

    text-to-image (T2I) generation system has been relatively overlooked. In this

    paper, we first evaluate the safety of the commercial T2I generation systems,

    such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive

    prompts. From this empirical study, we find that Copilot and Gemini block only

    12% and 17% of the attacks with naive prompts, respectively, while ChatGPT

    blocks 84% of them. Then, we further propose a stronger automated jailbreaking

    pipeline for T2I generation systems, which produces prompts that bypass their

    safety guards. Our automated jailbreaking framework leverages an LLM optimizer

    to generate prompts to maximize degree of violation from the generated images

    without any weight updates or gradient computation. Surprisingly, our simple

    yet effective approach successfully jailbreaks the ChatGPT with 11.0% block

    rate, making it generate copyrighted contents in 76% of the time. Finally, we

    explore various defense strategies, such as post-generation filtering and

    machine unlearning techniques, but found that they were inadequate, which

    suggests the necessity of stronger defense mechanisms.'
  arxivId: '2405.16567'
  arxiv_tags:
  - cs.AI
  - cs.CR
  authors: Minseon Kim, Hyomin Lee, Boqing Gong, Huishuai Zhang, Sung Ju Hwang
  created_at: '2025-01-05T08:24:23.493631'
  issue_number: 150
  issue_url: https://github.com/dmarx/papers-feed/issues/150
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:31:57.733Z'
  main_tex_file: null
  published_date: '2024-05-26T13:32:24Z'
  state: open
  title: Automatic Jailbreaking of the Text-to-Image Generative AI Systems
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.16567
'2405.17472':
  abstract: 'Text-to-image diffusion models can be fine-tuned in custom domains to
    adapt

    to specific user preferences, but such adaptability has also been utilized for

    illegal purposes, such as forging public figures'' portraits, duplicating

    copyrighted artworks and generating explicit contents. Existing work focused on

    detecting the illegally generated contents, but cannot prevent or mitigate

    illegal adaptations of diffusion models. Other schemes of model unlearning and

    reinitialization, similarly, cannot prevent users from relearning the knowledge

    of illegal model adaptation with custom data. In this paper, we present

    FreezeAsGuard, a new technique that addresses these limitations and enables

    irreversible mitigation of illegal adaptations of diffusion models. Our

    approach is that the model publisher selectively freezes tensors in pre-trained

    diffusion models that are critical to illegal model adaptations, to mitigate

    the fine-tuned model''s representation power in illegal adaptations, but

    minimize the impact on other legal adaptations. Experiment results in multiple

    text-to-image application domains show that FreezeAsGuard provides 37% stronger

    power in mitigating illegal model adaptations compared to competitive

    baselines, while incurring less than 5% impact on legal model adaptations. The

    source code is available at: https://github.com/pittisl/FreezeAsGuard.'
  arxivId: '2405.17472'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CR
  - cs.CV
  authors: Kai Huang, Haoming Wang, Wei Gao
  created_at: '2025-01-05T08:23:50.509760'
  issue_number: 167
  issue_url: https://github.com/dmarx/papers-feed/issues/167
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-22T18:25:59.099Z'
  main_tex_file: null
  published_date: '2024-05-24T03:23:51Z'
  state: open
  title: "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via\n \
    \ Selective Tensor Freezing"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.17472
'2405.20053':
  abstract: 'Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context

    learning capabilities; however, their behaviors are often difficult to control.

    By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible

    to fine-tune unsupervised LMs to follow instructions and produce outputs that

    reflect human preferences. Despite its benefits, RLHF has been shown to

    potentially harm a language model''s reasoning capabilities and introduce

    artifacts such as hallucinations where the model may fabricate facts. To

    address this issue we introduce Direct Preference Heads (DPH), a fine-tuning

    framework that enables LMs to learn human preference signals through an

    auxiliary reward head without directly affecting the output distribution of the

    language modeling head. We perform a theoretical analysis of our objective

    function and find strong ties to Conservative Direct Preference Optimization

    (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All

    evaluation suite and demonstrate that our method produces models which achieve

    higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct

    Preference Optimization (DPO) alone.'
  arxivId: '2405.20053'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic
  created_at: '2025-01-05T18:41:24.669444'
  issue_number: 31
  issue_url: https://github.com/dmarx/papers-feed/issues/31
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T17:25:17.781Z'
  main_tex_file: null
  published_date: '2024-05-30T13:38:52Z'
  state: open
  title: "Would I Lie To You? Inference Time Alignment of Language Models using\n\
    \  Direct Preference Heads"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.20053
'2405.21060':
  abstract: 'While Transformers have been the main architecture behind deep learning''s

    success in language modeling, state-space models (SSMs) such as Mamba have

    recently been shown to match or outperform Transformers at small to medium

    scale. We show that these families of models are actually quite closely

    related, and develop a rich framework of theoretical connections between SSMs

    and variants of attention, connected through various decompositions of a

    well-studied class of structured semiseparable matrices. Our state space

    duality (SSD) framework allows us to design a new architecture (Mamba-2) whose

    core layer is an a refinement of Mamba''s selective SSM that is 2-8X faster,

    while continuing to be competitive with Transformers on language modeling.'
  arxivId: '2405.21060'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao, Albert Gu
  created_at: '2025-01-04T15:02:45.955944'
  issue_number: 345
  issue_url: https://github.com/dmarx/papers-feed/issues/345
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T06:11:12.620Z'
  main_tex_file: null
  published_date: '2024-05-31T17:50:01Z'
  state: open
  title: "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through\
    \ Structured State Space Duality"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.21060
'2406.01981':
  abstract: 'The size of large language models (LLMs) has scaled dramatically in recent

    years and their computational and data requirements have surged

    correspondingly. State-of-the-art language models, even at relatively smaller

    sizes, typically require training on at least a trillion tokens. This rapid

    advancement has eclipsed the growth of open-source datasets available for

    large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),

    a dataset under a permissive license comprising 1.3 trillion tokens, assembled

    by integrating several major respected open-source datasets into a single,

    high-quality corpus. We apply rigorous filtering and deduplication processes,

    both within and across datasets, to maintain and enhance the quality derived

    from the original datasets. Our evaluations show that Zyda not only competes

    favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but

    also substantially improves the performance of comparable models from the

    Pythia suite. Our rigorous data processing methods significantly enhance Zyda''s

    effectiveness, outperforming even the best of its constituent datasets when

    used independently.'
  arxivId: '2406.01981'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim,
    James Whittington, Quentin Anthony
  created_at: '2025-01-04T06:52:48.608606'
  issue_number: 641
  issue_url: https://github.com/dmarx/papers-feed/issues/641
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.650339'
  last_visited: '2024-12-30T20:04:46.858Z'
  main_tex_file: null
  published_date: '2024-06-04T05:47:17Z'
  state: open
  title: 'Zyda: A 1.3T Dataset for Open Language Modeling'
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2406.01981
'2406.06158':
  abstract: 'While the impressive performance of modern neural networks is often

    attributed to their capacity to efficiently extract task-relevant features from

    data, the mechanisms underlying this rich feature learning regime remain

    elusive, with much of our theoretical understanding stemming from the opposing

    lazy regime. In this work, we derive exact solutions to a minimal model that

    transitions between lazy and rich learning, precisely elucidating how

    unbalanced layer-specific initialization variances and learning rates determine

    the degree of feature learning. Our analysis reveals that they conspire to

    influence the learning regime through a set of conserved quantities that

    constrain and modify the geometry of learning trajectories in parameter and

    function space. We extend our analysis to more complex linear models with

    multiple neurons, outputs, and layers and to shallow nonlinear networks with

    piecewise linear activation functions. In linear networks, rapid feature

    learning only occurs from balanced initializations, where all layers learn at

    similar speeds. While in nonlinear networks, unbalanced initializations that

    promote faster learning in earlier layers can accelerate rich learning. Through

    a series of experiments, we provide evidence that this unbalanced rich regime

    drives feature learning in deep finite-width networks, promotes

    interpretability of early layers in CNNs, reduces the sample complexity of

    learning hierarchical data, and decreases the time to grokking in modular

    arithmetic. Our theory motivates further exploration of unbalanced

    initializations to enhance efficient feature learning.'
  arxivId: '2406.06158'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt,
    Andrew Saxe, Surya Ganguli
  created_at: '2025-01-05T08:24:29.697291'
  issue_number: 120
  issue_url: https://github.com/dmarx/papers-feed/issues/120
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:10:41.441Z'
  main_tex_file: null
  published_date: '2024-06-10T10:42:37Z'
  state: open
  title: "Get rich quick: exact solutions reveal how unbalanced initializations\n\
    \  promote rapid feature learning"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06158
'2406.07522':
  abstract: 'Efficiently modeling sequences with infinite context length has long
    been a

    challenging problem. Previous approaches have either suffered from quadratic

    computational complexity or limited extrapolation ability in length

    generalization. In this work, we present Samba, a simple hybrid architecture

    that layer-wise combines Mamba, a selective State Space Model (SSM), with

    Sliding Window Attention (SWA). Samba selectively compresses a given sequence

    into recurrent hidden states while still maintaining the ability to precisely

    recall recent memories with the attention mechanism. We scale Samba up to 3.8B

    parameters with 3.2T training tokens and demonstrate that it significantly

    outperforms state-of-the-art models across a variety of benchmarks. Pretrained

    on sequences of 4K length, Samba shows improved perplexity in context lengths

    of up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba

    efficiently extrapolates to a 256K context length with perfect memory recall on

    the Passkey Retrieval task, and exhibits superior retrieval extrapolation on

    the challenging Phonebook task compared to full-attention models. As a

    linear-time sequence model, Samba achieves a 3.73x higher throughput compared

    to Transformers with grouped-query attention for user prompts of 128K length,

    and a 3.64x speedup when generating 64K tokens with unlimited streaming. Our

    code for training on open source data is publicly available at

    https://github.com/microsoft/Samba.'
  arxivId: '2406.07522'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen
  created_at: '2025-01-17T05:34:38.132852'
  issue_number: 985
  issue_url: https://github.com/dmarx/papers-feed/issues/985
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-17T05:37:04.997640'
  last_visited: '2025-01-17T05:35:26.692000+00:00'
  main_tex_file: null
  published_date: '2024-06-11T17:50:51Z'
  state: open
  title: "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n\
    \  Language Modeling"
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2406.07522
'2406.09162':
  abstract: 'Recent advancements in image generation have enabled the creation of

    high-quality images from text conditions. However, when facing multi-modal

    conditions, such as text combined with reference appearances, existing methods

    struggle to balance multiple conditions effectively, typically showing a

    preference for one modality over others. To address this challenge, we

    introduce EMMA, a novel image generation model accepting multi-modal prompts

    built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA

    seamlessly incorporates additional modalities alongside text to guide image

    generation through an innovative Multi-modal Feature Connector design, which

    effectively integrates textual and supplementary modal information using a

    special attention mechanism. By freezing all parameters in the original T2I

    diffusion model and only adjusting some additional layers, we reveal an

    interesting finding that the pre-trained T2I diffusion model can secretly

    accept multi-modal prompts. This interesting property facilitates easy

    adaptation to different existing frameworks, making EMMA a flexible and

    effective tool for producing personalized and context-aware images and even

    videos. Additionally, we introduce a strategy to assemble learned EMMA modules

    to produce images conditioned on multiple modalities simultaneously,

    eliminating the need for additional training with mixed multi-modal prompts.

    Extensive experiments demonstrate the effectiveness of EMMA in maintaining high

    fidelity and detail in generated images, showcasing its potential as a robust

    solution for advanced multi-modal conditional image generation tasks.'
  arxivId: '2406.09162'
  arxiv_tags:
  - cs.CV
  authors: Yucheng Han, Rui Wang, Chi Zhang, Juntao Hu, Pei Cheng, Bin Fu, Hanwang
    Zhang
  created_at: '2025-01-04T14:49:45.263062'
  issue_number: 410
  issue_url: https://github.com/dmarx/papers-feed/issues/410
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:45.266317'
  last_visited: '2024-12-28T08:40:07.461Z'
  main_tex_file: null
  published_date: '2024-06-13T14:26:43Z'
  state: open
  title: "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal\n\
    \  Prompts"
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2406.09162
'2406.10670':
  abstract: "Selecting high-quality data for pre-training is crucial in shaping the\n\
    downstream task performance of language models. A major challenge lies in\nidentifying\
    \ this optimal subset, a problem generally considered intractable,\nthus necessitating\
    \ scalable and effective heuristics. In this work, we propose\na data selection\
    \ method, CoLoR-Filter (Conditional Loss Reduction Filtering),\nwhich leverages\
    \ an empirical Bayes-inspired approach to derive a simple and\ncomputationally\
    \ efficient selection criterion based on the relative loss values\nof two auxiliary\
    \ models.\n  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically\n\
    on two language modeling tasks: (1) selecting data from C4 for domain\nadaptation\
    \ to evaluation on Books and (2) selecting data from C4 for a suite of\ndownstream\
    \ multiple-choice question answering tasks. We demonstrate favorable\nscaling\
    \ both as we subselect more aggressively and using small auxiliary models\nto\
    \ select data for large target models. As one headline result, CoLoR-Filter\n\
    data selected using a pair of 150m parameter auxiliary models can train a 1.2b\n\
    parameter target model to match a 1.2b parameter model trained on 25b randomly\n\
    selected tokens with 25x less data for Books and 11x less data for the\ndownstream\
    \ tasks.\n  Code: https://github.com/davidbrandfonbrener/color-filter-olmo\n \
    \ Filtered data:\nhttps://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4"
  arxivId: '2406.10670'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz,
    Sham Kakade
  created_at: '2025-01-04T06:53:27.614374'
  issue_number: 630
  issue_url: https://github.com/dmarx/papers-feed/issues/630
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:27.615183'
  last_visited: '2024-12-30T20:02:17.082Z'
  main_tex_file: null
  published_date: '2024-06-15T15:28:02Z'
  state: open
  title: "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language\n\
    \  Model Pre-training"
  total_reading_time_seconds: 7
  url: https://arxiv.org/abs/2406.10670
'2406.12027':
  abstract: 'Artists are increasingly concerned about advancements in image generation

    models that can closely replicate their unique artistic styles. In response,

    several protection tools against style mimicry have been developed that

    incorporate small adversarial perturbations into artworks published online. In

    this work, we evaluate the effectiveness of popular protections -- with

    millions of downloads -- and show they only provide a false sense of security.

    We find that low-effort and "off-the-shelf" techniques, such as image

    upscaling, are sufficient to create robust mimicry methods that significantly

    degrade existing protections. Through a user study, we demonstrate that all

    existing protections can be easily bypassed, leaving artists vulnerable to

    style mimicry. We caution that tools based on adversarial perturbations cannot

    reliably protect artists from the misuse of generative AI, and urge the

    development of alternative non-technological solutions.'
  arxivId: '2406.12027'
  arxiv_tags:
  - cs.CR
  authors: Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr
  created_at: '2025-01-05T08:24:14.522530'
  issue_number: 154
  issue_url: https://github.com/dmarx/papers-feed/issues/154
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:38:53.342Z'
  main_tex_file: null
  published_date: '2024-06-17T18:51:45Z'
  state: open
  title: "Adversarial Perturbations Cannot Reliably Protect Artists From\n  Generative\
    \ AI"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.12027
'2407.01392':
  abstract: 'This paper presents Diffusion Forcing, a new training paradigm where
    a

    diffusion model is trained to denoise a set of tokens with independent

    per-token noise levels. We apply Diffusion Forcing to sequence generative

    modeling by training a causal next-token prediction model to generate one or

    several future tokens without fully diffusing past ones. Our approach is shown

    to combine the strengths of next-token prediction models, such as

    variable-length generation, with the strengths of full-sequence diffusion

    models, such as the ability to guide sampling to desirable trajectories. Our

    method offers a range of additional capabilities, such as (1) rolling-out

    sequences of continuous tokens, such as video, with lengths past the training

    horizon, where baselines diverge and (2) new sampling and guiding schemes that

    uniquely profit from Diffusion Forcing''s variable-horizon and causal

    architecture, and which lead to marked performance gains in decision-making and

    planning tasks. In addition to its empirical success, our method is proven to

    optimize a variational lower bound on the likelihoods of all subsequences of

    tokens drawn from the true joint distribution. Project website:

    https://boyuan.space/diffusion-forcing'
  arxivId: '2407.01392'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - cs.RO
  authors: Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake,
    Vincent Sitzmann
  created_at: '2025-01-04T15:03:03.858592'
  issue_number: 295
  issue_url: https://github.com/dmarx/papers-feed/issues/295
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-26T22:05:31.374Z'
  main_tex_file: null
  published_date: '2024-07-01T15:43:25Z'
  state: open
  title: 'Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.01392
'2407.01492':
  abstract: 'The data mixture for large language model pre-training significantly
    impacts

    performance, yet how to determine an effective mixture remains unclear. We

    propose RegMix to automatically identify a high-performing data mixture by

    formulating it as a regression task. RegMix involves training a set of small

    models with diverse data mixtures and fitting a regression model to predict

    their performance given their respective mixtures. With the fitted regression

    model, we simulate the top-ranked mixture and use it to train a large-scale

    model with orders of magnitude more compute. To empirically validate RegMix, we

    train 512 models with 1M parameters for 1B tokens of different mixtures to fit

    the regression model and find the optimal mixture. Using this mixture we train

    a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we

    find performs best among 64 candidate 1B parameter models with other mixtures.

    Further, our method demonstrates superior performance compared to human

    selection and achieves results that match or surpass DoReMi, while utilizing

    only 10% of the compute budget. Our experiments also show that (1) Data

    mixtures significantly impact performance with single-task performance

    variations of up to 14.6%; (2) Web corpora rather than data perceived as

    high-quality like Wikipedia have the strongest positive correlation with

    downstream performance; (3) Domains interact in complex ways often

    contradicting common sense, thus automatic approaches like RegMix are needed;

    (4) Data mixture effects transcend scaling laws, and our approach captures the

    complexity by considering all domains together. Our code is available at

    https://github.com/sail-sg/regmix.'
  arxivId: '2407.01492'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou,
    Tianyu Pang, Jing Jiang, Min Lin
  created_at: '2025-01-04T06:53:09.609827'
  issue_number: 654
  issue_url: https://github.com/dmarx/papers-feed/issues/654
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:09.611828'
  last_visited: '2024-12-30T20:17:16.629000+00:00'
  main_tex_file: null
  published_date: '2024-07-01T17:31:03Z'
  state: open
  title: 'RegMix: Data Mixture as Regression for Language Model Pre-training'
  total_reading_time_seconds: 42
  url: https://arxiv.org/abs/2407.01492
'2407.05872':
  abstract: 'Robust and effective scaling of models from small to large width typically

    requires the precise adjustment of many algorithmic and architectural details,

    such as parameterization and optimizer choices. In this work, we propose a new

    perspective on parameterization by investigating a key assumption in prior work

    about the alignment between parameters and data and derive new theoretical

    results under weaker assumptions and a broader set of optimizers. Our extensive

    empirical investigation includes tens of thousands of models trained with all

    combinations of three optimizers, four parameterizations, several alignment

    assumptions, more than a dozen learning rates, and fourteen model sizes up to

    26.8B parameters. We find that the best learning rate scaling prescription

    would often have been excluded by the assumptions in prior work. Our results

    show that all parameterizations, not just maximal update parameterization

    (muP), can achieve hyperparameter transfer; moreover, our novel per-layer

    learning rate prescription for standard parameterization outperforms muP.

    Finally, we demonstrate that an overlooked aspect of parameterization, the

    epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow

    and propose Adam-atan2, a new numerically stable, scale-invariant version of

    Adam that eliminates the epsilon hyperparameter entirely.'
  arxivId: '2407.05872'
  arxiv_tags:
  - cs.LG
  authors: Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman
    Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling,
    Jaehoon Lee, Jeffrey Pennington
  created_at: '2025-01-04T15:03:18.848085'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T15:03:18.849005'
  last_visited: '2024-12-25T10:23:47.480000+00:00'
  main_tex_file: null
  published_date: '2024-07-08T12:32:51Z'
  state: open
  title: Scaling Exponents Across Parameterizations and Optimizers
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2407.05872
'2407.08608':
  abstract: 'Attention, as a core layer of the ubiquitous Transformer architecture,
    is the

    bottleneck for large language models and long-context applications.

    FlashAttention elaborated an approach to speed up attention on GPUs through

    minimizing memory reads/writes. However, it has yet to take advantage of new

    capabilities present in recent hardware, with FlashAttention-2 achieving only

    35% utilization on the H100 GPU. We develop three main techniques to speed up

    attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to

    (1) overlap overall computation and data movement via warp-specialization and

    (2) interleave block-wise matmul and softmax operations, and (3) block

    quantization and incoherent processing that leverages hardware support for FP8

    low-precision. We demonstrate that our method, FlashAttention-3, achieves

    speedup on H100 GPUs by 1.5-2.0$\times$ with FP16 reaching up to 740 TFLOPs/s

    (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate

    that FP8 FlashAttention-3 achieves 2.6$\times$ lower numerical error than a

    baseline FP8 attention.'
  arxivId: '2407.08608'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani,
    Tri Dao
  created_at: '2025-01-04T14:49:48.246613'
  issue_number: 343
  issue_url: https://github.com/dmarx/papers-feed/issues/343
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:45.957021'
  last_visited: '2024-12-28T06:10:45.698Z'
  main_tex_file: null
  published_date: '2024-07-11T15:44:48Z'
  state: open
  title: "FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision"
  total_reading_time_seconds: 39
  url: https://arxiv.org/abs/2407.08608
'2407.21783':
  abstract: 'Modern artificial intelligence (AI) systems are powered by foundation
    models.

    This paper presents a new set of foundation models, called Llama 3. It is a

    herd of language models that natively support multilinguality, coding,

    reasoning, and tool usage. Our largest model is a dense Transformer with 405B

    parameters and a context window of up to 128K tokens. This paper presents an

    extensive empirical evaluation of Llama 3. We find that Llama 3 delivers

    comparable quality to leading language models such as GPT-4 on a plethora of

    tasks. We publicly release Llama 3, including pre-trained and post-trained

    versions of the 405B parameter language model and our Llama Guard 3 model for

    input and output safety. The paper also presents the results of experiments in

    which we integrate image, video, and speech capabilities into Llama 3 via a

    compositional approach. We observe this approach performs competitively with

    the state-of-the-art on image, video, and speech recognition tasks. The

    resulting models are not yet being broadly released as they are still under

    development.'
  arxivId: '2407.21783'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.CV
  authors: Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan,
    Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra,
    Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien
    Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh
    Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,
    Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,
    Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
    Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan,
    Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,
    Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán,
    Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai,
    Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah
    Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann,
    Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana
    Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer
    Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen
    Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca,
    Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,
    Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid
    El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia,
    Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins,
    Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de
    Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin
    Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie
    Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes
    Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier
    Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic,
    Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,
    Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,
    Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit
    Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross
    Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,
    Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan
    Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang,
    Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot,
    Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha,
    Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao,
    Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez,
    Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei
    Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang
    Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle
    Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue
    Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos,
    Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya
    Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg,
    Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus,
    Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton,
    Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita
    Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,
    Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth
    Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,
    Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
    Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao
    Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,
    Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins,
    David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem
    Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine
    Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman,
    Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian,
    Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,
    Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,
    Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,
    Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph,
    Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor
    Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,
    James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff
    Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,
    Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard,
    Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou
    U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan,
    Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang,
    Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng
    Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani,
    Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim
    Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer,
    Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan,
    Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad
    Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata
    Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich
    Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar,
    Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,
    Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,
    Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham
    Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah
    Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin
    Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha
    Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh
    Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy
    Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,
    Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,
    Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng,
    Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal
    Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun
    Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi,
    Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu,
    Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,
    Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan
    Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi,
    Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,
    Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick,
    Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, Zhiyu Ma
  created_at: '2025-01-04T06:52:42.685103'
  issue_number: 702
  issue_url: https://github.com/dmarx/papers-feed/issues/702
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T21:58:38.440Z'
  main_tex_file: null
  published_date: '2024-07-31T17:54:27Z'
  state: open
  title: The Llama 3 Herd of Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.21783
'2407.21787':
  abstract: 'Scaling the amount of compute used to train language models has dramatically

    improved their capabilities. However, when it comes to inference, we often

    limit models to making only one attempt at a problem. Here, we explore

    inference compute as another axis for scaling, using the simple technique of

    repeatedly sampling candidate solutions from a model. Across multiple tasks and

    models, we observe that coverage -- the fraction of problems that are solved by

    any generated sample -- scales with the number of samples over four orders of

    magnitude. Interestingly, the relationship between coverage and the number of

    samples is often log-linear and can be modelled with an exponentiated power

    law, suggesting the existence of inference-time scaling laws. In domains like

    coding and formal proofs, where answers can be automatically verified, these

    increases in coverage directly translate into improved performance. When we

    apply repeated sampling to SWE-bench Lite, the fraction of issues solved with

    DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250

    samples, outperforming the single-sample state-of-the-art of 43%. In domains

    without automatic verifiers, we find that common methods for picking from a

    sample collection (majority voting and reward models) plateau beyond several

    hundred samples and fail to fully scale with the sample budget.'
  arxivId: '2407.21787'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le,
    Christopher Ré, Azalia Mirhoseini
  created_at: '2025-01-04T14:49:12.234492'
  issue_number: 513
  issue_url: https://github.com/dmarx/papers-feed/issues/513
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:12.237648'
  last_visited: '2024-12-29T20:31:06.783Z'
  main_tex_file: null
  published_date: '2024-07-31T17:57:25Z'
  state: open
  title: 'Large Language Monkeys: Scaling Inference Compute with Repeated Sampling'
  total_reading_time_seconds: 17
  url: https://arxiv.org/abs/2407.21787
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2025-01-04T14:49:09.218184'
  issue_number: 57
  issue_url: https://github.com/dmarx/papers-feed/issues/57
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:12.236101'
  last_visited: '2024-12-17T13:48:02.489Z'
  main_tex_file: null
  published_date: '2024-08-06T17:35:05Z'
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 39
  url: https://arxiv.org/abs/2408.03314
'2408.06072':
  abstract: 'We present CogVideoX, a large-scale text-to-video generation model based
    on

    diffusion transformer, which can generate 10-second continuous videos aligned

    with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360

    pixels. Previous video generation models often had limited movement and short

    durations, and is difficult to generate videos with coherent narratives based

    on text. We propose several designs to address these issues. First, we propose

    a 3D Variational Autoencoder (VAE) to compress videos along both spatial and

    temporal dimensions, to improve both compression rate and video fidelity.

    Second, to improve the text-video alignment, we propose an expert transformer

    with the expert adaptive LayerNorm to facilitate the deep fusion between the

    two modalities. Third, by employing a progressive training and multi-resolution

    frame pack technique, CogVideoX is adept at producing coherent, long-duration,

    different shape videos characterized by significant motions. In addition, we

    develop an effective text-video data processing pipeline that includes various

    data preprocessing strategies and a video captioning method, greatly

    contributing to the generation quality and semantic alignment. Results show

    that CogVideoX demonstrates state-of-the-art performance across both multiple

    machine metrics and human evaluations. The model weight of both 3D Causal VAE,

    Video caption model and CogVideoX are publicly available at

    https://github.com/THUDM/CogVideo.'
  arxivId: '2408.06072'
  arxiv_tags:
  - cs.CV
  authors: Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng
    Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu,
    Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, Jie Tang
  created_at: '2025-01-04T06:53:30.623623'
  issue_number: 597
  issue_url: https://github.com/dmarx/papers-feed/issues/597
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:30.624413'
  last_visited: '2024-12-30T15:18:13.864Z'
  main_tex_file: null
  published_date: '2024-08-12T11:47:11Z'
  state: open
  title: 'CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer'
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2408.06072
'2408.11810':
  abstract: 'Diffusion Models have emerged as powerful generative models for high-quality

    image synthesis, with many subsequent image editing techniques based on them.

    However, the ease of text-based image editing introduces significant risks,

    such as malicious editing for scams or intellectual property infringement.

    Previous works have attempted to safeguard images from diffusion-based editing

    by adding imperceptible perturbations. These methods are costly and

    specifically target prevalent Latent Diffusion Models (LDMs), while

    Pixel-domain Diffusion Models (PDMs) remain largely unexplored and robust

    against such attacks. Our work addresses this gap by proposing a novel

    attacking framework with a feature representation attack loss that exploits

    vulnerabilities in denoising UNets and a latent optimization strategy to

    enhance the naturalness of protected images. Extensive experiments demonstrate

    the effectiveness of our approach in attacking dominant PDM-based editing

    methods (e.g., SDEdit) while maintaining reasonable protection fidelity and

    robustness against common defense methods. Additionally, our framework is

    extensible to LDMs, achieving comparable performance to existing approaches.'
  arxivId: '2408.11810'
  arxiv_tags:
  - cs.CV
  authors: Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao, Ernie Chu, Cheng-Fu Chou, Jun-Cheng
    Chen
  created_at: '2025-01-05T08:24:05.511992'
  issue_number: 158
  issue_url: https://github.com/dmarx/papers-feed/issues/158
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:45:08.960Z'
  main_tex_file: null
  published_date: '2024-08-21T17:56:34Z'
  state: open
  title: "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain\n \
    \ Diffusion Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2408.11810
'2408.14837':
  abstract: 'We present GameNGen, the first game engine powered entirely by a neural
    model

    that enables real-time interaction with a complex environment over long

    trajectories at high quality. GameNGen can interactively simulate the classic

    game DOOM at over 20 frames per second on a single TPU. Next frame prediction

    achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are

    only slightly better than random chance at distinguishing short clips of the

    game from clips of the simulation. GameNGen is trained in two phases: (1) an

    RL-agent learns to play the game and the training sessions are recorded, and

    (2) a diffusion model is trained to produce the next frame, conditioned on the

    sequence of past frames and actions. Conditioning augmentations enable stable

    auto-regressive generation over long trajectories.'
  arxivId: '2408.14837'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter
  created_at: '2025-01-04T15:03:06.878007'
  issue_number: 290
  issue_url: https://github.com/dmarx/papers-feed/issues/290
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:03:06.880492'
  last_visited: '2024-12-26T21:43:51.081Z'
  main_tex_file: null
  published_date: '2024-08-27T07:46:07Z'
  state: open
  title: Diffusion Models Are Real-Time Game Engines
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/2408.14837
'2409.04431':
  abstract: 'Attention is a key part of the transformer architecture. It is a

    sequence-to-sequence mapping that transforms each sequence element into a

    weighted sum of values. The weights are typically obtained as the softmax of

    dot products between keys and queries. Recent work has explored alternatives to

    softmax attention in transformers, such as ReLU and sigmoid activations. In

    this work, we revisit sigmoid attention and conduct an in-depth theoretical and

    empirical analysis. Theoretically, we prove that transformers with sigmoid

    attention are universal function approximators and benefit from improved

    regularity compared to softmax attention. Through detailed empirical analysis,

    we identify stabilization of large initial attention norms during the early

    stages of training as a crucial factor for the successful training of models

    with sigmoid attention, outperforming prior attempts. We also introduce

    FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid

    attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100

    GPUs. Experiments across language, vision, and speech show that properly

    normalized sigmoid attention matches the strong performance of softmax

    attention on a wide range of domains and scales, which previous attempts at

    sigmoid attention were unable to fully achieve. Our work unifies prior art and

    establishes best practices for sigmoid attention as a drop-in softmax

    replacement in transformers.'
  arxivId: '2409.04431'
  arxiv_tags:
  - cs.LG
  authors: Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge,
    Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russ
    Webb
  created_at: '2025-01-04T06:52:15.619463'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:15.620261'
  last_visited: '2025-01-02T19:38:28.512000+00:00'
  main_tex_file: null
  published_date: '2024-09-06T17:53:26Z'
  state: open
  title: Theory, Analysis, and Best Practices for Sigmoid Self-Attention
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2409.04431
'2409.04580':
  abstract: 'We present a complete analysis of Fermi Large Area Telescope (LAT) data
    of

    GRB 221009A, the brightest Gamma-Ray Burst (GRB) ever detected. The burst

    emission above 30 MeV detected by the LAT preceded by 1 s the low-energy (< 10

    MeV) pulse that triggered the Fermi Gamma-Ray Burst Monitor (GBM), as has been

    observed in other GRBs. The prompt phase of GRB 221009A lasted a few hundred

    seconds. It was so bright that we identify a Bad Time Interval (BTI) of 64

    seconds caused by the extremely high flux of hard X-rays and soft gamma rays,

    during which the event reconstruction efficiency was poor and the dead time

    fraction quite high. The late-time emission decayed as a power law, but the

    extrapolation of the late-time emission during the first 450 seconds suggests

    that the afterglow started during the prompt emission. We also found that

    high-energy events observed by the LAT are incompatible with synchrotron

    origin, and, during the prompt emission, are more likely related to an extra

    component identified as synchrotron self-Compton (SSC). A remarkable 400 GeV

    photon, detected by the LAT 33 ks after the GBM trigger and directionally

    consistent with the location of GRB 221009A, is hard to explain as a product of

    SSC or TeV electromagnetic cascades, and the process responsible for its origin

    is uncertain. Because of its proximity and energetic nature, GRB 221009A is an

    extremely rare event.'
  arxivId: '2409.04580'
  arxiv_tags:
  - astro-ph.HE
  authors: M. Axelsson, M. Ajello, M. Arimoto, L. Baldini, J. Ballet, M. G. Baring,
    C. Bartolini, D. Bastieri, J. Becerra Gonzalez, R. Bellazzini, B. Berenji, E.
    Bissaldi, R. D. Blandford, R. Bonino, P. Bruel, S. Buson, R. A. Cameron, R. Caputo,
    P. A. Caraveo, E. Cavazzuti, C. C. Cheung, G. Chiaro, N. Cibrario, S. Ciprini,
    G. Cozzolongo, P. Cristarella Orestano, M. Crnogorcevic, A. Cuoco, S. Cutini,
    F. D'Ammando, S. De Gaetano, N. Di Lalla, A. Dinesh, R. Di Tria, L. Di Venere,
    A. Domínguez, S. J. Fegan, E. C. Ferrara, A. Fiori, A. Franckowiak, Y. Fukazawa,
    S. Funk, P. Fusco, G. Galanti, F. Gargano, C. Gasbarra, S. Germani, F. Giacchino,
    N. Giglietto, M. Giliberti, R. Gill, F. Giordano, M. Giroletti, J. Granot, D.
    Green, I. A. Grenier, S. Guiriec, M. Gustafsson, M. Hashizume, E. Hays, J. W.
    Hewitt, D. Horan, T. Kayanoki, M. Kuss, A. Laviron, J. Li, I. Liodakis, F. Longo,
    F. Loparco, L. Lorusso, B. Lott, M. N. Lovellette, P. Lubrano, S. Maldera, D.
    Malyshev, A. Manfreda, G. Martí-Devesa, R. Martinelli, I. Martinez Castellanos,
    M. N. Mazziotta, J. E. McEnery, I. Mereu, M. Meyer, P. F. Michelson, N. Mirabal,
    W. Mitthumsiri, T. Mizuno, P. Monti-Guarnieri, M. E. Monzani, T. Morishita, A.
    Morselli, I. V. Moskalenko, M. Negro, R. Niwa, N. Omodei, M. Orienti, E. Orlando,
    D. Paneque, G. Panzarini, M. Persic, M. Pesce-Rollins, V. Petrosian, R. Pillera,
    F. Piron, T. A. Porter, G. Principe, J. L. Racusin, S. Rainò, R. Rando, B. Rani,
    M. Razzano, S. Razzaque, A. Reimer, O. Reimer, F. Ryde, M. Sánchez-Conde, P. M.
    Saz Parkinson, D. Serini, C. Sgrò, V. Sharma, E. J. Siskind, G. Spandre, P. Spinelli,
    D. J. Suson, H. Tajima, D. Tak, J. B. Thayer, D. F. Torres, J. Valverde, G. Zaharijas,
    S. Lesage, M. S. Briggs, E. Burns, S. Bala, P. N. Bhat, W. H. Cleveland, S. Dalessi,
    C. de Barra, M. Gibby, M. M. Giles, R. Hamburg, B. A. Hristov, C. M. Hui, D. Kocevski,
    B. Mailyan, C. Malacaria, S. McBreen, S. Poolakkil, O. J. Roberts, L. Scotton,
    P. Veres, A. von Kienlin, C. A. Wilson-Hodge, J. Wood
  created_at: '2025-01-13T06:08:21.060751'
  issue_number: 953
  issue_url: https://github.com/dmarx/papers-feed/issues/953
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-13T06:27:58.445781'
  last_visited: '2025-01-13T06:26:26.740000+00:00'
  main_tex_file: null
  published_date: '2024-09-06T19:42:28Z'
  state: open
  title: 'GRB 221009A: the B.O.A.T Burst that Shines in Gamma Rays'
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2409.04580
'2409.05816':
  abstract: 'Quality pretraining data is often seen as the key to high-performance

    language models. However, progress in understanding pretraining data has been

    slow due to the costly pretraining runs required for data selection

    experiments. We present a framework that avoids these costs and selects

    high-quality pretraining data without any LLM training of our own. Our work is

    based on a simple observation: LLM losses on many pretraining texts are

    correlated with downstream benchmark performance, and selecting

    high-correlation documents is an effective pretraining data selection method.

    We build a new statistical framework for data selection centered around

    estimates of perplexity-benchmark correlations and perform data selection using

    a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of

    thousands of web domains. In controlled pretraining experiments at the 160M

    parameter scale on 8 benchmarks, our approach outperforms DSIR on every

    benchmark, while matching the best data selector found in DataComp-LM, a

    hand-engineered bigram classifier.'
  arxivId: '2409.05816'
  arxiv_tags:
  - cs.CL
  - cs.LG
  - stat.ML
  authors: Tristan Thrush, Christopher Potts, Tatsunori Hashimoto
  created_at: '2025-01-04T06:53:00.603803'
  issue_number: 658
  issue_url: https://github.com/dmarx/papers-feed/issues/658
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:00.604590'
  last_visited: '2024-12-30T20:20:51.220Z'
  main_tex_file: null
  published_date: '2024-09-09T17:23:29Z'
  state: open
  title: Improving Pretraining Data Using Perplexity Correlations
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2409.05816
'2409.08514':
  abstract: 'Audio restoration has become increasingly significant in modern society,
    not

    only due to the demand for high-quality auditory experiences enabled by

    advanced playback devices, but also because the growing capabilities of

    generative audio models necessitate high-fidelity audio. Typically, audio

    restoration is defined as a task of predicting undistorted audio from damaged

    input, often trained using a GAN framework to balance perception and

    distortion. Since audio degradation is primarily concentrated in mid- and

    high-frequency ranges, especially due to codecs, a key challenge lies in

    designing a generator capable of preserving low-frequency information while

    accurately reconstructing high-quality mid- and high-frequency content.

    Inspired by recent advancements in high-sample-rate music separation, speech

    enhancement, and audio codec models, we propose Apollo, a generative model

    designed for high-sample-rate audio restoration. Apollo employs an explicit

    frequency band split module to model the relationships between different

    frequency bands, allowing for more coherent and higher-quality restored audio.

    Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently

    outperforms existing SR-GAN models across various bit rates and music genres,

    particularly excelling in complex scenarios involving mixtures of multiple

    instruments and vocals. Apollo significantly improves music restoration quality

    while maintaining computational efficiency. The source code for Apollo is

    publicly available at https://github.com/JusperLee/Apollo.'
  arxivId: '2409.08514'
  arxiv_tags:
  - cs.SD
  - cs.AI
  - eess.AS
  authors: Kai Li, Yi Luo
  created_at: '2025-01-04T14:48:42.234049'
  issue_number: 576
  issue_url: https://github.com/dmarx/papers-feed/issues/576
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:42.234855'
  last_visited: '2024-12-30T05:02:38.295Z'
  main_tex_file: null
  published_date: '2024-09-13T03:25:34Z'
  state: open
  title: 'Apollo: Band-sequence Modeling for High-Quality Audio Restoration'
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2409.08514
'2409.08861':
  abstract: 'Dynamical generative models that produce samples through an iterative

    process, such as Flow Matching and denoising diffusion models, have seen

    widespread use, but there have not been many theoretically-sound methods for

    improving these models with reward fine-tuning. In this work, we cast reward

    fine-tuning as stochastic optimal control (SOC). Critically, we prove that a

    very specific memoryless noise schedule must be enforced during fine-tuning, in

    order to account for the dependency between the noise variable and the

    generated samples. We also propose a new algorithm named Adjoint Matching which

    outperforms existing SOC algorithms, by casting SOC problems as a regression

    problem. We find that our approach significantly improves over existing methods

    for reward fine-tuning, achieving better consistency, realism, and

    generalization to unseen human preference reward models, while retaining sample

    diversity.'
  arxivId: '2409.08861'
  arxiv_tags:
  - cs.LG
  - math.OC
  - stat.ML
  authors: Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen
  created_at: '2025-01-13T06:45:25.929648'
  issue_number: 958
  issue_url: https://github.com/dmarx/papers-feed/issues/958
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-13T06:52:55.474406'
  last_visited: '2025-01-13T06:50:13.853000+00:00'
  main_tex_file: null
  published_date: '2024-09-13T14:22:14Z'
  state: open
  title: "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with\n\
    \  Memoryless Stochastic Optimal Control"
  total_reading_time_seconds: 21
  url: https://arxiv.org/abs/2409.08861
'2409.11321':
  abstract: "There is growing evidence of the effectiveness of Shampoo, a higher-order\n\
    preconditioning method, over Adam in deep learning optimization tasks. However,\n\
    Shampoo's drawbacks include additional hyperparameters and computational\noverhead\
    \ when compared to Adam, which only updates running averages of first-\nand second-moment\
    \ quantities. This work establishes a formal connection between\nShampoo (implemented\
    \ with the 1/2 power) and Adafactor -- a memory-efficient\napproximation of Adam\
    \ -- showing that Shampoo is equivalent to running\nAdafactor in the eigenbasis\
    \ of Shampoo's preconditioner. This insight leads to\nthe design of a simpler\
    \ and computationally efficient algorithm:\n$\\textbf{S}$hampo$\\textbf{O}$ with\
    \ $\\textbf{A}$dam in the\n$\\textbf{P}$reconditioner's eigenbasis (SOAP).\n \
    \ With regards to improving Shampoo's computational efficiency, the most\nstraightforward\
    \ approach would be to simply compute Shampoo's\neigendecomposition less frequently.\
    \ Unfortunately, as our empirical results\nshow, this leads to performance degradation\
    \ that worsens with this frequency.\nSOAP mitigates this degradation by continually\
    \ updating the running average of\nthe second moment, just as Adam does, but in\
    \ the current (slowly changing)\ncoordinate basis. Furthermore, since SOAP is\
    \ equivalent to running Adam in a\nrotated space, it introduces only one additional\
    \ hyperparameter (the\npreconditioning frequency) compared to Adam. We empirically\
    \ evaluate SOAP on\nlanguage model pre-training with 360m and 660m sized models.\
    \ In the large batch\nregime, SOAP reduces the number of iterations by over 40%\
    \ and wall clock time\nby over 35% compared to AdamW, with approximately 20% improvements\
    \ in both\nmetrics compared to Shampoo. An implementation of SOAP is available\
    \ at\nhttps://github.com/nikhilvyas/SOAP."
  arxivId: '2409.11321'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener,
    Lucas Janson, Sham Kakade
  created_at: '2025-01-05T18:40:57.663975'
  issue_number: 804
  issue_url: https://github.com/dmarx/papers-feed/issues/804
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T18:45:46.713842'
  last_visited: '2025-01-05T18:45:46.230Z'
  main_tex_file: null
  published_date: '2024-09-17T16:18:05Z'
  state: open
  title: 'SOAP: Improving and Stabilizing Shampoo using Adam'
  total_reading_time_seconds: 9
  url: https://arxiv.org/abs/2409.11321
'2409.13731':
  abstract: 'The recently developed retrieval-augmented generation (RAG) technology
    has

    enabled the efficient construction of domain-specific applications. However, it

    also has limitations, including the gap between vector similarity and the

    relevance of knowledge reasoning, as well as insensitivity to knowledge logic,

    such as numerical values, temporal relations, expert rules, and others, which

    hinder the effectiveness of professional knowledge services. In this work, we

    introduce a professional domain knowledge service framework called Knowledge

    Augmented Generation (KAG). KAG is designed to address the aforementioned

    challenges with the motivation of making full use of the advantages of

    knowledge graph(KG) and vector retrieval, and to improve generation and

    reasoning performance by bidirectionally enhancing large language models (LLMs)

    and KGs through five key aspects: (1) LLM-friendly knowledge representation,

    (2) mutual-indexing between knowledge graphs and original chunks, (3)

    logical-form-guided hybrid reasoning engine, (4) knowledge alignment with

    semantic reasoning, and (5) model capability enhancement for KAG. We compared

    KAG with existing RAG methods in multihop question answering and found that it

    significantly outperforms state-of-theart methods, achieving a relative

    improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We

    have successfully applied KAG to two professional knowledge Q&A tasks of Ant

    Group, including E-Government Q&A and E-Health Q&A, achieving significant

    improvement in professionalism compared to RAG methods.'
  arxivId: '2409.13731'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong,
    Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu,
    Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou
  created_at: '2025-01-04T06:52:18.611027'
  issue_number: 720
  issue_url: https://github.com/dmarx/papers-feed/issues/720
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T15:07:25.587Z'
  main_tex_file: null
  published_date: '2024-09-10T02:00:28Z'
  state: open
  title: "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.13731
'2409.16986':
  abstract: 'Data selection is of great significance in pre-training large language

    models, given the variation in quality within the large-scale available

    training corpora. To achieve this, researchers are currently investigating the

    use of data influence to measure the importance of data instances, $i.e.,$ a

    high influence score indicates that incorporating this instance to the training

    set is likely to enhance the model performance. Consequently, they select the

    top-$k$ instances with the highest scores. However, this approach has several

    limitations. (1) Computing the influence of all available data is

    time-consuming. (2) The selected data instances are not diverse enough, which

    may hinder the pre-trained model''s ability to generalize effectively to various

    downstream tasks. In this paper, we introduce \texttt{Quad}, a data selection

    approach that considers both quality and diversity by using data influence to

    achieve state-of-the-art pre-training results. In particular, noting that

    attention layers capture extensive semantic details, we have adapted the

    accelerated $iHVP$ computation methods for attention layers, enhancing our

    ability to evaluate the influence of data, $i.e.,$ its quality. For the

    diversity, \texttt{Quad} clusters the dataset into similar data instances

    within each cluster and diverse instances across different clusters. For each

    cluster, if we opt to select data from it, we take some samples to evaluate the

    influence to prevent processing all instances. To determine which clusters to

    select, we utilize the classic Multi-Armed Bandit method, treating each cluster

    as an arm. This approach favors clusters with highly influential instances

    (ensuring high quality) or clusters that have been selected less frequently

    (ensuring diversity), thereby well balancing between quality and diversity.'
  arxivId: '2409.16986'
  arxiv_tags:
  - cs.AI
  authors: Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin
    Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, Ye Yuan, Guoren Wang, Conghui
    He
  created_at: '2025-01-04T06:52:54.606152'
  issue_number: 637
  issue_url: https://github.com/dmarx/papers-feed/issues/637
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.651949'
  last_visited: '2024-12-30T20:04:00.391Z'
  main_tex_file: null
  published_date: '2024-09-25T14:49:29Z'
  state: open
  title: "Harnessing Diversity for Important Data Selection in Pretraining Large\n\
    \  Language Models"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2409.16986
'2409.19256':
  abstract: 'Reinforcement Learning from Human Feedback (RLHF) is widely used in Large

    Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow,

    where each node represents computation of a neural network (NN) and each edge

    denotes data dependencies between the NNs. RLHF complicates the dataflow by

    expanding each node into a distributed LLM training or generation program, and

    each edge into a many-to-many multicast. Traditional RL frameworks execute the

    dataflow using a single controller to instruct both intra-node computation and

    inter-node communication, which can be inefficient in RLHF due to large control

    dispatch overhead for distributed intra-node computation. Existing RLHF systems

    adopt a multi-controller paradigm, which can be inflexible due to nesting

    distributed computation and data communication. We propose HybridFlow, which

    combines single-controller and multi-controller paradigms in a hybrid manner to

    enable flexible representation and efficient execution of the RLHF dataflow. We

    carefully design a set of hierarchical APIs that decouple and encapsulate

    computation and data dependencies in the complex RLHF dataflow, allowing

    efficient operation orchestration to implement RLHF algorithms and flexible

    mapping of the computation onto various devices. We further design a

    3D-HybridEngine for efficient actor model resharding between training and

    generation phases, with zero memory redundancy and significantly reduced

    communication overhead. Our experimental results demonstrate

    1.53$\times$~20.57$\times$ throughput improvement when running various RLHF

    algorithms using HybridFlow, as compared with state-of-the-art baselines.

    HybridFlow source code will be available at https://github.com/volcengine/verl.'
  arxivId: '2409.19256'
  arxiv_tags:
  - cs.LG
  - cs.DC
  - I.2
  authors: Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang,
    Yanghua Peng, Haibin Lin, Chuan Wu
  created_at: '2025-01-05T08:23:20.685734'
  issue_number: 790
  issue_url: https://github.com/dmarx/papers-feed/issues/790
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:20.687827'
  last_visited: '2025-01-05T08:22:18.012Z'
  main_tex_file: null
  published_date: '2024-09-28T06:20:03Z'
  state: open
  title: 'HybridFlow: A Flexible and Efficient RLHF Framework'
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/2409.19256
'2409.19606':
  abstract: 'We present hyper-connections, a simple yet effective method that can
    serve as

    an alternative to residual connections. This approach specifically addresses

    common drawbacks observed in residual connection variants, such as the seesaw

    effect between gradient vanishing and representation collapse. Theoretically,

    hyper-connections allow the network to adjust the strength of connections

    between features at different depths and dynamically rearrange layers. We

    conduct experiments focusing on the pre-training of large language models,

    including dense and sparse models, where hyper-connections show significant

    performance improvements over residual connections. Additional experiments

    conducted on vision tasks also demonstrate similar improvements. We anticipate

    that this method will be broadly applicable and beneficial across a wide range

    of AI problems.'
  arxivId: '2409.19606'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.CV
  - cs.NE
  authors: Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu,
    Qiyang Min, Xun Zhou
  created_at: '2025-01-04T06:52:21.616669'
  issue_number: 718
  issue_url: https://github.com/dmarx/papers-feed/issues/718
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T08:05:54.690Z'
  main_tex_file: null
  published_date: '2024-09-29T07:57:07Z'
  state: open
  title: Hyper-Connections
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.19606
'2410.00286':
  abstract: 'The prompt spectra of gamma-ray bursts are known to follow broadband

    continuum behavior over decades in energy. GRB 221009A, given the moniker the

    brightest of all time (BOAT), is the brightest gamma-ray burst identified in

    half a century of observations, and was first identified by the Fermi Gamma-ray

    Burst Monitor (GBM). On behalf of the Fermi-GBM Team, Lesage et al. (2023)

    described the initial GBM analysis. Ravasio et al. (2024) report the

    identification of a spectral line in part of the prompt emission of this burst,

    which they describe as evolving over 80 s from $\sim$12 MeV to 6 MeV. We report

    a GBM Team analysis on the Ravasio Line: 1) We cannot identify an instrumental

    effect that could have produced this signal, and 2) our method of calculating

    the statistical significance of the line shows it easily exceeds the 5$\sigma$

    discovery threshold. We additionally comment on the claim of the line beginning

    at earlier time intervals, up to 37 MeV, as reported in Zhang et al. (2024). We

    find that it is reasonable to utilize these measurements for characterization

    of the line evolution, with caution. We encourage theoretical studies exploring

    this newly discovered gamma-ray burst spectral feature, unless any rigorous

    alternative explanation unrelated to the emission from GRB 221009A is

    identified.'
  arxivId: '2410.00286'
  arxiv_tags:
  - astro-ph.HE
  - stat.AP
  authors: Eric Burns, Stephen Lesage, Adam Goldstein, Michael S. Briggs, Peter Veres,
    Suman Bala, Cuan de Barra, Elisabetta Bissaldi, William H Cleveland, Misty M Giles,
    Matthew Godwin, Boyan A. Hristov, C. Michelle Hui, Daniel Kocevski, Bagrat Mailyan,
    Christian Malacaria, Sheila McBreen, Robert Preece, Oliver J. Roberts, Lorenzo
    Scotton, A. von Kienlin, Colleen A. Wilson-Hodge, Joshua Wood
  created_at: '2025-01-13T06:08:24.046079'
  issue_number: 951
  issue_url: https://github.com/dmarx/papers-feed/issues/951
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-13T06:08:24.046881'
  last_visited: '2025-01-13T06:06:36.272Z'
  main_tex_file: null
  published_date: '2024-09-30T23:43:52Z'
  state: open
  title: Fermi-GBM Team Analysis on The Ravasio Line
  total_reading_time_seconds: 21
  url: https://arxiv.org/abs/2410.00286
'2410.01131':
  abstract: 'We propose a novel neural network architecture, the normalized Transformer

    (nGPT) with representation learning on the hypersphere. In nGPT, all vectors

    forming the embeddings, MLP, attention matrices and hidden states are unit norm

    normalized. The input stream of tokens travels on the surface of a hypersphere,

    with each layer contributing a displacement towards the target output

    predictions. These displacements are defined by the MLP and attention blocks,

    whose vector components also reside on the same hypersphere. Experiments show

    that nGPT learns much faster, reducing the number of training steps required to

    achieve the same accuracy by a factor of 4 to 20, depending on the sequence

    length.'
  arxivId: '2410.01131'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2025-01-04T15:03:15.852952'
  issue_number: 857
  issue_url: https://github.com/dmarx/papers-feed/issues/857
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-07T23:26:00.695853'
  last_visited: '2025-01-07T23:23:22.376Z'
  main_tex_file: null
  published_date: '2024-10-01T23:50:09Z'
  state: open
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  total_reading_time_seconds: 122
  url: https://arxiv.org/abs/2410.01131
'2410.02423':
  abstract: 'In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm

    for solving imaging inverse problems. PnP methods leverage the strength of

    pre-trained denoisers, often deep neural networks, by integrating them in

    optimization schemes. While they achieve state-of-the-art performance on

    various inverse problems in imaging, PnP approaches face inherent limitations

    on more generative tasks like inpainting. On the other hand, generative models

    such as Flow Matching pushed the boundary in image sampling yet lack a clear

    method for efficient use in image restoration. We propose to combine the PnP

    framework with Flow Matching (FM) by defining a time-dependent denoiser using
    a

    pre-trained FM model. Our algorithm alternates between gradient descent steps

    on the data-fidelity term, reprojections onto the learned FM path, and

    denoising. Notably, our method is computationally efficient and

    memory-friendly, as it avoids backpropagation through ODEs and trace

    computations. We evaluate its performance on denoising, super-resolution,

    deblurring, and inpainting tasks, demonstrating superior results compared to

    existing PnP algorithms and Flow Matching based state-of-the-art methods.'
  arxivId: '2410.02423'
  arxiv_tags:
  - cs.CV
  - cs.LG
  authors: Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
  created_at: '2025-01-05T08:24:35.492733'
  issue_number: 115
  issue_url: https://github.com/dmarx/papers-feed/issues/115
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:43:50.011Z'
  main_tex_file: null
  published_date: '2024-10-03T12:13:56Z'
  state: open
  title: 'PnP-Flow: Plug-and-Play Image Restoration with Flow Matching'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.02423
'2410.05437':
  abstract: 'We propose ESPACE, an LLM compression technique based on dimensionality

    reduction of activations. Unlike prior works on weight-centric tensor

    decomposition, ESPACE projects activations onto a pre-calibrated set of

    principal components. The activation-centrality of the approach enables

    retraining LLMs with no loss of expressivity; while at inference, weight

    decomposition is obtained as a byproduct of matrix multiplication

    associativity. Theoretical results on the construction of projection matrices

    with optimal computational accuracy are provided. Experimentally, we find

    ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small

    accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At

    lower compression rates of 20% to 40%, ESPACE drives GPT3 models to

    outperforming their baseline, by up to a 0.38 decrease in perplexity for

    GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency

    on existing hardware. Comparison with related works on compressing Llama2-7B

    via matrix factorization shows that ESPACE is a first step in advancing the

    state-of-the-art in tensor decomposition compression of LLMs.'
  arxivId: '2410.05437'
  arxiv_tags:
  - cs.LG
  authors: Charbel Sakr, Brucek Khailany
  created_at: '2025-01-10T20:44:15.235532'
  issue_number: 915
  issue_url: https://github.com/dmarx/papers-feed/issues/915
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T20:44:15.236395'
  last_visited: '2025-01-10T20:41:20.568Z'
  main_tex_file: null
  published_date: '2024-10-07T18:59:22Z'
  state: open
  title: 'ESPACE: Dimensionality Reduction of Activations for Model Compression'
  total_reading_time_seconds: 23
  url: https://arxiv.org/abs/2410.05437
'2410.08800':
  abstract: 'This paper presents a comprehensive overview of the data preparation
    pipeline

    developed for the OpenGPT-X project, a large-scale initiative aimed at creating

    open and high-performance multilingual large language models (LLMs). The

    project goal is to deliver models that cover all major European languages, with

    a particular focus on real-world applications within the European Union. We

    explain all data processing steps, starting with the data selection and

    requirement definition to the preparation of the final datasets for model

    training. We distinguish between curated data and web data, as each of these

    categories is handled by distinct pipelines, with curated data undergoing

    minimal filtering and web data requiring extensive filtering and deduplication.

    This distinction guided the development of specialized algorithmic solutions

    for both pipelines. In addition to describing the processing methodologies, we

    provide an in-depth analysis of the datasets, increasing transparency and

    alignment with European data regulations. Finally, we share key insights and

    challenges faced during the project, offering recommendations for future

    endeavors in large-scale multilingual data preparation for LLMs.'
  arxivId: '2410.08800'
  arxiv_tags:
  - cs.CL
  - H.3.1; I.2.7
  authors: Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer,
    Benny Jörg Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard
    Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian Küch, Luzian
    Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas
    Flores-Herr, Joachim Köhler, Johannes Leveling
  created_at: '2025-01-04T06:52:45.621160'
  issue_number: 645
  issue_url: https://github.com/dmarx/papers-feed/issues/645
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:45.622955'
  last_visited: '2024-12-30T20:06:07.375Z'
  main_tex_file: null
  published_date: '2024-10-11T13:34:24Z'
  state: open
  title: Data Processing for the OpenGPT-X Model Family
  total_reading_time_seconds: 47
  url: https://arxiv.org/abs/2410.08800
'2410.10792':
  abstract: 'Generative models transform random noise into images; their inversion
    aims to

    transform images back to structured noise for recovery and editing. This paper

    addresses two key tasks: (i) inversion and (ii) editing of a real image using

    stochastic equivalents of rectified flow models (such as Flux). Although

    Diffusion Models (DMs) have recently dominated the field of generative modeling

    for images, their inversion presents faithfulness and editability challenges

    due to nonlinearities in drift and diffusion. Existing state-of-the-art DM

    inversion approaches rely on training of additional parameters or test-time

    optimization of latent variables; both are expensive in practice. Rectified

    Flows (RFs) offer a promising alternative to diffusion models, yet their

    inversion has been underexplored. We propose RF inversion using dynamic optimal

    control derived via a linear quadratic regulator. We prove that the resulting

    vector field is equivalent to a rectified stochastic differential equation.

    Additionally, we extend our framework to design a stochastic sampler for Flux.

    Our inversion method allows for state-of-the-art performance in zero-shot

    inversion and editing, outperforming prior works in stroke-to-image synthesis

    and semantic image editing, with large-scale human evaluations confirming user

    preference.'
  arxivId: '2410.10792'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - stat.ML
  authors: Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai,
    Wen-Sheng Chu
  created_at: '2025-01-04T14:48:30.274158'
  issue_number: 590
  issue_url: https://github.com/dmarx/papers-feed/issues/590
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T15:14:34.707Z'
  main_tex_file: null
  published_date: '2024-10-14T17:56:24Z'
  state: open
  title: "Semantic Image Inversion and Editing using Rectified Stochastic\n  Differential\
    \ Equations"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.10792
'2410.13835':
  abstract: "Practitioners have consistently observed three puzzling phenomena in\n\
    transformer-based large language models (LLMs): attention sinks, value-state\n\
    drains, and residual-state peaks, collectively referred to as extreme-token\n\
    phenomena. These phenomena are characterized by certain so-called \"sink tokens\"\
    \nreceiving disproportionately high attention weights, exhibiting significantly\n\
    smaller value states, and having much larger residual-state norms than those of\n\
    other tokens. These extreme tokens give rise to various challenges in LLM\ninference,\
    \ quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token\
    \ phenomena. First, we show\nthat these phenomena arise in very simple architectures\
    \ -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy\
    \ (BB) task.\nIn this setting, we identify an active-dormant mechanism, where\
    \ attention heads\nbecome sinks for specific input domains while remaining non-sinks\
    \ for others.\nOur theoretical analysis of the training dynamics reveals that\
    \ these phenomena\nare driven by a mutual reinforcement mechanism. Building on\
    \ these insights, we\npropose strategies to mitigate extreme-token phenomena during\
    \ pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next,\
    \ we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing\
    \ that many\nattention heads exhibit a similar active-dormant mechanism as in\
    \ the BB task,\nand that the mutual reinforcement mechanism also governs the emergence\
    \ of\nextreme-token phenomena during LLM pretraining. Our results reveal that\
    \ many of\nthe static and dynamic properties of extreme-token phenomena predicted\
    \ by the\nBB task align with observations in pretrained LLMs."
  arxivId: '2410.13835'
  arxiv_tags:
  - cs.LG
  authors: Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei
  created_at: '2025-01-04T06:52:39.613983'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:39.614755'
  last_visited: '2024-12-30T22:17:13.815000+00:00'
  main_tex_file: null
  published_date: '2024-10-17T17:54:06Z'
  state: open
  title: "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token\
    \ Phenomena in LLMs"
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2410.13835
'2410.15468':
  abstract: 'We consider emergence from the perspective of dynamics: states of a system

    evolving with time. We focus on the role of a decomposition of wholes into

    parts, and attempt to characterize relationships between levels without

    reference to whether higher-level properties are "novel" or "unexpected." We

    offer a classification of different varieties of emergence, with and without

    new ontological elements at higher levels.'
  arxivId: '2410.15468'
  arxiv_tags:
  - physics.hist-ph
  - cond-mat.stat-mech
  authors: Sean M. Carroll, Achyuth Parola
  created_at: '2025-01-05T08:24:47.514351'
  issue_number: 107
  issue_url: https://github.com/dmarx/papers-feed/issues/107
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T05:48:56.244Z'
  main_tex_file: null
  published_date: '2024-10-20T18:45:11Z'
  state: open
  title: What Emergence Can Possibly Mean
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.15468
'2410.21265':
  abstract: 'An old idea in optimization theory says that since the gradient is a
    dual

    vector it may not be subtracted from the weights without first being mapped to

    the primal space where the weights reside. We take this idea seriously in this

    paper and construct such a duality map for general neural networks. Our map,

    which we call modular dualization, forms a unifying theoretical basis for

    training algorithms that are a) fast and b) scalable. Modular dualization

    involves first assigning operator norms to layers based on the semantics of

    each layer, and then using these layerwise norms to recursively induce a

    duality map on the weight space of the full neural architecture. We conclude by

    deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers

    -- the latter two methods are based on a rectangular Newton-Schulz iteration

    (Kovarik, 1970; Bj\"orck & Bowie, 1971). A variant of our methods was used to

    set speed records for training NanoGPT. Overall, we hope that our theory of

    modular duality will yield a next generation of fast and scalable optimizers

    for general neural architectures.'
  arxivId: '2410.21265'
  arxiv_tags:
  - cs.LG
  - cs.NE
  - stat.ML
  authors: Jeremy Bernstein, Laker Newhouse
  created_at: '2025-01-04T06:52:09.651323'
  issue_number: 757
  issue_url: https://github.com/dmarx/papers-feed/issues/757
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T19:43:53.760Z'
  main_tex_file: null
  published_date: '2024-10-28T17:57:31Z'
  state: open
  title: Modular Duality in Deep Learning
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.21265
'2410.24054':
  abstract: 'We develop EigenVI, an eigenvalue-based approach for black-box variational

    inference (BBVI). EigenVI constructs its variational approximations from

    orthogonal function expansions. For distributions over $\mathbb{R}^D$, the

    lowest order term in these expansions provides a Gaussian variational

    approximation, while higher-order terms provide a systematic way to model

    non-Gaussianity. These approximations are flexible enough to model complex

    distributions (multimodal, asymmetric), but they are simple enough that one can

    calculate their low-order moments and draw samples from them. EigenVI can also

    model other types of random variables (e.g., nonnegative, bounded) by

    constructing variational approximations from different families of orthogonal

    functions. Within these families, EigenVI computes the variational

    approximation that best matches the score function of the target distribution

    by minimizing a stochastic estimate of the Fisher divergence. Notably, this

    optimization reduces to solving a minimum eigenvalue problem, so that EigenVI

    effectively sidesteps the iterative gradient-based optimizations that are

    required for many other BBVI algorithms. (Gradient-based methods can be

    sensitive to learning rates, termination criteria, and other tunable

    hyperparameters.) We use EigenVI to approximate a variety of target

    distributions, including a benchmark suite of Bayesian models from posteriordb.

    On these distributions, we find that EigenVI is more accurate than existing

    methods for Gaussian BBVI.'
  arxivId: '2410.24054'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - stat.CO
  authors: Diana Cai, Chirag Modi, Charles C. Margossian, Robert M. Gower, David M.
    Blei, Lawrence K. Saul
  created_at: '2025-01-05T08:24:44.485323'
  issue_number: 109
  issue_url: https://github.com/dmarx/papers-feed/issues/109
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T05:52:47.117Z'
  main_tex_file: null
  published_date: '2024-10-31T15:48:34Z'
  state: open
  title: "EigenVI: score-based variational inference with orthogonal function\n  expansions"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.24054
'2411.04282':
  abstract: 'Large language models (LLMs) have shown impressive capabilities, but
    still

    struggle with complex reasoning tasks requiring multiple steps. While

    prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at

    inference time, optimizing reasoning capabilities during training remains

    challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled

    framework that formulates reasoning as sampling from a latent distribution and

    optimizes it via variational approaches. LaTRO enables LLMs to concurrently

    improve both their reasoning process and ability to evaluate reasoning quality,

    without requiring external feedback or reward models. We validate LaTRO through

    experiments on GSM8K and ARC-Challenge datasets using multiple model

    architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of

    12.5% over base models and 9.6% over supervised fine-tuning across

    Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that

    pre-trained LLMs possess latent reasoning capabilities that can be unlocked and

    enhanced through our proposed optimization approach in a self-improvement

    manner. The code of LaTRO is available at

    \url{https://github.com/SalesforceAIResearch/LaTRO}.'
  arxivId: '2411.04282'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.LG
  - stat.ML
  - I.2.7
  authors: Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby
    Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
  created_at: '2025-01-05T08:23:47.508546'
  issue_number: 169
  issue_url: https://github.com/dmarx/papers-feed/issues/169
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T21:35:41.978Z'
  main_tex_file: null
  published_date: '2024-11-06T22:02:30Z'
  state: open
  title: "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities\
    \ via Self-Rewarding"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.04282
'2411.04330':
  abstract: 'Low precision training and inference affect both the quality and cost
    of

    language models, but current scaling laws do not account for this. In this

    work, we devise "precision-aware" scaling laws for both training and inference.

    We propose that training in lower precision reduces the model''s "effective

    parameter count," allowing us to predict the additional loss incurred from

    training in low precision and post-train quantization. For inference, we find

    that the degradation introduced by post-training quantization increases as

    models are trained on more data, eventually making additional pretraining data

    actively harmful. For training, our scaling laws allow us to predict the loss

    of a model with different parts in different precisions, and suggest that

    training larger models in lower precision may be compute optimal. We unify the

    scaling laws for post and pretraining quantization to arrive at a single

    functional form that predicts degradation from training and inference in varied

    precisions. We fit on over 465 pretraining runs and validate our predictions on

    model sizes up to 1.7B parameters trained on up to 26B tokens.'
  arxivId: '2411.04330'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas
    Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
  created_at: '2025-01-04T06:53:12.611718'
  issue_number: 651
  issue_url: https://github.com/dmarx/papers-feed/issues/651
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:12.612468'
  last_visited: '2024-12-30T20:16:37.184Z'
  main_tex_file: null
  published_date: '2024-11-07T00:10:10Z'
  state: open
  title: Scaling Laws for Precision
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2411.04330
'2411.05899':
  abstract: 'Bayes'' rule naturally allows for inference refinement in a streaming
    fashion,

    without the need to recompute posteriors from scratch whenever new data

    arrives. In principle, Bayesian streaming is straightforward: we update our

    prior with the available data and use the resulting posterior as a prior when

    processing the next data chunk. In practice, however, this recipe entails i)

    approximating an intractable posterior at each time step; and ii) encapsulating

    results appropriately to allow for posterior propagation. For continuous state

    spaces, variational inference (VI) is particularly convenient due to its

    scalability and the tractability of variational posteriors. For discrete state

    spaces, however, state-of-the-art VI results in analytically intractable

    approximations that are ill-suited for streaming settings. To enable streaming

    Bayesian inference over discrete parameter spaces, we propose streaming Bayes

    GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed

    GFlowNets -- a powerful class of amortized samplers for discrete compositional

    objects. Notably, SB-GFlowNet approximates the initial posterior using a

    standard GFlowNet and subsequently updates it using a tailored procedure that

    requires only the newly observed data. Our case studies in linear preference

    learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets

    in sampling from an unnormalized posterior in a streaming setting. As expected,

    we also observe that SB-GFlowNets is significantly faster than repeatedly

    training a GFlowNet from scratch to sample from the full posterior.'
  arxivId: '2411.05899'
  arxiv_tags:
  - cs.LG
  authors: Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita
  created_at: '2025-01-04T15:02:48.870281'
  issue_number: 339
  issue_url: https://github.com/dmarx/papers-feed/issues/339
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:48.872465'
  last_visited: '2024-12-28T06:09:10.852Z'
  main_tex_file: null
  published_date: '2024-11-08T15:53:56Z'
  state: open
  title: Streaming Bayes GFlowNets
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2411.05899
'2411.06068':
  abstract: 'In this technical report, we present Zyda-2: a five trillion token dataset

    for language model pretraining. Zyda-2 was used to train our Zamba2 series of

    models which are state-of-the-art for their weight class. We build Zyda-2 by

    collating high-quality open-source tokens such as FineWeb and DCLM, then

    distilling them to the highest-quality subset via cross-deduplication and

    model-based quality filtering. Zyda-2 is released under a permissive open

    license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2'
  arxivId: '2411.06068'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge
  created_at: '2025-01-04T06:52:36.611803'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:36.612646'
  last_visited: '2024-12-30T22:18:39.627000+00:00'
  main_tex_file: null
  published_date: '2024-11-09T04:57:41Z'
  state: open
  title: 'Zyda-2: a 5 Trillion Token High-Quality Dataset'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2411.06068
'2411.12372':
  abstract: 'Large language models are increasingly becoming a cornerstone technology
    in

    artificial intelligence, the sciences, and society as a whole, yet the optimal

    strategies for dataset composition and filtering remain largely elusive. Many

    of the top-performing models lack transparency in their dataset curation and

    model development processes, posing an obstacle to the development of fully

    open language models. In this paper, we identify three core data-related

    challenges that must be addressed to advance open-source language models. These

    include (1) transparency in model development, including the data curation

    process, (2) access to large quantities of high-quality data, and (3)

    availability of artifacts and metadata for dataset curation and analysis. To

    address these challenges, we release RedPajama-V1, an open reproduction of the

    LLaMA training dataset. In addition, we release RedPajama-V2, a massive

    web-only dataset consisting of raw, unfiltered text data together with quality

    signals and metadata. Together, the RedPajama datasets comprise over 100

    trillion tokens spanning multiple domains and with their quality signals

    facilitate the filtering of data, aiming to inspire the development of numerous

    new datasets. To date, these datasets have already been used in the training of

    strong language models used in production, such as Snowflake Arctic,

    Salesforce''s XGen and AI2''s OLMo. To provide insight into the quality of

    RedPajama, we present a series of analyses and ablation studies with

    decoder-only language models with up to 1.6B parameters. Our findings

    demonstrate how quality signals for web data can be effectively leveraged to

    curate high-quality subsets of the dataset, underscoring the potential of

    RedPajama to advance the development of transparent and high-performing

    language models at scale.'
  arxivId: '2411.12372'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton
    Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun,
    Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher
    Ré, Irina Rish, Ce Zhang
  created_at: '2025-01-04T06:53:06.611354'
  issue_number: 656
  issue_url: https://github.com/dmarx/papers-feed/issues/656
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:18:27.856Z'
  main_tex_file: null
  published_date: '2024-11-19T09:35:28Z'
  state: open
  title: 'RedPajama: an Open Dataset for Training Large Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.12372
'2411.18933':
  abstract: 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video

    object segmentation and tracking anything. Key components of SAM 2 that drive

    the impressive video object segmentation performance include a large multistage

    image encoder for frame feature extraction and a memory mechanism that stores

    memory contexts from past frames to help current frame segmentation. The high

    computation complexity of multistage image encoder and memory module has

    limited its applications in real-world tasks, e.g., video object segmentation

    on mobile devices. To address this limitation, we propose EfficientTAMs,

    lightweight track anything models that produce high-quality results with low

    latency and model size. Our idea is based on revisiting the plain,

    nonhierarchical Vision Transformer (ViT) as an image encoder for video object

    segmentation, and introducing an efficient memory module, which reduces the

    complexity for both frame feature extraction and memory computation for current

    frame segmentation. We take vanilla lightweight ViTs and efficient memory

    module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets

    for video object segmentation and track anything tasks. We evaluate on multiple

    video segmentation benchmarks including semi-supervised VOS and promptable

    video segmentation, and find that our proposed EfficientTAM with vanilla ViT

    perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and

    ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs

    also perform favorably over original SAM with ~20x speedup on A100 and ~20x

    parameter reduction. On mobile devices such as iPhone 15 Pro Max, our

    EfficientTAMs can run at ~10 FPS for performing video object segmentation with

    reasonable quality, highlighting the capability of small models for on-device

    video object segmentation applications.'
  arxivId: '2411.18933'
  arxiv_tags:
  - cs.CV
  authors: Yunyang Xiong, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, Zechun
    Liu, Saksham Suri, Balakrishnan Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman
    Krishnamoorthi, Bilge Soran, Vikas Chandra
  created_at: '2025-01-05T18:41:21.664186'
  issue_number: 37
  issue_url: https://github.com/dmarx/papers-feed/issues/37
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T20:13:52.905Z'
  main_tex_file: null
  published_date: '2024-11-28T05:52:10Z'
  state: open
  title: Efficient Track Anything
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.18933
'2411.19108':
  abstract: 'As a fundamental backbone for video generation, diffusion models are

    challenged by low inference speed due to the sequential nature of denoising.

    Previous methods speed up the models by caching and reusing model outputs at

    uniformly selected timesteps. However, such a strategy neglects the fact that

    differences among model outputs are not uniform across timesteps, which hinders

    selecting the appropriate model outputs to cache, leading to a poor balance

    between inference efficiency and visual quality. In this study, we introduce

    Timestep Embedding Aware Cache (TeaCache), a training-free caching approach

    that estimates and leverages the fluctuating differences among model outputs

    across timesteps. Rather than directly using the time-consuming model outputs,

    TeaCache focuses on model inputs, which have a strong correlation with the

    modeloutputs while incurring negligible computational cost. TeaCache first

    modulates the noisy inputs using the timestep embeddings to ensure their

    differences better approximating those of model outputs. TeaCache then

    introduces a rescaling strategy to refine the estimated differences and

    utilizes them to indicate output caching. Experiments show that TeaCache

    achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%

    Vbench score) degradation of visual quality.'
  arxivId: '2411.19108'
  arxiv_tags:
  - cs.CV
  authors: Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao,
    Yingya Zhang, Qixiang Ye, Fang Wan
  created_at: '2025-01-04T14:48:33.226940'
  issue_number: 586
  issue_url: https://github.com/dmarx/papers-feed/issues/586
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:33.229442'
  last_visited: '2024-12-30T15:10:27.225Z'
  main_tex_file: null
  published_date: '2024-11-28T12:50:05Z'
  state: open
  title: 'Timestep Embedding Tells: It''s Time to Cache for Video Diffusion Model'
  total_reading_time_seconds: 40
  url: https://arxiv.org/abs/2411.19108
'2411.19722':
  abstract: 'Removing modeling constraints and unifying architectures across domains
    has

    been a key driver of the recent progress in training large multimodal models.

    However, most of these models still rely on many separately trained components

    such as modality-specific encoders and decoders. In this work, we further

    streamline joint generative modeling of images and text. We propose an

    autoregressive decoder-only transformer - JetFormer - which is trained to

    directly maximize the likelihood of raw data, without relying on any separately

    pretrained components, and can understand and generate both text and images.

    Specifically, we leverage a normalizing flow model to obtain a soft-token image

    representation that is jointly trained with an autoregressive multimodal

    transformer. The normalizing flow model serves as both an image encoder for

    perception tasks and an image decoder for image generation tasks during

    inference. JetFormer achieves text-to-image generation quality competitive with

    recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained

    image autoencoders, which are trained with a complex mixture of losses,

    including perceptual ones. At the same time, JetFormer demonstrates robust

    image understanding capabilities. To the best of our knowledge, JetFormer is

    the first model that is capable of generating high-fidelity images and

    producing strong log-likelihood bounds.'
  arxivId: '2411.19722'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Michael Tschannen, André Susano Pinto, Alexander Kolesnikov
  created_at: '2025-01-05T18:41:12.932924'
  issue_number: 40
  issue_url: https://github.com/dmarx/papers-feed/issues/40
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T18:41:15.660935'
  last_visited: '2024-12-15T22:16:44.245Z'
  main_tex_file: null
  published_date: '2024-11-29T14:14:59Z'
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_seconds: 300
  url: https://arxiv.org/abs/2411.19722
'2412.00733':
  abstract: 'Existing methodologies for animating portrait images face significant

    challenges, particularly in handling non-frontal perspectives, rendering

    dynamic objects around the portrait, and generating immersive, realistic

    backgrounds. In this paper, we introduce the first application of a pretrained

    transformer-based video generative model that demonstrates strong

    generalization capabilities and generates highly dynamic, realistic videos for

    portrait animation, effectively addressing these challenges. The adoption of a

    new video backbone model makes previous U-Net-based methods for identity

    maintenance, audio conditioning, and video extrapolation inapplicable. To

    address this limitation, we design an identity reference network consisting of

    a causal 3D VAE combined with a stacked series of transformer layers, ensuring

    consistent facial identity across video sequences. Additionally, we investigate

    various speech audio conditioning and motion frame mechanisms to enable the

    generation of continuous video driven by speech audio. Our method is validated

    through experiments on benchmark and newly proposed wild datasets,

    demonstrating substantial improvements over prior methods in generating

    realistic portraits characterized by diverse orientations within dynamic and

    immersive scenes. Further visualizations and the source code are available at:

    https://fudan-generative-vision.github.io/hallo3/.'
  arxivId: '2412.00733'
  arxiv_tags:
  - cs.CV
  - cs.GR
  - cs.LG
  authors: Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan
    Mu, Hang Zhou, Jingdong Wang, Siyu Zhu
  created_at: '2025-01-10T18:23:26.486602'
  issue_number: 909
  issue_url: https://github.com/dmarx/papers-feed/issues/909
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T18:23:26.487447'
  last_visited: '2025-01-10T18:21:47.668Z'
  main_tex_file: null
  published_date: '2024-12-01T08:54:30Z'
  state: open
  title: "Hallo3: Highly Dynamic and Realistic Portrait Image Animation with\n  Diffusion\
    \ Transformer Networks"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2412.00733
'2412.01023':
  abstract: 'Most real-world datasets consist of a natural hierarchy between classes
    or an

    inherent label structure that is either already available or can be constructed

    cheaply. However, most existing representation learning methods ignore this

    hierarchy, treating labels as permutation invariant. Recent work [Zeng et al.,

    2022] proposes using this structured information explicitly, but the use of

    Euclidean distance may distort the underlying semantic context [Chen et al.,

    2013]. In this work, motivated by the advantage of hyperbolic spaces in

    modeling hierarchical relationships, we propose a novel approach HypStructure:

    a Hyperbolic Structured regularization approach to accurately embed the label

    hierarchy into the learned representations. HypStructure is a

    simple-yet-effective regularizer that consists of a hyperbolic tree-based

    representation loss along with a centering loss, and can be combined with any

    standard task loss to learn hierarchy-informed features. Extensive experiments

    on several large-scale vision benchmarks demonstrate the efficacy of

    HypStructure in reducing distortion and boosting generalization performance

    especially under low dimensional scenarios. For a better understanding of

    structured representation, we perform eigenvalue analysis that links the

    representation geometry to improved Out-of-Distribution (OOD) detection

    performance seen empirically. The code is available at

    \url{https://github.com/uiuctml/HypStructure}.'
  arxivId: '2412.01023'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao
  created_at: '2025-01-04T14:49:39.264149'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:39.264976'
  last_visited: '2024-12-29T02:32:00.838000+00:00'
  main_tex_file: null
  published_date: '2024-12-02T00:56:44Z'
  state: open
  title: Learning Structured Representations with Hyperbolic Embeddings
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2412.01023
'2412.02595':
  abstract: 'Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved

    significant benchmark gains via aggressive model-based filtering, but at the

    cost of removing 90% of data. This limits their suitability for long token

    horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how

    to achieve better trade-offs between accuracy and data quantity by a

    combination of classifier ensembling, synthetic data rephrasing, and reduced

    reliance on heuristic filters. When training 8B parameter models for 1T tokens,

    using a high-quality subset of our data improves MMLU by 5.6 over DCLM,

    demonstrating the efficacy of our methods for boosting accuracies over a

    relatively short token horizon. Furthermore, our full 6.3T token dataset

    matches DCLM on MMLU, but contains four times more unique real tokens than

    DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B

    parameter model trained for 15T tokens, of which 7.2T came from our dataset, is

    better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5

    on average across ten diverse tasks. The dataset is available at

    https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html'
  arxivId: '2412.02595'
  arxiv_tags:
  - cs.CL
  authors: Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl,
    Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro
  created_at: '2025-01-04T06:53:24.608407'
  issue_number: 632
  issue_url: https://github.com/dmarx/papers-feed/issues/632
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:02:25.502Z'
  main_tex_file: null
  published_date: '2024-12-03T17:28:50Z'
  state: open
  title: "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon\n  Pretraining\
    \ Dataset"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.02595
'2412.03603':
  abstract: 'Recent advancements in video generation have significantly impacted daily

    life for both individuals and industries. However, the leading video generation

    models remain closed-source, resulting in a notable performance gap between

    industry capabilities and those available to the public. In this report, we

    introduce HunyuanVideo, an innovative open-source video foundation model that

    demonstrates performance in video generation comparable to, or even surpassing,

    that of leading closed-source models. HunyuanVideo encompasses a comprehensive

    framework that integrates several key elements, including data curation,

    advanced architectural design, progressive model scaling and training, and an

    efficient infrastructure tailored for large-scale model training and inference.

    As a result, we successfully trained a video generative model with over 13

    billion parameters, making it the largest among all open-source models. We

    conducted extensive experiments and implemented a series of targeted designs to

    ensure high visual quality, motion dynamics, text-video alignment, and advanced

    filming techniques. According to evaluations by professionals, HunyuanVideo

    outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,

    and three top-performing Chinese video generative models. By releasing the code

    for the foundation model and its applications, we aim to bridge the gap between

    closed-source and open-source communities. This initiative will empower

    individuals within the community to experiment with their ideas, fostering a

    more dynamic and vibrant video generation ecosystem. The code is publicly

    available at https://github.com/Tencent/HunyuanVideo.'
  arxivId: '2412.03603'
  arxiv_tags:
  - cs.CV
  authors: Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng
    Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin
    Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan,
    Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai
    Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng,
    Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang
    Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang,
    Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong
  created_at: '2025-01-04T06:53:33.619685'
  issue_number: 591
  issue_url: https://github.com/dmarx/papers-feed/issues/591
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:27.533139'
  last_visited: '2024-12-30T15:16:27.366Z'
  main_tex_file: null
  published_date: '2024-12-03T23:52:37Z'
  state: open
  title: 'HunyuanVideo: A Systematic Framework For Large Video Generative Models'
  total_reading_time_seconds: 74
  url: https://arxiv.org/abs/2412.03603
'2412.04384':
  abstract: '3D semantic occupancy prediction is an important task for robust

    vision-centric autonomous driving, which predicts fine-grained geometry and

    semantics of the surrounding scene. Most existing methods leverage dense

    grid-based scene representations, overlooking the spatial sparsity of the

    driving scenes. Although 3D semantic Gaussian serves as an object-centric

    sparse alternative, most of the Gaussians still describe the empty region with

    low efficiency. To address this, we propose a probabilistic Gaussian

    superposition model which interprets each Gaussian as a probability

    distribution of its neighborhood being occupied and conforms to probabilistic

    multiplication to derive the overall geometry. Furthermore, we adopt the exact

    Gaussian mixture model for semantics calculation to avoid unnecessary

    overlapping of Gaussians. To effectively initialize Gaussians in non-empty

    region, we design a distribution-based initialization module which learns the

    pixel-aligned occupancy distribution instead of the depth of surfaces. We

    conduct extensive experiments on nuScenes and KITTI-360 datasets and our

    GaussianFormer-2 achieves state-of-the-art performance with high efficiency.

    Code: https://github.com/huang-yh/GaussianFormer.'
  arxivId: '2412.04384'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.LG
  authors: Yuanhui Huang, Amonnut Thammatadatrakoon, Wenzhao Zheng, Yunpeng Zhang,
    Dalong Du, Jiwen Lu
  created_at: '2025-01-14T18:29:51.192224'
  issue_number: 972
  issue_url: https://github.com/dmarx/papers-feed/issues/972
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-14T18:29:51.193732'
  last_visited: '2025-01-14T18:28:38.121Z'
  main_tex_file: null
  published_date: '2024-12-05T17:59:58Z'
  state: open
  title: "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D\n\
    \  Occupancy Prediction"
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2412.04384
'2412.04619':
  abstract: 'Language models (LMs), like other neural networks, often favor shortcut

    heuristics based on surface-level patterns. Although LMs behave like n-gram

    models early in training, they must eventually learn hierarchical syntactic

    representations to correctly apply grammatical rules out-of-distribution (OOD).

    In this work, we use case studies of English grammar to explore how complex,

    diverse training data drives models to generalize OOD. We construct a framework

    that unifies our understanding of random variation with training dynamics, rule

    selection with memorization, and data diversity with complexity. We show that

    these factors are nuanced, and that intermediate levels of diversity and

    complexity lead to inconsistent behavior across random seeds and to unstable

    training dynamics. Our findings emphasize the critical role of training data in

    shaping generalization patterns and illuminate how competing model strategies

    lead to inconsistent generalization outcomes across random seeds. Code is

    available at https://github.com/sunnytqin/concept_comp.git.'
  arxivId: '2412.04619'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tian Qin, Naomi Saphra, David Alvarez-Melis
  created_at: '2025-01-05T08:23:53.656301'
  issue_number: 164
  issue_url: https://github.com/dmarx/papers-feed/issues/164
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T08:23:53.657116'
  last_visited: '2024-12-22T17:55:18.862Z'
  main_tex_file: null
  published_date: '2024-12-05T21:12:37Z'
  state: open
  title: 'Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization'
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/2412.04619
'2412.06264':
  abstract: 'Flow Matching (FM) is a recent framework for generative modeling that
    has

    achieved state-of-the-art performance across various domains, including image,

    video, audio, speech, and biological structures. This guide offers a

    comprehensive and self-contained review of FM, covering its mathematical

    foundations, design choices, and extensions. By also providing a PyTorch

    package featuring relevant examples (e.g., image and text generation), this

    work aims to serve as a resource for both novice and experienced researchers

    interested in understanding, applying and further developing FM.'
  arxivId: '2412.06264'
  arxiv_tags:
  - cs.LG
  authors: Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian
    Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat
  created_at: '2025-01-05T08:24:32.514575'
  issue_number: 117
  issue_url: https://github.com/dmarx/papers-feed/issues/117
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:52:20.913Z'
  main_tex_file: null
  published_date: '2024-12-09T07:22:38Z'
  state: open
  title: Flow Matching Guide and Code
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.06264
'2412.06769':
  abstract: 'Large language models (LLMs) are restricted to reason in the "language

    space", where they typically express the reasoning process with a

    chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue

    that language space may not always be optimal for reasoning. For example, most

    word tokens are primarily for textual coherence and not essential for

    reasoning, while some critical tokens require complex planning and pose huge

    challenges to LLMs. To explore the potential of LLM reasoning in an

    unrestricted latent space instead of using natural language, we introduce a new

    paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden

    state of the LLM as a representation of the reasoning state (termed "continuous

    thought"). Rather than decoding this into a word token, we feed it back to the

    LLM as the subsequent input embedding directly in the continuous space.

    Experiments show that Coconut can effectively augment the LLM on several

    reasoning tasks. This novel latent reasoning paradigm leads to emergent

    advanced reasoning patterns: the continuous thought can encode multiple

    alternative next reasoning steps, allowing the model to perform a breadth-first

    search (BFS) to solve the problem, rather than prematurely committing to a

    single deterministic path like CoT. Coconut outperforms CoT in certain logical

    reasoning tasks that require substantial backtracking during planning, with

    fewer thinking tokens during inference. These findings demonstrate the promise

    of latent reasoning and offer valuable insights for future research.'
  arxivId: '2412.06769'
  arxiv_tags:
  - cs.CL
  authors: Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston,
    Yuandong Tian
  created_at: '2025-01-04T06:52:12.616167'
  issue_number: 752
  issue_url: https://github.com/dmarx/papers-feed/issues/752
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:12.618753'
  last_visited: '2025-01-02T19:39:10.844Z'
  main_tex_file: null
  published_date: '2024-12-09T18:55:56Z'
  state: open
  title: Training Large Language Models to Reason in a Continuous Latent Space
  total_reading_time_seconds: 32
  url: https://arxiv.org/abs/2412.06769
'2412.06771':
  abstract: 'User prompts for generative AI models are often underspecified, leading
    to

    sub-optimal responses. This problem is particularly evident in text-to-image

    (T2I) generation, where users commonly struggle to articulate their precise

    intent. This disconnect between the user''s vision and the model''s

    interpretation often forces users to painstakingly and repeatedly refine their

    prompts. To address this, we propose a design for proactive T2I agents equipped

    with an interface to (1) actively ask clarification questions when uncertain,

    and (2) present their understanding of user intent as an understandable belief

    graph that a user can edit. We build simple prototypes for such agents and

    verify their effectiveness through both human studies and automated evaluation.

    We observed that at least 90% of human subjects found these agents and their

    belief graphs helpful for their T2I workflow. Moreover, we develop a scalable

    automated evaluation approach using two agents, one with a ground truth image

    and the other tries to ask as few questions as possible to align with the

    ground truth. On DesignBench, a benchmark we created for artists and designers,

    the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we

    observed that these T2I agents were able to ask informative questions and

    elicit crucial information to achieve successful alignment with at least 2

    times higher VQAScore (Lin et al., 2024) than the standard single-turn T2I

    generation. Demo: https://github.com/google-deepmind/proactive_t2i_agents.'
  arxivId: '2412.06771'
  arxiv_tags:
  - cs.AI
  - cs.CV
  - cs.LG
  authors: Meera Hahn, Wenjun Zeng, Nithish Kannen, Rich Galt, Kartikeya Badola, Been
    Kim, Zi Wang
  created_at: '2025-01-08T21:41:18.164992'
  issue_number: 859
  issue_url: https://github.com/dmarx/papers-feed/issues/859
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-09T14:43:31.362013'
  last_visited: '2025-01-09T14:42:27.418000+00:00'
  main_tex_file: null
  published_date: '2024-12-09T18:56:32Z'
  state: open
  title: "Proactive Agents for Multi-Turn Text-to-Image Generation Under\n  Uncertainty"
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2412.06771
'2412.06845':
  abstract: 'Recently, Large Language Models (LLMs) have undergone a significant

    transformation, marked by a rapid rise in both their popularity and

    capabilities. Leading this evolution are proprietary LLMs like GPT-4 and

    GPT-o1, which have captured widespread attention in the AI community due to

    their remarkable performance and versatility. Simultaneously, open-source LLMs,

    such as LLaMA and Mistral, have made great contributions to the ever-increasing

    popularity of LLMs due to the ease to customize and deploy the models across

    diverse applications. Although open-source LLMs present unprecedented

    opportunities for innovation and research, the commercialization of LLMs has

    raised concerns about transparency, reproducibility, and safety. Many

    open-source LLMs fail to meet fundamental transparency requirements by

    withholding essential components like training code and data, and some use

    restrictive licenses whilst claiming to be "open-source," which may hinder

    further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a

    fully open-source LLM developed in accordance with the Model Openness Framework

    (MOF), a ranked classification system that evaluates AI models based on model

    completeness and openness, adhering to principles of open science, open source,

    open data, and open access. Our model achieves the highest MOF classification

    level of "open science" through the comprehensive release of pre-training code

    and configurations, training and fine-tuning datasets, and intermediate and

    final checkpoints. Experiments show that our model achieves superior

    performance in zero-shot evaluation compared with popular 7B models and

    performs competitively in few-shot evaluation.'
  arxivId: '2412.06845'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht,
    Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue
    Chen, Yong He, Yanzhi Wang
  created_at: '2025-01-04T06:52:51.688399'
  issue_number: 639
  issue_url: https://github.com/dmarx/papers-feed/issues/639
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.651113'
  last_visited: '2024-12-30T20:04:21.071Z'
  main_tex_file: null
  published_date: '2024-12-08T02:01:46Z'
  state: open
  title: Fully Open Source Moxin-7B Technical Report
  total_reading_time_seconds: 53
  url: https://arxiv.org/abs/2412.06845
'2412.09621':
  abstract: 'Learning to understand dynamic 3D scenes from imagery is crucial for

    applications ranging from robotics to scene reconstruction. Yet, unlike other

    problems where large-scale supervised training has enabled rapid progress,

    directly supervising methods for recovering 3D motion remains challenging due

    to the fundamental difficulty of obtaining ground truth annotations. We present

    a system for mining high-quality 4D reconstructions from internet stereoscopic,

    wide-angle videos. Our system fuses and filters the outputs of camera pose

    estimation, stereo depth estimation, and temporal tracking methods into

    high-quality dynamic 3D reconstructions. We use this method to generate

    large-scale data in the form of world-consistent, pseudo-metric 3D point clouds

    with long-term motion trajectories. We demonstrate the utility of this data by

    training a variant of DUSt3R to predict structure and 3D motion from real-world

    image pairs, showing that training on our reconstructed data enables

    generalization to diverse real-world scenes. Project page:

    https://stereo4d.github.io'
  arxivId: '2412.09621'
  arxiv_tags:
  - cs.CV
  authors: Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander
    Holynski
  created_at: '2025-01-05T18:41:15.655826'
  issue_number: 52
  issue_url: https://github.com/dmarx/papers-feed/issues/52
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T18:41:15.656598'
  last_visited: '2024-12-16T08:58:26.647Z'
  main_tex_file: null
  published_date: '2024-12-12T18:59:54Z'
  state: open
  title: 'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos'
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/2412.09621
'2412.10271':
  abstract: 'The development and evaluation of Large Language Models (LLMs) has primarily

    focused on their task-solving capabilities, with recent models even surpassing

    human performance in some areas. However, this focus often neglects whether

    machine-generated language matches the human level of diversity, in terms of

    vocabulary choice, syntactic construction, and expression of meaning, raising

    questions about whether the fundamentals of language generation have been fully

    addressed. This paper emphasizes the importance of examining the preservation

    of human linguistic richness by language models, given the concerning surge in

    online content produced or aided by LLMs. We propose a comprehensive framework

    for evaluating LLMs from various linguistic diversity perspectives including

    lexical, syntactic, and semantic dimensions. Using this framework, we benchmark

    several state-of-the-art LLMs across all diversity dimensions, and conduct an

    in-depth case study for syntactic diversity. Finally, we analyze how different

    development and deployment choices impact the linguistic diversity of LLM

    outputs.'
  arxivId: '2412.10271'
  arxiv_tags:
  - cs.CL
  authors: Yanzhu Guo, Guokan Shang, Chloé Clavel
  created_at: '2025-01-04T06:52:57.610109'
  issue_number: 635
  issue_url: https://github.com/dmarx/papers-feed/issues/635
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.652874'
  last_visited: '2024-12-30T20:03:18.882Z'
  main_tex_file: null
  published_date: '2024-12-13T16:46:03Z'
  state: open
  title: Benchmarking Linguistic Diversity of Large Language Models
  total_reading_time_seconds: 52
  url: https://arxiv.org/abs/2412.10271
'2412.11766':
  abstract: 'Modeling human behavior is essential to accurately predict epidemic spread,

    with behaviors like vaccine hesitancy complicating control efforts. While

    epidemic spread is often treated as a simple contagion, vaccine uptake may

    follow complex contagion dynamics, where individuals'' decisions depend on

    multiple social contacts. Recently, the concept of complex contagion has

    received strong theoretical underpinnings thanks to the generalization of

    spreading phenomena from pairwise to higher-order interactions. Although

    several potential applications have been suggested, examples of complex

    contagions motivated by real data remain scarce. Surveys on COVID-19 vaccine

    hesitancy in the US suggest that vaccination attitudes may indeed depend on the

    vaccination status of social peers, aligning with complex contagion principles.

    In this work, we examine the interactions between epidemic spread, vaccination,

    and vaccine uptake attitudes under complex contagion. Using the SIR model with

    a dynamic, threshold-based vaccination campaign, we simulate scenarios on an

    age-structured multilayer network informed by US contact data. Our results

    offer insights into the role of social dynamics in shaping vaccination behavior

    and epidemic outcomes.'
  arxivId: '2412.11766'
  arxiv_tags:
  - physics.soc-ph
  - cs.SI
  authors: Alfonso de Miguel-Arribas, Alberto Aleta, Yamir Moreno
  created_at: '2025-01-04T15:02:54.891264'
  issue_number: 332
  issue_url: https://github.com/dmarx/papers-feed/issues/332
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T15:02:54.892474'
  last_visited: '2024-12-28T05:44:44.089Z'
  main_tex_file: null
  published_date: '2024-12-16T13:37:27Z'
  state: open
  title: "Interplay of epidemic spreading and vaccine uptake under complex social\n\
    \  contagion"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.11766
'2412.11768':
  abstract: 'In this work, we question the necessity of adaptive gradient methods
    for

    training deep neural networks. SGD-SaI is a simple yet effective enhancement to

    stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning

    rate Scaling at Initialization (SaI) to distinct parameter groups, guided by

    their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning

    rates without relying on adaptive second-order momentum, SGD-SaI helps prevent

    training imbalances from the very first iteration and cuts the optimizer''s

    memory usage by half compared to AdamW. Despite its simplicity and efficiency,

    SGD-SaI consistently matches or outperforms AdamW in training a variety of

    Transformer-based tasks, effectively overcoming a long-standing challenge of

    using SGD for training Transformers. SGD-SaI excels in ImageNet-1K

    classification with Vision Transformers(ViT) and GPT-2 pretraining for large

    language models (LLMs, transformer decoder-only), demonstrating robustness to

    hyperparameter variations and practicality for diverse applications. We further

    tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion

    models, where it consistently outperforms state-of-the-art optimizers. From a

    memory efficiency perspective, SGD-SaI achieves substantial memory savings for

    optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)

    and 25.15 GB for Llama2-7B compared to AdamW in full-precision training

    settings.'
  arxivId: '2412.11768'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen
  created_at: '2025-01-04T14:49:03.584492'
  issue_number: 833
  issue_url: https://github.com/dmarx/papers-feed/issues/833
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T23:11:31.444118'
  last_visited: '2025-01-05T23:10:17.881Z'
  main_tex_file: null
  published_date: '2024-12-16T13:41:37Z'
  state: open
  title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
  total_reading_time_seconds: 83
  url: https://arxiv.org/abs/2412.11768
'2412.12095':
  abstract: 'We introduce Causal Diffusion as the autoregressive (AR) counterpart
    of

    Diffusion models. It is a next-token(s) forecasting framework that is friendly

    to both discrete and continuous modalities and compatible with existing

    next-token prediction models like LLaMA and GPT. While recent works attempt to

    combine diffusion with AR models, we show that introducing sequential

    factorization to a diffusion model can substantially improve its performance

    and enables a smooth transition between AR and diffusion generation modes.

    Hence, we propose CausalFusion - a decoder-only transformer that

    dual-factorizes data across sequential tokens and diffusion noise levels,

    leading to state-of-the-art results on the ImageNet generation benchmark while

    also enjoying the AR advantage of generating an arbitrary number of tokens for

    in-context reasoning. We further demonstrate CausalFusion''s multimodal

    capabilities through a joint image generation and captioning model, and

    showcase CausalFusion''s ability for zero-shot in-context image manipulations.

    We hope that this work could provide the community with a fresh perspective on

    training multimodal models over discrete and continuous data.'
  arxivId: '2412.12095'
  arxiv_tags:
  - cs.CV
  authors: Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan
  created_at: '2025-01-10T20:40:46.392642'
  issue_number: 912
  issue_url: https://github.com/dmarx/papers-feed/issues/912
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T21:03:37.384735'
  last_visited: '2025-01-10T21:02:08.602000+00:00'
  main_tex_file: null
  published_date: '2024-12-16T18:59:29Z'
  state: open
  title: Causal Diffusion Transformers for Generative Modeling
  total_reading_time_seconds: 36
  url: https://arxiv.org/abs/2412.12095
'2412.13061':
  abstract: 'Encoding video content into compact latent tokens has become a fundamental

    step in video generation and understanding, driven by the need to address the

    inherent redundancy in pixel-level representations. Consequently, there is a

    growing demand for high-performance, open-source video tokenizers as

    video-centric research gains prominence. We introduce VidTok, a versatile video

    tokenizer that delivers state-of-the-art performance in both continuous and

    discrete tokenizations. VidTok incorporates several key advancements over

    existing approaches: 1) model architecture such as convolutional layers and

    up/downsampling modules; 2) to address the training instability and codebook

    collapse commonly associated with conventional Vector Quantization (VQ), we

    integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)

    improved training strategies, including a two-stage training process and the

    use of reduced frame rates. By integrating these advancements, VidTok achieves

    substantial improvements over existing methods, demonstrating superior

    performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,

    under standardized evaluation settings.'
  arxivId: '2412.13061'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.LG
  authors: Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, Jiang Bian
  created_at: '2025-01-07T08:22:28.937840'
  issue_number: 848
  issue_url: https://github.com/dmarx/papers-feed/issues/848
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-07T08:26:18.055954'
  last_visited: '2025-01-07T08:25:03.828000+00:00'
  main_tex_file: null
  published_date: '2024-12-17T16:27:11Z'
  state: open
  title: 'VidTok: A Versatile and Open-Source Video Tokenizer'
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2412.13061
'2412.13145':
  abstract: 'Could an AI have conscious experiences? Any answer to this question should

    conform to Evidentialism - that is, it should be based not on intuition, dogma

    or speculation but on solid scientific evidence. I argue that such evidence is

    hard to come by and that the only justifiable stance on the prospects of

    artificial consciousness is agnosticism. In the current debate, the main

    division is between biological views that are sceptical of artificial

    consciousness and functional views that are sympathetic to it. I argue that

    both camps make the same mistake of over-estimating what the evidence tells us.

    Scientific insights into consciousness have been achieved through the study of

    conscious organisms. Although this has enabled cautious assessments of

    consciousness in various creatures, extending this to AI faces serious

    obstacles. AI thus presents consciousness researchers with a dilemma: either

    reach a verdict on artificial consciousness but violate Evidentialism; or

    respect Evidentialism but offer no verdict on the prospects of artificial

    consciousness. The dominant trend in the literature has been to take the first

    option while purporting to follow the scientific evidence. I argue that if we

    truly follow the evidence, we must take the second option and adopt

    agnosticism.'
  arxivId: '2412.13145'
  arxiv_tags:
  - cs.AI
  authors: Tom McClelland
  created_at: '2025-01-05T08:24:56.493090'
  issue_number: 102
  issue_url: https://github.com/dmarx/papers-feed/issues/102
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T05:22:42.487Z'
  main_tex_file: null
  published_date: '2024-12-17T18:11:12Z'
  state: open
  title: Agnosticism About Artificial Consciousness
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.13145
'2412.13663':
  abstract: 'Encoder-only transformer models such as BERT offer a great performance-size

    tradeoff for retrieval and classification tasks with respect to larger

    decoder-only models. Despite being the workhorse of numerous production

    pipelines, there have been limited Pareto improvements to BERT since its

    release. In this paper, we introduce ModernBERT, bringing modern model

    optimizations to encoder-only models and representing a major Pareto

    improvement over older encoders. Trained on 2 trillion tokens with a native

    8192 sequence length, ModernBERT models exhibit state-of-the-art results on a

    large pool of evaluations encompassing diverse classification tasks and both

    single and multi-vector retrieval on different domains (including code). In

    addition to strong downstream performance, ModernBERT is also the most speed

    and memory efficient encoder and is designed for inference on common GPUs.'
  arxivId: '2412.13663'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar
    Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom
    Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli
  created_at: '2025-01-05T08:24:38.495671'
  issue_number: 113
  issue_url: https://github.com/dmarx/papers-feed/issues/113
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:30:57.155Z'
  main_tex_file: null
  published_date: '2024-12-18T09:39:44Z'
  state: open
  title: "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast,\
    \ Memory Efficient, and Long Context Finetuning and Inference"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.13663
'2412.14294':
  abstract: 'We propose a novel block for video modelling. It relies on a

    time-space-channel factorisation with dedicated blocks for each dimension:

    gated linear recurrent units (LRUs) perform information mixing over time,

    self-attention layers perform mixing over space, and MLPs over channels. The

    resulting architecture TRecViT performs well on sparse and dense tasks, trained

    in supervised or self-supervised regimes. Notably, our model is causal and

    outperforms or is on par with a pure attention model ViViT-L on large scale

    video datasets (SSv2, Kinetics400), while having $3\times$ less parameters,

    $12\times$ smaller memory footprint, and $5\times$ lower FLOPs count. Code and

    checkpoints will be made available online at

    https://github.com/google-deepmind/trecvit.'
  arxivId: '2412.14294'
  arxiv_tags:
  - cs.CV
  - cs.LG
  authors: Viorica Pătrăucean, Xu Owen He, Joseph Heyward, Chuhan Zhang, Mehdi S.
    M. Sajjadi, George-Cristian Muraru, Artem Zholus, Mahdi Karami, Ross Goroshin,
    Yutian Chen, Simon Osindero, João Carreira, Razvan Pascanu
  created_at: '2025-01-11T07:35:11.387190'
  issue_number: 923
  issue_url: https://github.com/dmarx/papers-feed/issues/923
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-11T07:38:52.726106'
  last_visited: '2025-01-11T07:35:29.299000+00:00'
  main_tex_file: null
  published_date: '2024-12-18T19:44:30Z'
  state: open
  title: 'TRecViT: A Recurrent Video Transformer'
  total_reading_time_seconds: 9
  url: https://arxiv.org/abs/2412.14294
'2412.15285':
  abstract: 'Pretraining large language models effectively requires strategic data

    selection, blending and ordering. However, key details about data mixtures

    especially their scalability to longer token horizons and larger model sizes

    remain underexplored due to limited disclosure by model developers. To address

    this, we formalize the concept of two-phase pretraining and conduct an

    extensive systematic study on how to select and mix data to maximize model

    accuracies for the two phases. Our findings illustrate that a two-phase

    approach for pretraining outperforms random data ordering and natural

    distribution of tokens by 3.4% and 17% on average accuracies. We provide

    in-depth guidance on crafting optimal blends based on quality of the data

    source and the number of epochs to be seen. We propose to design blends using

    downsampled data at a smaller scale of 1T tokens and then demonstrate effective

    scaling of our approach to larger token horizon of 15T tokens and larger model

    size of 25B model size. These insights provide a series of steps practitioners

    can follow to design and scale their data blends.'
  arxivId: '2412.15285'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad
    Shoeybi, Bryan Catanzaro
  created_at: '2025-01-04T06:52:33.613492'
  issue_number: 643
  issue_url: https://github.com/dmarx/papers-feed/issues/643
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.649544'
  last_visited: '2024-12-30T20:04:52.739Z'
  main_tex_file: null
  published_date: '2024-12-18T18:41:18Z'
  state: open
  title: "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase\n\
    \  Pretraining"
  total_reading_time_seconds: 49
  url: https://arxiv.org/abs/2412.15285
'2412.17758':
  abstract: 'ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily

    due to an evaluation setup that prevents direct comparison of answer choices

    rather than inherent complexity. Although some researchers have quietly shifted

    to a more appropriate scheme over the last year, the implications of this

    change have yet to be widely acknowledged. We highlight this overlooked shift,

    show how similar evaluation practices falsely imply reasoning deficits in other

    benchmarks, and demonstrate that fairer methods dramatically reduce performance

    gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing

    so, we reveal how evaluation shapes perceived difficulty and offer guidelines

    to ensure that multiple-choice evaluations accurately reflect actual model

    capabilities.'
  arxivId: '2412.17758'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Łukasz Borchmann
  created_at: '2025-01-04T14:49:27.229963'
  issue_number: 458
  issue_url: https://github.com/dmarx/papers-feed/issues/458
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:27.230757'
  last_visited: '2024-12-29T08:28:16.461Z'
  main_tex_file: null
  published_date: '2024-12-23T18:14:36Z'
  state: open
  title: 'In Case You Missed It: ARC ''Challenge'' Is Not That Challenging'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.17758
'2412.17805':
  abstract: 'Learning a robust video Variational Autoencoder (VAE) is essential for

    reducing video redundancy and facilitating efficient video generation. Directly

    applying image VAEs to individual frames in isolation can result in temporal

    inconsistencies and suboptimal compression rates due to a lack of temporal

    compression. Existing Video VAEs have begun to address temporal compression;

    however, they often suffer from inadequate reconstruction performance. In this

    paper, we present a novel and powerful video autoencoder capable of

    high-fidelity video encoding. First, we observe that entangling spatial and

    temporal compression by merely extending the image VAE to a 3D VAE can

    introduce motion blur and detail distortion artifacts. Thus, we propose

    temporal-aware spatial compression to better encode and decode the spatial

    information. Additionally, we integrate a lightweight motion compression model

    for further temporal compression. Second, we propose to leverage the textual

    information inherent in text-to-video datasets and incorporate text guidance

    into our model. This significantly enhances reconstruction quality,

    particularly in terms of detail preservation and temporal stability. Third, we

    further improve the versatility of our model through joint training on both

    images and videos, which not only enhances reconstruction quality but also

    enables the model to perform both image and video autoencoding. Extensive

    evaluations against strong recent baselines demonstrate the superior

    performance of our method. The project website can be found

    at~\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.'
  arxivId: '2412.17805'
  arxiv_tags:
  - cs.CV
  authors: Yazhou Xing, Yang Fei, Yingqing He, Jingye Chen, Jiaxin Xie, Xiaowei Chi,
    Qifeng Chen
  created_at: '2025-01-04T06:52:24.663099'
  issue_number: 716
  issue_url: https://github.com/dmarx/papers-feed/issues/716
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T08:03:08.915Z'
  main_tex_file: null
  published_date: '2024-12-23T18:58:24Z'
  state: open
  title: Large Motion Video Autoencoding with Cross-modal Video VAE
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.17805
'2412.17847':
  abstract: 'Progress in AI is driven largely by the scale and quality of training
    data.

    Despite this, there is a deficit of empirical analysis examining the attributes

    of well-established datasets beyond text. In this work we conduct the largest

    and first-of-its-kind longitudinal audit across modalities--popular text,

    speech, and video datasets--from their detailed sourcing trends and use

    restrictions to their geographical and linguistic representation. Our manual

    analysis covers nearly 4000 public datasets between 1990-2024, spanning 608

    languages, 798 sources, 659 organizations, and 67 countries. We find that

    multimodal machine learning applications have overwhelmingly turned to

    web-crawled, synthetic, and social media platforms, such as YouTube, for their

    training sets, eclipsing all other sources since 2019. Secondly, tracing the

    chain of dataset derivations we find that while less than 33% of datasets are

    restrictively licensed, over 80% of the source content in widely-used text,

    speech, and video datasets, carry non-commercial restrictions. Finally, counter

    to the rising number of languages and geographies represented in public AI

    training datasets, our audit demonstrates measures of relative geographical and

    multilingual representation have failed to significantly improve their coverage

    since 2013. We believe the breadth of our audit enables us to empirically

    examine trends in data sourcing, restrictions, and Western-centricity at an

    ecosystem-level, and that visibility into these questions are essential to

    progress in responsible AI. As a contribution to ongoing improvements in

    dataset transparency and responsible use, we release our entire multimodal

    audit, allowing practitioners to trace data provenance across text, speech, and

    video.'
  arxivId: '2412.17847'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.CY
  - cs.LG
  - cs.MM
  authors: Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,
    William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad
    Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun
    Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas,
    Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm,
    Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda,
    Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui
    Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker,
    Jad Kabbara
  created_at: '2025-01-04T06:53:21.616757'
  issue_number: 633
  issue_url: https://github.com/dmarx/papers-feed/issues/633
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:21.617516'
  last_visited: '2024-12-30T20:03:13.039Z'
  main_tex_file: null
  published_date: '2024-12-19T01:30:19Z'
  state: open
  title: Bridging the Data Provenance Gap Across Text, Speech and Video
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.17847
'2412.18069':
  abstract: 'Large language models can generate factually inaccurate content, a problem

    known as hallucination. Recent works have built upon retrieved-augmented

    generation to improve factuality through iterative prompting but these methods

    are limited by the traditional RAG design. To address these challenges, we

    introduce EWE (Explicit Working Memory), a novel approach that enhances

    factuality in long-form text generation by integrating a working memory that

    receives real-time feedback from external resources. The memory is refreshed

    based on online fact-checking and retrieval feedback, allowing EWE to rectify

    false claims during the generation process and ensure more accurate and

    reliable outputs. Our experiments demonstrate that Ewe outperforms strong

    baselines on four fact-seeking long-form generation datasets, increasing the

    factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing

    the helpfulness of the responses. Further analysis reveals that the design of

    rules for memory updates, configurations of memory units, and the quality of

    the retrieval datastore are crucial factors for influencing model performance.'
  arxivId: '2412.18069'
  arxiv_tags:
  - cs.CL
  authors: Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke Zettlemoyer,
    Gargi Gosh, Wen-tau Yih
  created_at: '2025-01-05T19:04:47.334344'
  issue_number: 805
  issue_url: https://github.com/dmarx/papers-feed/issues/805
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-05T19:04:47.338057'
  last_visited: '2025-01-05T19:03:17.946Z'
  main_tex_file: null
  published_date: '2024-12-24T00:55:59Z'
  state: open
  title: Improving Factuality with Explicit Working Memory
  total_reading_time_seconds: 27
  url: https://arxiv.org/abs/2412.18069
'2412.18082':
  abstract: 'The item cold-start problem is crucial for online recommender systems,
    as the

    success of the cold-start phase determines whether items can transition into

    popular ones. Prompt learning, a powerful technique used in natural language

    processing (NLP) to address zero- or few-shot problems, has been adapted for

    recommender systems to tackle similar challenges. However, existing methods

    typically rely on content-based properties or text descriptions for prompting,

    which we argue may be suboptimal for cold-start recommendations due to 1)

    semantic gaps with recommender tasks, 2) model bias caused by warm-up items

    contribute most of the positive feedback to the model, which is the core of the

    cold-start problem that hinders the recommender quality on cold-start items. We

    propose to leverage high-value positive feedback, termed pinnacle feedback as

    prompt information, to simultaneously resolve the above two problems. We

    experimentally prove that compared to the content description proposed in

    existing works, the positive feedback is more suitable to serve as prompt

    information by bridging the semantic gaps. Besides, we propose item-wise

    personalized prompt networks to encode pinnaclce feedback to relieve the model

    bias by the positive feedback dominance problem. Extensive experiments on four

    real-world datasets demonstrate the superiority of our model over

    state-of-the-art methods. Moreover, PROMO has been successfully deployed on a

    popular short-video sharing platform, a billion-user scale commercial

    short-video application, achieving remarkable performance gains across various

    commercial metrics within cold-start scenarios'
  arxivId: '2412.18082'
  arxiv_tags:
  - cs.IR
  - cs.AI
  authors: Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi
    Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian
  created_at: '2025-01-04T14:48:54.229792'
  issue_number: 570
  issue_url: https://github.com/dmarx/papers-feed/issues/570
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:54.231751'
  last_visited: '2024-12-30T04:50:12.821000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T01:38:19Z'
  state: open
  title: Prompt Tuning for Item Cold-start Recommendation
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2412.18082
'2412.18860':
  abstract: 'We introduce a bootstrapping approach to train long-context language
    models

    by exploiting their short-context capabilities only. Our method utilizes a

    simple agent workflow to synthesize diverse long-context instruction tuning

    data, thereby eliminating the necessity for manual data collection and

    annotation. The proposed data synthesis workflow requires only a short-context

    language model, a text retriever, and a document collection, all of which are

    readily accessible within the open-source ecosystem. Subsequently, language

    models are fine-tuned using the synthesized data to extend their context

    lengths. In this manner, we effectively transfer the short-context capabilities

    of language models to long-context scenarios through a bootstrapping process.

    We conduct experiments with the open-source Llama-3 family of models and

    demonstrate that our method can successfully extend the context length to up to

    1M tokens, achieving superior performance across various benchmarks.'
  arxivId: '2412.18860'
  arxiv_tags:
  - cs.CL
  - cs.IR
  authors: Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei
  created_at: '2025-01-04T14:48:48.243492'
  issue_number: 539
  issue_url: https://github.com/dmarx/papers-feed/issues/539
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:54.232387'
  last_visited: '2024-12-30T04:37:40.310Z'
  main_tex_file: null
  published_date: '2024-12-25T10:08:54Z'
  state: open
  title: Bootstrap Your Own Context Length
  total_reading_time_seconds: 44
  url: https://arxiv.org/abs/2412.18860
'2412.18956':
  abstract: 'When you have a question, the most effective way to have the question

    answered is to directly connect with experts on the topic and have a

    conversation with them. Prior to the invention of writing, this was the only

    way. Although effective, this solution exhibits scalability challenges. Writing

    allowed knowledge to be materialized, preserved, and replicated, enabling the

    development of different technologies over the centuries to connect information

    seekers with relevant information. This progression ultimately culminated in

    the ten-blue-links web search paradigm we''re familiar with, just before the

    recent emergence of generative AI. However, we often forget that consuming

    static content is an imperfect solution. With the advent of large language

    models, it has become possible to develop a superior experience by allowing

    users to directly engage with experts. These interactions can of course satisfy

    information needs, but expert models can do so much more. This coming future

    requires reimagining search.'
  arxivId: '2412.18956'
  arxiv_tags:
  - cs.IR
  authors: Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne
  created_at: '2025-01-04T14:48:51.238557'
  issue_number: 572
  issue_url: https://github.com/dmarx/papers-feed/issues/572
  labels:
  - paper
  - rating:downvote
  last_read: '2025-01-04T14:48:51.239805'
  last_visited: '2024-12-30T04:52:50.157000+00:00'
  main_tex_file: null
  published_date: '2024-12-25T18:09:34Z'
  state: open
  title: 'Musings About the Future of Search: A Return to the Past?'
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2412.18956
'2412.19442':
  abstract: 'Large Language Models (LLMs) have revolutionized a wide range of domains
    such

    as natural language processing, computer vision, and multi-modal tasks due to

    their ability to comprehend context and perform logical reasoning. However, the

    computational and memory demands of LLMs, particularly during inference, pose

    significant challenges when scaling them to real-world, long-context, and

    real-time applications. Key-Value (KV) cache management has emerged as a

    critical optimization technique for accelerating LLM inference by reducing

    redundant computations and improving memory utilization. This survey provides
    a

    comprehensive overview of KV cache management strategies for LLM acceleration,

    categorizing them into token-level, model-level, and system-level

    optimizations. Token-level strategies include KV cache selection, budget

    allocation, merging, quantization, and low-rank decomposition, while

    model-level optimizations focus on architectural innovations and attention

    mechanisms to enhance KV reuse. System-level approaches address memory

    management, scheduling, and hardware-aware designs to improve efficiency across

    diverse computing environments. Additionally, the survey provides an overview

    of both text and multimodal datasets and benchmarks used to evaluate these

    strategies. By presenting detailed taxonomies and comparative analyses, this

    work aims to offer useful insights for researchers and practitioners to support

    the development of efficient and scalable KV cache management techniques,

    contributing to the practical deployment of LLMs in real-world applications.

    The curated paper list for KV cache management is in:

    \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.'
  arxivId: '2412.19442'
  arxiv_tags:
  - cs.AI
  - cs.DC
  authors: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen,
    Nicole Hu, Wei Dong, Qing Li, Lei Chen
  created_at: '2025-01-04T14:48:57.229727'
  issue_number: 535
  issue_url: https://github.com/dmarx/papers-feed/issues/535
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:57.232180'
  last_visited: '2024-12-30T04:35:23.041Z'
  main_tex_file: null
  published_date: '2024-12-27T04:17:57Z'
  state: open
  title: "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2412.19442
'2412.19792':
  abstract: 'Language model alignment has become a critical step in training modern

    generative language models. The goal of alignment is to finetune a reference

    model such that the win rate of a sample from the aligned model over a sample

    from the reference model is high, subject to a KL divergence constraint. Today,

    we are increasingly using inference-time algorithms (e.g., Best-of-N,

    controlled decoding, tree search) to decode from language models rather than

    standard sampling. However, the alignment objective does not capture such

    inference-time decoding procedures. We show that the existing alignment

    framework is sub-optimal in view of such inference-time methods. We then modify

    the alignment objective and propose a framework for inference-aware alignment

    (IAPO). We prove that for any inference-time decoding algorithm, the optimal

    solution that optimizes the inference-time win rate of the aligned policy

    against the reference policy is the solution to the typical RLHF problem with
    a

    transformation of the reward. This motivates us to provide the KL-regularized

    calibrate-and-transform RL (CTRL) algorithm to solve this problem, which

    involves a reward calibration step and a KL-regularized reward maximization

    step with a transformation of the calibrated reward. We particularize our study

    to two important inference-time strategies: best-of-N sampling and best-of-N

    jailbreaking, where N responses are sampled from the model and the one with the

    highest or lowest reward is selected. We propose specific transformations for

    these strategies and demonstrate that our framework offers significant

    improvements over existing state-of-the-art methods for language model

    alignment. Empirically, we outperform baselines that are designed without

    taking inference-time decoding into consideration by 8-12% and 4-9% on

    inference-time win rates over the Anthropic helpfulness and harmlessness dialog

    benchmark datasets.'
  arxivId: '2412.19792'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.IT
  - math.IT
  authors: Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael
    Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha,
    Ananda Theertha Suresh, Ahmad Beirami
  created_at: '2025-01-04T06:51:54.729533'
  issue_number: 763
  issue_url: https://github.com/dmarx/papers-feed/issues/763
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:00.645791'
  last_visited: '2025-01-03T08:59:10.679Z'
  main_tex_file: null
  published_date: '2024-12-27T18:45:36Z'
  state: open
  title: 'InfAlign: Inference-aware language model alignment'
  total_reading_time_seconds: 49
  url: https://arxiv.org/abs/2412.19792
'2412.20292':
  abstract: 'We obtain the first analytic, interpretable and predictive theory of

    creativity in convolutional diffusion models. Indeed, score-based diffusion

    models can generate highly creative images that lie far from their training

    data. But optimal score-matching theory suggests that these models should only

    be able to produce memorized training examples. To reconcile this

    theory-experiment gap, we identify two simple inductive biases, locality and

    equivariance, that: (1) induce a form of combinatorial creativity by preventing

    optimal score-matching; (2) result in a fully analytic, completely

    mechanistically interpretable, equivariant local score (ELS) machine that, (3)

    without any training can quantitatively predict the outputs of trained

    convolution only diffusion models (like ResNets and UNets) with high accuracy

    (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our

    ELS machine reveals a locally consistent patch mosaic model of creativity, in

    which diffusion models create exponentially many novel images by mixing and

    matching different local training set patches in different image locations. Our

    theory also partially predicts the outputs of pre-trained self-attention

    enabled UNets (median $r^2 \sim 0.75$ on CIFAR10), revealing an intriguing role

    for attention in carving out semantic coherence from local patch mosaics.'
  arxivId: '2412.20292'
  arxiv_tags:
  - cs.LG
  - cond-mat.dis-nn
  - cs.AI
  - q-bio.NC
  - stat.ML
  - I.2.10
  authors: Mason Kamb, Surya Ganguli
  created_at: '2025-01-04T06:52:30.603137'
  issue_number: 708
  issue_url: https://github.com/dmarx/papers-feed/issues/708
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:30.604171'
  last_visited: '2025-01-01T16:00:07.088Z'
  main_tex_file: null
  published_date: '2024-12-28T22:33:29Z'
  state: open
  title: An analytic theory of creativity in convolutional diffusion models
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2412.20292
'2501.00663':
  abstract: 'Over more than a decade there has been an extensive research effort on
    how to

    effectively utilize recurrent models and attention. While recurrent models aim

    to compress the data into a fixed-size memory (called hidden state), attention

    allows attending to the entire context window, capturing the direct

    dependencies of all tokens. This more accurate modeling of dependencies,

    however, comes with a quadratic cost, limiting the model to a fixed-length

    context. We present a new neural long-term memory module that learns to

    memorize historical context and helps attention to attend to the current

    context while utilizing long past information. We show that this neural memory

    has the advantage of fast parallelizable training while maintaining a fast

    inference. From a memory perspective, we argue that attention due to its

    limited context but accurate dependency modeling performs as a short-term

    memory, while neural memory due to its ability to memorize the data, acts as a

    long-term, more persistent, memory. Based on these two modules, we introduce a

    new family of architectures, called Titans, and present three variants to

    address how one can effectively incorporate memory into this architecture. Our

    experimental results on language modeling, common-sense reasoning, genomics,

    and time series tasks show that Titans are more effective than Transformers and

    recent modern linear recurrent models. They further can effectively scale to

    larger than 2M context window size with higher accuracy in needle-in-haystack

    tasks compared to baselines.'
  arxivId: '2501.00663'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: Ali Behrouz, Peilin Zhong, Vahab Mirrokni
  created_at: '2025-01-10T08:07:02.306242'
  issue_number: 984
  issue_url: https://github.com/dmarx/papers-feed/issues/984
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T08:07:02.307560'
  last_visited: '2025-01-17T02:36:23.950Z'
  main_tex_file: null
  published_date: '2024-12-31T22:32:03Z'
  state: open
  title: 'Titans: Learning to Memorize at Test Time'
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2501.00663
'2501.02976':
  abstract: 'Image diffusion models have been adapted for real-world video

    super-resolution to tackle over-smoothing issues in GAN-based methods. However,

    these models struggle to maintain temporal consistency, as they are trained on

    static images, limiting their ability to capture temporal dynamics effectively.

    Integrating text-to-video (T2V) models into video super-resolution for improved

    temporal modeling is straightforward. However, two key challenges remain:

    artifacts introduced by complex degradations in real-world scenarios, and

    compromised fidelity due to the strong generative capacity of powerful T2V

    models (\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of

    restored videos, we introduce\textbf{~\name}

    (\textbf{S}patial-\textbf{T}emporal \textbf{A}ugmentation with T2V models for

    \textbf{R}eal-world video super-resolution), a novel approach that leverages

    T2V models for real-world video super-resolution, achieving realistic spatial

    details and robust temporal consistency. Specifically, we introduce a Local

    Information Enhancement Module (LIEM) before the global attention block to

    enrich local details and mitigate degradation artifacts. Moreover, we propose
    a

    Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus

    on different frequency components across diffusion steps. Extensive experiments

    demonstrate\textbf{~\name}~outperforms state-of-the-art methods on both

    synthetic and real-world datasets.'
  arxivId: '2501.02976'
  arxiv_tags:
  - cs.CV
  authors: Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu
    Zhang, Jian Yang, Zhenheng Yang, Ying Tai
  created_at: '2025-01-10T06:25:07.936139'
  issue_number: 902
  issue_url: https://github.com/dmarx/papers-feed/issues/902
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T06:28:39.452667'
  last_visited: '2025-01-10T06:25:50.587Z'
  main_tex_file: null
  published_date: '2025-01-06T12:36:21Z'
  state: open
  title: "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for\n  Real-World\
    \ Video Super-Resolution"
  total_reading_time_seconds: 46
  url: https://arxiv.org/abs/2501.02976
'2501.03006':
  abstract: 'Text-to-video generative models have made significant strides, enabling

    diverse applications in entertainment, advertising, and education. However,

    generating RGBA video, which includes alpha channels for transparency, remains

    a challenge due to limited datasets and the difficulty of adapting existing

    models. Alpha channels are crucial for visual effects (VFX), allowing

    transparent elements like smoke and reflections to blend seamlessly into

    scenes. We introduce TransPixar, a method to extend pretrained video models for

    RGBA generation while retaining the original RGB capabilities. TransPixar

    leverages a diffusion transformer (DiT) architecture, incorporating

    alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB

    and alpha channels with high consistency. By optimizing attention mechanisms,

    TransPixar preserves the strengths of the original RGB model and achieves

    strong alignment between RGB and alpha channels despite limited training data.

    Our approach effectively generates diverse and consistent RGBA videos,

    advancing the possibilities for VFX and interactive content creation.'
  arxivId: '2501.03006'
  arxiv_tags:
  - cs.CV
  authors: Luozhou Wang, Yijun Li, Zhifei Chen, Jui-Hsien Wang, Zhifei Zhang, He Zhang,
    Zhe Lin, Yingcong Chen
  created_at: '2025-01-10T05:17:17.451567'
  issue_number: 885
  issue_url: https://github.com/dmarx/papers-feed/issues/885
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T05:24:23.109329'
  last_visited: '2025-01-10T05:23:00.328000+00:00'
  main_tex_file: null
  published_date: '2025-01-06T13:32:16Z'
  state: open
  title: 'TransPixar: Advancing Text-to-Video Generation with Transparency'
  total_reading_time_seconds: 23
  url: https://arxiv.org/abs/2501.03006
'2501.03082':
  abstract: 'An extra hard spectral component that extends to GeV energies, in additional

    to the typical sub- MeV Band component, appears in several gamma-ray burst

    (GRBs) detected by Fermi Large Area Telescopes (LAT). Only in one case (i.e.,

    GRB 090926A), a spectral break feature at the high energy end is identified in

    the extra hard component, but the photon counts are not enough to distinguish

    between the cutoff model and the broken power law model for the spectral break.

    In this work, we report the detection of an extra hard component showing the

    spectral break in GRB 240825A. We find that a broken power-law model fits the

    spectral data of the extra component better than a single power-law with an

    exponential cutoff in the time resolved spectrum for the second emission pulse,

    with a break at about 50 MeV. This spectral feature disfavors the gamma-ray

    opacity to pair creation as the origin of the spectral break, but points to an

    intrinsic peak for the extra component. The low ratio between the peak of the

    extra hard component and that of the Band component challenges the synchrotron

    self-Compton origin for the extra component. Alternative scenarios, such as the

    inverse Compton scattering of the photosphere emission, are discussed. In

    addition, we find a clear transition from the prompt emission to afterglow

    emission at GeV energies in GRB 240825A, manifested by a temporal steep decay

    and an unique spectral evolution.'
  arxivId: '2501.03082'
  arxiv_tags:
  - astro-ph.HE
  authors: Hai-Ming Zhang, Zi-Qi Wang, Cui-Yuan Dai, Yi-Yun Huang, Ruo-Yu Liu, En-Wei
    Liang, Xiang-Yu Wang
  created_at: '2025-01-13T06:08:18.246317'
  issue_number: 955
  issue_url: https://github.com/dmarx/papers-feed/issues/955
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-13T06:07:18.596Z'
  main_tex_file: null
  published_date: '2025-01-06T15:25:59Z'
  state: open
  title: A two-hump spectrum in the prompt emission of GRB 240825A
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2501.03082
'2501.03847':
  abstract: 'Diffusion models have demonstrated impressive performance in generating

    high-quality videos from text prompts or images. However, precise control over

    the video generation process, such as camera manipulation or content editing,

    remains a significant challenge. Existing methods for controlled video

    generation are typically limited to a single control type, lacking the

    flexibility to handle diverse control demands. In this paper, we introduce

    Diffusion as Shader (DaS), a novel approach that supports multiple video

    control tasks within a unified architecture. Our key insight is that achieving

    versatile video control necessitates leveraging 3D control signals, as videos

    are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods

    limited to 2D control signals, DaS leverages 3D tracking videos as control

    inputs, making the video diffusion process inherently 3D-aware. This innovation

    allows DaS to achieve a wide range of video controls by simply manipulating the

    3D tracking videos. A further advantage of using 3D tracking videos is their

    ability to effectively link frames, significantly enhancing the temporal

    consistency of the generated videos. With just 3 days of fine-tuning on 8 H800

    GPUs using less than 10k videos, DaS demonstrates strong control capabilities

    across diverse tasks, including mesh-to-video generation, camera control,

    motion transfer, and object manipulation.'
  arxivId: '2501.03847'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.GR
  authors: Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong,
    Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, Yuan Liu
  created_at: '2025-01-10T05:54:15.297122'
  issue_number: 892
  issue_url: https://github.com/dmarx/papers-feed/issues/892
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T05:54:15.298855'
  last_visited: '2025-01-10T05:52:18.660Z'
  main_tex_file: null
  published_date: '2025-01-07T15:01:58Z'
  state: open
  title: "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video\n  Generation\
    \ Control"
  total_reading_time_seconds: 22
  url: https://arxiv.org/abs/2501.03847
'2501.04227':
  abstract: 'Historically, scientific discovery has been a lengthy and costly process,

    demanding substantial time and resources from initial conception to final

    results. To accelerate scientific discovery, reduce research costs, and improve

    research quality, we introduce Agent Laboratory, an autonomous LLM-based

    framework capable of completing the entire research process. This framework

    accepts a human-provided research idea and progresses through three

    stages--literature review, experimentation, and report writing to produce

    comprehensive research outputs, including a code repository and a research

    report, while enabling users to provide feedback and guidance at each stage. We

    deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple

    researchers to assess its quality by participating in a survey, providing human

    feedback to guide the research process, and then evaluate the final paper. We

    found that: (1) Agent Laboratory driven by o1-preview generates the best

    research outcomes; (2) The generated machine learning code is able to achieve

    state-of-the-art performance compared to existing methods; (3) Human

    involvement, providing feedback at each stage, significantly improves the

    overall quality of research; (4) Agent Laboratory significantly reduces

    research expenses, achieving an 84% decrease compared to previous autonomous

    research methods. We hope Agent Laboratory enables researchers to allocate more

    effort toward creative ideation rather than low-level coding and writing,

    ultimately accelerating scientific discovery.'
  arxivId: '2501.04227'
  arxiv_tags:
  - cs.HC
  - cs.AI
  - cs.CL
  - cs.LG
  authors: Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong
    Yu, Jiang Liu, Zicheng Liu, Emad Barsoum
  created_at: '2025-01-10T05:49:05.432395'
  issue_number: 887
  issue_url: https://github.com/dmarx/papers-feed/issues/887
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-10T05:49:05.434811'
  last_visited: '2025-01-10T05:47:26.623Z'
  main_tex_file: null
  published_date: '2025-01-08T01:58:42Z'
  state: open
  title: 'Agent Laboratory: Using LLM Agents as Research Assistants'
  total_reading_time_seconds: 44
  url: https://arxiv.org/abs/2501.04227
'2501.04697':
  abstract: 'Grokking, the sudden generalization that occurs after prolonged overfitting,

    is a surprising phenomenon challenging our understanding of deep learning.

    Although significant progress has been made in understanding grokking, the

    reasons behind the delayed generalization and its dependence on regularization

    remain unclear. In this work, we argue that without regularization, grokking

    tasks push models to the edge of numerical stability, introducing floating

    point errors in the Softmax function, which we refer to as Softmax Collapse

    (SC). We demonstrate that SC prevents grokking and that mitigating SC enables

    grokking without regularization. Investigating the root cause of SC, we find

    that beyond the point of overfitting, the gradients strongly align with what we

    call the na\"ive loss minimization (NLM) direction. This component of the

    gradient does not alter the model''s predictions but decreases the loss by

    scaling the logits, typically by scaling the weights along their current

    direction. We show that this scaling of the logits explains the delay in

    generalization characteristic of grokking and eventually leads to SC, halting

    further learning. To validate our hypotheses, we introduce two key

    contributions that address the challenges in grokking tasks: StableMax, a new

    activation function that prevents SC and enables grokking without

    regularization, and $\perp$Grad, a training algorithm that promotes quick

    generalization in grokking tasks by preventing NLM altogether. These

    contributions provide new insights into grokking, elucidating its delayed

    generalization, reliance on regularization, and the effectiveness of existing

    grokking-inducing methods. Code for this paper is available at

    https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.'
  arxivId: '2501.04697'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  - stat.ML
  authors: Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal
  created_at: '2025-01-17T05:46:34.882119'
  issue_number: 996
  issue_url: https://github.com/dmarx/papers-feed/issues/996
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-17T18:18:33.078619'
  last_visited: '2025-01-17T18:17:21.137Z'
  main_tex_file: null
  published_date: '2025-01-08T18:58:48Z'
  state: open
  title: Grokking at the Edge of Numerical Stability
  total_reading_time_seconds: 18
  url: https://arxiv.org/abs/2501.04697
'2501.05242':
  abstract: '3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis

    in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM

    methods utilizing 3DGS have failed to provide high-quality novel view rendering

    for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods

    perform well for RGB-D cameras but suffer significant degradation in rendering

    quality for monocular cameras. In this paper, we present Scaffold-SLAM, which

    delivers simultaneous localization and high-quality photorealistic mapping

    across monocular, stereo, and RGB-D cameras. We introduce two key innovations

    to achieve this state-of-the-art visual quality. First, we propose

    Appearance-from-Motion embedding, enabling 3D Gaussians to better model image

    appearance variations across different camera poses. Second, we introduce a

    frequency regularization pyramid to guide the distribution of Gaussians,

    allowing the model to effectively capture finer details in the scene. Extensive

    experiments on monocular, stereo, and RGB-D datasets demonstrate that

    Scaffold-SLAM significantly outperforms state-of-the-art methods in

    photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D

    datasets for monocular cameras.'
  arxivId: '2501.05242'
  arxiv_tags:
  - cs.CV
  - 68T40(Primary)68T45, 68U99 (Secondary)
  - I.4.8; I.3.7
  authors: Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun
  created_at: '2025-01-11T09:07:19.696624'
  issue_number: 931
  issue_url: https://github.com/dmarx/papers-feed/issues/931
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-11T09:07:19.697839'
  last_visited: '2025-01-11T09:05:35.504Z'
  main_tex_file: null
  published_date: '2025-01-09T13:50:26Z'
  state: open
  title: "Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and\n\
    \  Photorealistic Mapping"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2501.05242
'2501.05441':
  abstract: 'There is a widely-spread claim that GANs are difficult to train, and
    GAN

    architectures in the literature are littered with empirical tricks. We provide

    evidence against this claim and build a modern GAN baseline in a more

    principled manner. First, we derive a well-behaved regularized relativistic GAN

    loss that addresses issues of mode dropping and non-convergence that were

    previously tackled via a bag of ad-hoc tricks. We analyze our loss

    mathematically and prove that it admits local convergence guarantees, unlike

    most existing relativistic losses. Second, our new loss allows us to discard

    all ad-hoc tricks and replace outdated backbones used in common GANs with

    modern architectures. Using StyleGAN2 as an example, we present a roadmap of

    simplification and modernization that results in a new minimalist baseline --

    R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,

    ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against

    state-of-the-art GANs and diffusion models.'
  arxivId: '2501.05441'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin
  created_at: '2025-01-11T08:07:28.010024'
  issue_number: 938
  issue_url: https://github.com/dmarx/papers-feed/issues/938
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-12T17:53:23.123Z'
  main_tex_file: null
  published_date: '2025-01-09T18:53:06Z'
  state: open
  title: The GAN is dead; long live the GAN! A Modern GAN Baseline
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2501.05441
'2501.05743':
  abstract: 'The planar three-gluon form factor for the chiral stress tensor operator
    in

    planar maximally supersymmetric Yang-Mills theory is an analog of the

    Higgs-to-three-gluon scattering amplitude in QCD. The amplitude (symbol)

    bootstrap program has provided a wealth of high-loop perturbative data about

    this form factor, with results up to eight loops available. The symbol of the

    form factor at $L$ loops is given by words of length $2L$ in six letters with

    associated integer coefficients. In this paper, we analyze this data,

    describing patterns of zero coefficients and relations between coefficients. We

    find many sequences of words whose coefficients are given by closed-form

    expressions which we expect to be valid at any loop order. Moreover, motivated

    by our previous machine-learning analysis, we identify simple recursion

    relations that relate the coefficient of a word to the coefficients of

    particular lower-loop words. These results open an exciting door for

    understanding scattering amplitudes at all loop orders.'
  arxivId: '2501.05743'
  arxiv_tags:
  - hep-th
  - hep-ph
  authors: Tianji Cai, François Charton, Kyle Cranmer, Lance J. Dixon, Garrett W.
    Merz, Matthias Wilhelm
  created_at: '2025-01-13T04:22:03.526033'
  issue_number: 942
  issue_url: https://github.com/dmarx/papers-feed/issues/942
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-13T04:41:06.637199'
  last_visited: '2025-01-13T04:39:53.737000+00:00'
  main_tex_file: null
  published_date: '2025-01-10T06:19:48Z'
  state: open
  title: "Recurrent Features of Amplitudes in Planar $\\mathcal{N}=4$ Super\n  Yang-Mills\
    \ Theory"
  total_reading_time_seconds: 78
  url: https://arxiv.org/abs/2501.05743
'2501.06252':
  abstract: 'Self-adaptive large language models (LLMs) aim to solve the challenges
    posed

    by traditional fine-tuning methods, which are often computationally intensive

    and static in their ability to handle diverse tasks. We introduce

    $\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for

    unseen tasks in real-time by selectively adjusting only the singular components

    of their weight matrices. During inference, $\text{Transformer}^2$ employs a

    two-pass mechanism: first, a dispatch system identifies the task properties,

    and then task-specific "expert" vectors, trained using reinforcement learning,

    are dynamically mixed to obtain targeted behavior for the incoming prompt. Our

    method outperforms ubiquitous approaches such as LoRA, with fewer parameters

    and greater efficiency. $\text{Transformer}^2$ demonstrates versatility across

    different LLM architectures and modalities, including vision-language tasks.

    $\text{Transformer}^2$ represents a significant leap forward, offering a

    scalable, efficient solution for enhancing the adaptability and task-specific

    performance of LLMs, paving the way for truly dynamic, self-organizing AI

    systems.'
  arxivId: '2501.06252'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: Qi Sun, Edoardo Cetin, Yujin Tang
  created_at: '2025-01-15T03:51:49.310517'
  issue_number: 975
  issue_url: https://github.com/dmarx/papers-feed/issues/975
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-15T05:05:34.185386'
  last_visited: '2025-01-15T05:04:30.386000+00:00'
  main_tex_file: null
  published_date: '2025-01-09T01:19:21Z'
  state: open
  title: '$\text{Transformer}^2$: Self-adaptive LLMs'
  total_reading_time_seconds: 48
  url: https://arxiv.org/abs/2501.06252
