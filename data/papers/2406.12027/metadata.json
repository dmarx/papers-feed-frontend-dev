{
  "arxivId": "2406.12027",
  "title": "Adversarial Perturbations Cannot Reliably Protect Artists From\n  Generative AI",
  "authors": "Robert H\u00f6nig, Javier Rando, Nicholas Carlini, Florian Tram\u00e8r",
  "abstract": "Artists are increasingly concerned about advancements in image generation\nmodels that can closely replicate their unique artistic styles. In response,\nseveral protection tools against style mimicry have been developed that\nincorporate small adversarial perturbations into artworks published online. In\nthis work, we evaluate the effectiveness of popular protections -- with\nmillions of downloads -- and show they only provide a false sense of security.\nWe find that low-effort and \"off-the-shelf\" techniques, such as image\nupscaling, are sufficient to create robust mimicry methods that significantly\ndegrade existing protections. Through a user study, we demonstrate that all\nexisting protections can be easily bypassed, leaving artists vulnerable to\nstyle mimicry. We caution that tools based on adversarial perturbations cannot\nreliably protect artists from the misuse of generative AI, and urge the\ndevelopment of alternative non-technological solutions.",
  "url": "https://arxiv.org/abs/2406.12027",
  "issue_number": 154,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/154",
  "created_at": "2025-01-05T08:24:14.522530",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-22T16:38:53.342Z",
  "main_tex_file": null,
  "published_date": "2024-06-17T18:51:45Z",
  "arxiv_tags": [
    "cs.CR"
  ]
}