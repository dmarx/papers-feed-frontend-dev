<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI</title>
				<funder ref="#_6BNgGuA">
					<orgName type="full">ETH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-17">17 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Hönig</surname></persName>
							<email>robert.hoenig@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Javier</forename><surname>Rando</surname></persName>
							<email>javier.rando@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
							<email>florian.tramer@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-17">17 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">77729D644930FDFB9F5E288186D4CD33</idno>
					<idno type="arXiv">arXiv:2406.12027v1[cs.CR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections-with millions of downloads-and show they only provide a false sense of security. We find that low-effort and "off-the-shelf" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that all existing protections can be easily bypassed, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.</p><p>Code and images released at: <ref type="url" target="https://github.com/ethz-spylab/robust-style-mimicry">https://github.com/ethz-spylab/robust-style-mimicry</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Style mimicry is a popular application of text-to-image generative models. Given a few images from an artist, a model can be finetuned to generate new images in that style (e.g., a spaceship in the style of Van Gogh). But style mimicry has the potential to cause significant harm if misused. In particular, many contemporary artists worry that others could now produce images that copy their unique art style, and potentially steal away customers <ref type="bibr">(Heikkilä, 2022)</ref>. As a response, several protections have been developed to protect artists from style mimicry <ref type="bibr">(Shan et al., 2023a;</ref><ref type="bibr" target="#b47">Van Le et al., 2023;</ref><ref type="bibr">Liang et al., 2023)</ref>. These protections add adversarial perturbations to images that artists publish online, in order to inhibit the finetuning process. These protections have received significant attention from the media-with features in the New York Times <ref type="bibr" target="#b10">(Hill, 2023)</ref>, CNN <ref type="bibr" target="#b45">(Thorbecke, 2023)</ref> and Scientific American <ref type="bibr" target="#b17">(Leffer, 2023)</ref>-and have been downloaded over 1M times <ref type="bibr">(Shan et al., 2023a</ref>).</p><p>Yet, it is unclear to what extent these tools actually protect artists against style mimicry, especially if someone actively attempts to circumvent them <ref type="bibr" target="#b29">(Radiya-Dixit et al., 2021)</ref>. In this work, we show that state-of-the-art style protection tools-Glaze <ref type="bibr">(Shan et al., 2023a)</ref>, Mist <ref type="bibr">(Liang et al., 2023)</ref> and <ref type="bibr">Anti-DreamBooth (Van Le et al., 2023)</ref>-are ineffective when faced with simple robust mimicry methods. The robust mimicry methods we consider range from low-effort strategies-such as using a different finetuning script, or adding Gaussian noise to the images before training-to multi-step strategies that combine off-the-shelf tools. We validate our results with a user study, which reveals that robust mimicry methods can produce results indistinguishable in quality from those obtained from unprotected artworks (see Figure <ref type="figure" target="#fig_0">1</ref> for an illustrative example).</p><p>We show that existing protection tools merely provide a false sense of security. Our robust mimicry methods do not require the development of new tools or fine-tuneing methods, but only carefully Existing protection tools add small perturbations to published artwork to prevent mimicry <ref type="bibr">(Shan et al., 2023a;</ref><ref type="bibr">Liang et al., 2023;</ref><ref type="bibr" target="#b47">Van Le et al., 2023)</ref>. However, these protections fail against robust mimicry methods, giving a false sense of security and leaving artists vulnerable. Artwork by @nulevoy (Stas Voloshin), reproduced with permission. combining standard image processing techniques which already existed at the time that these protection tools were first introduced!. Therefore, we believe that even low-skilled forgers could have easily circumvented these tools since their inception.</p><p>Although we evaluate specific protection tools that exist today, the limitations of style mimicry protections are inherent. Artists are necessarily at a disadvantage since they have to act first (i.e., once someone downloads protected art, the protection can no longer be changed). To be effective, protective tools face the challenging task of creating perturbations that transfer to any finetuning technique, even ones chosen adaptively in the future. A similar conclusion was drawn by <ref type="bibr" target="#b29">Radiya-Dixit et al. (Radiya-Dixit et al., 2021)</ref>, who argued that adversarial perturbations cannot protect users from facial recognition systems. We thus caution that adversarial machine learning techniques will not be able to reliably protect artists from generative style mimicry, and urge the development of alternative measures to protect artists.</p><p>We disclosed our results to the affected protection tools prior to publication, so that they could determine the best course of action for existing users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Text-to-image diffusion models. A latent diffusion model consists of an image autoencoder and a denoiser. The autoencoder is trained to encode and decode images using a lower-dimensional latent space. The denoiser predicts the noise added to latent representations of images in a diffusion process <ref type="bibr" target="#b12">(Ho et al., 2020)</ref>. Latent diffusion models can generate images from text prompts by conditioning the denoiser on image captions <ref type="bibr" target="#b32">(Rombach et al., 2022)</ref>. Popular text-to-image diffusion models include open models such as Stable Diffusion <ref type="bibr" target="#b32">(Rombach et al., 2022)</ref> and Kandinsky <ref type="bibr" target="#b31">(Razzhigaev et al., 2023)</ref>, as well as closed models like Imagen <ref type="bibr" target="#b33">(Saharia et al., 2022)</ref> and DALL-E <ref type="bibr" target="#b30">(Ramesh et al.;</ref><ref type="bibr" target="#b0">Betker et al., 2023)</ref>.</p><p>Style mimicry. Style mimicry uses generative models to create images matching a target artistic style. Existing techniques vary in complexity and quality (see Appendix G). An effective method is to finetune a diffusion model using a few images in the targeted style. Some artists worry that style mimicry can be misused to reproduce their work without permission and steal away customers <ref type="bibr">(Heikkilä, 2022)</ref>.</p><p>Style mimicry protections. Several tools have been proposed to prevent unauthorized style mimicry. These tools allow artists to include small perturbations-optimized to disrupt style mimicry techniques-in their images before publishing. The most popular protections are Glaze <ref type="bibr">(Shan et al., 2023a)</ref> and Mist <ref type="bibr">(Liang et al., 2023)</ref>. Additionally, Anti-DreamBooth <ref type="bibr" target="#b47">(Van Le et al., 2023)</ref> was introduced to prevent fake personalized images, but we also find it effective for style mimicry. Both Glaze and Mist target the encoder in latent diffusion models; they perturb images to obtain latent representations that decode to images in a different style (see Appendix H.1). On the other hand, Anti-DreamBooth targets the denoiser and maximizes the prediction error on the latent representations of the perturbed images (see Appendix H.2).</p><p>Circumventing style mimicry protections. Although not initially designed for this purpose, adversarial purification <ref type="bibr" target="#b50">(Yoon et al., 2021;</ref><ref type="bibr" target="#b40">Shi et al., 2020;</ref><ref type="bibr" target="#b35">Samangouei et al., 2018)</ref> could be used to remove the perturbations introduced by style mimicry protections. DiffPure <ref type="bibr" target="#b25">(Nie et al., 2022)</ref> is the strongest purification method and Mist claims robustness against it. Another existing method for purification is upscaling <ref type="bibr" target="#b24">(Mustafa et al., 2019)</ref>. Similarly, Mist and Glaze claim robustness against upscaling. Section 4.1 highlights flaws in previous evaluations and how a careful application of both methods can effectively remove mimicry protections.</p><p>IMPRESS <ref type="bibr" target="#b1">(Cao et al., 2024)</ref> was the first purification method designed specifically to circumvent style mimicry protections. While IMPRESS claims to circumvent Glaze, the authors of Glaze critique the method's evaluation <ref type="bibr">(Shan et al., 2023b)</ref>, namely the reliance on automated metrics instead of a user study, as well as the method's poor performance on contemporary artists. Our work addresses these limitations by considering simpler and stronger purification methods, and evaluating them rigorously with a user study and across a variety of historical and contemporary artists. Our results show that the main idea of IMPRESS is sound, and that very similar robust mimicry methods are effective.</p><p>Unlearnable examples . Style mimicry protections build upon a line of work that aims to make data "unlearnable" by machine learning models <ref type="bibr" target="#b37">(Shan et al., 2020;</ref><ref type="bibr" target="#b13">Huang et al., 2021;</ref><ref type="bibr" target="#b3">Cherepanova et al., 2021;</ref><ref type="bibr" target="#b34">Salman et al., 2023)</ref>. These methods typically rely on some form of adversarial optimization, inspired by adversarial examples <ref type="bibr" target="#b42">(Szegedy et al., 2013)</ref>. Ultimately, these techniques always fall short of an adaptive adversary that enjoys a second-mover advantage: once unlearnable examples have been collected, their protection can no longer be changed, and the adversary can thereafter select a learning method tailored towards breaking the protections <ref type="bibr" target="#b29">(Radiya-Dixit et al., 2021;</ref><ref type="bibr" target="#b5">Fowl et al., 2021;</ref><ref type="bibr" target="#b44">Tao et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Threat Model</head><p>The goal of style mimicry is to produce images, of some chosen content, that mimic the style of a targeted artist. Since artistic style is challenging to formalize or quantify, we refrain from doing so and define a mimicry attempt as successful if it generates new images that a human observer would qualify as possessing the artist's style.</p><p>We assume two parties, the artist who places art online (e.g., in their portfolio), and a forger who performs style mimicry using these images. The challenge for the forger is that the artist first protects their original art collection before releasing it online, using a state-of-the-art protection tool such as Glaze, Mist or Anti-DreamBooth. We make the conservative assumption that all the artist's images available online are protected. If a mimicry method succeeds in this setting, we call it robust.</p><p>In this work, we consider style forgers who finetune a text-to-image model on an artist's images-the most successful style mimicry method to date <ref type="bibr">(Shan et al., 2023a)</ref>. Specifically, the forger finetunes a pretrained model f on protected images X from the artist to obtain a finetuned model f . The forger has full control over the protected images and finetuning process, and can arbitrarily modify to maximize the mimicry success. Our robust mimicry methods combine a number of "off-the-shelf" manipulations that allow even low-skilled parties to bypass existing style mimicry protections. In fact, our most successful methods require only black-box access to a finetuning API for the model f , and could thus also be applied to proprietary text-to-image models that expose such an interface.  <ref type="bibr">(Shan et al., 2023a)</ref> do not generalize across fine-tuning setups. We mimic the style of the contemporary artist @nulevoy from Glaze-protected images by using: (b) the finetuning script provided by Glaze authors; and (c) an alternative off-the-shelf finetuning script from HuggingFace. In both cases, we perform "naive" style mimicry with no effort to bypass Glaze's protections. Glaze protections are successful using finetuning from the original paper, but significantly degrade with our script. Our finetuning is also better for unprotected images (see Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Robust Style Mimicry</head><p>We say that a style mimicry method is robust if it can emulate an artist's style using only protected artwork. While methods for robust mimicry have already been proposed, we note a number of limitations in these methods and their evaluation in Section 4.1. We then propose our own methods (Section 4.3) and evaluation (Section 5) which address these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Limitations of Prior Robust Mimicry Methods and of Their Evaluations</head><p>(1) Some mimicry protections do not generalize across finetuning setups. Most forgers are inherently ill-intentioned since they ignore artists' genuine requests not to use their art for generative AI <ref type="bibr">(Heikkilä, 2022)</ref>. A successful protection must thus resist circumvention attempts from a reasonably resourced forger who may try out a variety of tools. Yet, in preliminary experiments, we found that Glaze <ref type="bibr">(Shan et al., 2023a)</ref> performed significantly worse than claimed in the original evaluation, even before actively attempting to circumvent it. After discussion with the authors of Glaze, we found small differences between our off-the-shelf finetuning script, and the one used in Glaze's original evaluation (which the authors shared with us).<ref type="foot" target="#foot_0">foot_0</ref> These minor differences in finetuning are sufficient to significantly degrade Glaze's protections (see Figure <ref type="figure">2</ref> for qualitative examples). Since our off-the-shelf finetuning script was not designed to bypass style mimicry protections, these results already hint at the superficial and brittle protections that existing tools provide: artists have no control over the finetuning script or hyperparameters a forger would use, so protections must be robust across these choices.</p><p>(2) Existing robust mimicry attempts are sub-optimal. Prior evaluations of protections fail to reflect the capabilities of moderately resourceful forgers, who employ state-of-the-art methods (even off-the-shelf ones). For instance, Mist <ref type="bibr">(Liang et al., 2023)</ref> evaluates against DiffPure purifications using an outdated and low-resolution purification model. Using DiffPure with a more recent model, we observe significant improvements. Glaze <ref type="bibr">(Shan et al., 2023a)</ref> is not evaluated against any version of DiffPure, but claims protection against Compressed Upscaling, which first compresses an image with JPEG and then upscales it with a dedicated model. Yet, we will show that by simply swapping the JPEG compression with Gaussian noising, we create Noisy Upscaling as a variant that is highly successful at removing mimicry protections (see Figure <ref type="figure" target="#fig_14">26</ref> for a comparison between both methods).</p><p>(3) Existing evaluations are non-comprehensive. Comparing the robustness of prior protections is challenging because the original evaluations use different sets of artists, prompts, and finetuning setups. Moreover, some evaluations rely on automated metrics (e.g., CLIP similarity) which are unreliable for measuring style mimicry <ref type="bibr">(Shan et al., 2023a,b)</ref>. Due to the brittleness of protection methods and the subjectivity of mimicry assessments, we believe a unified evaluation is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Unified and Rigorous Evaluation of Robust Mimicry Methods</head><p>To address the limitations presented in Section 4.1, we introduce a unified evaluation protocol to reliably assess how existing protections fare against a variety of simple and natural robust mimicry methods. Our solutions to each of the numbered limitations above are: (1) The attacker uses a popular "off-the-shelf" finetuning script for the strongest open-source model that all protections claim to be effective for: Stable Diffusion 2.1. This finetuning script is chosen independently of any of these protections, and we treat it as a black-box. (2) We design four robust mimicry methods, described in Section 4.3. We prioritize simplicity and ease of use for low-expertise attackers by combining a variety of off-the-shelf tools. (3) We design and conduct a user study to evaluate each mimicry protection against each robust mimicry method on a common set of artists and prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Our Robust Mimicry Methods</head><p>We now describe four robust mimicry methods that we designed to assess the robustness of protections. We primarily prioritize simple methods that only require preprocessing protected images. These methods present a higher risk because they are more accessible, do not require technical expertise, and can be used in black-box scenarios (e.g. if finetuning is provided as an API service). For completeness, we further propose one white-box method, inspired by IMPRESS <ref type="bibr" target="#b1">(Cao et al., 2024)</ref>.</p><p>We note that the methods we propose have been considered (at least in part) in prior work that found them to be ineffective against style mimicry protections <ref type="bibr">(Shan et al., 2023a;</ref><ref type="bibr">Liang et al., 2023;</ref><ref type="bibr">Shan et al., 2023b</ref>). Yet, as we noted in Section 4.1, these evaluations suffered from a number of limitations. We thus re-evaluate these methods (or slight variants thereof) and will show that they are significantly more successful than previously claimed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Black-box preprocessing methods.</head><p>✦ Gaussian noising. As a simple preprocessing step, we add small amounts of Gaussian noise to protected images. This approach can be used ahead of any black-box diffusion model. ✦ DiffPure. We use image-to-image models to remove perturbations introduced by the protections, also called DiffPure <ref type="bibr" target="#b25">(Nie et al., 2022)</ref> (see Appendix I.1). This method is black-box, but requires two different models: the purifier, and the one used for style mimicry. We use Stable Diffusion XL as our purifier.</p><p>✦ Noisy Upscaling. We introduce a simple and effective variant of the two-stage upscaling purification considered in Glaze <ref type="bibr">(Shan et al., 2023a)</ref>. Their method first performs JPEG compression (to minimize perturbations) and then uses the Stable Diffusion Upscaler <ref type="bibr" target="#b32">(Rombach et al., 2022)</ref> (to mitigate degradations in quality). Yet, we find that upscaling actually magnifies JPEG compression artifacts instead of removing them. To design a better purification method, we observe that the Upscaler is trained on images augmented with Gaussian noise. Therefore, we purify a protected image by first applying Gaussian noise and then applying the Upscaler. This Noisy Upscaling method introduces no perceptible artifacts and significantly reduces protections (see Figure <ref type="figure" target="#fig_14">26</ref> for an example and Appendix I.2 for details).</p><p>White-box methods.</p><p>✦ IMPRESS++. For completeness, we design a white-box method to assess whether more complex methods can further enhance the robustness of style mimicry. Our method builds on IMPRESS <ref type="bibr" target="#b1">(Cao et al., 2024)</ref> but adopts a different loss function and further applies negative prompting <ref type="bibr" target="#b23">(Miyake et al., 2023)</ref> and denoising to improve the robustness of the sampling procedure (see Appendix I.3 and Figure <ref type="figure">27</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Protection tools. We evaluate three protection tools-Mist, Glaze and Anti-DreamBooth-against four robust mimicry methods-Gaussian noising, DiffPure, Noisy Upscaling and IMPRESS++-and a baseline mimicry method. We refer to a combination of a protection tool and a mimicry method as a scenario. We thus analyze fifteen possible scenarios. Appendix J describes our experimental setup for style mimicry and protections in detail. We apply Noisy Upscaling for prompts: "a shoe" and "an astronaut riding a horse".</p><p>Artists. We evaluate each style mimicry scenario on images from 10 different artists, which we selected to maximize style diversity. To address limitations in prior evaluations <ref type="bibr">(Shan et al., 2023b)</ref>, we use five historical artists as well as five contemporary artists who are unlikely to be highly represented in the generative model's training set (two of these were also used in Glaze's evaluation).<ref type="foot" target="#foot_2">foot_2</ref> All details about artist selection are included in Appendix J.</p><p>Implementation. Our mimicry methods finetune Stable Diffusion 2.1 <ref type="bibr" target="#b32">(Rombach et al., 2022)</ref>, the best open-source model available at the time when the protections we study were introduced. We use an off-the-shelf finetuning script from HuggingFace (see Appendix J.1 for details). We first validate that our style mimicry pipeline is successful on unprotected art using a user study, detailed in Appendix K.1. For protections, we use the original codebases to reproduce Mist and Anti-Dreambooth. Since Glaze does not have a public codebase (and the authors were unable to share one), we use the released Windows application binary (version 1.1.1) as a black-box. We set each scheme's hyperparameters to maximize protections. See Appendix J.2 for details on the configuration for each protection.</p><p>We perform robust mimicry by finetuning on 18 different images per artist. We then generate images for 10 different prompts. These prompts are designed to cover diverse motifs that the base model, Stable Diffusion 2.1, can successfully generate. See Appendix K for details about prompt design.</p><p>User study. To measure the success of each style mimicry scenario, we rely only on human evaluations since previous work found automated metrics (e.g., using CLIP <ref type="bibr" target="#b28">(Radford et al., 2021)</ref>) to be unreliable <ref type="bibr">(Shan et al., 2023a,b)</ref>. Moreover, style protections not only prevent style transfer, but also reduce the overall quality of the generated images (see Figure <ref type="figure" target="#fig_2">3</ref> for examples). We thus design a user study to evaluate image quality and style transfer as independent attributes of the generations. <ref type="foot" target="#foot_3">3</ref>Our user study asks annotators on Amazon Mechanical Turk (MTurk) to compare image pairs, where one image is generated by a robust mimicry method, and the other from a baseline state-of-the-art mimicry method that uses unprotected art of the artist. A perfectly robust mimicry method would generate images of quality and style indistinguishable from those generated directly from unprotected art. We perform two separate studies: one assessing image quality (e.g., which image looks "better") and another evaluating stylistic transfer (i.e., which image captures the artist's original style better, disregarding potential quality artifacts). Our results show that these two metrics obtain very similar results across all scenarios. Appendix K describes our user study and interface in detail.</p><p>As noted by the authors of Glaze <ref type="bibr">(Shan et al., 2023a)</ref>, the users of platforms like MTurk might not have high artistic expertise. However, we believe that the judgment of non-artists is also relevant as they ultimately represent a large fraction of the consumers of digital art. Thus, if lay people consider mimicry attempts to be successful, mimicked art could hurt an artist's business. Also, to mitigate potential issues with the quality of annotations <ref type="bibr" target="#b15">(Kennedy et al., 2020)</ref>, we put in place several control mechanisms to filter out low-quality annotations to the best of our abilities (details in Appendix K).</p><p>Evaluation metric. We define the success rate of a robust mimicry method as the percentage of annotators (5 per comparison) who prefer outputs from the robust mimicry method over those from a baseline method finetuned on unprotected art (when judging either style match or overall image quality). Formally, we define the success rate for an artist in a specific scenario as:</p><formula xml:id="formula_0">success rate = 1 10 • 5 10 prompt 5 annotator 1[robust mimicry preferred over unprotected mimicry] (1)</formula><p>A perfectly robust mimicry method would thus obtain a success rate of 50%, indicating that its outputs are indistinguishable in quality and style from those from the baseline, unprotected method. In contrast, a very successful protection would result in success rates of around 0% for robust mimicry methods, indicating that mimicry on top of protected images always yields worse outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In Figure <ref type="figure">4</ref>, we report the distribution of success rates per artist (N=10) for each scenario. We averaged the quality and stylistic transfer success rates to simplify the analysis (detailed results can be found in Appendix C). Since the forger can try multiple mimicry methods for each prompt, and then decide which one worked best, we also evaluate a "best-of-4" method that picks the most successful mimicry method for each generation (according to human evaluators).</p><p>median artist most protected artist most vulnerable artist 0% 25% 50% 75% Success rate Naive mimicry Gaussian noising IMPRESS++ DiffPure Noisy Upscaling Best-of-4 Anti-DreamBooth 0% 25% 50% 75% Success rate Glaze 0% 25% 50% 75% Success rate Mist Figure 4: Success rate per artist (N=10) on all mimicry scenarios. Box plots represent success rates for most protected, quartiles, median and least protected artists, respectively. Success rates around 50% indicate that robust mimicry outputs are indistinguishable in style and quality from mimicry outputs based on unprotected images. Best-of-4 selects the most successful method for each prompt. 6.1 Main Findings: All Protections are Easily Circumvented</p><p>We find that all existing protective tools create a false sense of security and leave artists vulnerable to style mimicry. Indeed, our best robust mimicry methods produce images that are, on average, indistinguishable from baseline mimicry attempts using unprotected art. Since many of our simple mimicry methods only use tools that were available before the protections were released, style forgers may have already circumvented these protections since their inception.</p><p>Noisy upscaling is the most effective method for robust mimicry, with a median success rate above 40% for each protection tool (recall that 50% success indicates that the robust method is indistinguishable from a mimicry using unprotected images). This method only requires preprocessing images and black-box access to the model via a finetuning API. Other simple preprocessing methods like Gaussian noising or DiffPure also significantly reduce the effectiveness of protections. The more complex white-box method IMPRESS++ does not provide significant advantages. Sample generations for each method are in Appendix B.</p><p>A style forger does not have to use a single robust mimicry method, but can test all of them and select the most successful. This "best-of-4" approach always beats the baseline mimicry method over unprotected images (which attempts a single method and not four) for all protections.</p><p>Appendix A shows images at each step of the robust mimicry process (i.e., protections, preprocessing, and sampling). Appendix B shows example generations for each protection and mimicry method. Appendix C has detailed success rates broken down per artist, for both image style and quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Analysis</head><p>We now discuss key insights and lessons learned from these results.</p><p>Glaze protections break down without any circumvention attempt. Results for Glaze without robust mimicry (see "Naive mimicry" row in Figure <ref type="figure">4</ref>) show that the tool's protections are often ineffective. Without any robustness intervention, 30% of the images generated with our off-the-shelf finetuning are rated as better than the baseline results using only unprotected images. This contrasts with Glaze's original evaluation, which claimed a success rate of at most 10% for robust mimicry. <ref type="foot" target="#foot_4">4</ref>This difference is likely due to the protection's brittleness to slight changes in the finetuning setup (as we illustrated in Section 4.1). With our best robust mimicry method (noisy upscaling) the median success rate across artists rises further to 40%, and our best-of-4 strategy yields results indistinguishable from the baseline for a majority of artists.</p><p>Robust mimicry works for contemporary and historical artists alike. <ref type="bibr">Shan et al. (2023b)</ref> note that one of IMPRESS' main limitations is that "purification has a limited effect when tested on artists that are not well-known historical artists already embedded in original training data". Yet, we find that our best-performing robust mimicry method-Noisy Upscaling-has a similar success rate for historical artists (42.2%) and contemporary artists with little representation in the model's training set (43.5%).</p><p>Protections are highly non-uniform across artists. As we observe from Figure <ref type="figure">4</ref>, the effectiveness of protections varies significantly across artists: the least vulnerable artist (left-most whisker) enjoys much stronger mimicry protections than the median artist or the most vulnerable artist (right-most whisker). We find that robust mimicry is the least successful for artists where the baseline mimicry from unprotected images gives poor results to begin with (cf. results for artist A 1 in Appendix C and Appendix K.1). Yet, since existing tools do not provide artists with a way to check how vulnerable they are, these tools still provide a false sense of security for all artists. This highlights an inherent asymmetry between protection tools and mimicry methods: protections should hold for all artists alike, while a mimicry method might successfully target only specific artists.</p><p>Robust mimicry failures still remove protection artifacts. We manually checked the cases where all annotators ranked mimicry from unprotected art as better than robust mimicry with Noisy Upscaling. Figure <ref type="figure" target="#fig_3">5</ref> shows two examples. We find that in many instances, the model fails to mimic the style accurately even from unprotected art. In these cases, robust mimicry is still able to generate clear images that are similar to unprotected mimicry, but neither matches the original style well. 7 Discussion and Broader Impact Adversarial perturbations do not protect artists from style mimicry. Our work is not intended as an exhaustive search for the best robust mimicry method, but as a demonstration of the brittleness of existing protections. Because these protections have received significant attention, artists may believe they are effective. But our experiments show they are not. As we have learned from adversarial ML, whoever acts first (in this case, the artist) is at a fundamental disadvantage <ref type="bibr" target="#b29">(Radiya-Dixit et al., 2021)</ref>. We urge the community to acknowledge these limitations and think critically when performing future evaluations.</p><p>Just like adversarial examples defenses, mimicry protections should be evaluated adaptively. In adversarial settings, where one group wants to prevent another group from achieving some goal, it is necessary to consider "adaptive attacks" that are specifically designed to evade the defense <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017)</ref>. Unfortunately, as repeatedly seen in the literature on machine learning robustness, even after adaptive attacks were introduced, many evaluations remained flawed and defenses were broken by (stronger) adaptive attacks <ref type="bibr" target="#b46">(Tramer et al., 2020)</ref>. We show it is the same with mimicry protections: simple adaptive attacks significantly reduce their effectiveness. Surprisingly, most protections we study claim robustness against input transformations <ref type="bibr">(Liang et al., 2023;</ref><ref type="bibr">Shan et al., 2023a)</ref>, but minor modifications were sufficient to circumvent them.</p><p>We hope that the literature on style mimicry prevention will learn from the failings of the adversarial example literature: performing reliable, future-proof evaluations is much harder than proposing a new defense. Especially when techniques are widely publicized in the popular press, we believe it is necessary to provide users with exceptionally high degrees of confidence in their efficacy.</p><p>Protections are broken from day one, and cannot improve over time. Our most successful robust style mimicry methods rely solely on techniques that existed before the protections were introduced. Also, protections applied to online images cannot easily be changed (i.e., even if the image is perturbed again and re-uploaded, the older version may still be available in an internet archive) <ref type="bibr" target="#b29">(Radiya-Dixit et al., 2021)</ref>. It is thus challenging for a broken protection method to be fixed retroactively. Of course, an artist can apply the new tool to their images going forward, but pre-existing images with weaker protections (or none at all) will significantly boost an attacker's success <ref type="bibr">(Shan et al., 2023a)</ref>.</p><p>Nevertheless, the Glaze and Mist protection tools recently received significant updates (after we had concluded our user study). Yet, we find that the newest 2.0 versions do not protect against our robust mimicry attempts either (see Appendix E and F). A future version could explicitly target the methods we studied, but this would not change the fact that all previously protected art would remain vulnerable, and that future attacks could again attempt to adaptively evade the newest protections. The same holds true for attempts to design similar protections for other data modalities, such as video <ref type="bibr" target="#b26">(Passananti et al., 2024)</ref> or audio <ref type="bibr" target="#b7">(Gokul &amp; Dubnov, 2024)</ref>.</p><p>Ethics and broader impact. The goal of our research is to help artists better decide how to protect their artwork and business. We do not focus on creating the best mimicry method, but rather on highlighting limitations in popular perturbation tools-especially since using these tools incurs a cost, as they degrade the quality of published art. We will disclose our results to the affected protection tools prior to publication, so that they can determine the best course of action for their users.</p><p>Further, we argue that having no protection tools is preferable to having insecure ones. Insecure protections may mislead artists to believe it is safe to release their work, enabling forgery and putting them in a worse situation than if they had been more cautious in the absence of any protection.</p><p>With respect to our paper, all the art featured in this paper comes either from historical artists, or from contemporary artists who explicitly permitted us to display their work. We hope our results will inform improved non-technical protections for artists in the era of generative AI.</p><p>Limitations and future work. A larger study with more than 10 artists and more annotators may help us better understand the difference in vulnerability across artists. The protections we study are not designed in awareness of our robust mimicry methods. However, we do not believe this limits the extent to which our general claims hold: artists will always be at a disadvantage if attackers can design adaptive methods to circumvent the protections.</p><p>Boyang Zheng, Chumeng Liang, Xiaoyu Wu, and Yan Liu. Understanding and improving adversarial attacks on latent diffusion model. arXiv preprint arXiv:2310.04687, 2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Art Examples</head><p>This section illustrates how images look like at every stage of our work. We include (1) original artwork from a contemporary artist (@nulevoy) 5 as a reference in Figure <ref type="figure" target="#fig_4">6</ref>, (2) the original artwork after applying each of the available protections in Figure <ref type="figure">7</ref>, (3) one image after applying the cross product of all protections and preprocessing methods in Figure <ref type="figure">8</ref>, (4) baseline generations from a model trained on unprotected art in Figure <ref type="figure" target="#fig_6">9</ref>, and (5) robust mimicry generations for each scenario in Figure <ref type="figure" target="#fig_0">10</ref>.   Figure <ref type="figure" target="#fig_0">10</ref>: Generations in the style of @nulevoy using robust mimicry methods for the prompt "an astronaut riding a horse". Each row represents which protection was applied to the finetuning data. Each column represents the robust mimicry method used. The first column indicates naive mimicry was applied (i.e. we trained directly on the protected images). Figure <ref type="figure" target="#fig_6">9</ref> includes sample generations from a model trained on artwork without protections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Robust Mimicry Generations</head><p>Albrecht Durer, "a shoe with a plant growing inside"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anti-DB + Gaussian noising Unprotected</head><p>Glaze + Gaussian noising Unprotected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mist + Gaussian noising Unprotected</head><p>Edvard Munch, "a shoe" Edvard Munch, "a piano" A5, "a feathered car"</p><p>Anna O.-Lebedeva, "a piano" A5, "a village in a thunderstorm" Edward Hopper, "a golden apple" Edward Hopper, "a feathered car"</p><p>Figure <ref type="figure" target="#fig_0">11</ref>: Style mimicry for all protections using naive mimicry-no robust method is used and we finetune directly on protected images. We randomly chose artists and prompts. Each image pair shows the protected generation and generation from unprotected art. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Results Broken Down per Artist</head><p>We present next the results obtained for each artist in each scenario. Table <ref type="table" target="#tab_2">2</ref> plots the success rate for each method against each protection for all artists, and Table <ref type="table" target="#tab_3">3</ref> includes the detailed success rates.  Inter-annotator agreement 3/5 votes agree 4/5 votes agree 5/5 votes agree</p><p>Figure <ref type="figure" target="#fig_0">18</ref>: Inter-annotator agreement for generations from robust mimicry with Noisy Upscaling and generations from models finetuned on protected art directly (naive mimicry). We plot the percentage of comparisons for which the preferred option was selected by 3, 4 or 5 annotators, respectively.</p><p>The graph shows a higher consensus for naive mimicry, since the differences are clearer, and more variance for robust mimicry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Differences with Glaze Finetuning</head><p>In Section 4.1 and Figure <ref type="figure">2</ref>, we discussed the brittleness of Glaze protections against small changes in the finetuning script. We also found our finetuning setup to be better at baseline style mimicry from unprotected art (see Figure <ref type="figure" target="#fig_6">19</ref>). Figure <ref type="figure" target="#fig_6">19</ref>: The finetuning script shared by Glaze authors produce substantially worse mimicry even from unprotected art. We apply both finetuning scripts directly on unprotected art from @nulevoy.</p><p>The main reason behind this difference might be that the script uses Stable Diffusion 1.5, instead of version 2.1 as reported in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Findings on Glaze 2.0</head><p>After concluding our user study, Glaze <ref type="bibr">(Shan et al., 2023a)</ref> released an updated version of their tool (v2.0). According to the official release, "This new version significantly improved Glaze robustness against the newest AI models". Although we could not run the entire user study with the latest protections, we reproduced some of our experiments to verify if protections were more robust under robust mimicry. We believe this comparison is fair to Glaze since we are using newer models-such as Stable Diffusion XL for upscaling. These models, although released before Glaze 1.1.1, may not have been considered in the tool's design and are now explicitly accounted for.</p><p>The official release specifically mentions "Significantly improved robustness against Stable Diffusion 1, 2, SDXL, especially for smooth surface art (e.g. anime, cartoon)". Therefore, we decided to test this new tool with the contemporary artist nulevoy, who draws in a cartoon style and gave us permission to display their artwork. As with the previous version, we only have access to the publicly available Windows application that uses unknown parameters. We protect the images using the "highest" protection option. Our main findings are:</p><p>1. Glaze v2.0 introduces more visible perturbations uniformly over the images. See  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Findings on Mist v2</head><p>After responsibly disclosing our work to defense developers, authors from Mist brought to our attention the recent release of their latest Mist v2 with improved resilience <ref type="bibr">(Zheng et al., 2023)</ref>. As we did with Glaze v2.0 (see Section E), we reproduced some of our experiments with the latest protections to verify the success of robust mimicry. Their original implementation still uses the outdated version 1.5 of Stable Diffusion. We change to SD 2.1 to match our previous experiments<ref type="foot" target="#foot_5">foot_5</ref> . Our findings, as we saw with Glaze v2.0, highlight that improved protections are still not effective against low-effort robust mimicry. More specifically, the latest version of Mist:</p><p>1. introduces visible perturbations over the images. See Figure <ref type="figure" target="#fig_11">23</ref> 2. does not improve protections against robust mimicry. See Figure <ref type="figure">24</ref> 3. creates protection that are easily removable with Noisy Upscaling. See Figure <ref type="figure" target="#fig_3">25</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Existing Style Mimicry Protections</head><p>Naming convention. on the context, style mimicry protections may be viewed either as attacks or as the targets of attacks. In an artistic setting, artists see style mimicry as an attack and utilize methods like Glaze as a defense. Conversely, in the context of adversarial robustness, Glaze can be seen as an attack against style mimicry methods through adversarial perturbations. The research community has not reached a consensus on terminology: Glaze's authors consider style mimicry an attack and label Glaze as a defense, while the authors of Mist and Anti-DreamBooth describe their approaches as attacks. In our work, we distance ourselves from the attack/defense terminology and instead refer to these mechanisms as protections, and to the party performing mimicry as the "style forger".</p><p>Existing protections can either target the encoder or the decoder of text-to-image models. We classify them accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Encoder Protections</head><p>Encoder protections include adversarial perturbations in the images X so that the encoder E ϕ of the model maps images to latent representations that, when reconstructed, recover images in a different style. Concretely, an encoder protection first defines a target latent representation t x ∈ Latent for each image x ∈ X that is different to its own style. For instance, the target latent representation for Edvard Munch could be Vincent Van Gogh. Then, protection P optimizes the objective</p><formula xml:id="formula_1">min δx d Lat (E ϕ (x + δ x ) , t x ) subject to d Img (x + δ x , x) ≤ p.</formula><p>(2)</p><p>Glaze <ref type="bibr">(Shan et al., 2023a)</ref> is an instance of an encoder protection. Glaze first selects an adversarial target style S adv that style mimicry should learn instead of the style S to be protected. Then, Glaze uses Img2Img style transfer to create a variation x Sadv in style S adv of each image x ∈ X. The latent representation of variation x Sadv is used as the target latent representation t x for each image x ∈ X.</p><p>Glaze selects the target style S adv from a pre-defined set of 50 styles S adv . First, Glaze computes the distance between the mean CLIP embedding of the images X and the prompt P S ′ corresponding to each style S ′ ∈ S adv . Then, Glaze randomly samples target style S adv from the 50 to the 75 percentile of target styles S adv sorted by distance.</p><p>Glaze implements Objective (2) with the penalty method <ref type="bibr" target="#b49">(Wright, 2006)</ref> as</p><formula xml:id="formula_2">min δx ∥E ϕ (x + δ x ) , t x ∥ 2 2 + α • max(LPIPS(x + δ x , x) -p, 0)<label>(3)</label></formula><p>where LPIPS <ref type="bibr" target="#b51">(Zhang et al., 2018)</ref> is a choice for metric d Img that aims to measure user-perceived image distortion. Glaze then optimizes Objective (3) with the Adam (Kingma &amp; Ba, 2014) optimizer.</p><p>Mist ϕ <ref type="bibr">(Liang et al., 2023</ref>) is a different encoder protection from the Mist project<ref type="foot" target="#foot_6">foot_6</ref> . Mist ϕ optimizes perturbations with PGD to minimize the squared L 2 -induced distance between the latent representation of the artists' images and some unrelated target image.</p><p>In their original work, Mist is only evaluated against DreamBooth, Style Transfer, and Textual Inversion, but not against finetuning. Also, the original Mist work refers to Mist ϕ as Mist operating in textural mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Denoiser Protections</head><p>Denoiser protections use the prediction error of the denoiser ϵ θ as a proxy of the quality of style mimicry, making it a feasible target for adversarial optimization. Current Denoiser protections, such as Mist <ref type="bibr">(Liang et al., 2023)</ref> and <ref type="bibr">Anti-DreamBooth (Van Le et al., 2023)</ref> assume that poorly reconstructed images will fail to mimic style Anti-DreamBooth <ref type="bibr" target="#b47">(Van Le et al., 2023)</ref> uses the prediction error of the denoiser ϵ θadv as a proxy for the mimicry quality, where denoiser ϵ θadv corresponds to the denoiser from a finetuned model trained on images with the style to be Since perturbations maximizing the error with the pretrained decoder can be easily circunvented with finetuning, Anti-DreamBooth uses a technique they refer to as Alternating Surrogate and Perturbation Learning (ASPL). The intuition behind ASPL is trying to simulate finetuning on the art and maximizing the error during finetuning. For this purpose, they interleave finetuning steps with perturbation optimization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Robust Mimicry Methods</head><p>This section details the robust mimicry methods we use in our work. These methods are not aimed at maximizing performance. Instead, they demonstrate how various "off-the-shelf" and low-effort techniques can significantly weaken style mimicry protections.</p><p>Formally, given protected images X and a pretrained text-to-image model f , we define a general robust mimicry pipeline that finetunes a model f and then produces an image Z for a given prompt as follows (a successful method may not require modifications in all stages):</p><formula xml:id="formula_3">f ← Finetune(f ; PreProcess(X))</formula><p>Z ← PostProcess(Sample( f , "prompt")).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 DiffPure</head><p>DiffPure <ref type="bibr" target="#b25">(Nie et al., 2022)</ref> uses image generation diffusion models to adversarially purify images X prot . DiffPure processes each image x adv ∈ X prot with t timesteps of a diffusion process to obtain the diffused image</p><formula xml:id="formula_4">x t adv = √ α t • x adv + √ 1 -α t • ϵ,</formula><p>where α is the noise schedule of the diffusion process and noise ϵ is sampled from N (0, I). Then, DiffPure constructs the purified image DiffPure(x adv ) by applying reverse diffusion to image x t adv for t timesteps with an image generation diffusion model DM. Nie et al. prove that under certain idealized conditions, DiffPure is likely to weaken adversarial perturbations in image x adv .</p><p>If the text-to-image model M supports unconditional image generation, then we can use model M for the reverse diffusion process. For example, Stable Diffusion <ref type="bibr" target="#b32">(Rombach et al., 2022)</ref> generates images unconditionally when the prompt P equals the empty string. Under these conditions, Img2Img is equivalent to DiffPure. Therefore, in the context of defenses for style mimicry, we refer to Img2Img applied with an empty prompt P as unconditional DiffPure, and to Img2Img applied with a non-empty prompt P as conditional DiffPure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Noisy Upscaling</head><p>Upscaling increases the resolution of an image by predicting new pixels that enhance the level of detail. Upscaling images can purify adversarially perturbed images <ref type="bibr" target="#b24">(Mustafa et al., 2019)</ref>. However, we discover that applying upscaling directly on protected images fails to remove the perturbations.</p><p>We define Noisy Upscaling as a way to address the shortcomings of upscaling. Noisy Upscaling first applies Gaussian noising and then upscales the noisy image. Noisy Upscaling has a more profound effect than the sum of its parts: Gaussian noising only adds noise to an image x adv , but does not remove the adversarial perturbation δ x . Similarly, we observe upscaling to roughly preserve perturbation δ x . In contrast, NoisyUpscale(x adv ) shows neither visually perceptible noise, nor adversarial perturbations. Figure <ref type="figure" target="#fig_14">26</ref> illustrates the improvements. We explain these phenomena as follows.</p><p>First, we use the Stable Diffusion Upscaler (Upscale SD ), which is trained on noise-augmented images and accepts the corresponding noise level L as a class-conditioning label. We can therefore condition Upscale SD on the noise level L σ 2 , corresponding to the variance σ 2 used by GaussianNoising, to remove the noise that GaussianNoising adds.</p><p>Second, we note that upscaling has shown success against adversarial perturbations for classifiers <ref type="bibr" target="#b24">(Mustafa et al., 2019)</ref>, but not against adversarial perturbations for generative models <ref type="bibr">(Liang et al., 2023;</ref><ref type="bibr">Shan et al., 2023a)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 IMPRESS++</head><p>We enhance the IMPRESS algorithm <ref type="bibr" target="#b1">(Cao et al., 2024)</ref>. We change the loss of the reverse encoding optimization from patch similarity to l ∞ and include two additional steps: negative prompting and post-processing. All in all, IMPRESS++ first preprocesses protected images with Gaussian noise and reverse encoder optimization, then samples using negative prompting and finally post-processes the generated images with DiffPure to remove noise.</p><p>Reverse encoder optimization. Reverse encoder optimization is a preprocessing defense against encoder protections. It adds additional perturbations ∆ ∆ ′ to images X prot so that the latent representation</p><formula xml:id="formula_5">t x ′ adv = E ϕ (x ′ adv ) of each protected image x ′ adv = x adv + δ xadv satisfies D ϕ ′ t x ′ adv ≈ x ′ adv (4)</formula><p>and each perturbation δ xadv ∈ ∆ ∆ ′ satisfies d Img (x adv + δ xadv , x adv ) ≤ p.</p><p>(5)</p><p>If Equation (4) holds, then style mimicry finetuning learns the style of images X ′ prot . In addition, the combination of Equation ( <ref type="formula">5</ref>) with the image similarity constraint d Img (x + δ x , x) ≤ p in Objective (2) ensures that the defended images X ′ prot look similar to the original images X. Therefore, style mimicry finetuning on images X ′ prot should learn a style similar to style S. Reverse encoder optimization aims to achieve Equation (4) and Equation ( <ref type="formula">5</ref>) by optimizing the objective min</p><formula xml:id="formula_6">δx adv d Lat (E ϕ (x adv + δ xadv ) , E ϕ (x adv )) subject to d Img (x adv + δ xadv , x adv ) ≤ p<label>(6)</label></formula><p>with PGD.</p><p>Negative prompting. Negative prompting <ref type="bibr" target="#b23">(Miyake et al., 2023)</ref> is a technique to guide image generation of a diffusion-based text-to-image model M away from a prompt P neg . To this end, negative prompting manipulates the classifier-free guidance <ref type="bibr" target="#b11">(Ho &amp; Salimans, 2022)</ref>, which computes the denoiser output of model M as εθ (z, t, P ) = (1 + w) • ϵ θ (z, t, P ) -w • ϵ θ (z, t, "")</p><p>where parameter w controls the guidance strength. Negative prompting simply substitutes the empty string "" with P neg to obtain εθ (z, t, P ) = (1 + w) • ϵ θ (z, t, P ) -w • ϵ θ (z, t, P neg ) .</p><p>We design a routine for D InF that leverages negative prompting to guide model M away from adversarial generations. To this end, we first apply Textual Inversion with adversarial images X prot to encode the style of adversarial generations S adv into a special word w * . We then set prompt P neg = "art by w * ".</p><p>Naive negative prompting offers no strength control. little strength may fail to guide model M away from the adversarial style S adv . Too much strength may guide towards the style opposite to style S adv in the latent space of model M, which is not necessarily the desired style S. We use negative prompt weights (muerrilla, 2023) to control the strength of negative prompting. The negative prompt weights technique introduces the strength control parameter c to interpolate between Equation ( <ref type="formula" target="#formula_7">7</ref>) and Equation ( <ref type="formula" target="#formula_8">8</ref>  <ref type="bibr" target="#b1">(Cao et al., 2024)</ref>. Negative prompting improves image consistency and denoising reduces artifacts in generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Experimental Setup</head><p>This section describes our general experimental setup and specifies the settings and hyperparameters of the methods we use. When possible, we use default values from the machine learning literature. For implementation details see our official repository: <ref type="url" target="https://github.com/ethz-spylab/robust-style-mimicry">https://github.com/ethz-spylab/ robust-style-mimicry</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 Style Mimicry Experimental Details</head><p>As described in Section 3, our threat model considers style mimicry with a latent diffusion text-toimage model M that is finetuned on a set of images X in a style S. This section specifies our choices for model M, images X, style S, the hyperparameters for finetuning M, and the hyperparameters for generating images with the finetuned model. Where possible, we try to replicate the style mimicry setup used by Shan et al. to evaluate Glaze, and highlight any differences.</p><p>Model We use Stable Diffusion version 2.1 <ref type="bibr" target="#b41">(Stability AI, 2022)</ref>, the same model used to optimize the protections we evaluate <ref type="bibr">(Shan et al., 2023a;</ref><ref type="bibr">Liang et al., 2023;</ref><ref type="bibr" target="#b47">Van Le et al., 2023)</ref>.</p><p>Dataset. We collate 10 image sets X A : A ∈ A from 10 different artists A. Each image set X A contains 18 images that we choose manually to follow a consistent style S A . We select the artists A from contemporary and historical artists: We select 5 contemporary artists from ArtStation<ref type="foot" target="#foot_7">foot_7</ref> and 5 historical artists from the WikiArt dataset <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>. We found 2 of the 4 artists used by Glaze and included them in our evaluation. We manually select the remaining 8 artists to cover a broad variety of styles. Glaze additionally verified that the images of the contemporary artists in their evaluation are not included in the training dataset of the model M. Unfortunately, the LAION-5B dataset <ref type="bibr" target="#b36">(Schuhmann et al., 2022)</ref> used to train SD 2.1 was taken offline <ref type="bibr" target="#b4">(Cole, 2023)</ref>, so we are unable to perform this verification. Instead, we verify for each contemporary artist A ∈ A that SD 2.1 is to High and Render Quality to Slowest, to obtain the strongest protections. Appendix E includes qualitative results on an updated version released after we concluded our user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3 Robust Mimicry Methods Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3.1 Gaussian noising</head><p>We manually tune the Gaussian noising strength to σ 2 = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3.2 DiffPure</head><p>We use conditional DiffPure with the best-performing publicly available image generation diffusion model, Stable Diffusion XL 1.0 (SDXL) <ref type="bibr" target="#b27">(Podell et al., 2023)</ref>. We implement conditional DiffPure using the HuggingFace AutoPipelineForImage2Image pipeline. We use classifier-free guidance scale guidance scale = 7.5 with prompt P = C x for image x. We manually tune the number of diffusion timesteps t via the strength pipeline argument to strength = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3.3 IMPRESS++</head><p>Reverse Optimization Like Mist ϕ , we set the PGD perturbation budget to p = 8/255 and the PGD step size to α = 1/255. We manually tune the number of PGD iterations to N PGD = 400.</p><p>Noisy Upscaling We manually tune the Gaussian noising strength to σ = 0.1. We then use the Stable Diffusion Upscaler<ref type="foot" target="#foot_8">foot_8</ref> with the maximum denoising strength L.<ref type="foot" target="#foot_9">foot_9</ref> . We note that the Stable Diffusion Upscaler is trained on diffused images of the form</p><formula xml:id="formula_9">x α = √ α • x + √ 1 -α • N (0, I).</formula><p>In contrast, noisy upscaling noises images additively, that is, without the factor √ α. However, we note that for √ 1 -α = σ = 0.1, we have √ α = 0.995 ≈ 1. In practice, we observe no qualitative difference in the generated images.</p><p>Negative Prompting We manually tune the negative prompting strength to c = 0.5. We use the Stable Diffusion web UI<ref type="foot" target="#foot_10">foot_10</ref> to apply Textual Inversion on the adversarial images X prot . We follow the Textual Inversion setup used by Liang et al. to evaluate Mist and set the length of the token vector t to n = 8, the embedding initialization text to "style *", the learning rate to γ = 0.005, the batch size to 1, and the number of training steps to 500.</p><p>DiffPure post To make IMPRESS++ work under a single-model availability, we apply DiffPure post with the same model that we use for image generation, SD 2.1. We implement DiffPure post using the HuggingFace AutoPipelineForImage2Image pipeline. We use the classifier-free guidance scale guidance scale = 7.5 with prompt P = C x + ", artistic" for x. We manually tune the number of diffusion timesteps t via the strength pipeline argument to the value strength = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K User Study</head><p>This user study was approved by our institution's IRB.</p><p>Design. Our user study asks annotators to compare outputs from one robust mimicry method against a baseline where images are generated from a model trained on the original art without protections-for a fixed set of prompts P.</p><p>We present participants with both generations and a gallery with original art in the target style. We ask participants to decide which image is better in terms of style and quality, separately. For this, we ask them two different questions:</p><p>1. Based on noise, artifacts, detail, prompt fit, and your impression, which image has higher quality?</p><p>2. Overall, ignoring quality, which image better fits the style of the style samples?</p><p>For each comparison, we collect data from 5 users. We randomize several aspects of our study to minimize user bias. We randomly select the order of robust mimicry and baseline generations.</p><p>Second, we randomly shuffle the order of all image comparisons to prevent all images from the same mimicry method to appear consecutively. Finally, we also randomly sample the seeds that models use to generate images to prevent repeating the same baseline image across different comparisons.</p><p>Differences with Glaze's user study. Our study does not exactly replicate the design of Glaze's user study for two reasons. First, the Glaze study provided annotators with four AI-generated images and four original images, asking if the generated images successfully mimicked the original artwork. This evaluation fails to account for the commonly encountered scenario where current models are incapable of reliably mimicking an artist's style even from unprotected art. Second, we believe the relative assessment recorded in our study ("Which of these two mimicry attempts is more successful?") is easier for humans than the absolute assessment used in the Glaze study ("Is this mimicry attempt successful").</p><p>Prompts. We curate a small dataset of 10 prompts P. We design the prompts to satisfy two criteria:</p><p>1. The prompts should cover diverse motifs with varying complexity. This ensures that we can detect if a scenario compromised the prompt-following capabilities of a style mimicry model.</p><p>2. The prompts should only include prompts for which our finetuning base model M, SD 2.1, can successfully generate a matching image. This reduces the impact of potential human bias against common defects of SD 2.1.</p><p>To satisfy criterion 1 and increase variety, we instruct ChatGPT to generate prompt suggestions for four different categories:</p><p>1. Simple prompts with template "a {subject}".</p><p>2. Two-entity prompts with template "a {subject} {ditransitive verb} a {object}".</p><p>3. Entity-attribute prompts with template "a {adjective} {subject}".</p><p>4. Entity-scene prompts with template "a {subject} in a {scene}".</p><p>The chat we used to generate our prompts can be accessed at <ref type="url" target="https://chatgpt.com/share/ea3d1290-f137-4131-baca-2fa1c92b3859">https://chatgpt.com/share/ ea3d1290-f137-4131-baca-2fa1c92b3859</ref>. To satisfy criterion 2, we generate images with SD 2.1 on prompts suggested by ChatGPT and manually filter out prompts with defect generations (e.g. a horse with 6 legs). We populate the final set of prompts P with 4 simple prompts, 2 two-entity prompts, 2 entity-attribute prompts, and 2 entity-scene prompts (see Figure <ref type="figure" target="#fig_6">29</ref>). Figure 29: Our set of prompts. We manually wrote the prompts "a astronaut riding a horse" and "a village in a thunderstorm". ChatGPT wrote the remaining prompts.</p><p>Quality control. We first run a pilot study where we directly ask users to answer the previous questions about style and quality. This study resulted in very low-quality responses that are barely better than random choice. We enhanced the study to introduce several quality control measures to improve response quality and filter out low-quality annotations:</p><p>1. We limit our study to desktop users so that images are sufficiently large to perceive artifacts introduced by protections.</p><p>2. We precede the questions we use for our study with four dummy questions about the noise, artifacts, detail, and prompt matching of the images. The dummy questions force annotators to pay attention and gather information useful to answer the target questions.</p><p>3. We precede our study with a training session that shows for question 1, 2, and each of the four dummy questions an image pair with a clear, objective answer. The training session helps users to understand the study questions. We introduced this stage after gathering valuable feedback for annotators.</p><p>4. We add control comparisons to detect annotators who did not understand the tasks or were answering randomly. We generated several images from the baseline model trained on the original art. For each of these images, we created two ablations. For question 1 (quality), we include Gaussian noise to degrade its quality but preserve the same information. For question 2 (style), we apply Img2Img to remove the artist style and map the image back to photorealism using the prompt "high quality photo, award winning". We randomly include control comparisons between the original generations and these ablations, and we only accept labels from users who answered correctly at least 80% of the control questions.</p><p>Execution. We execute our study on Amazon Mechanical Turk (MTurk). We design and evaluate an MTurk Human Intelligence Task (HIT) for each artist A ∈ A, shown in Figure <ref type="figure" target="#fig_2">30</ref>. Each HIT includes image pair comparisons for a single artist A under all scenarios S ∈ M, as well 10 quality control image pairs, 10 style control image pairs, and 6 training image pairs. We generate an image pair for each of the 10 prompts and each of 15 scenarios, for a total of 10 • 15 + 10 + 10 + 6 = 176 image pairs per HIT. We estimate study participants to spend 5 minutes on the training image pairs and 30 seconds per remaining image pair, so 90 minutes in total. We compensate study participants at a rate of $16/hour, so $24 per HIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1 Style Mimicry Setup Validation</head><p>We execute an additional user study to validate that our style mimicry setup in Appendix G successfully mimics style from unprotected images. For each prompt P ∈ P and artist A ∈ A, our validation study uses the baseline model trained on uprotected art to generate one image. Inspired by the evaluation by Glaze <ref type="bibr">(Shan et al., 2023a)</ref>, we ask participants to evaluate the style mimicry success by answering the question:</p><p>How successfully does the style of the image mimic the style of the style samples? Ignore the content and only focus on the style.</p><p>To answer this question, we show a participant the image x O A and the images X A that serve as style samples. The participant can answer the question on a 5-point Likert scale with options 1. Not successful at all 2. Not very successful 3. Somewhat successful 4. Successful 5. Very successful We also execute the style mimicry validation study on MTurk. We design and evaluate a single HIT for all questions, shown in Figure <ref type="figure" target="#fig_20">33</ref>. We estimate study participants to spend 15 seconds on each question, and to spend 1 minute to familiarize themselves with a new style, so 35 minutes in total. We compensate study participants at a rate of $18/hour, so $10.50 per HIT.</p><p>We find that style mimicry is successful in over 70% of the comparisons. Results are detailed in Figure <ref type="figure" target="#fig_2">31</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Artists are vulnerable to style mimicry from generative models finetuned on their art. Existing protection tools add small perturbations to published artwork to prevent mimicry(Shan et al.,  2023a; Liang et al., 2023;<ref type="bibr" target="#b47">Van Le et al., 2023)</ref>. However, these protections fail against robust mimicry methods, giving a false sense of security and leaving artists vulnerable. Artwork by @nulevoy (Stas Voloshin), reproduced with permission.</figDesc><graphic coords="2,399.85,239.02,73.37,73.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure2: The protections of Glaze(Shan et al., 2023a)  do not generalize across fine-tuning setups. We mimic the style of the contemporary artist @nulevoy from Glaze-protected images by using: (b) the finetuning script provided by Glaze authors; and (c) an alternative off-the-shelf finetuning script from HuggingFace. In both cases, we perform "naive" style mimicry with no effort to bypass Glaze's protections. Glaze protections are successful using finetuning from the original paper, but significantly degrade with our script. Our finetuning is also better for unprotected images (see Appendix D).</figDesc><graphic coords="4,189.18,72.00,154.44,75.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Examples of robust style mimicry for two different artists: @greg-f (contemporary) and Edvard Munch (historical). Cherry-picked examples with strong protections and successful robust mimicry. We apply Noisy Upscaling for prompts: "a shoe" and "an astronaut riding a horse".</figDesc><graphic coords="6,119.34,171.47,136.55,68.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Randomly selected comparisons where all 5 annotators preferred mimicry from unprotected art over robust mimicry. Both use Noisy Upscaling for robust mimicry.</figDesc><graphic coords="9,266.19,166.23,64.50,64.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 4 samples from the original artwork from @nulevoy.</figDesc><graphic coords="15,108.00,173.73,95.04,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Artwork in Figure 6 after applying different protections.</figDesc><graphic coords="15,108.00,539.00,95.04,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Generations in the style of @nulevoy after finetuning on unprotected images. Each generation is sampled with a different seed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Comparison of perturbations by Glaze v1.1.1 and v2.0 on artwork from @nulevoy.</figDesc><graphic coords="28,207.10,467.58,94.53,94.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Comparison of robust style mimicry (Noisy Upscaling) on artwork from @nulevoy protected with both versions of Glaze. Images in Figure 6 serve as a reference for the artistic style.</figDesc><graphic coords="28,108.00,467.58,94.53,94.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Original artwork from @nulevoy and the resulting images after applying Noisy Upscaling to artwork protected with Glaze v2.0. See protected images in Figure 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Comparison of perturbations introduced by Mist v1 and v2 on artwork from @nulevoy.</figDesc><graphic coords="29,207.10,467.12,94.53,94.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>Figure 24: Comparison of robust style mimicry (Noisy Upscaling) on artwork from @nulevoy protected with both versions of Mist. Images in Figure 6 serve as a reference for the artistic style.</figDesc><graphic coords="29,108.00,467.12,94.53,94.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Illustration of Noisy Upscaling on a random image from @nulevoy. Unlike naive upscaling and Compressed Upscaling, Noisy Upscaling removes protections while preserving the details in the original artwork.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><figDesc>Figure 27 illustrates the improvements introduced by each additional step.</figDesc><graphic coords="34,108.00,192.82,118.79,118.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: The Mist target image Target Target Mist is the default target image in the reference Mist implementation and one of the successful target images evaluated by Liang &amp; Wu.</figDesc><graphic coords="36,246.60,72.00,118.80,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 30 :Figure 31 :</head><label>3031</label><figDesc>Figure 30: The interface of our user study.</figDesc><graphic coords="39,108.00,72.00,395.99,223.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 32 :</head><label>32</label><figDesc>Figure 32: User ratings of clean style mimicry success. Each bar indicates the percentage of votes for the corresponding success level over all clean style mimicry generations for the corresponding artist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 33 :</head><label>33</label><figDesc>Figure 33: The interface of our style mimicry setup validation study.</figDesc><graphic coords="40,108.00,472.32,395.97,169.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Success rates averaged across artists for all style mimicry scenarios. Higher percentages indicate more successful mimicry, and 50% would indicate perfect mimicry.</figDesc><table><row><cell>Method</cell><cell cols="6">Naive mimicry Gaussian noising IMPRESS++ DiffPure Noisy Upscaling Best-of-4</cell></row><row><cell>Protection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Anti-DB</cell><cell>11.6%</cell><cell>20.6%</cell><cell>32.2%</cell><cell>26.6%</cell><cell>45.0%</cell><cell>56.6%</cell></row><row><cell>Glaze</cell><cell>22.2%</cell><cell>29.6%</cell><cell>35.4%</cell><cell>32.0%</cell><cell>39.4%</cell><cell>56.6%</cell></row><row><cell>Mist</cell><cell>9.0%</cell><cell>21.0%</cell><cell>37.4%</cell><cell>35.8%</cell><cell>42.8%</cell><cell>62.0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) Quality</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Naive mimicry Gaussian noising IMPRESS++ DiffPure Noisy Upscaling Best-of-4</cell></row><row><cell>Protection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Anti-DB</cell><cell>21.8%</cell><cell>31.2%</cell><cell>28.6%</cell><cell>31.0%</cell><cell>44.0%</cell><cell>52.4%</cell></row><row><cell>Glaze</cell><cell>30.8%</cell><cell>35.4%</cell><cell>27.8%</cell><cell>37.6%</cell><cell>41.6%</cell><cell>51.2%</cell></row><row><cell>Mist</cell><cell>19.4%</cell><cell>35.4%</cell><cell>31.6%</cell><cell>37.4%</cell><cell>44.2%</cell><cell>53.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b) Style</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Success rates per artist for style and quality questions, respectively.</figDesc><table><row><cell>Each line plot shows,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>User preference ratings of all style mimicry scenarios S ∈ M for each artist A ∈ A by name. Each cell states the percentage of votes that prefer an image generated under the corresponding scenario S and artist A ∈ A over a matching image generated under clean style mimicry. Higher percentages indicate weaker attacks or better defenses.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="6">Naive mimicry Gaussian noising IMPRESS++ DiffPure Noisy Upscaling Best-of-4</cell></row><row><cell cols="2">Protection Artist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Anti-DB</cell><cell>A 1</cell><cell>4%</cell><cell>6%</cell><cell>8%</cell><cell>18%</cell><cell>26%</cell><cell>30%</cell></row><row><cell></cell><cell>A 2</cell><cell>14%</cell><cell>48%</cell><cell>54%</cell><cell>32%</cell><cell>50%</cell><cell>62%</cell></row><row><cell></cell><cell>A 3</cell><cell>10%</cell><cell>8%</cell><cell>18%</cell><cell>16%</cell><cell>40%</cell><cell>46%</cell></row><row><cell></cell><cell>A 4</cell><cell>14%</cell><cell>22%</cell><cell>20%</cell><cell>14%</cell><cell>54%</cell><cell>70%</cell></row><row><cell></cell><cell>A 5</cell><cell>16%</cell><cell>16%</cell><cell>22%</cell><cell>24%</cell><cell>54%</cell><cell>60%</cell></row><row><cell></cell><cell>Albrecht Durer</cell><cell>2%</cell><cell>22%</cell><cell>32%</cell><cell>26%</cell><cell>42%</cell><cell>70%</cell></row><row><cell></cell><cell>Alphonse Mucha</cell><cell>16%</cell><cell>22%</cell><cell>44%</cell><cell>42%</cell><cell>60%</cell><cell>66%</cell></row><row><cell></cell><cell>Anna O.-Lebedeva</cell><cell>38%</cell><cell>40%</cell><cell>56%</cell><cell>40%</cell><cell>44%</cell><cell>76%</cell></row><row><cell></cell><cell>Edvard Munch</cell><cell>2%</cell><cell>14%</cell><cell>40%</cell><cell>40%</cell><cell>46%</cell><cell>56%</cell></row><row><cell></cell><cell>Edward Hopper</cell><cell>0%</cell><cell>8%</cell><cell>28%</cell><cell>14%</cell><cell>34%</cell><cell>30%</cell></row><row><cell>Glaze</cell><cell>A 1</cell><cell>8%</cell><cell>20%</cell><cell>22%</cell><cell>10%</cell><cell>12%</cell><cell>24%</cell></row><row><cell></cell><cell>A 2</cell><cell>12%</cell><cell>42%</cell><cell>40%</cell><cell>28%</cell><cell>44%</cell><cell>60%</cell></row><row><cell></cell><cell>A 3</cell><cell>12%</cell><cell>26%</cell><cell>18%</cell><cell>26%</cell><cell>34%</cell><cell>52%</cell></row><row><cell></cell><cell>A 4</cell><cell>22%</cell><cell>20%</cell><cell>20%</cell><cell>54%</cell><cell>54%</cell><cell>60%</cell></row><row><cell></cell><cell>A 5</cell><cell>18%</cell><cell>34%</cell><cell>34%</cell><cell>24%</cell><cell>40%</cell><cell>52%</cell></row><row><cell></cell><cell>Albrecht Durer</cell><cell>2%</cell><cell>16%</cell><cell>40%</cell><cell>28%</cell><cell>26%</cell><cell>54%</cell></row><row><cell></cell><cell>Alphonse Mucha</cell><cell>40%</cell><cell>44%</cell><cell>58%</cell><cell>42%</cell><cell>56%</cell><cell>66%</cell></row><row><cell></cell><cell>Anna O.-Lebedeva</cell><cell>42%</cell><cell>46%</cell><cell>54%</cell><cell>44%</cell><cell>34%</cell><cell>70%</cell></row><row><cell></cell><cell>Edvard Munch</cell><cell>40%</cell><cell>16%</cell><cell>42%</cell><cell>42%</cell><cell>38%</cell><cell>62%</cell></row><row><cell></cell><cell>Edward Hopper</cell><cell>26%</cell><cell>32%</cell><cell>26%</cell><cell>22%</cell><cell>56%</cell><cell>66%</cell></row><row><cell>Mist</cell><cell>A 1</cell><cell>0%</cell><cell>6%</cell><cell>20%</cell><cell>4%</cell><cell>12%</cell><cell>28%</cell></row><row><cell></cell><cell>A 2</cell><cell>14%</cell><cell>50%</cell><cell>50%</cell><cell>46%</cell><cell>48%</cell><cell>76%</cell></row><row><cell></cell><cell>A 3</cell><cell>0%</cell><cell>10%</cell><cell>22%</cell><cell>24%</cell><cell>60%</cell><cell>60%</cell></row><row><cell></cell><cell>A 4</cell><cell>0%</cell><cell>16%</cell><cell>24%</cell><cell>36%</cell><cell>66%</cell><cell>70%</cell></row><row><cell></cell><cell>A 5</cell><cell>12%</cell><cell>22%</cell><cell>40%</cell><cell>28%</cell><cell>50%</cell><cell>54%</cell></row><row><cell></cell><cell>Albrecht Durer</cell><cell>10%</cell><cell>24%</cell><cell>28%</cell><cell>46%</cell><cell>38%</cell><cell>60%</cell></row><row><cell></cell><cell>Alphonse Mucha</cell><cell>32%</cell><cell>18%</cell><cell>60%</cell><cell>56%</cell><cell>54%</cell><cell>66%</cell></row><row><cell></cell><cell>Anna O.-Lebedeva</cell><cell>20%</cell><cell>38%</cell><cell>54%</cell><cell>50%</cell><cell>34%</cell><cell>74%</cell></row><row><cell></cell><cell>Edvard Munch</cell><cell>2%</cell><cell>22%</cell><cell>54%</cell><cell>44%</cell><cell>28%</cell><cell>72%</cell></row><row><cell></cell><cell>Edward Hopper</cell><cell>0%</cell><cell>4%</cell><cell>22%</cell><cell>24%</cell><cell>38%</cell><cell>60%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) Quality</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="6">Naive mimicry Gaussian noising IMPRESS++ DiffPure Noisy Upscaling Best-of-4</cell></row><row><cell cols="2">Protection Artist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Anti-DB</cell><cell>A 1</cell><cell>0%</cell><cell>4%</cell><cell>4%</cell><cell>10%</cell><cell>34%</cell><cell>36%</cell></row><row><cell></cell><cell>A 2</cell><cell>14%</cell><cell>20%</cell><cell>40%</cell><cell>16%</cell><cell>48%</cell><cell>54%</cell></row><row><cell></cell><cell>A 3</cell><cell>10%</cell><cell>14%</cell><cell>26%</cell><cell>28%</cell><cell>42%</cell><cell>46%</cell></row><row><cell></cell><cell>A 4</cell><cell>36%</cell><cell>58%</cell><cell>42%</cell><cell>56%</cell><cell>54%</cell><cell>56%</cell></row><row><cell></cell><cell>A 5</cell><cell>4%</cell><cell>0%</cell><cell>10%</cell><cell>32%</cell><cell>60%</cell><cell>66%</cell></row><row><cell></cell><cell>Albrecht Durer</cell><cell>20%</cell><cell>32%</cell><cell>36%</cell><cell>28%</cell><cell>44%</cell><cell>50%</cell></row><row><cell></cell><cell>Alphonse Mucha</cell><cell>56%</cell><cell>56%</cell><cell>42%</cell><cell>52%</cell><cell>48%</cell><cell>58%</cell></row><row><cell></cell><cell>Anna O.-Lebedeva</cell><cell>32%</cell><cell>50%</cell><cell>24%</cell><cell>30%</cell><cell>28%</cell><cell>56%</cell></row><row><cell></cell><cell>Edvard Munch</cell><cell>6%</cell><cell>30%</cell><cell>26%</cell><cell>20%</cell><cell>46%</cell><cell>50%</cell></row><row><cell></cell><cell>Edward Hopper</cell><cell>40%</cell><cell>48%</cell><cell>36%</cell><cell>38%</cell><cell>36%</cell><cell>52%</cell></row><row><cell>Glaze</cell><cell>A 1</cell><cell>8%</cell><cell>14%</cell><cell>8%</cell><cell>14%</cell><cell>30%</cell><cell>34%</cell></row><row><cell></cell><cell>A 2</cell><cell>36%</cell><cell>42%</cell><cell>26%</cell><cell>46%</cell><cell>44%</cell><cell>52%</cell></row><row><cell></cell><cell>A 3</cell><cell>24%</cell><cell>24%</cell><cell>16%</cell><cell>40%</cell><cell>32%</cell><cell>50%</cell></row><row><cell></cell><cell>A 4</cell><cell>56%</cell><cell>58%</cell><cell>32%</cell><cell>44%</cell><cell>58%</cell><cell>66%</cell></row><row><cell></cell><cell>A 5</cell><cell>12%</cell><cell>18%</cell><cell>18%</cell><cell>30%</cell><cell>32%</cell><cell>40%</cell></row><row><cell></cell><cell>Albrecht Durer</cell><cell>22%</cell><cell>28%</cell><cell>26%</cell><cell>26%</cell><cell>38%</cell><cell>38%</cell></row><row><cell></cell><cell>Alphonse Mucha</cell><cell>48%</cell><cell>54%</cell><cell>36%</cell><cell>54%</cell><cell>52%</cell><cell>56%</cell></row><row><cell></cell><cell>Anna O.-Lebedeva</cell><cell>26%</cell><cell>32%</cell><cell>40%</cell><cell>38%</cell><cell>44%</cell><cell>68%</cell></row><row><cell></cell><cell>Edvard Munch</cell><cell>38%</cell><cell>32%</cell><cell>36%</cell><cell>40%</cell><cell>48%</cell><cell>56%</cell></row><row><cell></cell><cell>Edward Hopper</cell><cell>38%</cell><cell>52%</cell><cell>40%</cell><cell>44%</cell><cell>38%</cell><cell>52%</cell></row><row><cell>Mist</cell><cell>A 1</cell><cell>0%</cell><cell>6%</cell><cell>4%</cell><cell>0%</cell><cell>22%</cell><cell>18%</cell></row><row><cell></cell><cell>A 2</cell><cell>6%</cell><cell>38%</cell><cell>44%</cell><cell>42%</cell><cell>64%</cell><cell>72%</cell></row><row><cell></cell><cell>A 3</cell><cell>6%</cell><cell>28%</cell><cell>26%</cell><cell>36%</cell><cell>34%</cell><cell>44%</cell></row><row><cell></cell><cell>A 4</cell><cell>36%</cell><cell>58%</cell><cell>46%</cell><cell>52%</cell><cell>48%</cell><cell>54%</cell></row><row><cell></cell><cell>A 5</cell><cell>4%</cell><cell>14%</cell><cell>18%</cell><cell>26%</cell><cell>58%</cell><cell>56%</cell></row><row><cell></cell><cell>Albrecht Durer</cell><cell>28%</cell><cell>32%</cell><cell>24%</cell><cell>36%</cell><cell>50%</cell><cell>60%</cell></row><row><cell></cell><cell>Alphonse Mucha</cell><cell>34%</cell><cell>50%</cell><cell>34%</cell><cell>50%</cell><cell>48%</cell><cell>64%</cell></row><row><cell></cell><cell>Anna O.-Lebedeva</cell><cell>32%</cell><cell>48%</cell><cell>44%</cell><cell>56%</cell><cell>38%</cell><cell>64%</cell></row><row><cell></cell><cell>Edvard Munch</cell><cell>10%</cell><cell>38%</cell><cell>36%</cell><cell>40%</cell><cell>42%</cell><cell>64%</cell></row><row><cell></cell><cell>Edward Hopper</cell><cell>38%</cell><cell>42%</cell><cell>40%</cell><cell>36%</cell><cell>38%</cell><cell>38%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b) Style</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The two finetuning scripts mainly differ in the choice of library, model, and hyperparameters. We use a standard HuggingFace script and Stable Diffusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2.1 (the model evaluated in the Glaze paper).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Contemporary Artists were selected from Artstation. We keep them anonymous throughout this work-and refrain from showcasing their art-except for artists who gave us explicit permission to share their identity and art. We will share all images used in our experiments upon request with researchers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The user study was approved by our institution's IRB.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>The original evaluation in Glaze directly asks annotators whether a mimicry is successful or not, rather than a binary comparison between a robust mimicry and a baseline mimicry as in our setup.Shan et al. (2023a)  report that mimicry fails in 4% of cases for unprotected images, and succeeds in 6% of cases for protected images. This bounds the success rate for robust mimicry-according to our definition in Equation (1)-by at most 10%.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Both models share the same encoder for which protections are optimized.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Mist project also contains a denoiser attack that we fail to reproduce as a robust protection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>www.artstation.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>www.huggingface.co/stabilityai/stable-diffusion-x4-upscaler</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>We inadvertently set the denoising strength to L = 320 instead of the actual maximum denoising strength L = 350. We observe no qualitative difference in the generated images.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>https://github.com/AUTOMATIC1111/stable-diffusion-webui</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank all the MTurkers that engaged with our tasks, especially those that provided valuable feedback during our preliminary studies to improve the survey. We thank the contemporary artists <rs type="person">Stas Voloshin</rs> (@nulevoy) and <rs type="person">Gregory Fromenteau</rs> (@greg-f) for allowing us to display their artwork in this paper. JR is supported by an <rs type="funder">ETH</rs> <rs type="grantName">AI Center doctoral fellowship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6BNgGuA">
					<orgName type="grant-name">AI Center doctoral fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anti-DB + Gaussian noising Unprotected</head><p>Glaze + Gaussian noising Unprotected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mist + Gaussian noising Unprotected</head><p>Edvard Munch, "a shoe"</p><p>Edvard Munch, "a piano"</p><p>A 5 , "a feathered car"</p><p>Anna O.-Lebedeva, "a piano"</p><p>A 5 , "a village in a thunderstorm" Edward Hopper, "a golden apple"</p><p>Edward Hopper, "a feathered car"</p><p>Figure <ref type="figure">12</ref>: Style mimicry for all protections using Gaussian Noising. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anti-DB + Gaussian noising Unprotected</head><p>Glaze + Gaussian noising Unprotected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mist + Gaussian noising Unprotected</head><p>Edvard Munch, "a shoe"</p><p>Edvard Munch, "a piano"</p><p>A 5 , "a feathered car"</p><p>Anna O.-Lebedeva, "a piano"</p><p>A 5 , "a village in a thunderstorm" Edward Hopper, "a golden apple"</p><p>Edward Hopper, "a feathered car"</p><p>Figure <ref type="figure">13</ref>: Style mimicry for all protections using DiffPure. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anti-DB + Gaussian noising Unprotected</head><p>Glaze + Gaussian noising Unprotected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mist + Gaussian noising Unprotected</head><p>Edvard Munch, "a shoe" Edvard Munch, "a piano"</p><p>A 5 , "a feathered car"</p><p>Anna O.-Lebedeva, "a piano"</p><p>A 5 , "a village in a thunderstorm" Edward Hopper, "a golden apple"</p><p>Edward Hopper, "a feathered car"</p><p>Figure <ref type="figure">14</ref>: Style mimicry for all protections using IMPRESS++. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anti-DB + Gaussian noising Unprotected</head><p>Glaze + Gaussian noising Unprotected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mist + Gaussian noising Unprotected</head><p>Edvard Munch, "a shoe" Edvard Munch, "a piano"</p><p>A 5 , "a feathered car"</p><p>Anna O.-Lebedeva, "a piano"</p><p>A 5 , "a village in a thunderstorm" Edward Hopper, "a golden apple"</p><p>Edward Hopper, "a feathered car"</p><p>Figure <ref type="figure">15</ref>: Style mimicry for all protections using Noisy Upscaling. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Mimicry Quality Versus Style</head><p>This section includes the detailed results from our user study. As mentioned in Section 5, we ask users to assess quality and stylistic fit separately in our study. Figure <ref type="figure">16</ref> and 17 show the results for each of these evaluations separately (the results in the main body represent the average of the two). Finally, Table <ref type="table">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Methods for Style Mimicry</head><p>This section summarizes the existing methods that a style forger can use to perform style mimicry.</p><p>Our work only considers finetuning since it is reported to be the most effective <ref type="bibr">(Shan et al., 2023a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Prompting</head><p>Well-known artistic styles contained in the training data (e.g. Van Gogh) can be mimicked by prompting a text-to-image model with a description of the style or the name of the artist. For example, a prompt can be augmented with " painted in a cubistic style" " painted by van Gogh" to mimic those styles, respectively. Prompting is easy to apply and does not require changes to the model. However, it fails to mimic styles that are not sufficiently represented in the training data of model-often from the most vulnerable artists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Img2Img</head><p>Img2Img creates an updated version of an image with guidance from a prompt. For this, Img2Img processes image x with t timesteps of a diffusion process to obtain the diffused image x t . Then, Img2Img uses the model with guidance from prompt P to reverse the diffusion process into the output image variation x P . Analogous to prompting, a prompt suffices to transfer a well-known style, but Img2Img also fails for unknown styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Textual Inversion</head><p>Textual inversion <ref type="bibr" target="#b6">(Gal et al., 2022)</ref> optimizes the embedding of some n new tokens t = [t 1 , . . . , t n ] that are appended to image prompts P so that generations closely mimic the style of a given set of images. The tokens are optimized via gradient descent on the model training loss so that P + t generates images that mimic the target style. Textual inversion requires white-box access to the target model, but enables the mimicry of unknown styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Finetuning</head><p>Finetuning updates the weights of a pretrained text-to-image model to introduce a new functionality.</p><p>In this case, finetuning allows a forger to "teach" the generative model an unknown style using a set of images in the target style and their captions (e.g. an astronaut riding a horse). First, all captions are augmented with some special word, like the name of the artist, to create prompts P x = C x + "by w * ". Then, the model weights are updated to minimize the reconstruction loss of the given images following the augmented prompts. At inference time, the forger can append "by w * " to any prompt to obtain art in the target style</p><p>The authors of Glaze identify this finetuning setup as the strongest style mimicry method <ref type="bibr">(Shan et al., 2023a)</ref>. We validate the success of our style mimicry with a user study detailed in Appendix K.1 unable to mimic the style S A manually inspecting SD 2.1 generations for prompts of the form "An {object} by {artist}". We center-crop each image x to 512 × 512 pixels and generate a caption C x for x with the BLIP-2 model <ref type="bibr" target="#b18">(Li et al., 2023)</ref>.</p><p>Finetuning hyperparameters. Glaze does not specify which finetuning script they use, but they claim to "follow the same training parameters as <ref type="bibr" target="#b32">(Rombach et al., 2022)</ref>. We use 5 • 10 -6 learning rate and batch size of 32." This batch size misfits their small finetuning image sets that contain no more than 34 images. Moreover, the finetuning code that Shan et al. kindly sent us upon request uses DreamBooth finetuning with Stable Diffusion 1.5, instead of version 2.1 as described in their work.</p><p>In light of these discrepancies, and assuming that mimicry protections should be agnostic to the finetuning setup used, we use an "off-the-shelf" HuggingFace finetuning script for Stable Diffusion <ref type="bibr" target="#b48">(von Platen et al., 2024)</ref> and manually tune hyperparameters for optimal style mimicry before protections are applied. Concretely, we use 2,000 training steps, batch size 4, learning rate 5 • 10 -6 , and set the remaining hyperparameters to their default values. We pair each image x with the prompt P x = C x +" by w * ", where w * = "nulevoy" 9 .</p><p>Generation hyperparameters We use the DPM-Solver++(2M) Karras <ref type="bibr" target="#b22">(Lu et al., 2022;</ref><ref type="bibr" target="#b14">Karras et al., 2022)</ref> scheduler for 50 steps to generate images of size 768 × 768. This scheduler generates images with slightly higher quality than the PNDM <ref type="bibr" target="#b21">(Liu et al., 2021)</ref> scheduler used by Glaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Protections Experimental Details</head><p>We evaluate three different protections: Mist <ref type="bibr">(Liang et al., 2023)</ref>, Glaze <ref type="bibr">(Shan et al., 2023a)</ref>, and Anti-DreamBooth <ref type="bibr" target="#b47">(Van Le et al., 2023)</ref>. For a fair comparison, we fix the perturbation budget p for each adversarial perturbation δ x created by Mist and Anti-DreamBooth to p = 8/255, which is the same budget that Liang et al. use to evaluate Mist. It is not possible to evaluate Glaze with exactly this perturbation budget, for three reasons: First, Glaze uses LPIPS for the image similarity measure d Img , which does not bound the L ∞ norm. Second, Glaze implements the metric d Img as a soft bound in Objective (3), which offers no hard bound guarantees. Third, Glaze is closed-source software whose perturbation budget control only offers the settings Default, Medium, and High. Upon request, the Glaze authors refused to share a codebase where we could control the hyperparameters. Therefore, we evaluate Glaze through their official public tool with the setting High to evaluate our protections under the highest protections. In our evaluation, we perceive images processed with Glaze to be equally or less perturbed than images processed with Mist and Anti-DreamBooth.</p><p>Next, we describe specific hyperparameters we use to reproduce each of the protections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2.1 Anti-DreamBooth</head><p>Van Le et al. implement Anti-DreamBooth against DreamBooth finetuning. We adapt their implementation to our vanilla finetuning for style mimicry, using the same hyperparameters where possible:</p><p>We set the number of iterations to N = 50, the PGD perturbation budget to p = 8/255, the PGD step size to α = 5 • 10 -3 , and the number of PGD steps per ASPL iteration to N PGD = 6. We minimize the loss L Finetune with the vanilla finetuning setup in Appendix J.1 for 300 training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2.2 Mist ϕ</head><p>We replicate the evaluation that Liang &amp; Wu use to evaluate Mist ϕ against Stable Diffusion. We set the PGD perturbation budget to p = 8/255, the number of PGD iterations to N PGD = 100, the PGD step size to α = 1/255, and the target image to T = Target Mist shown in Figure <ref type="figure">28</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2.3 Glaze</head><p>The Glaze authors were unable to share a codebase upon request. We thus use their publicly released Windows application binary. We use the latest available version of Glaze, v1.1.1. We set Intensity 9 @nulevoy is the first ArtStation artist that we experimented with. In our experiments, we found "nulevoy" a suitable choice for the special word w * and use it for all artists. We check that all of nulevoy's images are published after the release date of LAION-5B to ensure that SD 2.1 has no prior knowledge about nulevoy's style. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Compute Resources</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving image generation with better captions</title>
		<author>
			<persName><forename type="first">James</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/papers/dall-e-3.pdf" />
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Impress: Evaluating the resilience of imperceptible perturbations against unauthorized data usage in diffusion-based generative ai</title>
		<author>
			<persName><forename type="first">Bochuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM workshop on artificial intelligence and security</title>
		<meeting>the 10th ACM workshop on artificial intelligence and security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lowkey: Leveraging adversarial attacks to protect social media users from facial recognition</title>
		<author>
			<persName><forename type="first">Valeriia</forename><surname>Cherepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07922</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Largest dataset powering ai images removed after discovery of child sexual abuse material. 404 Media</title>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Cole</surname></persName>
		</author>
		<ptr target="https://www.404media.co/laion-datasets-removed-stanford-csam-child-abuse/" />
		<imprint>
			<date type="published" when="2023-12">Dec 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial examples make strong poisons</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping-Yeh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30339" to="30351" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion</title>
		<author>
			<persName><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Poscuda: Position based convolution for unlearnable audio datasets</title>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02135</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">This artist is dominating ai-generated art. and he&apos;s not happy about it</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="9" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">This artist is dominating ai-generated art. and he&apos;s not happy about it</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology Review</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">This tool could protect artists from ai-generated art that steals their style</title>
		<author>
			<persName><forename type="first">Kashmir</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unlearnable examples: Making personal data unexploitable</title>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Monazam</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04898</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26565" to="26577" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The shape of and solutions to the mturk quality crisis</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Burleigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">D</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Jewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J G</forename><surname>Winter</surname></persName>
		</author>
		<idno type="DOI">10.1017/psrm.2020.6</idno>
	</analytic>
	<monogr>
		<title level="j">Political Science Research and Methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="614" to="629" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Your personal information is probably being used to train generative ai models</title>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Leffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19730" to="19742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mist: Towards improved adversarial examples for diffusion models</title>
		<author>
			<persName><forename type="first">Chumeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12683</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples</title>
		<author>
			<persName><forename type="first">Chumeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruhui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20763" to="20786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pseudo numerical methods for diffusion models on manifolds</title>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Miyake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Iohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiyuki</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16807</idno>
		<ptr target="https://github.com/muerrilla/stable-diffusion-NPW" />
	</analytic>
	<monogr>
		<title level="m">muerrilla. Negative prompt weight: Extension for stable diffusion web ui</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image superresolution as a defense against adversarial attacks</title>
		<author>
			<persName><forename type="first">Aamir</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1711" to="1724" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion models for adversarial purification</title>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16805" to="16827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Disrupting style mimicry attacks on video imagery</title>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Passananti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.06865</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Data poisoning won&apos;t save you from facial recognition</title>
		<author>
			<persName><forename type="first">Evani</forename><surname>Radiya-Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14851</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Razzhigaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arseniy</forename><surname>Shakhmatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Maltseva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Arkhipkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Ryabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Kuts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Dimitrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03502</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Raising the cost of malicious ai-powered image editing</title>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Khaddaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06588</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fawkes: Protecting privacy against unauthorized deep learning models</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th USENIX security symposium (USENIX Security 20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1589" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Protecting artists from style mimicry by {Text-to-Image} models</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Glaze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd USENIX Security Symposium (USENIX Security 23)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2187" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.07731</idno>
		<title level="m">A response to glaze purification via impress</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online adversarial purification based on selfsupervised learning</title>
		<author>
			<persName><forename type="first">Changhao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chester</forename><surname>Holtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Stability</surname></persName>
		</author>
		<ptr target="https://huggingface.co/stabilityai/stable-diffusion-2-1" />
		<title level="m">Stable diffusion 2.1</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2024" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved artgan for conditional synthesis of natural image and artwork</title>
		<author>
			<persName><forename type="first">Chee</forename><surname>Wei Ren Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernan</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoshi</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName><surname>Tanaka</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2866698</idno>
		<ptr target="https://doi.org/10.1109/TIP.2018.2866698" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="394" to="409" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Better safe than sorry: Preventing delusive adversaries with adversarial training</title>
		<author>
			<persName><forename type="first">Lue</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songcan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16209" to="16225" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">It gave us some way to fight back: New tools aim to protect art and images from ai&apos;s grasp</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Thorbecke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On adaptive attacks to adversarial example defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1633" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Antidreambooth: Protecting users from personalized text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Van Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuan</forename><surname>Hoang Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2116" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Diffusers: State-of-the-art diffusion models</title>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Cuenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mishig</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Davaadorj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayak</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/diffusers" />
		<imprint>
			<date type="published" when="2024-04">apr 2024</date>
		</imprint>
	</monogr>
	<note>If you use this software, please cite it using the metadata from this file</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Wright</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical optimization</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial purification with score-based generative models</title>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12062" to="12072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
