# Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI

## Abstract

## 

Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections-with millions of downloads-and show they only provide a false sense of security. We find that low-effort and "off-the-shelf" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that all existing protections can be easily bypassed, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.

Code and images released at: [https://github.com/ethz-spylab/robust-style-mimicry](https://github.com/ethz-spylab/robust-style-mimicry).

## Introduction

Style mimicry is a popular application of text-to-image generative models. Given a few images from an artist, a model can be finetuned to generate new images in that style (e.g., a spaceship in the style of Van Gogh). But style mimicry has the potential to cause significant harm if misused. In particular, many contemporary artists worry that others could now produce images that copy their unique art style, and potentially steal away customers [(Heikkilä, 2022)](#). As a response, several protections have been developed to protect artists from style mimicry [(Shan et al., 2023a;](#)[Van Le et al., 2023;](#b47)[Liang et al., 2023)](#). These protections add adversarial perturbations to images that artists publish online, in order to inhibit the finetuning process. These protections have received significant attention from the media-with features in the New York Times [(Hill, 2023)](#b10), CNN [(Thorbecke, 2023)](#b45) and Scientific American [(Leffer, 2023)](#b17)-and have been downloaded over 1M times [(Shan et al., 2023a](#)).

Yet, it is unclear to what extent these tools actually protect artists against style mimicry, especially if someone actively attempts to circumvent them [(Radiya-Dixit et al., 2021)](#b29). In this work, we show that state-of-the-art style protection tools-Glaze [(Shan et al., 2023a)](#), Mist [(Liang et al., 2023)](#) and [Anti-DreamBooth (Van Le et al., 2023)](#)-are ineffective when faced with simple robust mimicry methods. The robust mimicry methods we consider range from low-effort strategies-such as using a different finetuning script, or adding Gaussian noise to the images before training-to multi-step strategies that combine off-the-shelf tools. We validate our results with a user study, which reveals that robust mimicry methods can produce results indistinguishable in quality from those obtained from unprotected artworks (see Figure [1](#fig_0) for an illustrative example).

We show that existing protection tools merely provide a false sense of security. Our robust mimicry methods do not require the development of new tools or fine-tuneing methods, but only carefully Existing protection tools add small perturbations to published artwork to prevent mimicry [(Shan et al., 2023a;](#)[Liang et al., 2023;](#)[Van Le et al., 2023)](#b47). However, these protections fail against robust mimicry methods, giving a false sense of security and leaving artists vulnerable. Artwork by @nulevoy (Stas Voloshin), reproduced with permission. combining standard image processing techniques which already existed at the time that these protection tools were first introduced!. Therefore, we believe that even low-skilled forgers could have easily circumvented these tools since their inception.

Although we evaluate specific protection tools that exist today, the limitations of style mimicry protections are inherent. Artists are necessarily at a disadvantage since they have to act first (i.e., once someone downloads protected art, the protection can no longer be changed). To be effective, protective tools face the challenging task of creating perturbations that transfer to any finetuning technique, even ones chosen adaptively in the future. A similar conclusion was drawn by [Radiya-Dixit et al. (Radiya-Dixit et al., 2021)](#b29), who argued that adversarial perturbations cannot protect users from facial recognition systems. We thus caution that adversarial machine learning techniques will not be able to reliably protect artists from generative style mimicry, and urge the development of alternative measures to protect artists.

We disclosed our results to the affected protection tools prior to publication, so that they could determine the best course of action for existing users.

## Background and Related Work

Text-to-image diffusion models. A latent diffusion model consists of an image autoencoder and a denoiser. The autoencoder is trained to encode and decode images using a lower-dimensional latent space. The denoiser predicts the noise added to latent representations of images in a diffusion process [(Ho et al., 2020)](#b12). Latent diffusion models can generate images from text prompts by conditioning the denoiser on image captions [(Rombach et al., 2022)](#b32). Popular text-to-image diffusion models include open models such as Stable Diffusion [(Rombach et al., 2022)](#b32) and Kandinsky [(Razzhigaev et al., 2023)](#b31), as well as closed models like Imagen [(Saharia et al., 2022)](#b33) and DALL-E [(Ramesh et al.;](#b30)[Betker et al., 2023)](#b0).

Style mimicry. Style mimicry uses generative models to create images matching a target artistic style. Existing techniques vary in complexity and quality (see Appendix G). An effective method is to finetune a diffusion model using a few images in the targeted style. Some artists worry that style mimicry can be misused to reproduce their work without permission and steal away customers [(Heikkilä, 2022)](#).

Style mimicry protections. Several tools have been proposed to prevent unauthorized style mimicry. These tools allow artists to include small perturbations-optimized to disrupt style mimicry techniques-in their images before publishing. The most popular protections are Glaze [(Shan et al., 2023a)](#) and Mist [(Liang et al., 2023)](#). Additionally, Anti-DreamBooth [(Van Le et al., 2023)](#b47) was introduced to prevent fake personalized images, but we also find it effective for style mimicry. Both Glaze and Mist target the encoder in latent diffusion models; they perturb images to obtain latent representations that decode to images in a different style (see Appendix H.1). On the other hand, Anti-DreamBooth targets the denoiser and maximizes the prediction error on the latent representations of the perturbed images (see Appendix H.2).

Circumventing style mimicry protections. Although not initially designed for this purpose, adversarial purification [(Yoon et al., 2021;](#b50)[Shi et al., 2020;](#b40)[Samangouei et al., 2018)](#b35) could be used to remove the perturbations introduced by style mimicry protections. DiffPure [(Nie et al., 2022)](#b25) is the strongest purification method and Mist claims robustness against it. Another existing method for purification is upscaling [(Mustafa et al., 2019)](#b24). Similarly, Mist and Glaze claim robustness against upscaling. Section 4.1 highlights flaws in previous evaluations and how a careful application of both methods can effectively remove mimicry protections.

IMPRESS [(Cao et al., 2024)](#b1) was the first purification method designed specifically to circumvent style mimicry protections. While IMPRESS claims to circumvent Glaze, the authors of Glaze critique the method's evaluation [(Shan et al., 2023b)](#), namely the reliance on automated metrics instead of a user study, as well as the method's poor performance on contemporary artists. Our work addresses these limitations by considering simpler and stronger purification methods, and evaluating them rigorously with a user study and across a variety of historical and contemporary artists. Our results show that the main idea of IMPRESS is sound, and that very similar robust mimicry methods are effective.

Unlearnable examples . Style mimicry protections build upon a line of work that aims to make data "unlearnable" by machine learning models [(Shan et al., 2020;](#b37)[Huang et al., 2021;](#b13)[Cherepanova et al., 2021;](#b3)[Salman et al., 2023)](#b34). These methods typically rely on some form of adversarial optimization, inspired by adversarial examples [(Szegedy et al., 2013)](#b42). Ultimately, these techniques always fall short of an adaptive adversary that enjoys a second-mover advantage: once unlearnable examples have been collected, their protection can no longer be changed, and the adversary can thereafter select a learning method tailored towards breaking the protections [(Radiya-Dixit et al., 2021;](#b29)[Fowl et al., 2021;](#b5)[Tao et al., 2021)](#b44).

## Threat Model

The goal of style mimicry is to produce images, of some chosen content, that mimic the style of a targeted artist. Since artistic style is challenging to formalize or quantify, we refrain from doing so and define a mimicry attempt as successful if it generates new images that a human observer would qualify as possessing the artist's style.

We assume two parties, the artist who places art online (e.g., in their portfolio), and a forger who performs style mimicry using these images. The challenge for the forger is that the artist first protects their original art collection before releasing it online, using a state-of-the-art protection tool such as Glaze, Mist or Anti-DreamBooth. We make the conservative assumption that all the artist's images available online are protected. If a mimicry method succeeds in this setting, we call it robust.

In this work, we consider style forgers who finetune a text-to-image model on an artist's images-the most successful style mimicry method to date [(Shan et al., 2023a)](#). Specifically, the forger finetunes a pretrained model f on protected images X from the artist to obtain a finetuned model f . The forger has full control over the protected images and finetuning process, and can arbitrarily modify to maximize the mimicry success. Our robust mimicry methods combine a number of "off-the-shelf" manipulations that allow even low-skilled parties to bypass existing style mimicry protections. In fact, our most successful methods require only black-box access to a finetuning API for the model f , and could thus also be applied to proprietary text-to-image models that expose such an interface.  [(Shan et al., 2023a)](#) do not generalize across fine-tuning setups. We mimic the style of the contemporary artist @nulevoy from Glaze-protected images by using: (b) the finetuning script provided by Glaze authors; and (c) an alternative off-the-shelf finetuning script from HuggingFace. In both cases, we perform "naive" style mimicry with no effort to bypass Glaze's protections. Glaze protections are successful using finetuning from the original paper, but significantly degrade with our script. Our finetuning is also better for unprotected images (see Appendix D).

## Robust Style Mimicry

We say that a style mimicry method is robust if it can emulate an artist's style using only protected artwork. While methods for robust mimicry have already been proposed, we note a number of limitations in these methods and their evaluation in Section 4.1. We then propose our own methods (Section 4.3) and evaluation (Section 5) which address these limitations.

## Limitations of Prior Robust Mimicry Methods and of Their Evaluations

(1) Some mimicry protections do not generalize across finetuning setups. Most forgers are inherently ill-intentioned since they ignore artists' genuine requests not to use their art for generative AI [(Heikkilä, 2022)](#). A successful protection must thus resist circumvention attempts from a reasonably resourced forger who may try out a variety of tools. Yet, in preliminary experiments, we found that Glaze [(Shan et al., 2023a)](#) performed significantly worse than claimed in the original evaluation, even before actively attempting to circumvent it. After discussion with the authors of Glaze, we found small differences between our off-the-shelf finetuning script, and the one used in Glaze's original evaluation (which the authors shared with us).[foot_0](#foot_0) These minor differences in finetuning are sufficient to significantly degrade Glaze's protections (see Figure [2](#) for qualitative examples). Since our off-the-shelf finetuning script was not designed to bypass style mimicry protections, these results already hint at the superficial and brittle protections that existing tools provide: artists have no control over the finetuning script or hyperparameters a forger would use, so protections must be robust across these choices.

(2) Existing robust mimicry attempts are sub-optimal. Prior evaluations of protections fail to reflect the capabilities of moderately resourceful forgers, who employ state-of-the-art methods (even off-the-shelf ones). For instance, Mist [(Liang et al., 2023)](#) evaluates against DiffPure purifications using an outdated and low-resolution purification model. Using DiffPure with a more recent model, we observe significant improvements. Glaze [(Shan et al., 2023a)](#) is not evaluated against any version of DiffPure, but claims protection against Compressed Upscaling, which first compresses an image with JPEG and then upscales it with a dedicated model. Yet, we will show that by simply swapping the JPEG compression with Gaussian noising, we create Noisy Upscaling as a variant that is highly successful at removing mimicry protections (see Figure [26](#fig_14) for a comparison between both methods).

(3) Existing evaluations are non-comprehensive. Comparing the robustness of prior protections is challenging because the original evaluations use different sets of artists, prompts, and finetuning setups. Moreover, some evaluations rely on automated metrics (e.g., CLIP similarity) which are unreliable for measuring style mimicry [(Shan et al., 2023a,b)](#). Due to the brittleness of protection methods and the subjectivity of mimicry assessments, we believe a unified evaluation is needed.

## A Unified and Rigorous Evaluation of Robust Mimicry Methods

To address the limitations presented in Section 4.1, we introduce a unified evaluation protocol to reliably assess how existing protections fare against a variety of simple and natural robust mimicry methods. Our solutions to each of the numbered limitations above are: (1) The attacker uses a popular "off-the-shelf" finetuning script for the strongest open-source model that all protections claim to be effective for: Stable Diffusion 2.1. This finetuning script is chosen independently of any of these protections, and we treat it as a black-box. (2) We design four robust mimicry methods, described in Section 4.3. We prioritize simplicity and ease of use for low-expertise attackers by combining a variety of off-the-shelf tools. (3) We design and conduct a user study to evaluate each mimicry protection against each robust mimicry method on a common set of artists and prompts.

## Our Robust Mimicry Methods

We now describe four robust mimicry methods that we designed to assess the robustness of protections. We primarily prioritize simple methods that only require preprocessing protected images. These methods present a higher risk because they are more accessible, do not require technical expertise, and can be used in black-box scenarios (e.g. if finetuning is provided as an API service). For completeness, we further propose one white-box method, inspired by IMPRESS [(Cao et al., 2024)](#b1).

We note that the methods we propose have been considered (at least in part) in prior work that found them to be ineffective against style mimicry protections [(Shan et al., 2023a;](#)[Liang et al., 2023;](#)[Shan et al., 2023b](#)). Yet, as we noted in Section 4.1, these evaluations suffered from a number of limitations. We thus re-evaluate these methods (or slight variants thereof) and will show that they are significantly more successful than previously claimed.

## Black-box preprocessing methods.

✦ Gaussian noising. As a simple preprocessing step, we add small amounts of Gaussian noise to protected images. This approach can be used ahead of any black-box diffusion model. ✦ DiffPure. We use image-to-image models to remove perturbations introduced by the protections, also called DiffPure [(Nie et al., 2022)](#b25) (see Appendix I.1). This method is black-box, but requires two different models: the purifier, and the one used for style mimicry. We use Stable Diffusion XL as our purifier.

✦ Noisy Upscaling. We introduce a simple and effective variant of the two-stage upscaling purification considered in Glaze [(Shan et al., 2023a)](#). Their method first performs JPEG compression (to minimize perturbations) and then uses the Stable Diffusion Upscaler [(Rombach et al., 2022)](#b32) (to mitigate degradations in quality). Yet, we find that upscaling actually magnifies JPEG compression artifacts instead of removing them. To design a better purification method, we observe that the Upscaler is trained on images augmented with Gaussian noise. Therefore, we purify a protected image by first applying Gaussian noise and then applying the Upscaler. This Noisy Upscaling method introduces no perceptible artifacts and significantly reduces protections (see Figure [26](#fig_14) for an example and Appendix I.2 for details).

White-box methods.

✦ IMPRESS++. For completeness, we design a white-box method to assess whether more complex methods can further enhance the robustness of style mimicry. Our method builds on IMPRESS [(Cao et al., 2024)](#b1) but adopts a different loss function and further applies negative prompting [(Miyake et al., 2023)](#b23) and denoising to improve the robustness of the sampling procedure (see Appendix I.3 and Figure [27](#) for details).

## Experimental Setup

Protection tools. We evaluate three protection tools-Mist, Glaze and Anti-DreamBooth-against four robust mimicry methods-Gaussian noising, DiffPure, Noisy Upscaling and IMPRESS++-and a baseline mimicry method. We refer to a combination of a protection tool and a mimicry method as a scenario. We thus analyze fifteen possible scenarios. Appendix J describes our experimental setup for style mimicry and protections in detail. We apply Noisy Upscaling for prompts: "a shoe" and "an astronaut riding a horse".

Artists. We evaluate each style mimicry scenario on images from 10 different artists, which we selected to maximize style diversity. To address limitations in prior evaluations [(Shan et al., 2023b)](#), we use five historical artists as well as five contemporary artists who are unlikely to be highly represented in the generative model's training set (two of these were also used in Glaze's evaluation).[foot_2](#foot_2) All details about artist selection are included in Appendix J.

Implementation. Our mimicry methods finetune Stable Diffusion 2.1 [(Rombach et al., 2022)](#b32), the best open-source model available at the time when the protections we study were introduced. We use an off-the-shelf finetuning script from HuggingFace (see Appendix J.1 for details). We first validate that our style mimicry pipeline is successful on unprotected art using a user study, detailed in Appendix K.1. For protections, we use the original codebases to reproduce Mist and Anti-Dreambooth. Since Glaze does not have a public codebase (and the authors were unable to share one), we use the released Windows application binary (version 1.1.1) as a black-box. We set each scheme's hyperparameters to maximize protections. See Appendix J.2 for details on the configuration for each protection.

We perform robust mimicry by finetuning on 18 different images per artist. We then generate images for 10 different prompts. These prompts are designed to cover diverse motifs that the base model, Stable Diffusion 2.1, can successfully generate. See Appendix K for details about prompt design.

User study. To measure the success of each style mimicry scenario, we rely only on human evaluations since previous work found automated metrics (e.g., using CLIP [(Radford et al., 2021)](#b28)) to be unreliable [(Shan et al., 2023a,b)](#). Moreover, style protections not only prevent style transfer, but also reduce the overall quality of the generated images (see Figure [3](#fig_2) for examples). We thus design a user study to evaluate image quality and style transfer as independent attributes of the generations. [3](#foot_3)Our user study asks annotators on Amazon Mechanical Turk (MTurk) to compare image pairs, where one image is generated by a robust mimicry method, and the other from a baseline state-of-the-art mimicry method that uses unprotected art of the artist. A perfectly robust mimicry method would generate images of quality and style indistinguishable from those generated directly from unprotected art. We perform two separate studies: one assessing image quality (e.g., which image looks "better") and another evaluating stylistic transfer (i.e., which image captures the artist's original style better, disregarding potential quality artifacts). Our results show that these two metrics obtain very similar results across all scenarios. Appendix K describes our user study and interface in detail.

As noted by the authors of Glaze [(Shan et al., 2023a)](#), the users of platforms like MTurk might not have high artistic expertise. However, we believe that the judgment of non-artists is also relevant as they ultimately represent a large fraction of the consumers of digital art. Thus, if lay people consider mimicry attempts to be successful, mimicked art could hurt an artist's business. Also, to mitigate potential issues with the quality of annotations [(Kennedy et al., 2020)](#b15), we put in place several control mechanisms to filter out low-quality annotations to the best of our abilities (details in Appendix K).

Evaluation metric. We define the success rate of a robust mimicry method as the percentage of annotators (5 per comparison) who prefer outputs from the robust mimicry method over those from a baseline method finetuned on unprotected art (when judging either style match or overall image quality). Formally, we define the success rate for an artist in a specific scenario as:

$success rate = 1 10 • 5 10 prompt 5 annotator 1[robust mimicry preferred over unprotected mimicry] (1)$A perfectly robust mimicry method would thus obtain a success rate of 50%, indicating that its outputs are indistinguishable in quality and style from those from the baseline, unprotected method. In contrast, a very successful protection would result in success rates of around 0% for robust mimicry methods, indicating that mimicry on top of protected images always yields worse outputs.

## Results

In Figure [4](#), we report the distribution of success rates per artist (N=10) for each scenario. We averaged the quality and stylistic transfer success rates to simplify the analysis (detailed results can be found in Appendix C). Since the forger can try multiple mimicry methods for each prompt, and then decide which one worked best, we also evaluate a "best-of-4" method that picks the most successful mimicry method for each generation (according to human evaluators).

median artist most protected artist most vulnerable artist 0% 25% 50% 75% Success rate Naive mimicry Gaussian noising IMPRESS++ DiffPure Noisy Upscaling Best-of-4 Anti-DreamBooth 0% 25% 50% 75% Success rate Glaze 0% 25% 50% 75% Success rate Mist Figure 4: Success rate per artist (N=10) on all mimicry scenarios. Box plots represent success rates for most protected, quartiles, median and least protected artists, respectively. Success rates around 50% indicate that robust mimicry outputs are indistinguishable in style and quality from mimicry outputs based on unprotected images. Best-of-4 selects the most successful method for each prompt. 6.1 Main Findings: All Protections are Easily Circumvented

We find that all existing protective tools create a false sense of security and leave artists vulnerable to style mimicry. Indeed, our best robust mimicry methods produce images that are, on average, indistinguishable from baseline mimicry attempts using unprotected art. Since many of our simple mimicry methods only use tools that were available before the protections were released, style forgers may have already circumvented these protections since their inception.

Noisy upscaling is the most effective method for robust mimicry, with a median success rate above 40% for each protection tool (recall that 50% success indicates that the robust method is indistinguishable from a mimicry using unprotected images). This method only requires preprocessing images and black-box access to the model via a finetuning API. Other simple preprocessing methods like Gaussian noising or DiffPure also significantly reduce the effectiveness of protections. The more complex white-box method IMPRESS++ does not provide significant advantages. Sample generations for each method are in Appendix B.

A style forger does not have to use a single robust mimicry method, but can test all of them and select the most successful. This "best-of-4" approach always beats the baseline mimicry method over unprotected images (which attempts a single method and not four) for all protections.

Appendix A shows images at each step of the robust mimicry process (i.e., protections, preprocessing, and sampling). Appendix B shows example generations for each protection and mimicry method. Appendix C has detailed success rates broken down per artist, for both image style and quality.

## Analysis

We now discuss key insights and lessons learned from these results.

Glaze protections break down without any circumvention attempt. Results for Glaze without robust mimicry (see "Naive mimicry" row in Figure [4](#)) show that the tool's protections are often ineffective. Without any robustness intervention, 30% of the images generated with our off-the-shelf finetuning are rated as better than the baseline results using only unprotected images. This contrasts with Glaze's original evaluation, which claimed a success rate of at most 10% for robust mimicry. [4](#foot_4)This difference is likely due to the protection's brittleness to slight changes in the finetuning setup (as we illustrated in Section 4.1). With our best robust mimicry method (noisy upscaling) the median success rate across artists rises further to 40%, and our best-of-4 strategy yields results indistinguishable from the baseline for a majority of artists.

Robust mimicry works for contemporary and historical artists alike. [Shan et al. (2023b)](#) note that one of IMPRESS' main limitations is that "purification has a limited effect when tested on artists that are not well-known historical artists already embedded in original training data". Yet, we find that our best-performing robust mimicry method-Noisy Upscaling-has a similar success rate for historical artists (42.2%) and contemporary artists with little representation in the model's training set (43.5%).

Protections are highly non-uniform across artists. As we observe from Figure [4](#), the effectiveness of protections varies significantly across artists: the least vulnerable artist (left-most whisker) enjoys much stronger mimicry protections than the median artist or the most vulnerable artist (right-most whisker). We find that robust mimicry is the least successful for artists where the baseline mimicry from unprotected images gives poor results to begin with (cf. results for artist A 1 in Appendix C and Appendix K.1). Yet, since existing tools do not provide artists with a way to check how vulnerable they are, these tools still provide a false sense of security for all artists. This highlights an inherent asymmetry between protection tools and mimicry methods: protections should hold for all artists alike, while a mimicry method might successfully target only specific artists.

Robust mimicry failures still remove protection artifacts. We manually checked the cases where all annotators ranked mimicry from unprotected art as better than robust mimicry with Noisy Upscaling. Figure [5](#fig_3) shows two examples. We find that in many instances, the model fails to mimic the style accurately even from unprotected art. In these cases, robust mimicry is still able to generate clear images that are similar to unprotected mimicry, but neither matches the original style well. 7 Discussion and Broader Impact Adversarial perturbations do not protect artists from style mimicry. Our work is not intended as an exhaustive search for the best robust mimicry method, but as a demonstration of the brittleness of existing protections. Because these protections have received significant attention, artists may believe they are effective. But our experiments show they are not. As we have learned from adversarial ML, whoever acts first (in this case, the artist) is at a fundamental disadvantage [(Radiya-Dixit et al., 2021)](#b29). We urge the community to acknowledge these limitations and think critically when performing future evaluations.

Just like adversarial examples defenses, mimicry protections should be evaluated adaptively. In adversarial settings, where one group wants to prevent another group from achieving some goal, it is necessary to consider "adaptive attacks" that are specifically designed to evade the defense [(Carlini & Wagner, 2017)](#b2). Unfortunately, as repeatedly seen in the literature on machine learning robustness, even after adaptive attacks were introduced, many evaluations remained flawed and defenses were broken by (stronger) adaptive attacks [(Tramer et al., 2020)](#b46). We show it is the same with mimicry protections: simple adaptive attacks significantly reduce their effectiveness. Surprisingly, most protections we study claim robustness against input transformations [(Liang et al., 2023;](#)[Shan et al., 2023a)](#), but minor modifications were sufficient to circumvent them.

We hope that the literature on style mimicry prevention will learn from the failings of the adversarial example literature: performing reliable, future-proof evaluations is much harder than proposing a new defense. Especially when techniques are widely publicized in the popular press, we believe it is necessary to provide users with exceptionally high degrees of confidence in their efficacy.

Protections are broken from day one, and cannot improve over time. Our most successful robust style mimicry methods rely solely on techniques that existed before the protections were introduced. Also, protections applied to online images cannot easily be changed (i.e., even if the image is perturbed again and re-uploaded, the older version may still be available in an internet archive) [(Radiya-Dixit et al., 2021)](#b29). It is thus challenging for a broken protection method to be fixed retroactively. Of course, an artist can apply the new tool to their images going forward, but pre-existing images with weaker protections (or none at all) will significantly boost an attacker's success [(Shan et al., 2023a)](#).

Nevertheless, the Glaze and Mist protection tools recently received significant updates (after we had concluded our user study). Yet, we find that the newest 2.0 versions do not protect against our robust mimicry attempts either (see Appendix E and F). A future version could explicitly target the methods we studied, but this would not change the fact that all previously protected art would remain vulnerable, and that future attacks could again attempt to adaptively evade the newest protections. The same holds true for attempts to design similar protections for other data modalities, such as video [(Passananti et al., 2024)](#b26) or audio [(Gokul & Dubnov, 2024)](#b7).

Ethics and broader impact. The goal of our research is to help artists better decide how to protect their artwork and business. We do not focus on creating the best mimicry method, but rather on highlighting limitations in popular perturbation tools-especially since using these tools incurs a cost, as they degrade the quality of published art. We will disclose our results to the affected protection tools prior to publication, so that they can determine the best course of action for their users.

Further, we argue that having no protection tools is preferable to having insecure ones. Insecure protections may mislead artists to believe it is safe to release their work, enabling forgery and putting them in a worse situation than if they had been more cautious in the absence of any protection.

With respect to our paper, all the art featured in this paper comes either from historical artists, or from contemporary artists who explicitly permitted us to display their work. We hope our results will inform improved non-technical protections for artists in the era of generative AI.

Limitations and future work. A larger study with more than 10 artists and more annotators may help us better understand the difference in vulnerability across artists. The protections we study are not designed in awareness of our robust mimicry methods. However, we do not believe this limits the extent to which our general claims hold: artists will always be at a disadvantage if attackers can design adaptive methods to circumvent the protections.

Boyang Zheng, Chumeng Liang, Xiaoyu Wu, and Yan Liu. Understanding and improving adversarial attacks on latent diffusion model. arXiv preprint arXiv:2310.04687, 2023.

## A Detailed Art Examples

This section illustrates how images look like at every stage of our work. We include (1) original artwork from a contemporary artist (@nulevoy) 5 as a reference in Figure [6](#fig_4), (2) the original artwork after applying each of the available protections in Figure [7](#), (3) one image after applying the cross product of all protections and preprocessing methods in Figure [8](#), (4) baseline generations from a model trained on unprotected art in Figure [9](#fig_6), and (5) robust mimicry generations for each scenario in Figure [10](#fig_0).   Figure [10](#fig_0): Generations in the style of @nulevoy using robust mimicry methods for the prompt "an astronaut riding a horse". Each row represents which protection was applied to the finetuning data. Each column represents the robust mimicry method used. The first column indicates naive mimicry was applied (i.e. we trained directly on the protected images). Figure [9](#fig_6) includes sample generations from a model trained on artwork without protections.

## B Robust Mimicry Generations

Albrecht Durer, "a shoe with a plant growing inside"

## Anti-DB + Gaussian noising Unprotected

Glaze + Gaussian noising Unprotected

## Mist + Gaussian noising Unprotected

Edvard Munch, "a shoe" Edvard Munch, "a piano" A5, "a feathered car"

Anna O.-Lebedeva, "a piano" A5, "a village in a thunderstorm" Edward Hopper, "a golden apple" Edward Hopper, "a feathered car"

Figure [11](#fig_0): Style mimicry for all protections using naive mimicry-no robust method is used and we finetune directly on protected images. We randomly chose artists and prompts. Each image pair shows the protected generation and generation from unprotected art. 

## C.2 Results Broken Down per Artist

We present next the results obtained for each artist in each scenario. Table [2](#tab_2) plots the success rate for each method against each protection for all artists, and Table [3](#tab_3) includes the detailed success rates.  Inter-annotator agreement 3/5 votes agree 4/5 votes agree 5/5 votes agree

Figure [18](#fig_0): Inter-annotator agreement for generations from robust mimicry with Noisy Upscaling and generations from models finetuned on protected art directly (naive mimicry). We plot the percentage of comparisons for which the preferred option was selected by 3, 4 or 5 annotators, respectively.

The graph shows a higher consensus for naive mimicry, since the differences are clearer, and more variance for robust mimicry.

## D Differences with Glaze Finetuning

In Section 4.1 and Figure [2](#), we discussed the brittleness of Glaze protections against small changes in the finetuning script. We also found our finetuning setup to be better at baseline style mimicry from unprotected art (see Figure [19](#fig_6)). Figure [19](#fig_6): The finetuning script shared by Glaze authors produce substantially worse mimicry even from unprotected art. We apply both finetuning scripts directly on unprotected art from @nulevoy.

The main reason behind this difference might be that the script uses Stable Diffusion 1.5, instead of version 2.1 as reported in their paper.

## E Findings on Glaze 2.0

After concluding our user study, Glaze [(Shan et al., 2023a)](#) released an updated version of their tool (v2.0). According to the official release, "This new version significantly improved Glaze robustness against the newest AI models". Although we could not run the entire user study with the latest protections, we reproduced some of our experiments to verify if protections were more robust under robust mimicry. We believe this comparison is fair to Glaze since we are using newer models-such as Stable Diffusion XL for upscaling. These models, although released before Glaze 1.1.1, may not have been considered in the tool's design and are now explicitly accounted for.

The official release specifically mentions "Significantly improved robustness against Stable Diffusion 1, 2, SDXL, especially for smooth surface art (e.g. anime, cartoon)". Therefore, we decided to test this new tool with the contemporary artist nulevoy, who draws in a cartoon style and gave us permission to display their artwork. As with the previous version, we only have access to the publicly available Windows application that uses unknown parameters. We protect the images using the "highest" protection option. Our main findings are:

1. Glaze v2.0 introduces more visible perturbations uniformly over the images. See  

## F Findings on Mist v2

After responsibly disclosing our work to defense developers, authors from Mist brought to our attention the recent release of their latest Mist v2 with improved resilience [(Zheng et al., 2023)](#). As we did with Glaze v2.0 (see Section E), we reproduced some of our experiments with the latest protections to verify the success of robust mimicry. Their original implementation still uses the outdated version 1.5 of Stable Diffusion. We change to SD 2.1 to match our previous experiments[foot_5](#foot_5) . Our findings, as we saw with Glaze v2.0, highlight that improved protections are still not effective against low-effort robust mimicry. More specifically, the latest version of Mist:

1. introduces visible perturbations over the images. See Figure [23](#fig_11) 2. does not improve protections against robust mimicry. See Figure [24](#) 3. creates protection that are easily removable with Noisy Upscaling. See Figure [25](#fig_3).  

## H Existing Style Mimicry Protections

Naming convention. on the context, style mimicry protections may be viewed either as attacks or as the targets of attacks. In an artistic setting, artists see style mimicry as an attack and utilize methods like Glaze as a defense. Conversely, in the context of adversarial robustness, Glaze can be seen as an attack against style mimicry methods through adversarial perturbations. The research community has not reached a consensus on terminology: Glaze's authors consider style mimicry an attack and label Glaze as a defense, while the authors of Mist and Anti-DreamBooth describe their approaches as attacks. In our work, we distance ourselves from the attack/defense terminology and instead refer to these mechanisms as protections, and to the party performing mimicry as the "style forger".

Existing protections can either target the encoder or the decoder of text-to-image models. We classify them accordingly.

## H.1 Encoder Protections

Encoder protections include adversarial perturbations in the images X so that the encoder E ϕ of the model maps images to latent representations that, when reconstructed, recover images in a different style. Concretely, an encoder protection first defines a target latent representation t x ∈ Latent for each image x ∈ X that is different to its own style. For instance, the target latent representation for Edvard Munch could be Vincent Van Gogh. Then, protection P optimizes the objective

$min δx d Lat (E ϕ (x + δ x ) , t x ) subject to d Img (x + δ x , x) ≤ p.$(2)

Glaze [(Shan et al., 2023a)](#) is an instance of an encoder protection. Glaze first selects an adversarial target style S adv that style mimicry should learn instead of the style S to be protected. Then, Glaze uses Img2Img style transfer to create a variation x Sadv in style S adv of each image x ∈ X. The latent representation of variation x Sadv is used as the target latent representation t x for each image x ∈ X.

Glaze selects the target style S adv from a pre-defined set of 50 styles S adv . First, Glaze computes the distance between the mean CLIP embedding of the images X and the prompt P S ′ corresponding to each style S ′ ∈ S adv . Then, Glaze randomly samples target style S adv from the 50 to the 75 percentile of target styles S adv sorted by distance.

Glaze implements Objective (2) with the penalty method [(Wright, 2006)](#b49) as

$min δx ∥E ϕ (x + δ x ) , t x ∥ 2 2 + α • max(LPIPS(x + δ x , x) -p, 0)(3)$where LPIPS [(Zhang et al., 2018)](#b51) is a choice for metric d Img that aims to measure user-perceived image distortion. Glaze then optimizes Objective (3) with the Adam (Kingma & Ba, 2014) optimizer.

Mist ϕ [(Liang et al., 2023](#)) is a different encoder protection from the Mist project[foot_6](#foot_6) . Mist ϕ optimizes perturbations with PGD to minimize the squared L 2 -induced distance between the latent representation of the artists' images and some unrelated target image.

In their original work, Mist is only evaluated against DreamBooth, Style Transfer, and Textual Inversion, but not against finetuning. Also, the original Mist work refers to Mist ϕ as Mist operating in textural mode.

## H.2 Denoiser Protections

Denoiser protections use the prediction error of the denoiser ϵ θ as a proxy of the quality of style mimicry, making it a feasible target for adversarial optimization. Current Denoiser protections, such as Mist [(Liang et al., 2023)](#) and [Anti-DreamBooth (Van Le et al., 2023)](#) assume that poorly reconstructed images will fail to mimic style Anti-DreamBooth [(Van Le et al., 2023)](#b47) uses the prediction error of the denoiser ϵ θadv as a proxy for the mimicry quality, where denoiser ϵ θadv corresponds to the denoiser from a finetuned model trained on images with the style to be Since perturbations maximizing the error with the pretrained decoder can be easily circunvented with finetuning, Anti-DreamBooth uses a technique they refer to as Alternating Surrogate and Perturbation Learning (ASPL). The intuition behind ASPL is trying to simulate finetuning on the art and maximizing the error during finetuning. For this purpose, they interleave finetuning steps with perturbation optimization steps.

## I Robust Mimicry Methods

This section details the robust mimicry methods we use in our work. These methods are not aimed at maximizing performance. Instead, they demonstrate how various "off-the-shelf" and low-effort techniques can significantly weaken style mimicry protections.

Formally, given protected images X and a pretrained text-to-image model f , we define a general robust mimicry pipeline that finetunes a model f and then produces an image Z for a given prompt as follows (a successful method may not require modifications in all stages):

$f ← Finetune(f ; PreProcess(X))$Z ← PostProcess(Sample( f , "prompt")).

## I.1 DiffPure

DiffPure [(Nie et al., 2022)](#b25) uses image generation diffusion models to adversarially purify images X prot . DiffPure processes each image x adv ∈ X prot with t timesteps of a diffusion process to obtain the diffused image

$x t adv = √ α t • x adv + √ 1 -α t • ϵ,$where α is the noise schedule of the diffusion process and noise ϵ is sampled from N (0, I). Then, DiffPure constructs the purified image DiffPure(x adv ) by applying reverse diffusion to image x t adv for t timesteps with an image generation diffusion model DM. Nie et al. prove that under certain idealized conditions, DiffPure is likely to weaken adversarial perturbations in image x adv .

If the text-to-image model M supports unconditional image generation, then we can use model M for the reverse diffusion process. For example, Stable Diffusion [(Rombach et al., 2022)](#b32) generates images unconditionally when the prompt P equals the empty string. Under these conditions, Img2Img is equivalent to DiffPure. Therefore, in the context of defenses for style mimicry, we refer to Img2Img applied with an empty prompt P as unconditional DiffPure, and to Img2Img applied with a non-empty prompt P as conditional DiffPure.

## I.2 Noisy Upscaling

Upscaling increases the resolution of an image by predicting new pixels that enhance the level of detail. Upscaling images can purify adversarially perturbed images [(Mustafa et al., 2019)](#b24). However, we discover that applying upscaling directly on protected images fails to remove the perturbations.

We define Noisy Upscaling as a way to address the shortcomings of upscaling. Noisy Upscaling first applies Gaussian noising and then upscales the noisy image. Noisy Upscaling has a more profound effect than the sum of its parts: Gaussian noising only adds noise to an image x adv , but does not remove the adversarial perturbation δ x . Similarly, we observe upscaling to roughly preserve perturbation δ x . In contrast, NoisyUpscale(x adv ) shows neither visually perceptible noise, nor adversarial perturbations. Figure [26](#fig_14) illustrates the improvements. We explain these phenomena as follows.

First, we use the Stable Diffusion Upscaler (Upscale SD ), which is trained on noise-augmented images and accepts the corresponding noise level L as a class-conditioning label. We can therefore condition Upscale SD on the noise level L σ 2 , corresponding to the variance σ 2 used by GaussianNoising, to remove the noise that GaussianNoising adds.

Second, we note that upscaling has shown success against adversarial perturbations for classifiers [(Mustafa et al., 2019)](#b24), but not against adversarial perturbations for generative models [(Liang et al., 2023;](#)[Shan et al., 2023a)](#).  

## I.3 IMPRESS++

We enhance the IMPRESS algorithm [(Cao et al., 2024)](#b1). We change the loss of the reverse encoding optimization from patch similarity to l ∞ and include two additional steps: negative prompting and post-processing. All in all, IMPRESS++ first preprocesses protected images with Gaussian noise and reverse encoder optimization, then samples using negative prompting and finally post-processes the generated images with DiffPure to remove noise.

Reverse encoder optimization. Reverse encoder optimization is a preprocessing defense against encoder protections. It adds additional perturbations ∆ ∆ ′ to images X prot so that the latent representation

$t x ′ adv = E ϕ (x ′ adv ) of each protected image x ′ adv = x adv + δ xadv satisfies D ϕ ′ t x ′ adv ≈ x ′ adv (4)$and each perturbation δ xadv ∈ ∆ ∆ ′ satisfies d Img (x adv + δ xadv , x adv ) ≤ p.

(5)

If Equation (4) holds, then style mimicry finetuning learns the style of images X ′ prot . In addition, the combination of Equation ( [5](#)) with the image similarity constraint d Img (x + δ x , x) ≤ p in Objective (2) ensures that the defended images X ′ prot look similar to the original images X. Therefore, style mimicry finetuning on images X ′ prot should learn a style similar to style S. Reverse encoder optimization aims to achieve Equation (4) and Equation ( [5](#)) by optimizing the objective min

$δx adv d Lat (E ϕ (x adv + δ xadv ) , E ϕ (x adv )) subject to d Img (x adv + δ xadv , x adv ) ≤ p(6)$with PGD.

Negative prompting. Negative prompting [(Miyake et al., 2023)](#b23) is a technique to guide image generation of a diffusion-based text-to-image model M away from a prompt P neg . To this end, negative prompting manipulates the classifier-free guidance [(Ho & Salimans, 2022)](#b11), which computes the denoiser output of model M as εθ (z, t, P ) = (1 + w) • ϵ θ (z, t, P ) -w • ϵ θ (z, t, "")

where parameter w controls the guidance strength. Negative prompting simply substitutes the empty string "" with P neg to obtain εθ (z, t, P ) = (1 + w) • ϵ θ (z, t, P ) -w • ϵ θ (z, t, P neg ) .

We design a routine for D InF that leverages negative prompting to guide model M away from adversarial generations. To this end, we first apply Textual Inversion with adversarial images X prot to encode the style of adversarial generations S adv into a special word w * . We then set prompt P neg = "art by w * ".

Naive negative prompting offers no strength control. little strength may fail to guide model M away from the adversarial style S adv . Too much strength may guide towards the style opposite to style S adv in the latent space of model M, which is not necessarily the desired style S. We use negative prompt weights (muerrilla, 2023) to control the strength of negative prompting. The negative prompt weights technique introduces the strength control parameter c to interpolate between Equation ( [7](#formula_7)) and Equation ( [8](#formula_8)[(Cao et al., 2024)](#b1). Negative prompting improves image consistency and denoising reduces artifacts in generated images.

## J Experimental Setup

This section describes our general experimental setup and specifies the settings and hyperparameters of the methods we use. When possible, we use default values from the machine learning literature. For implementation details see our official repository: [https://github.com/ethz-spylab/ robust-style-mimicry](https://github.com/ethz-spylab/robust-style-mimicry)

## J.1 Style Mimicry Experimental Details

As described in Section 3, our threat model considers style mimicry with a latent diffusion text-toimage model M that is finetuned on a set of images X in a style S. This section specifies our choices for model M, images X, style S, the hyperparameters for finetuning M, and the hyperparameters for generating images with the finetuned model. Where possible, we try to replicate the style mimicry setup used by Shan et al. to evaluate Glaze, and highlight any differences.

Model We use Stable Diffusion version 2.1 [(Stability AI, 2022)](#b41), the same model used to optimize the protections we evaluate [(Shan et al., 2023a;](#)[Liang et al., 2023;](#)[Van Le et al., 2023)](#b47).

Dataset. We collate 10 image sets X A : A ∈ A from 10 different artists A. Each image set X A contains 18 images that we choose manually to follow a consistent style S A . We select the artists A from contemporary and historical artists: We select 5 contemporary artists from ArtStation[foot_7](#foot_7) and 5 historical artists from the WikiArt dataset [(Tan et al., 2019)](#b43). We found 2 of the 4 artists used by Glaze and included them in our evaluation. We manually select the remaining 8 artists to cover a broad variety of styles. Glaze additionally verified that the images of the contemporary artists in their evaluation are not included in the training dataset of the model M. Unfortunately, the LAION-5B dataset [(Schuhmann et al., 2022)](#b36) used to train SD 2.1 was taken offline [(Cole, 2023)](#b4), so we are unable to perform this verification. Instead, we verify for each contemporary artist A ∈ A that SD 2.1 is to High and Render Quality to Slowest, to obtain the strongest protections. Appendix E includes qualitative results on an updated version released after we concluded our user study.

## J.3 Robust Mimicry Methods Experimental Details

## J.3.1 Gaussian noising

We manually tune the Gaussian noising strength to σ 2 = 0.05.

## J.3.2 DiffPure

We use conditional DiffPure with the best-performing publicly available image generation diffusion model, Stable Diffusion XL 1.0 (SDXL) [(Podell et al., 2023)](#b27). We implement conditional DiffPure using the HuggingFace AutoPipelineForImage2Image pipeline. We use classifier-free guidance scale guidance scale = 7.5 with prompt P = C x for image x. We manually tune the number of diffusion timesteps t via the strength pipeline argument to strength = 0.2.

## J.3.3 IMPRESS++

Reverse Optimization Like Mist ϕ , we set the PGD perturbation budget to p = 8/255 and the PGD step size to α = 1/255. We manually tune the number of PGD iterations to N PGD = 400.

Noisy Upscaling We manually tune the Gaussian noising strength to σ = 0.1. We then use the Stable Diffusion Upscaler[foot_8](#foot_8) with the maximum denoising strength L.[foot_9](#foot_9) . We note that the Stable Diffusion Upscaler is trained on diffused images of the form

$x α = √ α • x + √ 1 -α • N (0, I).$In contrast, noisy upscaling noises images additively, that is, without the factor √ α. However, we note that for √ 1 -α = σ = 0.1, we have √ α = 0.995 ≈ 1. In practice, we observe no qualitative difference in the generated images.

Negative Prompting We manually tune the negative prompting strength to c = 0.5. We use the Stable Diffusion web UI[foot_10](#foot_10) to apply Textual Inversion on the adversarial images X prot . We follow the Textual Inversion setup used by Liang et al. to evaluate Mist and set the length of the token vector t to n = 8, the embedding initialization text to "style *", the learning rate to γ = 0.005, the batch size to 1, and the number of training steps to 500.

DiffPure post To make IMPRESS++ work under a single-model availability, we apply DiffPure post with the same model that we use for image generation, SD 2.1. We implement DiffPure post using the HuggingFace AutoPipelineForImage2Image pipeline. We use the classifier-free guidance scale guidance scale = 7.5 with prompt P = C x + ", artistic" for x. We manually tune the number of diffusion timesteps t via the strength pipeline argument to the value strength = 0.2.

## K User Study

This user study was approved by our institution's IRB.

Design. Our user study asks annotators to compare outputs from one robust mimicry method against a baseline where images are generated from a model trained on the original art without protections-for a fixed set of prompts P.

We present participants with both generations and a gallery with original art in the target style. We ask participants to decide which image is better in terms of style and quality, separately. For this, we ask them two different questions:

1. Based on noise, artifacts, detail, prompt fit, and your impression, which image has higher quality?

2. Overall, ignoring quality, which image better fits the style of the style samples?

For each comparison, we collect data from 5 users. We randomize several aspects of our study to minimize user bias. We randomly select the order of robust mimicry and baseline generations.

Second, we randomly shuffle the order of all image comparisons to prevent all images from the same mimicry method to appear consecutively. Finally, we also randomly sample the seeds that models use to generate images to prevent repeating the same baseline image across different comparisons.

Differences with Glaze's user study. Our study does not exactly replicate the design of Glaze's user study for two reasons. First, the Glaze study provided annotators with four AI-generated images and four original images, asking if the generated images successfully mimicked the original artwork. This evaluation fails to account for the commonly encountered scenario where current models are incapable of reliably mimicking an artist's style even from unprotected art. Second, we believe the relative assessment recorded in our study ("Which of these two mimicry attempts is more successful?") is easier for humans than the absolute assessment used in the Glaze study ("Is this mimicry attempt successful").

Prompts. We curate a small dataset of 10 prompts P. We design the prompts to satisfy two criteria:

1. The prompts should cover diverse motifs with varying complexity. This ensures that we can detect if a scenario compromised the prompt-following capabilities of a style mimicry model.

2. The prompts should only include prompts for which our finetuning base model M, SD 2.1, can successfully generate a matching image. This reduces the impact of potential human bias against common defects of SD 2.1.

To satisfy criterion 1 and increase variety, we instruct ChatGPT to generate prompt suggestions for four different categories:

1. Simple prompts with template "a {subject}".

2. Two-entity prompts with template "a {subject} {ditransitive verb} a {object}".

3. Entity-attribute prompts with template "a {adjective} {subject}".

4. Entity-scene prompts with template "a {subject} in a {scene}".

The chat we used to generate our prompts can be accessed at [https://chatgpt.com/share/ ea3d1290-f137-4131-baca-2fa1c92b3859](https://chatgpt.com/share/ea3d1290-f137-4131-baca-2fa1c92b3859). To satisfy criterion 2, we generate images with SD 2.1 on prompts suggested by ChatGPT and manually filter out prompts with defect generations (e.g. a horse with 6 legs). We populate the final set of prompts P with 4 simple prompts, 2 two-entity prompts, 2 entity-attribute prompts, and 2 entity-scene prompts (see Figure [29](#fig_6)). Figure 29: Our set of prompts. We manually wrote the prompts "a astronaut riding a horse" and "a village in a thunderstorm". ChatGPT wrote the remaining prompts.

Quality control. We first run a pilot study where we directly ask users to answer the previous questions about style and quality. This study resulted in very low-quality responses that are barely better than random choice. We enhanced the study to introduce several quality control measures to improve response quality and filter out low-quality annotations:

1. We limit our study to desktop users so that images are sufficiently large to perceive artifacts introduced by protections.

2. We precede the questions we use for our study with four dummy questions about the noise, artifacts, detail, and prompt matching of the images. The dummy questions force annotators to pay attention and gather information useful to answer the target questions.

3. We precede our study with a training session that shows for question 1, 2, and each of the four dummy questions an image pair with a clear, objective answer. The training session helps users to understand the study questions. We introduced this stage after gathering valuable feedback for annotators.

4. We add control comparisons to detect annotators who did not understand the tasks or were answering randomly. We generated several images from the baseline model trained on the original art. For each of these images, we created two ablations. For question 1 (quality), we include Gaussian noise to degrade its quality but preserve the same information. For question 2 (style), we apply Img2Img to remove the artist style and map the image back to photorealism using the prompt "high quality photo, award winning". We randomly include control comparisons between the original generations and these ablations, and we only accept labels from users who answered correctly at least 80% of the control questions.

Execution. We execute our study on Amazon Mechanical Turk (MTurk). We design and evaluate an MTurk Human Intelligence Task (HIT) for each artist A ∈ A, shown in Figure [30](#fig_2). Each HIT includes image pair comparisons for a single artist A under all scenarios S ∈ M, as well 10 quality control image pairs, 10 style control image pairs, and 6 training image pairs. We generate an image pair for each of the 10 prompts and each of 15 scenarios, for a total of 10 • 15 + 10 + 10 + 6 = 176 image pairs per HIT. We estimate study participants to spend 5 minutes on the training image pairs and 30 seconds per remaining image pair, so 90 minutes in total. We compensate study participants at a rate of $16/hour, so $24 per HIT.

## K.1 Style Mimicry Setup Validation

We execute an additional user study to validate that our style mimicry setup in Appendix G successfully mimics style from unprotected images. For each prompt P ∈ P and artist A ∈ A, our validation study uses the baseline model trained on uprotected art to generate one image. Inspired by the evaluation by Glaze [(Shan et al., 2023a)](#), we ask participants to evaluate the style mimicry success by answering the question:

How successfully does the style of the image mimic the style of the style samples? Ignore the content and only focus on the style.

To answer this question, we show a participant the image x O A and the images X A that serve as style samples. The participant can answer the question on a 5-point Likert scale with options 1. Not successful at all 2. Not very successful 3. Somewhat successful 4. Successful 5. Very successful We also execute the style mimicry validation study on MTurk. We design and evaluate a single HIT for all questions, shown in Figure [33](#fig_20). We estimate study participants to spend 15 seconds on each question, and to spend 1 minute to familiarize themselves with a new style, so 35 minutes in total. We compensate study participants at a rate of $18/hour, so $10.50 per HIT.

We find that style mimicry is successful in over 70% of the comparisons. Results are detailed in Figure [31](#fig_2).  

![Figure1: Artists are vulnerable to style mimicry from generative models finetuned on their art. Existing protection tools add small perturbations to published artwork to prevent mimicry(Shan et al., 2023a; Liang et al., 2023;Van Le et al., 2023). However, these protections fail against robust mimicry methods, giving a false sense of security and leaving artists vulnerable. Artwork by @nulevoy (Stas Voloshin), reproduced with permission.]()

![Figure2: The protections of Glaze(Shan et al., 2023a) do not generalize across fine-tuning setups. We mimic the style of the contemporary artist @nulevoy from Glaze-protected images by using: (b) the finetuning script provided by Glaze authors; and (c) an alternative off-the-shelf finetuning script from HuggingFace. In both cases, we perform "naive" style mimicry with no effort to bypass Glaze's protections. Glaze protections are successful using finetuning from the original paper, but significantly degrade with our script. Our finetuning is also better for unprotected images (see Appendix D).]()

![Figure3: Examples of robust style mimicry for two different artists: @greg-f (contemporary) and Edvard Munch (historical). Cherry-picked examples with strong protections and successful robust mimicry. We apply Noisy Upscaling for prompts: "a shoe" and "an astronaut riding a horse".]()

![Figure 5: Randomly selected comparisons where all 5 annotators preferred mimicry from unprotected art over robust mimicry. Both use Noisy Upscaling for robust mimicry.]()

![Figure 6: 4 samples from the original artwork from @nulevoy.]()

![Figure 7: Artwork in Figure 6 after applying different protections.]()

![Figure9: Generations in the style of @nulevoy after finetuning on unprotected images. Each generation is sampled with a different seed.]()

![Figure 20: Comparison of perturbations by Glaze v1.1.1 and v2.0 on artwork from @nulevoy.]()

![Figure 21: Comparison of robust style mimicry (Noisy Upscaling) on artwork from @nulevoy protected with both versions of Glaze. Images in Figure 6 serve as a reference for the artistic style.]()

![Figure 22: Original artwork from @nulevoy and the resulting images after applying Noisy Upscaling to artwork protected with Glaze v2.0. See protected images in Figure 20.]()

![Figure 23: Comparison of perturbations introduced by Mist v1 and v2 on artwork from @nulevoy.]()

![Figure 24: Comparison of robust style mimicry (Noisy Upscaling) on artwork from @nulevoy protected with both versions of Mist. Images in Figure 6 serve as a reference for the artistic style.]()

![Figure 26: Illustration of Noisy Upscaling on a random image from @nulevoy. Unlike naive upscaling and Compressed Upscaling, Noisy Upscaling removes protections while preserving the details in the original artwork.]()

![Figure 27 illustrates the improvements introduced by each additional step.]()

![Figure 28: The Mist target image Target Target Mist is the default target image in the reference Mist implementation and one of the successful target images evaluated by Liang & Wu.]()

![Figure 30: The interface of our user study.]()

![Figure 32: User ratings of clean style mimicry success. Each bar indicates the percentage of votes for the corresponding success level over all clean style mimicry generations for the corresponding artist.]()

![Figure 33: The interface of our style mimicry setup validation study.]()

![Success rates averaged across artists for all style mimicry scenarios. Higher percentages indicate more successful mimicry, and 50% would indicate perfect mimicry.]()

![Success rates per artist for style and quality questions, respectively.]()

![User preference ratings of all style mimicry scenarios S ∈ M for each artist A ∈ A by name. Each cell states the percentage of votes that prefer an image generated under the corresponding scenario S and artist A ∈ A over a matching image generated under clean style mimicry. Higher percentages indicate weaker attacks or better defenses.]()

The two finetuning scripts mainly differ in the choice of library, model, and hyperparameters. We use a standard HuggingFace script and Stable Diffusion

2.1 (the model evaluated in the Glaze paper).

Contemporary Artists were selected from Artstation. We keep them anonymous throughout this work-and refrain from showcasing their art-except for artists who gave us explicit permission to share their identity and art. We will share all images used in our experiments upon request with researchers.

The user study was approved by our institution's IRB.

The original evaluation in Glaze directly asks annotators whether a mimicry is successful or not, rather than a binary comparison between a robust mimicry and a baseline mimicry as in our setup.Shan et al. (2023a)  report that mimicry fails in 4% of cases for unprotected images, and succeeds in 6% of cases for protected images. This bounds the success rate for robust mimicry-according to our definition in Equation (1)-by at most 10%.

Both models share the same encoder for which protections are optimized.

Mist project also contains a denoiser attack that we fail to reproduce as a robust protection.

www.artstation.com

www.huggingface.co/stabilityai/stable-diffusion-x4-upscaler

We inadvertently set the denoising strength to L = 320 instead of the actual maximum denoising strength L = 350. We observe no qualitative difference in the generated images.

https://github.com/AUTOMATIC1111/stable-diffusion-webui

