<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparing the information content of probabilistic representation spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-19">19 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kieran</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
							<email>kieranm@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Bioengineering</orgName>
								<orgName type="department" key="dep2">Dept. of Physics &amp; Astronomy</orgName>
								<orgName type="department" key="dep3">Depts. of Bioengineering</orgName>
								<orgName type="department" key="dep4">Electrical &amp; Systems Engineering, Physics &amp; Astronomy, Neurology, Psychiatry</orgName>
								<orgName type="institution" key="instit1">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit4">The Santa Fe Institute</orgName>
								<orgName type="institution" key="instit5">Montreal Neurological Institute</orgName>
								<orgName type="institution" key="instit6">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Dillavou</surname></persName>
							<email>dillavou@sas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Bioengineering</orgName>
								<orgName type="department" key="dep2">Dept. of Physics &amp; Astronomy</orgName>
								<orgName type="department" key="dep3">Depts. of Bioengineering</orgName>
								<orgName type="department" key="dep4">Electrical &amp; Systems Engineering, Physics &amp; Astronomy, Neurology, Psychiatry</orgName>
								<orgName type="institution" key="instit1">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit4">The Santa Fe Institute</orgName>
								<orgName type="institution" key="instit5">Montreal Neurological Institute</orgName>
								<orgName type="institution" key="instit6">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dani</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Bioengineering</orgName>
								<orgName type="department" key="dep2">Dept. of Physics &amp; Astronomy</orgName>
								<orgName type="department" key="dep3">Depts. of Bioengineering</orgName>
								<orgName type="department" key="dep4">Electrical &amp; Systems Engineering, Physics &amp; Astronomy, Neurology, Psychiatry</orgName>
								<orgName type="institution" key="instit1">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit4">The Santa Fe Institute</orgName>
								<orgName type="institution" key="instit5">Montreal Neurological Institute</orgName>
								<orgName type="institution" key="instit6">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Comparing the information content of probabilistic representation spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-19">19 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">966DEAE209C17B19FABA7DE9EF3F2A90</idno>
					<idno type="arXiv">arXiv:2405.21042v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-18T18:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Probabilistic representation spaces convey information about a dataset and are shaped by factors such as the training data, network architecture, and loss function. Comparing the information content of such spaces is crucial for understanding the learning process, yet most existing methods assume point-based representations, neglecting the distributional nature of probabilistic spaces. To address this gap, we propose two information-theoretic measures to compare general probabilistic representation spaces by extending classic methods to compare the information content of hard clustering assignments. Additionally, we introduce a lightweight method of estimation that is based on fingerprinting a representation space with a sample of the dataset, designed for scenarios where the communicated information is limited to a few bits. We demonstrate the utility of these measures in three case studies. First, in the context of unsupervised disentanglement, we identify recurring information fragments within individual latent dimensions of VAE and InfoGAN ensembles. Second, we compare the full latent spaces of models and reveal consistent information content across datasets and methods, despite variability during training. Finally, we leverage the differentiability of our measures to perform model fusion, synthesizing the information content of weak learners into a single, coherent representation. Across these applications, the direct comparison of information content offers a natural basis for characterizing the processing of information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The comparison of representation spaces is a problem that has received much attention, particularly as a route to a deeper understanding of information processing systems <ref type="bibr" target="#b29">(Klabunde et al., 2023;</ref><ref type="bibr" target="#b39">Mao et al., 2024;</ref><ref type="bibr" target="#b23">Huh et al., 2024;</ref><ref type="bibr" target="#b35">Lin &amp; Kriegeskorte, 2024)</ref> and with applications in tasks like transfer learning, ensembling, and other forms of representational alignment <ref type="bibr" target="#b56">(Sucholutsky et al., 2023;</ref><ref type="bibr" target="#b44">Muttenthaler et al., 2024)</ref>. Existing methods are applicable to point-based representation spaces, including centered kernel alignment (CKA) <ref type="bibr" target="#b31">(Kornblith et al., 2019)</ref> and representational similarity analysis (RSA) <ref type="bibr" target="#b32">(Kriegeskorte et al., 2008)</ref>. For representation spaces whose citizens are probability distributions, such as in variational autoencoders (VAEs) or in biological systems with inherent stochasticity, failure to account for the distributed nature of representations can miss important aspects of the relational structure between data points <ref type="bibr" target="#b11">(Duong et al., 2023)</ref>. Few methods account for the distributional nature of representations <ref type="bibr" target="#b29">(Klabunde et al., 2023)</ref>.</p><p>One of the few existing methods for comparing probabilistic representation spaces, stochastic shape metrics <ref type="bibr" target="#b11">(Duong et al., 2023)</ref>, relies on geometric assumptions and fixed transformations, which can limit its flexibility. In contrast, we adopt an information-theoretic approach that evaluates representation spaces based on the information they transmit, providing a method that is agnostic to properties like dimensionality or Figure <ref type="figure">1</ref>: Similarity of representation spaces. In this work, we generalize measures to compare the information content of clustering assignments to apply to probabilistic representation spaces. (a) A hard clustering assignment, such as the living/non-living distinction conveyed by clustering V , communicates certain information about the dataset (here, CIFAR-10 images). Comparing the information content of different clustering assignments enables comparative analyses between algorithms, model fusion, and benchmarking. (b) We generalize measures for comparing hard clustering assignments to be applicable to probabilistic representation spaces, by recognizing the latter as soft clustering assignments. When cast in terms of information content, there is no requirement for the dimensionality of the spaces to match, and hard clusterings (e.g. labels or annotations) can be compared to probabilistic spaces.</p><p>whether the spaces are discrete or continuous. Specifically, we compare two probabilistic representation spaces using quantities derived from the mutual information between them, capturing their relational structure while ensuring broad applicability across diverse contexts.</p><p>We take as a motivating example the task of unsupervised disentanglement, whose goal is to break information about a dataset into useful factors of variation <ref type="bibr" target="#b19">(Higgins et al., 2018;</ref><ref type="bibr" target="#b38">Locatello et al., 2019;</ref><ref type="bibr">Träuble et al., 2021;</ref><ref type="bibr" target="#b2">Balabin et al., 2023;</ref><ref type="bibr" target="#b58">Van Steenkiste et al., 2019)</ref>. As an example, a representation space might be trained on images of cars so that color, orientation, and model information are separated into different latent dimensions without any supervision about such factors. When ground truth factors of variation are unavailable for evaluation, as is generally the case for real-world datasets, existing evaluation methods assess the degree of consensus in an ensemble of trained models <ref type="bibr" target="#b9">(Duan et al., 2020;</ref><ref type="bibr" target="#b36">Lin et al., 2020)</ref>. However, the relatedness of representation spaces has failed to account for the probabilistic nature of the representations, reducing posterior distributions to their means and then using point-based comparisons such as correlation <ref type="bibr" target="#b9">(Duan et al., 2020)</ref>. With a direct comparison of the information content of representation spaces, we stand to improve the characterization of consensus and of unsupervised disentanglement more generally.</p><p>In this work, we generalize measures of similarity of the information content of hard clustering assignments (Fig. <ref type="figure">1a</ref>) to be applicable to probabilistic representation spaces (Fig. <ref type="figure">1b</ref>). To motivate the generalization, we treat probabilistic representation spaces as soft clustering assignments of data, whereby partial distinguishability between data points is expressed by the overlap between posterior distributions. We then assess the information content of learned representation spaces produced by models in an ensemble of random initializations, and study the effects of method, training progress, and dataset on information processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The proposed method builds upon classic means of comparing the information content of different cluster assignments (clusterings) of a dataset and generalizes them to compare probabilistic representation spaces. We then use the method to empirically study generative models designed to fragment information about a dataset in a learned latent space, i.e., for unsupervised disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity of clusterings and of representation spaces.</head><p>The capacity to compare transformations of data produced by different machine learning models enables ensemble learning, a deeper understanding of methodology, and benchmarking <ref type="bibr" target="#b47">(Punera &amp; Ghosh, 2007)</ref>. <ref type="bibr" target="#b55">Strehl &amp; Ghosh (2002)</ref> used outputs of clustering algorithms to perform ensemble learning, based on a measure of similarity between clustering assignments that we will extend in this work: the normalized mutual information (NMI). Referred to as consensus clustering or ensemble clustering, efforts to combine multiple clustering outputs can leverage any of a variety of similarity measures <ref type="bibr" target="#b63">(Wagner &amp; Wagner, 2007;</ref><ref type="bibr" target="#b60">Vinh et al., 2009;</ref><ref type="bibr">2010;</ref><ref type="bibr" target="#b59">Vega-Pons &amp; Ruiz-Shulcloper, 2011;</ref><ref type="bibr" target="#b22">Huang et al., 2017)</ref>. Another measure backed by information theory is the variation of information (VI), coined by <ref type="bibr" target="#b41">Meilă (2003)</ref> for clustering but recognized as a metric distance between information sources at least twice before <ref type="bibr" target="#b54">(Shannon, 1953;</ref><ref type="bibr" target="#b8">Crutchfield, 1990)</ref>.</p><p>Commonly referred to simply as 'clustering', hard clusterings assign every input datum to one and only one output cluster and have been generalized to multiple forms of soft clustering <ref type="bibr" target="#b5">(Campagner et al., 2023)</ref>. We focus specifically on fuzzy clustering, where membership is assigned to multiple clusters and must sum to one for each datum <ref type="bibr">(Zadeh, 1965;</ref><ref type="bibr" target="#b10">Dunn, 1973;</ref><ref type="bibr" target="#b50">Ruspini et al., 2019)</ref>. We observe that a probabilistic representation space communicates for each datum a soft assignment over latent vectors, with the degree of membership expressed by the probability density of posterior distributions. Comparisons of hard clustering have been extended to fuzzy clustering for measures that indirectly assess information content, such as the Rand index <ref type="bibr" target="#b47">(Punera &amp; Ghosh, 2007;</ref><ref type="bibr" target="#b24">Hullermeier et al., 2011;</ref><ref type="bibr" target="#b12">D'Ambrosio et al., 2021;</ref><ref type="bibr" target="#b0">Andrews et al., 2022;</ref><ref type="bibr" target="#b64">Wang et al., 2022)</ref>. Information-based measures have been extended for specific types of soft clusters <ref type="bibr" target="#b4">(Campagner &amp; Ciucci, 2019;</ref><ref type="bibr" target="#b5">Campagner et al., 2023)</ref> but none, to our awareness, for comparing the information content of fuzzy clusterings over a continuous representation space.</p><p>A rich area of research compares point-based representation spaces via the pairwise geometric similarity of a common set of data points in the space <ref type="bibr" target="#b31">(Kornblith et al., 2019;</ref><ref type="bibr" target="#b17">Hermann &amp; Lampinen, 2020;</ref><ref type="bibr" target="#b29">Klabunde et al., 2023)</ref>, building upon representational similarity analysis from neuroscience <ref type="bibr" target="#b32">(Kriegeskorte et al., 2008)</ref>. In place of geometric similarity, topological similarity more closely probes the information available for downstream processing by employing specific similarity functions with tunable parameters <ref type="bibr" target="#b35">(Lin &amp; Kriegeskorte, 2024;</ref><ref type="bibr" target="#b65">Williams, 2024)</ref>. For comparing probabilistic representation spaces, stochastic shape metrics <ref type="bibr" target="#b11">(Duong et al., 2023)</ref> extend a distance metric between point-based representation spaces based on aligning one with another through prescribed transformations (e.g., rotations) <ref type="bibr" target="#b66">(Williams et al., 2021)</ref>. By contrast, the informationtheoretic lens we adopt requires no enumeration of transformations nor tuning of parameters, and directly assesses the information available for processing by downstream neural networks.</p><p>Unsupervised disentanglement. Disentanglement is the problem of splitting information into useful pieces, possibly for interpretability, compositionality, or stronger representations for downstream tasks. Shown to be impossible in the fully unsupervised case <ref type="bibr" target="#b38">(Locatello et al., 2019;</ref><ref type="bibr">Khemakhem et al., 2020)</ref>, research has moved to investigate how to utilize weak supervision <ref type="bibr">(Khemakhem et al., 2020;</ref><ref type="bibr" target="#b51">Sanchez et al., 2020;</ref><ref type="bibr" target="#b62">Vowels et al., 2020;</ref><ref type="bibr" target="#b43">Murphy et al., 2022)</ref> and incorporate inductive biases <ref type="bibr" target="#b2">(Balabin et al., 2023;</ref><ref type="bibr" target="#b6">Chen et al., 2018;</ref><ref type="bibr" target="#b49">Rolinek et al., 2019;</ref><ref type="bibr" target="#b70">Zietlow et al., 2021;</ref><ref type="bibr" target="#b21">Hsu et al., 2024)</ref>.</p><p>A significant challenge for disentanglement is evaluation when no ground truth factorization is available. While one proposed route to evaluation relies on a characterization of the posterior distributions in a model <ref type="bibr">(PIPE, Estermann &amp; Wattenhofer (2023)</ref>), a more common approach assesses consensus among trained models in an ensemble of randomly initialized repeats. The motivation, first proposed in <ref type="bibr" target="#b9">Duan et al. (2020)</ref>, is that disentangled models are more similar to each other than are entangled ones because there are intuitively more ways to entangle information than to disentangle it. There, the similarity between two models was evaluated with an ad hoc function of dimension-wise similarity, which was computed as the rank correlation or the weights of a linear model between embeddings in different one-dimensional spaces. ModelCentrality <ref type="bibr" target="#b36">(Lin et al., 2020)</ref> quantified similarity between models with the FactorVAE score <ref type="bibr" target="#b27">(Kim &amp; Mnih, 2018)</ref> where one model's embeddings serve as the labels for another model. However, none of these methods fully account for the distributional nature of representations, instead relying on point estimates such as posterior means or single embeddings. In contrast, our approach focuses on capturing the probabilistic structure of representation spaces, and additionally shifts the emphasis from model similarity to channel (or latent subspace) similarity under the premise that the fragmentation of information, central to disentanglement, is more naturally studied via the information fragments themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to compare the information transmitted about a dataset by different representation spaces, and we will use the comparison of hard clustering assignments (e.g., the output of k-means) as a point of reference. Analogously to hard clustering (Fig. <ref type="figure">1a</ref>), probabilistic representation spaces communicate a soft assignment over embeddings that is expressed by a probability distribution in the space for each data point (Fig. <ref type="figure">1b</ref>).</p><p>While the proposed method can be applied to any representation space with probabilistic embeddings, we will focus primarily on variational autoencoders (VAEs) <ref type="bibr" target="#b28">(Kingma &amp; Welling, 2014)</ref>. Let the random variable X represent a sample x ∼ p(x) from the dataset under study. X is transformed by a stochastic encoder parameterized by a neural network to a variable U = f (X, ϵ), where ϵ is a source of stochasticity. We will make use of two fundamental quantities in information theory <ref type="bibr" target="#b7">(Cover &amp; Thomas, 1999)</ref>, the entropy of a random variable H(Z) = E z∼p(z) <ref type="bibr">[-log p(z)</ref>] and the mutual information between two random variables</p><formula xml:id="formula_0">I(Y ; Z) = H(Y ) + H(Z) -H(Y, Z).</formula><p>The encoder maps each data point x to a posterior distribution in the latent space, p(u|x), after which a point u ∼ p(u|x) is sampled for downstream processing. Commonly, the posterior distributions are parameterized as normal distributions with diagonal covariance matrices, which will facilitate many of the involved measurements but is not required for the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparing representation spaces as soft clusterings</head><p>Consider a hard clustering of data as communicating certain information about the data (Fig. <ref type="figure">1a</ref>). By observing the cluster assignment U instead of a sample X from the dataset, information I(X; U ) will have been conveyed. For a hard clustering, every data point is assigned unambiguously to a cluster-i.e., H(U |X) = 0-which makes the communicated information equal to the entropy of the clustering, I(X; U ) = H(U ). We note that maximizing communicated information, such as by assigning each data point to its own cluster, does not yield a useful representation. The value of a representation lies in the balance between the information preserved and the irrelevant variation discarded, highlighting the importance of assessing its specific information content.</p><p>Given two hard clustering assignments U and V for the same data X, the mutual information I(U ; V ) measures the amount of shared information content they express about the data. Previous works have found it useful to relate the mutual information to functions of the entropies H(U ) and H(V ). <ref type="bibr" target="#b55">Strehl &amp; Ghosh (2002)</ref> proposed the normalized mutual information (NMI) as the ratio of the mutual information and the geometric mean of the entropies,</p><formula xml:id="formula_1">NMI(U, V ) = I(U ; V ) H(U )H(V ) . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Meilă ( <ref type="formula">2003</ref>) proposed the variation of information (VI), a metric distance between clusterings that is proportional to the difference of the mutual information and the arithmetic mean of the entropies,</p><formula xml:id="formula_3">VI(U, V ) = -2 I(U ; V ) - H(U ) + H(V ) 2 . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>Soft clustering extends hard clustering to allow each datum to have partial membership in multiple clusters, allowing partial distinguishability between data points to be communicated <ref type="bibr">(Zadeh, 1965;</ref><ref type="bibr" target="#b50">Ruspini et al., 2019)</ref> (Fig. <ref type="figure">1b</ref>). While soft clustering is predominantly performed over a discrete set of clusters, here we view each point u in a continuous latent space as a cluster, with the posterior distribution p(u|x) assigning membership over the continuum.</p><p>The utility of NMI or VI over raw mutual information largely resides in the standardization of values. For two identical hard clustering assignments, U and U ′ , NMI(U, U ′ ) = 1 and VI(U, U ′ ) = 0. In contrast, soft assignments include additional entropy stemming from uncertainty that is unrelated to the information communicated about the dataset, as is evident from the relation</p><formula xml:id="formula_5">H(U ) = I(X; U )+H(U |X) with H(U |X) &gt; 0.</formula><p>As a result, the entropy terms in Eqns. 1 and 2 that ground the mutual information I(U ; V ) have components that disrupt the standardization of values. To address this issue, we propose replacing the entropy of a clustering assignment with the mutual information between two copies of that assignment: specifically, substituting H(U ) with I(U ; U ′ ), and similarly for V . For hard clustering,</p><formula xml:id="formula_6">H(U ) = I(X; U ) = I(U ; U ′ ),</formula><p>making these quantities interchangeable. Only the third quantity, I(U ; U ′ ), maintains the standardization of NMI = 1 and VI = 0 for identical soft clustering assignments. The generalized forms then become</p><formula xml:id="formula_7">NMI(U, V ) = I(U ; V ) I(U ; U ′ )I(V ; V ′ ) , and<label>(3)</label></formula><formula xml:id="formula_8">VI(U, V ) = -2 I(U ; V ) - I(U ; U ′ ) + I(V ; V ′ ) 2 . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>The generalization leaves NMI and VI unchanged for hard clustering, where I(U ; U ′ ) = H(U ), and enables the measures to be applied to both soft and hard assignments. We note that the generalized VI is no longer a proper metric, as the triangle inequality is not guaranteed (Appx. D).</p><p>The conditional independence of clustering assignments given the data-i.e., I(U ; V |X) = 0-allows each mutual information term in Eqns. 3 and 4 to be rewritten with regard to information communicated about the data. Namely, I(U ; V ) = I(X; U ) + I(X; V ) -I(X; U, V ), and</p><formula xml:id="formula_10">I(U ; U ′ ) = 2I(X; U ) -I(X; U, U ′ ).</formula><p>Because we have access to the posterior distributions, it is easier to estimate the mutual information between the data X and a representation space (or a combination thereof) than it is to estimate the information between two representation spaces <ref type="bibr" target="#b46">(Poole et al., 2019)</ref>.</p><p>The extended NMI and VI benefit from the generality of information theory: the information content of two probabilistic representation spaces can be compared regardless of dimensionality or parameterization of posteriors, and a soft clustering can be compared to a hard clustering, whether from a quantized latent space or a discrete labelling process (Fig. <ref type="figure">1b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Routes to estimation</head><p>We propose two routes to estimating NMI and VI that offer a tradeoff between precision and speed; both leverage the known posterior distributions to calculate the information transmitted about the dataset by combinations of representation spaces, I(X; •). The first route is to compute I(X; •) with a straightforward Monte Carlo estimate using the aggregated posterior over the entire dataset of size L,</p><formula xml:id="formula_11">I(X; U ) = E x∼p(x) E u∼p(u|x) log p(u|x) L i p(u|x i ) . (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>For the second route, we can use a measure of statistical similarity between posteriors p(u|x 1 ) and p(u|x 2 ) in a given representation space-the Bhattacharyya coefficient <ref type="bibr" target="#b25">(Kailath, 1967)</ref>, BC(p, q) = Z p(z)q(z)dz-to quickly "fingerprint" the information content of spaces with the pairwise distinguishability of a sample of data points <ref type="bibr" target="#b42">(Murphy &amp; Bassett, 2023)</ref>. The BC between two multivariate normal distributions can be efficiently computed in bulk via array operations. Once a matrix BC ij := BC(p(u|x i ), p(u|x j )) of the pairwise values for a random sample of N data points is obtained, a lower bound for the information transmitted by the channel can be estimated with <ref type="bibr" target="#b30">(Kolchinsky &amp; Tracey, 2017)</ref>. For fast estimation of the information content of a representation space with respect to a ground truth generative factor, we can treat the labels as an effective hard clustering according to that factor, where BC values are either zero or one. Finally, the matrix BC ij for the combination of spaces U 1 and U 2 is their elementwise product (Appx. C). In other words, receiving both of the messages from U 1 and U 2 leads to a distinguishability between data points that is simply the product of the distinguishabilities under U 1 and U 2 separately. Together, the properties allow us to fingerprint each representation space through Bhattacharyya matrices, and then perform all subsequent analysis with only the matrices-i.e., without having to load the models into memory again.</p><formula xml:id="formula_13">I(X; U ) ≥ 1 N N i log 1 N N j BC ij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discovering consistently learned information fragments via OPTICS clustering of latent dimensions</head><p>In the context of disentanglement, we are interested in the similarity of information contained in individual dimensions across an ensemble of models. We compute the pairwise similarities between all dimensions of all models-for example, 10 dimensions each from 50 models in an ensemble for a 500×500 matrix of similarity values in Sec. 4.2-and then use density-based clustering to identify information content that is found consistently. OPTICS <ref type="bibr" target="#b1">(Ankerst et al., 1999)</ref> is an algorithm that orders elements in a set according to similarity and produces a "reachability" profile where natural groupings of elements are indicated through valleys. An OPTICS profile is analogous to a dendrogram produced by hierarchical clustering, but can be more comprehensible for large sets of points <ref type="bibr" target="#b52">(Sander et al., 2003)</ref>. The consistency of information fragments in the latent dimensions of an ensemble of representation spaces can then be visualized with the OPTICS reachability profile, the reordered pairwise similarity matrix-which will have block diagonal form if the fragments are consistent-and the information contained about ground truth generative factors, if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model fusion</head><p>Consider a set of representation spaces found by an ensemble of weak learners. In the spirit of "knowledge reuse" <ref type="bibr" target="#b55">(Strehl &amp; Ghosh, 2002)</ref> originally applied to ensembles of hard clusters, we might obtain a superior representation space from the synthesis of the set. In contrast to other assessments of representation space similarity <ref type="bibr" target="#b9">(Duan et al., 2020;</ref><ref type="bibr" target="#b27">Kim &amp; Mnih, 2018;</ref><ref type="bibr" target="#b11">Duong et al., 2023)</ref>, the proposed measures of similarity are composed of mutual information terms, which can be optimized with differentiable operations by a variety of means <ref type="bibr" target="#b46">(Poole et al., 2019)</ref>. We optimize a synthesis space by maximizing its average similarity with a set of reference spaces, performing gradient descent directly on the encoding of the data points used for the Bhattacharyya matrices. Instead of requiring many models to remain in memory during training of a new encoder, the Bhattacharyya matrices are computed for the ensemble once and then used for comparison during training, and training is fast because of the involved array operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of related methods on synthetic spaces</head><p>We first evaluated the similarity of synthetic representation spaces using ours and related methods (Fig. <ref type="figure" target="#fig_0">2</ref>). A dataset of 64 points was embedded to one-and two-dimensional representation spaces, each communicating different information about the dataset. To assess the information available for downstream processing, we trained a classification head for each of the nine latent spaces to predict the input x from a sample of the corresponding posterior, u ∼ p(u|x). As shown in Fig. <ref type="figure" target="#fig_0">2b</ref>, the classification head's average predictions, p(x j |x i ) = E u∼p(u|xi) [p(x j |u)], reflected the overlap of posterior distributions in latent space by assigning non-zero probabilities to multiple outputs. The structure of these output probabilities closely matched the similarity patterns in the distinguishability matrices of panel c, supporting our use of the latter as fingerprints of the information in the latent space. Unlike the classification head's outputs, the distinguishability matrices were directly accessible from the latent space without additional network training.</p><p>We evaluated the pairwise similarity between the nine latent spaces of panel a using several methods (Fig. <ref type="figure" target="#fig_0">2d</ref>). To assess the information content available for downstream processing, we compared the average predictions of the trained classification heads for each input x i through the Jensen-Shannon divergence, ⟨JSD(p α (x|x i )||p β (x|x i )⟩ i . If classification heads for latent spaces α and β produce similar output distributions, the latent spaces likely share similar information content. Next, CKA <ref type="bibr" target="#b31">(Kornblith et al., 2019)</ref> can use different measures of representation similarity as its basis of comparison; we used the inner product between means of the posterior distributions for the representational similarity in linear CKA, an exponential function of the Euclidean distance for nonlinear CKA, and the Bhattacharyya coefficient between posteriors as a statistical basis of representational similarity. Nonlinear CKA requires selecting a distance parameter for each space: we used the arithmetic mean of the standard deviation of every representation. Next, we computed similarity according to stochastic shape metrics <ref type="bibr" target="#b11">(Duong et al., 2023)</ref>. Finally, we computed VI and NMI, both with a Monte Carlo approach and the significantly faster Bhattacharyya fingerprint-based approach.</p><p>Spearman's rank correlation of the similarity values between all pairs of synthetic spaces reveals multiple relationships between our proposed measures and the baselines. First, for this small dataset, the Bhattacharyya estimates of NMI and VI are highly consistent with the Monte Carlo alternatives while offering a speed up of 100×. Second, VI captures similarity much the same as when comparing the outputs of downstream classification heads (JSD) even though the former can be evaluated directly from the latent space. Third, NMI relates the latent spaces in much the same way as the Bhattacharyya variant of CKA. While this ad hoc variant of CKA lacks the information-theoretic underpinnings of NMI, it offers an easy-to-use alternative by simply replacing a point-based distance with a statistical distance between representations. We additionally observe differences between the methods in relative similarities between the latent spaces. All methods except linear CKA and stochastic shape metrics detect the similarity of the quasi-one-dimensional encodings (representation spaces i, iv, and vi). These encodings are not related by simple transformations, though a downstream neural network can extract similar information from each (see JSD). Representation spaces vii and ix are similar by information content, as variations of splitting the data into two groups, but they are deemed different by all but NMI, VI, and the Bhattacharyya-based CKA. While stochastic shape metrics satisfy the desirable properties of a metric, the generalized NMI/VI offer a measure of similarity more relevant for downstream processing owing to their utilization of mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised detection of structure: channel similarity</head><p>We next analyzed the consistency of information fragmentation in ensembles of generative models trained on image datasets. Using fifty models with ten latent dimensions (channels) each, for a variety of datasets, methods, and hyperparameters (some released with Locatello et al. ( <ref type="formula">2019</ref>)<ref type="foot" target="#foot_1">foot_1</ref> ), we assessed structure in an ensemble's channels. Every latent dimension of every model in an ensemble was compared pairwise to every other; we used the Bhattacharyya fingerprint approach with a sample of 1000 images randomly selected from the training set. Before using OPTICS to group latent dimensions by similarity (described in Sec. 3.3), we removed dimensions transmitting less than 0.01 bits of information. We found NMI to more successfully detect channels with similar information content, and use it in this section (comparison with VI in Appx. E).</p><p>In Fig. <ref type="figure">3</ref>, the matrices display the pairwise NMI between all informative dimensions in the ensemble, and have been reorganized by OPTICS such that highly similar latent dimensions are close together and appear as blocks along the diagonal. On the left of each similarity matrix is the NMI with the ground truth generative factors (or other label information), and above is the OPTICS reachability profile where identified groupings of consistently learned channels are indicated with shading and Roman numerals.</p><p>In Fig. <ref type="figure">3a</ref>, the regularization of a β-VAE is increased for the dsprites dataset <ref type="bibr" target="#b18">(Higgins et al., 2017)</ref>.</p><p>Although there are more channels that convey information for lower regularization (β=4), there is little discernible structure in the population of channels aside from a group of channels that communicate roughly the same information about scale. With β = 16, a block diagonal structure is found, and channels with the same xpos, ypos, and scale information are found repeatedly across runs in the ensemble. We applied the same analysis to an ensemble of InfoGAN-CR models <ref type="bibr" target="#b36">(Lin et al., 2020)</ref>, whose models additionally encoded shape information consistently. We approximated the latent distribution for an image by first encoding it, then re-generating 256 new images with the predicted latent dimensions fixed and the remaining (unconstrained) dimensions randomly resampled, and finally using the moments of the newly predicted latent representations as parameters for a Gaussian distribution. We note that in any scenario where a natural probability distribution exists per datum in the representation space, the method can be used.</p><p>Fig. <ref type="figure">3b</ref> shows remarkable consistency of information fragmentation by an ensemble of β-VAEs trained on the cars3d dataset <ref type="bibr" target="#b48">(Reed et al., 2015)</ref>, though not with regards to the provided generative factors. The object factor is broken repeatedly into the same set of information fragments, which is sensible given that it is a fine-grained factor of variation that comprises many lower-entropy factors such as color and height. More surprising is that three of the fragments contain a mix of camera pose information and object information that is consistently learned across the ensemble. The majority of existing disentanglement metrics rely on ground truth generative factors-including the FactorVAE score <ref type="bibr" target="#b27">(Kim &amp; Mnih, 2018)</ref>, mutual information gap (Chen et al., 2018), DCI <ref type="bibr" target="#b13">(Eastwood &amp; Williams, 2018)</ref>, and InfoMEC <ref type="bibr" target="#b20">(Hsu et al., 2023)</ref>-and would miss the consistency found when comparing the information content of individual latent dimensions.</p><p>Given a group of latent dimensions identified by OPTICS, we take as a representative the dimension that maximizes the average similarity to others in the group. Latent traversals visualize the fragmented information: groups i and iii communicate different partial information about both pose and color, while group vi conveys information about the color of the car with no pose information.</p><p>Is this particular way of fragmenting information about the cars3d dataset reproduced across different methods? In Fig. <ref type="figure">3c</ref>  Finally, we studied the manner of information fragmentation on datasets which are not simply an exhaustive set of combinations of generative factors (Fig. <ref type="figure">3d,</ref><ref type="figure">e</ref>). For β-VAE ensembles trained on fashion-mnist <ref type="bibr" target="#b67">(Xiao et al., 2017)</ref> and celebA <ref type="bibr" target="#b37">(Liu et al., 2015)</ref>, some information fragments are more consistently learned than others. A particular piece of information that distinguishes between shoes and clothing (group iii) was repeatedly learned for fashion-mnist; for celebA, a remarkably consistent fragment of information conveyed background color (group v). The remaining information was less consistently fragmented across the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Assessing the content of the full latent space</head><p>If an ensemble of models fragments information into channels inconsistently (e.g., Fig. <ref type="figure">3a</ref>), is it because the information content of the full latent space varies across runs? We compared the information contained in the full 10-dimensional latent spaces of five models from each VAE ensemble (Fig. <ref type="figure">4</ref>). The amount of information was generally too large for estimation via Bhattacharyya matrices, which saturates at the logarithm of the fingerprint size, so we used Monte Carlo estimates of the mutual information. Error bars in Fig. <ref type="figure">4</ref> are the propagated uncertainty from the mutual information measurements, with additional details in Appx. C. Whereas NMI proved more useful for the channel similarity analysis, VI tended to be more revealing about the heterogeneity of full latent spaces. Due to the form of its normalization, NMI becomes unreliable-i.e., is plagued by large uncertainty-when the information contained in the latent space I(U ; X) is small.</p><p>Across methods and hyperparameters for each method, the information contained in the full latent space was largely consistent over an ensemble (Fig. <ref type="figure">4a</ref>; models from <ref type="bibr" target="#b38">Locatello et al. (2019)</ref>). For the cars3d dataset, most latent spaces conveyed the full entropy of the dataset, meaning all representations were well-separated for the dataset of almost 18,000 images, making the information content trivially similar. By contrast, none of the models trained on smallnorb contained more than around 80% of the information about the dataset, and the similarity of latent spaces across members of an ensemble was more dependent on method and hyperparameter. Ensembles of β-VAE and β-TCVAE were fairly consistent even though the transmitted information dropped considerably with increasing β; DIP-I <ref type="bibr" target="#b33">(Kumar et al., 2018)</ref> showed the opposite behavior, where consistency varied strongly with λ od but transmitted information did not.</p><p>How does the consistency of latent space information evolve over the course of training? For β-VAE ensembles, we again compared the full latent spaces for five models with random initializations (Fig. <ref type="figure">4b</ref>). For all datasets considered (cars3d, smallnorb, celebA), consistency across the models dropped at around the same point in training that total information increased dramatically; after this point, all models encapsulated nearly the same information in their latent spaces up to convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model fusion in a toy example</head><p>Finally, we demonstrate the capacity for model fusion with the proposed measures of representation space similarity. Consider a dataset with a single generative factor with SO(2) symmetry, such as an object's hue. Difficulties can arise when the global structure of a degree of freedom is incompatible with the latent space <ref type="bibr" target="#b16">(Falorsi et al., 2018;</ref><ref type="bibr" target="#b69">Zhou et al., 2019;</ref><ref type="bibr" target="#b14">Esmaeili et al., 2024)</ref>. In this example, a one-dimensional latent space cannot represent the global circular topology of SO(2), leading to discontinuities in the representation.</p><p>To clearly demonstrate the synthesis of information from multiple representation spaces, we trained an ensemble of β-VAEs, each with a one-dimensional latent space that was insufficient to represent the global structure of the generative factor. Fig. <ref type="figure" target="#fig_2">5a</ref> shows an example latent space and its associated distinguishability matrix, showing pairwise Bhattacharyya coefficients between posterior distributions. The matrix reveals flaws in the latent space where similar values of the generative factor have dissimilar representations.</p><p>Assuming the flaws are randomly distributed from one training run to the next-which need not be true-the fusion of multiple such latent spaces might yield an improved representation of the generative factor. We performed gradient descent directly on posterior distributions in a two-dimensional latent space, with the objective to maximize average similarity with the ensemble. Namely, we computed either NMI or the exp(-VI) using the Bhattacharyya matrix corresponding to the optimizable representations-recalculated every training step-and those of the ensemble. As shown in Fig. <ref type="figure" target="#fig_2">5b,</ref><ref type="figure">c</ref>, the synthesized latent space more closely captured the global structure of the generative factor as the ensemble size grew. To quantify the performance, we employed the continuity metric used in <ref type="bibr" target="#b14">Esmaeili et al. (2024)</ref> and adapted from <ref type="bibr" target="#b16">Falorsi et al. (2018)</ref>, substituting the Bhattacharyya distance between posteriors for the Euclidean distance typically used for point-based representations (Appx. F).</p><p>Both NMI and VI proved effective in boosting an ensemble of weak representation spaces to represent the generative factor, with fidelity (measured by continuity) improving for larger ensembles. By comparison, directly maximizing the mutual information between the synthesis space and the ensemble, ⟨I(U ; V i )⟩ i , resulted in scattered representations with no overlap (Fig. <ref type="figure" target="#fig_2">5c</ref>, right). The normalization terms of NMI and VI were essential for preserving the relational structure between data points. Finally, it is worth noting that after training the ensemble of weak models, neither the original data nor the models were needed to train the synthesis space: the latent representations were optimized directly from the Bhattacharyya matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The processing of information can be directly assessed when representation spaces are probabilistic because information theoretic quantities are well-defined. In this work, we generalized two classic measures for comparing the information content of clustering assignments to make them applicable to probabilistic spaces. By focusing on the information content of a representation space, we can assess what information is available for downstream processing, while remaining agnostic to aspects of the spaces such as their dimensionality, their discrete or continuous nature, and even whether a space serves as auxiliary information like labels or annotations <ref type="bibr" target="#b45">(Newman &amp; Clauset, 2016;</ref><ref type="bibr" target="#b53">Savić, 2018;</ref><ref type="bibr" target="#b3">Bazinet et al., 2023)</ref>. While the examples in this work focused on relatively low-dimensional latent spaces that are common in practice, scaling to higher-dimensional c b a With differentiable formulations for NMI and VI, model fusion and the more general problem of representational alignment <ref type="bibr" target="#b56">(Sucholutsky et al., 2023;</ref><ref type="bibr" target="#b44">Muttenthaler et al., 2024)</ref> can be effectively approached from an information theoretic perspective. Potential applications include aligning representation spaces across models trained on different subsets of data, improving ensemble methods, and evaluating consistency of representations in multitask learning or domain adaptation, where reconciling heterogeneous latent spaces is often crucial.</p><p>A more subtle contribution of this work is to shift the current focus of unsupervised disentanglement evaluation. As is clear from the cars3d information fragmentation (Fig. <ref type="figure">3b</ref>), existing metrics that compare latent dimensions to ground truth generative factors can completely miss consistent information fragmentation. Unsupervised methods of evaluation <ref type="bibr" target="#b9">(Duan et al., 2020;</ref><ref type="bibr" target="#b36">Lin et al., 2020)</ref> assess consistency of fragmentation at the scale of models, which can obscure much about the manner of fragmentation. Consider the fragmentation of information about the celebA dataset in Fig. <ref type="figure">3e</ref>: an assessment of the similarity of models would find middling values across the ensemble and miss that two fragments of information are remarkably consistent. We argue that more fine-grained inspection of information fragmentation is essential for a deeper understanding of disentanglement in practice.</p><p>The current work largely focused on comparing repeat training runs in an ensemble, making computational costs a consideration. We found 50 models per ensemble to be sufficient to assess channel similarity, and 5 models per ensemble for the full latent space. We also found that there is much to learn from ensembles of relatively simple models. On our machine, training a single model from a recently proposed method (QLAE, <ref type="bibr" target="#b20">Hsu et al. (2023)</ref>) took more than five hours, and in the same amount of time we could train ten β-VAEs with a simpler architecture. Model fusion with weak models that are inexpensive to train, as in Sec. 4.4, might offer a promising alternative to representation learning with more computationally expensive models.  We include in Fig. <ref type="figure" target="#fig_4">11</ref> the full set of comparisons between the Bhattacharyya fingerprint and the average predictions of a trained classification head for the nine synthetic latent spaces of Sec. 4.1. Fig. <ref type="figure" target="#fig_5">12</ref> shows the pairwise scatter plots for the methods of comparing the latent spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Appendix: Information estimation using Bhattacharyya distinguishability matrices and Monte Carlo estimator</head><p>To estimate the mutual information I(U ; X) from the Bhattacharyya distinguishability matrices, we have employed the lower bound derived in <ref type="bibr" target="#b30">Kolchinsky &amp; Tracey (2017)</ref> for the information communicated through a channel about a mixture distribution (following the most updated version on arXiv<ref type="foot" target="#foot_2">foot_2</ref> ). The bound simplifies greatly when the empirical distribution is assumed to be a reasonable approximation for the data distribution, and then we further assume the sample of data used for the fingerprint allows for an adequate approximation of the marginal distribution in latent space. First we reproduce the bound from Sec. V of <ref type="bibr" target="#b30">Kolchinsky &amp; Tracey (2017)</ref> using the notation of this work, and then we describe our assumptions to apply the bound as an estimate of the information contained in a probabilistic representation space.</p><p>Let X be the input to a channel, following a mixture distribution with N components,</p><formula xml:id="formula_14">x ∼ p(x) = N i=1 c i p i (x), and U the output of the same channel, u ∼ p(u) = N i=1 c i X p(u|x)p i (x)dx . Then we have I(X; U ) ≥ - i c i ln j c j BC ij + H(U |C) -H(U |X),<label>(6)</label></formula><p>where C is a random variable representing the component identity.</p><p>In this work, we assume that the data distribution can be approximated by the empirical distribution, p(x) ≈ N i δ(x -x i )/N , simplifying Eqn. 6 so that c i ≡ 1/N and H(U |C) = H(U |X) because the identity of the component is equivalent to the identity of the datum. Finally, we assume that the set of posterior distributions for a representative sample of size M of the dataset, taken for the fingerprint, adequately approximates the empirical distribution. Larger samples may be necessary in different scenarios when the Figure <ref type="figure">13</ref>: Information estimates with Bhattacharyya fingerprints and Monte Carlo. For 500 channels randomly sampled from across all models released by <ref type="bibr" target="#b38">Locatello et al. (2019)</ref> for the cars3d dataset (including all methods and all hyperparameters), we estimated the amount of information transmitted by each channel I(U ; X) using the Bhattacharyya matrix fingerprints and Monte Carlo (MC) sampling. Error bars are displayed for the MC estimates, though they are generally smaller than the markers. The dashed black line represents equality between the two estimates, and the solid blue line is the logarithm of the fingerprint size, which is the saturation point for the Bhattacharyya estimate. Listed run times are for a single channel, excluding the time to load models but including inference and the calculation necessary for I(U ; X). amount of information transmitted by channels is larger than a handful of bits, but M = 1000 appeared sufficient for the analyses of this work.</p><p>The generalizations of NMI and VI required the information conveyed by two measurements from different channels, I(X; U, V ) as well as from the same channel, I(X; U, U ′ ). The matrix of Bhattacharyya coefficients given measurements U and V is simply the elementwise product of the coefficients given U and the coefficients given V . The posterior in the joint space of U and V is factorizable given x-i.e., p(u, v|x) = p(u|x)p(v|x)because the stochasticity in each channel is independent. The same is true for the joint space of U and U ′ , two draws from the same channel. The Bhattacharyya coefficient of the joint variable simplifies,</p><formula xml:id="formula_15">BC U V ij = U V p(u, v|x i )p(u, v|x j )dudv = U p(u|x i )p(u|x j )du V p(v|x i )p(v|x j )dv = BC U ij × BC V ij .<label>(7)</label></formula><p>For the Monte Carlo estimator, we sampled random data points from the dataset x ∼ p(x) and then a random embedding vector from each posterior distribution, u ∼ p(u|x). Then we computed the log ratio of the likelihood under the corresponding posterior p(u|x) and the aggregated posterior p(u), which was gotten by iterating over the entire dataset. We repeated N times to estimate the following expectation:</p><formula xml:id="formula_16">I(X; U ) = E x∼p(x) E u∼p(u|x) log p(u|x) p(u) (8)</formula><p>For the analysis of Fig. <ref type="figure">4</ref>, N = 2 × 10 5 for all datasets except dsprites and celebA, where N = 6 × 10 4 ; the standard error of the estimate was on the order of 0.01 bits or less.</p><p>Computing the fully aggregated posterior is time consuming; could a subsample of the dataset be used instead? In Fig. <ref type="figure" target="#fig_6">14</ref>, we varied the fraction of the dataset used to compute the (partially) aggregated posterior for a full 10-dimensional latent space for three models that transmit an intermediate amount of information about the dataset on which they were trained. Each point represents the mutual information estimate following Eqn. 8 for a random subset of the dataset, and the extent of its error bar is given by the standard error: the standard deviation of the set of values divided by the square root of the size of the set. We find that the transmitted information is relatively robust to using half of the dataset when computing the aggregated posterior, but deleterious effects grow when reducing the dataset by a factor of 10 or more. Interestingly, the dsprites estimate was largely insensitive to using 3% of the dataset, perhaps due to its highly structured nature.</p><p>In Fig. <ref type="figure">4</ref>, the NMI and VI values are averaged over all pairwise comparisons for the five representation spaces, with weights given by the inverse squared uncertainty on each pairwise value. The uncertainty was calculated by propagating the uncertainty of Monte Carlo estimates of I(X; U ) according to the expression for VI or NMI. To be specific, as VI(U, V ) = 2I(X; U, V ) -I(X; U, U ′ ) -I(X; V, V ′ ), the propagated uncertainty is given by ∆VI(U, V ) = 4∆I(X; U, V ) 2 -∆I(X; U, U ′ ) 2 -∆I(X; V, V ′ ) 2 , (9) with ∆I(X; U, V ) the standard error from the Monte Carlo estimate. The expression for the uncertainty on the NMI estimate contains many more terms, and we will not reproduce them here. Finally, the error bars in Fig. <ref type="figure">4</ref> are the standard error of the weighted mean,</p><formula xml:id="formula_17">∆Q = n i (∆Q i ) -2 -1 2 , (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>for quantity Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Appendix: Broken triangle inequality for the generalized VI</head><p>Here we provide an example of three clusterings that breaks the triangle inequality for the generalized VI. Thus, while VI for hard clusters satisfies the properties of a metric <ref type="bibr" target="#b8">(Crutchfield, 1990)</ref>, the generalization to soft clusters does not.</p><p>In Fig. <ref type="figure" target="#fig_7">15</ref>, there are three clusterings (U , V , and W ) of four equally likely data points (x ∈ {a, b, c, d}). U , V , and W each communicate one bit of information about X, but U and W are hard clusterings while V is a soft clustering. We have I(U ; V ) = I(V ; W ) = 0.5 bits, I(U ; W ) = 0 bits, I(V ; V ′ ) = 0.5 bits, and I(U ; U ′ ) = I(W ; W ′ ) = H(U ) = 1 bit. The generalized VI from U to V and then from V to W is less than from U to W directly. Figure <ref type="figure">16</ref>: Direct comparison of NMI and VI. We convert both measures to a distance measure (black) and to a similarity measure (blue gray) and compare them for the pairwise channel comparisons from the ensembles of Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Appendix: Are NMI and VI interchangeable?</head><p>NMI and VI, aside from the inversion required to convert from similarity to distance, can both be seen as a normalized mutual information. Are they interchangeable, or do they assess structure differently?</p><p>In Fig. <ref type="figure">16</ref>, we compare NMI and VI (estimated via Bhattacharyya matrices) as similarity or distance measures for the pairwise comparisons between channels used in Fig. <ref type="figure">3</ref>. Specifically, as measures of similarity, we plot NMI against exp(-VI), and for distance we plotlog(NMI) against VI. We find that NMI and VI are non-trivially related, shown clearly by the horizontal and vertical swaths of points where one of the two measures is roughly constant while the other varies considerably. Interestingly, the corresponding NMI and VI comparisons in Fig. <ref type="figure">16</ref> show multiple distinct arcs of channel similarity, as well as clear vertical bands where NMI has discerning power and VI does not.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing similarity measures for synthetic embedding spaces. (a) A dataset of 64 points, x = 1, ..., 64, is transformed into nine representation spaces marked i-ix. Each posterior distribution p(u|x) is a Gaussian with diagonal covariance matrix and standard deviations indicated by the colored ellipses. (b) We trained a classification head on top of the latent spaces of a to predict the input x, given a sample from the posterior distribution p(u|x). The predicted probability distributions p(x j |x i ) are displayed as a matrix, with row corresponding to the input x i . (c) The pairwise distinguishability of data points x i and x j , as computed by the Bhattacharyya coefficient, serves as a fingerprint of the information content of the latent space. (d) The pairwise similarity of the representation spaces in panel a, found by a variety of methods. Runtimes to calculate the full matrix are shown above each method, except for Jensen-Shannon divergence (JSD) because it required training an additional classification network on top of each latent space. The stochastic shape metric requires the dimensionality of the compared spaces to match; undefined entries are grayed out. The Spearman rank correlation between similarity measures is shown in the bottom right.</figDesc><graphic coords="7,450.89,450.70,85.29,85.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Assessing the consistency of channel information in ensembles of models. We used NMI as a similarity measure for OPTICS to detect fragments of information that are consistently stored in individual channels in an ensemble of trained models. (a) The channel consistency of models trained on dsprites, for β-VAE and InfoGAN-CR. The information with respect to generative factors is shown on the left of each similarity matrix. The β = 4 β-VAE fragmented information inconsistently compared to the other two ensembles. (b) For a β-VAE ensemble trained on cars3d, the information content of channels was highly consistent, with seven distinct combinations of the three generative factors. Latent traversals for a representative channel from each grouping visualize the information content. (c) We compare the information content of the representatives from panel b to that of channels in β-TCVAE and FactorVAE ensembles. (d,e) Channel similarity and latent traversals for β-VAE ensembles trained on fashion-mnist and celebA. Additional channel similarity analyses and latent traversals can be found in Appx. A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fusing weak representation spaces. (a) Example of a one-dimensional latent space of a β-VAE trained on a dataset generated from a single periodic factor (color hue), which has SO(2) symmetry. The latent space exhibits flaws where similar values of the generative factor are mapped to dissimilar representations, as seen in the posterior distributions (left) and the distinguishability matrix of Bhattacharyya coefficients between posteriors, BC ij (right). (b) We optimized a synthesis representation space to maximize similarity with an ensemble of such one-dimensional latent spaces. The continuity of statistical distances between neighboring points, an assessment of the fidelity of the global structure of the generative factor, improved as the ensemble size grew. Error bars show the standard deviation over five experiments, and values are offset horizontally for visibility. (c) Synthesized two-dimensional representation spaces (posterior means shown as points; covariances as shaded ellipses) and their corresponding distinguishability matrices. Panels compare results when maximizing average NMI (left, middle) and mutual information (right).</figDesc><graphic coords="12,464.82,175.03,74.37,74.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :Figure 8 :Figure 9 :Figure</head><label>6789</label><figDesc>Figure 6: Latent traversals for cars3d groups from Fig. 3b,c. We traverse the representative channels for the groups found by OPTICS, and reorder them to align with the ordering for β-VAE (left). Note that group vi splits in two groups for both the β-TCVAE and the FactorVAE. The tint of the windows and the color of the car were encoded jointly for the β-VAE, and then separately for the other two methods. Traversals are over the range [-2, 2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Full comparison of Bhattacharyya fingerprints and classification outputs from latent spaces of Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Pairwise scatter plot comparison of similarity measures from Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Partially aggregated posterior for the Monte Carlo estimator. For three models selected from those of Fig. 4, we seleced random subsets of the full dataset to use when computing the aggregated posterior for estimating the transmitted information I(U ; X) of the full 10-dimensional latent space. The points are offset horizontally by random amounts for visibility, and the error bars show the standard error on the mean of the Monte Carlo samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Example that breaks the triangle inequality for generalized VI. Four datapoints a, b, c, d are clustered according to scheme U , V , or W . The total VI from U to V and then V to W is less than the VI from U to W .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Published in Transactions on Machine LearningResearch (02/2025)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https://github.com/google-research/disentanglement_lib/tree/master</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://arxiv.org/abs/1706.02419</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://github.com/google-research/disentanglement_lib/tree/master</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://github.com/fjxmlzn/InfoGAN-CR</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>https://github.com/ahwillia/netrep</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: Extended channel similarity results</head><p>In Fig. <ref type="figure">6</ref> we present additional latent traversals for the cars3d channel groupings presented in Fig. <ref type="figure">3c</ref>. With the channels most centrally located in each group (as before), we also display latent traversals for β-TCVAE (β=10) and FactorVAE (γ = 30). Fig. <ref type="figure">7</ref> visualizes the channel similarity structure on mnist <ref type="bibr" target="#b34">(LeCun et al., 2010)</ref> and fashion-mnist, with latent traversals for four channels per grouping to show the consistency of the encoded information.</p><p>In <ref type="bibr">Figs. 8,</ref><ref type="bibr">9</ref>, and 10, we repeat the structural analysis of Sec. 4.2 with all β-VAE hyperparameters explored by the authors of <ref type="bibr" target="#b38">Locatello et al. (2019)</ref>. The effect of increasing β is clearly observed by the emergence of block diagonal channel similarity matrices, though with fewer informative channels for increased β.</p><p>Consider the cars3d ensembles in Fig. <ref type="figure">8</ref>. With β = 1, 2, the information fragmentation is not consistent across repeat runs, even though the amount of information shared with the three generative factors is fairly consistent. This highlights the value of directly comparing the information content of channels with NMI or VI instead of comparing indirectly via information content about known generative factors, with the added benefit of being fully unsupervised. For β = 4 the learned fragments of information start to coalesce, with information regularization breaking the degeneracy that plagues unsupervised disentanglement <ref type="bibr" target="#b38">(Locatello et al., 2019)</ref>. For β = 8, 16, the regularization is strong enough to form consistent fragments of information across random initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Appendix: Implementation specifics</head><p>Code to reproduce the experiments of Sec.</p><p>4 can be found at the following repository: <ref type="url" target="https://github.com/murphyka/representation-space-info-comparison">https://github.com/murphyka/representation-space-info-comparison</ref>. The heart of the codebase is in utils.py, containing the Bhattacharyya and Monte Carlo calculations of I(U ; X) and the NMI/VI calculations.</p><p>All experiments were implemented in TensorFlow and run on a single computer with a 12 GB GeForce RTX 3060 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models:</head><p>For the dsprites, smallnorb, and cars3d datasets, we used the trained models that were publicly released by the authors of <ref type="bibr" target="#b38">Locatello et al. (2019)</ref>. Thus, all of the model and channel numbers recorded above the latent traversals in Fig. <ref type="figure">3b</ref> correspond to models that can be downloaded from that paper's github page 3 . Simply add the model offset corresponding to the β = 16 β-VAE for cars3d, 9250 (e.g., for the traversal labeled with model 31 ch 3, download model 9281 and traverse latent dimension 3, 0 indexed).</p><p>For the InfoGAN-CR models on dsprites, we used the trained models that were uploaded with Lin et al. ( <ref type="formula">2020</ref>) 4 .</p><p>For results on the training progression of smallnorb, celebA, and cars3d, we used the same architecture and training details from <ref type="bibr" target="#b38">Locatello et al. (2019)</ref>.</p><p>For the MNIST and Fashion-MNIST ensembles, we trained 50 β-VAEs with a 10-dimensional latent space. The encoder had the following architecture: The models were trained for 2 × 10 5 steps, with a Bernoulli loss on the pixels, the Adam optimizer with a learning rate of 10 -4 , and a batch size of 64.</p><p>Clustering analysis: We used the OPTICS implementation from sklearn 5 with 'precomputed' distance metric and min_samples= 20 (and all other parameters their default values). For distance matrices we converted NMI to a distance withlog max(NMI, 10 -4 ).</p><p>Ensemble learning: For the ensemble learning toy problem (Sec. 4.4), we trained 250 simple β-VAEs (β=0.03) whose encoder and decoder were each fully connected networks with two layers of 256 tanh activation. The input was two-dimensional, the latent space was one-dimensional, and the output was two-dimensional.</p><p>Published in Transactions on Machine Learning Research (02/2025)</p><p>The loss was MSE, the optimizer was Adam with learning rate 10 -3 , and the batch size was 2048, trained for 3000 steps. Data was sampled anew each batch, uniformly at random from the unit circle.</p><p>To perform ensemble learning, we evaluated the Bhattacharyya matrices for 200 evenly spaced points around the unit circle for each model in the ensemble. Then we directly optimized the parameters for 200 posterior distributions (Gaussians with diagonal covariance matrices) in a two-dimensional latent space, so as to maximize the average similarity (NMI, exponentiated negative VI, or mutual information) between the Bhattacharyya matrix for the trainable embeddings and those of the ensemble. We used SGD with a learning rate of 3 for 20,000 iterations, and repeated for 5 trials for each ensemble size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic shape metrics:</head><p>We used publicly released code on github 6 , using the GaussianStochasticMetric with α = 1 and the parallelized pairwise distances for the timing calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CKA:</head><p>We replaced the dot-product similarity matrices K and L in the Hilbert-Schmidt Independence Criterion (HSIC) with the Bhattacharyya matrices, HSIC(BC (1) , BC (2) ) = 1 (n -1) 2 T r(BC (1) </p><p>and then followed the prescribed normalization in <ref type="bibr" target="#b31">Kornblith et al. (2019)</ref>.</p><p>Continuity metric: <ref type="bibr" target="#b16">Falorsi et al. (2018)</ref> used the ratio of neighbor distances in representation space to the corresponding distances in data space, with neighbors taken along continuous paths in data space, as the basis for a discrete continuity metric. It indicated whether any ratios were above some multiplicative factor of some percentile value in the distribution, and thus depended on two parameter choices. <ref type="bibr" target="#b14">Esmaeili et al. (2024)</ref> removed one of the parameters-the multiplicative factor-yielding a continuous continuity metric that reports the maximum ratio value over the 90 th percentile value.</p><p>The central premise of this work is to respect the nature of representations as probability distributions, so we used the Bhattacharyya distance (i.e., D ij = -log BC ij )) between posteriors instead of Euclidean distances between posterior means in representation space. Other than this modification, we left the continuity metric as in <ref type="bibr" target="#b14">Esmaeili et al. (2024)</ref>: as the maximum ratio value over the 90 th percentile value.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On assessments of agreement between fuzzy partitions</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Jeffrey L Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsey</forename><forename type="middle">D</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><surname>Hvingelby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="326" to="342" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optics: Ordering points to identify the clustering structure</title>
		<author>
			<persName><forename type="first">Mihael</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod record</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Balabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Voronkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serguei</forename><surname>Barannikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12696</idno>
		<title level="m">Disentanglement learning via topology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards a biologically annotated brain connectome</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Bazinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justine</forename><forename type="middle">Y</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bratislav</forename><surname>Misic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="747" to="760" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Orthopartitions and soft clustering: soft mutual information measures for clustering validation</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Campagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Ciucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A general framework for evaluating and comparing soft clusterings</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Campagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Ciucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">623</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="70" to="93" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information and its metric</title>
		<author>
			<persName><forename type="first">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Crutchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear Structures in Physical Systems: Pattern Formation, Chaos, and Waves Proceedings of the Second Woodward Conference San Jose State University</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">November 17-18, 1989. 1990</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised model selection for variational disentangled representation learning</title>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Saraiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006">2020. 2, 3, 6</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A fuzzy relative of the isodata process and its use in detecting compact well-separated clusters</title>
		<author>
			<persName><surname>Joseph C Dunn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representational dissimilarity metric spaces for stochastic neural networks</title>
		<author>
			<persName><forename type="first">Lyndon</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josue</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jules</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Olieslagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xjb563TH-GH" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adjusted concordance index: an extensionl of the adjusted rand index to fuzzy partitions</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmela</forename><surname>Amodio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Iorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Pandolfo</surname></persName>
		</author>
		<author>
			<persName><surname>Siciliano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="112" to="128" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A framework for the quantitative evaluation of disentangled representations</title>
		<author>
			<persName><forename type="first">Cian</forename><surname>Eastwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topological obstructions and how to avoid them</title>
		<author>
			<persName><forename type="first">Babak</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dava: Disentangling adversarial variational autoencoder</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Estermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno>2023. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco S</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04689</idno>
		<title level="m">Explorations in homeomorphic variational auto-encoding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What shapes feature representations? exploring datasets, architectures, and training</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lampinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9995" to="10006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02230</idno>
		<title level="m">Towards a definition of disentangled representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disentanglement via latent quantization</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dorrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tripod: Three complementary inductive biases for disentangled representation learning</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibn</forename><surname>Jubayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaylee</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10282</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Locally weighted ensemble clustering</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1473" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.07987</idno>
		<title level="m">The platonic representation hypothesis</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparing fuzzy partitions: A generalization of the rand index and related measures</title>
		<author>
			<persName><forename type="first">Eyke</forename><surname>Hullermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Rifqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Henzgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Senge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="546" to="556" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The divergence and bhattacharyya distance measures in signal selection</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on communication technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ica: A unifying framework</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="2207" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2006">2018. 3, 6</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Max</forename><surname>Klabunde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Strohmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Lemmerich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06329</idno>
		<title level="m">Similarity of neural network models: A survey of functional and representational measures</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimating mixture entropy with pairwise distances</title>
		<author>
			<persName><forename type="first">Artemy</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2006">2019. 1, 3, 6</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representational similarity analysis-connecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in systems neuroscience</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational inference of disentangled latent concepts from unlabeled observations</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1kG7GZAW.11" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnist handwritten digit database</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="j">ATT Labs</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The topology and geometry of neural representations</title>
		<author>
			<persName><forename type="first">Baihan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2024">2317881121. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Infogan-cr and modelcentrality: Selfsupervised model training and selection for disentangling gans</title>
		<author>
			<persName><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2020">2020. 2, 3, 8, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15" />
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2019">2019. 2, 3, 8, 10, 18, 25</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The training process of many deep networks explores the same lowdimensional manifold</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Griniasty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Kheng Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">P</forename><surname>Mark K Transtrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Sethna</surname></persName>
		</author>
		<author>
			<persName><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2310002121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Formal limitations on the measurement of mutual information</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<idno>PMLR, 2020. 12</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="875" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparing clusterings by the variation of information</title>
		<author>
			<persName><forename type="first">Marina</forename><surname>Meilă</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-08-24">2003. August 24-27, 2003. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpretability with full complexity by constraining feature information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kieran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><forename type="middle">S</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><surname>Bassett</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=R_OL5mLhsv.5" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning abcs: Approximate bijective correspondence for isolating factors of variation with weak supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kieran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikumar</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameesh</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><surname>Makadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16010" to="16020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Muttenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieda</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><surname>Lampinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.06509</idno>
		<title level="m">Aligning machine and human visual representations across abstraction levels</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure and inference in annotated networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11863</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/poole19a/poole19a.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Soft cluster ensembles</title>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Punera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in fuzzy clustering and its applications</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="69" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep visual analogy-making</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational autoencoders pursue pca directions (by accident)</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Rolinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12406" to="12415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fuzzy clustering: A historical perspective</title>
		<author>
			<persName><forename type="first">Enrique</forename><forename type="middle">H</forename><surname>Ruspini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning disentangled representations via mutual information estimation</title>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">Hugo</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Serrurier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automatic extraction of clusters from hierarchical clustering representations</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kovarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining: 7th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-04-30">2003. April 30-May 2, 2003 Proceedings 7. 2003</date>
			<biblScope unit="page" from="75" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analysis of annotated social and information networks: Methods and applications</title>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Savić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 8th International Conference on Web Intelligence, Mining and Semantics</meeting>
		<imprint>
			<date type="published" when="2011">2018. 11</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The lattice theory of information</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the IRE professional Group on Information Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="107" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002-12-03">Dec. 2002. 3, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Muttenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Bradley C Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iris</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Groen</surname></persName>
		</author>
		<author>
			<persName><surname>Achterberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13018</idno>
		<title level="m">Getting aligned on representational alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On disentangled representations learned from correlated data</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Träuble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Dittadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10401" to="10412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Are disentangled representations helpful for abstract visual reasoning? Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey of clustering ensemble algorithms</title>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Ruiz-Shulcloper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="337" to="372" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: is a correction for chance necessary?</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v11/vinh10a.html.3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">95</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Nestedvae: Isolating common factors via weak supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necati</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9202" to="9212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Comparing clusterings: an overview</title>
		<author>
			<persName><forename type="first">Silke</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of fuzzy clustering validity evaluation methods</title>
		<author>
			<persName><forename type="first">Hong-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie-Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="297" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Equivalence between representational similarity analysis, centered kernel alignment, and canonical correlations analysis</title>
		<author>
			<persName><forename type="first">Alex H</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=zMdnnFasgC.3" />
	</analytic>
	<monogr>
		<title level="m">UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Generalized shape metrics on neural representations</title>
		<author>
			<persName><forename type="first">Alex H</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4738" to="4750" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Lotfi Asker Zadeh. Fuzzy sets. Information and control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="353" />
			<date type="published" when="1965">1965. 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Demystifying inductive biases for (beta-)vae based architectures</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Rolinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/zietlow21a.html.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
