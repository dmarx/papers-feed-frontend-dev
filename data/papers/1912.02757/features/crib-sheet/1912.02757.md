- **Deep Ensembles Overview**: Empirically improve accuracy, uncertainty, and robustness in deep learning models; inspired by bootstrap but effective with random initialization.
  
- **Bayesian Neural Networks (BNNs)**: Learn distributions over parameters; MAP estimation provides point estimates but lacks distributional information.

- **Loss Landscape**: High-dimensional function space; empirical results show simple properties, e.g., monotonic loss decrease along linear paths.

- **Diversity-Accuracy Plane**: Concept developed to illustrate the relationship between diversity of predictions and accuracy in ensembles.

- **Function Space Diversity**: Random initializations explore different modes in function space, while single trajectories or subspaces yield similar predictions.

- **Similarity Metrics**:
  - **Cosine Similarity in Weight Space**: \( \text{cos}(\theta_1, \theta_2) = \frac{\theta_1 \cdot \theta_2}{||\theta_1|| \, ||\theta_2||} \)
  - **Disagreement in Function Space**: \( 1 - \frac{1}{N} \sum_{n=1}^{N} [f(x_n; \theta_1) = f(x_n; \theta_2)] \)

- **Subspace Sampling Methods**:
  - **Random Subspace**: Explore directions from optimized solution \( \theta_0 \).
  - **Dropout Subspace**: Apply dropout with varying keep probabilities.
  - **Diagonal Gaussian**: Sample from \( \theta_i \sim N(\mu_i, \sigma_i) \) for each parameter.
  - **Low-Rank Gaussian**: Use PCA to sample from a low-rank approximation of the covariance.

- **Empirical Findings**: 
  - Random initializations yield diverse solutions; subspace methods often cluster predictions.
  - Deep ensembles outperform BNNs in accuracy and uncertainty, especially under dataset shift.

- **Evaluation Datasets**: CIFAR-10, CIFAR-100, ImageNet; performance assessed on corrupted datasets (CIFAR-10-C, ImageNet-C) for robustness.

- **Key References**:
  - Lakshminarayanan et al. (2017) on deep ensembles.
  - Ovadia et al. (2019) on uncertainty quantification benchmarks.
  - Fort and Jastrzebski (2019) on loss landscape properties.

- **Experimental Setup**: 
  - Architectures: SmallCNN, MediumCNN, ResNet20v1; training details (epochs, batch size, optimizer).
  - Consistent trends across architectures and datasets.

- **Visualizations**: Use t-SNE plots to illustrate prediction space similarity across different methods and initializations.