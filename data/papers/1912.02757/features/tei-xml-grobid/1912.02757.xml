<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Ensembles: A Loss Landscape Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-06-25">25 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
							<email>sfort1@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Deep Ensembles: A Loss Landscape Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-25">25 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">1421B9AE1EFF85244C955BAA4A0DA3CB</idno>
					<idno type="arXiv">arXiv:1912.02757v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity-accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider a typical classification problem, where x n ∈ R D denotes the D-dimensional features and y n ∈ [1, . . . , K] denotes the class label. Assume we have a parametric model p(y|x, θ) for the conditional distribution where θ denotes weights and biases of a neural network, and p(θ) is a prior distribution over parameters. The Bayesian posterior over parameters is given by p(θ|{x n , y n } N n=1 ) ∝ p(θ) N n=1 p(y n |x n , θ). Computing the exact posterior distribution over θ is computationally expensive (if not impossible) when p(y n |x n , θ) is a deep neural network (NN). A variety of approximations have been developed for Bayesian neural networks, including Laplace approximation <ref type="bibr" target="#b0">[MacKay, 1992]</ref>, Markov chain Monte Carlo methods <ref type="bibr" target="#b1">[Neal, 1996</ref><ref type="bibr" target="#b2">, Welling and Teh, 2011</ref><ref type="bibr" target="#b3">, Springenberg et al., 2016]</ref>, variational Bayesian methods <ref type="bibr" target="#b4">[Graves, 2011</ref><ref type="bibr" target="#b5">, Blundell et al., 2015</ref><ref type="bibr" target="#b6">, Louizos and Welling, 2017</ref><ref type="bibr" target="#b7">, Wen et al., 2018]</ref> and Monte-Carlo dropout <ref type="bibr">[Gal and</ref><ref type="bibr">Ghahramani, 2016, Srivastava et al., 2014]</ref>. While computing the posterior is challenging, it is usually easy to perform maximum-a-posteriori (MAP) estimation, which corresponds to a mode of the posterior. The MAP solution can be written as the minimizer of the following loss: (1)</p><p>The MAP solution is computationally efficient, but only gives a point estimate and not a distribution over parameters. Deep ensembles, proposed by <ref type="bibr" target="#b10">Lakshminarayanan et al. [2017]</ref>, train an ensemble of neural networks by initializing at M different values and repeating the minimization multiple times which could lead to M different solutions, if the loss is non-convex. <ref type="bibr" target="#b10">Lakshminarayanan et al. [2017]</ref> found adversarial training provides additional benefits in some of their experiments, but we will ignore adversarial training and focus only on ensembles with random initialization.</p><p>Given finite training data, many parameter values could equally well explain the observations, and capturing these diverse solutions is crucial for quantifying epistemic uncertainty <ref type="bibr" target="#b11">[Kendall and Gal, 2017]</ref>. Bayesian neural networks learn a distribution over weights, and a good posterior approximation should be able to learn multi-modal posterior distributions in theory. Deep ensembles were inspired by the bootstrap <ref type="bibr" target="#b12">[Breiman, 1996]</ref>, which has useful theoretical properties. However, it has been empirically observed by <ref type="bibr" target="#b10">Lakshminarayanan et al. [2017]</ref>, <ref type="bibr" target="#b13">Lee et al. [2015]</ref> that training individual networks with just random initialization is sufficient in practice and using the bootstrap can even hurt performance (e.g. for small ensemble sizes). Furthermore, <ref type="bibr" target="#b14">Ovadia et al. [2019]</ref> and Gustafsson et al.</p><p>[2019] independently benchmarked existing methods for uncertainty quantification on a variety of datasets and architectures, and observed that ensembles tend to outperform approximate Bayesian neural networks in terms of both accuracy and uncertainty, particularly under dataset shift.</p><p>Figure <ref type="figure">1</ref>: Cartoon illustration of the hypothesis.</p><p>x-axis indicates parameter values and y-axis plots the negative loss -L(θ, {x n , y n } N n=1 ) on train and validation data.</p><p>These empirical observations raise an important question: Why do deep ensembles trained with just random initialization work so well in practice?</p><p>One possible hypothesis is that ensembles tend to sample from different modes<ref type="foot" target="#foot_0">foot_0</ref> in function space, whereas variational Bayesian methods (which minimize D KL (q(θ)|p(θ|{x n , y n } N n=1 )) might fail to explore multiple modes even though they are effective at capturing uncertainty within a single mode. See Figure <ref type="figure">1</ref> for a cartoon illustration. Note that while the MAP solution is a local optimum for the training loss,it may not necessarily be a local optimum for the validation loss.</p><p>Recent work on understanding loss landscapes <ref type="bibr" target="#b16">[Garipov et al., 2018</ref><ref type="bibr" target="#b17">, Draxler et al., 2018</ref><ref type="bibr" target="#b18">, Fort and Jastrzebski, 2019]</ref> allows us to investigate this hypothesis. Note that prior work on loss landscapes has focused on mode-connectivity and low-loss tunnels, but has not explicitly focused on how diverse the functions from different modes are. The experiments in these papers (as well as other papers on deep ensembles) provide indirect evidence for this hypothesis, either through downstream metrics (e.g. accuracy and calibration) or by visualizing the performance along the low-loss tunnel. We complement these works by explicitly measuring function space diversity within training trajectories and subspaces thereof (dropout, diagonal Gaussian, low-rank Gaussian and random subspaces) and across different randomly initialized trajectories across multiple datasets, architectures, and dataset shift. Our findings show that the functions sampled along a single training trajectory or subspace thereof tend to be very similar in predictions (while potential far away in the weight space), whereas functions sampled from different randomly initialized trajectories tend to be very diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The loss landscape of neural networks (also called the objective landscape) -the space of weights and biases that the network navigates during training -is a high dimensional function and therefore could potentially be very complicated. However, many empirical results show surprisingly simple properties of the loss surface. <ref type="bibr" target="#b19">Goodfellow and Vinyals [2014]</ref> observed that the loss along a linear path from an initialization to the corresponding optimum is monotonically decreasing, encountering no significant obstacles along the way. <ref type="bibr" target="#b20">Li et al. [2018]</ref> demonstrated that constraining optimization to a random, low-dimensional hyperplane in the weight space leads to results comparable to full-space optimization, provided that the dimension exceeds a modest threshold. This was geometrically understood and extended by <ref type="bibr" target="#b21">Fort and Scherlis [2019]</ref>. <ref type="bibr" target="#b16">Garipov et al. [2018]</ref>, <ref type="bibr" target="#b17">Draxler et al. [2018]</ref> demonstrate that while a linear path between two independent optima hits a high loss area in the middle, there in fact exist continuous, low-loss paths connecting any pair of optima (or at least any pair empirically studied so far). These observations are unified into a single phenomenological model in <ref type="bibr" target="#b18">[Fort and Jastrzebski, 2019]</ref>. While low-loss tunnels create functions with near-identical low values of loss along the path, the experiments of <ref type="bibr" target="#b18">Fort and Jastrzebski [2019]</ref>, <ref type="bibr" target="#b16">Garipov et al. [2018]</ref> provide preliminary evidence that these functions tend to be very different in function space, changing significantly in the middle of the tunnel, see Appendix A for a review and additional empirical evidence that complements their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setup</head><p>We explored the CIFAR-10 [ <ref type="bibr" target="#b22">Krizhevsky, 2009]</ref>, CIFAR-100 <ref type="bibr" target="#b22">[Krizhevsky, 2009]</ref>, and ImageNet <ref type="bibr" target="#b23">[Deng et al., 2009]</ref> datasets. We train convolutional neural networks on the CIFAR-10 dataset, which contains 50K training examples from 10 classes. To verify that our findings translate across architectures, we use the following 3 architectures on CIFAR-10:</p><p>• SmallCNN: channels <ref type="bibr">[16,</ref><ref type="bibr">32,</ref><ref type="bibr">32]</ref> for 10 epochs which achieves 64% test accuracy.</p><p>• MediumCNN: channels <ref type="bibr">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">256]</ref> for 40 epochs which achieves 71% test accuracy.</p><p>• ResNet20v1 <ref type="bibr">[He et al., 2016a]</ref>: for 200 epochs which achieves 90% test accuracy.</p><p>We use the Adam optimizer <ref type="bibr" target="#b25">[Kingma and Ba, 2015]</ref> for training and to make sure the effects we observe are general, we validate that our results hold for vanilla stochastic gradient descent (SGD) as well (not shown due to space limitations). We use batch size 128 and dropout 0.1 for training SmallCNN and MediumCNN. We used 40 epochs of training for each. To generate weight space and prediction space similarity results, we use a constant learning rate of 1.6 × 10 -3 and halfing it every 10 epochs, unless specified otherwise. We do not use any data augmentation with those two architectures. For ResNet20v1, we use the data augmentation and learning rate schedule used in Keras examples. <ref type="foot" target="#foot_1">4</ref> The overall trends are consistent across all architectures, datasets, and other hyperparameter and non-linearity choices we explored.</p><p>To test if our observations generalize to other datasets, we also ran certain experiments on more complex datasets such as CIFAR-100 <ref type="bibr" target="#b22">[Krizhevsky, 2009]</ref> which contains 50K examples belonging to 100 classes and ImageNet <ref type="bibr" target="#b23">[Deng et al., 2009]</ref>, which contains roughly 1M examples belonging to 1000 classes. CIFAR-100 is trained using the same ResNet20v1 as above with Adam optimizer, batch size 128 and total epochs of 200. The learning rate starts from 10 -3 and decays to (10 -4 , 5 × 10 -5 , 10 -5 , 5 × 10 -7 ) at epochs <ref type="bibr">(100,</ref><ref type="bibr">130,</ref><ref type="bibr">160,</ref><ref type="bibr">190)</ref>. ImageNet is trained with ResNet50v2 <ref type="bibr">[He et al., 2016b]</ref> and momentum optimizer (0.9 momentum), with batch size 256 and 160 epochs. The learning rate starts from 0.15 and decays to (0.015, 0.0015) at epochs (80, 120).</p><p>In addition to evaluating on regular test data, we also evaluate the performance of the methods on corrupted versions of the dataset using the CIFAR-10-C and ImageNet-C benchmarks <ref type="bibr" target="#b27">[Hendrycks and Dietterich, 2019]</ref> which contain corrupted versions of original images with 19 corruption types (15 for ImageNet-C) and varying intensity values (1-5), and was used by <ref type="bibr" target="#b14">Ovadia et al. [2019]</ref> to measure calibration of the uncertainty estimates under dataset shift. Following <ref type="bibr" target="#b14">Ovadia et al. [2019]</ref>, we measure accuracy as well as Brier score <ref type="bibr" target="#b28">[Brier, 1950]</ref> (lower values indicate better uncertainty estimates). We use the SVHN dataset <ref type="bibr" target="#b28">[Netzer et al., 2011]</ref> to evaluate how different methods trained on CIFAR-10 dataset react to out-of-distribution (OOD) inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visualizing Function Space Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Similarity of Functions Within and Across Randomly Initialized Trajectories</head><p>First, we compute the similarity between different checkpoints along a single trajectory. In Figure <ref type="figure" target="#fig_1">2</ref>(a), we plot the cosine similarity in weight space, defined as cos(θ 1 , θ 2 ) = θ 1 θ2 ||θ1||||θ2|| . In Figure <ref type="figure" target="#fig_1">2</ref>(b), we plot the disagreement in function space, defined as the fraction of points the checkpoints disagree on, that is, 1</p><formula xml:id="formula_0">N N n=1 [f (x n ; θ 1 ) = f (x n ; θ 2 )]</formula><p>, where f (x; θ) denotes the class label predicted by the network for input x. We observe that the checkpoints along a trajectory are largely similar both in the weight space and the function space. Next, we evaluate how diverse the final solutions from different random initializations are. The functions from different initialization are different, as demonstrated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity of Functions Within Subspace from Each Trajectory and Across Trajectories</head><p>In addition to the checkpoints along a trajectory, we also construct subspaces based on each individual trajectory. Scalable variational Bayesian methods typically approximate the distribution of weights along the training trajectory, hence visualizing the diversity of functions between the subspaces helps understand the difference between Bayesian neural networks and ensembles. We use a representative set of four subspace sampling methods: Monte Carlo dropout, a diagonal Gaussian approximation, a low-rank covariance matrix Gaussian approximation and a random subspace approximation. Unlike dropout and Gaussian approximations which assume a parametric form for the variational posterior, the random subspace method explores all high-quality solutions within the subspace and hence could be thought of as a non-parametric variational approximation to the posterior. Due to space constraints, we do not consider Markov Chain Monte Carlo (MCMC) methods in this work; <ref type="bibr" target="#b30">Zhang et al. [2020]</ref> show that popular stochastic gradient MCMC (SGMCMC) methods may not explore multiple modes and propose cyclic SGMCMC. We compare diversity of random initialization and cyclic SGMCMC in Appendix C. In the descriptions of the methods, let θ 0 be the optimized weight-space solution (the weights and biases of our trained neural net) around which we will construct the subspace.</p><p>• Random subspace sampling: We start at an optimized solution θ 0 and choose a random direction v in the weight space. We step in that direction by choosing different values of t and looking at predictions at configurations θ 0 + tv. We repeat this for many random directions v. • Dropout subspace: We start at an optimized solution θ 0 apply dropout with a randomly chosen p keep , evaluate predictions at dropout p keep (θ 0 ) and repeat this many times with different p keep . • Diagonal Gaussian subspace: We start at an optimized solution θ 0 and look at the most recent iterations of training proceeding it. For each trainable parameter θ i , we calculate the mean µ i and standard deviation σ i independently for each parameter, which corresponds to a diagonal covariance matrix. This is similar to SWAG-diagonal <ref type="bibr" target="#b31">[Maddox et al., 2019]</ref>. To sample solutions from the subspace, we repeatedly draw samples where each parameter independently as θ i ∼ N (µ i , σ i ). • Low-rank Gaussian subspace: We follow the same procedure as the diagonal Gaussian subspace above to compute the mean µ i for each trainable parameter. For the covariance, we use a rankk approximation, by calculating the top k principal components of the recent weight vectors {v i ∈ R params } k . We sample from a k-dimensional normal distribution and obtain the weight configurations as θ ∼ µ</p><formula xml:id="formula_1">+ i N (0 k , 1 k )v i .</formula><p>Throughout the text, we use the terms low-rank and PCA Gaussian interchangeably.</p><p>Figure <ref type="figure">4</ref>: Results using SmallCNN on CIFAR-10: t-SNE plots of validation set predictions for each trajectory along with four different subspace generation methods (showed by squares), in addition to 3 independently initialized and trained runs (different colors). As visible in the plot, the subspacesampled functions stay in the prediction-space neighborhood of the run around which they were constructed, demonstrating that truly different functions are not sampled.</p><p>Figure <ref type="figure">4</ref> shows that functions sampled from a subspace (denoted by colored squares) corresponding to a particular initialization, are much more similar to each other. While some subspaces are more diverse, they still do not overlap with functions from another randomly initialized trajectory. As additional evidence, Figure <ref type="figure" target="#fig_2">5</ref> provides a two-dimensional visualization of the radial landscape along the directions of two different optima. The 2D sections of the weight space visualized are defined by the origin (all weights are 0) and two independently initialized and trained optima. The weights of the two trajectories (shown in red and blue) are initialized using standard techniques and they increase radially with training due to their softmax cross-entropy loss. The left subplot shows that different randomly initialized trajectories eventually achieve similar accuracy. We also sample from a Gaussian subspace along trajectory 1 (shown in pink). The middle and the right subplots show function space similarity (defined as the fraction of points on which they agree on the class prediction) of the parameters along the path to optima 1 and 2. Solutions along each trajectory (and Gaussian subspace) are much more similar to their respective optima, which is consistent with the cosine similarity and t-SNE plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Diversity versus Accuracy plots</head><p>To illustrate the difference in another fashion, we sample functions from a single subspace and plot accuracy versus diversity, as measured by disagreement between predictions from the baseline solution. From a bias-variance trade-off perspective, we require a procedure to produce functions that are accurate (which leads to low bias by aggregation) as well as de-correlated (which leads to lower variance by aggregation). Hence, the diversity vs accuracy plot allows us to visualize the trade-off that can be achieved by subspace sampling methods versus deep ensembles.</p><p>The diversity score quantifies the difference of two functions (a base solution and a sampled one), by measuring fraction of datapoints on which their predictions differ. We chose this approach due to its simplicity; we also computed the KL-divergence and other distances between the output probability distributions, leading to equivalent conclusions. Let d diff denote the fraction of predictions on which the two functions differ. It is 0 when the two functions make identical class predictions, and 1 when they differ on every single example. To account for the fact that the lower the accuracy of a function, the higher its potential d diff due to the possibility of the wrong answers being random and uncorrelated between the two functions, we normalize this by (1 -a), where a is the accuracy of the sampled solution. We also derive idealized lower and upper limits of these curves (showed in dashed lines) by perturbing the reference solution's predictions (lower limit) and completely random predictions at a given accuracy (upper limit), see Appendix D for a discussion. Figure <ref type="figure" target="#fig_3">6</ref> shows the results on CIFAR-10. Comparing these subspace points (colored dots) to the baseline optima (green star) and the optima from different random initializations (denoted by red stars), we observe that random initializations are much more effective at sampling diverse and accurate solutions, than subspace based methods constructed out of a single trajectory. The results are consistent across different architectures and datasets. Figure <ref type="figure" target="#fig_5">7</ref> shows results on CIFAR-100 and ImageNet. We observe that solutions obtained by subspace sampling methods have a worse trade off between accuracy and prediction diversity, compared to independently initialized and trained optima. Interestingly, the separation between the subspace sampling methods and independent optima in the diversity-accuracy plane gets more pronounced the more difficult the problem and the more powerful the network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluating the Relative Effects of Ensembling versus Subspace Methods</head><p>Our hypothesis in Figure <ref type="figure">1</ref> and the empirical observations in the previous section suggest that subspace-based methods and ensembling should provide complementary benefits in terms of uncertainty and accuracy. Since our goal is not to propose a new method, but to carefully test this hypothesis, we evaluate the performance of the following four variants for controlled comparison:</p><p>• Baseline: optimum at the end of a single trajectory.</p><p>• Subspace sampling: average predictions over the solutions sampled from a subspace.</p><p>• Ensemble: train baseline multiple times with random initialization and average the predictions.</p><p>• Ensemble + Subspace sampling: train multiple times with random initialization, and use subspace sampling within each trajectory.</p><p>To maintain the accuracy of random samples at a reasonable level for fair comparison, we reject the sample if validation accuracy is below 0.65. For the CIFAR-10 experiment, we use a rank-4 approximation of the random samples using PCA. Note that diagonal Gaussian, low-rank Gaussian and random subspace sampling methods to approximate each mode of the posterior leads to an increase in the number of parameters required for each mode. However, using just the mean weights for each mode would not cause such an increase. <ref type="bibr" target="#b32">Izmailov et al. [2018]</ref> proposed stochastic weight averaging (SWA) for better generalization. One could also compute an (exponential moving) average of the weights along the trajectory, inspired by Polyak-Ruppert averaging in convex optimization, (see also <ref type="bibr" target="#b33">[Mandt et al., 2017]</ref> for a Bayesian view on iterate averaging). As weight averaging (WA) has been already studied by <ref type="bibr" target="#b32">Izmailov et al. [2018]</ref>, we do not discuss it in detail. Our goal is to test if WA finds a better point estimate within each mode (see cartoon illustration in Figure <ref type="figure">1</ref>) and provides complementary benefits to ensembling over random initialization. In our experiments, we use WA on the last few epochs which corresponds to using just the mean of the parameters within each mode.</p><p>Figure <ref type="figure">8</ref> shows the results on CIFAR-10. The results validate our hypothesis that (i) subspace sampling and ensembling provide complementary benefits, and (ii) the relative benefits of ensembling are higher as it averages predictions over more diverse solutions.</p><p>Figure <ref type="figure">8</ref>: Results using MediumCNN on CIFAR-10 showing the complementary benefits of ensemble and subspace methods as a function of ensemble size. We used 10 samples for each subspace method.</p><p>Effect of function space diversity on dataset shift We test the same hypothesis under dataset shift <ref type="bibr">[Ovadia et al., 2019, Hendrycks and</ref><ref type="bibr" target="#b27">Dietterich, 2019]</ref>. Left and middle subplots of Figure <ref type="figure" target="#fig_6">9</ref> show accuracy and Brier score on the CIFAR-10-C benchmark. We observe again that ensembles and subspace sampling methods provide complementary benefits.</p><p>The diversity versus accuracy plot compares diversity to a reference solution, but it is also important to also look at the diversity between multiple samples of the same method, as this will effectively determine the efficiency of the method in terms of the bias-variance trade-off. Function space diversity is particularly important to avoid overconfident predictions under dataset shift, as averaging over similar functions would not reduce overconfidence. To visualize this, we draw 5 samples of each method and compute the average Jensen-Shannon divergence between their predictions, defined as</p><formula xml:id="formula_2">M m=1 KL(p θm (y|x)||p(y|x))</formula><p>where KL denotes the Kullback-Leibler divergence and p(y|x) = (1/M ) m p θm (y|x). Right subplot of Figure <ref type="figure" target="#fig_6">9</ref> shows the results on CIFAR-10-C for increasing corruption intensity. We observe that Jensen-Shannon divergence is the highest between independent random initializations, and lower for subspace sampling methods; the difference is higher under dataset shift, which explains the findings of <ref type="bibr" target="#b14">Ovadia et al. [2019]</ref> that deep ensembles outperform other methods under dataset shift. We also observe similar trends when testing on an OOD dataset such as SVHN: JS divergence is 0.384 for independent runs, 0.153 for within-trajectory, 0.155 for random sampling, 0.087 for rank-5 PCA Gaussian and 0.034 for diagonal Gaussian.</p><p>Results on ImageNet To illustrate the effect on another challenging dataset, we repeat these experiments on ImageNet <ref type="bibr" target="#b23">[Deng et al., 2009]</ref> using ResNet50v2 architecture. Due to computational constraints, we do not evaluate PCA subspace on ImageNet. Figure <ref type="figure" target="#fig_7">10</ref> shows results on ImageNet test set (zero corruption intensity) and ImageNet-C for increasing corruption intensities. Similar to CIFAR-10, random subspace performs best within subspace sampling methods, and provides complementary benefits to random initialization. We empirically observed that the relative gains of WA (or subspace sampling) are smaller when the individual models converge to a better optima within each mode. Carefully choosing which points to average, e.g. using cyclic learning rate as done in fast geometric ensembling <ref type="bibr" target="#b16">[Garipov et al., 2018]</ref> can yield further benefits. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Through extensive experiments, we show that trajectories of randomly initialized neural networks explore different modes in function space, which explains why deep ensembles trained with just random initializations work well in practice. Subspace sampling methods such as weight averaging, Monte Carlo dropout, and various versions of local Gaussian approximations, sample functions that might lie relatively far from the starting point in the weight space, however, they remain similar in function space, giving rise to an insufficiently diverse set of predictions. Using the concept of the diversity-accuracy plane, we demonstrate empirically that current variational Bayesian methods do not reach the trade-off between diversity and accuracy achieved by independently trained models. There are several interesting directions for future research: understanding the role of random initialization on training dynamics (see Appendix B for a preliminary investigation), exploring methods which achieve higher diversity than deep ensembles (e.g. through explicit decorrelation), and developing parameter-efficient methods (e.g. implicit ensembles or Bayesian deep learning algorithms) that achieve better diversity-accuracy trade-off than deep ensembles. connected in terms of accuracy/loss, their functional forms remain distinct and they do not collapse into a single mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Effect of randomness: random initialization versus random shuffling</head><p>Random seed affects both initial parameter values as well the order of shuffling of data points. We run experiments to decouple the effect of random initialization and shuffling; Figure <ref type="figure" target="#fig_9">S4</ref> shows the results. We observe that both of them provide complementary sources of randomness, with random initialization being the dominant of the two. As expected, random mini-batch shuffling adds more randomness at higher learning rates due to gradient noise. C Comparison to cSG-MCMC <ref type="bibr" target="#b30">Zhang et al. [2020]</ref> show that vanilla stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods do not explore multiple modes in the posterior and instead propose cyclic stochastic gradient MCMC (cSG-MCMC) to achieve that. We ran a suite of verification experiments to determine whether the diversity of functions found using the proposed cSG-MCMC algorithm matches that of independently randomly initialized and trained models.</p><p>We used the code published by the authors <ref type="bibr" target="#b30">Zhang et al. [2020]</ref> <ref type="foot" target="#foot_2">foot_2</ref> to match exactly the setup of their paper. We ran cSG-MCMC from 3 random initializations, each for a total of 150 epochs amounting to 3 cycles of the 50 epoch period learning rate schedule. We used a ResNet-18 and ran experiments on both CIFAR-10 and CIFAR-100. We measured the function diversity between a) independently initialized and trained runs, and b) between different cyclic learning rate periods within the same run of the cSG-MCMC. The latter (b) should be comparable to the former (a) if cSG-MCMC was as successful as vanilla deep ensembles at producing diverse functions. We show that both for CIFAR-10 and CIFAR-100, vanilla ensembles generate statistically significantly more diverse sets of functions than cSG-MCMC, as shown in Figure <ref type="figure">C</ref>. While cSG-MCMC is doing well in absolute terms, the shared initialization for cSG-MCMC training seems to lead to lower diversity than deep ensembles with multiple random initializations. Another difference between the methods is that individual members of deep ensemble can be trained in parallel unlike cSG-MCMC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cSG-MCMC Ensemble</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Modeling the accuracy -diversity trade off</head><p>In our diversity-accuracy plots (e.g. Figure <ref type="figure" target="#fig_3">6</ref>), subspace samples trade off their accuracy for diversity in a characteristic way. To better understand where this relationship comes from, we derive several limiting curves based on an idealized model. We also propose a 1-parameter family of functions that provide a surprisingly good fit (given the simplicity of the model) to our observation, as shown in Figure <ref type="figure" target="#fig_14">S6</ref>.</p><p>We will be studying a pair of functions in a C-class problem: the reference solution f * of accuracy a * , and another function f of accuracy a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Uncorrelated predictions: the best case</head><p>The best case scenario is when the predicted labels are uncorrelated with the reference solution's labels. On a particular example, the probability that the reference solution got it correctly is a * , and the probability that the new solution got it correctly is a. On those examples, the predictions do not differ since they both have to be equal to the ground truth label. The probability that the reference solution is correct on an example while the new solution is wrong is a * (1 -a). The probability that the reference solution is wrong on an example while the new solution is correct is (1 -a * )a. On the examples where both solutions are wrong (probability (1 -a * )(1 -a)), there are two cases:</p><p>1. the solutions agree (an additional factor of 1/(C -1)), or 2. the two solutions disagree (an additional factor of (C -2)/(C -1)).</p><p>Only case 2 contributes to the fraction of labels on which they disagree. Hence we end up with This curve corresponds to the upper limit in Figure <ref type="figure" target="#fig_3">6</ref>. The diversity reached in practice is not as high as the theoretical optimum even for the independently initialized and optimized solutions, which provides scope for future work.</p><p>D.2 Correlated predictions: the lower limit By inspecting Figure <ref type="figure" target="#fig_3">6</ref> as well as a priori, we would expect a function f close to the reference function f * in the weight space to have correlated predictions. We can model this by imagining that the predictions of f are just the predictions of the reference solution f * perturbed by perturbations of a particular strength (which we vary). Let the probability of a label changing be p. We will consider four cases:</p><p>1. the label of the correctly classified image does not flip (probability a * (1 -p)), 2. it flips (probability a * p), 3. an incorrectly labelled image does not flip (probability (1 -a * )(1 -p)), and 4. it flips (probability (1 -a * )p). (2)</p><p>This curve corresponds to the lower limit in Figure <ref type="figure" target="#fig_3">6</ref>.  We can improve upon this model by considering two separate probabilities of labels flipping: p + , which is the probability that a correctly labelled example will flip, and p -, corresponding to the probability that a wrongly labelled example will flip its label. By repeating the previous analysis, we obtain</p><formula xml:id="formula_3">d diff (p + , p -; a * , C) = a * (1 -p + ) + (1 -a * )p - 1 C -1 ,<label>(3)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>θMAP = arg min θ L(θ, {x n , y n } N n=1 ) = arg min θ -log p(θ) -N n=1 log p(y n |x n , θ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results using SmallCNN on CIFAR-10. Left plot: Cosine similarity between checkpoints to measure weight space alignment along optimization trajectory. Middle plot: The fraction of labels on which the predictions from different checkpoints disagree. Right plot: t-SNE plot of predictions from checkpoints corresponding to 3 different randomly initialized trajectories (in different colors). by the similarity plots in Figure 3. Comparing this with Figures 2(a) and 2(b), we see that functions within a single trajectory exhibit higher similarity and functions across different trajectories exhibit much lower similarity.Next, we take the predictions from different checkpoints along the individual training trajectories from multiple initializations and compute a t-SNE plot<ref type="bibr" target="#b29">[Maaten and Hinton, 2008]</ref> to visualize their similarity in function space. More precisely, for each checkpoint we take the softmax output for a set of examples, flatten the vector and use it to represent the model's predictions. The t-SNE algorithm is then used to reduce it to a 2D point in the t-SNE plot. Figure2(c) shows that the functions explored by different trajectories (denoted by circles with different colors) are far away, while functions explored within a single trajectory (circles with the same color) tend to be much more similar.</figDesc><graphic coords="4,112.92,391.83,188.10,85.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results using MediumCNN on CIFAR-10: Radial loss landscape cut between the origin and two independent optima. Left plot shows accuracy of models along the paths of the two independent trajectories, and the middle and right plots show function space similarity to the two optima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Diversity versus accuracy plots for 3 models trained on CIFAR-10: SmallCNN, Medium-CNN and a ResNet20v1. Independently initialized and optimized solutions (red stars) achieve better diversity vs accuracy trade-off than the four different subspace sampling methods.</figDesc><graphic coords="6,112.18,343.67,126.72,90.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>(a) ResNet20v1 trained on CIFAR-100. (b) ResNet50v2 trained on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Diversity vs. accuracy plots for ResNet20v1 on CIFAR-100, and ResNet50v2 on ImageNet.</figDesc><graphic coords="6,162.42,587.21,138.61,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Results using MediumCNN on CIFAR-10-C for varying levels of corruption intensity. Left plot shows accuracy, medium plot shows Brier score and right plot shows Jensen-Shannon divergence.</figDesc><graphic coords="8,108.00,72.00,132.80,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Results using ResNet50v2 on ImageNet test and ImageNet-C for varying corruptions.</figDesc><graphic coords="8,108.00,446.89,395.99,66.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>Figure S3: Results using MediumCNN on CIFAR-10: Radial loss landscape cut between the origin and two independent optima along an optimized low-loss connector and function space similarity (agreement of predictions) to the two optima along the same planes.</figDesc><graphic coords="12,113.77,76.98,188.50,94.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S4 :</head><label>S4</label><figDesc>Figure S4: The effect of random initializations and random training batches on the diversity of predictions. For runs on a GPU, the same initialization and the same training batches (red) do not lead to the exact same predictions. On a TPU, such runs always learn the same function and have therefore 0 diversity of predictions.</figDesc><graphic coords="12,166.16,381.57,277.20,277.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure S5 :</head><label>S5</label><figDesc>Figure S5: Comparison of function space diversities between the cSG-MCMC (blue) and deep ensembles (red). The left panel shows the experiments with ResNet-18 on CIFAR-10 and the right panel shows the experiments on CIFAR-100. In both cases, deep ensembles produced a statistically significantly more diverse set of functions than cSG-MCMC as measured by our function diversity metric. The plots show the mean and 1σ confidence intervals based on 4 experiments each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>d</head><figDesc>diff (a; a * , C) = (1 -a * )a + (1 -a)a * + (1 -a * )(1 -a) C -2 C -1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>The resulting accuracy a(p) obtains a contribution a * (1-p) from case 1) and a contribution (1-a * )p with probability 1/(C -1) from case 4). Therefore a(p) = a * (1 -p) + p(1 -a * )/(C -1). Inverting this relationship, we get p(a) = (C -1)(a * -a)/(Ca * -1). The fraction of labels on which the solutions disagree is simply p by our definition of p, and therefore d diff (a; a * , C) = (C -1)(a * -a) Ca * -1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>D. 3</head><label>3</label><figDesc>Correlated predictions: 1-parameter family</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure S6 :</head><label>S6</label><figDesc>Figure S6: Theoretical model of the accuracy-diversity trade-off and a comparison to ResNet20v1 on CIFAR-100. The left panel shows accuracy-diversity trade offs modelled by a 1-parameter family of functions specified by an exponent e. The right panel shows real subspace samples for a ResNet20v1 trained on CIFAR-100 and the best fitting function with e = 0.22.</figDesc><graphic coords="14,108.00,415.60,395.99,175.62" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>We use the term 'mode' to refer to unique functions f θ (x). Due to weight space symmetries, different parameters can correspond to the same function, i.e. f θ 1 (x) = f θ 2 (x) even though θ1 = θ2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://keras.io/examples/cifar10_resnet/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://github.com/ruqizhang/csgmcmc</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Identical loss does not imply identical functions in prediction space</head><p>To make our paper self-contained, we review the literature on loss surfaces and mode connectivity <ref type="bibr" target="#b16">[Garipov et al., 2018</ref><ref type="bibr" target="#b17">, Draxler et al., 2018</ref><ref type="bibr" target="#b18">, Fort and Jastrzebski, 2019]</ref>. We provide visualizations of the loss surface which confirm the findings of prior work as well as complement them. Figure <ref type="figure">S1</ref> shows the radial loss landscape (train as well as the validation set) along the directions of two different optima. The left subplot shows that different trajectories achieve similar values of the loss, and the right subplot shows the similarity of these functions to their respective optima (in particular the fraction of labels predicted on which they differ divided by their error rate). While the loss values from different optima are similar, the functions are different, which confirms that random initialization leads to different modes in function space. In order to visualize the 2-dimensional cut through the loss landscape and the associated predictions along a curved low-loss path, we divide the path into linear segments, and compute the loss and prediction similarities on a triangle given by this segment on one side and the origin of the weight space on the other. We perform this operation on each of the linear segments from which the low-loss path is constructed, and place them next to each other for visualization. Figure <ref type="figure">S3</ref> visualizes the loss along the manifold, as well as the similarity to the original optima. Note that the regions between radial yellow lines consist of segments, and we stitch these segments together in Figure <ref type="figure">S3</ref>.</p><p>The accuracy plots show that as we traverse along the low-loss tunnel, the accuracy remains fairly constant as expected. However, the prediction similarity plot shows that the low-loss tunnel does not correspond to similar solutions in function space. What it shows is that while the modes are and</p><p>The previously derived lower limit corresponds to the case p + = p -= p ∈ [0, 1], where the probability of flipping the correct labels is the same as the probability of flipping the incorrect labels. The absolute worst case scenario would correspond to the situation where p + = p, while p -= 0, i.e. only the correctly labelled examples flip.</p><p>We found that tying the two probabilities together via an exponent e as p + = p and p -= p e = p e + generates a realistic looking trade off between accuracy and diversity. We show the resulting functions for several values of e in Figure <ref type="figure">S6</ref>. For e &lt; 1, the chance of flipping the wrong label is larger than that of the correct label, simulating a robustness of the learned solution to perturbations. We found the closest match to our data provided by e = 0.22.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bayesian methods for adaptive models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bayesian Learning for Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian Learning via Stochastic Gradient Langevin Dynamics</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>In NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiplicative Normalizing Flows for Variational Bayesian Neural Networks</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flipout: Efficient pseudoindependent weight perturbations on mini-batches</title>
		<author>
			<persName><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Balaji Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in Bayesian deep learning for computer vision</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Why M heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? Evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Evaluating scalable Bayesian deep learning methods for robust computer vision</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Fredrik K Gustafsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><surname>Schön</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01620</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Loss surfaces, mode connectivity, and fast ensembling of DNNs</title>
		<author>
			<persName><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kambis</forename><surname>Veschgini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Salmhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00885</idno>
		<title level="m">Essentially no barriers in neural network energy landscape</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large scale structure of neural network loss landscapes</title>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6544</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Goldilocks zone: Towards better understanding of neural network loss landscapes</title>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Scherlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability. Monthly weather review</title>
		<author>
			<persName><forename type="first">Glenn W Brier ; Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="1950">1950. 2011</date>
		</imprint>
	</monogr>
	<note>Reading Digits in Natural Images with Unsupervised Feature Learning</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cyclical stochastic gradient mcmc for bayesian deep learning</title>
		<author>
			<persName><forename type="first">Ruqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkeS1RVtPS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple baseline for bayesian uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13132" to="13143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent as approximate Bayesian inference</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
