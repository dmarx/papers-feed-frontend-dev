- Decision to use deep ensembles for improving model accuracy and uncertainty quantification
- Choice of random initialization over bootstrap methods for ensemble training
- Selection of Bayesian neural networks as a theoretical comparison
- Hypothesis formulation regarding mode exploration in function space
- Decision to investigate loss landscape properties in relation to ensemble performance
- Choice of datasets (CIFAR-10, CIFAR-100, ImageNet) for experimental validation
- Selection of neural network architectures (SmallCNN, MediumCNN, ResNet20v1) for experiments
- Decision to use Adam optimizer and its parameters for training
- Choice to evaluate performance on corrupted datasets (CIFAR-10-C, ImageNet-C)
- Decision to measure function space similarity using cosine similarity and prediction disagreement
- Choice of subspace sampling methods for comparison (Monte Carlo dropout, diagonal Gaussian, low-rank Gaussian, random subspace)
- Decision to exclude MCMC methods from the current study
- Choice of metrics for evaluating uncertainty estimates (accuracy, Brier score)
- Decision to visualize function space diversity within and across trajectories
- Choice to focus on the diversity-accuracy plane concept for analysis