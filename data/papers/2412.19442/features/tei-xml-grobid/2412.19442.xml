<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on Large Language Model Acceleration based on KV Cache Management</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-02">2 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
							<email>haoyang-comp.li@polyu.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><forename type="middle">•</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing and Data Science</orgName>
								<orgName type="institution" key="instit1">Nicole Hu</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong</orgName>
								<orgName type="institution" key="instit3">Nanyang Tech- nological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anxin</forename><surname>Tian</surname></persName>
							<email>atian@connect.ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Tianhao</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhanchao</forename><forename type="middle">•</forename><surname>Xu</surname></persName>
							<email>xuzhanchaomail@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Xuejia</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing and Data Science</orgName>
								<orgName type="institution" key="instit1">Nicole Hu</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong</orgName>
								<orgName type="institution" key="instit3">Nanyang Tech- nological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicole</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
							<email>weidong@ntu.edu.sg.</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE, Lei</roleName><forename type="first">Qing</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>IEEE</roleName><forename type="first">Chen</forename><surname>Fellow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<email>leichen@cse.ust.hk</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing and Data Science</orgName>
								<orgName type="institution" key="instit1">Nicole Hu</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong</orgName>
								<orgName type="institution" key="instit3">Nanyang Tech- nological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Poly- technic University</orgName>
								<address>
									<addrLine>Haoyang Li Qing Li</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technol- ogy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Tech- nology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey on Large Language Model Acceleration based on KV Cache Management</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-02">2 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">3F87984DFCDD0ACA20929CECCD0B6942</idno>
					<idno type="arXiv">arXiv:2412.19442v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, modellevel, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardwareaware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: <ref type="url" target="https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management">https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Large Language Models (LLMs) [1], [2], trained on massive corpora, have revolutionized various domains such as natural language processing [3], [4], [5], computer vision [6], [7], [8], and multi-modal [9], [10], [11] tasks. Their ability to understand context and perform logical reasoning has enabled</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>remarkable success in various fields, such as time series analysis <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, recommendation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, autonomous driving <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and healthcare <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. These breakthroughs are powered by state-of-the-art architectures and training paradigms, enabling models to achieve unparalleled performance across diverse tasks. Prominent LLMs, such as GPT <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, LLaMA <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, DeepSeek <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, Mistral <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, and GLM <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, are built on the foundational transformer architecture <ref type="bibr" target="#b32">[33]</ref>, which excels at capturing long-range dependencies in sequential data. However, despite their powerful capabilities, the computational and memory demands of LLMs, particularly during inference, present significant challenges when scaling them to real-world, long-context, and real-time applications.</p><p>A critical bottleneck in LLM inference lies in the efficient management of Key-Value (KV) pairs. Recently, caching techniques <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> have been extensively employed to store previously computed intermediate results, allowing their reuse in subsequent inference steps to accelerate the model, such as graph neural networks <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Fortunately, the auto-regressive generation mechanism inherent to LLMs presents an opportunity to leverage KV caching for efficient text generation. Specifically, auto-regressive generation enables LLMs to produce text token by token, with each token conditioned on all previously generated ones. While this approach is highly effective for generating coherent and contextually relevant outputs, it suffers from poor scalability with long input sequences, as the computational and memory requirements grow quadratically with sequence length. The KV cache addresses this issue by storing key and value matrices from previous decoding steps, enabling their reuse and significantly reducing redundant computations.</p><p>Several recent surveys <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> have explored the domain of efficient LLMs. These surveys primarily examine various aspects of LLM efficiency, presenting valuable insights while leaving room for further refinement and innovation. In particular, many of these works primarily focus on holistic approaches to improving LLM efficiency, examining a wide range of techniques across multiple dimensions, such as span data-level optimizations (e.g., prompt engineering), model architecture-level optimizations (e.g., efficient transformer designs), and system-level optimizations (e.g., task scheduling). For instance, Ding et al. <ref type="bibr" target="#b41">[42]</ref> explore efficiency techniques that integrate data-level and model architecture perspectives, while Miao et al. <ref type="bibr" target="#b42">[43]</ref> examine efficient LLM inference from a comprehensive system-level perspective. Similarly, Tang et al. <ref type="bibr" target="#b45">[46]</ref>, Wan et al. <ref type="bibr" target="#b43">[44]</ref>, and Xu et al. <ref type="bibr" target="#b47">[48]</ref> provide analyses that encompass data, model, and systemlevel optimizations, reflecting holistic approaches to LLM acceleration.</p><p>On the other hand, some surveys focus on more specialized aspects for LLM acceleration. For example, Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, Park et al. <ref type="bibr" target="#b39">[40]</ref>, Wang et al. <ref type="bibr" target="#b40">[41]</ref>, and Tang et al. <ref type="bibr" target="#b45">[46]</ref> focus on model compression as a key aspect of model-level optimization. Similarly, Kachris et al. <ref type="bibr" target="#b46">[47]</ref> examine hardware acceleration strategies tailored for LLMs, while Xu et al. <ref type="bibr" target="#b47">[48]</ref> investigate parameter-efficient tuning approaches. Albalak et al. <ref type="bibr" target="#b48">[49]</ref> discuss data selection strategies to enhance the efficiency of LLM training, and Xia et al. <ref type="bibr" target="#b50">[51]</ref> highlight collaborative techniques, such as speculative decoding <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, to accelerate model inference. Li et al. <ref type="bibr" target="#b53">[54]</ref> focus on prompt compression. Similar to our work, Shi et al. <ref type="bibr" target="#b54">[55]</ref>, Li et al. <ref type="bibr" target="#b55">[56]</ref>, and Yuan et al. <ref type="bibr" target="#b56">[57]</ref> also explore the use of KV caches to accelerate LLMs. However, our survey is both complementary and more comprehensive, offering a detailed taxonomy of KV cache management for text-based and multi-modal LLMs. We categorize techniques across token-level, model-level, and system-level perspectives and include benchmarks for both text and multi-modal scenarios. In particular, complementing existing KV cache surveys, we provide a detailed comparison of the differences and advantages of existing models at the token-level, modellevel, and system-level.</p><p>Specifically, this survey provides a comprehensive overview of the current state of KV cache management and its role in accelerating LLM inference. We begin by introducing the transformer architecture and the role of the KV cache in enabling efficient auto-regressive text generation. We then analyze the challenges associated with KV cache management, including its impact on computational complexity, memory usage, and real-time performance. Following this, we present a taxonomy of existing optimization techniques, categorizing them into token-level, model-level, and system-level optimization approaches. Additionally, we discuss datasets and evaluation metrics used to benchmark these techniques and provide insights into their effectiveness across various tasks and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>Large language models (LLMs), pretrained on vast corpora, have demonstrated superior capabilities in context understanding and logical reasoning. These models have achieved remarkable success across a wide range of tasks in various domains, including natural language processing <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and computer vision <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Mainstream LLMs, such as GPT <ref type="bibr" target="#b57">[58]</ref>, LLaMA <ref type="bibr" target="#b23">[24]</ref>, and DeepSeek <ref type="bibr" target="#b25">[26]</ref>, are primarily built on the transformer architecture <ref type="bibr" target="#b32">[33]</ref>. To explore the role of Key-Value (KV) cache management in accelerating LLM computations, we first outline the core components of the transformer model and then introduce the mechanisms for managing the KV cache to accelerate the LLMs. Important notations in this survey are summarized in Tab. 1. Conditional probability</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer Architecture</head><p>Transformers <ref type="bibr" target="#b32">[33]</ref> have become the backbone of LLMs due to their ability to efficiently capture long-range dependencies sequential data, such as text. This capability makes them particularly well-suited for tasks like machine translation, text generation, and image captioning. The transformer architecture follows an encoder-decoder structure, where most LLMs utilize only the decoder component. We first introduce the core components of the Transformer decoder and then describe the critical auto-regressive generation mechanism. Particularly, we do not describe certain components in transformer, such as normalization, as they do not impact the understanding of KV cache management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Transformer Decoder</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, a decoder-based transformer architecture is composed of multiple stacked Transformer blocks, each designed to process sequential data effectively. Typically, a Transformer block consists of two core components, i.e., a Multi-Head Self-Attention (MHSA) mechanism and a Feed Forward Network (FFN). These blocks are arranged sequentially, where the output of one block is passed as input to the next. This iterative design allows the model to refine its understanding of the input sequence progressively, making it highly effective for tasks such as text generation and language modeling. Positional Encoding. Before the input sequence is processed by the Transformer blocks, it undergoes a preprocessing phase. First, a tokenizer processes the input sentence X by splitting it into discrete units, such as words or subwords. The resulting sequence can be represented as</p><formula xml:id="formula_0">X = [x 1 , x 2 , • • • , x |X| ].</formula><p>These tokens are then mapped to dense vector representations using an embedding layer, i.e., X = I X E ⊤ , where I X ∈ {0, 1} n×d vocab represents the one- hot vector of tokenized input X, E ∈ R d vocab ×dx is the embedding matrix, and X = [x 1 , x 2 , • • • , x |X| ] ∈ R n×dx is the resulting matrix of embedded token representations. Since the Transformer architecture does not inherently account for the order of tokens in a sequence, positional encodings are added to the token embeddings X to incorporate positional information. This can be expressed as X = X + P E(X), where P E(X) ∈ R n×dx represents a function <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> (e.g., sine and cosine-based positional encoding) that generates positional embeddings for the input X. Transformer Block. Once the input features are prepared, they are passed through a series of stacked Transformer blocks. Each block begins with the Multi-Head Self-Attention (MHSA) mechanism, which captures both local and global dependencies. For each token, the self-attention mechanism computes a weighted sum over all other tokens in the sequence, where the weights are derived from the similarity between the tokens. Particularly, since the operations within each transformer block are identical, we use a single transformer block as an example. Specifically, given the input to a block, denoted as X ∈ R |X|×d , the MHSA mechanism computes the query vectors Q i ∈ R |X|×d k , key vectors K i ∈ R |X|×d k , and value vectors V i ∈ R |X|×dv . These vectors are obtained through learned linear transformations as follows:</p><formula xml:id="formula_1">Q i = XW Qi , K i = XW Ki , V i = XW Vi ,<label>(1)</label></formula><p>where W Qi ∈ R dx×d k , W Ki ∈ R dx×d k and W Vi ∈ R dx×dv are the learned weight parameters. Then, the self-attention operation is applied to each triple (Q i , K i , V i ), and obtain the output of the i-th attention head Z i as follows:</p><formula xml:id="formula_2">Z i = Attention(Q i , K i , V i ) = Softmax Q i K ⊤ i √ d k V i ,<label>(2)</label></formula><p>where √ d k is a scaling factor to ensure the numerical stability. To capture diverse relationships, multiple attention with h heads are applied to X in parallel, and their outputs are concatenated with one transformation as follows:</p><formula xml:id="formula_3">Z = Concat(Z 1 , Z 2 , . . . , Z h )W O ,<label>(3)</label></formula><p>where Concat is concatenation operation and W O ∈ R dv×do is the trainable parameters. Following the self-attention mechanism, the output is passed through a Feed Forward Network (FFN). The FFN is a fully connected neural network that applies two linear transformations separated by a nonlinear activation function σ(•) (e.g, ReLU <ref type="bibr" target="#b61">[62]</ref>) :</p><formula xml:id="formula_4">FFN(Z) = σ(ZW 1 + b 1 )W 2 + b 2<label>(4)</label></formula><p>where W 1 ∈ R do×d1 and W 2 ∈ R d1×d2 are two parameters. Also, b 1 ∈ R d1 and b 2 ∈ R d2 are two bias vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Auto-regressive Generation Mechanism</head><p>LLMs employ an autoregressive mechanism to generate text token by token, with each token conditioned on the previously generated ones. This iterative process ensures that the output sequence remains coherent and contextually appropriate. Formally, given an input sequence of tokens X = [x 1 , x 2 , • • • , x t ], the model predicts the next token x t+1 at each decoding step t by modeling the conditional probability distribution as follows: where h t ∈ R d h represents the hidden state of the LLM regarding X at step t, W out ∈ R d h ×vocab is the output projection matrix, and b out is the bias vector. The softmax function converts the logits into a probability distribution over the vocabulary. Then, at each decoding step, the model generates the next token x t+1 by sampling from the predicted probability distribution:</p><formula xml:id="formula_5">P (x t+1 |x 1 , x 2 , • • • , x t ) = Softmax(h t W out + b out ),<label>(5)</label></formula><formula xml:id="formula_6">x t+1 ∼ P (x t+1 |x 1 , x 2 , • • • , x t ).<label>(6)</label></formula><p>The generated token x t+1 is then appended to the sequence X = [x 1 , • • • , x t , x t+1 ], and the process continues until a special end-of-sequence (EOS) token is generated or a predefined maximum length is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Key-Value Cache in Transformer Models</head><p>Auto-regressive generation is a powerful mechanism that enables LLMs to produce high-quality, contextually coherent text. However, it presents computational challenges for long sequences, as the Keys and Values need to be recomputed for each token during the generation process. The KV cache optimization addresses this issue by storing the previously computed Keys and Values and reusing them for subsequent token generation, thereby reducing redundant computations and improving inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Auto-regressive Generation with KV Cache</head><p>Here, we describe how caching KV pairs of tokens accelerates LLM inference. Specifically, at each decoding step t, the model performs self-attention over the entire sequence X = [x 1 , • • • , x t-1 , x t ] to generate the next token x t+1 . This process requires the computation of Keys and Values matrices for all previously processed tokens in</p><formula xml:id="formula_7">X = [x 1 , • • • , x t ].</formula><p>Notably, when generating the token x t , the LLM has already computed the Keys and Values for the tokens in</p><formula xml:id="formula_8">X[1 : t -1] = [x 1 , • • • , x t-1 ].</formula><p>The KV cache optimizes this process by storing the previously computed Keys and</p><p>Values matrices for X[1 : t -1] and reusing them, thereby only requiring the computation of Keys and Values for the new token x t . This significantly improves efficiency by eliminating redundant computations. Formally, at decoding step t, the new token embedding x t is used to compute the query vector q t i , key vector k t i , and value vector v t i as follows:</p><formula xml:id="formula_9">q t i = x t W Qi , k t i = x t W Ki , v t i = x t W Vi ,<label>(7)</label></formula><p>The newly computed k t i and v t i are then appended to the cached key and value matrices from previous steps:</p><formula xml:id="formula_10">K t i = Concat( Kt-1 i , k t i ), V t i = Concat( Vt-1 i , V t i ),<label>(8)</label></formula><p>where Kt-1 i ∈ R t-1×d k and Vt-1 i ∈ R t-1×dv represent the cached key and value matrices of tokens in X[1 : t -1]. These cached matrices are then used in the scaled dotproduct attention computation for token x t . The attention output z t i for the token x t at step t is calculated as:</p><formula xml:id="formula_11">z t i = Softmax q t i K t i ⊤ √ d k V t i ,<label>(9)</label></formula><p>Then, a similar KV reuse process can be applied to different attention heads in each layer of the LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Time and Space Complexity Analysis</head><p>Given a transformer-based L-layer LLM with h attention heads per layer and an input sequence of length</p><formula xml:id="formula_12">X = [x 1 , • • • , x t ],</formula><p>we analyze the time saved and the space required to store cached KV pairs. For simplicity, we assume the Keys and Values of t c tokens are stored for all heads across all LLM layers. Saved Time. For each token, the saved computation time comes from avoiding the repeated computation of Keys and Values in Equation (1), self-attention result in Equation ( <ref type="formula" target="#formula_2">2</ref>), and linear transformation in Equation <ref type="bibr" target="#b2">(3)</ref>. We omit the time analyze on operations in transformer that do not affect the understanding of KV cache acceleration, such as layer norm and position encoding.</p><p>• QKV Computation. The time of computing Queries, Keys and Values for each token in Equation ( <ref type="formula" target="#formula_1">1</ref>) is</p><formula xml:id="formula_13">△ 1 = O(2d x d k + d x d v ). • Self-attention Result. Additionally, computing each at- tention result z i in Equation (2) takes O(t(d k + d v )). • Linear Transformation. To merge the h attention results in Equation (3) the time is △ 2 = O(hd v + d v d o )</formula><p>. Therefore, for t c cached tokens across h attention heads and L layers, the total saved computation time is:</p><formula xml:id="formula_14">O (L • h • t c • t • (d k + d v ) + L • h • t c (△ 1 + △ 2 ))<label>(10)</label></formula><p>Thus, the saved time is directly proportional to the number of cached tokens t c , significantly accelerating model computation, especially for longer sequences (when t is large). Extra Space. Compared to computation without caching, additional space is required to store the cached KV pairs for t c tokens across h attention heads and L layers. Assuming each Key and Value is stored in Float16 precision, the total extra space needed can be expressed as:</p><formula xml:id="formula_15">O(L • h • t c • 2 • sizeof (F loat16))<label>(11)</label></formula><p>Thus, for the same LLM model, the extra space required to store the KV pairs primarily depends on the number of cached tokens and the precision of the cached Keys and Values. To address this, existing approaches explore various techniques to reduce the extra space consumption, such as caching only the most important Keys and Values or applying quantization techniques to lower the bit precision of the stored Keys and Values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges in KV Cache Management</head><p>As analyzed in Sec. 2.2.2, reusing cached KV pairs enables the LLM to avoid recomputing past tokens, resulting in significant speedups during inference. However, as sequence lengths grow, the size of the KV cache increases proportionally, placing significant pressure on memory. Consequently, it becomes challenging to manage this cache effectively to accelerate LLM computation without excessive space usage.</p><p>• Cache Eviction Policies: Determining which items to evict when the cache reaches its capacity is a complex problem. Popular policies <ref type="bibr" target="#b34">[35]</ref> like Least Recently Used (LRU) or Least Frequently Used (LFU) do not align with LLMs patterns, leading to suboptimal performance. • Memory Management: The memory required for the KV cache grows linearly with both the sequence length and the number of layers, which can quickly exceed the hardware memory limits, especially for long sequences. Consequently, managing the collaboration between different types of storage hardware (e.g., GPU, CPU, or external memory) becomes a significant challenge. • Latency Bottlenecks: Accessing and updating the cache at each decoding step can introduce latency, particularly for hardware with limited memory bandwidth. • Compression Trade-offs: Compressing the KV cache can reduce memory usage but may degrade model performance if key information is lost. • Dynamic Workloads: Handling dynamic and unpredictable workloads, where access patterns and data requirements frequently change, requires adaptive caching strategies that can respond in real time. • Distributed Coordination: In distributed KV caches, maintaining coordination across multiple nodes to ensure consistency, fault tolerance, and efficient resource usage adds significant complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TAXONOMY</head><p>In the above sections, we analyzed how the number of cached Key-Value (KV) pairs significantly impacts both the computation time and the additional memory required during inference. Efficient KV cache management is critical to balancing performance improvements and resource utilization, especially as sequence lengths and model sizes continue to grow. After carefully reviewing existing approaches, we categorize KV cache optimization strategies into three levels: token-level optimization, model-level optimization, and system-level optimizations. Each level addresses specific aspects of the challenges associated with KV cache management and offers distinct techniques to enhance efficiency. The detailed taxonomy is illustrated in Fig. <ref type="figure">2</ref>.</p><p>KV Cache Management for Large Language Models Datasets and Benchmarks (Sec. 7) Multi-modal Datasets (Sec. 7.2) Text Datasets (Sec. 7.1) System-level Optimization (Sec. 6) Hardware-aware Design (Sec. 6.3) SSD-based Design (Sec. 6.3.4) Heterogeneous Design (Sec. 6.3.3) I/O-based Design (Sec. 6.3.2) Single/Multi-GPU Design (Sec. 6.3.1) Scheduling (Sec. 6.2) Layer-specific and Hierarchical Scheduling (Sec. 6.2.3) Preemptive and Fairnessoriented Scheduling (Sec. 6.2.2) Prefix-aware Scheduling (Sec. 6.2.1) Memory Management (Sec. 6.1) Prefix-aware Design (Sec. 6.1.2) Architectural Design (Sec. 6.1.1) Model-level Optimization (Sec. 5) Non-transformer Architecture (Sec. 5.3) Hybrid Architecture (Sec. 5.3.2) Adaptive Sequence Processing Architecture (Sec. 5.3.1) Architecture Alteration (Sec. 5.2) Augmented Architecture (Sec. 5.2.2) Enhanced Attention (Sec. 5.2.1) Attention Grouping and Sharing (Sec. 5.1) Cross-Layer Sharing (Sec. 5.1.2) Intra-Layer Grouping (Sec. 5.1.1) Token-level Optimization (Sec. 4) KV Cache Low-rank Decomposition (Sec. 4.5) Learned Low-rank Approximation (Sec 4.5.3) Tensor Decomposition (Sec 4.5.2) Singular Value Decomposition (Sec 4.5.1) KV Cache Quantization (Sec. 4.4) Outlier Redistribution (Sec 4.4.3) Mixed-precision Quantization (Sec 4.4.2) Fixed-precision Quantization (Sec 4.4.1) KV Cache Merging (Sec. 4.3) Cross-layer Merging (Sec 4.3.2) Intra-layer Merging (Sec 4.3.1) KV Cache Budget Allocation (Sec. 4.2) Head-wise Budget Allocation (Sec 4.2.2) Layer-wise Budget Allocation (Sec 4.2.1) KV Cache Selection (Sec. 4.1) Dynamic Selection without Permanent Eviction (Sec 4.1.3) Dynamic Selection with Permanent Eviction (Sec 4.1.2) Static KV Cache Selection (Sec 4.1.1) Fig. 2. Taxonomy of KV Cache Management for Large Language Models. • Token-Level Optimization refers to improving KV cache management efficiency by focusing on fine-grained the careful selection, organization, and compression at the token level, requiring no architectural changes to the orig-inal model. While KV cache selection (Sec. 4.1) focuses on prioritizing and storing only the most relevant tokens. KV cache budget allocation (Sec. 4.2) dynamically distributes memory resources across tokens to ensure efficient cache utilization under limited memory. Furthermore, KV cache merging (Sec. 4.3) reduces redundancy by combining similar or overlapping KV pairs, while KV Cache Quantization (Sec. 4.4) minimizes the memory footprint by reducing the precision of cached KV pairs. Finally, KV cache low-rank decomposition (Sec. 4.5) uses low-rank decomposition technique to reduce cache size. • Model-level Optimization refers to designing an efficient model structure to optimize KV cache management. This can further refer to several strategies: Attention grouping and sharing (Sec. 5.1) methods examine the redundant functionality of key and values and group and share KV cache within or across transformer layers. Architecture alterations (Sec. 5.2 emerge to design new attention mechanisms or construct extrinsic modules for KV optimization. Furthermore, there are also works designing or combining non-transformer architectures 5.3 that adopt other memory efficient designs like recurrent neural networks to optimize the KV cache in traditional transformers. • System-level Optimization refers to optimizing the KV Cache management through two classic low-level aspects: memory management (Sec. 6.1) and scheduling (Sec. 6.2). While memory management techniques focusing on architectural innovations like virtual memory adaptation, intelligent prefix sharing, and layer-aware resource allocation, scheduling strategies have evolved to address diverse optimization goals through prefix-aware methods for maximizing cache reuse, preemptive techniques for fair context switching, and layer-specific mechanisms for fine-grained cache control. In addition, we provide a detailed introduction for hardware accelerator design in Sec. 6.3, including single/multi-GPU, I/O-based solutions, heterogeneous computing and SSD-based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TOKEN-LEVEL OPTIMIZATION</head><p>In the token level, optimization focuses exclusively on improving KV cache based on the characteristics and patterns of KV pairs of tokens, without considering enhancements from model architecture improvements or system parallelization techniques. In general, token-level optimization methods are primarily guided by observations from LLMs and sequential inputs. Existing approaches can be categorized into five main types: KV cache selection, KV cache budget allocation, KV cache merging, KV cache quantization, and KV cache low-rank decomposition. The taxonomy of the token-level optimization is shown in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KV Cache Selection</head><p>KV cache selection mechanisms have emerged as a critical optimization strategy, aimed at reducing memory utilization of KV caches, minimizing inference latency, and enhancing overall throughput in large language models. These optimization objectives have driven the development of various selection methodologies, which can be classified into two distinct categories: (1) static KV cache selection, which performs token filtering exclusively during the prefilling phase, with selected tokens remaining fixed throughout subsequent decoding steps; and (2) dynamic KV cache selection, which continuously updates KV cache during the decoding phase, enabling adaptive cache management. In dynamic KV cache selection approaches, KV cache tokens that are not selected may be permanently evicted or offloaded to hierarchical caching devices such as CPU memory, implementing a multi-tier storage strategy. Given that real-time KV cache selection during decoding may incur substantial computational overhead, several studies have focused on developing optimized retrieval algorithms to enhance the efficiency of this process. These optimizations include blocklevel retrieval instead of token-level granularity to reduce search complexity, asynchronous query mechanisms to hide latency, and parallel retrieval pipelines to accelerate the selection process. These optimization efforts aim to mitigate the computational burden while maintaining the effectiveness of token selection. The summary of the KV cache selection is listed in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Static KV Cache Selection</head><p>Static KV cache selection methods perform a one-time compression on the KV Cache immediately after the prefilling phase is completed. The model then uses this compressed KV cache for subsequent decoding inference. FastGen <ref type="bibr" target="#b132">[133]</ref> introduces a pattern-aware approach by identifying five fundamental attention structures and implementing targeted selection strategies. These include proximity-based retention for local attention patterns, selective preservation of critical tokens for punctuation-focused attention, frequency-based filtering for sparse attention distributions, and complete token retention for broad attention patterns.</p><p>SnapKV <ref type="bibr" target="#b133">[134]</ref> simplifies FastGen's approach by focusing solely on retrieving tokens based on their importance scores. It demonstrates that among all prompt tokens, only a portion carries crucial information for response generation, with these tokens maintaining their significance during the generation phase. The approach employs an end-positioned observation window to detect these important contextual tokens. Their corresponding key-value pairs are then concatenated with the tokens from the observation window. Attention-Gate <ref type="bibr" target="#b134">[135]</ref> introduces a learnable KV-Cache eviction mechanism that processes the entire context sequence and generates token-wise eviction decisions through a parameterized policy network, enabling dynamic in-context memory management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Dynamic Selection with Permanent Eviction</head><p>This category of methods performs frequent KV cache selection during the decoding phase, permanently removing unselected KV cache tokens from memory. Early works employ a sliding-window mechanism to address longtext inference challenges, where tokens falling outside the window are permanently evicted and become inaccessible. StreamingLLM <ref type="bibr" target="#b135">[136]</ref> uncovers a crucial phenomenon in transformer attention where preserved key-value pairs from initial sequence tokens maintain crucial model performance. This attention sink effect manifests through asymmetric attention weight accumulation at early positions, regardless of semantic significance. The approach leverages this characteristic by incorporating attention sink positions with recent context for efficient processing. LM-Infinite <ref type="bibr" target="#b136">[137]</ref> demonstrates that conventional techniques, including sliding-window patterns and relative positional Token-level Optimization KV Cache Low-rank Decomposition (Sec. 4.5) Learned Low-rank Approximation (Sec 4.5.3) LESS <ref type="bibr" target="#b62">[63]</ref>, MatryoshkaKV <ref type="bibr" target="#b63">[64]</ref> Tensor Decomposition (Sec 4.5.2) DecoQuant <ref type="bibr" target="#b64">[65]</ref> Singular Value Decomposition (Sec 4.5.1) ECKVH <ref type="bibr" target="#b65">[66]</ref>, EigenAttention <ref type="bibr" target="#b66">[67]</ref>, ZDC <ref type="bibr" target="#b67">[68]</ref>, LoRC <ref type="bibr" target="#b68">[69]</ref>, ShadowKV <ref type="bibr" target="#b69">[70]</ref>, Palu <ref type="bibr" target="#b70">[71]</ref> KV Cache Quantization (Sec. 4.4) Outlier Redistribution (Sec 4.4.3) Attention-Gate <ref type="bibr" target="#b134">[135]</ref> Fig. <ref type="figure">3</ref>. Taxonomy of the Token-level Optimization for KV Cache Management.</p><p>encodings, fail to resolve length generalization issues. The study introduces a novel methodology through the integration of Λ-shaped attention masking and attention distance ceiling mechanisms.</p><p>Recent works have explored leveraging attention scores as a criterion for selecting significant KV cache tokens. H2O <ref type="bibr" target="#b126">[127]</ref> observes that attention computations are primarily driven by a select group of high-impact tokens, known as Heavy Hitters (H2). This method reformulates cache optimization as a dynamic submodular problem, utilizing cumulative attention scores to guide token retention decisions. Unlike H2O, BUZZ <ref type="bibr" target="#b127">[128]</ref> employs a beehive-like structure that selects Heavy Hitters in local KV cache segments. NACL <ref type="bibr" target="#b128">[129]</ref> identifies a fundamental limitation in H2O, namely their dependence on potentially biased local attention statistics. To overcome this issue, they develop an alternative approach implementing a diversified random eviction strategy for token selection. Scissorhands <ref type="bibr" target="#b129">[130]</ref> builds upon the temporal significance principle, which suggests that tokens demonstrating historical importance maintain their influence in subsequent computational steps. This observation enables the preservation of repetitive attention patterns through selective token retention. Additionally, Keyformer <ref type="bibr" target="#b130">[131]</ref> reveals that token removal distorts the</p><p>TABLE 2 Comparison of KV cache selection strategies. Method Initial tokens Top-k tokens Recent tokens Permanent eviction Dynamic selection Selection granularity Remark FastGen [133] ✓ ✓ ✓ ✓ token five attention structures SnapKV [134] ✓ ✓ ✓ token observation window-based Attention-Gate [135] ✓ ✓ token learned eviction policy StreamingLLM [136] ✓ ✓ ✓ ✓ token initial and recent tokens LM-Infinite [137] ✓ ✓ ✓ ✓ token distance ceiling H2O [127] ✓ ✓ ✓ ✓ token accmulative attention score BUZZ [128] ✓ ✓ ✓ ✓ ✓ token beehive-like structure Scissorhands [130] ✓ ✓ ✓ ✓ token persistence of importance NACL [129] ✓ ✓ ✓ ✓ token diversified random eviction Keyformer [131] ✓ ✓ ✓ ✓ token gumbel logit adjustment InfLLM [121] ✓ ✓ ✓ ✓ block block-level KV management Quest [122] ✓ ✓ block new block representation PQCache [98] ✓ ✓ ✓ ✓ block product quantization SqueezedAttention [123] ✓ ✓ cluster hierarchical clusters RetrievalAttention [124] ✓ ✓ ✓ ✓ Token ANN search EM-LLM [125] ✓ ✓ ✓ ✓ event episodic events SparQ [138] ✓ ✓ ✓ token low-dimensioanl retrieval InfiniGen [139] ✓ ✓ token asynchronous prefetching RecycledAttention [140] ✓ ✓ ✓ token periodic top-k selection MagicPIG [141] ✓ ✓ ✓ ✓ token Local Sensitive Hash</p><p>underlying softmax probability distribution. Considering the pivotal role of softmax distributions in token significance evaluation, they incorporate regularization techniques to mitigate these distributional perturbations. SepLLM <ref type="bibr" target="#b131">[132]</ref> observes that separator tokens (e.g., commas, periods, and line breaks) receive disproportionately high attention scores and naturally summarize text segments. Building on this, SepLLM retains separator tokens together with initial tokens, important tokens, and recent tokens in the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Dynamic Selection without Permanent Eviction</head><p>The aforementioned permanent eviction-based approaches face two significant limitations. First, the irreversible eviction of tokens potentially impairs the model's performance on long-sequence tasks, particularly in needle-in-a-haystack scenarios, and these methods prove challenging to adapt to multi-turn dialogue contexts. Second, KV cache selection during the decoding phase introduces computational overhead, adversely affecting decoding latency and compromising end-to-end acceleration. To address these challenges, several studies have focused on developing decoding-phase KV cache selection strategies without permanent eviction. These approaches typically employ multi-tier cache systems (e.g., CPU-GPU hierarchical caching) and leverage advanced data structures and system-level enhancements to optimize retrieval efficiency, enabling efficient inference with reduced GPU KV cache footprint.</p><p>To accelerate the retrieval of critical tokens, several research efforts have proposed index-based approaches that organize and access KV cache at block or cluster granularity, enabling efficient query and extraction operations. InfLLM <ref type="bibr" target="#b120">[121]</ref> maintains full KV cache in blocks while facilitating long sequence processing through a hierarchical stor-age strategy. The framework employs CPU-GPU memory orchestration, preserving essential tokens and current computational units in GPU memory while offloading less frequently accessed units to CPU memory. To further enhance top-k block retrieval precision, the Quest <ref type="bibr" target="#b121">[122]</ref> framework presents a refined block representation approach based on minimal and maximal key values in KV cache blocks. PQ-Cache <ref type="bibr" target="#b97">[98]</ref> also implements block-based KV cache management and identifies salient tokens through Maximum Inner-Product Search (MIPS), leveraging Product Quantization (PQ) codes and centroids. SqueezedAttention <ref type="bibr" target="#b122">[123]</ref> employs K-means clustering in an offline stage to group semantically similar keys, with each group represented by a centroid. During inference, it compares input queries against these centroids to identify and load only the semantically relevant keys from the context. Similarly, RetrievalAttention <ref type="bibr" target="#b123">[124]</ref> index KV cache tokens using approximate nearest neighbor search (ANNS) techniques. Additionally, EM-LLM <ref type="bibr" target="#b124">[125]</ref> dynamically segments incoming tokens into episodic events. Besides, it implements a hybrid retrieval mechanism that combines semantic similarity matching with temporal context to efficiently access relevant KV cache segments. Similarly, ClusterKV <ref type="bibr" target="#b125">[126]</ref> groups tokens into semantic clusters and selectively recalls them during inference, achieving both high accuracy and efficiency for LLMs.</p><p>To accelerate top-k token identification, SparQ <ref type="bibr" target="#b137">[138]</ref> identifies the r most significant elements in the incoming query vector and selectively retrieves the corresponding components along the hidden dimension of the cached key matrix K for approximate attention computation. To overlap prefetching latency, InfiniGen <ref type="bibr" target="#b138">[139]</ref> employs asynchronous prefetching, utilizing indices of salient KV entries selected by queries from the previous layer to retrieve KV</p><p>TABLE 3 Comparison of KV cache budget allocation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Layer-wise Head-wise Retrieval-head Input-specific Extra-calibration Remark</head><p>PyramidKV <ref type="bibr" target="#b115">[116]</ref> ✓ pyramid-shaped PyramidInfer <ref type="bibr" target="#b116">[117]</ref> ✓ pyramid-shaped DynamicKV <ref type="bibr" target="#b117">[118]</ref> ✓ ✓ maximize attention retention rate PrefixKV <ref type="bibr" target="#b118">[119]</ref> ✓ ✓ maximize attention retention rate CAKE <ref type="bibr" target="#b141">[142]</ref> ✓ ✓ layer-specific preference score SimLayerKV <ref type="bibr" target="#b119">[120]</ref> ✓ ✓ KV cache compression for lazy layers AdaKV <ref type="bibr" target="#b109">[110]</ref> ✓ ✓ minimize attention computation loss CriticalKV <ref type="bibr" target="#b110">[111]</ref> ✓ ✓ minimize attention computation loss LeanKV <ref type="bibr" target="#b111">[112]</ref> ✓ ✓ maximize attention retention rate RazorAttention <ref type="bibr" target="#b112">[113]</ref> ✓ ✓ ✓ echo and induction heads HeadKV <ref type="bibr" target="#b113">[114]</ref> ✓ ✓ ✓ retrieval and reasoning heads DuoAttention <ref type="bibr" target="#b114">[115]</ref> ✓ ✓ ✓ learned retrieval heads cache entries in the current layer. To ensure maximum model performance, RecycledAttention <ref type="bibr" target="#b139">[140]</ref> sustains the entire KV cache during inference computations, yielding no improvements in memory efficiency. The approach performs periodic top-k token selection to identify salient tokens. Moreover, MagicPIG <ref type="bibr" target="#b140">[141]</ref> shows that attention-based topk selection may incur performance degradation. To address this limitation, they introduce a novel heterogeneous computing framework leveraging Locality Sensitive Hashing (LSH) techniques. The system stores LSH hash tables and performs attention estimation on CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Summary and Future Directions</head><p>Static KV cache selection algorithms demonstrate superior decoding efficiency overall; however, their efficacy remains to be thoroughly validated in multi-turn dialogues and extended decoding length scenarios. Dynamic KV cache selection algorithms, while adaptive, introduce additional computational overhead during the decoding phase due to frequent cache selection operations. Multi-tier cache architectures and prefetching schemes partially mitigate these challenges, yet their capability to achieve rapid and accurate retrieval within acceptable decoding latency constraints requires further empirical validation, particularly in realworld applications involving long sequences. Furthermore, existing selection methods predominantly rely on attention score-based top-k selection mechanisms. However, based on existing positional encoding schemes, current top-k approaches may not be able to effectively identify and extract relevant tokens in ultra-long sequence tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KV Cache Budget Allocation</head><p>The hierarchical architecture of LLMs leads to diverse information extraction patterns across layers, with each layer's KV-cache contributing differently to model performance. This inherent heterogeneity indicates that uniform KV-cache compression across layers may be suboptimal. KV cache budget allocation addresses this challenge by intelligently distributing memory resources based on each component's importance to prediction accuracy, thereby optimizing memory utilization while minimizing accuracy degradation. Current budget allocation strategies can be categorized into two levels of granularity: layer-wise budget allocation, which assigns different compression ratios across model layers, and the more fine-grained head-wise budget allocation, which enables precise memory distribution across individual attention heads within each layer, offering more flexible and targeted optimization opportunities. The summary of KV budget allocation is listed in Tab. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Layer-wise Budget Allocation</head><p>In contrast to conventional approaches with uniform KV cache sizes, PyramidKV <ref type="bibr" target="#b115">[116]</ref> employs a pyramid-shaped memory allocation strategy, assigning larger cache capacities to lower layers that progressively decrease in upper layers. This design is supported by the observation that lower layers exhibit uniform attention distributions across input sequences, while upper layers show concentrated attention on specific tokens. PyramidInfer <ref type="bibr" target="#b116">[117]</ref> also adopts a pyramid-shaped budget allocation strategy while selecting tokens with high attention values at each layer. Additionally, during the decoding phase, PyramidInfer dynamically maintains a set of significant tokens through frequent updates driven by attention values. Unlike previous methods, DynamicKV <ref type="bibr" target="#b117">[118]</ref> implements an input-adaptive budget allocation strategy by analyzing attention patterns. Specifically, it computes the average attention scores between recent and historical tokens, identifies the top-k tokens with highest attention values across layers, and proportionally distributes the budget based on the density of significant tokens in each layer. Similarly, PrefixKV <ref type="bibr" target="#b118">[119]</ref> identifies the most important tokens for each layer by computing the average attention score of tokens within that layer.</p><p>PrefixKV <ref type="bibr" target="#b118">[119]</ref> then uses a unified threshold to determine the number of retained tokens, adaptively adjusting the retention for each layer based on its importance distribution. CAKE <ref type="bibr" target="#b141">[142]</ref> examines attention scores through two lenses: the spatial distribution of inter-token attention and the temporal evolution of attention focus. These measurements are combined to compute layer-specific importance scores, which further guide the allocation of memory resources. Additionally, SimLayerKV <ref type="bibr" target="#b119">[120]</ref> identifies lazy layers -those exhibiting limited effectiveness in capturing long-range de- ✓ Token Attention Score Many-to-One ✓ AIM <ref type="bibr" target="#b105">[106]</ref> ✓ Token Cosine Similarity Many-to-One ✓ Look-M <ref type="bibr" target="#b106">[107]</ref> ✓ Token Cosine Similarity Many-to-One ✓ KVMerger <ref type="bibr" target="#b107">[108]</ref> ✓ Token Weighted Gaussian Kernel Many-to-One ✓ CHAI <ref type="bibr" target="#b108">[109]</ref> ✓ Head Attention Score Many-to-One ✓ MinCache <ref type="bibr" target="#b98">[99]</ref> ✓ Token Angular Distance Two-to-One ✓ KVSharer <ref type="bibr" target="#b99">[100]</ref> ✓ Layer Euclidean distance Many-to-One ✓ pendencies. The framework then selectively preserves cache entries, maintaining initial and recent tokens for lazy layers while retaining complete KV cache for non-lazy layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Head-wise Budget Allocation</head><p>AdaKV <ref type="bibr" target="#b109">[110]</ref> leverages the observation that attention patterns exhibit distinct concentrations across different heads. It implements head-specific memory allocation by optimizing an L1 loss bound between the original and pruned multihead attention outputs. Within the constraints of a layerwise budget, the method distributes cache capacity among heads to maximize the preserved attention information collectively. Building upon AdaKV, CriticalKV <ref type="bibr" target="#b110">[111]</ref> introduces significant enhancements by recognizing that the importance of KV cache entries extends beyond attention weights to encompass value states and pretrained parameter matrices. Leveraging this insight, the framework implements a novel selection algorithm that identifies essential cache entries by minimizing the maximum potential output perturbation. LeanKV <ref type="bibr" target="#b111">[112]</ref> implements a fine-grained memory optimization strategy that operates independently for each attention head and input request. The method identifies the smallest subset of tokens necessary to preserve the majority of information flow, allocating cache space based on a predefined attention score threshold -typically maintaining 95% of the total attention mass. Retrieval head-based methods represent a specialized category of head-wise allocation strategies that focuses on identifying and prioritizing attention heads crucial for extracting key information from long sequences. This approach allocates larger cache budgets to these specialized heads, known as retrieval heads <ref type="bibr" target="#b142">[143]</ref>, due to their significant role in information extraction. RazorAttention <ref type="bibr" target="#b112">[113]</ref> characterizes two distinct categories of retrieval heads: echo heads, which focus on previously occurring identical tokens, and induction heads, which attend to antecedent tokens that precede current token repetitions. This framework implements differential caching strategies, maintaining complete cache entries for retrieval heads while condensing remote tokens into consolidated compensation tokens for nonretrieval heads. HeadKV <ref type="bibr" target="#b113">[114]</ref> further enhances RazorAt-tention by introducing a novel head assessment framework that simultaneously evaluates both retrieval and reasoning capabilities to optimize KV cache allocation strategies. DuoAttention <ref type="bibr" target="#b114">[115]</ref> further introduces a parameterized approach to distinguish between two categories of attention mechanisms: retrieval heads, essential for comprehensive long-context processing, and Streaming heads, which primarily engage with recent tokens and attention sinks. This classification is achieved through learned parameters that automatically identify retrieval heads requiring full attention spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Summary and Future Directions</head><p>Despite recent advances and growing attention in KV cache budget allocation research, several critical challenges remain unaddressed. First, the relationship between allocation strategies and model performance requires further investigation. For instance, a notable discrepancy exists between pyramid-shaped allocation strategies <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b116">[117]</ref> advocating larger budgets for lower layers, and retrieval head-based studies <ref type="bibr" target="#b112">[113]</ref>, <ref type="bibr" target="#b113">[114]</ref> which demonstrate that lower layers rarely exhibit retrieval head characteristics and thus require minimal cache resources. Additionally, the field lacks comprehensive experimental comparisons, particularly regarding the compatibility and performance benefits of headwise budget allocation strategies with state-of-the-art frameworks like vLLM <ref type="bibr" target="#b143">[144]</ref> and FlashAttention <ref type="bibr" target="#b144">[145]</ref>. Also, existing methods, such as PyramidInfer <ref type="bibr" target="#b116">[117]</ref>, demonstrate some adaptability to input attention patterns. However, future research could target real-time, task-specific allocation strategies that dynamically adjust memory budgets during inference based on input characteristics, task complexity, or downstream requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">KV Cache Merging</head><p>KV cache merging offers a promising solution by compressing or consolidating KV caches without significantly degrading model accuracy. Rather than a uniform compression strategy, KV cache merging techniques leverage the inherent redundancy within and across layers to dynamically optimize memory utilization. These methods aim to reduce the size of KV caches while preserving critical information necessary for accurate attention computations, enabling efficient inference in resource-constrained settings. Existing KV cache merging strategies can be categorized into two primary approaches: intra-layer merging, which focuses on consolidating KV caches within individual layers to reduce memory usage per layer, and cross-layer merging, which targets redundancy across layers to eliminate unnecessary duplication. Both approaches offer complementary advantages, providing flexibility to balance memory savings and model performance degradation. The summary of the KV cache merging is listed in Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Intra-layer Merging</head><p>As the input sequence length increases, the number of Keys and Values grows, leading to higher computational costs for the attention process. To address this, CCM <ref type="bibr" target="#b100">[101]</ref>, LoMA <ref type="bibr" target="#b101">[102]</ref>, DMC <ref type="bibr" target="#b102">[103]</ref> propose to learn a compression module to compress KV of tokens.</p><p>Specifically, CCM <ref type="bibr" target="#b100">[101]</ref> inserts a special indicator token, [COMP], into the input sequence and compresses the accumulating past attention key/value (KV) pairs in each layer between these indicators into a compact memory space. This compression leverages techniques inspired by the Compressive Transformer <ref type="bibr" target="#b145">[146]</ref> and Gisting <ref type="bibr" target="#b146">[147]</ref>. Instead of computing attention across all tokens, CCM <ref type="bibr" target="#b100">[101]</ref> computes attention scores for each new token by referencing the merged token. Similarly, LoMA <ref type="bibr" target="#b101">[102]</ref> inserts a special token into the input sequence to determine which consecutive tokens should be compressed. LoMA <ref type="bibr" target="#b101">[102]</ref> performs compression using bidirectional attention, repetition zone supervision, and carefully designed attention masks and loss functions. DMC <ref type="bibr" target="#b102">[103]</ref> learns a variable to decide whether to append new KV pairs to the cache when necessary or to merge them into existing KV representations using a weighted average. Note that CCM <ref type="bibr" target="#b100">[101]</ref>, LoMA <ref type="bibr" target="#b101">[102]</ref>, and DMC <ref type="bibr" target="#b102">[103]</ref> require supervised learning to learn a compression module.</p><p>Instead, CaM <ref type="bibr" target="#b103">[104]</ref>, KVMerger <ref type="bibr" target="#b107">[108]</ref>, and D2O <ref type="bibr" target="#b104">[105]</ref> are training-free, which rely on observations and directly propose rule-based or heuristic-based merging strategies. Specifically, they separate the Keys and Values of tokens in each layer into important (retrained) and unimportant (evicted) tokens. They then keep potentially useful unimportant tokens by merging their Keys and Values with retained important tokens, ensuring that no valuable information is lost. Particularly, D2O <ref type="bibr" target="#b104">[105]</ref> merges merges the Key (or Value) of a evicted token with one retained token based on cosine similarity. Similar to D2O based on cosine similarity, AIM <ref type="bibr" target="#b105">[106]</ref> and Look-M <ref type="bibr" target="#b106">[107]</ref> merges Keys (resp. Values) of multiple tokens into one. CaM <ref type="bibr" target="#b103">[104]</ref> merges the Keys (or Values) of multiple evicted tokens with retained tokens based on attention scores to get the final merged results. Also, KVMerger <ref type="bibr" target="#b107">[108]</ref> first identifies the merge token sets by clustering consecutive tokens with high cosine similarity, ensuring that only adjacent tokens with strong contextual relevance are grouped together. Then, KVMerger merges the tokens in each merge set into the pivotal token (chosen based on the highest attention score) using Gaussian kernel weights, where closer tokens contribute more to the merged state.</p><p>Instead of merging the KV of multiple tokens into one, CHAI <ref type="bibr" target="#b108">[109]</ref> observes that heads in multi-head attention often produce highly correlated attention scores for tokens, particularly in the later layers of LLMs. To exploit this redundancy, CHAI <ref type="bibr" target="#b108">[109]</ref> clusters attention heads within each layer that produce similar outputs and computes attention for only a single representative head in each cluster. Specifically, within each cluster, CHAI <ref type="bibr" target="#b108">[109]</ref> selects one representative head to perform the attention computation, and the computed attention scores are shared across all heads in the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Cross-layer Merging</head><p>MiniCache <ref type="bibr" target="#b98">[99]</ref> observes that KV caches in middle-to-deep layers exhibit high angular similarity, making them ideal for merging. To achieve this, MiniCache <ref type="bibr" target="#b98">[99]</ref> merges the Key (and Value) pairs of each token from adjacent similar layers into a single shared representation. Specifically, MiniCache <ref type="bibr" target="#b98">[99]</ref> decomposes KV vectors into magnitude and direction components, storing only the shared directional vectors, token magnitudes, and unmergeable tokens to maximize memory efficiency. Differently, KVSharer <ref type="bibr" target="#b99">[100]</ref> observes a counterintuitive phenomenon: when the KV caches of two layers differ significantly, sharing one layer's KV cache with another during inference does not cause significant performance degradation. Based on this observation, KVSharer <ref type="bibr" target="#b99">[100]</ref> computes the Euclidean distance between the KV caches of all layer pairs, ranks the pairs by dissimilarity, and prioritizes the most dissimilar layers for sharing. Since KVSharer <ref type="bibr" target="#b99">[100]</ref> can share the KV cache of one layer to multiple other layers, the stored KV cache is eliminated significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Summary and Future Directions</head><p>KV cache merging represents a transformative approach to optimizing memory utilization in LLMs by consolidating or compressing KV caches while maintaining high model accuracy. However, there are several key directions and challenges for future exploration in this domain. Firstly, current KV cache merging methods are typically designed to work across a wide range of tasks, but fine-tuning merging strategies for specific tasks or domains could further enhance efficiency. For example, certain tasks may tolerate more aggressive merging due to inherent redundancy in their attention patterns, while others may require more conservative approaches to preserve accuracy. Adaptive merging mechanisms that adjust compression levels on-thefly based on task difficulty, sequence length, or available hardware resources are an exciting avenue for future work. Secondly, sparse attention mechanisms, which already reduce the computational complexity of attention by operating on subsets of tokens, could be combined with KV cache merging to achieve even greater efficiency. Exploring how merging complements sparsity-based approaches, such as block-sparse or low-rank attention, could lead to novel hybrid solutions. Thirdly, while empirical results show that merging does not significantly degrade performance, providing theoretical guarantees about the preservation of critical information could enhance the reliability of these methods. Future work might focus on quantifying the rela- WKVQuant <ref type="bibr" target="#b87">[88]</ref> Learnable shifting ✓ QAQ <ref type="bibr" target="#b147">[148]</ref> Adaptive quantization bits</p><formula xml:id="formula_16">✓ ✓ ✓ ✓ MiKV [90] Dynamic outlier-aware ✓ ✓ ✓ GEAR [89] Dynamic outlier-aware ✓ ✓ ZIPVL [91] Conventional ✓ ✓ ✓</formula><p>CacheGen <ref type="bibr" target="#b148">[149]</ref> Layer-wise, token-locality</p><formula xml:id="formula_17">Atom [150] Group-based ✓ ✓</formula><p>tionship between merging strategies, token importance, and attention accuracy to provide more formal guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">KV Cache Quantization</head><p>Quantization technique <ref type="bibr" target="#b150">[151]</ref>, <ref type="bibr" target="#b151">[152]</ref>, <ref type="bibr" target="#b152">[153]</ref>, <ref type="bibr" target="#b153">[154]</ref>, <ref type="bibr" target="#b154">[155]</ref> has been widely used to accelerate machine learning models from different aspects, such model parameter quantization <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b156">[157]</ref>, <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b158">[159]</ref> and data feature quantization <ref type="bibr" target="#b159">[160]</ref>, <ref type="bibr" target="#b160">[161]</ref>. Similarly, Key-Value (KV) cache quantization is emerging as a highly promising solution to address the memory and computational bottlenecks in LLMs. During autoregressive decoding, LLMs generate key-value pairs for every attention layer across all tokens in the sequence.</p><p>If we store all KV pairs in the memory with full precision, this cache grows exponentially with longer sequences, increasing the memory and bandwidth requirements significantly. Quantization reduces the precision of numerical representations (e.g., from FP32 to INT8 or INT4), drastically compressing the size of the KV cache. This compression can achieve up to 4x or more memory savings, making it feasible for LLMs to operate on resource-constrained devices like GPUs with limited memory or edge devices. However, the presence of outliers in Keys and Values poses a significant challenge for low-bit quantization, as these extreme values can lead to substantial performance degradation when compressed into reduced bit representations <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b163">[164]</ref>. Based on the techniques used, existing KV cache quantization approaches can be classified into three main categories: Fixed-precision quantization, where all Keys and Values are quantized to the same bitwidth; Mixed-precision quantization, which assigns higher precision to critical parts of the cache while using lower precision for less important components; and Outlier redistribution, which redistributes or smooths the outliers in Keys and Values to improve quantization quality. These methods collectively enable efficient KV cache compression while mitigating the performance degradation typically associated with low-bit quantization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Fixed-precision Quantization</head><p>Fixed-precision quantization proposes quantizing different Keys (different Values) of tokens to the same bit-width.</p><p>ZeroQuant <ref type="bibr" target="#b94">[95]</ref> propose per-token quantization for Keys and Values. As shown in Fig. <ref type="figure" target="#fig_1">4</ref>, the per-token quantization approach quantizes tokens individually. Particularly, Zero-Quant <ref type="bibr" target="#b94">[95]</ref> dynamically computes the min-max range for each token during inference. This ensures that each token is quantized based on its unique range, significantly reducing quantization error. Also FlexGen <ref type="bibr" target="#b95">[96]</ref> and QJL <ref type="bibr" target="#b96">[97]</ref> directly perform per-token quantization for Keys and Values, where the scaling factor and zero-point are shared among all elements within the same token. PQCache <ref type="bibr" target="#b97">[98]</ref> uses product quantization approaches <ref type="bibr" target="#b160">[161]</ref>, <ref type="bibr" target="#b164">[165]</ref> to compress KV pairs. However, uniform quantization approaches, which use a fixed bit-width for keys and values across all tokens, can often be suboptimal. It is because they ignore the varying importance of tokens <ref type="bibr" target="#b111">[112]</ref> and account for the outlier patterns in Keys and Values <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b147">[148]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Mixed-precision quantization</head><p>Unlike fixed-precision quantization, where all Keys or Values are quantized to the same bit-width (e.g., 4-bit or 8bit), mixed-precision quantization assigns higher or full precision to Keys and Values of critical tokens and parts while using lower precision for less critical parts. The summary of KV mixed-precision quantization is listed in Tab. 5. KVQuant <ref type="bibr" target="#b83">[84]</ref> proposes several strategies to quantize Keys and Values smoothly based on observations. Firstly, KVQuant observes that the key values exhibit outliers in specific channels prior to applying Rotary Positional Embedding (RoPE). However, after applying RoPE, the magnitudes of these outlier channels become less consistent, creating a unique challenge for low-precision quantization. Thus, KVQuant <ref type="bibr" target="#b83">[84]</ref> proposes to quantize the Keys per channel before applying the RoPE operations and to quantize the Values per token. Secondly, KVQuant <ref type="bibr" target="#b83">[84]</ref> observes that KV cache activations contain outliers that skew the quantization range. To address this, they isolate outliers per vector (e.g., per-channel or per-token), store them in a sparse format, and quantize the remaining values to a narrower range. Thirdly, LLMs disproportionately allocate high attention scores to the first token (i.e., attention sink), and quantizing the first token will damage the performance of LLMs. Thus, KVQuant <ref type="bibr" target="#b83">[84]</ref> retains the first token in full precision (FP16) while quantizing the rest of the sequence, which is also used by IntactKV <ref type="bibr" target="#b84">[85]</ref> and SKVQ <ref type="bibr" target="#b85">[86]</ref>. Similar to KVQuant <ref type="bibr" target="#b83">[84]</ref>, KIVI <ref type="bibr" target="#b86">[87]</ref> quantizes the Key cache per-channel, as certain channels exhibit large outliers, and the Value cache pertoken, as there are no significant outlier patterns in the Value cache. Additionally, KIVI <ref type="bibr" target="#b86">[87]</ref> retains the most recent Keys and Values in full precision, while quantizing older KVs. This approach is based on the observation that the most recent KVs are critical for generating subsequent tokens. Similar to KIVI <ref type="bibr" target="#b86">[87]</ref>, WKVQuant <ref type="bibr" target="#b87">[88]</ref> temporarily retains the most recent Keys and Values in full precision, while quantizing only the past KV cache. This approach helps preserve precision during computation. Additionally, WKVQuant <ref type="bibr" target="#b87">[88]</ref> introduces a two-dimensional quantization strategy, which optimizes parameter matrix to align the values in the KV cache into a smoother and more uniform range, significantly improving quantization quality. GEAR <ref type="bibr" target="#b88">[89]</ref>, MiKV <ref type="bibr" target="#b89">[90]</ref>, ZipCache <ref type="bibr" target="#b91">[92]</ref> and ZIPVL <ref type="bibr" target="#b90">[91]</ref> quantize the KV cache based on the importance of each to achieve efficient and effective compression. First, GEAR <ref type="bibr" target="#b88">[89]</ref> applies quantization to compress the majority of less important entries (e.g., 98%) to ultra-low precision, significantly reducing memory usage. Next, GEAR <ref type="bibr" target="#b88">[89]</ref> employs a lowrank matrix to approximate residual errors, capturing structured patterns in the data. Also, GEAR <ref type="bibr" target="#b88">[89]</ref> uses a sparse matrix stores outliers, correcting individual errors caused by these values. MiKV <ref type="bibr" target="#b89">[90]</ref> is a mixed-precision KV cache quantization approach. Based on the importance of each token, measured using existing methods like H2O <ref type="bibr" target="#b165">[166]</ref> and SnapKV <ref type="bibr" target="#b133">[134]</ref>, MiKV <ref type="bibr" target="#b89">[90]</ref> stores less important KV pairs in low precision while retaining the most important KV pairs in high precision. Instead of approximating the importance weight of each token, ZipCache <ref type="bibr" target="#b91">[92]</ref> accurately computes the importance of each token. Instead of computing importance score, PrefixQuant <ref type="bibr" target="#b92">[93]</ref> observes that token-wise outliers frequently occur at fixed positions (e.g., initial tokens) or low-semantic-value tokens (e.g., ".", "\n"). Based on this observation, PrefixQuant <ref type="bibr" target="#b92">[93]</ref> identifies high-frequency outlier tokens in LLMs offline and prefixes them in the KV cache, effectively eliminating token-wise outliers. Similarly, MiniKV <ref type="bibr" target="#b93">[94]</ref> observes that important tokens can be identified before generation and remain consistent throughout the generation process, retaining these important tokens in high precision.</p><p>QAQ <ref type="bibr" target="#b147">[148]</ref> proposes a quality adaptive quantization approach to dynamically determine the suitable quantization bit for each token, based on its importance and sensitivity, while handling outliers and exceptions to maintain model performance. SKVQ <ref type="bibr" target="#b85">[86]</ref> introduces the clipped dynamic quantization with channel reorder. First, SKVQ <ref type="bibr" target="#b85">[86]</ref> uses a transformation-invariant permutation to group similar channels based on their statistical characteristics and applies clipped dynamic quantization to mitigate the outlier problem. Second, SKVQ <ref type="bibr" target="#b85">[86]</ref> maintains high precision for the initial tokens and the most recent tokens while quantizing older tokens. Consequently, SKVQ <ref type="bibr" target="#b85">[86]</ref> effectively reduces quantization errors and improves the accuracy of the quantized model. CacheGen <ref type="bibr" target="#b148">[149]</ref> and AsymKV <ref type="bibr" target="#b166">[167]</ref> use layerwise asymmetric quantization, assigning higher-bit precision to key matrices in sensitive early layers and lowerbit precision to less sensitive layers, balancing memory efficiency and performance. Particularly, CacheGen <ref type="bibr" target="#b148">[149]</ref> also exploits token-wise locality by encoding deltas (differences) between KV tensors of nearby tokens instead of raw values. Atom <ref type="bibr" target="#b149">[150]</ref> identifies and separates outlier channels, reordering the matrix to group these outlier channels at the end, thereby ensuring regular memory access patterns for improved hardware utilization. Then, Atom <ref type="bibr" target="#b149">[150]</ref> quantizes outliers with higher precision, while normal channels are quantized to INT4 for maximum efficiency. In particular, Atom <ref type="bibr" target="#b149">[150]</ref> applies fine-grained group quantization by dividing matrices into smaller subgroups (e.g., 128 elements per group) and performing quantization independently within each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Outlier Redistribution</head><p>As previously mentioned, outliers in the Keys and Values present significant challenges for their quantization. Recent research has proposed two main approaches to address this issue: redistributing the outliers into newly appended virtual tokens or applying equivalent transformation functions to smooth the Keys and Values for improved quantization accuracy. The summary of existing outlier redistribution models are listed in Table. <ref type="bibr" target="#b5">6</ref>.</p><p>Specifically, MassiveActivation <ref type="bibr" target="#b71">[72]</ref> highlights the phenomenon of massive activations in large language models (LLMs), where a small subset of activations is exponentially larger than the rest. To address this, MassiveActivation <ref type="bibr" target="#b71">[72]</ref> proposes appending a virtual token to the inputs, allowing LLMs to encapsulate the massive outliers within these learned keys and values for each head. Then, we introduce the equivalent transformation function-based approaches. Firstly, QuaRot <ref type="bibr" target="#b72">[73]</ref>, Qserve <ref type="bibr" target="#b73">[74]</ref>, and Q-INT4 <ref type="bibr" target="#b74">[75]</ref> redistributes outlier values across all channels by Hadamard rotation, successfully lowering the maximum value of outlier tokens. The Hadamard rotation of activations can be incorporated into the preceding linear layer, thereby redistributing the outliers of Keys and Values into the parameters. Despite this improvement, outlier tokens still exhibit magnitudes hundreds of times greater than normal tokens, causing notable performance issues when using shared quantization scales across tokens <ref type="bibr" target="#b92">[93]</ref>. Expanding on this idea, SpinQuant <ref type="bibr" target="#b75">[76]</ref> proposes training an orthogonal matrix instead of relying on a random Hadamard matrix to achieve better performance. Similarly, DuQuant <ref type="bibr" target="#b76">[77]</ref> employs channel permutation to evenly distribute outliers across blocks and utilizes block rotation to further smooth outliers.</p><p>TABLE 6 The summary of outlier redistribution models in Sec. 4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Operation Formula Learn Remarks</head><p>MassiveAct. <ref type="bibr" target="#b71">[72]</ref> Add virtual tokens softmax</p><formula xml:id="formula_18">Q K T , k ′ √ d V v ′T ✓ Learnable k ′ , v ′ QuaRot [73] Hadamard rotation XW ⊤ = (XH)(H ⊤ W ⊤ ) × H ⊤ H = I</formula><p>Qserve <ref type="bibr" target="#b73">[74]</ref> Hadamard rotation</p><formula xml:id="formula_19">XW ⊤ = (XH)(H ⊤ W ⊤ ) × H ⊤ H = I Q-INT4 [75] Hadamard rotation XW ⊤ = (XH)(H ⊤ W ⊤ ) × H ⊤ H = I SmoothQuant [78] Scaling (X diag(s) -1 ) • (diag(s)W ⊤ ) × s ∈ R c i QS+ [79] Scaling, Shifting ((X -z) diag(s) -1 • diag(s) + z)W ⊤ × s ∈ R c i AWQ [82] Scaling arg mins XW ⊤ -X diag(s) -1 )Q(diag(s)W ⊤ ) ✓ Quantization Q(•)</formula><p>OmniQuant <ref type="bibr" target="#b82">[83]</ref> Scaling, shifiting</p><formula xml:id="formula_20">Qa X-δ s Qw s ⊙ W ⊤ + B + δW ⊤ ✓ Learnable Qa(•), Qw(•) DuQuant [77] Rotation, permutation [(X • Λ) R(1) • P • R(2) ] • [ R⊤ (2) • P ⊤ • R⊤ (1) (Λ -1 • W ⊤ )] × Matrices P, R</formula><p>AffineQuant <ref type="bibr" target="#b79">[80]</ref> Affine transform</p><formula xml:id="formula_21">arg min P XW ⊤ -XP -1 Q(PW ⊤ ) 2 F ✓ Quantization Q(•)</formula><p>FlatQuant <ref type="bibr" target="#b80">[81]</ref> Affine transform AffineQuant + P = P1 ⊗ P2 ✓ Decomposition SmoothQuant <ref type="bibr" target="#b77">[78]</ref> leverages a key observation that different tokens show similar patterns of variation across their channels. Based on this insight, it strategically shifts the quantization complexity from activations to weights through an offline process. Specifically, SmoothQuant <ref type="bibr" target="#b77">[78]</ref> introduces a mathematically equivalent per-channel scaling transformation: Y = (Xdiag(s) -1 ) • (diag(s)W) = X Ŵ where X represents Keys or Values, and the smoothing factor s ∈ R Ci is used to scale X. This transformation achieves two key benefits: it smooths the distribution of Keys and Values to facilitate easier quantization, and it allows the smoothing factors to be efficiently incorporated into the parameters of previous layers during offline processing. In particular, the smooth factor s is dynamically decided on based on inputs. Similarly, The OS+ <ref type="bibr" target="#b78">[79]</ref> introduces channel-wise shifting to eliminate outlier asymmetry and channel-wise scaling to reduce outlier concentration. These operations are seamlessly migrated to subsequent layers, maintaining equivalence with the floating-point model while improving quantization performance.</p><p>Instead of using handcrafted transformations <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref> to shift the quantization difficulty from activations to weights, AffineQuant <ref type="bibr" target="#b79">[80]</ref> uses an affine transformation matrix that combines both scaling and rotation transformations. This allows it to optimize weight distributions more effectively, aligning them better with the quantization function and reducing quantization errors. The affine transformation matrix provides richer flexibility compared to SmoothQuant's scalar-based scaling, enabling finer adjustments to the weight and activation distributions. Based on AffineQuant <ref type="bibr" target="#b79">[80]</ref>, FlatQuant <ref type="bibr" target="#b80">[81]</ref> introduces a fast and learnable affine transformations to enhance the flatness of weights and activations, which decomposes transformations into smaller matrices to reduce memory and computational costs. Similarly, AWQ <ref type="bibr" target="#b81">[82]</ref> and OmniQuant <ref type="bibr" target="#b82">[83]</ref> proposes differentiable and learnable equivalent transformations, which optimize the equivalent parameters (e.g., channel-wise scaling and shifting) in an end-to-end manner using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Summary and Future Directions</head><p>KV cache quantization is a crucial technique for reducing memory and computational overhead in large language models (LLMs) during autoregressive decoding. While significant progress has been made, this field remains dynamic and rapidly evolving, with several promising directions for future research. Firstly, one promising avenue is the development of real-time adaptive quantization methods. These techniques could dynamically adjust quantization levels during inference based on real-time metrics such as token importance, outlier presence, or sequence length. Such an approach could significantly enhance efficiency while maintaining performance, especially for processing long sequences with varying levels of complexity. Secondly, another important direction is extending KV cache quantization to multi-modal and multi-task models. Multi-modal models, which process inputs from diverse domains such as text, vision, and audio, and multi-task scenarios often exhibit highly diverse attention patterns and memory demands. This necessitates the design of more advanced and tailored quantization strategies to balance efficiency and accuracy in these increasingly complex settings.</p><p>Thirdly, hybrid quantization techniques also hold significant potential. By combining fixed-precision, mixedprecision, and outlier redistribution methods, researchers could develop more versatile and efficient quantization frameworks. For instance, integrating mixed-precision allocation schemes with outlier smoothing transformations could optimize both memory usage and performance, offering a flexible approach adaptable to a variety of tasks and models. finally, addressing the challenge of outliers remains a critical area of focus. Outliers can have a disproportionate impact on quantization efficiency and model performance. Future research could explore advanced outlier detection mechanisms or innovative encoding techniques to mitigate their effects. Improved handling of outliers could further enhance the effectiveness of quantization methods, enabling more robust and memory-efficient implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">KV Cache Low-rank Decomposition</head><p>Existing studies have demonstrated that the majority of information within KV caches can be captured by a small subset of their singular values or low-rank components, making low-rank decomposition a powerful tool for compression. By leveraging this property, KV cache low-rank decomposition techniques aim to reduce memory requirements while preserving the essential information required for accurate attention computations. Low-rank decomposition strategies can be classified into three main approaches: Singular Value Decomposition (SVD), which exploits the low-rank structure of KV matrices to retain the most critical singular values; Tensor Decomposition, which factorizes KV matrices into smaller components for minimal redundancy; and Learned Low-rank Approximation, which incorporates adaptive mechanisms to optimize compression based on learned representations. Each method provides a unique balance of computational efficiency and accuracy retention, enabling scalable and memory-efficient LLM inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Singular Value Decomposition</head><p>Firstly, ECKVH <ref type="bibr" target="#b65">[66]</ref>, EigenAttention <ref type="bibr" target="#b66">[67]</ref>, and ZDC <ref type="bibr" target="#b67">[68]</ref> shows that KV caches have a low-rank property, where a small number of top singular values retain most of the information. Using Singular Value Decomposition (SVD), the method compresses KV caches by grouping heads, applying SVD, and retaining top singular values, effectively reducing the number of KV heads with minimal error. Also, ZDC <ref type="bibr" target="#b67">[68]</ref> uses an adaptive hybrid compression ratio mechanism to assign higher compression to unimportant tokens in shallower layers while preserving more important tokens in deeper layers, leveraging the similarity of token characteristics in adjacent layers. Secondly, rather than decomposing KV pairs, LoRC <ref type="bibr" target="#b68">[69]</ref> employs a low-rank approximation of KV weight matrices and adopts a progressive compression strategy to efficiently compress KV caches without requiring model retraining. Specifically, LoRC <ref type="bibr" target="#b68">[69]</ref> uses SVD to compress the Keys and Values parameter matrices (i.e., W k i and</p><formula xml:id="formula_22">W v i ) as W k i = U k i Σ k i P k i ⊤ and W v i = U v i Σ v i P v i ⊤</formula><p>. Also, compression is applied conservatively in shallower layers to minimize error amplification and more aggressively in deeper layers. Then, instead of storing</p><formula xml:id="formula_23">K i = X i W k i and V i = X i W v i , it only stores Ki = X i U k i and Vi = X i U v i , along with Σ k i P k i ⊤ and Σ v i P v i ⊤</formula><p>. Also, ShadowKV <ref type="bibr" target="#b69">[70]</ref> performs SVD decomposition directly on pre-RoPE keys to reduce the dimensionality of the key representations. Palu <ref type="bibr" target="#b70">[71]</ref> applies SVD to compress both Keys and Values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Tensor Decomposition</head><p>Tensor decomposition <ref type="bibr" target="#b167">[168]</ref>, <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b169">[170]</ref> is a widely used algorithm for factorizing a matrix into a sequential product of local tensors, such as Matrix Product Operator (MPO) <ref type="bibr" target="#b170">[171]</ref> and turker decomposition <ref type="bibr" target="#b171">[172]</ref> . Taking Matrix Product Operator (MPO) <ref type="bibr" target="#b170">[171]</ref> as an example, the decomposition of a matrix W ∈ R I×J using MPO can be defined as:</p><formula xml:id="formula_24">TD(W) = n k=1 T (k) [d k-1 , i k , j k , d k ],<label>(12)</label></formula><p>where</p><formula xml:id="formula_25">T (k) represents the local tensor of size d k-1 × i k × j k × d k , with n k=1 i k = I and n k=1 j k = J.</formula><p>Here, n denotes the number of local tensors, collectively referred to as the decomposed tensors. As shown in Eaquation <ref type="bibr" target="#b11">(12)</ref>, MPO-based tensor decomposition is well-suited for KV cache compression as it reduces the memory footprint by factorizing large key and value matrices into smaller local tensors, enabling efficient storage while preserving essential information. This approach minimizes redundancy and maintains the structural integrity required for accurate attention computations. DecoQuant <ref type="bibr" target="#b64">[65]</ref> combines quantization with low-rank decomposition to effectively reduce quantization errors. Specifically, DecoQuant <ref type="bibr" target="#b64">[65]</ref> leverages the Matrix Product Operator (MPO) to decompose matrices into smaller local tensors. The larger tensors, which contain most of the parameters, are quantized to low-bit precision, while the smaller tensors retain high precision to minimize overall quantization error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Learned Low-rank Approximation</head><p>LESS <ref type="bibr" target="#b62">[63]</ref> introduces a novel learned-kernel-based lowrank approximation approach to efficiently approximate the results of the softmax function. Specifically, LESS <ref type="bibr" target="#b62">[63]</ref> replaces the softmax with a separable similarity metric, ϕ(q t )ψ(K t ) ⊤ , where ϕ and ψ are row-wise functions. Here, q t ∈ R 1×D represents the query, and K t ∈ R t×D represents the keys at step t. To elaborate, if ϕ and ψ are such that:</p><formula xml:id="formula_26">a t = softmax qtK ⊤ t √ D V t ≈ ϕ(qt)ψ(Kt) ⊤ Vt ϕ(qt)ψ(Kt) ⊤ 1 S×1</formula><p>, then we only need to cache the hidden states <ref type="bibr" target="#b63">[64]</ref> compresses KV caches along the feature dimension by leveraging trainable orthogonal projection matrices.</p><formula xml:id="formula_27">H t = ψ(K t ) ⊤ V t ∈ R R×D and the normalization factor z t = t s=1 ψ([K t ] s ) ∈ R 1×R for inference. Similarly, MatryoshkaKV</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Summary and Future Directions</head><p>KV cache low-rank decomposition is a powerful technique for compressing KV caches in LLMs while maintaining the quality of attention computations. Current methods primarily rely on fixed low-rank approximations applied uniformly across all layers or tokens. However, future advancements could focus on dynamic rank adjustment, where the rank is tailored based on token importance, sequence length, or layer-specific properties, enabling a more optimal balance between memory efficiency and performance. Additionally, real-time or streaming applications present a promising avenue for exploration. Since KV caches grow dynamically during inference, lightweight and incremental decomposition methods that can adapt efficiently to expanding sequences will be critical for supporting such scenarios without compromising latency or accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MODEL-LEVEL OPTIMIZATION</head><p>In model-level optimization, new architectures or mechanisms are designed for transformers to allow more efficient reuse of KV cache. Typically, these methods require retraining or fine-tuning of the model to come into operation. Nevertheless, efficient transformation pipelines have also been proposed to allow for a fast deployment to new architectures. According to where and how the refinement was MixCon <ref type="bibr" target="#b172">[173]</ref>, GoldFinch <ref type="bibr" target="#b173">[174]</ref>, Recur-Former <ref type="bibr" target="#b174">[175]</ref> Adaptive Sequence Processing Architecture (Sec. 5.3.1)</p><p>RWKV <ref type="bibr" target="#b175">[176]</ref>, Mamba <ref type="bibr" target="#b176">[177]</ref>, RetNet <ref type="bibr" target="#b177">[178]</ref>, MCSD <ref type="bibr" target="#b178">[179]</ref> Architecture Alteration (Sec. 5.2) Augmented Architecture (Sec. 5.2.2) YOCO <ref type="bibr" target="#b179">[180]</ref>, CEPE <ref type="bibr" target="#b180">[181]</ref>, XC-Cache <ref type="bibr" target="#b181">[182]</ref>, Block Transformer <ref type="bibr" target="#b182">[183]</ref> Enhanced Attention (Sec. 5.2.1) MLA <ref type="bibr" target="#b26">[27]</ref>, FLASH <ref type="bibr" target="#b183">[184]</ref>, Infini-Attention <ref type="bibr" target="#b184">[185]</ref> Attention Grouping and Sharing (Sec. 5.1)</p><p>Cross-Layer Sharing (Sec. 5.1.2) CLA <ref type="bibr" target="#b185">[186]</ref>, LCKV <ref type="bibr" target="#b186">[187]</ref>, SA <ref type="bibr" target="#b187">[188]</ref>, MLKV <ref type="bibr" target="#b188">[189]</ref>, LISA <ref type="bibr" target="#b189">[190]</ref>, Wu et al. <ref type="bibr" target="#b190">[191]</ref>, CLLA <ref type="bibr" target="#b191">[192]</ref>, DHA <ref type="bibr" target="#b192">[193]</ref>, SV-Former <ref type="bibr" target="#b193">[194]</ref> Intra-Layer Grouping (Sec. 5.1.1)</p><p>MQA <ref type="bibr" target="#b194">[195]</ref>, GQA <ref type="bibr" target="#b195">[196]</ref>, AsymGQA <ref type="bibr" target="#b196">[197]</ref>, Weighted GQA <ref type="bibr" target="#b197">[198]</ref>, QCQA <ref type="bibr" target="#b198">[199]</ref>, KDGQA <ref type="bibr" target="#b199">[200]</ref>, GQKVA <ref type="bibr" target="#b200">[201]</ref> Fig. <ref type="figure">5</ref>. Taxonomy of the model based KV optimization for Large Language Models.</p><p>made to the models, we separate related works to the grouping and sharing mechanisms within or cross layers (Sec. 5.1), implementing architecture modification or augmentation (Sec. 5.2), and incorporating non-transformer architectures for optimization (Sec. 5.3). The taxonomy of the model-level optimization is shown in Fig. <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Attention Grouping and Sharing</head><p>This section explores attention grouping and sharing methods as effective strategies for optimizing key-value (KV) management. We categorize the approaches into two distinct subtypes: intra-layer grouping (Sec. 5.1.1) that focuses on grouping query, key, and value heads within individual layers to reduce redundancy and improve efficiency, and cross-layer sharing 5.1.2 that shares key, value, or attention components across layers to improve information reuse and reduce KV cache requirements. The summary of attention grouping and sharing is listed in Tab. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Intra-layer Grouping</head><p>Shazeer first introduced Multi-Query Attention (MQA) <ref type="bibr" target="#b194">[195]</ref> that modified the traditional multi-head attention mechanism. In MQA, all attention heads in a transformer block share a single key and value. This simple strategy can greatly accelerate the decoding procedure. The experiments of the author show that MQA would gain much efficiency with only minor quality degradation incurring. MQA is a radical strategy that would cause not just quality degradation, but also training instability. GQA (Grouped Query Attention) <ref type="bibr" target="#b195">[196]</ref> introduced a trade-off solution by dividing the query heads into multiple groups, while each group shares its own keys and values. In addition, an uptraining process is proposed to efficiently convert existing MHA models to GQA configurations by mean-pooling the key and value heads associated with each group. Empirical evaluations demonstrated that GQA models achieve performance close to the original MHA models while offering inference time comparable to MQA.</p><p>There were several extensions based on GQA. AsymGQA <ref type="bibr" target="#b196">[197]</ref> extends GQA by proposing an activationinformed merging strategy. Instead of grouping the heads by uniform clustering, AsymGQA dynamically determines the grouping of quries based on their activations similarities during training and constructs an asymmetric group results, which leads to better optimization and generalization. Weighted GQA <ref type="bibr" target="#b197">[198]</ref> introduces additional trainable weights to each key and value head, which can be seamlessly integrated into existing GQA models. By tuning weights during training, it improves the performance of the model without additional inference overhead. QCQA <ref type="bibr" target="#b198">[199]</ref> utilizes an evolutionary algorithm to identify the optimal query head groupings for GQA, which is guided by a computationally efficient fitness function that leverages the weight-sharing error and the KV cache to evaluate text generation quality and memory capacity. KDGQA <ref type="bibr" target="#b199">[200]</ref> argues that many variances of GQA adopt a fixed grouping strategy, thus lacking dynamic adaptability to the evolving of key-value interactions during training. Their Dynamic Key-Driven GQA address these issues by allocating groups using key head norms adaptively during training, resulting in a flexible strategy to query head grouping and enhance the performance.</p><p>GQKVA <ref type="bibr" target="#b200">[201]</ref> advances the grouping strategy and comes up with a generalized query, key and value grouping mechanism. It first introduces MKVA and GKVA, in which the key and value are grouped to share the same query. Based on this, GQKVA is proposed to separately group the query and key-value pairs. Typically, queries are partitioned into g q groups, and keys and values are partitioned into g kv groups, and each combination of query and key-value pairs would interact using dot product attention. This results in g q × g kv distinct outputs. It generalized different group strategy on query, key and value and preserves good computational efficiency and comparable performance as MHA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 7</head><p>The summary of Model-based Attention Grouping and Sharing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Applied Location Intra-layer Grouped Component</head><p>Cross-layer Shared Component Retraining Required Intra-layer Cross-layer MQA <ref type="bibr" target="#b194">[195]</ref> ✓ K, V -✓ GQA <ref type="bibr" target="#b195">[196]</ref> ✓ K, V -Uptrain AsymGQA <ref type="bibr" target="#b196">[197]</ref> ✓ K,V -Finetune Weighted GQA <ref type="bibr" target="#b197">[198]</ref> ✓ K,V -Uptrain &amp; Finetune QCQA <ref type="bibr" target="#b198">[199]</ref> ✓</p><formula xml:id="formula_28">K, V - ✓ KDGQA [200] ✓ K, V - ✓ GQKVA [201] ✓ Q, K, V - ✓ CLA [186] ✓ ✓ K, V K, V ✓ LCKV [187] ✓ - K, V ✓ SA [188] ✓ - Attention Weight ✓ MLKV [189] ✓ ✓ K, V K, V Uptrain LISA [190] ✓ Q, K, V Lightweight adaption Wu et al. [191] ✓ - Q, K, V ✓ CLLA [192] ✓ - Q, K, V ✓ DHA [193] ✓ ✓ K, V Q, K, V</formula><p>Lightweight adaption SVFormer <ref type="bibr" target="#b193">[194]</ref> ✓ -V ✓</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Cross-layer Sharing</head><p>Brandon et al. introduce Cross Layer Attention (CLA) <ref type="bibr" target="#b185">[186]</ref> that extends the ideas of GQA and MQA by sharing the key and value heads between adjacent layers, further reduce the redundancy in the KV cache. This achieves an additional 2× KV cache size reduction compared to MQA, significantly improving memory efficiency without altering computational complexity.</p><p>LCKV <ref type="bibr" target="#b186">[187]</ref> proposes only to compute and cache the key and value for a small subset of layers, even only the top layer, then let queries in bottom layers pair the saved keys and values for inference. This method not only drastically improves the inference speed and reduces memory consumption but is also orthogonal to existing memorysaving techniques, enabling straightforward integration for further optimization. While such a mechanism makes next token computation depend on top layer keys and values of previous tokens, which contradict to the parallel training of transformers, LCKV introduces an approximate training methods to support parallel training.</p><p>SA (Shared Attention) <ref type="bibr" target="#b187">[188]</ref> proposes reuse of computed attention weights across multiple layers, rather than recalculating them for each layer. Unlike other methods focusing on sharing key-value caches, SA leverages the isotropic tendencies of attention distributions observed in pre-trained LLMs to directly share attention weights, greatly reducing both computational overhead and memory usage.</p><p>MLKV (Multi-Layer Key-Value) <ref type="bibr" target="#b188">[189]</ref> introduces a simple KV head sharing mechanism across multiple transformer layers. MLKV uses the same single KV head as MQA within a layer, but it also shares this KV head with multiple layers. This extreme strategy reduces the cache size to almost 1% of normal GQA strategies, and experiments show that MLKV still has comparable performance.</p><p>LISA (Lightweight Substitute for Attention) <ref type="bibr" target="#b189">[190]</ref> makes a comprehensive analysis for the similarity of attention patterns across layers. Directly sharing attention weights across layers is ineffective because of the misalignment of the attention head and the sensitivity of shallow layers. LISA <ref type="bibr" target="#b189">[190]</ref> addresses challenges by incorporating tiny feedforward networks to align attention heads between layers and using low-rank matrices to approximate variations in layer-wise attention weights. This achieves a 6× compression of query and key parameters while maintaining high accuracy and perplexity.</p><p>Wu et al. <ref type="bibr" target="#b190">[191]</ref> introduce a unified framework that systematically analyzes and optimizes the cross-layer Key-Value cache sharing mechanism. They consolidate several existing methods, explore novel variants within a cohesive structure, and make thorough evaluations of these methods. The study finds that 2 times reduction to KV cache size can outperform standard transformers in throughput without substantial accuracy loss, while further reduction requires alternative design with additional training costs. With the analysis results, they offer insight into the choice of appropriate KV sharing methods based on the specific requirement or constraints.</p><p>CLLA (Cross-Layer Latent Attention) <ref type="bibr" target="#b191">[192]</ref> introduces an integrated framework combining multiple strategies: attention head size and dimension reduction, cross-layer cache sharing, and KV cache quantization. By unifying these strategies, CLLA achieves extreme KV cache compression to less than 2% of the original model size while maintaining performance levels comparable with uncompressed models.</p><p>DHA (Decoupled Head Attention) <ref type="bibr" target="#b192">[193]</ref> addresses redundancy in MHA and adaptively configures shared groups for key and value heads across layers, reducing KV cache requirements. Observing that clustering and fusing similar heads can reduce KV cache size without significant performance reduction, DHA designs a search, fusion, and continued pre-training framework that can progressively transform MHA checkpoints into DHA models through linear fusion of head parameters, preserving the pre-trained knowledge with small pre-training budget. Observing that later layers in traditional transformers overly rely on narrow regions of attention, Zhou et al. <ref type="bibr" target="#b193">[194]</ref> introduce ResFormer that utilizes residual connections from the value embeddings of the first layer to all subsequent layers, effectively approximating cross-layer attention without incurring significant computational costs. They then propose a simplified variant SVFormer that shares a single value embedding across all layers, dramatically reducing the KV cache size by nearly half while maintaining competitive performance. The proposed architectures are flexible to incorporate with other KV-efficient strategies for additional memory savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Summary and Future Directions</head><p>This section highlights innovative strategies for optimizing memory and computational efficiency through intralayer grouping and cross-layer sharing mechanisms. However, several avenues for improvement remain. First, maintaining performance while optimizing efficiency, especially for precision-sensitive tasks, requires further investigation. Methods that implement radical grouping and sharing mechanisms may compromise the model fidelity for tasks requiring high precision. Second, scalability across diverse model architectures and sizes is essential. Works such as DHA <ref type="bibr" target="#b192">[193]</ref> and LISA <ref type="bibr" target="#b189">[190]</ref>, which rely on specific architectural assumptions, may struggle to generalize to emerging LLMs or non-standard configurations. Third, the dynamics of attention across both time and layers are largely underexplored. Most existing methods rely on static or predetermined grouping and sharing strategies, neglecting the temporal and contextual variations in attention patterns.</p><p>To address these challenges and unlock the full potential of attention optimization, future research should focus on the following aspects. First, developing universal frameworks for attention grouping and sharing that require minimal retraining to enhance adaptability and usability. Second, synergistic integration with other optimization techniques, such as quantization and pruning, has significant potential to achieve even greater efficiency gains. While some works like CLLA <ref type="bibr" target="#b191">[192]</ref> have begun to address these opportunities, more exploration could be carried out to unlock new levels of efficiency. Third, more dynamic and temporal modeling could be leveraged to adaptively adjust grouping and sharing during runtime to better capture the contextual requirements of different tasks and sequences. Finally, a deeper understanding of the downstream impacts of these techniques on fine-tuning and transfer learning is crucial for their effective application in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Architecture Alteration</head><p>This section explores architectural modifications to optimize KV cache usage. We categorize these methods into two subsections: methods that refine the attention mechanism for KV cache efficiency (Sec. 5.2.1), and methods that introduce structural changes for better KV management (5.2.2). Many of these works build upon the broader landscape of efficient attention mechanisms (e.g., Linear Transformer <ref type="bibr" target="#b201">[202]</ref>, Performer <ref type="bibr" target="#b202">[203]</ref>, LinFormer <ref type="bibr" target="#b203">[204]</ref>, etc.). Since our focus lies on methods directly impacting KV cache handling, for a comprehensive overview of efficient attention mechanisms, we refer readers to dedicated surveys <ref type="bibr" target="#b44">[45]</ref>. The summary of architecture alteration for KV reuse is listed in Tab. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Enhanced Attention</head><p>DeepSeek-V2 <ref type="bibr" target="#b26">[27]</ref> introduced Multi-Head Latent Attention (MLA) that adopts a low-rank KV joint compression mechanism, replacing the full KV cache with compressed latent vectors. The model adopts trainable projection and expansion matrices to do the compression. This compression mechanism significantly reduces the memory requirement of the KV cache and allows the model to handle sequences up to 128K tokens.</p><p>FLASH <ref type="bibr" target="#b183">[184]</ref> incorporates the Gated Attention Unit (GAU) to replace the MHA mechanism in traditional transformers. GAU utilizes a single-head attention mechanism with gating functions that selectively modulates importance in information flow. FLASH employs a linear approximation method for attention computation through GAU module, which makes the model efficiently handle long contexts without the quadratic scaling of traditional self-attention, thus mitigating heavy KV cache issues.</p><p>Infini-Attention <ref type="bibr" target="#b184">[185]</ref> adopts representation compression to store long-term content. Furthermore, they introduce a hybrid attention mechanism of masked local attention and long-term linear attention. The masked local attention replaces the standard MHA to let the model only concentrate on local contexts, while the long-term linear attention utilizes compressed memory for far-reaching dependencies and uses linear attention for efficient aggregation. Thus, TABLE <ref type="table">9</ref> The summary of Non-Transformer Architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Key Mechanism No Traditional KV Cache KV Cache Compression</head><p>RWKV <ref type="bibr" target="#b175">[176]</ref> RNN-like with Transformer parallelism ✓ Mamba <ref type="bibr" target="#b176">[177]</ref> Selective state-space model ✓ RetNet <ref type="bibr" target="#b177">[178]</ref> Retention mechanism ✓ MCSD <ref type="bibr" target="#b178">[179]</ref> Slope-decay fusion ✓ MixCon <ref type="bibr" target="#b172">[173]</ref> Transformer + Conba + MoE ✓ GoldFinch <ref type="bibr" target="#b173">[174]</ref> RWKV + Modified Transformer ✓ RecurFormer <ref type="bibr" target="#b174">[175]</ref> Mamba replacing some attention heads ✓ infini-attention combines both local fine-grained and longrange compressed states, allowing a seamless balance between long-term and short-term context modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Augmented</head><p>Architecture YOCO <ref type="bibr" target="#b179">[180]</ref> builds a decoder-decoder architecture composed of two modules: a self-decoder and a cross-decoder. The self-decoder efficiently encodes global key-value caches, while the cross-decoder reuses these caches via crossattention. This design ensures that key-value pairs are only cached once, substantially reducing GPU memory usage while maintaining global attention capabilities. YOCO's computation flow also enables the prefilling to early exit, allowing faster prefill stages without altering the final output. CEPE <ref type="bibr" target="#b180">[181]</ref> interleaves additional cross-attention layers between the self-attention and feed-forward layers in the decoder model. It employs a small encoder to process long inputs chunk-by-chunk to encoded representations as crossattention layers' inputs. In this way, CEPE can prevent the needs for KV cache for every token and reduce computational cost by processing contexts in parallel. This also facilitates an existing LLMs to expand its contexts while preserving the scalability and generalizability.</p><p>XC-Cache <ref type="bibr" target="#b181">[182]</ref> also utilizes an encoder to interleave cross-attention layers within existing self-attention layers in pre-trained decoder-only models to prevent explicit prompt caching. The encoder processes the context and converts it into a compact set of key-value pairs that summarize the essential information. It also finds that pre-trained causal decoders can be used to replace an encoder for representations extraction, further reducing the training costs on additional encoder.</p><p>Block Transformer <ref type="bibr" target="#b182">[183]</ref> introduces a hierarchical globalto-local architecture by combining coarse-grained global attention and fine-grained local attention. In lower layers, tokens are grouped into fixed-size blocks, allowing global context modeling with reduced KV cache overhead. In upper layers, attention operates within individual blocks, enabling lightweight, detailed token decoding with a smaller local KV cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Summary and Future Directions</head><p>This section explores research that introduces novel attention mechanisms or architectural modifications to improve KV cache management. Although these approaches demonstrate significant progress in enabling longer context windows and faster inference, several challenges remain. First, many methods, such as CEPE <ref type="bibr" target="#b180">[181]</ref> and XC-Cache <ref type="bibr" target="#b181">[182]</ref> demonstrate strong performance on retrieval-augmented tasks but may not generalize well across diverse workloads. This necessitates further research into task-adaptive KV cache optimization strategies that dynamically adjust caching behavior to optimize for different task demands. Secondly, integrating these novel mechanisms into existing pretrained models often requires extensive retraining, hindering their adoption in resource-constrained environments. Developing lightweight, modular approaches for retrofitting efficient KV caching into existing architectures is crucial for a wider practical impact. Finally, the robustness and stability of these new mechanisms under real-world conditions, such as noisy or dynamically changing inputs, require further investigation. Addressing these limitations could improve reliability and efficiency in practical deployments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Non-Transformer Architecture</head><p>While transformers are struggling with KV cache issues, researchers have revisited principles from traditional sequential architectures, such as recurrent neural networks (RNNs) <ref type="bibr" target="#b204">[205]</ref>, which inherently process sequences without the need for explicit KV caches. Inspired by the lightweight and memory-efficient design of RNNs and efficient attention mechanisms, non-transformer architectures <ref type="bibr" target="#b175">[176]</ref>, <ref type="bibr" target="#b176">[177]</ref>, <ref type="bibr" target="#b205">[206]</ref>, <ref type="bibr" target="#b206">[207]</ref>, <ref type="bibr" target="#b207">[208]</ref>, <ref type="bibr" target="#b208">[209]</ref> have emerged, such as Mamba <ref type="bibr" target="#b176">[177]</ref> and RWKV <ref type="bibr" target="#b175">[176]</ref>, offering promising alternatives. While there are a large type of new architectures, we only list methods associated with KV optimization. For further understanding to efficient non-transformer works, please refer to these surveys <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b209">[210]</ref>, <ref type="bibr" target="#b210">[211]</ref>, <ref type="bibr" target="#b211">[212]</ref>. The summary of non-transformer is listed in Tab. 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Adaptive Sequence Processing Architectures</head><p>RWKV <ref type="bibr" target="#b175">[176]</ref>, which means Receptance Weighted Key Value, is an architecture that combines the strengths of RNNs and transformers to achieve efficient sequence processing. RWKV integrates a linear attention mechanism, enabling parallelizable training like transformers while retaining the efficient inference characteristics of RNNs. By formulating the architecture to operate as either a transformer or an RNN, RWKV achieves constant computational and memory complexity during inference, overcoming the quadratic scaling issues of transformers.</p><p>Mamba <ref type="bibr" target="#b176">[177]</ref> is built based on state space sequence models (SSMs) <ref type="bibr" target="#b212">[213]</ref>, <ref type="bibr" target="#b213">[214]</ref>. Inspired by the state space systems, SSMs build scalable and memory-efficient long-range sequence modeling frameworks. Mamba improves SSMs by making parameters input-dependent, allowing information to be selectively propagated or forgotten along the sequence based on the current token. This addresses the inability of traditional SSMs to effectively handle the complexity of nonlinear dependencies in natural languages. Mamba omits attention and even MLP blocks, relying entirely on these selective state spaces for sequence modeling. It also develops a hardware-aware parallel algorithm for efficient recurrent computations in training and inference. Mamba achieves linear scaling in sequence length, demonstrating exceptional performance on sequences of up to a million tokens.</p><p>RetNet <ref type="bibr" target="#b177">[178]</ref> introduces Retentive Network that combines elements of recurrence and attention, presenting a novel retention mechanism for sequence modeling that offers training parallelism, low-cost inference, and scalable performance together. The proposed Multi-scale Retention Module (MSR) enables support to multiple computation paradigms: the parallel representation is similar to selfattention that adds support to casual masks and parallel training. The recurrent representation is similar to RNN that allows low-cost inference by maintaining state across sequence decoding. The chunkwise recurrent representation constructs a hybrid form to the former representations to further enables handling long sequences. These combined characteristics position RetNet as a strong alternative to transformers without a heavy KV cache mechanism.</p><p>MCSD <ref type="bibr" target="#b178">[179]</ref> features the new block called Multi-Channel Slope and Decay, which is made up of two sections: The slope section can capture local features across short temporal spans, and the decay section can capture global features across long temporal spans. The sections are fused through element-wise operations. During inference, the process would be reformat into a recurrent representation, allowing both spatial and temporal efficiency, minimizing the need for maintaining a large KV cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Hybrid Architecture</head><p>With these non-transformer architecture, some methods construct mixed models to alleviate KV cache necessities while keeping some peculiarities and merits of the selfattention mechanism.</p><p>MixCon <ref type="bibr" target="#b172">[173]</ref> introduces a new architecture called Conba. Inspired by control theory, the Conba layer incorperates a feedback and adaptive control mechanism that can adapt to different sequence-modeling tasks and requirements dynamically with good computational efficiency. Furthermore, MixCon integrates the Mixture of Experts (MoE) module, which dynamically selects the most relevant experts to process parts of the sequence. Combining the transformer layer, the Conba layer, and the MoE module, MixCon constructs a hybrid model with good balance between attention effectiveness and computational efficiency and significantly reduces the total size of the KV cache.</p><p>GoldFinch <ref type="bibr" target="#b173">[174]</ref> first introduces several new architectures, including the GOLD layer, which combines the Llama and RWKV channel mixer with several improvements, and the enhanced Finch model (RWKV-6) that has significantly reduced parameters without sacrificing efficiency and performance. GoldFinch also proposes a novel mechanism called TokenCat to produce a highly compressed global key cache using the output of Finch layers. GoldFinch builds a hybrid architecture that constructs the key cache in the early layers and consumes the key cache to produce output without the traditional value cache in the top layers, providing a compact and reusable cache pipeline with linear scaling.</p><p>RecurFormer <ref type="bibr" target="#b174">[175]</ref> argues that not all transformer heads need to participate in the self-attention mechanism. The work recognizes that certain attention heads show recencyaware behavior which focus on local and short-range dependencies, dissipate the computation resource but gives little contribution. After identifying these heads, RecurFormer replaces them with the Mamba components, achieving straightforward KV cache reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Summary and Future Directions</head><p>By exploring non-transformer modules such as recurrent and hybrid designs, these methods have introduced novel paradigms that balance performance with computational efficiency, and also alleviate the KV cache issues in traditional transformer architectures. Future research should focus on several key areas. First, improving the scalability of recurrent architectures, such as RWKV <ref type="bibr" target="#b175">[176]</ref> and Mamba <ref type="bibr" target="#b176">[177]</ref>, remains critical. Although these methods reduce memory and computational costs, their performance in capturing ultralong-range dependencies lags behind transformers. Second, hybrid designs such as MixCon <ref type="bibr" target="#b172">[173]</ref> and GoldFinch <ref type="bibr" target="#b173">[174]</ref> highlight the potential of integrating diverse modules, yet their complexity introduces challenges in training stability and interpretability. Third, the overall generalization capabilities and robustness of non-transformer architectures, while efficient, need require further exploration for diverse input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SYSTEM-LEVEL OPTIMIZATION</head><p>Recent system-level optimizations for KV cache in LLM inference can be broadly categorized into three main directions: memory management (Sec. 6.1), scheduling strategies (Sec. 6.2), and hardware-aware designs (Sec. 6.3). These complementary approaches collectively demonstrate the rich design space for system-level optimizations in LLM inference, each addressing different aspects of the performance, efficiency, and resource utilization challenges. The Taxonomy of the system-level optimization is in Fig. <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Memory Management</head><p>Recent advances in KV cache memory management for large language model (LLM) inference reveal three distinct approaches aimed at enhancing memory efficiency. Architectural designs, exemplified by vLLM with PagedAttention <ref type="bibr" target="#b143">[144]</ref> and vTensor <ref type="bibr" target="#b217">[218]</ref>, adapt classical operating system principles to create flexible, dynamic memory allocation systems that optimize the use of physical memory through sophisticated mapping and virtual memory abstractions. Prefix-aware designs like ChunkAttention <ref type="bibr" target="#b237">[238]</ref> and MemServe <ref type="bibr" target="#b238">[239]</ref> further refine this approach by organizing data structures to enable efficient cache deduplication and sharing of common prefixes, thereby improving both memory utilization and computational efficiency. Together, these innovations illustrate the potential for significant enhancements in LLM serving via memory management. FlexGen <ref type="bibr" target="#b95">[96]</ref>, InstInfer <ref type="bibr" target="#b214">[215]</ref> Heterogeneous Design (Sec. 6.3.3) NEO <ref type="bibr" target="#b215">[216]</ref>, FastDecode <ref type="bibr" target="#b216">[217]</ref>, FlexInfer <ref type="bibr" target="#b217">[218]</ref>, InfiniGen <ref type="bibr" target="#b138">[139]</ref>, Pensieve <ref type="bibr" target="#b218">[219]</ref>, FastServe <ref type="bibr" target="#b219">[220]</ref>, PartKVRec <ref type="bibr" target="#b220">[221]</ref> I/O-based Design (Sec. 6.3.2) FlashAttention <ref type="bibr" target="#b144">[145]</ref>, Bifurcated Attention <ref type="bibr" target="#b221">[222]</ref>, PartKVRec <ref type="bibr" target="#b220">[221]</ref>, HCache <ref type="bibr" target="#b222">[223]</ref>, Cake <ref type="bibr" target="#b223">[224]</ref>, FastSwitch <ref type="bibr" target="#b224">[225]</ref> Single/Multi-GPU Design (Sec. 6.3.1) HydraGen <ref type="bibr" target="#b225">[226]</ref>, DeFT <ref type="bibr" target="#b226">[227]</ref>, vLLM <ref type="bibr" target="#b143">[144]</ref>, ORCA <ref type="bibr" target="#b227">[228]</ref>, Dist-Serve <ref type="bibr" target="#b228">[229]</ref>, Multi-Bin Batching <ref type="bibr" target="#b229">[230]</ref>, Tree Attention <ref type="bibr" target="#b230">[231]</ref> Scheduling (Sec. 6.2) Layer-specific and Hierarchical Scheduling (Sec. 6.2.3) LayerKV <ref type="bibr" target="#b231">[232]</ref>, CachedAttention <ref type="bibr" target="#b232">[233]</ref>, ALISA <ref type="bibr" target="#b233">[234]</ref>, LAMPS <ref type="bibr" target="#b234">[235]</ref> Preemptive and Fairness-oriented Scheduling (Sec. 6.2.2) FastServe <ref type="bibr" target="#b219">[220]</ref>, FastSwitch <ref type="bibr" target="#b224">[225]</ref> Prefix-aware Scheduling (Sec. 6.2.1) BatchLLM <ref type="bibr" target="#b235">[236]</ref>, RadixAttention <ref type="bibr" target="#b236">[237]</ref> Memory Management (Sec. 6.1) Prefix-aware Design (Sec. 6.1.2) ChunkAttention <ref type="bibr" target="#b237">[238]</ref>, Mem-Serve <ref type="bibr" target="#b238">[239]</ref> Architectural Design (Sec. 6.1.1) vLLM <ref type="bibr" target="#b143">[144]</ref>, vTensor <ref type="bibr" target="#b217">[218]</ref>, LeanKV <ref type="bibr" target="#b111">[112]</ref> Fig. <ref type="figure">6</ref>. Taxonomy of the System-level Optimization for KV Cache Management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Architectural Design</head><p>The first category focuses on architectural innovations in memory management, led by vLLM with PagedAttention <ref type="bibr" target="#b143">[144]</ref>, which adapts OS-inspired paging concepts by partitioning KV caches into fixed-size blocks with noncontiguous storage. PagedAttention partitions KV caches into fixed-size blocks that can be stored non-contiguously in physical memory, while vLLM <ref type="bibr" target="#b143">[144]</ref> implements a virtual memory-like system that manages these blocks through a sophisticated mapping mechanism. This architecture separates logical and physical KV blocks, enabling dynamic memory allocation and flexible block management through block tables that track mapping relationships and fill states. This memory management approach enables efficient memory utilization both within and across requests, demonstrating how classical OS memory management principles can be effectively adapted for LLM inference optimization.</p><p>This approach is further enhanced by vTensor <ref type="bibr" target="#b217">[218]</ref>, which introduces a virtual memory abstraction that decouples computation from defragmentation through three key components: the vTensor Scheduler which generates memory management policies based on meta information, the vTensor Operation which translates these policies into CUDA VMM operations, and the vTensor Pool which maintains virtual tensor mappings. VTS processes instructions and creates policies based on memory state tracking, while VTO executes these policies through asynchronous GPU operations. VTP completes the cycle by managing virtual tensor storage and updating meta information for subsequent memory operations.</p><p>LeanKV <ref type="bibr" target="#b111">[112]</ref> combines unified paging with heterogeneous quantization and dynamic sparsity mechanisms. It implements Hetero-KV quantization to store keys and values at different precisions, complemented by a per-head dynamic sparsity mechanism that adapts memory allocation based on token importance across different attention heads and requests. To efficiently execute these strategies, LeanKV <ref type="bibr" target="#b111">[112]</ref> introduces an advanced on-GPU memory management system featuring three key components: unified paging for flexible memory organization, a circular free page list for efficient coordination, and a bidirectional page table for minimal metadata overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Prefix-aware Design</head><p>Some latest works emphasize optimizing data organization structures through prefix-aware designs. ChunkAttention <ref type="bibr" target="#b237">[238]</ref> restructures KV cache management by organizing chunks within a prefix tree structure, enabling runtime detection and sharing of common prefixes. It breaks down traditional monolithic KV cache tensors into smaller, manageable chunks organized within a prefix tree structure, enabling efficient runtime detection and sharing of common prefixes across multiple requests. This architectural design brings two significant memory management benefits: efficient KV cache deduplication through prefix tree-based organization, and improved data locality through a two-phase partition algorithm for self-attention computation. By enabling dynamic identification and sharing of common prompt prefixes across multiple requests, ChunkAttention <ref type="bibr" target="#b237">[238]</ref> optimizes both memory utilization and computational efficiency, demonstrating how intelligent chunking and prefix-aware cache management can significantly enhance LLM serving efficiency.</p><p>MemServe <ref type="bibr" target="#b238">[239]</ref> extends this concept to distributed settings with its MemPool system, which orchestrates both CPU DRAM and GPU HBM resources across serving instances, managing active and historical KV caches through a comprehensive set of distributed memory pool APIs. It presents a prompt token-based indexing layer for historical KV cache retrieval, cross-instance data exchange mechanisms that abstract away hardware heterogeneity, and a global scheduler implementing a prompt tree-based localityaware policy for enhanced cache reuse, collectively resulting in significant improvements in job completion time and time-to-first-token performance.</p><p>These approaches often complement each other, suggesting potential benefits in combining multiple strategies. For instance, LeanKV <ref type="bibr" target="#b111">[112]</ref>'s integration of compression with page-based management and MemServe <ref type="bibr" target="#b238">[239]</ref>'s combination of distributed memory management with prefix-aware caching demonstrate the effectiveness of hybrid approaches. The diversity of these solutions reflects both the complexity of KV cache management and the rich opportunity space for continued innovation in optimizing LLM inference systems. Tab.10 provides a comparison of various memory management techniques for KV Cache, highlighting key features such as paged memory, virtual memory, dynamic sparsity, prefix sharing, and distributed memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Summary and Future Directions</head><p>The exploration of memory management strategies for KV caches in large language model inference reveals a promising landscape of innovations that enhance memory efficiency and overall system performance. Architectural advancements, such as those seen in vLLM <ref type="bibr" target="#b143">[144]</ref> and LeanKV <ref type="bibr" target="#b111">[112]</ref>, adapt traditional memory management principles for modern AI applications by incorporating paging and virtual memory concepts for dynamic allocation. Prefix-aware designs like ChunkAttention <ref type="bibr" target="#b237">[238]</ref> and MemServe <ref type="bibr" target="#b238">[239]</ref> optimize data organization, enabling the detection and sharing of common prefixes, which reduces redundancy and speeds up inference.</p><p>Future work should advance memory management innovations through multiple synergistic directions: investigating adaptive memory hierarchies that dynamically adjust to workload patterns and resource constraints, exploring novel compression techniques that preserve quick access while reducing memory footprint, developing intelligent prefetching mechanisms that anticipate and preload frequently accessed cache entries, researching hardware-aware optimization strategies that leverage emerging memory technologies like computational storage and processingin-memory units, and designing distributed cache coherence protocols that efficiently maintain consistency across multiple inference nodes. Additionally, the exploration of machine learning-based approaches could enable predictive memory allocation that learns from historical access ✓ LeanKV <ref type="bibr" target="#b111">[112]</ref> ✓ ✓ ChunkAttention <ref type="bibr" target="#b237">[238]</ref> ✓</p><formula xml:id="formula_29">MemServe [239] ✓ ✓</formula><p>patterns, while the investigation of specialized data structures could yield more efficient prefix detection and sharing mechanisms. These advancements, combined with research into heterogeneous memory systems that intelligently coordinate different memory types based on access patterns and performance requirements, would significantly enhance the scalability and efficiency of LLM inference systems across diverse deployment scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Scheduling</head><p>Based on these scheduling-oriented works, we can categorize KV cache scheduling optimizations into three main approaches: 1) prefix-aware scheduling strategies, represented by BatchLLM <ref type="bibr" target="#b235">[236]</ref> and RadixAttention <ref type="bibr" target="#b236">[237]</ref>; 2) preemptive and fairness-oriented scheduling, exemplified by FastServe <ref type="bibr" target="#b219">[220]</ref> and FastSwitch <ref type="bibr" target="#b224">[225]</ref>; 3) layer-specific and hierarchical scheduling approaches, demonstrated by LayerKV <ref type="bibr" target="#b231">[232]</ref>, CachedAttention <ref type="bibr" target="#b232">[233]</ref>, and ALISA <ref type="bibr" target="#b233">[234]</ref>. These approaches collectively address different aspects of scheduling optimization, from memory efficiency to fairness and latency reduction, while specialized solutions like LAMPS <ref type="bibr" target="#b234">[235]</ref> extend these concepts to specific use cases such as API-augmented LLM requests, demonstrating the rich design space in KV cache scheduling optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Prefix-aware Scheduling</head><p>Unlike traditional LRU-based cache management systems where shared KV contexts might be prematurely evicted or unnecessarily extended in memory, BatchLLM <ref type="bibr" target="#b235">[236]</ref> implements explicit global prefix identification and coordinated scheduling of requests sharing common KV cache content.</p><p>It schedules requests at the granularity of prefix-sharing groups, ensuring optimal KV cache reuse while minimizing cache lifetime -requests with identical prefixes are deliberately scheduled together to maximize KV cache sharing efficiency. This scheduling approach is complemented by a dynamic programming algorithm that optimizes first-level prefix patterns, enabling more efficient KV cache management and reducing scheduling overhead. RadixAttention <ref type="bibr" target="#b236">[237]</ref> builds around a radix tree structure, replacing traditional FCFS scheduling with an intelligent cache-aware approach that prioritizes requests based on matched prefix lengths. It implements dynamic memory management where cached tokens and running requests share the same memory pool, controlled by an LRU eviction policy that strategically removes leaf nodes while preserving valuable ancestor prefixes. This is complemented by a</p><p>TABLE 11 Comparison of Scheduling Approaches for KV Cache Optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Prefix-aware Preemptive Fairness-oriented Layer-specific Hierarchical Dynamic</head><p>BatchLLM <ref type="bibr" target="#b235">[236]</ref> ✓ RadixAttention <ref type="bibr" target="#b236">[237]</ref> ✓ ✓ FastServe <ref type="bibr" target="#b219">[220]</ref> ✓ ✓ FastSwitch <ref type="bibr" target="#b224">[225]</ref> ✓ ✓ LayerKV <ref type="bibr" target="#b231">[232]</ref> ✓ CachedAttention <ref type="bibr" target="#b232">[233]</ref> ✓ ✓ ALISA <ref type="bibr" target="#b233">[234]</ref> ✓ ✓ LAMPS <ref type="bibr" target="#b234">[235]</ref> ✓ ✓ reference counting mechanism that prevents eviction of actively used cache entries during continuous batching while enabling efficient memory reclamation when nodes become unused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Preemptive and Fairness-oriented scheduling</head><p>FastServe <ref type="bibr" target="#b219">[220]</ref> implements a proactive KV cache management strategy that coordinates cache movement between GPU and host memory, overlapping data transmission with computation to minimize latency impact. This is integrated with a skip-join Multi-Level Feedback Queue scheduler that makes KV cache scheduling decisions based on input length information, allowing jobs to enter appropriate priority queues directly while avoiding unnecessary demotions through higher-priority queues. By combining token-level preemption with sophisticated KV cache management and intelligent queue placement, FastServe <ref type="bibr" target="#b219">[220]</ref> achieves significant performance improvements over traditional run-tocompletion systems like vLLM <ref type="bibr" target="#b143">[144]</ref>. FastSwitch <ref type="bibr" target="#b224">[225]</ref> introduces a fairness-oriented KV cache scheduling system that addresses the overhead challenges of preemptive scheduling in LLM serving. There are three key mechanisms: enhancing I/O utilization through intelligent cache movement scheduling, minimizing GPU idle time during context switches, and eliminating redundant I/O operations in multi-turn conversations. Unlike traditional block-based KV cache memory policies that prioritize memory efficiency at the cost of fragmentation and granularity limitations, FastSwitch <ref type="bibr" target="#b224">[225]</ref> implements a balanced approach that maintains efficient memory usage while facilitating smoother context switching. This integrated scheduling approach enables dynamic priority adjustments for fairness while minimizing the performance impact of context switches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Layer-specific and Hierarchical Scheduling</head><p>LayerKV <ref type="bibr" target="#b231">[232]</ref> introduces a novel layer-wise KV cache scheduling approach to address the growing TTFT (Time to First Token) latency challenges in large-context LLM serving. The contribution lies in its fine-grained, layer-specific KV cache block allocation and management strategy, which departs from traditional monolithic cache management approaches. By implementing layer-wise KV block scheduling and offloading mechanisms, LayerKV <ref type="bibr" target="#b231">[232]</ref> enables more efficient memory utilization and reduces queuing delays that typically occur when large context windows compete for limited GPU KV cache blocks. It is complemented by an SLO-aware scheduler that optimizes cache allocation decisions based on service level objectives, allowing for dynamic management of memory resources across model layers.</p><p>CachedAttention <ref type="bibr" target="#b232">[233]</ref> introduces a hierarchical scheduling approach consisting of three-tier strategies: layer-wise pre-loading coordinates KV cache movement across storage hierarchies using scheduler-aware fetching and eviction policies, asynchronous saving overlaps I/O operations with GPU computation, and intelligent cache placement decisions are made based on scheduler hints to ensure frequently accessed KV caches reside in faster memory tiers. It also presents a novel positional encoding decoupling mechanism that prevents KV cache invalidation during context window overflow through effective truncation strategies.</p><p>ALISA <ref type="bibr" target="#b233">[234]</ref> introduces a dual-level KV cache scheduling framework that combines algorithmic sparsity with system-level optimization. At the algorithm level, the Sparse Window Attention mechanism identifies and prioritizes the most important tokens for attention computation, creating a mixture of global dynamic and local static sparse patterns that significantly reduces KV cache memory requirements. At the system-level, its three-phase token-level dynamic scheduler that manages KV tensor allocation and optimizes the trade-off between caching and recomputation. The scheduler makes dynamic decisions about which tokens to cache in GPU memory versus recompute, based on their importance and system resource constraints.</p><p>LAMPS <ref type="bibr" target="#b234">[235]</ref> implements a predictive scheduling mechanism that estimates both pre-API outputs and optimal memory handling strategies during API calls, choosing between preserving, discarding, or swapping KV cache content based on predicted memory waste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Summary and Future Directions</head><p>Tab.11 compares scheduling approaches for KV cache optimization based on their support for prefix-awareness, preemptive scheduling, fairness, layer-specific optimizations, hierarchical structures, and dynamic adaptability. The advancements in scheduling strategies for KV cache management in large language model inference highlight a multifaceted approach to optimizing performance, memory efficiency, and fairness. By categorizing these strategies into prefix-aware, preemptive and fairness-oriented, and layer-specific scheduling, we see diverse methodologies addressing different challenges. For instance, prefix-aware strategies like BatchLLM <ref type="bibr" target="#b235">[236]</ref> and RadixAttention <ref type="bibr" target="#b236">[237]</ref> enhance cache reuse by intelligently grouping requests based on shared prefixes, minimizing cache lifetime and reducing overhead. Meanwhile, preemptive approaches such as FastServe <ref type="bibr" target="#b219">[220]</ref> and FastSwitch <ref type="bibr" target="#b224">[225]</ref> implement proactive management techniques that optimize cache movement and scheduling, significantly improving latency and ensuring fairness during context switching. Layer-specific scheduling methods like LayerKV <ref type="bibr" target="#b231">[232]</ref>, CachedAttention <ref type="bibr" target="#b232">[233]</ref>, and ALISA <ref type="bibr" target="#b233">[234]</ref> further refine cache allocation by implementing fine-grained management strategies tailored to the unique demands of different model layers.</p><p>Future work should advance these KV cache scheduling innovations through several interlinked dimensions: developing adaptive hybrid systems that dynamically select optimal scheduling strategies based on real-time workload characteristics, exploring predictive models that anticipate user request patterns to proactively optimize cache allocation, investigating automated parameter tuning mechanisms that adjust scheduling policies across different deployment scenarios, designing context-aware architectures that intelligently balance prefix sharing with fairness requirements, and researching novel cache coherence protocols that efficiently handle distributed inference scenarios. Additionally, the integration of reinforcement learning approaches could enable self-optimizing schedulers that learn from historical usage patterns, while the exploration of hardware-software co-design could yield specialized accelerators that directly support efficient KV cache management operations. These advancements would collectively enhance the robustness, efficiency, and adaptability of LLM inference systems across diverse operational conditions and deployment scales. Finally, considering LLM serving <ref type="bibr" target="#b239">[240]</ref>, different scheduling and sharing for multiple users and queries may lead to potential privacy leaks. Therefore, privacy protection techniques for LLM serving in multi-user scenarios, such as differential privacy <ref type="bibr" target="#b240">[241]</ref>, <ref type="bibr" target="#b241">[242]</ref>, <ref type="bibr" target="#b242">[243]</ref>, are worth further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hardware-aware Design</head><p>Recent hardware-aware optimizations for KV cache management span several key directions based on different hardware architectures and constraints. Single/Multi-GPU designs focus on optimizing memory access patterns, GPU kernel designs for efficient attention computation, and parallel processing with load balancing. IO-based designs optimize data movement across memory hierarchies through asynchronous I/O and intelligent prefetching mechanisms. Heterogeneous designs orchestrate computation and memory allocation across CPU-GPU tiers. SSD-based solutions have evolved from basic offloading approaches to more sophisticated designs, with InstInfer leveraging computational storage drives (CSDs) to perform in-storage attention computation, effectively bypassing PCIe bandwidth limitations. These approaches demonstrate how hardware-aware designs can significantly improve LLM inference efficiency by carefully considering and exploiting the characteristics of different hardware components and their interconnections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Single/Multi-GPU Design</head><p>Based on these works focusing on GPU-oriented designs, we can categorize the approaches into several key strategies for KV cache optimization. First, shared prefix optimization approaches like HydraGen <ref type="bibr" target="#b225">[226]</ref> and DeFT <ref type="bibr" target="#b226">[227]</ref> focus on efficient GPU memory utilization through batched prefix computations and tree-structured attention patterns. Rather than maintaining separate KV caches for each sequence with identical prefixes, HydraGen <ref type="bibr" target="#b225">[226]</ref> decomposes attention computation to leverage a single shared KV cache for common prefixes across multiple requests. It enables efficient GPU memory utilization through two mechanisms: batched prefix KV cache access across sequences and separate handling of unique suffix KV caches. For DeFT <ref type="bibr" target="#b226">[227]</ref>, its core contributions are twofold: KV-Guided Grouping, which optimizes GPU memory access patterns by intelligently managing shared prefix KV caches to minimize redundant global-to-shared memory transfers, and Flattened Tree KV Splitting, which ensures balanced workload distribution across GPU compute units while minimizing computational redundancy.</p><p>Second, distributed processing frameworks exemplified by vLLM <ref type="bibr" target="#b143">[144]</ref> and ORCA <ref type="bibr" target="#b227">[228]</ref> optimize multi-GPU scenarios through sophisticated memory management and synchronization mechanisms. vLLM <ref type="bibr" target="#b143">[144]</ref> also implements a KV cache manager that coordinates memory allocation across distributed GPU workers in model-parallel deployments, where each GPU handles a subset of attention heads while sharing the same logical-to-physical block mapping. This GPU-aware design enables efficient memory utilization through near-zero fragmentation and flexible KV cache sharing, while supporting Megatron-LM style tensor parallelism where GPUs execute in SPMD fashion with synchronized block-wise matrix operations. The scheduler broadcasts control messages containing input tokens and block tables to GPU workers, allowing them to independently process their assigned attention heads while maintaining memory coherence through all-reduce operations, effectively eliminating redundant memory management synchronization overhead and maximizing GPU utilization across distributed resources.</p><p>ORCA <ref type="bibr" target="#b227">[228]</ref> distributes model layers across GPUs using both intra-layer and inter-layer parallelism, where each worker process manages multiple GPU-controlling threads and coordinates KV cache access through an Attention KV manager. ORCA's GPU-aware design minimizes CPU-GPU synchronization overhead by separating control message communication from tensor data transfer (via NCCL), allowing each GPU thread to efficiently access KV cache memory using request IDs and token indices.</p><p>Third, phase-aware designs like DistServe <ref type="bibr" target="#b228">[229]</ref> separate prefill and decoding phases across GPU resources to optimize their distinct memory access patterns. Novel batching strategies are represented by Multi-Bin Batching <ref type="bibr" target="#b229">[230]</ref>, which focuses on length-aware request grouping for improved GPU utilization, while advanced parallel computation frameworks like Tree Attention <ref type="bibr" target="#b230">[231]</ref> introduce sophisticated reduction algorithms for efficient attention computation across multiple GPUs. DistServe <ref type="bibr" target="#b228">[229]</ref> recognizes that prefill and decoding phases have distinct KV cache</p><p>TABLE 12 Comparison of Hardware-aware Design Approaches for KV Cache Optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Single/Multi-GPU I/O-aware Heterogeneous SSD-based</head><p>Bifurcated Attention <ref type="bibr" target="#b221">[222]</ref> ✓ Cake <ref type="bibr" target="#b223">[224]</ref> ✓ DeFT <ref type="bibr" target="#b226">[227]</ref> ✓ DistServe <ref type="bibr" target="#b228">[229]</ref> ✓ FastDecode <ref type="bibr" target="#b216">[217]</ref> ✓ FastSwitch <ref type="bibr" target="#b224">[225]</ref> ✓ FlexGen <ref type="bibr" target="#b95">[96]</ref> ✓ FlexInfer <ref type="bibr" target="#b217">[218]</ref> ✓ FlashAttention <ref type="bibr" target="#b144">[145]</ref> ✓ ✓ HCache <ref type="bibr" target="#b222">[223]</ref> ✓ HydraGen <ref type="bibr" target="#b225">[226]</ref> ✓ InfiniGen <ref type="bibr" target="#b138">[139]</ref> ✓ InstInfer <ref type="bibr" target="#b214">[215]</ref> Multi-Bin Batching <ref type="bibr" target="#b229">[230]</ref> ✓ NEO <ref type="bibr" target="#b215">[216]</ref> ✓ ORCA <ref type="bibr" target="#b227">[228]</ref> ✓ PartKVRec <ref type="bibr" target="#b220">[221]</ref> ✓ Pensieve <ref type="bibr" target="#b218">[219]</ref> ✓ Tree Attention <ref type="bibr" target="#b230">[231]</ref> ✓ vLLM <ref type="bibr" target="#b143">[144]</ref> ✓ utilization characteristics and memory access patterns: prefill requires intensive computation with growing KV cache sizes for processing input tokens, while decoding maintains a fixed KV cache size for generating output tokens. By physically separating these phases onto different GPUs, DistServe enables optimized GPU memory management and KV cache access patterns specific to each phase, eliminating interference between prefill's bursty memory access patterns and decoding's steady-state KV cache utilization. Multi-Bin Batching <ref type="bibr" target="#b229">[230]</ref> introduces a length-aware batching strategy helps minimize GPU idle time and memory fragmentation that typically when processing requests of varying lengths in the same batch, as it ensures that the KV cache memory allocated for each batch is utilized more uniformly across all requests. Tree Attention <ref type="bibr" target="#b230">[231]</ref> implements a tree-based reduction algorithm that fundamentally changes how attention values are computed and aggregated across GPUs, enabling more efficient handling of KV cache data through partial reductions that significantly reduce memory bandwidth requirements and peak memory usage. These approaches can collectively demonstrate how hardware-aware designs can significantly improve the LLM efficiency by carefully considering GPU architecture characteristics and memory hierarchy constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">I/O-based Design</head><p>Recent I/O-focused optimizations for KV cache management span several key dimensions, targeting different levels of the memory hierarchy. At the GPU level, approaches like FlashAttention <ref type="bibr" target="#b144">[145]</ref> and Bifurcated Attention <ref type="bibr" target="#b221">[222]</ref> optimize data movement between HBM and SRAM through sophisticated tiling strategies and split attention computations, while CPU-GPU data movement optimizations are addressed by systems like PartKVRec <ref type="bibr" target="#b220">[221]</ref>, which tackles PCIe bandwidth bottlenecks through hybrid recomputation and transfer strategies, and HCache <ref type="bibr" target="#b222">[223]</ref>, which optimizes intermediate activation storage and restoration.</p><p>FlashAttention <ref type="bibr" target="#b144">[145]</ref> employs a tiling strategy that carefully manages KV cache access patterns, reducing redundant memory operations by keeping frequently accessed portions of the KV cache in fast SRAM while systematically fetching and evicting data blocks to minimize HBM accesses. Bifurcated Attention <ref type="bibr" target="#b221">[222]</ref> presents an I/O-aware approach to optimize KV cache access patterns during sharedcontext batch decoding by strategically splitting attention computations into two distinct GEMM operations. It specifically targets the memory bandwidth bottleneck in highbatch scenarios with long contexts by minimizing repeated KV cache accesses, maintaining the same computational FLOPs while drastically reducing memory I/O operations. For PartKVRec <ref type="bibr" target="#b220">[221]</ref>, its key innovation lies in its hybrid strategy of partial KV cache recomputation on the GPU while simultaneously transferring the remaining cache data from CPU memory, effectively hiding PCIe transfer latency. The implementation employs a sophisticated I/O-aware scheduling system that analyzes input characteristics and hardware capabilities to determine the optimal balance between recomputation and data transfer, dynamically managing KV cache movement to maximize PCIe bandwidth utilization while minimizing GPU idle time. HCache <ref type="bibr" target="#b222">[223]</ref> strategically stores and restores intermediate activations instead of complete KV cache states, implementing a bubblefree restoration scheduler that carefully balances computation and I/O operations to maximize bandwidth utilization. A key innovation is its chunk-based storage manager that addresses the I/O pattern mismatch between saving (layerbefore-token) and restoration (token-before-layer) operations, optimizing data layout and access patterns to reduce I/O overhead. Cake <ref type="bibr" target="#b223">[224]</ref> addresses the fundamental I/O bottleneck in loading cached KV states from disk to GPU memory. It introduces a bidirectional parallelized strategy that simultaneously leverages both computational and I/O resources. This hybrid approach dynamically balances between loading cached KV states from storage and computing them on GPUs, adapting automatically to varying system conditions without manual parameter tuning.</p><p>Context management optimizations are exemplified by FastSwitch <ref type="bibr" target="#b224">[225]</ref>, which implements efficient context switching mechanisms for multi-user scenarios through granular memory management policies. FastSwitch <ref type="bibr" target="#b224">[225]</ref> addresses I/O inefficiencies in traditional block-based KV cache approaches by implementing a more granular and continuous memory management policy that minimizes I/O overhead during preemption and context switching.</p><p>These approaches demonstrate how careful consideration of I/O patterns and memory hierarchy characteristics can significantly improve LLM inference efficiency by minimizing movement and maximizing bandwidth utilization across different storage tiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Heterogeneous Design</head><p>Recent heterogeneous computing approaches for KV Cache demonstrate diverse strategies for optimizing CPU-GPU collaboration. Systems like NEO <ref type="bibr" target="#b215">[216]</ref> and FastDecode <ref type="bibr" target="#b216">[217]</ref> implement strategic workload distribution through CPU offloading of attention computations, while FlexInfer <ref type="bibr" target="#b217">[218]</ref> introduces virtual memory abstractions for optimal resource coordination.</p><p>NEO <ref type="bibr" target="#b215">[216]</ref> advances heterogeneous computing for LLM inference by implementing strategic CPU offloading of attention computations and KV cache states. Through asymmetric GPU-CPU pipelining and load-aware scheduling, it optimally balances workloads across both computing platforms, enabling larger GPU batch sizes without latency penalties. For FastDecode <ref type="bibr" target="#b216">[217]</ref>, its key contribution lies in its strategic offloading of memory-bound KV cache operations to distributed CPU resources, leveraging the aggregate memory capacity and computing power of multiple CPU nodes rather than treating CPUs as mere storage devices. By utilizing CPUs for KV cache computations and storage while keeping compute-intensive operations on GPUs, it creates an efficient pipeline that maximizes resource utilization across the heterogeneous infrastructure, enabling larger batch sizes and higher throughput. FlexInfer <ref type="bibr" target="#b217">[218]</ref> orchestrates CPU-GPU resource utilization for LLM inference by introducing the virtual memory-based abstraction vTensor.</p><p>Advanced caching and prefetching mechanisms are exemplified by InfiniGen <ref type="bibr" target="#b138">[139]</ref>, which employs speculative prefetching for KV cache entries, and Pensieve <ref type="bibr" target="#b218">[219]</ref>, which implements multi-tier caching for conversation states. For InfiniGen <ref type="bibr" target="#b138">[139]</ref>, its key innovation lies in its prediction mechanism that operates across the heterogeneous architecture, using partial computation of attention inputs and modified query-key weights to identify and prefetch only the most relevant KV cache entries from CPU memory to GPU. Pensieve <ref type="bibr" target="#b218">[219]</ref> introduces a heterogeneous computing architecture specifically designed for multi-turn conversation LLM serving by implementing a sophisticated multi-tier caching strategy across GPU and CPU resources. This stateful approach manages KV cache data across the heterogeneous memory hierarchy, maintaining conversation history states across multiple hardware tiers rather than recomputing them for each interaction.</p><p>Sophisticated scheduling and preemption strategies are demonstrated by FastServe <ref type="bibr" target="#b219">[220]</ref>, which focuses on tokenlevel preemption and proactive memory management, and PartKVRec <ref type="bibr" target="#b220">[221]</ref>, which balances data transfer and recomputation through dynamic scheduling. For FastServe <ref type="bibr" target="#b219">[220]</ref>, its token-level preemption capability is supported by a sophisticated heterogeneous memory management system that proactively coordinates KV cache data movement between GPU and host memory. It implements a skip-join Multi-Level Feedback Queue scheduler that manages computational resources across the CPU-GPU boundary, optimizing both computation scheduling and data movement. PartKVRec <ref type="bibr" target="#b220">[221]</ref> employs a scheduler that dynamically optimizes the distribution of tasks across the heterogeneous hardware platform, using a profiler to analyze both hardware capabilities and workload characteristics.</p><p>These approaches collectively showcase how heterogeneous architectures can be effectively leveraged to overcome single-device limitations while maintaining efficient resource utilization and minimizing communication overhead between CPU and GPU resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Solid-state Disk (SSD)-based Design</head><p>Recent SSD-based approaches for KV cache management demonstrate an evolution in storage utilization strategies, from traditional extension of the memory hierarchy to computational storage innovations. FlexGen <ref type="bibr" target="#b95">[96]</ref> introduces an SSD-based approach to KV cache management that extends the memory hierarchy across GPU, CPU memory, and disk storage, optimizing high-throughput LLM inference on resource-constrained hardware through intelligent tensor storage and access pattern optimization determined by linear programming. The system's key innovations include coordinated data placement across all three storage tiers, optimized access patterns to minimize SSD latency impact, aggressive 4-bit compression for both model weights and attention cache, and efficient utilization of SSD storage as a memory hierarchy extension for KV cache management. InstInfer <ref type="bibr" target="#b214">[215]</ref> introduces a more revolutionary approach by leveraging computational storage drives (CSDs) to perform attention computations directly within the storage layer, transforming SSDs from passive storage devices into active computational units and utilizing the high internal bandwidth of flash memory channels to bypass traditional PCIe bandwidth limitations.</p><p>These approaches demonstrate how storage devices can be effectively integrated into LLM inference systems, either as memory hierarchy extensions or as computational resources, to enable efficient processing of large models and long sequences in resource-constrained environments. Tab.12 compares hardware-aware design approaches for KV cache optimization across four key features: Single/Multi-GPU support, I/O-awareness, heterogeneous computing, and SSD-based design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Summary and Future Directions</head><p>Recent advancements in hardware-aware designs for KV cache management emphasize optimizing performance based on specific hardware architectures and constraints, demonstrating significant enhancements in large language model inference efficiency. Approaches like HydraGen <ref type="bibr" target="#b225">[226]</ref> and vLLM <ref type="bibr" target="#b143">[144]</ref> in single and multi-GPU designs focus on efficient memory access patterns and load balancing, while I/O-based strategies such as FlashAttention <ref type="bibr" target="#b144">[145]</ref> and PartKVRec <ref type="bibr" target="#b220">[221]</ref> tackle data movement bottlenecks through intelligent prefetching and scheduling mechanisms. Additionally, heterogeneous designs exemplified by NEO <ref type="bibr" target="#b215">[216]</ref> and FastDecode <ref type="bibr" target="#b216">[217]</ref> effectively leverage CPU-GPU collaboration to maximize resource utilization.</p><p>Future work should advance this research through multiple interconnected directions: exploring novel architectural designs that combine specialized hardware accelerators with optimized memory hierarchies, investigating hybrid systems that leverage computational storage drives and processing-in-memory capabilities, developing selfadaptive algorithms that dynamically optimize resource allocation based on workload patterns, researching advanced compression techniques that maintain model fidelity while reducing memory requirements, and designing intelligent scheduling mechanisms that efficiently coordinate heterogeneous computing resources including CPUs, GPUs, and custom accelerators. These improvements, working in concert, would enhance both the performance and scalability of LLM inference systems across diverse deployment scenarios, from edge devices to data centers, while maintaining adaptability to emerging hardware innovations and varying computational demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">TEXT AND MULTI-MODAL DATASETS</head><p>In this section, we introduce the text and multi-modal datasets used to evaluate LLM efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Text Dataset</head><p>We collect a lot of long-context datasets from state-of-theart benchmark frameworks and various papers, including L-Eval <ref type="bibr" target="#b243">[244]</ref>, M4LE <ref type="bibr" target="#b244">[245]</ref>, BAMBOO <ref type="bibr" target="#b245">[246]</ref>, LongBench <ref type="bibr" target="#b246">[247]</ref>, LRA <ref type="bibr" target="#b247">[248]</ref>, SCROLLS <ref type="bibr" target="#b248">[249]</ref>, ZEROSCROLLS <ref type="bibr" target="#b249">[250]</ref>, LooGLE <ref type="bibr" target="#b250">[251]</ref>, LongEval <ref type="bibr" target="#b251">[252]</ref>, and StreamingEval <ref type="bibr" target="#b135">[136]</ref>. Specifically, we categorize these datasets into different tasks, including question answering, text summarization, text reasoning, text retrieval, and text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Question Answering (QA) Task</head><p>Dataset for this task usually consist of question-answer pairs, and documents that contains the answer to the question. For a model to run such task, documents and questions are usually used as the model input, while the output can differ greatly. Some datasets' answers are closed-ended, meaning that the model should only output its answer in designated form, typically multiple choice answers, while the open-ended answers take a more free form. According to the number of documents involved in a question-answer pair, we can categorize QA task datasets into single-doc QA(QA-SG) and multiple-doc QA(QA-MT). The detailed statistics of the datasets for question answering are provided in Table <ref type="table" target="#tab_13">13</ref>.</p><p>• Qasper <ref type="bibr" target="#b259">[260]</ref> consists of 5049 questions based on 1585 papers on NLP. Question is from NLP practitioners that only have read the abstract and title of a paper, then another set of practitioners answer these questions by reading through the whole paper. The supporting evidences is provided correspondingly. Each instance of the dataset consists of a question, an answer, corresponding paper and supporting evidence. Instances built by Long-Bench <ref type="bibr" target="#b246">[247]</ref> doesn't require evidence.</p><p>• HotpotQA <ref type="bibr" target="#b260">[261]</ref> is a typical for a multi-doc QA dataset.</p><p>It's built based on Wikipedia, and each instance consists of multiple documents, a question, an answer and supporting facts. Supporting facts is a set of paragraph indexes, annotated manually.</p><p>• AltQA <ref type="bibr" target="#b252">[253]</ref> is based on google's NQ <ref type="bibr" target="#b257">[258]</ref> dataset. The answer are all numerical. The original document is "altered" so that each occurrences of the numerical answer is different from the original document, so as to avoid data contamination from pretraining. This dataset is also used in BAMBOO <ref type="bibr" target="#b245">[246]</ref> benchmark. • PaperQA and MeetingQA from BAMBOO <ref type="bibr" target="#b245">[246]</ref> benchmark are question answering tasks in the form of multiple-choice. Each instance of the two datasets consists of question , evidence, answer and corresponding content.</p><p>• NarrativeQA <ref type="bibr" target="#b258">[259]</ref> uses complex narratives that are selfcontained as input documents. Both books and movie scripts are used. For question construction, annotators are only given a story summary, and are asked to write questions based on it. For each story(1572 stories in total), about 30 question-answer pairs are constructed from each summary-story pair. Notably, because of the consistency in story context, the task can be simplified to selecting a correct answer from all answers that relates to the story.</p><p>• MultifieldQA <ref type="bibr" target="#b246">[247]</ref> is an original dataset from Longbench. Its contents covers scientific papers, legal documents, government reports and google results. The dataset has both Chinese and English version, and each instance consists of context built on documents, and a question-answer pair.</p><p>• 2WikiMultihopQA <ref type="bibr" target="#b262">[263]</ref> is a multi-document QA dataset built on Wikipedia and Wikidata. WikiData is a Knowledge Graph database, from which the author was able to extract the (subject entity, property, object entity) triple that corresponds to a Wikipidia document. These triples are used as evidences in each QA pair, as a way for model to show its inference process. The dataset consists of 192,606 questions in total.</p><p>• Musique <ref type="bibr" target="#b263">[264]</ref> is also a multi-document dataset(or multihop dataset, as the paper refers to). Its data is extracted from existing single-hop QA datasets. These single-hop QAs are then composed into multi-hop QA pairs. In addition, Musique add some unanswerable QA pairs in order to further test model's ability. There are 24,814 answerable questions in Musique, and each answerable question corresponds to an unanswerable question.</p><p>• DuReader <ref type="bibr" target="#b264">[265]</ref> is a multi-document QA dataset, whose data is based on Baidu search results. It consists of 200,000 questions, 1,000,000 documents and 420,000 an- swers. Each instance contains a question, multiple possible answers(also possible to be empty), and multiple documents.</p><p>• TriviaQA <ref type="bibr" target="#b253">[254]</ref> is a multi-document reading comprehension QA dataset. All QA pairs are from 14 trivia websites, written by trivia enthusiasts. For each QA pair, 6 supporting documents(evidence) are provided, collected from Bing search API as well as Wikipedia. The total number of QA pairs is 95,956, with a total of 662,659 supporting documents, the average length of each document is 2895 words.</p><p>• TOEFL(L-Eval) <ref type="bibr" target="#b243">[244]</ref> collect lectures from the TOEFL Practice Online as context . Each instance consists of a long input of lectures, multiple instructions(questions) and corresponding answers. • Coursera(L-Eval) <ref type="bibr" target="#b243">[244]</ref> is a dataset built on Coursera website. Similar to TOFEL, Each instance consists of a long input of lectures, multiple instructions and corresponding answers.</p><p>• SFiction(L-Eval) <ref type="bibr" target="#b243">[244]</ref> is based on scientific fictions, in which context real-world principles don't apply. The questions contained in the documents ask the model to answer it based on either contextual information or realworld knowledge, as a way to test model hallucination.</p><p>• LongFQA(L-Eval) <ref type="bibr" target="#b243">[244]</ref> is an open-ended QA dataset on finance based on earnings call transcripts. • CUAD(L-Eval) <ref type="bibr" target="#b243">[244]</ref> is drawn from the CUAD <ref type="bibr" target="#b256">[257]</ref> dataset, which use legal contract as its context.</p><p>• QuALITY <ref type="bibr" target="#b261">[262]</ref> is a multiple-choice single-document QA dataset. It uses science fictions, magazine articles and nonfiction articles as input documents. The question is written by those that have read the full document. Each instance contains a document, a multiple-choice questions and corresponding answers. Notably, part of the questions are unanswerable.</p><p>• NewsQA <ref type="bibr" target="#b244">[245]</ref> and DuoRC <ref type="bibr" target="#b244">[245]</ref> are English QA datasets, constructed from news and movie plots, respectively. • C3 <ref type="bibr" target="#b244">[245]</ref> is a multiple-choice QA dataset, based on second- language Chinese exams.</p><p>• NQ <ref type="bibr" target="#b257">[258]</ref> is a QA dataset based on Wikipedia pages. Each instance(or example, as referred to in original paper) consists of a question, corresponding wikipedia page, a long answer and a short answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Text Summarization Task</head><p>A summarization dataset is a curated collection of texts and their corresponding summaries. They typically include diverse content, such as news articles, scientific papers, or conversational data, paired with concise and accurate summaries. The detailed statistics of the datasets for text summarization are listed in Table <ref type="table" target="#tab_14">14</ref>.</p><p>• CNN/Dailymail <ref type="bibr" target="#b265">[266]</ref>, GovReport <ref type="bibr" target="#b269">[270]</ref>, and XSum <ref type="bibr" target="#b266">[267]</ref> include a document and its corresponding summary in each instance. CNN/Dailymail is based on over 300,000 news articles, GovReport is based on 14,466 long government reports, and XSum is based on BBC news. • MultiNews <ref type="bibr" target="#b268">[269]</ref> is a multi-doc summary dataset, each instance consists of multiple news and a summary.</p><p>• Loogle <ref type="bibr" target="#b250">[251]</ref> is based on papers, WikiPedia, movie and TV scripts. Each long input text corresponds to muti-ple question-answer-summary triad. In total there are 776 documents and 6,448 questions. Average document length is 19.367 words.</p><p>• VCSUM <ref type="bibr" target="#b270">[271]</ref> is based on real-world Chinese meeting transcripts. Each meeting tarnscript corresponds to a headline, segmentation summaries and an overall summary. There're 239 meetings in total.</p><p>• SummScreenFD <ref type="bibr" target="#b271">[272]</ref> is based on TV transcripts.</p><p>Each instance consists of a TV transcript containing conversations, scenes and actor actions, and a summary(recapitulation, as referred to in original paper).</p><p>• BigPatent <ref type="bibr" target="#b272">[273]</ref> is based on 1,341,362 patent documents.</p><p>The highlight of this dataset is that important information is distributed evenly in patent documents, compared to other types of documents. Each instance contains a document and its corresponding summary(human written abstract).</p><p>• SPACE <ref type="bibr" target="#b273">[274]</ref> is based on reviews of 50 hotels. The highlight of the dataset is that the summaries are written in 6 different aspects, based on the hotel's review. Each hotel constructs an instance, containing the hotel's name, multiple reviews, summaries of different aspects and an overall summary.</p><p>• SQuality <ref type="bibr" target="#b274">[275]</ref> is based on the same stories domain as QuALITY <ref type="bibr" target="#b261">[262]</ref> dataset. It's a query-based summarization dataset. Each instance contains a story, multiple summarization questions, and multiple summarizations that corresponds to each questions. There are 625 QA pairs in total.</p><p>• CNNNews(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on CNN English news.</p><p>Each instance of the dataset is paired with a multisentence summary.</p><p>• CEPSUM(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on product information from Chinese e-commerce platform. Each instance contains a product description and corresponding summary. • LCSTS(M4LE) <ref type="bibr" target="#b244">[245]</ref> is a summarization dataset in Chinese. It consists of over 2 million posts from a Chinese micro-blogging website, each post is paired with a summary. M4LE selects instances whose article has over 30 words.</p><p>• NCLS(M4LE) <ref type="bibr" target="#b244">[245]</ref> is a summarization dataset with articles and corresponding summaries in different language, which highlights model's cross-lingual ability. Original NCLS is constructed from CNNNews and LCSTS. • WikiHow(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on procedural descriptions on Wikipedia. Each article is entitled with a beginning of "How to...". Each paragraph of the article describes one step in the procedure, and corresponds to short summary. These summaries are then put together as the suymmary of the article. • News2016(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on ove 2 million news articles in Chinese. For each article, its title is used as golden summary. M4LE remove instances whose length is less than 200 words or over 800 words. • PubMed(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on medical papers. In M4LE, each paper's abstract is used as the summary of the paper. • BookSum(M4LE) <ref type="bibr" target="#b244">[245]</ref> is a dataset containing 405 English books, whose contents covers plays, novels and short stories. Each chapter of the content corresponds to a human-written summary. • CNewsum(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on 304,307 news articles in Chinese. Each article corresponds to a human-written summary.</p><p>• CLTS+(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on CLTS <ref type="bibr" target="#b279">[280]</ref>. CLTS contains over 180,000 Chinese articles, and CLTS+ uses back translation to make summaries more abstractive. M4LE selects part of these instances for benchmark. • Arxiv(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on papers collected from arXiv.org. For each paper, its abstract is used as golden summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Text Reasoning Task</head><p>A reasoning task involves the ability of a model to draw logical conclusions, make inferences, or solve problems based on given information. It requires understanding relationships, patterns, or rules within the data to arrive at accurate and coherent outcomes.Natural Language Inference(NLI) can be considered a subset of reasoning. It highlights model's ability to perform logical inference instructed by natural language.In an NLI task, the typical goal is to determine the relationship between two pieces of text: a premise and a hypothesis. The detailed statistics of the datasets for text reasoning are listed in Tab. 15.</p><p>• Long Listops <ref type="bibr" target="#b247">[248]</ref> is a mathematical reasoning dataset. It inputs an listop expression, instructing the model to perform calculation and output the exact numeric answer. A listop expression has a hierarchical structure that involves a set of simple mathematical operators. The final answer is a number in 0-9, described in original paper as "a tenway classification task".</p><p>• GSM <ref type="bibr" target="#b277">[278]</ref> is a mathematcal reasoning dataset, which describes mathematical problems in natural language and ask the model to solve it.</p><p>• ContractNLI [277] uses contracts as context, and provides hypothesis, answer, and added evidence to each instance as well. The task requires model to judge the relationship between the hypothesis and context. Each instance contains 607 contracts, each contract has 17 annotated hypothesis and corresponding answers. • LSHT(LongBench) [247] is a Chinese classification dataset. It's based on Xinhua News. The model is asked to classify the input news articles into different categories. • SenHallu [246] and AbsHallu [246]use content and a related hypothesis as model's input, and instruct the model to determine whether the hypothesis is true based on the content. The false hypothesis(hallucination, as referred to by original paper) is generated by GPT. • MNDS News [279] is a classification dataset consisting of 10.917 news articles. The news articles have 17 first level categories and 109 second-level categories. 7.1.4 Text Retrieval Task A retrieval task in LLM benchmarks evaluates a model's ability to retrieve relevant information from a large collection of data based on a given query. It tests the model's</p><p>TABLE 16</p><p>Text Dataset-Retrieval. In the Avg. Len: average length, W: words. Particularly, LongEval, StreamingEval and TopicRet is more of a data generation method, which makes their length and instance number flexible, denoted by '-'. In the Metric column, Acc: Accuracy. F1 <ref type="bibr" target="#b280">[281]</ref> calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words.</p><p>Task Name Source Instances Avg Len Metric Lang. CLS/RET TREC(LongBench) [247] Web Question 200 5177 W Acc EN RET LongEval [252] Conversations --Acc EN RET StreamingEval [136] LongChat [252] --Acc EN RET TopicRet(L-Eval) [244] LongChat [252] --Acc EN RET DRCD(M4LE) [245] Wiki -3617 W Acc ZH CLS+RET MARC [245] E-Commerce 2200 3543 W F1 EN,ZH CLS+RET Online Shopping(M4LE) [245] E-Commerce 2200 3714 W F1 ZH CLS+RET MNDS News(M4LE) [245] MNDS News [279] -3805 W Acc EN CLS+RET THUCNews(M4LE) [245] News -3721 W Acc ZH</p><p>understanding of the query, semantic matching, and efficiency in identifying the most relevant documents or pieces of information. The detailed statistics of the datasets for text retrieval are listed in Table <ref type="table" target="#tab_0">16</ref>.</p><p>• LongChat <ref type="bibr" target="#b251">[252]</ref> has two subtask dataset for retrieval.</p><p>Coarse-grained Topic Retrieval dataset use a long document that talk about a number of different topics, and instrutct the model to retrieve the first topic of the document. Fine-grained Line retrieval, on the other hand, is more challenging, which present the model with multiple lines that contain a diffrernt number and label, with similar line patterns. The model is asked to retrieve the number of a specific labeled line.Notably, such dataset can be easily constructed or generated, so it's easy to create an ultra long dataset of this type. Because the dataset is easily constructed by definition, the length of the dataset and the number of instances is indefinite.</p><p>• StreamingEval <ref type="bibr" target="#b135">[136]</ref> construct a line retrieval task based on LongChat, which makes a query in every 10 lines, with its answer about 20 lines above, so as to evaluate the streaming conversation scenario.</p><p>• TopicRet <ref type="bibr" target="#b243">[244]</ref> on the other hand, is based on the coarsegrained topic retrieval task, but ask about the second or third topic instead of the first one, so as to make the task more challenging. • DRCD(M4LE) <ref type="bibr" target="#b244">[245]</ref> is a reading comprehension dataset.</p><p>In M4LE, DRCD is constructed into two subset, one(DRCD explicit) require model to return the articles' IDs related to a given topic, and another subset(DRCD semantic) requires the model to answer specific questions given multiple paragraphs.</p><p>• MARC <ref type="bibr" target="#b244">[245]</ref> consists of bilingual(namely English and Chinese) reviews. The model is asked to identify all positive reviews and retrieve them. • Online Shopping(M4LE) <ref type="bibr" target="#b244">[245]</ref> is based on 60K product reviews on Chinese e-commerce platforms. Reviews are categorized into positive and negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.5">Text Generation Task</head><p>Generation tasks require model to generate contents based on the given instructions and context. The detailed statistics of the datasets for text generation are listed in Table <ref type="table" target="#tab_19">17</ref>.</p><p>• MultiDoc2Dial <ref type="bibr" target="#b282">[283]</ref> gives model a dialogue history and all involved documents, and instruct model to generate the next turn of the dialogue. • OpenReview(L-Eval) <ref type="bibr" target="#b243">[244]</ref>, which is based on ASAP-Review <ref type="bibr" target="#b284">[285]</ref>, provides LLM with a paper and instruct it to generate a review. • ShowsPred and MeetingPred <ref type="bibr" target="#b245">[246]</ref> use dialogue history as input, and ask model to infer which role said the last turn of the conversation. Apart from natural language context, code generation is also an important implementation for LLMs.</p><p>• LCC <ref type="bibr" target="#b281">[282]</ref> gives model long code snippets as context, and instruct model to generate the following line of code. • RepoBench-P <ref type="bibr" target="#b285">[286]</ref> requires model to retrieve toe most relevant code snippets from a long input, and then generate code according to the instruction.</p><p>• PrivateEval <ref type="bibr" target="#b245">[246]</ref> use API documents and a code snippet as input, and instruct the model to generate code acccordingly. Notably, to avoid data contamination caused by pre-training, the keywords in API documents are modified, making the document "private".</p><p>• CodeU <ref type="bibr" target="#b245">[246]</ref> use the same practice of modifying keyword, only that it uses modified source code of public library, rather than API document, as an input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.6">Aggregation Task</head><p>Aggregation task involves understanding and aggregating information from the whole input to answer complex instructions, such as calculating the percentage of positive comments given a set of comments of different attitudes.</p><p>The detailed statistics of the datasets for text aggregation are listed in Table <ref type="table" target="#tab_20">18</ref>.</p><p>• SpaceDigest <ref type="bibr" target="#b249">[250]</ref> give the model a set of hotel reviews, and ask the model to output the percentage of positive reviews in the context. • BookSumSort <ref type="bibr" target="#b249">[250]</ref>, ReportSumSort <ref type="bibr" target="#b245">[246]</ref>, and Shows-Sort <ref type="bibr" target="#b245">[246]</ref> use shuffled paragraphs from book summaries, TV transcripts or government reports as context, and ask the model to sort them in the correct order.</p><p>• PassageCount <ref type="bibr" target="#b246">[247]</ref> selects multiple passage, duplicates some of the paragraphs, and put all those paragraphs into an instance after shuffling. The model is then asked to determine how many documents are used to construct this instance. • PassageRetrieval <ref type="bibr" target="#b246">[247]</ref>, on the other hand, selects 30 wikipedia passages, and use GPT-3.5-Turbo to write a summary for one of them. Then these passages and the generated summary are used as the model input. The model is then instructed to tell which passage was the summary generated from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.7">Evaluation Metric for Text Datasets</head><p>General evaluation metrics used by text datasets mentioned above include Exact Match <ref type="bibr" target="#b286">[287]</ref>, Partial Match, Accuracy, Recall, Precision, F1, BLEU <ref type="bibr" target="#b287">[288]</ref>, SacreBLEU <ref type="bibr" target="#b288">[289]</ref>, Rouge <ref type="bibr" target="#b289">[290]</ref>, METEOR <ref type="bibr" target="#b290">[291]</ref>, BERT <ref type="bibr" target="#b291">[292]</ref>, Edit Similarity, Pass@k <ref type="bibr" target="#b292">[293]</ref> , Exponential Similarity, Concordance Index, Mean Reciprocal Rank. In addition to general evaluation metrics, some more specific metrics are used in particular benchmarks. For datasets from L-Eval <ref type="bibr" target="#b243">[244]</ref>, the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. ∆L is the length difference between answer length and ground truth. For LooGLE <ref type="bibr" target="#b250">[251]</ref>, it utilizes GPT-4 for its QA and summarization task, using it for answer's semantic judgment.</p><p>• Exact Match (EM) <ref type="bibr" target="#b286">[287]</ref> is a metric used to evaluate the accuracy of models in tasks like question answering or text generation. It measures the percentage of predictions that exactly match the ground truth answer, considering both the content and format. • Partial Match (PM) metric evaluates the similarity between a model's output and the reference by allowing partial credit for partially correct answers. Unlike strict metrics like Exact Match (EM), PM accounts for overlaps or shared elements, such as keywords or phrases, making it more flexible in assessing performance. • Accuracy is a metric used to evaluate the overall performance of a model by measuring the proportion of correctly predicted instances (both positive and negative) out of the total instances. • Recall is a metric used to evaluate a model's ability to retrieve all relevant instances in a dataset. It is calculated as the ratio of correctly retrieved relevant items to the total number of relevant items, emphasizing completeness. • Precision is a metric used to evaluate the accuracy of a model by measuring the proportion of correctly predicted positive instances out of all predicted positive instances. • F1 is a performance measure that combines Precision and Recall into a single score using their harmonic mean. It provides a balanced evaluation, especially useful in datasets with imbalanced classes, by considering both false positives and false negatives.</p><p>• BLEU <ref type="bibr" target="#b287">[288]</ref>, is a widely used metric for evaluating the quality of machine-generated text, especially in machine translation. It works by comparing n-grams in the generated output with reference texts to measure overlap, while applying penalties for overly short outputs to ensure fluency.</p><p>• SacreBLEU <ref type="bibr" target="#b288">[289]</ref> is a standardized version of the BLEU metric used to evaluate machine translation quality. It simplifies BLEU's implementation by fixing preprocessing steps like reference handling to ensure consistent and reproducible results across different systems.</p><p>• Rouge <ref type="bibr" target="#b289">[290]</ref> and its variants measure model's performance by calculating overlap between model output and reference answer with unigram(Rouge-1), bigram(Rouge-2), LCS(Rouge-L), etc. Gold Rouge-1 in VCSUM dataset refers to using high-quality reference summaries (gold standards) for evaluation, ensuring reliable and meaningful comparisons.</p><p>• METEOR <ref type="bibr" target="#b290">[291]</ref> (Metric for Evaluation of Translation with Explicit ORdering) is a text evaluation metric designed to assess the quality of machine translation.</p><p>• BERT <ref type="bibr" target="#b291">[292]</ref> metric, often referred to as BERTScore, is a text evaluation metric that uses contextual embeddings from the BERT model to compare similarity between generated and reference texts. • Edit Similarity is a metric that measures the similarity between two text sequences based on the minimum number of edit operations required to transform one sequence into another. It is derived from the concept of edit distance such as Levenshtein distance.</p><p>• Pass@k <ref type="bibr" target="#b292">[293]</ref> evaluates the performance of a model by measuring the percentage that at least one of the top k generated outputs contains a correct solution. In datasets we surveyed, only Pass@1 is used. • Exponential Similarity is a metric that measures the similarity between two items by exponentially weighting their differences, giving more importance to smaller discrepancies.</p><p>• Concordance Index is a metric used to evaluate the predictive accuracy of models, particularly in survival analysis or ranking tasks. • Mean Reciprocal Rank (MRR) is an evaluation metric commonly used in information retrieval and recommendation systems to measure the quality of ranked results. It calculates the reciprocal of the rank of the first relevant item in a result list and averages it across all queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multimodal Datasets and Evaluation Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Multimodal Datasets</head><p>Multimodal datasets have emerged to address the need for a comprehensive understanding of the complex real world by integrating diverse data types such as text, images, audio, and video. These datasets drive advancements in AI, particularly in machine learning and deep learning, by offering rich and diverse data to train more robust and versatile models. We analyze the multimodal benchmarks listed in Table <ref type="table" target="#tab_21">19</ref>, highlighting their distinct focuses. Each benchmark is built upon one or more multimodal datasets, involving their collection, processing, and the use of specific validation metrics. Below, we provide a detailed introduction and description of each multimodal benchmark.</p><p>• LLaVA-Bench <ref type="bibr" target="#b293">[294]</ref> The benchmark is structured around image-ground-truth textual description-question-answer triplets, segmented across COCO and In-The-Wild datasets. It assesses a model's proficiency in multimodal instruction adherence and visual reasoning. By employing a suite of tasks and metrics, it quantifies the model's ability to comprehend and act on visual-language directives, articulate comprehensive descriptions, and engage in intricate reasoning processes.</p><p>• MMBench <ref type="bibr" target="#b294">[295]</ref> This benchmark serves as a bilingual multimodal benchmark, facilitating a comparative analysis of VLM performance across English and Chinese linguistic contexts. It distinctively assesses multimodal models using a hierarchical taxonomy of abilities, stringent quality assurance measures, and a dual-language evaluation framework. Unlike other benchmarks, MM-Bench <ref type="bibr" target="#b294">[295]</ref> incorporates the CircularEval strategy for comprehensive evaluation and utilizes LLMs for precise extraction of choices, setting it apart from its counterparts.</p><p>• MileBench <ref type="bibr" target="#b295">[296]</ref> evaluates the multi-modal long-context capabilities of LLMs, including both diagnostic and realistic evaluation sets. It emphasizes long-context and multi-image tasks. This unique focus allows it to capture the complexity and diversity of real-world multimodal challenges, setting it apart from existing benchmarks. The dataset in MileBench <ref type="bibr" target="#b295">[296]</ref> is characterized by its inclusion of long texts integrated with multiple images, reflecting real-world scenarios where context is key. It contains a diverse range of tasks that require both comprehension and generation.</p><p>• MLVU <ref type="bibr" target="#b296">[297]</ref> is a holistic benchmark, designed to gauge the capabilities of multi-modal LLMs in comprehending video content, transcends the constraints of its predecessors by significantly increasing video durations, encompassing diverse video genres, and crafting a spectrum of assessment tasks. This benchmark offers an extensive array of tasks and video genres to evaluate the comprehensive competencies of MLLMs. It highlights the substantial potential for enhancement in current methodologies and emphasizes the critical factors of context length, image comprehension quality, and the selection of LLM architecture for future progress.</p><p>• LongVideoBench <ref type="bibr" target="#b297">[298]</ref> This benchmark offers an extensive benchmarking framework aimed at assessing the capacity of large multimodal models (LMMs) to comprehend lengthy videos with subtitles, extending up to an hour. It places a strong focus on the retrieval and reasoning capabilities over extended, interwoven video and language data streams, tackling the challenge of singleframe bias and underscoring its proficiency in evaluating multimodal comprehension in long contexts. • Video-MME <ref type="bibr" target="#b298">[299]</ref> A benchmark for comprehensive evaluation, it assesses the proficiency of Multi-modal Large Language Models (MLLMs) in analyzing videos. This dataset comprises a wide array of 900 videos spanning diverse domains and subfields, ensuring extensive scenario coverage. It encompasses videos with lengths ranging from 11 seconds to 1 hour to gauge model flexibility across various time frames. Furthermore, it incorporates various data modalities, including subtitles and audio tracks, to evaluate the comprehensive competencies of MLLMs. The benchmark aims to test the models' capacity for sequential visual data comprehension, with an emphasis on temporal reasoning and the processing of multimodal inputs. • NExT-QA <ref type="bibr" target="#b299">[300]</ref> Advancing video comprehension from mere description to explanation of causal, temporal, and descriptive actions, a video question answering (VideoQA) benchmark has been established. This benchmark boasts a dataset with 5,440 videos and approxi-  <ref type="bibr" target="#b300">[301]</ref> Featuring a substantial dataset, the benchmark comprises 200 multiple-choice questionanswer (QA) pairs for each of the 20 temporal understanding tasks, amassing a total of 4,000 QA pairs. It draws from a variety of videos across 11 public datasets, spanning diverse domains and scenes, thereby testing models' abilities to comprehend temporal sequences. The benchmark automates the generation of multiple-choice QA pairs from existing video annotations, minimizing human involvement and ensuring a fair evaluation process. • MSVD-QA <ref type="bibr" target="#b301">[302]</ref> The MSVD dataset is a collection of 1,970 video clips with descriptive captions, initially for video captioning. It features diverse real-world scenarios and assesses multimodal learning models' capabilities in understanding video content and generating natural language descriptions. • MSRVTT-QA <ref type="bibr" target="#b301">[302]</ref> The MSR-VTT dataset comprises 10,000 video clips with 20 human-transcribed sentences each, focusing on connecting video content with language descriptions. It evaluates multimodal learning models' ability to comprehend video information and translate it into coherent captions, testing their video understanding and language generation skills in a more complex and diverse environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Evaluation Metric for Multimodal Datasets</head><p>The evaluation metrics for multimodal datasets include Relative Score, Accuracy, ROUGE-L, M-Avg, G-Avg, WUPS. Several common metrics, including Accuracy, ROUHE-L, have been introduced in Sec. 7.1.7. Here, we only introduce the special metrics of multimodal datasets, which include Relativa Score, M-Avg, G-Avg, WUPS as follows:</p><p>• Relative Score This metric is used in LLaVA-Bench to evaluate the performance of multimodal models by comparing their outputs to a reference model, typically textbased GPT-4. It is calculated as the percentage ratio of the candidate model's score to the reference model's score, based on dimensions such as helpfulness, relevance, accuracy, and level of detail. • M-Avg Multiple-Choice Average is calculated as the mean accuracy across all multiple-choice tasks in the MLVU benchmark. The accuracy for each task is determined by the proportion of correctly predicted answers compared to the total number of questions within that task. • G-Axg Generation Average s calculated as the mean score across all generation tasks in the MLVU benchmark. Each task is evaluated on multiple dimensions (e.g., Accuracy, Relevance, Completeness, and Reliability) using GPT-4, with scores ranging from 1 to 5. The overall score for each task is the average of these dimensions, and G-Avg is the mean of these task-level scores.</p><p>• WUPS <ref type="bibr" target="#b302">[303]</ref> Wu-Palmer Similarity measures the semantic similarity between two words based on their positions in a taxonomy (e.g., WordNet). It calculates how closely related two words are by considering their least common ancestor (LCS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Advancements in LLMs have driven significant progress on various fields, but their high computational and memory demands during inference pose challenges, especially for long-context and real-time applications. KV cache management offers an effective solution by optimizing memory, reducing redundant computation, and improving performance. This survey reviews KV cache management strategies across token-level, model-level, and system-level optimizations. Token-level optimizations focus on fine-grained control of KV cache through selection, budget allocation, merging, quantization, and low-rank decomposition, enabling efficient resource allocation without altering model architectures. Model-level optimizations leverage architectural innovations, such as attention grouping and nontransformer designs, to enhance the efficiency of KV reuse. System-level optimizations further complement these efforts by employing advanced memory management, scheduling techniques, and hardware-aware designs to optimize resource utilization across diverse computing environments. Despite the progress made, substantial opportunities remain for future exploration. Key areas include the development of real-time, task-specific budget allocation strategies, dynamic workload handling, advanced distributed coordination for KV cache in multi-node systems, and hardwareaware innovations to leverage emerging architectures like computational storage and processing-in-memory. Additionally, integrating reinforcement learning and adaptive algorithms could enable more intelligent and responsive KV cache management, further enhancing LLM efficiency across diverse deployment scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The decoder-only Transformer for LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Three types of quantization. Then matrix X ∈ R T ×C , where T is the number of tokens and C is the feature dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Positional encodingQ i , K i , V iQuery, Key, and Value matricesd k , dvQuery/Key and Value dimensionW Q i , W K i , W V i Weight matrices for computing Q i , K i , V i .</figDesc><table><row><cell></cell><cell>Notation Summary</cell></row><row><cell>Symbol</cell><cell>Definition</cell></row><row><cell>X</cell><cell>Input sequence of tokens</cell></row><row><cell>X</cell><cell>Dense representations of X</cell></row><row><cell>dx</cell><cell>Dimensionality of the input embeddings.</cell></row><row><cell>Z i</cell><cell>Self-attention Output</cell></row><row><cell>W O</cell><cell>Weight matrix</cell></row><row><cell>W 1 , W 2</cell><cell>Weight matrices</cell></row><row><cell>b 1 , b 2</cell><cell>Bias vectors</cell></row><row><cell>t</cell><cell>Sequence length index</cell></row><row><cell>tc</cell><cell>Number of tokens stored in the KV cache.</cell></row><row><cell>K t i , V t i Kt-1 i , Vt-1 i</cell><cell>Key and Value at step t Cached Key and Value</cell></row><row><cell>h</cell><cell>Number of attention heads per layer</cell></row><row><cell>L</cell><cell>Number of transformer layers</cell></row></table><note><p>E</p><p>Embedding matrix E ∈ R dvocab×dx . P E(X)</p><p>P (x t+1 |x 1 , • • • , xt)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>The summary of existing KV Cache merging approaches.</figDesc><table><row><cell>Model</cell><cell>Merge Layer Intra-layer Cross-layer</cell><cell>Merge Unit</cell><cell>Merge Metric</cell><cell>Merge Type</cell><cell>Training-free</cell></row><row><cell>CCM [101]</cell><cell>✓</cell><cell>Token</cell><cell>Sliding Window</cell><cell>Many-to-One</cell><cell>×</cell></row><row><cell>LoMA [102]</cell><cell>✓</cell><cell>Token</cell><cell>Sliding Window</cell><cell>Many-to-Many</cell><cell>×</cell></row><row><cell>DMC [103]</cell><cell>✓</cell><cell>Token</cell><cell>Learned Merge Indictor</cell><cell>Many-to-One</cell><cell>×</cell></row><row><cell>D2O [105]</cell><cell>✓</cell><cell>Token</cell><cell>Cosine Similarity</cell><cell>Two-to-One</cell><cell>✓</cell></row><row><cell>CaM [104]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>The summary of existing mixed-precision quantization models.</figDesc><table><row><cell>Model</cell><cell>Keys</cell><cell>Values</cell><cell cols="3">Important Tokens</cell><cell>Outlier storing</cell><cell>Channel Reorder</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Intial</cell><cell>Middle</cell><cell>Recent</cell></row><row><cell>KVQuant [84]</cell><cell>Channel, Pre-RoPE</cell><cell>Per-Token</cell><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell></row><row><cell>KIVI [87]</cell><cell>Channel</cell><cell>Per-Token</cell><cell></cell><cell></cell><cell>✓</cell></row><row><cell>SKVQ [86]</cell><cell cols="2">Dynamic outlier-aware</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>✓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>The summary of Model-based Intra-layer approaches.</figDesc><table><row><cell>Method</cell><cell cols="2">Alteration Type</cell><cell>KV Cache</cell><cell>Retraining</cell></row><row><cell></cell><cell>Enhanced</cell><cell>Augmented</cell><cell>Management</cell><cell>Requirement</cell></row><row><cell></cell><cell>Attention</cell><cell>Architecture</cell><cell></cell></row><row><cell>MLA [27]</cell><cell>✓</cell><cell></cell><cell>Latent compression</cell><cell>✓</cell></row><row><cell>FLASH [184]</cell><cell>✓</cell><cell></cell><cell>Linear approximation</cell><cell>✓</cell></row><row><cell>Infini-Attention [185]</cell><cell>✓</cell><cell></cell><cell>Compressive cache</cell><cell>✓</cell></row><row><cell>YOCO [180]</cell><cell></cell><cell>✓</cell><cell>Single global KV cache</cell><cell>✓</cell></row><row><cell>CEPE [181]</cell><cell></cell><cell>✓</cell><cell cols="2">Parallel encoding with cross-attn Lightweight</cell></row><row><cell>XC-Cache [182]</cell><cell></cell><cell>✓</cell><cell>Encoder cross-attention</cell><cell>✓</cell></row><row><cell>Block Transformer [183]</cell><cell></cell><cell>✓</cell><cell>Hierarchical local KV</cell><cell>Lightweight</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 10</head><label>10</label><figDesc>Comparison of Memory Management Techniques for KV CacheOptimization.</figDesc><table><row><cell>Method</cell><cell>Paged Memory</cell><cell>Virtual Memory</cell><cell>Dynamic Sparsity</cell><cell>Prefix Sharing</cell><cell>Distributed Memory</cell></row><row><cell>vLLM [144]</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vTensor [218]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 13</head><label>13</label><figDesc>Text question answering (QA) dataset. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Q: questions, Inst: instructions. Particularly, AltQA, PaperQA and MeetingQA have two datasets with different length levels, and is separated with /.Particularly, for datasets from L-Eval, the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. ∆L is the length difference between answer length and ground truth. For NarrativeQA, MRR: Mean Reciprocal Rank .</figDesc><table><row><cell>Task</cell><cell>Name</cell><cell>Source</cell><cell>Instances</cell><cell>Avg Len</cell><cell>Metric</cell><cell>Lang.</cell></row><row><cell>QA</cell><cell>AltQA [253]</cell><cell>Wikipedia</cell><cell>200/200</cell><cell>3243/13,084 Tok</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>QA</cell><cell>PaperQA(BAMBOO) [246]</cell><cell>Paper</cell><cell>100/100</cell><cell>3101/6838 Tok</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>QA</cell><cell>MeetingQA(BAMBOO [246]</cell><cell>Meeting</cell><cell>100/100</cell><cell>2738/9838 Tok</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>QA</cell><cell>TriviaQA [254]</cell><cell cols="2">Web Question, Wiki 95,956 Q, 662,659 Doc</cell><cell>17,370 W</cell><cell>EM, F1</cell><cell>EN</cell></row><row><cell>QA</cell><cell>TOEFL(L-Eval) [244]</cell><cell>TOFEL-QA [255]</cell><cell>15 Doc, 269 Inst</cell><cell>3907 Tok</cell><cell cols="2">Rouge-L, GPT-4, ∆L EN</cell></row><row><cell>QA</cell><cell>Coursera(L-Eval) [244]</cell><cell>Video Subtitles</cell><cell>15 Doc, 172 Inst</cell><cell>9075 Tok</cell><cell cols="2">Rouge-L, GPT-4, ∆L EN</cell></row><row><cell>QA</cell><cell>SFiction(L-Eval) [244]</cell><cell>SFGram [256], fiction</cell><cell>7 Doc, 64 Inst</cell><cell>16,381 Tok</cell><cell cols="2">Rouge-L, GPT-4, ∆L EN</cell></row><row><cell>QA</cell><cell>LongFQA(L-Eval) [244]</cell><cell>Financial Transcripts</cell><cell>6 Doc, 52 Inst</cell><cell>6032 Tok</cell><cell cols="2">Rouge-L, GPT-4, ∆L EN</cell></row><row><cell>QA</cell><cell>CUAD(L-Eval) [244]</cell><cell>CUAD [257]</cell><cell>20 Doc, 130 Inst</cell><cell>30,966 Tok</cell><cell cols="2">Rouge-L, GPT-4, ∆L EN</cell></row><row><cell>QA</cell><cell>DuoRC [245]</cell><cell>Movie</cell><cell></cell><cell>3572 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>QA</cell><cell>NQ [258]</cell><cell>Wiki</cell><cell>307,373</cell><cell>9005 W</cell><cell>Rouge</cell><cell>EN</cell></row><row><cell>QA-SG</cell><cell>NarrativeQA [259]</cell><cell>Story</cell><cell>1572 Doc</cell><cell>62,528 Tok</cell><cell>BLEU, METEOR, Rouge-L, MRR</cell><cell>EN</cell></row><row><cell cols="2">QA-SG NarrativeQA(LongBench) [247]</cell><cell>Story</cell><cell>200</cell><cell>18,409 W</cell><cell>F1</cell><cell>EN</cell></row><row><cell>QA-SG</cell><cell>Qasper [260]</cell><cell>Paper</cell><cell>1585</cell><cell>5001 W</cell><cell>F1</cell><cell>EN</cell></row><row><cell>QA-SG</cell><cell>Qasper(LongBench) [247]</cell><cell>Paper</cell><cell>200</cell><cell>3619 W</cell><cell>F1</cell><cell>EN</cell></row><row><cell>QA-SG</cell><cell>MultifieldQA-en [247]</cell><cell>Paper, Legal, Gov, Google</cell><cell>200</cell><cell>4459 W</cell><cell>F1</cell><cell>EN</cell></row><row><cell>QA-SG</cell><cell>MultifieldQA-zh [261]</cell><cell>Paper, Legal, Gov, Google</cell><cell>200</cell><cell>6701 W</cell><cell>F1</cell><cell>ZH</cell></row><row><cell>QA-SG</cell><cell>QuALITY [262]</cell><cell>Story, magazine</cell><cell>381 Doc, 6737 Q</cell><cell>4203 W</cell><cell>EM</cell><cell>EN</cell></row><row><cell>QA-MT</cell><cell>HotpotQA [261]</cell><cell>Wiki</cell><cell>112,779</cell><cell>1138 W</cell><cell>EM, F1</cell><cell>EN</cell></row><row><cell cols="2">QA-MT HotpotQA(LongBench) [247]</cell><cell>Wiki</cell><cell>200</cell><cell>9151 W</cell><cell>F1</cell><cell>EN</cell></row><row><cell>QA-MT</cell><cell>2WikiMultihopQA [263]</cell><cell>Wiki</cell><cell>192,606 Q</cell><cell>639 W</cell><cell>EM, F1</cell><cell>EN</cell></row><row><cell>QA-MT</cell><cell>MuSiQue [264]</cell><cell>Wiki</cell><cell>24,814</cell><cell>1827 W</cell><cell>F1</cell><cell>EN</cell></row><row><cell>QA-MT</cell><cell>DuReader [265]</cell><cell>Baidu</cell><cell>200,000 Q, 1,000,000 Doc</cell><cell>396 W</cell><cell cols="2">BLEU, Rouge-L ZH,EN</cell></row><row><cell>QA+RET</cell><cell>NewsQA(M4LE) [245]</cell><cell>News</cell><cell>-</cell><cell>3679 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>QA+RET</cell><cell>C3(M4LE) [245]</cell><cell>Textbook</cell><cell>-</cell><cell>3797 W</cell><cell>Acc</cell><cell>ZH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 14 Text</head><label>14</label><figDesc>Dataset-Summarization. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Q: questions, Inst: instructions. Particularly, SPACE has the concept of 'Entity', and R/Ent stands for reviews per entity. Sum stands for summary. In the Metric column, EM: Exact Match. PM: Partial Match. Acc: Accuracy. For MultiNews, Rouge-SU skip bigrams when having a distance larger than 4 words. Particularly, LooGLE utilizes GPT-4 for its QA and summarization task, using it for answer's semantic judgement.</figDesc><table><row><cell>Task</cell><cell>Name</cell><cell>Source</cell><cell>Instances</cell><cell>Avg Len</cell><cell>Metric</cell><cell>Lang.</cell></row><row><cell>SUM</cell><cell>CNN/Dailymail [266]</cell><cell>News</cell><cell>300,000</cell><cell>766 W</cell><cell>Rouge-1/2/L</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>XSum [267]</cell><cell>News</cell><cell>400,000</cell><cell>431 W</cell><cell>Rouge-1/2/L</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>QMSum [268]</cell><cell>Meeting</cell><cell cols="2">232 Meets, 1808 Q 9070 W</cell><cell>Rouge-1/2/L</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>MultiNews [269]</cell><cell>News</cell><cell>51,216</cell><cell>5866 W</cell><cell>Rouge-1/2/SU</cell><cell>EN</cell></row><row><cell>SUM-QB+ Reasoning+ QA</cell><cell>LooGLE [251]</cell><cell>Papers, Wiki, Movie, TV</cell><cell>776 Doc, 6448 Q</cell><cell>19,367 W 24,005 Tok</cell><cell>BLEU, Rouge, METEOR, EM, PM BERT, GPT4,</cell><cell>EN,ZH</cell></row><row><cell>SUM</cell><cell>GovReport [270]</cell><cell>Gov</cell><cell>19,466</cell><cell>9409.4 W</cell><cell>Rouge-1/2/L</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>VCSUM [271]</cell><cell>Meeting</cell><cell>239</cell><cell>14,107 Tok</cell><cell>F1, Gold Rouge-1</cell><cell>ZH</cell></row><row><cell>SUM</cell><cell>SummScreenFD [272]</cell><cell>TV</cell><cell>269,000</cell><cell>6613 Tok</cell><cell>Rouge</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>BigPatent [273]</cell><cell>Patent</cell><cell>1,341,362</cell><cell>3573 W</cell><cell>Rouge-1/2/L</cell><cell>EN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>50 Entities,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SUM</cell><cell>SPACE [274]</cell><cell>Review</cell><cell>1,140,000 Reviews, 100R/Ent,</cell><cell>15,532 W</cell><cell>Rouge-1/2/L</cell><cell>EN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1050 Sum</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rouge-1/2/L,</cell><cell></cell></row><row><cell>SUM</cell><cell>SQuALITY [275]</cell><cell>Story</cell><cell>625</cell><cell>5200 W</cell><cell>METEOR,</cell><cell>EN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BERT</cell><cell></cell></row><row><cell cols="2">SUM+RET CNNNews(M4LE) [245]</cell><cell>News</cell><cell>-</cell><cell>3754 W</cell><cell>Rouge-L</cell><cell>EN</cell></row><row><cell cols="3">SUM+RET CEPSUM(M4LE) [245] E-Commerce</cell><cell>-</cell><cell>4003 W</cell><cell>Rouge-L</cell><cell>ZH</cell></row><row><cell>SUM+RET</cell><cell>LCSTS(M4LE) [245]</cell><cell>News</cell><cell>-</cell><cell>4102 W</cell><cell>Rouge-L</cell><cell>ZH</cell></row><row><cell>SUM+RET</cell><cell>NCLS(M4LE) [245]</cell><cell>NCLS [276]</cell><cell>-</cell><cell>3470 W</cell><cell>Rouge-L</cell><cell>EN,ZH</cell></row><row><cell>SUM+RET</cell><cell>WikiHow [245]</cell><cell>Wiki</cell><cell>-</cell><cell>3514 W</cell><cell>Rouge-L</cell><cell>EN</cell></row><row><cell>SUM+RET</cell><cell>News2016 [245]</cell><cell>News</cell><cell>-</cell><cell>3785 W</cell><cell>Rouge-L</cell><cell>ZH</cell></row><row><cell>SUM</cell><cell>Pubmed(M4LE) [245]</cell><cell>Medical</cell><cell>1267</cell><cell>3678 W</cell><cell>Rouge-L</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>BookSum(M4LE) [245]</cell><cell>Book</cell><cell>-</cell><cell>2643 W</cell><cell>Rouge-L</cell><cell>EN</cell></row><row><cell>SUM</cell><cell>CNewsum(M4LE) [245]</cell><cell>News</cell><cell>690</cell><cell>1883 W</cell><cell>Rouge-L</cell><cell>ZH</cell></row><row><cell>SUM</cell><cell>CLTS+(M4LE) [245]</cell><cell>News</cell><cell>-</cell><cell>3158 W</cell><cell>Rouge-L</cell><cell>ZH</cell></row><row><cell>SUM</cell><cell>Arxiv(M4LE) [245]</cell><cell>Paper</cell><cell>1550</cell><cell>3748 W</cell><cell>Rouge-L</cell><cell>EN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 15 Text</head><label>15</label><figDesc>Reasoning/Classification Datasets. CLS: Classification. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Inst: instructions. In the Metric column, EM: Exact Match. Acc: Accuracy.</figDesc><table><row><cell>Task</cell><cell>Name</cell><cell>Source</cell><cell>Instances</cell><cell>Avg Len</cell><cell>Metric</cell><cell>Lang.</cell></row><row><cell>CLS/Reasoning</cell><cell>Long ListOps [248]</cell><cell>Generated</cell><cell>100,003</cell><cell>3106 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>Reasoning</cell><cell>ContractNLI [277]</cell><cell>Legal</cell><cell>10,319</cell><cell>2254 Tok</cell><cell>EM</cell><cell>EN</cell></row><row><cell>CLS</cell><cell>LSHT(LongBench) [247]</cell><cell>News</cell><cell>200</cell><cell>22,337 W</cell><cell>Acc</cell><cell>ZH</cell></row><row><cell>Reasoning</cell><cell>GSM(16 shot) [244]</cell><cell cols="2">GSM8K [278] 100 Doc, 100 Inst</cell><cell>5557 Tok</cell><cell cols="2">Rouge-L, GPT-4, ∆L EN</cell></row><row><cell>Reasoning</cell><cell>SenHallu(BAMBOO) [246]</cell><cell>Paper</cell><cell>200/200</cell><cell cols="3">3170/6357 Tok Precision, Recall, F1 EN</cell></row><row><cell>Reasoning</cell><cell>AbsHallu(BAMBOO) [246]</cell><cell>Paper</cell><cell>200/200</cell><cell cols="3">3314/6445 Tok Precision, Recall, F1 EN</cell></row><row><cell>CLS</cell><cell>MNDS News [279]</cell><cell>News</cell><cell>10,917</cell><cell>637 W</cell><cell>Acc</cell><cell>EN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 17 Text</head><label>17</label><figDesc>Dataset-Generation.In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Inst: instructions. In the Metric column, EM: Exact Match. Acc: Accuracy.</figDesc><table><row><cell>Task</cell><cell>Name</cell><cell>Source</cell><cell>Instances</cell><cell>Avg Len</cell><cell>Metric</cell><cell>Lang.</cell></row><row><cell>GEN</cell><cell>LCC [282]</cell><cell>Code</cell><cell>360000</cell><cell>1337 W</cell><cell>EM, Edit Sim</cell><cell>Python/CSharp/Java</cell></row><row><cell>GEN</cell><cell>RepoBench-P(LongBench) [247]</cell><cell>Code</cell><cell>500</cell><cell>4206 W</cell><cell>Edit Sim</cell><cell>Python/Java</cell></row><row><cell>GEN/RET</cell><cell>MultiDoc2Dial [283]</cell><cell>Doc2Dial [284]</cell><cell>488 Doc, 4796 Dialogues</cell><cell>4283 T</cell><cell>F1, EM, SacreBLEU, Recall</cell><cell>EN</cell></row><row><cell>GEN</cell><cell cols="3">OpenReview(L-Eval) [244] ASAP-Review [285] 20 Doc 60 Inst</cell><cell>11,170 Tok</cell><cell>Rouge-L, GPT-4, ∆L</cell><cell>EN</cell></row><row><cell>GEN</cell><cell>ASAP-Review [285]</cell><cell>Paper</cell><cell>8877 Papers, 25,986 Reviews</cell><cell>6782 W/Paper</cell><cell>Rouge-1/2/L, BERT</cell><cell>EN</cell></row><row><cell>GEN</cell><cell>ShowsPred [246]</cell><cell>TV Shows</cell><cell>100/100</cell><cell>2389/4860 Tok</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>GEN</cell><cell>MeetingPred [246]</cell><cell>Meeting</cell><cell>100/100</cell><cell>3689/11578 Tok</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>GEN-Code</cell><cell>PrivateEval [246]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 18 Text</head><label>18</label><figDesc>Dataset-Aggregation. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Inst: instructions.In the Metric column, Acc: Accuracy. ES: Exponential Similarity, CI: Concordance Index</figDesc><table><row><cell>Task</cell><cell>Name</cell><cell cols="2">Source Instances</cell><cell>Avg Len</cell><cell cols="2">Metric Lang.</cell></row><row><cell>AGG</cell><cell>SpaceDigest [250]</cell><cell>Reviews</cell><cell>500</cell><cell>5481 W</cell><cell>ES</cell><cell>EN</cell></row><row><cell>AGG</cell><cell>BookSumSort [250]</cell><cell>Literature</cell><cell>500</cell><cell>6840 W</cell><cell>CI</cell><cell>EN</cell></row><row><cell>AGG</cell><cell>PassageRetrieval-en [247]</cell><cell>Wiki</cell><cell>200</cell><cell>9289 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>AGG</cell><cell>PassageRetrieval-zh [247]</cell><cell>C4 Dataset</cell><cell>200</cell><cell>6745 W</cell><cell cols="2">Acc ZH</cell></row><row><cell>AGG</cell><cell>PassageCount [247]</cell><cell>Wiki</cell><cell>200</cell><cell>11,141 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell cols="6">AGG ShowsReport(BAMBOO) [246] TV Shows 200/200 2992/6411 Tok CI</cell><cell>EN</cell></row><row><cell cols="6">AGG ReportSumSort(BAMBOO) [246] Reports 150/150 3753/8309 Tok CI</cell><cell>EN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 19 Multimodal</head><label>19</label><figDesc>Dataset. Specfically, for data type, Img: Image; T: text; V: Video. For task abbreviation, Conv: conversation task; Desc: description task; Reas: reasoning task; Perc: perception task; Pred: prediction; NTH: needle in the haystack; SUMM: summary. For instance and average column, Q: questions; W: words; s: seconds. For example, 54 Img, 150 Q denote that there are 54 images with 150 questions.annotated question-answer pairs, sorted into causal, temporal, and descriptive categories. It poses a challenge to QA models to engage in reasoning about causal and temporal actions and to decipher complex object interactions within daily activities. Distinguished from other video benchmarks, this benchmark specifically focuses on causal and temporal action reasoning within realistic videos that are rich in object interactions. It stands as one of the largest manually annotated VideoQA datasets, offering support for both multiple-choice and open-ended questions, and includes a variety of videos that mirror real-life scenarios.</figDesc><table><row><cell>Tasks</cell><cell>Name</cell><cell>Data</cell><cell>Source</cell><cell>Instance</cell><cell>Average</cell><cell>Metric</cell><cell>Language</cell></row><row><cell>Conv, Desc, Reas</cell><cell cols="4">LLaVA-Bench [294] Img, T COCO, In-The-Wild 54 Img, 150 Q</cell><cell>1 Img, 59.9 W</cell><cell>Relative Score</cell><cell>EN</cell></row><row><cell>Perc, Reas</cell><cell>MMBench [295]</cell><cell>Img, T</cell><cell>Internet</cell><cell>2948 Q</cell><cell>1 Img, 114.5 W</cell><cell>Acc</cell><cell>EN/CN</cell></row><row><cell>Pred, Count, NIH, Retrieval</cell><cell>MileBench [296]</cell><cell>Img, T</cell><cell>Public, self-building</cell><cell>6440 Q</cell><cell cols="2">15.2 Img, 422.3 W Acc, ROUGE-L</cell><cell>EN</cell></row><row><cell>Reas, NIH, SUMM, Desc, Order, Count</cell><cell>MLVU [297]</cell><cell>V, T</cell><cell>Public, self-collection</cell><cell>1334 V, 2593 Q</cell><cell cols="2">704.6s V, 39.7 W M-Avg, G-Avg</cell><cell>EN</cell></row><row><cell>Reas, Retrieval</cell><cell cols="2">LongVideoBench [298] V, T</cell><cell>web-collected</cell><cell>3763 V, 6678 Q</cell><cell>730.5s V, 49.5 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>Perc, Recognition, Reas</cell><cell>Video-MME [299]</cell><cell>V, T</cell><cell>YouTube</cell><cell>900 V, 2700 Q</cell><cell>1017.9s V</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>Desc, Reas</cell><cell>NExT-QA [300]</cell><cell>V, T</cell><cell>YouTube, TV Show, Public</cell><cell>1000 V, 47962 Q</cell><cell>44s V, 25.5 W</cell><cell>Acc, WUPS</cell><cell>EN</cell></row><row><cell>Perc, Count, Reas</cell><cell>MVBench [301]</cell><cell>V, T</cell><cell>Public</cell><cell>4000 Q</cell><cell>16.7s V, 31.3 W</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>Decs</cell><cell>MSVD-QA [302]</cell><cell>V, T</cell><cell>MSVD</cell><cell>1970 V, 50505 Q</cell><cell>10s V</cell><cell>Acc</cell><cell>EN</cell></row><row><cell>Desc</cell><cell>MSRVYY-QA [302]</cell><cell>V, T</cell><cell>MSRVTT</cell><cell>10000 V, 243690 Q</cell><cell>15s V</cell><cell>Acc</cell><cell>EN</cell></row><row><cell cols="2">mately 52K manually</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>• MVBench</p></note></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code 152/152 3149/6230 Tok Pass@1 EN, Python GEN-Code CodeU(L-Eval) [244] Code 90 Doc 10 Inst 31,575 Tok Rouge-L, GPT-4, ∆L Python</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on large language models: Applications, challenges, limitations, and practical usage</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Authorea Preprints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey on model compression for large language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07633</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive overview of large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saqib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.06435</idno>
		<idno>abs/2307.06435</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.06435" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent advances in natural language processing via large pre-trained language models: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P B</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1145/3605943</idno>
		<ptr target="https://doi.org/10.1145/3605943" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large language models for generative information extraction: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">186357</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paperfiles/paper/2023/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>dcf277ea32ce3288914faf369fe6de0-Abstract</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-language models for vision tasks: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2024.3369699</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2024.3369699" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5625" to="5644" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards language models that can see: Computer vision through the LENS of natural language</title>
		<author>
			<persName><forename type="first">W</forename><surname>Berrios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.16410</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2306.16410" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2023">2306.16410, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mmllms: Recent advances in multimodal large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-acl.738</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.findings-acl.738" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">430</biblScope>
			<biblScope unit="page" from="12" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on multimodal large language models for autonomous driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACVW60836.2024.00106</idno>
		<ptr target="https://doi.org/10.1109/WACVW60836.2024.00106" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision Workshops, WACVW 2024 -Workshops</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">January 1-6, 2024. 2024</date>
			<biblScope unit="page" from="958" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal large language models: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/BigData59044.2023.10386743</idno>
		<ptr target="https://doi.org/10.1109/BigData59044.2023.10386743" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data, BigData 2023</title>
		<editor>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Slezak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gruca</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">December 15-18, 2023. 2023</date>
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large models for time series and spatio-temporal data: A survey and outlook</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10196</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on time-series pre-trained models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">User modeling in the era of large language models: Current research and future directions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11518</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on large language models for recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Llm4drive: A survey of large language models for autonomous driving</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2024 Workshop on Open-World Agents</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Driving with llms: Fusing object-level vector modality for explainable autonomous driving</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sinavski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ünermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karnsund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Drive like a human: Rethinking autonomous driving with large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="910" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large ai models in health informatics: Applications, challenges, and the future</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-W</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05112</idno>
		<title level="m">A survey of large language models in medicine: Progress, application, and challenge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.13971</idno>
		<idno>abs/2302.13971</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.13971" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Let-Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.70</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.acl-long.70" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1280" to="1297" />
		</imprint>
	</monogr>
	<note>ACL 2024</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dengr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2405.04434" />
		<imprint/>
	</monogr>
	<note>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model,&quot; Jun. 2024. [Online</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepseek-vl: Towards real-world vision-language understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.05525</idno>
		<idno>abs/2403.05525</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.05525" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixtral of experts</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antoniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Sayed</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.04088</idno>
		<idno>abs/2401.04088</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.04088" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Sayed</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.06825</idno>
		<idno>abs/2310.06825</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.06825" />
		<title level="m">Mistral 7b,&quot; CoRR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GLM-130B: an open bilingual pre-trained model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=-Aw0rrrPUF" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GLM: general language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.26</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.acl-long.26" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Villavicencio</surname></persName>
		</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">May 22-27, 2022. 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on cache management mechanisms for real-time embedded systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gracioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mancuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Fr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pellizzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of web cache replacement strategies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Podlipnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="374" to="398" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cache-based GNN system for dynamic graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482237</idno>
		<ptr target="https://doi.org/10.1145/3459637.3482237" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
		<meeting><address><addrLine>Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">November 1 -5, 2021. 2021</date>
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Orca: Scalable temporal graph neural network training with theoretical guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3588737</idno>
		<ptr target="https://doi.org/10.1145/3588737" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Manag. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pagraph: Scaling gnn training on large graphs via computation-aware caching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Cloud Computing</title>
		<meeting>the 11th ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A survey on efficient training of transformers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01107</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A comprehensive survey of compression algorithms for language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.15347</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Model compression and efficient inference for large language models: A survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09748</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zharkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00678</idno>
		<title level="m">The efficiency spectrum of large language models: An algorithmic survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards efficient generative large language model serving: A survey from algorithms to systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Oliaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.15234</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03863</idno>
		<title level="m">Efficient large language models: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A survey on efficient inference for large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14294</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A survey on transformer compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05964</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A survey on hardware accelerators for large language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kachris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09890</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Parameterefficient fine-tuning methods for pretrained language models: A critical review and assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Z</forename><forename type="middle">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.12148</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A survey on data selection for language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16827</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Awesome-llm-kv-cache: A curated list of awesome llm inference papers with codes</title>
		<author>
			<persName><surname>Zefan-Cai</surname></persName>
		</author>
		<ptr target="https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache" />
	</analytic>
	<monogr>
		<title level="m">2024, open-source software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.07851</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast inference from transformers via speculative decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Leviathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speculative decoding with big little decoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Prompt compression for large language models: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12388</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Keep the cost down: A review on methods to optimize llm&apos;s kv-cache consumption</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.18003</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Scbench: A kv cache-centric analysis of long-context methods</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.10319</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.01527</idno>
		<title level="m">Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Length extrapolation of transformers: A survey from the perspective of position encoding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.17044</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Rethinking positional encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02561</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09398</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2410.14731" />
		<imprint>
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Unlocking data-free low-bit quantization with matrix decomposition for kv cache compression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.12591</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Effectively compress kv heads for llm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07056</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Eigen attention: Attention in low-rank space for kv cache compression</title>
		<author>
			<persName><forename type="first">U</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.05646</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Zero-delay qkv compression for mitigating kv cache and network bottlenecks in llm inference</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04107</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Lorc: Low-rank compression for llms kv cache with a progressive compression strategy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.03111</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Shadowkv: Kv cache in shadows for high-throughput long-context llm inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.21465</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Palu: Compressing kv-cache with low-rank projection</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21118</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Massive activations in large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.17762</idno>
		<idno>abs/2402.17762</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2402.17762" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohtashami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Croci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.00456</idno>
		<title level="m">Quarot: Outlier-free 4-bit inference in rotated llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Qserve: W4A8KV4 quantization and system co-design for efficient LLM serving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.04532</idno>
		<idno>abs/2405.04532</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.04532" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Training transformers with 4-bit integers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paperfiles/paper/2023/hash/99" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>fc8bc48b917c301a80cb74d91c0c06-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Spinquant-llm quantization with learned rotations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16406</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Duquant: Distributing outliers via dual transformation makes stronger quantized llms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Smoothquant: Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/xiao23c.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA, ser</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="87" to="125" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09145</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Affinequant: Affine transformation quantization for large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.12544</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Flatquant: Flatness matters for llm quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.09426</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">AWQ: activation-aware weight quantization for on-device LLM compression and acceleration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Sa</surname></persName>
		</editor>
		<meeting>the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 13-16, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>paper files/paper/2024/hash/ 42a452cbafa9dd64e9ba4aa95cc1ef21-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Omniquant: Omnidirectionally calibrated quantization for large language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.13137</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Kvquant: Towards 10 million context length llm inference with kv cache quantization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mohammadzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.18079</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Intactkv: Improving large language model quantization by keeping pivot tokens intact</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01241</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Skvq: Sliding-window key and value cache quantization for large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.06219</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Kivi: A tuning-free asymmetric 2bit quantization for kv cache</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02750</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Wkvquant: Quantizing weight and key/value cache for large language models gains more</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.12065</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05527</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18096</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Zipvl: Efficient large vision-language models with dynamic token sparsification and kv cache compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.08584</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Zipcache: Accurate and efficient kv cache quantization with salient token identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14256</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Prefixquant: Static quantization beats dynamic through prefixed outliers in llms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.05265</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Minikv: Pushing the limits of llm inference via 2-bit layer-discriminative kv cache</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.18077" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Zeroquant: Efficient and affordable post-training quantization for large-scale transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Flexgen: Highthroughput generative inference of large language models with a single GPU</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/sheng23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA, ser</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="31" to="094" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Qjl: 1-bit quantized jl transform for kv cache quantization with zero overhead</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zandieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03482</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Pqcache: Product quantization-based kvcache for long context llm inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.12820</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Minicache: KV cache compression in depth dimension for large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.14366</idno>
		<idno>abs/2405.14366</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.14366" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Kvsharer: Efficient inference via layer-wise dissimilar KV cache sharing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.18517</idno>
		<idno>abs/2410.18517</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2410.18517" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Compressed context memory for online language model interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=64kSvC4iPg" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">LoMA: Lossless Compressed Memory Attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2401.09486" />
		<imprint>
			<date type="published" when="2024-02">Feb. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nawrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lancucki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ponti</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tDRYrAkOB7" />
	</analytic>
	<monogr>
		<title level="m">Forty-First International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Cam: Cache merging for memory-efficient llms inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=LCTmppB165" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">D2O: dynamic discriminative operations for efficient generative inference of large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.13035</idno>
		<idno>abs/2406.13035</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2406.13035" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Aim: Adaptive inference of multi-modal llms via token merging and pruning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03248</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">LOOK-M: look-once optimization in KV cache for efficient multimodal long-context inference</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.findings-emnlp.235" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2024</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">November 12-16, 2024. 2024</date>
			<biblScope unit="page" from="4065" to="4078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Model tells you where to merge: Adaptive KV cache merging for llms on long-context tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.08454</idno>
		<idno>abs/2407.08454</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.08454" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">CHAI: Clustered Head Attention for Efficient LLM Inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hosmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2403.08058" />
		<imprint>
			<date type="published" when="2024-04">Apr. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Ada-kv: Optimizing KV cache eviction by adaptive budget allocation for efficient LLM inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2407.11550</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Identify critical KV cache in LLM inference from an output perturbation perspective</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=lRTDMGYCpy" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Thirteenth International Conference on Learning Representations, 2024, under review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Unifying kv cache compression for large language models with leankv</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C S</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.03131" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Razorattention: Efficient KV cache compression through retrieval heads</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2407.15891</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Not all heads matter: A head-level KV cache compression method with integrated retrieval and reasoning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/2410.19258</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Duoattention: Efficient long-context LLM inference with retrieval and streaming heads</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/2410.10819</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Pyramidkv: Dynamic KV cache compression based on pyramidal information funneling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/2406.02069</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Pyramidinfer: Pyramid KV cache compression for high-throughput LLM inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3258" to="3270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">DynamicKV: Task-aware adaptive KV cache compression for long context LLMs</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=uHkfU4TaPh" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Thirteenth International Conference on Learning Representations, 2024, under review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Prefixkv: Adaptive prefix kv cache is what vision instruction-following models need for efficient generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.03409" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Simlayerkv: A simple framework for layer-level KV cache reduction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/2410.13846</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Infllm: Training-free long-context extrapolation for llms with an efficient context memory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.04617" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">QUEST: query-aware sparsity for efficient long-context LLM inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Squeezed attention: Accelerating long context length llm inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mohammadzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maheswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.09688" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Retrievalattention: Accelerating long-context LLM inference via vector retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<idno>abs/2409.10516</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Humanlike episodic memory for infinite context llms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fountas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benfeghoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oomerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lampouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bou-Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2407.09450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Clusterkv: Manipulating llm kv cache in semantic space for recallable compression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03213</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">H2O: heavy-hitter oracle for efficient generative inference of large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>paper files/paper/2023/hash/ 6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Buzz: Beehive-structured sparse kv cache with segmented heavy hitters for efficient llm inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.23079" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">NACL: A general and effective KV cache eviction framework for llms at inference time</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/2408.03675</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrillidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. December 10 -16, 2023, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Keyformer: KV cache reduction through key tokens selection for efficient generative inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Adnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soloveychik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024</title>
		<meeting>the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 13-16, 2024. mlsys.org, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Sepllm: Accelerate large language models by compressing one segment into one separator</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.12094</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Model tells you what to discard: Adaptive KV cache compression for llms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Snapkv: Llm knows what you are looking for before generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Venkitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Locatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14469</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Incontext kv-cache eviction for llms via attention-gate</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/2410.12876</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Efficient streaming language models with attention sinks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NG7sS51zVF" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Lm-infinite: Zero-shot extreme length generalization for large language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">June 16-21, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3991" to="4008" />
		</imprint>
	</monogr>
	<note>NAACL 2024</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Sparq attention: Bandwidth-efficient LLM inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ribar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chelombiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hudlass-Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Orr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Infinigen: Efficient generative inference of large language models with dynamic kv cache management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.19707" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Recycled attention: Efficient inference for long-context language models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.05787" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Magicpig: LSH sampling for efficient LLM generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sadhukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2410.16179</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">CAKE: Cascading and adaptive KV cache eviction with layer preferences</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=EQgEMAD4kv" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Thirteenth International Conference on Learning Representations, 2024, under review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Retrieval head mechanistically explains long-context factuality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/2404.15574</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023</title>
		<meeting>the 29th Symposium on Operating Systems Principles, SOSP 2023<address><addrLine>Koblenz, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">October 23-26, 2023. 2023</date>
			<biblScope unit="page" from="611" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paperfiles/paper/2022/hash/67" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Compressive transformers for longrange sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylKikSYDH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Learning to compress prompts with gist tokens</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paperfiles/paper/2023/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>f143aa2154e7f4d5e22d68-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Qaq: Quality adaptive quantization for llm kv cache</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04643</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Cachegen: Kv cache compression and streaming for fast large language model serving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2024 Conference</title>
		<meeting>the ACM SIGCOMM 2024 Conference</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="38" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Atom: Low-bit quantization for efficient and accurate llm serving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="196" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Fixed point quantization of deep convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Annapureddy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Integer quantization for deep learning inference: Principles and empirical evaluation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09602</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Deep learning optimization for edge devices: Analysis of training quantization parameters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kwasniewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ozga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zajac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruminski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IECON 2019-45th Annual Conference of the IEEE Industrial Electronics Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Adaptive quantization for deep neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A linear speedup analysis of distributed deep learning with sparse and quantized communication</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Gptq: Accurate post-training quantization for generative pre-trained transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17323</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Qlora: Efficient finetuning of quantized llms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Quantizable transformers: Removing outliers by helping attention heads do nothing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="75" to="067" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Quantized cnn: A unified approach to accelerate and compress convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4730" to="4743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Dataset quantization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="17" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">8-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30" to="318" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Gpt3. int8 (</note>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Understanding and overcoming the challenges of efficient transformer quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12948</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Outlier suppression: Pushing the limit of low-bit transformer language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17" to="402" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">A survey of product quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE Transactions on Media Technology and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">H2o: Heavy-hitter oracle for efficient generative inference of large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">710</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Asymkv: Enabling 1-bit quantization of kv cache with layer-wise asymmetric quantization configurations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13212</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Tensor factorization via matrix factorization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics. PMLR</title>
		<imprint>
			<biblScope unit="page" from="507" to="516" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Tensor factorization for low-rank tensor completion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1152" to="1163" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Global optimality in tensor factorization, deep learning, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07540</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Enabling lightweight fine-tuning for pre-trained language model compression based on matrix product operators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02205</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Low-rank tucker decomposition of large tensors using tensorsketch</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">MixCon: A Hybrid Architecture for Efficient and Adaptive Sequence Modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA240593</idno>
		<ptr target="https://ebooks.iospress.nl/doi/10.3233/FAIA240593" />
	</analytic>
	<monogr>
		<title level="m">Frontiers in Artificial Intelligence and Applications</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Endriss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Melo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bugarín-Diz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alonso-Moral</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Barro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Heintz</surname></persName>
		</editor>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cheah</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2407.12077" />
		<imprint>
			<date type="published" when="2024-07">Jul. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">RecurFormer: Not All Transformer Heads Need Self-Attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2410.12850" />
		<imprint>
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">RWKV: Reinventing RNNs for the Transformer Era</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arcadinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Gv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Koptyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S I</forename><surname>Mantri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Wind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wozniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-J</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2305.13048" />
		<imprint>
			<date type="published" when="2023-12">Dec. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2312.00752" />
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Retentive Network: A Successor to Transformer for Large Language Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2307.08621" />
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">MCSD: An Efficient Language Model with Diverse Fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2406.12230" />
		<imprint>
			<date type="published" when="2024-07">Jul. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">You Only Cache Once: Decoder-Decoder Architectures for Language Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2405.05254" />
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Long-Context Language Modeling with Parallel Context Encoding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.142</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.acl-long.142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2588" to="2610" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2024</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Noël</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taslakian</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.findings-emnlp.896" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2024</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">November 12-16, 2024. 2024</date>
			<biblScope unit="page" from="15" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Block Transformer: Global-to-Local Language Modeling for Fast Inference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Yun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2406.02657" />
		<imprint>
			<date type="published" when="2024-11">Nov. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Transformer Quality in Linear Time</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/hua22a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9099" to="9117" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research,</note>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Leave No Context Behind: Efficient Infinite Context Transformers with Infiniattention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2404.07143" />
		<imprint>
			<date type="published" when="2024-08">Aug. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<title level="m" type="main">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kelly</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2405.12981" />
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Layer-Condensed KV Cache for Efficient Inference of Large Language Models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.602</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.acl-long.602" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="175" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2024</note>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">Beyond KV Caching: Shared Attention for Efficient LLMs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2407.12866" />
		<imprint>
			<date type="published" when="2024-07">Jul. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M K</forename><surname>Zuhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Adilazuarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Purwarianti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2406.09297" />
		<imprint>
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Cross-layer Attention Sharing for Large Language Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.01890" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2410.14442" />
		<imprint>
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Lossless KV Cache Compression to 2%</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2410.15252" />
		<imprint>
			<date type="published" when="2024-10">Oct. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2406.06567" />
		<imprint>
			<date type="published" when="2024-06">Jun. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">Value Residual Learning For Alleviating Attention Concentration In Transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2410.17897" />
		<imprint>
			<date type="published" when="2024-12">Dec. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Fast Transformer Decoding: One Write-Head is All You Need</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1911.02150" />
		<imprint>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lebr Ón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2305.13245" />
		<imprint>
			<date type="published" when="2023-12">Dec. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Optimised Grouped-Query Attention Mechanism for Transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=13MMghY6Kh" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Efficient Systems for Foundation Models II @ ICML2024</title>
		<imprint>
			<date type="published" when="2024-07">Jul. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Weighted Grouped Query Attention in Transformers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chinnakonduru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohapatra</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2407.10855" />
		<imprint>
			<date type="published" when="2024-07">Jul. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">QCQA: Quality and Capacity-aware grouped Query Attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2406.10247" />
		<imprint>
			<date type="published" when="2024-06">Jun. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khaquan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafveez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Samiwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Raza</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2408.08454" />
		<imprint>
			<date type="published" when="2024-08">Aug. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values</title>
		<author>
			<persName><forename type="first">F</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajimolahoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ataiefard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2311.03426" />
		<imprint>
			<date type="published" when="2023-12">Dec. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>in International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Recent advances in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barfett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01078</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Integrating mamba and transformer for long-short range time series forecasting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14757</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Liquid structural state-space models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chahine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12951</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Simplified state space layers for sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04933</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Pretraining without attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10544</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.18861</idno>
		<title level="m">A survey on vision mamba: Models, applications and challenges</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.01129</idno>
		<title level="m">A survey of mamba</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Agneeswaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16112</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">On the Parameterization and Initialization of Diagonal State Space Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Combining Recurrent, Convolutional, and Continuoustime Models with Linear State Space Layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=yWd42CWN3c" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021-11">Nov. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<monogr>
		<title level="m" type="main">Instinfer: In-storage attention offloading for cost-effective long-context llm inference</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.04992" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title level="m" type="main">Neo: Saving gpu memory crisis with cpu offloading for online llm inference</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.01142" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<monogr>
		<title level="m" type="main">Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.11421" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">vtensor: Flexible virtual tensor management for efficient llm serving</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.15309" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Stateful large language model serving with pensieve</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.05516</idno>
		<idno>abs/2312.05516</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.05516" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Fast distributed inference serving for large language models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.05920" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<title level="m" type="main">Efficient llm inference with i/o-aware partial kv cache recomputation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Zarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.17089" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<title level="m" type="main">Bifurcated attention: Accelerating massively parallel decoding with shared prefixes in llms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Gonugondla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Gouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.08845" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Fast state restoration in LLM serving with hcache</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.05004</idno>
		<idno>abs/2410.05004</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2410.05004" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Compute or load kv cache? why not both?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.03065" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">Fastswitch: Optimizing context switching efficiency in fairness-aware large language model serving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.18424" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<monogr>
		<title level="m" type="main">Hydragen: High-throughput llm inference with shared prefixes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.05099" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<monogr>
		<title level="m" type="main">Deft: Decoding with flash tree-attention for efficient tree-structured llm inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.00242" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Orca: A distributed serving system for transformer-based generative models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi22/presentation/yu" />
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Weatherspoon</surname></persName>
		</editor>
		<meeting><address><addrLine>Carlsbad, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">July 11-13, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="521" to="538" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin" />
	</analytic>
	<monogr>
		<title level="m">18th USENIX Symposium on Operating Systems Design and Implementation</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gavrilovska</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Terry</surname></persName>
		</editor>
		<meeting><address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 10-12, 2024. 2024</date>
			<biblScope unit="volume">2024</biblScope>
			<biblScope unit="page" from="193" to="210" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Multibin batching for increasing LLM inference throughput</title>
		<author>
			<persName><forename type="first">O</forename><surname>Guldogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kunde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pedarsani</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=WVmarX0RNd" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<monogr>
		<title level="m" type="main">Tree attention: Topology-aware decoding for long-context attention on gpu clusters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Millidge</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.04093" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main">Layerkv: Optimizing large language model serving with layer-wise kv cache management</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.00428" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<monogr>
		<title level="m" type="main">Cost-efficient large language model serving for multi-turn conversations with cachedattention</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zuo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.19708" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<title level="m" type="main">Alisa: Accelerating large language model inference via sparsity-aware kv caching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.17312" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<monogr>
		<title level="m" type="main">Fast inference for augmented large language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shahout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.18248" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
		<title level="m" type="main">Batchllm: Optimizing large batched llm inference with global prefix sharing and throughput-oriented token batching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.03594" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<monogr>
		<title level="m" type="main">Sglang: Efficient execution of structured language model programs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.07104" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<monogr>
		<title level="m" type="main">Chunkattention: Efficient selfattention with prefix-aware kv cache and two-phase partition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.15220" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<title level="m" type="main">Memserve: Context caching for disaggregated llm serving with elastic memory pool</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.17565" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<title level="m" type="main">Cacheblend: Fast large language model serving with cached knowledge fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16444</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">A survey on differential privacy for unstructured data content</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Residual sensitivity for differentially private multi-way joins</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Management of Data</title>
		<meeting>the 2021 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="432" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Continual observation under userlevel differential privacy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2190" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<monogr>
		<title level="m" type="main">L-Eval: Instituting Standardized Evaluation for Long Context Language Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.11088" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<monogr>
		<title level="m" type="main">M4le: A multi-ability multi-range multi-task multi-domain long-context evaluation benchmark for large language models</title>
		<author>
			<persName><forename type="first">Wai-Chung</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><surname>Xingshan</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yufei</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yusen</surname></persName>
		</author>
		<author>
			<persName><surname>Liangyou</surname></persName>
		</author>
		<author>
			<persName><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><surname>Lifeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19240</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b245">
	<monogr>
		<title level="m" type="main">Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.13345</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.14508" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<monogr>
		<title level="m" type="main">Long Range Arena: A Benchmark for Efficient Transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.04006" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<title level="m" type="main">SCROLLS: Standardized CompaRison Over Long Language Sequences</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.03533" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.14196" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
		<title level="m" type="main">LooGLE: Can Long-Context Language Models Understand Long Contexts?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2311.04939" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<monogr>
		<title level="m" type="main">How long can open-source llms truly promise on context length?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-06-29-longchat" />
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<title level="m" type="main">Giraffe: Adventures in expanding context lengths in llms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naidu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.10882" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<monogr>
		<title level="m" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.03551" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<monogr>
		<title level="m" type="main">Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06378</idno>
		<ptr target="http://arxiv.org/abs/1608.06378" />
		<imprint>
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">Sfgram: a dataset containing thousands of scienc-fiction books and novels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schaetti</surname></persName>
		</author>
		<ptr target="https://github.com/nschaetti/EchoTorch" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<monogr>
		<title level="m" type="main">CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ball</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.06268" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Natural Questions: A Benchmark for Question Answering Research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<ptr target="https://direct.mit.edu/tacl/article/43518" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koˇcisk Ý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<ptr target="https://TBD" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">TBD</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>TBD</note>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<title level="m" type="main">A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105.03011" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<monogr>
		<title level="m" type="main">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<ptr target="http://arxiv.org/abs/1809.09600" />
		<imprint>
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<monogr>
		<title level="m" type="main">QuALITY: Question Answering with Long Input Texts, Yes!</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.08608" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Duong Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.580" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6609" to="6625" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b263">
	<monogr>
		<title level="m" type="main">MuSiQue: Multihop Questions via Single-hop Question Composition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108.00573" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<monogr>
		<title level="m" type="main">DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.05073" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/K16-1028" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<monogr>
		<title level="m" type="main">Don&apos;t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1808.08745" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<monogr>
		<title level="m" type="main">QMSum: A New Benchmark for Query-based Multidomain Meeting Summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.05938" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<monogr>
		<title level="m" type="main">Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.01749" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<monogr>
		<title level="m" type="main">Efficient Attentions for Long Document Summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02112</idno>
		<ptr target="http://arxiv.org/abs/2104.02112" />
		<imprint>
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<monogr>
		<title level="m" type="main">VCSUM: A Versatile Chinese Meeting Summarization Dataset</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.05280" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<monogr>
		<title level="m" type="main">SummScreen: A Dataset for Abstractive Screenplay Summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.07091" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1212" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Extractive Opinion Summarization in Quantized Transformer Spaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Amplayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00366/98621/Extractive-Opinion-Summarization-in-Quantized</idno>
		<ptr target="https://direct.mit.edu/tacl/article/doi/10.1162/tacla00366/98621/Extractive-Opinion-Summarization-in-Quantized" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="277" to="293" />
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
	<note>Available</note>
</biblStruct>

<biblStruct xml:id="b274">
	<monogr>
		<title level="m" type="main">SQuALITY: Building a long-document summarization dataset the hard way</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>2205.11465</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">NCLS: Neural Cross-Lingual Summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">ContractNLI: A dataset for document-level natural language inference for contracts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koreeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.164" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="1907" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<monogr>
		<title level="m" type="main">Training Verifiers to Solve Math Word Problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petukhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fachada</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/2306-5729/8/5/74" />
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2023-04">Apr. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing: 9th CCF International Conference</title>
		<title level="s">Lecture Notes in Computer Science Ser</title>
		<meeting><address><addrLine>Zhengzhou, China; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing AG</publisher>
			<date type="published" when="2020">October 14-18, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">12430</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part I, ser</note>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</editor>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11">Nov. 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<monogr>
		<title level="m" type="main">LongCoder: A Long-Range Pre-trained Language Model for Code Completion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.14893" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta<address><addrLine>Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6162" to="6176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<monogr>
		<title level="m" type="main">doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Lastras</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.06623" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<monogr>
		<title level="m" type="main">Can We Automate Scientific Reviewing?</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.00176" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<monogr>
		<title level="m" type="main">RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.03091" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1606.05250</idno>
		<ptr target="http://arxiv.org/abs/1606.05250" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</editor>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07">Jul. 2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<ptr target="https://aclanthology.org/W18-6319" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Turchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Verspoor</surname></persName>
		</editor>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-1013" />
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07">Jul. 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W11-2107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Monz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</editor>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07">Jul. 2011</date>
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<monogr>
		<title level="m" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.09675" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.03374" />
		<imprint/>
	</monogr>
	<note>Evaluating large language models trained on code,&quot; 2021. [Online</note>
</biblStruct>

<biblStruct xml:id="b293">
	<monogr>
		<title level="m" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<monogr>
		<title level="m" type="main">Mmbench: Is your multimodal model an all-around player</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Yuan Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06281</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.18532</idno>
		<title level="m">Milebench: Benchmarking mllms in long context</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b296">
	<monogr>
		<title level="m" type="main">Mlvu: A comprehensive benchmark for multi-task long video understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04264</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b297">
	<monogr>
		<title level="m" type="main">Longvideobench: A benchmark for long-context interleaved video-language understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.15754" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<monogr>
		<title level="m" type="main">Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.21075</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Next-qa: Next phase of question-answering to explaining temporal actions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="9777" to="9786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<monogr>
		<title level="m" type="main">Videochat: Chat-centric video understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06355</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<monogr>
		<title level="m" type="main">A new similarity measure for taxonomy based on edge counting</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Acharya</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1211.4709" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
