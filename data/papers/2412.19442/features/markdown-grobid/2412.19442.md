# A Survey on Large Language Model Acceleration based on KV Cache Management

## Abstract

## 

Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, modellevel, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardwareaware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: [https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management](https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management).

## INTRODUCTION

Large Language Models (LLMs) [1], [2], trained on massive corpora, have revolutionized various domains such as natural language processing [3], [4], [5], computer vision [6], [7], [8], and multi-modal [9], [10], [11] tasks. Their ability to understand context and perform logical reasoning has enabled

## 

remarkable success in various fields, such as time series analysis [[12]](#b11), [[13]](#b12), recommendation [[14]](#b13), [[15]](#b14), autonomous driving [[16]](#b15), [[17]](#b16), [[18]](#b17), and healthcare [[19]](#b18), [[20]](#b19). These breakthroughs are powered by state-of-the-art architectures and training paradigms, enabling models to achieve unparalleled performance across diverse tasks. Prominent LLMs, such as GPT [[21]](#b20), [[22]](#b21), [[23]](#b22), LLaMA [[24]](#b23), [[25]](#b24), DeepSeek [[26]](#b25), [[27]](#b26), [[28]](#b27), Mistral [[29]](#b28), [[30]](#b29), and GLM [[31]](#b30), [[32]](#b31), are built on the foundational transformer architecture [[33]](#b32), which excels at capturing long-range dependencies in sequential data. However, despite their powerful capabilities, the computational and memory demands of LLMs, particularly during inference, present significant challenges when scaling them to real-world, long-context, and real-time applications.

A critical bottleneck in LLM inference lies in the efficient management of Key-Value (KV) pairs. Recently, caching techniques [[34]](#b33), [[35]](#b34) have been extensively employed to store previously computed intermediate results, allowing their reuse in subsequent inference steps to accelerate the model, such as graph neural networks [[36]](#b35), [[37]](#b36), [[38]](#b37). Fortunately, the auto-regressive generation mechanism inherent to LLMs presents an opportunity to leverage KV caching for efficient text generation. Specifically, auto-regressive generation enables LLMs to produce text token by token, with each token conditioned on all previously generated ones. While this approach is highly effective for generating coherent and contextually relevant outputs, it suffers from poor scalability with long input sequences, as the computational and memory requirements grow quadratically with sequence length. The KV cache addresses this issue by storing key and value matrices from previous decoding steps, enabling their reuse and significantly reducing redundant computations.

Several recent surveys [[2]](#b1), [[39]](#b38), [[40]](#b39), [[41]](#b40), [[42]](#b41), [[43]](#b42), [[44]](#b43), [[45]](#b44), [[46]](#b45), [[47]](#b46), [[48]](#b47), [[49]](#b48), [[50]](#b49) have explored the domain of efficient LLMs. These surveys primarily examine various aspects of LLM efficiency, presenting valuable insights while leaving room for further refinement and innovation. In particular, many of these works primarily focus on holistic approaches to improving LLM efficiency, examining a wide range of techniques across multiple dimensions, such as span data-level optimizations (e.g., prompt engineering), model architecture-level optimizations (e.g., efficient transformer designs), and system-level optimizations (e.g., task scheduling). For instance, Ding et al. [[42]](#b41) explore efficiency techniques that integrate data-level and model architecture perspectives, while Miao et al. [[43]](#b42) examine efficient LLM inference from a comprehensive system-level perspective. Similarly, Tang et al. [[46]](#b45), Wan et al. [[44]](#b43), and Xu et al. [[48]](#b47) provide analyses that encompass data, model, and systemlevel optimizations, reflecting holistic approaches to LLM acceleration.

On the other hand, some surveys focus on more specialized aspects for LLM acceleration. For example, Zhu et al. [[2]](#b1), Park et al. [[40]](#b39), Wang et al. [[41]](#b40), and Tang et al. [[46]](#b45) focus on model compression as a key aspect of model-level optimization. Similarly, Kachris et al. [[47]](#b46) examine hardware acceleration strategies tailored for LLMs, while Xu et al. [[48]](#b47) investigate parameter-efficient tuning approaches. Albalak et al. [[49]](#b48) discuss data selection strategies to enhance the efficiency of LLM training, and Xia et al. [[51]](#b50) highlight collaborative techniques, such as speculative decoding [[52]](#b51), [[53]](#b52), to accelerate model inference. Li et al. [[54]](#b53) focus on prompt compression. Similar to our work, Shi et al. [[55]](#b54), Li et al. [[56]](#b55), and Yuan et al. [[57]](#b56) also explore the use of KV caches to accelerate LLMs. However, our survey is both complementary and more comprehensive, offering a detailed taxonomy of KV cache management for text-based and multi-modal LLMs. We categorize techniques across token-level, model-level, and system-level perspectives and include benchmarks for both text and multi-modal scenarios. In particular, complementing existing KV cache surveys, we provide a detailed comparison of the differences and advantages of existing models at the token-level, modellevel, and system-level.

Specifically, this survey provides a comprehensive overview of the current state of KV cache management and its role in accelerating LLM inference. We begin by introducing the transformer architecture and the role of the KV cache in enabling efficient auto-regressive text generation. We then analyze the challenges associated with KV cache management, including its impact on computational complexity, memory usage, and real-time performance. Following this, we present a taxonomy of existing optimization techniques, categorizing them into token-level, model-level, and system-level optimization approaches. Additionally, we discuss datasets and evaluation metrics used to benchmark these techniques and provide insights into their effectiveness across various tasks and applications.

## PRELIMINARY

Large language models (LLMs), pretrained on vast corpora, have demonstrated superior capabilities in context understanding and logical reasoning. These models have achieved remarkable success across a wide range of tasks in various domains, including natural language processing [[3]](#b2), [[4]](#b3), [[5]](#b4) and computer vision [[6]](#b5), [[7]](#b6), [[8]](#b7). Mainstream LLMs, such as GPT [[58]](#b57), LLaMA [[24]](#b23), and DeepSeek [[26]](#b25), are primarily built on the transformer architecture [[33]](#b32). To explore the role of Key-Value (KV) cache management in accelerating LLM computations, we first outline the core components of the transformer model and then introduce the mechanisms for managing the KV cache to accelerate the LLMs. Important notations in this survey are summarized in Tab. 1. Conditional probability

## Transformer Architecture

Transformers [[33]](#b32) have become the backbone of LLMs due to their ability to efficiently capture long-range dependencies sequential data, such as text. This capability makes them particularly well-suited for tasks like machine translation, text generation, and image captioning. The transformer architecture follows an encoder-decoder structure, where most LLMs utilize only the decoder component. We first introduce the core components of the Transformer decoder and then describe the critical auto-regressive generation mechanism. Particularly, we do not describe certain components in transformer, such as normalization, as they do not impact the understanding of KV cache management.

## Transformer Decoder

As shown in Figure [1](#fig_0), a decoder-based transformer architecture is composed of multiple stacked Transformer blocks, each designed to process sequential data effectively. Typically, a Transformer block consists of two core components, i.e., a Multi-Head Self-Attention (MHSA) mechanism and a Feed Forward Network (FFN). These blocks are arranged sequentially, where the output of one block is passed as input to the next. This iterative design allows the model to refine its understanding of the input sequence progressively, making it highly effective for tasks such as text generation and language modeling. Positional Encoding. Before the input sequence is processed by the Transformer blocks, it undergoes a preprocessing phase. First, a tokenizer processes the input sentence X by splitting it into discrete units, such as words or subwords. The resulting sequence can be represented as

$X = [x 1 , x 2 , • • • , x |X| ].$These tokens are then mapped to dense vector representations using an embedding layer, i.e., X = I X E ⊤ , where I X ∈ {0, 1} n×d vocab represents the one- hot vector of tokenized input X, E ∈ R d vocab ×dx is the embedding matrix, and X = [x 1 , x 2 , • • • , x |X| ] ∈ R n×dx is the resulting matrix of embedded token representations. Since the Transformer architecture does not inherently account for the order of tokens in a sequence, positional encodings are added to the token embeddings X to incorporate positional information. This can be expressed as X = X + P E(X), where P E(X) ∈ R n×dx represents a function [[59]](#b58), [[60]](#b59), [[61]](#b60) (e.g., sine and cosine-based positional encoding) that generates positional embeddings for the input X. Transformer Block. Once the input features are prepared, they are passed through a series of stacked Transformer blocks. Each block begins with the Multi-Head Self-Attention (MHSA) mechanism, which captures both local and global dependencies. For each token, the self-attention mechanism computes a weighted sum over all other tokens in the sequence, where the weights are derived from the similarity between the tokens. Particularly, since the operations within each transformer block are identical, we use a single transformer block as an example. Specifically, given the input to a block, denoted as X ∈ R |X|×d , the MHSA mechanism computes the query vectors Q i ∈ R |X|×d k , key vectors K i ∈ R |X|×d k , and value vectors V i ∈ R |X|×dv . These vectors are obtained through learned linear transformations as follows:

$Q i = XW Qi , K i = XW Ki , V i = XW Vi ,(1)$where W Qi ∈ R dx×d k , W Ki ∈ R dx×d k and W Vi ∈ R dx×dv are the learned weight parameters. Then, the self-attention operation is applied to each triple (Q i , K i , V i ), and obtain the output of the i-th attention head Z i as follows:

$Z i = Attention(Q i , K i , V i ) = Softmax Q i K ⊤ i √ d k V i ,(2)$where √ d k is a scaling factor to ensure the numerical stability. To capture diverse relationships, multiple attention with h heads are applied to X in parallel, and their outputs are concatenated with one transformation as follows:

$Z = Concat(Z 1 , Z 2 , . . . , Z h )W O ,(3)$where Concat is concatenation operation and W O ∈ R dv×do is the trainable parameters. Following the self-attention mechanism, the output is passed through a Feed Forward Network (FFN). The FFN is a fully connected neural network that applies two linear transformations separated by a nonlinear activation function σ(•) (e.g, ReLU [[62]](#b61)) :

$FFN(Z) = σ(ZW 1 + b 1 )W 2 + b 2(4)$where W 1 ∈ R do×d1 and W 2 ∈ R d1×d2 are two parameters. Also, b 1 ∈ R d1 and b 2 ∈ R d2 are two bias vectors.

## Auto-regressive Generation Mechanism

LLMs employ an autoregressive mechanism to generate text token by token, with each token conditioned on the previously generated ones. This iterative process ensures that the output sequence remains coherent and contextually appropriate. Formally, given an input sequence of tokens X = [x 1 , x 2 , • • • , x t ], the model predicts the next token x t+1 at each decoding step t by modeling the conditional probability distribution as follows: where h t ∈ R d h represents the hidden state of the LLM regarding X at step t, W out ∈ R d h ×vocab is the output projection matrix, and b out is the bias vector. The softmax function converts the logits into a probability distribution over the vocabulary. Then, at each decoding step, the model generates the next token x t+1 by sampling from the predicted probability distribution:

$P (x t+1 |x 1 , x 2 , • • • , x t ) = Softmax(h t W out + b out ),(5)$$x t+1 ∼ P (x t+1 |x 1 , x 2 , • • • , x t ).(6)$The generated token x t+1 is then appended to the sequence X = [x 1 , • • • , x t , x t+1 ], and the process continues until a special end-of-sequence (EOS) token is generated or a predefined maximum length is reached.

## Key-Value Cache in Transformer Models

Auto-regressive generation is a powerful mechanism that enables LLMs to produce high-quality, contextually coherent text. However, it presents computational challenges for long sequences, as the Keys and Values need to be recomputed for each token during the generation process. The KV cache optimization addresses this issue by storing the previously computed Keys and Values and reusing them for subsequent token generation, thereby reducing redundant computations and improving inference efficiency.

## Auto-regressive Generation with KV Cache

Here, we describe how caching KV pairs of tokens accelerates LLM inference. Specifically, at each decoding step t, the model performs self-attention over the entire sequence X = [x 1 , • • • , x t-1 , x t ] to generate the next token x t+1 . This process requires the computation of Keys and Values matrices for all previously processed tokens in

$X = [x 1 , • • • , x t ].$Notably, when generating the token x t , the LLM has already computed the Keys and Values for the tokens in

$X[1 : t -1] = [x 1 , • • • , x t-1 ].$The KV cache optimizes this process by storing the previously computed Keys and

Values matrices for X[1 : t -1] and reusing them, thereby only requiring the computation of Keys and Values for the new token x t . This significantly improves efficiency by eliminating redundant computations. Formally, at decoding step t, the new token embedding x t is used to compute the query vector q t i , key vector k t i , and value vector v t i as follows:

$q t i = x t W Qi , k t i = x t W Ki , v t i = x t W Vi ,(7)$The newly computed k t i and v t i are then appended to the cached key and value matrices from previous steps:

$K t i = Concat( Kt-1 i , k t i ), V t i = Concat( Vt-1 i , V t i ),(8)$where Kt-1 i ∈ R t-1×d k and Vt-1 i ∈ R t-1×dv represent the cached key and value matrices of tokens in X[1 : t -1]. These cached matrices are then used in the scaled dotproduct attention computation for token x t . The attention output z t i for the token x t at step t is calculated as:

$z t i = Softmax q t i K t i ⊤ √ d k V t i ,(9)$Then, a similar KV reuse process can be applied to different attention heads in each layer of the LLM.

## Time and Space Complexity Analysis

Given a transformer-based L-layer LLM with h attention heads per layer and an input sequence of length

$X = [x 1 , • • • , x t ],$we analyze the time saved and the space required to store cached KV pairs. For simplicity, we assume the Keys and Values of t c tokens are stored for all heads across all LLM layers. Saved Time. For each token, the saved computation time comes from avoiding the repeated computation of Keys and Values in Equation (1), self-attention result in Equation ( [2](#formula_2)), and linear transformation in Equation [(3)](#b2). We omit the time analyze on operations in transformer that do not affect the understanding of KV cache acceleration, such as layer norm and position encoding.

• QKV Computation. The time of computing Queries, Keys and Values for each token in Equation ( [1](#formula_1)) is

$△ 1 = O(2d x d k + d x d v ). • Self-attention Result. Additionally, computing each at- tention result z i in Equation (2) takes O(t(d k + d v )). • Linear Transformation. To merge the h attention results in Equation (3) the time is △ 2 = O(hd v + d v d o )$. Therefore, for t c cached tokens across h attention heads and L layers, the total saved computation time is:

$O (L • h • t c • t • (d k + d v ) + L • h • t c (△ 1 + △ 2 ))(10)$Thus, the saved time is directly proportional to the number of cached tokens t c , significantly accelerating model computation, especially for longer sequences (when t is large). Extra Space. Compared to computation without caching, additional space is required to store the cached KV pairs for t c tokens across h attention heads and L layers. Assuming each Key and Value is stored in Float16 precision, the total extra space needed can be expressed as:

$O(L • h • t c • 2 • sizeof (F loat16))(11)$Thus, for the same LLM model, the extra space required to store the KV pairs primarily depends on the number of cached tokens and the precision of the cached Keys and Values. To address this, existing approaches explore various techniques to reduce the extra space consumption, such as caching only the most important Keys and Values or applying quantization techniques to lower the bit precision of the stored Keys and Values.

## Challenges in KV Cache Management

As analyzed in Sec. 2.2.2, reusing cached KV pairs enables the LLM to avoid recomputing past tokens, resulting in significant speedups during inference. However, as sequence lengths grow, the size of the KV cache increases proportionally, placing significant pressure on memory. Consequently, it becomes challenging to manage this cache effectively to accelerate LLM computation without excessive space usage.

• Cache Eviction Policies: Determining which items to evict when the cache reaches its capacity is a complex problem. Popular policies [[35]](#b34) like Least Recently Used (LRU) or Least Frequently Used (LFU) do not align with LLMs patterns, leading to suboptimal performance. • Memory Management: The memory required for the KV cache grows linearly with both the sequence length and the number of layers, which can quickly exceed the hardware memory limits, especially for long sequences. Consequently, managing the collaboration between different types of storage hardware (e.g., GPU, CPU, or external memory) becomes a significant challenge. • Latency Bottlenecks: Accessing and updating the cache at each decoding step can introduce latency, particularly for hardware with limited memory bandwidth. • Compression Trade-offs: Compressing the KV cache can reduce memory usage but may degrade model performance if key information is lost. • Dynamic Workloads: Handling dynamic and unpredictable workloads, where access patterns and data requirements frequently change, requires adaptive caching strategies that can respond in real time. • Distributed Coordination: In distributed KV caches, maintaining coordination across multiple nodes to ensure consistency, fault tolerance, and efficient resource usage adds significant complexity.

## TAXONOMY

In the above sections, we analyzed how the number of cached Key-Value (KV) pairs significantly impacts both the computation time and the additional memory required during inference. Efficient KV cache management is critical to balancing performance improvements and resource utilization, especially as sequence lengths and model sizes continue to grow. After carefully reviewing existing approaches, we categorize KV cache optimization strategies into three levels: token-level optimization, model-level optimization, and system-level optimizations. Each level addresses specific aspects of the challenges associated with KV cache management and offers distinct techniques to enhance efficiency. The detailed taxonomy is illustrated in Fig. [2](#).

KV Cache Management for Large Language Models Datasets and Benchmarks (Sec. 7) Multi-modal Datasets (Sec. 7.2) Text Datasets (Sec. 7.1) System-level Optimization (Sec. 6) Hardware-aware Design (Sec. 6.3) SSD-based Design (Sec. 6.3.4) Heterogeneous Design (Sec. 6.3.3) I/O-based Design (Sec. 6.3.2) Single/Multi-GPU Design (Sec. 6.3.1) Scheduling (Sec. 6.2) Layer-specific and Hierarchical Scheduling (Sec. 6.2.3) Preemptive and Fairnessoriented Scheduling (Sec. 6.2.2) Prefix-aware Scheduling (Sec. 6.2.1) Memory Management (Sec. 6.1) Prefix-aware Design (Sec. 6.1.2) Architectural Design (Sec. 6.1.1) Model-level Optimization (Sec. 5) Non-transformer Architecture (Sec. 5.3) Hybrid Architecture (Sec. 5.3.2) Adaptive Sequence Processing Architecture (Sec. 5.3.1) Architecture Alteration (Sec. 5.2) Augmented Architecture (Sec. 5.2.2) Enhanced Attention (Sec. 5.2.1) Attention Grouping and Sharing (Sec. 5.1) Cross-Layer Sharing (Sec. 5.1.2) Intra-Layer Grouping (Sec. 5.1.1) Token-level Optimization (Sec. 4) KV Cache Low-rank Decomposition (Sec. 4.5) Learned Low-rank Approximation (Sec 4.5.3) Tensor Decomposition (Sec 4.5.2) Singular Value Decomposition (Sec 4.5.1) KV Cache Quantization (Sec. 4.4) Outlier Redistribution (Sec 4.4.3) Mixed-precision Quantization (Sec 4.4.2) Fixed-precision Quantization (Sec 4.4.1) KV Cache Merging (Sec. 4.3) Cross-layer Merging (Sec 4.3.2) Intra-layer Merging (Sec 4.3.1) KV Cache Budget Allocation (Sec. 4.2) Head-wise Budget Allocation (Sec 4.2.2) Layer-wise Budget Allocation (Sec 4.2.1) KV Cache Selection (Sec. 4.1) Dynamic Selection without Permanent Eviction (Sec 4.1.3) Dynamic Selection with Permanent Eviction (Sec 4.1.2) Static KV Cache Selection (Sec 4.1.1) Fig. 2. Taxonomy of KV Cache Management for Large Language Models. • Token-Level Optimization refers to improving KV cache management efficiency by focusing on fine-grained the careful selection, organization, and compression at the token level, requiring no architectural changes to the orig-inal model. While KV cache selection (Sec. 4.1) focuses on prioritizing and storing only the most relevant tokens. KV cache budget allocation (Sec. 4.2) dynamically distributes memory resources across tokens to ensure efficient cache utilization under limited memory. Furthermore, KV cache merging (Sec. 4.3) reduces redundancy by combining similar or overlapping KV pairs, while KV Cache Quantization (Sec. 4.4) minimizes the memory footprint by reducing the precision of cached KV pairs. Finally, KV cache low-rank decomposition (Sec. 4.5) uses low-rank decomposition technique to reduce cache size. • Model-level Optimization refers to designing an efficient model structure to optimize KV cache management. This can further refer to several strategies: Attention grouping and sharing (Sec. 5.1) methods examine the redundant functionality of key and values and group and share KV cache within or across transformer layers. Architecture alterations (Sec. 5.2 emerge to design new attention mechanisms or construct extrinsic modules for KV optimization. Furthermore, there are also works designing or combining non-transformer architectures 5.3 that adopt other memory efficient designs like recurrent neural networks to optimize the KV cache in traditional transformers. • System-level Optimization refers to optimizing the KV Cache management through two classic low-level aspects: memory management (Sec. 6.1) and scheduling (Sec. 6.2). While memory management techniques focusing on architectural innovations like virtual memory adaptation, intelligent prefix sharing, and layer-aware resource allocation, scheduling strategies have evolved to address diverse optimization goals through prefix-aware methods for maximizing cache reuse, preemptive techniques for fair context switching, and layer-specific mechanisms for fine-grained cache control. In addition, we provide a detailed introduction for hardware accelerator design in Sec. 6.3, including single/multi-GPU, I/O-based solutions, heterogeneous computing and SSD-based solutions.

## TOKEN-LEVEL OPTIMIZATION

In the token level, optimization focuses exclusively on improving KV cache based on the characteristics and patterns of KV pairs of tokens, without considering enhancements from model architecture improvements or system parallelization techniques. In general, token-level optimization methods are primarily guided by observations from LLMs and sequential inputs. Existing approaches can be categorized into five main types: KV cache selection, KV cache budget allocation, KV cache merging, KV cache quantization, and KV cache low-rank decomposition. The taxonomy of the token-level optimization is shown in Fig. [3](#).

## KV Cache Selection

KV cache selection mechanisms have emerged as a critical optimization strategy, aimed at reducing memory utilization of KV caches, minimizing inference latency, and enhancing overall throughput in large language models. These optimization objectives have driven the development of various selection methodologies, which can be classified into two distinct categories: (1) static KV cache selection, which performs token filtering exclusively during the prefilling phase, with selected tokens remaining fixed throughout subsequent decoding steps; and (2) dynamic KV cache selection, which continuously updates KV cache during the decoding phase, enabling adaptive cache management. In dynamic KV cache selection approaches, KV cache tokens that are not selected may be permanently evicted or offloaded to hierarchical caching devices such as CPU memory, implementing a multi-tier storage strategy. Given that real-time KV cache selection during decoding may incur substantial computational overhead, several studies have focused on developing optimized retrieval algorithms to enhance the efficiency of this process. These optimizations include blocklevel retrieval instead of token-level granularity to reduce search complexity, asynchronous query mechanisms to hide latency, and parallel retrieval pipelines to accelerate the selection process. These optimization efforts aim to mitigate the computational burden while maintaining the effectiveness of token selection. The summary of the KV cache selection is listed in Tab. 2.

## Static KV Cache Selection

Static KV cache selection methods perform a one-time compression on the KV Cache immediately after the prefilling phase is completed. The model then uses this compressed KV cache for subsequent decoding inference. FastGen [[133]](#b132) introduces a pattern-aware approach by identifying five fundamental attention structures and implementing targeted selection strategies. These include proximity-based retention for local attention patterns, selective preservation of critical tokens for punctuation-focused attention, frequency-based filtering for sparse attention distributions, and complete token retention for broad attention patterns.

SnapKV [[134]](#b133) simplifies FastGen's approach by focusing solely on retrieving tokens based on their importance scores. It demonstrates that among all prompt tokens, only a portion carries crucial information for response generation, with these tokens maintaining their significance during the generation phase. The approach employs an end-positioned observation window to detect these important contextual tokens. Their corresponding key-value pairs are then concatenated with the tokens from the observation window. Attention-Gate [[135]](#b134) introduces a learnable KV-Cache eviction mechanism that processes the entire context sequence and generates token-wise eviction decisions through a parameterized policy network, enabling dynamic in-context memory management.

## Dynamic Selection with Permanent Eviction

This category of methods performs frequent KV cache selection during the decoding phase, permanently removing unselected KV cache tokens from memory. Early works employ a sliding-window mechanism to address longtext inference challenges, where tokens falling outside the window are permanently evicted and become inaccessible. StreamingLLM [[136]](#b135) uncovers a crucial phenomenon in transformer attention where preserved key-value pairs from initial sequence tokens maintain crucial model performance. This attention sink effect manifests through asymmetric attention weight accumulation at early positions, regardless of semantic significance. The approach leverages this characteristic by incorporating attention sink positions with recent context for efficient processing. LM-Infinite [[137]](#b136) demonstrates that conventional techniques, including sliding-window patterns and relative positional Token-level Optimization KV Cache Low-rank Decomposition (Sec. 4.5) Learned Low-rank Approximation (Sec 4.5.3) LESS [[63]](#b62), MatryoshkaKV [[64]](#b63) Tensor Decomposition (Sec 4.5.2) DecoQuant [[65]](#b64) Singular Value Decomposition (Sec 4.5.1) ECKVH [[66]](#b65), EigenAttention [[67]](#b66), ZDC [[68]](#b67), LoRC [[69]](#b68), ShadowKV [[70]](#b69), Palu [[71]](#b70) KV Cache Quantization (Sec. 4.4) Outlier Redistribution (Sec 4.4.3) Attention-Gate [[135]](#b134) Fig. [3](#). Taxonomy of the Token-level Optimization for KV Cache Management.

encodings, fail to resolve length generalization issues. The study introduces a novel methodology through the integration of Λ-shaped attention masking and attention distance ceiling mechanisms.

Recent works have explored leveraging attention scores as a criterion for selecting significant KV cache tokens. H2O [[127]](#b126) observes that attention computations are primarily driven by a select group of high-impact tokens, known as Heavy Hitters (H2). This method reformulates cache optimization as a dynamic submodular problem, utilizing cumulative attention scores to guide token retention decisions. Unlike H2O, BUZZ [[128]](#b127) employs a beehive-like structure that selects Heavy Hitters in local KV cache segments. NACL [[129]](#b128) identifies a fundamental limitation in H2O, namely their dependence on potentially biased local attention statistics. To overcome this issue, they develop an alternative approach implementing a diversified random eviction strategy for token selection. Scissorhands [[130]](#b129) builds upon the temporal significance principle, which suggests that tokens demonstrating historical importance maintain their influence in subsequent computational steps. This observation enables the preservation of repetitive attention patterns through selective token retention. Additionally, Keyformer [[131]](#b130) reveals that token removal distorts the

TABLE 2 Comparison of KV cache selection strategies. Method Initial tokens Top-k tokens Recent tokens Permanent eviction Dynamic selection Selection granularity Remark FastGen [133] ✓ ✓ ✓ ✓ token five attention structures SnapKV [134] ✓ ✓ ✓ token observation window-based Attention-Gate [135] ✓ ✓ token learned eviction policy StreamingLLM [136] ✓ ✓ ✓ ✓ token initial and recent tokens LM-Infinite [137] ✓ ✓ ✓ ✓ token distance ceiling H2O [127] ✓ ✓ ✓ ✓ token accmulative attention score BUZZ [128] ✓ ✓ ✓ ✓ ✓ token beehive-like structure Scissorhands [130] ✓ ✓ ✓ ✓ token persistence of importance NACL [129] ✓ ✓ ✓ ✓ token diversified random eviction Keyformer [131] ✓ ✓ ✓ ✓ token gumbel logit adjustment InfLLM [121] ✓ ✓ ✓ ✓ block block-level KV management Quest [122] ✓ ✓ block new block representation PQCache [98] ✓ ✓ ✓ ✓ block product quantization SqueezedAttention [123] ✓ ✓ cluster hierarchical clusters RetrievalAttention [124] ✓ ✓ ✓ ✓ Token ANN search EM-LLM [125] ✓ ✓ ✓ ✓ event episodic events SparQ [138] ✓ ✓ ✓ token low-dimensioanl retrieval InfiniGen [139] ✓ ✓ token asynchronous prefetching RecycledAttention [140] ✓ ✓ ✓ token periodic top-k selection MagicPIG [141] ✓ ✓ ✓ ✓ token Local Sensitive Hash

underlying softmax probability distribution. Considering the pivotal role of softmax distributions in token significance evaluation, they incorporate regularization techniques to mitigate these distributional perturbations. SepLLM [[132]](#b131) observes that separator tokens (e.g., commas, periods, and line breaks) receive disproportionately high attention scores and naturally summarize text segments. Building on this, SepLLM retains separator tokens together with initial tokens, important tokens, and recent tokens in the cache.

## Dynamic Selection without Permanent Eviction

The aforementioned permanent eviction-based approaches face two significant limitations. First, the irreversible eviction of tokens potentially impairs the model's performance on long-sequence tasks, particularly in needle-in-a-haystack scenarios, and these methods prove challenging to adapt to multi-turn dialogue contexts. Second, KV cache selection during the decoding phase introduces computational overhead, adversely affecting decoding latency and compromising end-to-end acceleration. To address these challenges, several studies have focused on developing decoding-phase KV cache selection strategies without permanent eviction. These approaches typically employ multi-tier cache systems (e.g., CPU-GPU hierarchical caching) and leverage advanced data structures and system-level enhancements to optimize retrieval efficiency, enabling efficient inference with reduced GPU KV cache footprint.

To accelerate the retrieval of critical tokens, several research efforts have proposed index-based approaches that organize and access KV cache at block or cluster granularity, enabling efficient query and extraction operations. InfLLM [[121]](#b120) maintains full KV cache in blocks while facilitating long sequence processing through a hierarchical stor-age strategy. The framework employs CPU-GPU memory orchestration, preserving essential tokens and current computational units in GPU memory while offloading less frequently accessed units to CPU memory. To further enhance top-k block retrieval precision, the Quest [[122]](#b121) framework presents a refined block representation approach based on minimal and maximal key values in KV cache blocks. PQ-Cache [[98]](#b97) also implements block-based KV cache management and identifies salient tokens through Maximum Inner-Product Search (MIPS), leveraging Product Quantization (PQ) codes and centroids. SqueezedAttention [[123]](#b122) employs K-means clustering in an offline stage to group semantically similar keys, with each group represented by a centroid. During inference, it compares input queries against these centroids to identify and load only the semantically relevant keys from the context. Similarly, RetrievalAttention [[124]](#b123) index KV cache tokens using approximate nearest neighbor search (ANNS) techniques. Additionally, EM-LLM [[125]](#b124) dynamically segments incoming tokens into episodic events. Besides, it implements a hybrid retrieval mechanism that combines semantic similarity matching with temporal context to efficiently access relevant KV cache segments. Similarly, ClusterKV [[126]](#b125) groups tokens into semantic clusters and selectively recalls them during inference, achieving both high accuracy and efficiency for LLMs.

To accelerate top-k token identification, SparQ [[138]](#b137) identifies the r most significant elements in the incoming query vector and selectively retrieves the corresponding components along the hidden dimension of the cached key matrix K for approximate attention computation. To overlap prefetching latency, InfiniGen [[139]](#b138) employs asynchronous prefetching, utilizing indices of salient KV entries selected by queries from the previous layer to retrieve KV

TABLE 3 Comparison of KV cache budget allocation strategies.

## Method Layer-wise Head-wise Retrieval-head Input-specific Extra-calibration Remark

PyramidKV [[116]](#b115) ✓ pyramid-shaped PyramidInfer [[117]](#b116) ✓ pyramid-shaped DynamicKV [[118]](#b117) ✓ ✓ maximize attention retention rate PrefixKV [[119]](#b118) ✓ ✓ maximize attention retention rate CAKE [[142]](#b141) ✓ ✓ layer-specific preference score SimLayerKV [[120]](#b119) ✓ ✓ KV cache compression for lazy layers AdaKV [[110]](#b109) ✓ ✓ minimize attention computation loss CriticalKV [[111]](#b110) ✓ ✓ minimize attention computation loss LeanKV [[112]](#b111) ✓ ✓ maximize attention retention rate RazorAttention [[113]](#b112) ✓ ✓ ✓ echo and induction heads HeadKV [[114]](#b113) ✓ ✓ ✓ retrieval and reasoning heads DuoAttention [[115]](#b114) ✓ ✓ ✓ learned retrieval heads cache entries in the current layer. To ensure maximum model performance, RecycledAttention [[140]](#b139) sustains the entire KV cache during inference computations, yielding no improvements in memory efficiency. The approach performs periodic top-k token selection to identify salient tokens. Moreover, MagicPIG [[141]](#b140) shows that attention-based topk selection may incur performance degradation. To address this limitation, they introduce a novel heterogeneous computing framework leveraging Locality Sensitive Hashing (LSH) techniques. The system stores LSH hash tables and performs attention estimation on CPU.

## Summary and Future Directions

Static KV cache selection algorithms demonstrate superior decoding efficiency overall; however, their efficacy remains to be thoroughly validated in multi-turn dialogues and extended decoding length scenarios. Dynamic KV cache selection algorithms, while adaptive, introduce additional computational overhead during the decoding phase due to frequent cache selection operations. Multi-tier cache architectures and prefetching schemes partially mitigate these challenges, yet their capability to achieve rapid and accurate retrieval within acceptable decoding latency constraints requires further empirical validation, particularly in realworld applications involving long sequences. Furthermore, existing selection methods predominantly rely on attention score-based top-k selection mechanisms. However, based on existing positional encoding schemes, current top-k approaches may not be able to effectively identify and extract relevant tokens in ultra-long sequence tasks.

## KV Cache Budget Allocation

The hierarchical architecture of LLMs leads to diverse information extraction patterns across layers, with each layer's KV-cache contributing differently to model performance. This inherent heterogeneity indicates that uniform KV-cache compression across layers may be suboptimal. KV cache budget allocation addresses this challenge by intelligently distributing memory resources based on each component's importance to prediction accuracy, thereby optimizing memory utilization while minimizing accuracy degradation. Current budget allocation strategies can be categorized into two levels of granularity: layer-wise budget allocation, which assigns different compression ratios across model layers, and the more fine-grained head-wise budget allocation, which enables precise memory distribution across individual attention heads within each layer, offering more flexible and targeted optimization opportunities. The summary of KV budget allocation is listed in Tab. 3.

## Layer-wise Budget Allocation

In contrast to conventional approaches with uniform KV cache sizes, PyramidKV [[116]](#b115) employs a pyramid-shaped memory allocation strategy, assigning larger cache capacities to lower layers that progressively decrease in upper layers. This design is supported by the observation that lower layers exhibit uniform attention distributions across input sequences, while upper layers show concentrated attention on specific tokens. PyramidInfer [[117]](#b116) also adopts a pyramid-shaped budget allocation strategy while selecting tokens with high attention values at each layer. Additionally, during the decoding phase, PyramidInfer dynamically maintains a set of significant tokens through frequent updates driven by attention values. Unlike previous methods, DynamicKV [[118]](#b117) implements an input-adaptive budget allocation strategy by analyzing attention patterns. Specifically, it computes the average attention scores between recent and historical tokens, identifies the top-k tokens with highest attention values across layers, and proportionally distributes the budget based on the density of significant tokens in each layer. Similarly, PrefixKV [[119]](#b118) identifies the most important tokens for each layer by computing the average attention score of tokens within that layer.

PrefixKV [[119]](#b118) then uses a unified threshold to determine the number of retained tokens, adaptively adjusting the retention for each layer based on its importance distribution. CAKE [[142]](#b141) examines attention scores through two lenses: the spatial distribution of inter-token attention and the temporal evolution of attention focus. These measurements are combined to compute layer-specific importance scores, which further guide the allocation of memory resources. Additionally, SimLayerKV [[120]](#b119) identifies lazy layers -those exhibiting limited effectiveness in capturing long-range de- ✓ Token Attention Score Many-to-One ✓ AIM [[106]](#b105) ✓ Token Cosine Similarity Many-to-One ✓ Look-M [[107]](#b106) ✓ Token Cosine Similarity Many-to-One ✓ KVMerger [[108]](#b107) ✓ Token Weighted Gaussian Kernel Many-to-One ✓ CHAI [[109]](#b108) ✓ Head Attention Score Many-to-One ✓ MinCache [[99]](#b98) ✓ Token Angular Distance Two-to-One ✓ KVSharer [[100]](#b99) ✓ Layer Euclidean distance Many-to-One ✓ pendencies. The framework then selectively preserves cache entries, maintaining initial and recent tokens for lazy layers while retaining complete KV cache for non-lazy layers.

## Head-wise Budget Allocation

AdaKV [[110]](#b109) leverages the observation that attention patterns exhibit distinct concentrations across different heads. It implements head-specific memory allocation by optimizing an L1 loss bound between the original and pruned multihead attention outputs. Within the constraints of a layerwise budget, the method distributes cache capacity among heads to maximize the preserved attention information collectively. Building upon AdaKV, CriticalKV [[111]](#b110) introduces significant enhancements by recognizing that the importance of KV cache entries extends beyond attention weights to encompass value states and pretrained parameter matrices. Leveraging this insight, the framework implements a novel selection algorithm that identifies essential cache entries by minimizing the maximum potential output perturbation. LeanKV [[112]](#b111) implements a fine-grained memory optimization strategy that operates independently for each attention head and input request. The method identifies the smallest subset of tokens necessary to preserve the majority of information flow, allocating cache space based on a predefined attention score threshold -typically maintaining 95% of the total attention mass. Retrieval head-based methods represent a specialized category of head-wise allocation strategies that focuses on identifying and prioritizing attention heads crucial for extracting key information from long sequences. This approach allocates larger cache budgets to these specialized heads, known as retrieval heads [[143]](#b142), due to their significant role in information extraction. RazorAttention [[113]](#b112) characterizes two distinct categories of retrieval heads: echo heads, which focus on previously occurring identical tokens, and induction heads, which attend to antecedent tokens that precede current token repetitions. This framework implements differential caching strategies, maintaining complete cache entries for retrieval heads while condensing remote tokens into consolidated compensation tokens for nonretrieval heads. HeadKV [[114]](#b113) further enhances RazorAt-tention by introducing a novel head assessment framework that simultaneously evaluates both retrieval and reasoning capabilities to optimize KV cache allocation strategies. DuoAttention [[115]](#b114) further introduces a parameterized approach to distinguish between two categories of attention mechanisms: retrieval heads, essential for comprehensive long-context processing, and Streaming heads, which primarily engage with recent tokens and attention sinks. This classification is achieved through learned parameters that automatically identify retrieval heads requiring full attention spans.

## Summary and Future Directions

Despite recent advances and growing attention in KV cache budget allocation research, several critical challenges remain unaddressed. First, the relationship between allocation strategies and model performance requires further investigation. For instance, a notable discrepancy exists between pyramid-shaped allocation strategies [[116]](#b115), [[117]](#b116) advocating larger budgets for lower layers, and retrieval head-based studies [[113]](#b112), [[114]](#b113) which demonstrate that lower layers rarely exhibit retrieval head characteristics and thus require minimal cache resources. Additionally, the field lacks comprehensive experimental comparisons, particularly regarding the compatibility and performance benefits of headwise budget allocation strategies with state-of-the-art frameworks like vLLM [[144]](#b143) and FlashAttention [[145]](#b144). Also, existing methods, such as PyramidInfer [[117]](#b116), demonstrate some adaptability to input attention patterns. However, future research could target real-time, task-specific allocation strategies that dynamically adjust memory budgets during inference based on input characteristics, task complexity, or downstream requirements.

## KV Cache Merging

KV cache merging offers a promising solution by compressing or consolidating KV caches without significantly degrading model accuracy. Rather than a uniform compression strategy, KV cache merging techniques leverage the inherent redundancy within and across layers to dynamically optimize memory utilization. These methods aim to reduce the size of KV caches while preserving critical information necessary for accurate attention computations, enabling efficient inference in resource-constrained settings. Existing KV cache merging strategies can be categorized into two primary approaches: intra-layer merging, which focuses on consolidating KV caches within individual layers to reduce memory usage per layer, and cross-layer merging, which targets redundancy across layers to eliminate unnecessary duplication. Both approaches offer complementary advantages, providing flexibility to balance memory savings and model performance degradation. The summary of the KV cache merging is listed in Tab. 4.

## Intra-layer Merging

As the input sequence length increases, the number of Keys and Values grows, leading to higher computational costs for the attention process. To address this, CCM [[101]](#b100), LoMA [[102]](#b101), DMC [[103]](#b102) propose to learn a compression module to compress KV of tokens.

Specifically, CCM [[101]](#b100) inserts a special indicator token, [COMP], into the input sequence and compresses the accumulating past attention key/value (KV) pairs in each layer between these indicators into a compact memory space. This compression leverages techniques inspired by the Compressive Transformer [[146]](#b145) and Gisting [[147]](#b146). Instead of computing attention across all tokens, CCM [[101]](#b100) computes attention scores for each new token by referencing the merged token. Similarly, LoMA [[102]](#b101) inserts a special token into the input sequence to determine which consecutive tokens should be compressed. LoMA [[102]](#b101) performs compression using bidirectional attention, repetition zone supervision, and carefully designed attention masks and loss functions. DMC [[103]](#b102) learns a variable to decide whether to append new KV pairs to the cache when necessary or to merge them into existing KV representations using a weighted average. Note that CCM [[101]](#b100), LoMA [[102]](#b101), and DMC [[103]](#b102) require supervised learning to learn a compression module.

Instead, CaM [[104]](#b103), KVMerger [[108]](#b107), and D2O [[105]](#b104) are training-free, which rely on observations and directly propose rule-based or heuristic-based merging strategies. Specifically, they separate the Keys and Values of tokens in each layer into important (retrained) and unimportant (evicted) tokens. They then keep potentially useful unimportant tokens by merging their Keys and Values with retained important tokens, ensuring that no valuable information is lost. Particularly, D2O [[105]](#b104) merges merges the Key (or Value) of a evicted token with one retained token based on cosine similarity. Similar to D2O based on cosine similarity, AIM [[106]](#b105) and Look-M [[107]](#b106) merges Keys (resp. Values) of multiple tokens into one. CaM [[104]](#b103) merges the Keys (or Values) of multiple evicted tokens with retained tokens based on attention scores to get the final merged results. Also, KVMerger [[108]](#b107) first identifies the merge token sets by clustering consecutive tokens with high cosine similarity, ensuring that only adjacent tokens with strong contextual relevance are grouped together. Then, KVMerger merges the tokens in each merge set into the pivotal token (chosen based on the highest attention score) using Gaussian kernel weights, where closer tokens contribute more to the merged state.

Instead of merging the KV of multiple tokens into one, CHAI [[109]](#b108) observes that heads in multi-head attention often produce highly correlated attention scores for tokens, particularly in the later layers of LLMs. To exploit this redundancy, CHAI [[109]](#b108) clusters attention heads within each layer that produce similar outputs and computes attention for only a single representative head in each cluster. Specifically, within each cluster, CHAI [[109]](#b108) selects one representative head to perform the attention computation, and the computed attention scores are shared across all heads in the cluster.

## Cross-layer Merging

MiniCache [[99]](#b98) observes that KV caches in middle-to-deep layers exhibit high angular similarity, making them ideal for merging. To achieve this, MiniCache [[99]](#b98) merges the Key (and Value) pairs of each token from adjacent similar layers into a single shared representation. Specifically, MiniCache [[99]](#b98) decomposes KV vectors into magnitude and direction components, storing only the shared directional vectors, token magnitudes, and unmergeable tokens to maximize memory efficiency. Differently, KVSharer [[100]](#b99) observes a counterintuitive phenomenon: when the KV caches of two layers differ significantly, sharing one layer's KV cache with another during inference does not cause significant performance degradation. Based on this observation, KVSharer [[100]](#b99) computes the Euclidean distance between the KV caches of all layer pairs, ranks the pairs by dissimilarity, and prioritizes the most dissimilar layers for sharing. Since KVSharer [[100]](#b99) can share the KV cache of one layer to multiple other layers, the stored KV cache is eliminated significantly.

## Summary and Future Directions

KV cache merging represents a transformative approach to optimizing memory utilization in LLMs by consolidating or compressing KV caches while maintaining high model accuracy. However, there are several key directions and challenges for future exploration in this domain. Firstly, current KV cache merging methods are typically designed to work across a wide range of tasks, but fine-tuning merging strategies for specific tasks or domains could further enhance efficiency. For example, certain tasks may tolerate more aggressive merging due to inherent redundancy in their attention patterns, while others may require more conservative approaches to preserve accuracy. Adaptive merging mechanisms that adjust compression levels on-thefly based on task difficulty, sequence length, or available hardware resources are an exciting avenue for future work. Secondly, sparse attention mechanisms, which already reduce the computational complexity of attention by operating on subsets of tokens, could be combined with KV cache merging to achieve even greater efficiency. Exploring how merging complements sparsity-based approaches, such as block-sparse or low-rank attention, could lead to novel hybrid solutions. Thirdly, while empirical results show that merging does not significantly degrade performance, providing theoretical guarantees about the preservation of critical information could enhance the reliability of these methods. Future work might focus on quantifying the rela- WKVQuant [[88]](#b87) Learnable shifting ✓ QAQ [[148]](#b147) Adaptive quantization bits

$✓ ✓ ✓ ✓ MiKV [90] Dynamic outlier-aware ✓ ✓ ✓ GEAR [89] Dynamic outlier-aware ✓ ✓ ZIPVL [91] Conventional ✓ ✓ ✓$CacheGen [[149]](#b148) Layer-wise, token-locality

$Atom [150] Group-based ✓ ✓$tionship between merging strategies, token importance, and attention accuracy to provide more formal guarantees.

## KV Cache Quantization

Quantization technique [[151]](#b150), [[152]](#b151), [[153]](#b152), [[154]](#b153), [[155]](#b154) has been widely used to accelerate machine learning models from different aspects, such model parameter quantization [[156]](#b155), [[157]](#b156), [[158]](#b157), [[159]](#b158) and data feature quantization [[160]](#b159), [[161]](#b160). Similarly, Key-Value (KV) cache quantization is emerging as a highly promising solution to address the memory and computational bottlenecks in LLMs. During autoregressive decoding, LLMs generate key-value pairs for every attention layer across all tokens in the sequence.

If we store all KV pairs in the memory with full precision, this cache grows exponentially with longer sequences, increasing the memory and bandwidth requirements significantly. Quantization reduces the precision of numerical representations (e.g., from FP32 to INT8 or INT4), drastically compressing the size of the KV cache. This compression can achieve up to 4x or more memory savings, making it feasible for LLMs to operate on resource-constrained devices like GPUs with limited memory or edge devices. However, the presence of outliers in Keys and Values poses a significant challenge for low-bit quantization, as these extreme values can lead to substantial performance degradation when compressed into reduced bit representations [[162]](#b161), [[163]](#b162), [[164]](#b163). Based on the techniques used, existing KV cache quantization approaches can be classified into three main categories: Fixed-precision quantization, where all Keys and Values are quantized to the same bitwidth; Mixed-precision quantization, which assigns higher precision to critical parts of the cache while using lower precision for less important components; and Outlier redistribution, which redistributes or smooths the outliers in Keys and Values to improve quantization quality. These methods collectively enable efficient KV cache compression while mitigating the performance degradation typically associated with low-bit quantization. 

## Fixed-precision Quantization

Fixed-precision quantization proposes quantizing different Keys (different Values) of tokens to the same bit-width.

ZeroQuant [[95]](#b94) propose per-token quantization for Keys and Values. As shown in Fig. [4](#fig_1), the per-token quantization approach quantizes tokens individually. Particularly, Zero-Quant [[95]](#b94) dynamically computes the min-max range for each token during inference. This ensures that each token is quantized based on its unique range, significantly reducing quantization error. Also FlexGen [[96]](#b95) and QJL [[97]](#b96) directly perform per-token quantization for Keys and Values, where the scaling factor and zero-point are shared among all elements within the same token. PQCache [[98]](#b97) uses product quantization approaches [[161]](#b160), [[165]](#b164) to compress KV pairs. However, uniform quantization approaches, which use a fixed bit-width for keys and values across all tokens, can often be suboptimal. It is because they ignore the varying importance of tokens [[112]](#b111) and account for the outlier patterns in Keys and Values [[84]](#b83), [[148]](#b147).

## Mixed-precision quantization

Unlike fixed-precision quantization, where all Keys or Values are quantized to the same bit-width (e.g., 4-bit or 8bit), mixed-precision quantization assigns higher or full precision to Keys and Values of critical tokens and parts while using lower precision for less critical parts. The summary of KV mixed-precision quantization is listed in Tab. 5. KVQuant [[84]](#b83) proposes several strategies to quantize Keys and Values smoothly based on observations. Firstly, KVQuant observes that the key values exhibit outliers in specific channels prior to applying Rotary Positional Embedding (RoPE). However, after applying RoPE, the magnitudes of these outlier channels become less consistent, creating a unique challenge for low-precision quantization. Thus, KVQuant [[84]](#b83) proposes to quantize the Keys per channel before applying the RoPE operations and to quantize the Values per token. Secondly, KVQuant [[84]](#b83) observes that KV cache activations contain outliers that skew the quantization range. To address this, they isolate outliers per vector (e.g., per-channel or per-token), store them in a sparse format, and quantize the remaining values to a narrower range. Thirdly, LLMs disproportionately allocate high attention scores to the first token (i.e., attention sink), and quantizing the first token will damage the performance of LLMs. Thus, KVQuant [[84]](#b83) retains the first token in full precision (FP16) while quantizing the rest of the sequence, which is also used by IntactKV [[85]](#b84) and SKVQ [[86]](#b85). Similar to KVQuant [[84]](#b83), KIVI [[87]](#b86) quantizes the Key cache per-channel, as certain channels exhibit large outliers, and the Value cache pertoken, as there are no significant outlier patterns in the Value cache. Additionally, KIVI [[87]](#b86) retains the most recent Keys and Values in full precision, while quantizing older KVs. This approach is based on the observation that the most recent KVs are critical for generating subsequent tokens. Similar to KIVI [[87]](#b86), WKVQuant [[88]](#b87) temporarily retains the most recent Keys and Values in full precision, while quantizing only the past KV cache. This approach helps preserve precision during computation. Additionally, WKVQuant [[88]](#b87) introduces a two-dimensional quantization strategy, which optimizes parameter matrix to align the values in the KV cache into a smoother and more uniform range, significantly improving quantization quality. GEAR [[89]](#b88), MiKV [[90]](#b89), ZipCache [[92]](#b91) and ZIPVL [[91]](#b90) quantize the KV cache based on the importance of each to achieve efficient and effective compression. First, GEAR [[89]](#b88) applies quantization to compress the majority of less important entries (e.g., 98%) to ultra-low precision, significantly reducing memory usage. Next, GEAR [[89]](#b88) employs a lowrank matrix to approximate residual errors, capturing structured patterns in the data. Also, GEAR [[89]](#b88) uses a sparse matrix stores outliers, correcting individual errors caused by these values. MiKV [[90]](#b89) is a mixed-precision KV cache quantization approach. Based on the importance of each token, measured using existing methods like H2O [[166]](#b165) and SnapKV [[134]](#b133), MiKV [[90]](#b89) stores less important KV pairs in low precision while retaining the most important KV pairs in high precision. Instead of approximating the importance weight of each token, ZipCache [[92]](#b91) accurately computes the importance of each token. Instead of computing importance score, PrefixQuant [[93]](#b92) observes that token-wise outliers frequently occur at fixed positions (e.g., initial tokens) or low-semantic-value tokens (e.g., ".", "\n"). Based on this observation, PrefixQuant [[93]](#b92) identifies high-frequency outlier tokens in LLMs offline and prefixes them in the KV cache, effectively eliminating token-wise outliers. Similarly, MiniKV [[94]](#b93) observes that important tokens can be identified before generation and remain consistent throughout the generation process, retaining these important tokens in high precision.

QAQ [[148]](#b147) proposes a quality adaptive quantization approach to dynamically determine the suitable quantization bit for each token, based on its importance and sensitivity, while handling outliers and exceptions to maintain model performance. SKVQ [[86]](#b85) introduces the clipped dynamic quantization with channel reorder. First, SKVQ [[86]](#b85) uses a transformation-invariant permutation to group similar channels based on their statistical characteristics and applies clipped dynamic quantization to mitigate the outlier problem. Second, SKVQ [[86]](#b85) maintains high precision for the initial tokens and the most recent tokens while quantizing older tokens. Consequently, SKVQ [[86]](#b85) effectively reduces quantization errors and improves the accuracy of the quantized model. CacheGen [[149]](#b148) and AsymKV [[167]](#b166) use layerwise asymmetric quantization, assigning higher-bit precision to key matrices in sensitive early layers and lowerbit precision to less sensitive layers, balancing memory efficiency and performance. Particularly, CacheGen [[149]](#b148) also exploits token-wise locality by encoding deltas (differences) between KV tensors of nearby tokens instead of raw values. Atom [[150]](#b149) identifies and separates outlier channels, reordering the matrix to group these outlier channels at the end, thereby ensuring regular memory access patterns for improved hardware utilization. Then, Atom [[150]](#b149) quantizes outliers with higher precision, while normal channels are quantized to INT4 for maximum efficiency. In particular, Atom [[150]](#b149) applies fine-grained group quantization by dividing matrices into smaller subgroups (e.g., 128 elements per group) and performing quantization independently within each group.

## Outlier Redistribution

As previously mentioned, outliers in the Keys and Values present significant challenges for their quantization. Recent research has proposed two main approaches to address this issue: redistributing the outliers into newly appended virtual tokens or applying equivalent transformation functions to smooth the Keys and Values for improved quantization accuracy. The summary of existing outlier redistribution models are listed in Table. [6](#b5).

Specifically, MassiveActivation [[72]](#b71) highlights the phenomenon of massive activations in large language models (LLMs), where a small subset of activations is exponentially larger than the rest. To address this, MassiveActivation [[72]](#b71) proposes appending a virtual token to the inputs, allowing LLMs to encapsulate the massive outliers within these learned keys and values for each head. Then, we introduce the equivalent transformation function-based approaches. Firstly, QuaRot [[73]](#b72), Qserve [[74]](#b73), and Q-INT4 [[75]](#b74) redistributes outlier values across all channels by Hadamard rotation, successfully lowering the maximum value of outlier tokens. The Hadamard rotation of activations can be incorporated into the preceding linear layer, thereby redistributing the outliers of Keys and Values into the parameters. Despite this improvement, outlier tokens still exhibit magnitudes hundreds of times greater than normal tokens, causing notable performance issues when using shared quantization scales across tokens [[93]](#b92). Expanding on this idea, SpinQuant [[76]](#b75) proposes training an orthogonal matrix instead of relying on a random Hadamard matrix to achieve better performance. Similarly, DuQuant [[77]](#b76) employs channel permutation to evenly distribute outliers across blocks and utilizes block rotation to further smooth outliers.

TABLE 6 The summary of outlier redistribution models in Sec. 4.4.3.

## Model Operation Formula Learn Remarks

MassiveAct. [[72]](#b71) Add virtual tokens softmax

$Q K T , k ′ √ d V v ′T ✓ Learnable k ′ , v ′ QuaRot [73] Hadamard rotation XW ⊤ = (XH)(H ⊤ W ⊤ ) × H ⊤ H = I$Qserve [[74]](#b73) Hadamard rotation

$XW ⊤ = (XH)(H ⊤ W ⊤ ) × H ⊤ H = I Q-INT4 [75] Hadamard rotation XW ⊤ = (XH)(H ⊤ W ⊤ ) × H ⊤ H = I SmoothQuant [78] Scaling (X diag(s) -1 ) • (diag(s)W ⊤ ) × s ∈ R c i QS+ [79] Scaling, Shifting ((X -z) diag(s) -1 • diag(s) + z)W ⊤ × s ∈ R c i AWQ [82] Scaling arg mins XW ⊤ -X diag(s) -1 )Q(diag(s)W ⊤ ) ✓ Quantization Q(•)$OmniQuant [[83]](#b82) Scaling, shifiting

$Qa X-δ s Qw s ⊙ W ⊤ + B + δW ⊤ ✓ Learnable Qa(•), Qw(•) DuQuant [77] Rotation, permutation [(X • Λ) R(1) • P • R(2) ] • [ R⊤ (2) • P ⊤ • R⊤ (1) (Λ -1 • W ⊤ )] × Matrices P, R$AffineQuant [[80]](#b79) Affine transform

$arg min P XW ⊤ -XP -1 Q(PW ⊤ ) 2 F ✓ Quantization Q(•)$FlatQuant [[81]](#b80) Affine transform AffineQuant + P = P1 ⊗ P2 ✓ Decomposition SmoothQuant [[78]](#b77) leverages a key observation that different tokens show similar patterns of variation across their channels. Based on this insight, it strategically shifts the quantization complexity from activations to weights through an offline process. Specifically, SmoothQuant [[78]](#b77) introduces a mathematically equivalent per-channel scaling transformation: Y = (Xdiag(s) -1 ) • (diag(s)W) = X Ŵ where X represents Keys or Values, and the smoothing factor s ∈ R Ci is used to scale X. This transformation achieves two key benefits: it smooths the distribution of Keys and Values to facilitate easier quantization, and it allows the smoothing factors to be efficiently incorporated into the parameters of previous layers during offline processing. In particular, the smooth factor s is dynamically decided on based on inputs. Similarly, The OS+ [[79]](#b78) introduces channel-wise shifting to eliminate outlier asymmetry and channel-wise scaling to reduce outlier concentration. These operations are seamlessly migrated to subsequent layers, maintaining equivalence with the floating-point model while improving quantization performance.

Instead of using handcrafted transformations [[77]](#b76), [[78]](#b77), [[79]](#b78) to shift the quantization difficulty from activations to weights, AffineQuant [[80]](#b79) uses an affine transformation matrix that combines both scaling and rotation transformations. This allows it to optimize weight distributions more effectively, aligning them better with the quantization function and reducing quantization errors. The affine transformation matrix provides richer flexibility compared to SmoothQuant's scalar-based scaling, enabling finer adjustments to the weight and activation distributions. Based on AffineQuant [[80]](#b79), FlatQuant [[81]](#b80) introduces a fast and learnable affine transformations to enhance the flatness of weights and activations, which decomposes transformations into smaller matrices to reduce memory and computational costs. Similarly, AWQ [[82]](#b81) and OmniQuant [[83]](#b82) proposes differentiable and learnable equivalent transformations, which optimize the equivalent parameters (e.g., channel-wise scaling and shifting) in an end-to-end manner using gradient descent.

## Summary and Future Directions

KV cache quantization is a crucial technique for reducing memory and computational overhead in large language models (LLMs) during autoregressive decoding. While significant progress has been made, this field remains dynamic and rapidly evolving, with several promising directions for future research. Firstly, one promising avenue is the development of real-time adaptive quantization methods. These techniques could dynamically adjust quantization levels during inference based on real-time metrics such as token importance, outlier presence, or sequence length. Such an approach could significantly enhance efficiency while maintaining performance, especially for processing long sequences with varying levels of complexity. Secondly, another important direction is extending KV cache quantization to multi-modal and multi-task models. Multi-modal models, which process inputs from diverse domains such as text, vision, and audio, and multi-task scenarios often exhibit highly diverse attention patterns and memory demands. This necessitates the design of more advanced and tailored quantization strategies to balance efficiency and accuracy in these increasingly complex settings.

Thirdly, hybrid quantization techniques also hold significant potential. By combining fixed-precision, mixedprecision, and outlier redistribution methods, researchers could develop more versatile and efficient quantization frameworks. For instance, integrating mixed-precision allocation schemes with outlier smoothing transformations could optimize both memory usage and performance, offering a flexible approach adaptable to a variety of tasks and models. finally, addressing the challenge of outliers remains a critical area of focus. Outliers can have a disproportionate impact on quantization efficiency and model performance. Future research could explore advanced outlier detection mechanisms or innovative encoding techniques to mitigate their effects. Improved handling of outliers could further enhance the effectiveness of quantization methods, enabling more robust and memory-efficient implementations.

## KV Cache Low-rank Decomposition

Existing studies have demonstrated that the majority of information within KV caches can be captured by a small subset of their singular values or low-rank components, making low-rank decomposition a powerful tool for compression. By leveraging this property, KV cache low-rank decomposition techniques aim to reduce memory requirements while preserving the essential information required for accurate attention computations. Low-rank decomposition strategies can be classified into three main approaches: Singular Value Decomposition (SVD), which exploits the low-rank structure of KV matrices to retain the most critical singular values; Tensor Decomposition, which factorizes KV matrices into smaller components for minimal redundancy; and Learned Low-rank Approximation, which incorporates adaptive mechanisms to optimize compression based on learned representations. Each method provides a unique balance of computational efficiency and accuracy retention, enabling scalable and memory-efficient LLM inference.

## Singular Value Decomposition

Firstly, ECKVH [[66]](#b65), EigenAttention [[67]](#b66), and ZDC [[68]](#b67) shows that KV caches have a low-rank property, where a small number of top singular values retain most of the information. Using Singular Value Decomposition (SVD), the method compresses KV caches by grouping heads, applying SVD, and retaining top singular values, effectively reducing the number of KV heads with minimal error. Also, ZDC [[68]](#b67) uses an adaptive hybrid compression ratio mechanism to assign higher compression to unimportant tokens in shallower layers while preserving more important tokens in deeper layers, leveraging the similarity of token characteristics in adjacent layers. Secondly, rather than decomposing KV pairs, LoRC [[69]](#b68) employs a low-rank approximation of KV weight matrices and adopts a progressive compression strategy to efficiently compress KV caches without requiring model retraining. Specifically, LoRC [[69]](#b68) uses SVD to compress the Keys and Values parameter matrices (i.e., W k i and

$W v i ) as W k i = U k i Σ k i P k i ⊤ and W v i = U v i Σ v i P v i ⊤$. Also, compression is applied conservatively in shallower layers to minimize error amplification and more aggressively in deeper layers. Then, instead of storing

$K i = X i W k i and V i = X i W v i , it only stores Ki = X i U k i and Vi = X i U v i , along with Σ k i P k i ⊤ and Σ v i P v i ⊤$. Also, ShadowKV [[70]](#b69) performs SVD decomposition directly on pre-RoPE keys to reduce the dimensionality of the key representations. Palu [[71]](#b70) applies SVD to compress both Keys and Values.

## Tensor Decomposition

Tensor decomposition [[168]](#b167), [[169]](#b168), [[170]](#b169) is a widely used algorithm for factorizing a matrix into a sequential product of local tensors, such as Matrix Product Operator (MPO) [[171]](#b170) and turker decomposition [[172]](#b171) . Taking Matrix Product Operator (MPO) [[171]](#b170) as an example, the decomposition of a matrix W ∈ R I×J using MPO can be defined as:

$TD(W) = n k=1 T (k) [d k-1 , i k , j k , d k ],(12)$where

$T (k) represents the local tensor of size d k-1 × i k × j k × d k , with n k=1 i k = I and n k=1 j k = J.$Here, n denotes the number of local tensors, collectively referred to as the decomposed tensors. As shown in Eaquation [(12)](#b11), MPO-based tensor decomposition is well-suited for KV cache compression as it reduces the memory footprint by factorizing large key and value matrices into smaller local tensors, enabling efficient storage while preserving essential information. This approach minimizes redundancy and maintains the structural integrity required for accurate attention computations. DecoQuant [[65]](#b64) combines quantization with low-rank decomposition to effectively reduce quantization errors. Specifically, DecoQuant [[65]](#b64) leverages the Matrix Product Operator (MPO) to decompose matrices into smaller local tensors. The larger tensors, which contain most of the parameters, are quantized to low-bit precision, while the smaller tensors retain high precision to minimize overall quantization error.

## Learned Low-rank Approximation

LESS [[63]](#b62) introduces a novel learned-kernel-based lowrank approximation approach to efficiently approximate the results of the softmax function. Specifically, LESS [[63]](#b62) replaces the softmax with a separable similarity metric, ϕ(q t )ψ(K t ) ⊤ , where ϕ and ψ are row-wise functions. Here, q t ∈ R 1×D represents the query, and K t ∈ R t×D represents the keys at step t. To elaborate, if ϕ and ψ are such that:

$a t = softmax qtK ⊤ t √ D V t ≈ ϕ(qt)ψ(Kt) ⊤ Vt ϕ(qt)ψ(Kt) ⊤ 1 S×1$, then we only need to cache the hidden states [[64]](#b63) compresses KV caches along the feature dimension by leveraging trainable orthogonal projection matrices.

$H t = ψ(K t ) ⊤ V t ∈ R R×D and the normalization factor z t = t s=1 ψ([K t ] s ) ∈ R 1×R for inference. Similarly, MatryoshkaKV$
## Summary and Future Directions

KV cache low-rank decomposition is a powerful technique for compressing KV caches in LLMs while maintaining the quality of attention computations. Current methods primarily rely on fixed low-rank approximations applied uniformly across all layers or tokens. However, future advancements could focus on dynamic rank adjustment, where the rank is tailored based on token importance, sequence length, or layer-specific properties, enabling a more optimal balance between memory efficiency and performance. Additionally, real-time or streaming applications present a promising avenue for exploration. Since KV caches grow dynamically during inference, lightweight and incremental decomposition methods that can adapt efficiently to expanding sequences will be critical for supporting such scenarios without compromising latency or accuracy.

## MODEL-LEVEL OPTIMIZATION

In model-level optimization, new architectures or mechanisms are designed for transformers to allow more efficient reuse of KV cache. Typically, these methods require retraining or fine-tuning of the model to come into operation. Nevertheless, efficient transformation pipelines have also been proposed to allow for a fast deployment to new architectures. According to where and how the refinement was MixCon [[173]](#b172), GoldFinch [[174]](#b173), Recur-Former [[175]](#b174) Adaptive Sequence Processing Architecture (Sec. 5.3.1)

RWKV [[176]](#b175), Mamba [[177]](#b176), RetNet [[178]](#b177), MCSD [[179]](#b178) Architecture Alteration (Sec. 5.2) Augmented Architecture (Sec. 5.2.2) YOCO [[180]](#b179), CEPE [[181]](#b180), XC-Cache [[182]](#b181), Block Transformer [[183]](#b182) Enhanced Attention (Sec. 5.2.1) MLA [[27]](#b26), FLASH [[184]](#b183), Infini-Attention [[185]](#b184) Attention Grouping and Sharing (Sec. 5.1)

Cross-Layer Sharing (Sec. 5.1.2) CLA [[186]](#b185), LCKV [[187]](#b186), SA [[188]](#b187), MLKV [[189]](#b188), LISA [[190]](#b189), Wu et al. [[191]](#b190), CLLA [[192]](#b191), DHA [[193]](#b192), SV-Former [[194]](#b193) Intra-Layer Grouping (Sec. 5.1.1)

MQA [[195]](#b194), GQA [[196]](#b195), AsymGQA [[197]](#b196), Weighted GQA [[198]](#b197), QCQA [[199]](#b198), KDGQA [[200]](#b199), GQKVA [[201]](#b200) Fig. [5](#). Taxonomy of the model based KV optimization for Large Language Models.

made to the models, we separate related works to the grouping and sharing mechanisms within or cross layers (Sec. 5.1), implementing architecture modification or augmentation (Sec. 5.2), and incorporating non-transformer architectures for optimization (Sec. 5.3). The taxonomy of the model-level optimization is shown in Fig. [5](#).

## Attention Grouping and Sharing

This section explores attention grouping and sharing methods as effective strategies for optimizing key-value (KV) management. We categorize the approaches into two distinct subtypes: intra-layer grouping (Sec. 5.1.1) that focuses on grouping query, key, and value heads within individual layers to reduce redundancy and improve efficiency, and cross-layer sharing 5.1.2 that shares key, value, or attention components across layers to improve information reuse and reduce KV cache requirements. The summary of attention grouping and sharing is listed in Tab. 7.

## Intra-layer Grouping

Shazeer first introduced Multi-Query Attention (MQA) [[195]](#b194) that modified the traditional multi-head attention mechanism. In MQA, all attention heads in a transformer block share a single key and value. This simple strategy can greatly accelerate the decoding procedure. The experiments of the author show that MQA would gain much efficiency with only minor quality degradation incurring. MQA is a radical strategy that would cause not just quality degradation, but also training instability. GQA (Grouped Query Attention) [[196]](#b195) introduced a trade-off solution by dividing the query heads into multiple groups, while each group shares its own keys and values. In addition, an uptraining process is proposed to efficiently convert existing MHA models to GQA configurations by mean-pooling the key and value heads associated with each group. Empirical evaluations demonstrated that GQA models achieve performance close to the original MHA models while offering inference time comparable to MQA.

There were several extensions based on GQA. AsymGQA [[197]](#b196) extends GQA by proposing an activationinformed merging strategy. Instead of grouping the heads by uniform clustering, AsymGQA dynamically determines the grouping of quries based on their activations similarities during training and constructs an asymmetric group results, which leads to better optimization and generalization. Weighted GQA [[198]](#b197) introduces additional trainable weights to each key and value head, which can be seamlessly integrated into existing GQA models. By tuning weights during training, it improves the performance of the model without additional inference overhead. QCQA [[199]](#b198) utilizes an evolutionary algorithm to identify the optimal query head groupings for GQA, which is guided by a computationally efficient fitness function that leverages the weight-sharing error and the KV cache to evaluate text generation quality and memory capacity. KDGQA [[200]](#b199) argues that many variances of GQA adopt a fixed grouping strategy, thus lacking dynamic adaptability to the evolving of key-value interactions during training. Their Dynamic Key-Driven GQA address these issues by allocating groups using key head norms adaptively during training, resulting in a flexible strategy to query head grouping and enhance the performance.

GQKVA [[201]](#b200) advances the grouping strategy and comes up with a generalized query, key and value grouping mechanism. It first introduces MKVA and GKVA, in which the key and value are grouped to share the same query. Based on this, GQKVA is proposed to separately group the query and key-value pairs. Typically, queries are partitioned into g q groups, and keys and values are partitioned into g kv groups, and each combination of query and key-value pairs would interact using dot product attention. This results in g q × g kv distinct outputs. It generalized different group strategy on query, key and value and preserves good computational efficiency and comparable performance as MHA.

## TABLE 7

The summary of Model-based Attention Grouping and Sharing approaches.

## Method Applied Location Intra-layer Grouped Component

Cross-layer Shared Component Retraining Required Intra-layer Cross-layer MQA [[195]](#b194) ✓ K, V -✓ GQA [[196]](#b195) ✓ K, V -Uptrain AsymGQA [[197]](#b196) ✓ K,V -Finetune Weighted GQA [[198]](#b197) ✓ K,V -Uptrain & Finetune QCQA [[199]](#b198) ✓

$K, V - ✓ KDGQA [200] ✓ K, V - ✓ GQKVA [201] ✓ Q, K, V - ✓ CLA [186] ✓ ✓ K, V K, V ✓ LCKV [187] ✓ - K, V ✓ SA [188] ✓ - Attention Weight ✓ MLKV [189] ✓ ✓ K, V K, V Uptrain LISA [190] ✓ Q, K, V Lightweight adaption Wu et al. [191] ✓ - Q, K, V ✓ CLLA [192] ✓ - Q, K, V ✓ DHA [193] ✓ ✓ K, V Q, K, V$Lightweight adaption SVFormer [[194]](#b193) ✓ -V ✓

## Cross-layer Sharing

Brandon et al. introduce Cross Layer Attention (CLA) [[186]](#b185) that extends the ideas of GQA and MQA by sharing the key and value heads between adjacent layers, further reduce the redundancy in the KV cache. This achieves an additional 2× KV cache size reduction compared to MQA, significantly improving memory efficiency without altering computational complexity.

LCKV [[187]](#b186) proposes only to compute and cache the key and value for a small subset of layers, even only the top layer, then let queries in bottom layers pair the saved keys and values for inference. This method not only drastically improves the inference speed and reduces memory consumption but is also orthogonal to existing memorysaving techniques, enabling straightforward integration for further optimization. While such a mechanism makes next token computation depend on top layer keys and values of previous tokens, which contradict to the parallel training of transformers, LCKV introduces an approximate training methods to support parallel training.

SA (Shared Attention) [[188]](#b187) proposes reuse of computed attention weights across multiple layers, rather than recalculating them for each layer. Unlike other methods focusing on sharing key-value caches, SA leverages the isotropic tendencies of attention distributions observed in pre-trained LLMs to directly share attention weights, greatly reducing both computational overhead and memory usage.

MLKV (Multi-Layer Key-Value) [[189]](#b188) introduces a simple KV head sharing mechanism across multiple transformer layers. MLKV uses the same single KV head as MQA within a layer, but it also shares this KV head with multiple layers. This extreme strategy reduces the cache size to almost 1% of normal GQA strategies, and experiments show that MLKV still has comparable performance.

LISA (Lightweight Substitute for Attention) [[190]](#b189) makes a comprehensive analysis for the similarity of attention patterns across layers. Directly sharing attention weights across layers is ineffective because of the misalignment of the attention head and the sensitivity of shallow layers. LISA [[190]](#b189) addresses challenges by incorporating tiny feedforward networks to align attention heads between layers and using low-rank matrices to approximate variations in layer-wise attention weights. This achieves a 6× compression of query and key parameters while maintaining high accuracy and perplexity.

Wu et al. [[191]](#b190) introduce a unified framework that systematically analyzes and optimizes the cross-layer Key-Value cache sharing mechanism. They consolidate several existing methods, explore novel variants within a cohesive structure, and make thorough evaluations of these methods. The study finds that 2 times reduction to KV cache size can outperform standard transformers in throughput without substantial accuracy loss, while further reduction requires alternative design with additional training costs. With the analysis results, they offer insight into the choice of appropriate KV sharing methods based on the specific requirement or constraints.

CLLA (Cross-Layer Latent Attention) [[192]](#b191) introduces an integrated framework combining multiple strategies: attention head size and dimension reduction, cross-layer cache sharing, and KV cache quantization. By unifying these strategies, CLLA achieves extreme KV cache compression to less than 2% of the original model size while maintaining performance levels comparable with uncompressed models.

DHA (Decoupled Head Attention) [[193]](#b192) addresses redundancy in MHA and adaptively configures shared groups for key and value heads across layers, reducing KV cache requirements. Observing that clustering and fusing similar heads can reduce KV cache size without significant performance reduction, DHA designs a search, fusion, and continued pre-training framework that can progressively transform MHA checkpoints into DHA models through linear fusion of head parameters, preserving the pre-trained knowledge with small pre-training budget. Observing that later layers in traditional transformers overly rely on narrow regions of attention, Zhou et al. [[194]](#b193) introduce ResFormer that utilizes residual connections from the value embeddings of the first layer to all subsequent layers, effectively approximating cross-layer attention without incurring significant computational costs. They then propose a simplified variant SVFormer that shares a single value embedding across all layers, dramatically reducing the KV cache size by nearly half while maintaining competitive performance. The proposed architectures are flexible to incorporate with other KV-efficient strategies for additional memory savings.

## Summary and Future Directions

This section highlights innovative strategies for optimizing memory and computational efficiency through intralayer grouping and cross-layer sharing mechanisms. However, several avenues for improvement remain. First, maintaining performance while optimizing efficiency, especially for precision-sensitive tasks, requires further investigation. Methods that implement radical grouping and sharing mechanisms may compromise the model fidelity for tasks requiring high precision. Second, scalability across diverse model architectures and sizes is essential. Works such as DHA [[193]](#b192) and LISA [[190]](#b189), which rely on specific architectural assumptions, may struggle to generalize to emerging LLMs or non-standard configurations. Third, the dynamics of attention across both time and layers are largely underexplored. Most existing methods rely on static or predetermined grouping and sharing strategies, neglecting the temporal and contextual variations in attention patterns.

To address these challenges and unlock the full potential of attention optimization, future research should focus on the following aspects. First, developing universal frameworks for attention grouping and sharing that require minimal retraining to enhance adaptability and usability. Second, synergistic integration with other optimization techniques, such as quantization and pruning, has significant potential to achieve even greater efficiency gains. While some works like CLLA [[192]](#b191) have begun to address these opportunities, more exploration could be carried out to unlock new levels of efficiency. Third, more dynamic and temporal modeling could be leveraged to adaptively adjust grouping and sharing during runtime to better capture the contextual requirements of different tasks and sequences. Finally, a deeper understanding of the downstream impacts of these techniques on fine-tuning and transfer learning is crucial for their effective application in real-world scenarios.

## Architecture Alteration

This section explores architectural modifications to optimize KV cache usage. We categorize these methods into two subsections: methods that refine the attention mechanism for KV cache efficiency (Sec. 5.2.1), and methods that introduce structural changes for better KV management (5.2.2). Many of these works build upon the broader landscape of efficient attention mechanisms (e.g., Linear Transformer [[202]](#b201), Performer [[203]](#b202), LinFormer [[204]](#b203), etc.). Since our focus lies on methods directly impacting KV cache handling, for a comprehensive overview of efficient attention mechanisms, we refer readers to dedicated surveys [[45]](#b44). The summary of architecture alteration for KV reuse is listed in Tab. 8.

## Enhanced Attention

DeepSeek-V2 [[27]](#b26) introduced Multi-Head Latent Attention (MLA) that adopts a low-rank KV joint compression mechanism, replacing the full KV cache with compressed latent vectors. The model adopts trainable projection and expansion matrices to do the compression. This compression mechanism significantly reduces the memory requirement of the KV cache and allows the model to handle sequences up to 128K tokens.

FLASH [[184]](#b183) incorporates the Gated Attention Unit (GAU) to replace the MHA mechanism in traditional transformers. GAU utilizes a single-head attention mechanism with gating functions that selectively modulates importance in information flow. FLASH employs a linear approximation method for attention computation through GAU module, which makes the model efficiently handle long contexts without the quadratic scaling of traditional self-attention, thus mitigating heavy KV cache issues.

Infini-Attention [[185]](#b184) adopts representation compression to store long-term content. Furthermore, they introduce a hybrid attention mechanism of masked local attention and long-term linear attention. The masked local attention replaces the standard MHA to let the model only concentrate on local contexts, while the long-term linear attention utilizes compressed memory for far-reaching dependencies and uses linear attention for efficient aggregation. Thus, TABLE [9](#) The summary of Non-Transformer Architectures.

## Method Key Mechanism No Traditional KV Cache KV Cache Compression

RWKV [[176]](#b175) RNN-like with Transformer parallelism ✓ Mamba [[177]](#b176) Selective state-space model ✓ RetNet [[178]](#b177) Retention mechanism ✓ MCSD [[179]](#b178) Slope-decay fusion ✓ MixCon [[173]](#b172) Transformer + Conba + MoE ✓ GoldFinch [[174]](#b173) RWKV + Modified Transformer ✓ RecurFormer [[175]](#b174) Mamba replacing some attention heads ✓ infini-attention combines both local fine-grained and longrange compressed states, allowing a seamless balance between long-term and short-term context modeling.

## Augmented

Architecture YOCO [[180]](#b179) builds a decoder-decoder architecture composed of two modules: a self-decoder and a cross-decoder. The self-decoder efficiently encodes global key-value caches, while the cross-decoder reuses these caches via crossattention. This design ensures that key-value pairs are only cached once, substantially reducing GPU memory usage while maintaining global attention capabilities. YOCO's computation flow also enables the prefilling to early exit, allowing faster prefill stages without altering the final output. CEPE [[181]](#b180) interleaves additional cross-attention layers between the self-attention and feed-forward layers in the decoder model. It employs a small encoder to process long inputs chunk-by-chunk to encoded representations as crossattention layers' inputs. In this way, CEPE can prevent the needs for KV cache for every token and reduce computational cost by processing contexts in parallel. This also facilitates an existing LLMs to expand its contexts while preserving the scalability and generalizability.

XC-Cache [[182]](#b181) also utilizes an encoder to interleave cross-attention layers within existing self-attention layers in pre-trained decoder-only models to prevent explicit prompt caching. The encoder processes the context and converts it into a compact set of key-value pairs that summarize the essential information. It also finds that pre-trained causal decoders can be used to replace an encoder for representations extraction, further reducing the training costs on additional encoder.

Block Transformer [[183]](#b182) introduces a hierarchical globalto-local architecture by combining coarse-grained global attention and fine-grained local attention. In lower layers, tokens are grouped into fixed-size blocks, allowing global context modeling with reduced KV cache overhead. In upper layers, attention operates within individual blocks, enabling lightweight, detailed token decoding with a smaller local KV cache.

## Summary and Future Directions

This section explores research that introduces novel attention mechanisms or architectural modifications to improve KV cache management. Although these approaches demonstrate significant progress in enabling longer context windows and faster inference, several challenges remain. First, many methods, such as CEPE [[181]](#b180) and XC-Cache [[182]](#b181) demonstrate strong performance on retrieval-augmented tasks but may not generalize well across diverse workloads. This necessitates further research into task-adaptive KV cache optimization strategies that dynamically adjust caching behavior to optimize for different task demands. Secondly, integrating these novel mechanisms into existing pretrained models often requires extensive retraining, hindering their adoption in resource-constrained environments. Developing lightweight, modular approaches for retrofitting efficient KV caching into existing architectures is crucial for a wider practical impact. Finally, the robustness and stability of these new mechanisms under real-world conditions, such as noisy or dynamically changing inputs, require further investigation. Addressing these limitations could improve reliability and efficiency in practical deployments.

## Non-Transformer Architecture

While transformers are struggling with KV cache issues, researchers have revisited principles from traditional sequential architectures, such as recurrent neural networks (RNNs) [[205]](#b204), which inherently process sequences without the need for explicit KV caches. Inspired by the lightweight and memory-efficient design of RNNs and efficient attention mechanisms, non-transformer architectures [[176]](#b175), [[177]](#b176), [[206]](#b205), [[207]](#b206), [[208]](#b207), [[209]](#b208) have emerged, such as Mamba [[177]](#b176) and RWKV [[176]](#b175), offering promising alternatives. While there are a large type of new architectures, we only list methods associated with KV optimization. For further understanding to efficient non-transformer works, please refer to these surveys [[45]](#b44), [[210]](#b209), [[211]](#b210), [[212]](#b211). The summary of non-transformer is listed in Tab. 9.

## Adaptive Sequence Processing Architectures

RWKV [[176]](#b175), which means Receptance Weighted Key Value, is an architecture that combines the strengths of RNNs and transformers to achieve efficient sequence processing. RWKV integrates a linear attention mechanism, enabling parallelizable training like transformers while retaining the efficient inference characteristics of RNNs. By formulating the architecture to operate as either a transformer or an RNN, RWKV achieves constant computational and memory complexity during inference, overcoming the quadratic scaling issues of transformers.

Mamba [[177]](#b176) is built based on state space sequence models (SSMs) [[213]](#b212), [[214]](#b213). Inspired by the state space systems, SSMs build scalable and memory-efficient long-range sequence modeling frameworks. Mamba improves SSMs by making parameters input-dependent, allowing information to be selectively propagated or forgotten along the sequence based on the current token. This addresses the inability of traditional SSMs to effectively handle the complexity of nonlinear dependencies in natural languages. Mamba omits attention and even MLP blocks, relying entirely on these selective state spaces for sequence modeling. It also develops a hardware-aware parallel algorithm for efficient recurrent computations in training and inference. Mamba achieves linear scaling in sequence length, demonstrating exceptional performance on sequences of up to a million tokens.

RetNet [[178]](#b177) introduces Retentive Network that combines elements of recurrence and attention, presenting a novel retention mechanism for sequence modeling that offers training parallelism, low-cost inference, and scalable performance together. The proposed Multi-scale Retention Module (MSR) enables support to multiple computation paradigms: the parallel representation is similar to selfattention that adds support to casual masks and parallel training. The recurrent representation is similar to RNN that allows low-cost inference by maintaining state across sequence decoding. The chunkwise recurrent representation constructs a hybrid form to the former representations to further enables handling long sequences. These combined characteristics position RetNet as a strong alternative to transformers without a heavy KV cache mechanism.

MCSD [[179]](#b178) features the new block called Multi-Channel Slope and Decay, which is made up of two sections: The slope section can capture local features across short temporal spans, and the decay section can capture global features across long temporal spans. The sections are fused through element-wise operations. During inference, the process would be reformat into a recurrent representation, allowing both spatial and temporal efficiency, minimizing the need for maintaining a large KV cache.

## Hybrid Architecture

With these non-transformer architecture, some methods construct mixed models to alleviate KV cache necessities while keeping some peculiarities and merits of the selfattention mechanism.

MixCon [[173]](#b172) introduces a new architecture called Conba. Inspired by control theory, the Conba layer incorperates a feedback and adaptive control mechanism that can adapt to different sequence-modeling tasks and requirements dynamically with good computational efficiency. Furthermore, MixCon integrates the Mixture of Experts (MoE) module, which dynamically selects the most relevant experts to process parts of the sequence. Combining the transformer layer, the Conba layer, and the MoE module, MixCon constructs a hybrid model with good balance between attention effectiveness and computational efficiency and significantly reduces the total size of the KV cache.

GoldFinch [[174]](#b173) first introduces several new architectures, including the GOLD layer, which combines the Llama and RWKV channel mixer with several improvements, and the enhanced Finch model (RWKV-6) that has significantly reduced parameters without sacrificing efficiency and performance. GoldFinch also proposes a novel mechanism called TokenCat to produce a highly compressed global key cache using the output of Finch layers. GoldFinch builds a hybrid architecture that constructs the key cache in the early layers and consumes the key cache to produce output without the traditional value cache in the top layers, providing a compact and reusable cache pipeline with linear scaling.

RecurFormer [[175]](#b174) argues that not all transformer heads need to participate in the self-attention mechanism. The work recognizes that certain attention heads show recencyaware behavior which focus on local and short-range dependencies, dissipate the computation resource but gives little contribution. After identifying these heads, RecurFormer replaces them with the Mamba components, achieving straightforward KV cache reduction.

## Summary and Future Directions

By exploring non-transformer modules such as recurrent and hybrid designs, these methods have introduced novel paradigms that balance performance with computational efficiency, and also alleviate the KV cache issues in traditional transformer architectures. Future research should focus on several key areas. First, improving the scalability of recurrent architectures, such as RWKV [[176]](#b175) and Mamba [[177]](#b176), remains critical. Although these methods reduce memory and computational costs, their performance in capturing ultralong-range dependencies lags behind transformers. Second, hybrid designs such as MixCon [[173]](#b172) and GoldFinch [[174]](#b173) highlight the potential of integrating diverse modules, yet their complexity introduces challenges in training stability and interpretability. Third, the overall generalization capabilities and robustness of non-transformer architectures, while efficient, need require further exploration for diverse input modalities.

## SYSTEM-LEVEL OPTIMIZATION

Recent system-level optimizations for KV cache in LLM inference can be broadly categorized into three main directions: memory management (Sec. 6.1), scheduling strategies (Sec. 6.2), and hardware-aware designs (Sec. 6.3). These complementary approaches collectively demonstrate the rich design space for system-level optimizations in LLM inference, each addressing different aspects of the performance, efficiency, and resource utilization challenges. The Taxonomy of the system-level optimization is in Fig. [6](#).

## Memory Management

Recent advances in KV cache memory management for large language model (LLM) inference reveal three distinct approaches aimed at enhancing memory efficiency. Architectural designs, exemplified by vLLM with PagedAttention [[144]](#b143) and vTensor [[218]](#b217), adapt classical operating system principles to create flexible, dynamic memory allocation systems that optimize the use of physical memory through sophisticated mapping and virtual memory abstractions. Prefix-aware designs like ChunkAttention [[238]](#b237) and MemServe [[239]](#b238) further refine this approach by organizing data structures to enable efficient cache deduplication and sharing of common prefixes, thereby improving both memory utilization and computational efficiency. Together, these innovations illustrate the potential for significant enhancements in LLM serving via memory management. FlexGen [[96]](#b95), InstInfer [[215]](#b214) Heterogeneous Design (Sec. 6.3.3) NEO [[216]](#b215), FastDecode [[217]](#b216), FlexInfer [[218]](#b217), InfiniGen [[139]](#b138), Pensieve [[219]](#b218), FastServe [[220]](#b219), PartKVRec [[221]](#b220) I/O-based Design (Sec. 6.3.2) FlashAttention [[145]](#b144), Bifurcated Attention [[222]](#b221), PartKVRec [[221]](#b220), HCache [[223]](#b222), Cake [[224]](#b223), FastSwitch [[225]](#b224) Single/Multi-GPU Design (Sec. 6.3.1) HydraGen [[226]](#b225), DeFT [[227]](#b226), vLLM [[144]](#b143), ORCA [[228]](#b227), Dist-Serve [[229]](#b228), Multi-Bin Batching [[230]](#b229), Tree Attention [[231]](#b230) Scheduling (Sec. 6.2) Layer-specific and Hierarchical Scheduling (Sec. 6.2.3) LayerKV [[232]](#b231), CachedAttention [[233]](#b232), ALISA [[234]](#b233), LAMPS [[235]](#b234) Preemptive and Fairness-oriented Scheduling (Sec. 6.2.2) FastServe [[220]](#b219), FastSwitch [[225]](#b224) Prefix-aware Scheduling (Sec. 6.2.1) BatchLLM [[236]](#b235), RadixAttention [[237]](#b236) Memory Management (Sec. 6.1) Prefix-aware Design (Sec. 6.1.2) ChunkAttention [[238]](#b237), Mem-Serve [[239]](#b238) Architectural Design (Sec. 6.1.1) vLLM [[144]](#b143), vTensor [[218]](#b217), LeanKV [[112]](#b111) Fig. [6](#). Taxonomy of the System-level Optimization for KV Cache Management.

## Architectural Design

The first category focuses on architectural innovations in memory management, led by vLLM with PagedAttention [[144]](#b143), which adapts OS-inspired paging concepts by partitioning KV caches into fixed-size blocks with noncontiguous storage. PagedAttention partitions KV caches into fixed-size blocks that can be stored non-contiguously in physical memory, while vLLM [[144]](#b143) implements a virtual memory-like system that manages these blocks through a sophisticated mapping mechanism. This architecture separates logical and physical KV blocks, enabling dynamic memory allocation and flexible block management through block tables that track mapping relationships and fill states. This memory management approach enables efficient memory utilization both within and across requests, demonstrating how classical OS memory management principles can be effectively adapted for LLM inference optimization.

This approach is further enhanced by vTensor [[218]](#b217), which introduces a virtual memory abstraction that decouples computation from defragmentation through three key components: the vTensor Scheduler which generates memory management policies based on meta information, the vTensor Operation which translates these policies into CUDA VMM operations, and the vTensor Pool which maintains virtual tensor mappings. VTS processes instructions and creates policies based on memory state tracking, while VTO executes these policies through asynchronous GPU operations. VTP completes the cycle by managing virtual tensor storage and updating meta information for subsequent memory operations.

LeanKV [[112]](#b111) combines unified paging with heterogeneous quantization and dynamic sparsity mechanisms. It implements Hetero-KV quantization to store keys and values at different precisions, complemented by a per-head dynamic sparsity mechanism that adapts memory allocation based on token importance across different attention heads and requests. To efficiently execute these strategies, LeanKV [[112]](#b111) introduces an advanced on-GPU memory management system featuring three key components: unified paging for flexible memory organization, a circular free page list for efficient coordination, and a bidirectional page table for minimal metadata overhead.

## Prefix-aware Design

Some latest works emphasize optimizing data organization structures through prefix-aware designs. ChunkAttention [[238]](#b237) restructures KV cache management by organizing chunks within a prefix tree structure, enabling runtime detection and sharing of common prefixes. It breaks down traditional monolithic KV cache tensors into smaller, manageable chunks organized within a prefix tree structure, enabling efficient runtime detection and sharing of common prefixes across multiple requests. This architectural design brings two significant memory management benefits: efficient KV cache deduplication through prefix tree-based organization, and improved data locality through a two-phase partition algorithm for self-attention computation. By enabling dynamic identification and sharing of common prompt prefixes across multiple requests, ChunkAttention [[238]](#b237) optimizes both memory utilization and computational efficiency, demonstrating how intelligent chunking and prefix-aware cache management can significantly enhance LLM serving efficiency.

MemServe [[239]](#b238) extends this concept to distributed settings with its MemPool system, which orchestrates both CPU DRAM and GPU HBM resources across serving instances, managing active and historical KV caches through a comprehensive set of distributed memory pool APIs. It presents a prompt token-based indexing layer for historical KV cache retrieval, cross-instance data exchange mechanisms that abstract away hardware heterogeneity, and a global scheduler implementing a prompt tree-based localityaware policy for enhanced cache reuse, collectively resulting in significant improvements in job completion time and time-to-first-token performance.

These approaches often complement each other, suggesting potential benefits in combining multiple strategies. For instance, LeanKV [[112]](#b111)'s integration of compression with page-based management and MemServe [[239]](#b238)'s combination of distributed memory management with prefix-aware caching demonstrate the effectiveness of hybrid approaches. The diversity of these solutions reflects both the complexity of KV cache management and the rich opportunity space for continued innovation in optimizing LLM inference systems. Tab.10 provides a comparison of various memory management techniques for KV Cache, highlighting key features such as paged memory, virtual memory, dynamic sparsity, prefix sharing, and distributed memory.

## Summary and Future Directions

The exploration of memory management strategies for KV caches in large language model inference reveals a promising landscape of innovations that enhance memory efficiency and overall system performance. Architectural advancements, such as those seen in vLLM [[144]](#b143) and LeanKV [[112]](#b111), adapt traditional memory management principles for modern AI applications by incorporating paging and virtual memory concepts for dynamic allocation. Prefix-aware designs like ChunkAttention [[238]](#b237) and MemServe [[239]](#b238) optimize data organization, enabling the detection and sharing of common prefixes, which reduces redundancy and speeds up inference.

Future work should advance memory management innovations through multiple synergistic directions: investigating adaptive memory hierarchies that dynamically adjust to workload patterns and resource constraints, exploring novel compression techniques that preserve quick access while reducing memory footprint, developing intelligent prefetching mechanisms that anticipate and preload frequently accessed cache entries, researching hardware-aware optimization strategies that leverage emerging memory technologies like computational storage and processingin-memory units, and designing distributed cache coherence protocols that efficiently maintain consistency across multiple inference nodes. Additionally, the exploration of machine learning-based approaches could enable predictive memory allocation that learns from historical access ✓ LeanKV [[112]](#b111) ✓ ✓ ChunkAttention [[238]](#b237) ✓

$MemServe [239] ✓ ✓$patterns, while the investigation of specialized data structures could yield more efficient prefix detection and sharing mechanisms. These advancements, combined with research into heterogeneous memory systems that intelligently coordinate different memory types based on access patterns and performance requirements, would significantly enhance the scalability and efficiency of LLM inference systems across diverse deployment scenarios.

## Scheduling

Based on these scheduling-oriented works, we can categorize KV cache scheduling optimizations into three main approaches: 1) prefix-aware scheduling strategies, represented by BatchLLM [[236]](#b235) and RadixAttention [[237]](#b236); 2) preemptive and fairness-oriented scheduling, exemplified by FastServe [[220]](#b219) and FastSwitch [[225]](#b224); 3) layer-specific and hierarchical scheduling approaches, demonstrated by LayerKV [[232]](#b231), CachedAttention [[233]](#b232), and ALISA [[234]](#b233). These approaches collectively address different aspects of scheduling optimization, from memory efficiency to fairness and latency reduction, while specialized solutions like LAMPS [[235]](#b234) extend these concepts to specific use cases such as API-augmented LLM requests, demonstrating the rich design space in KV cache scheduling optimization.

## Prefix-aware Scheduling

Unlike traditional LRU-based cache management systems where shared KV contexts might be prematurely evicted or unnecessarily extended in memory, BatchLLM [[236]](#b235) implements explicit global prefix identification and coordinated scheduling of requests sharing common KV cache content.

It schedules requests at the granularity of prefix-sharing groups, ensuring optimal KV cache reuse while minimizing cache lifetime -requests with identical prefixes are deliberately scheduled together to maximize KV cache sharing efficiency. This scheduling approach is complemented by a dynamic programming algorithm that optimizes first-level prefix patterns, enabling more efficient KV cache management and reducing scheduling overhead. RadixAttention [[237]](#b236) builds around a radix tree structure, replacing traditional FCFS scheduling with an intelligent cache-aware approach that prioritizes requests based on matched prefix lengths. It implements dynamic memory management where cached tokens and running requests share the same memory pool, controlled by an LRU eviction policy that strategically removes leaf nodes while preserving valuable ancestor prefixes. This is complemented by a

TABLE 11 Comparison of Scheduling Approaches for KV Cache Optimization.

## Method Prefix-aware Preemptive Fairness-oriented Layer-specific Hierarchical Dynamic

BatchLLM [[236]](#b235) ✓ RadixAttention [[237]](#b236) ✓ ✓ FastServe [[220]](#b219) ✓ ✓ FastSwitch [[225]](#b224) ✓ ✓ LayerKV [[232]](#b231) ✓ CachedAttention [[233]](#b232) ✓ ✓ ALISA [[234]](#b233) ✓ ✓ LAMPS [[235]](#b234) ✓ ✓ reference counting mechanism that prevents eviction of actively used cache entries during continuous batching while enabling efficient memory reclamation when nodes become unused.

## Preemptive and Fairness-oriented scheduling

FastServe [[220]](#b219) implements a proactive KV cache management strategy that coordinates cache movement between GPU and host memory, overlapping data transmission with computation to minimize latency impact. This is integrated with a skip-join Multi-Level Feedback Queue scheduler that makes KV cache scheduling decisions based on input length information, allowing jobs to enter appropriate priority queues directly while avoiding unnecessary demotions through higher-priority queues. By combining token-level preemption with sophisticated KV cache management and intelligent queue placement, FastServe [[220]](#b219) achieves significant performance improvements over traditional run-tocompletion systems like vLLM [[144]](#b143). FastSwitch [[225]](#b224) introduces a fairness-oriented KV cache scheduling system that addresses the overhead challenges of preemptive scheduling in LLM serving. There are three key mechanisms: enhancing I/O utilization through intelligent cache movement scheduling, minimizing GPU idle time during context switches, and eliminating redundant I/O operations in multi-turn conversations. Unlike traditional block-based KV cache memory policies that prioritize memory efficiency at the cost of fragmentation and granularity limitations, FastSwitch [[225]](#b224) implements a balanced approach that maintains efficient memory usage while facilitating smoother context switching. This integrated scheduling approach enables dynamic priority adjustments for fairness while minimizing the performance impact of context switches.

## Layer-specific and Hierarchical Scheduling

LayerKV [[232]](#b231) introduces a novel layer-wise KV cache scheduling approach to address the growing TTFT (Time to First Token) latency challenges in large-context LLM serving. The contribution lies in its fine-grained, layer-specific KV cache block allocation and management strategy, which departs from traditional monolithic cache management approaches. By implementing layer-wise KV block scheduling and offloading mechanisms, LayerKV [[232]](#b231) enables more efficient memory utilization and reduces queuing delays that typically occur when large context windows compete for limited GPU KV cache blocks. It is complemented by an SLO-aware scheduler that optimizes cache allocation decisions based on service level objectives, allowing for dynamic management of memory resources across model layers.

CachedAttention [[233]](#b232) introduces a hierarchical scheduling approach consisting of three-tier strategies: layer-wise pre-loading coordinates KV cache movement across storage hierarchies using scheduler-aware fetching and eviction policies, asynchronous saving overlaps I/O operations with GPU computation, and intelligent cache placement decisions are made based on scheduler hints to ensure frequently accessed KV caches reside in faster memory tiers. It also presents a novel positional encoding decoupling mechanism that prevents KV cache invalidation during context window overflow through effective truncation strategies.

ALISA [[234]](#b233) introduces a dual-level KV cache scheduling framework that combines algorithmic sparsity with system-level optimization. At the algorithm level, the Sparse Window Attention mechanism identifies and prioritizes the most important tokens for attention computation, creating a mixture of global dynamic and local static sparse patterns that significantly reduces KV cache memory requirements. At the system-level, its three-phase token-level dynamic scheduler that manages KV tensor allocation and optimizes the trade-off between caching and recomputation. The scheduler makes dynamic decisions about which tokens to cache in GPU memory versus recompute, based on their importance and system resource constraints.

LAMPS [[235]](#b234) implements a predictive scheduling mechanism that estimates both pre-API outputs and optimal memory handling strategies during API calls, choosing between preserving, discarding, or swapping KV cache content based on predicted memory waste.

## Summary and Future Directions

Tab.11 compares scheduling approaches for KV cache optimization based on their support for prefix-awareness, preemptive scheduling, fairness, layer-specific optimizations, hierarchical structures, and dynamic adaptability. The advancements in scheduling strategies for KV cache management in large language model inference highlight a multifaceted approach to optimizing performance, memory efficiency, and fairness. By categorizing these strategies into prefix-aware, preemptive and fairness-oriented, and layer-specific scheduling, we see diverse methodologies addressing different challenges. For instance, prefix-aware strategies like BatchLLM [[236]](#b235) and RadixAttention [[237]](#b236) enhance cache reuse by intelligently grouping requests based on shared prefixes, minimizing cache lifetime and reducing overhead. Meanwhile, preemptive approaches such as FastServe [[220]](#b219) and FastSwitch [[225]](#b224) implement proactive management techniques that optimize cache movement and scheduling, significantly improving latency and ensuring fairness during context switching. Layer-specific scheduling methods like LayerKV [[232]](#b231), CachedAttention [[233]](#b232), and ALISA [[234]](#b233) further refine cache allocation by implementing fine-grained management strategies tailored to the unique demands of different model layers.

Future work should advance these KV cache scheduling innovations through several interlinked dimensions: developing adaptive hybrid systems that dynamically select optimal scheduling strategies based on real-time workload characteristics, exploring predictive models that anticipate user request patterns to proactively optimize cache allocation, investigating automated parameter tuning mechanisms that adjust scheduling policies across different deployment scenarios, designing context-aware architectures that intelligently balance prefix sharing with fairness requirements, and researching novel cache coherence protocols that efficiently handle distributed inference scenarios. Additionally, the integration of reinforcement learning approaches could enable self-optimizing schedulers that learn from historical usage patterns, while the exploration of hardware-software co-design could yield specialized accelerators that directly support efficient KV cache management operations. These advancements would collectively enhance the robustness, efficiency, and adaptability of LLM inference systems across diverse operational conditions and deployment scales. Finally, considering LLM serving [[240]](#b239), different scheduling and sharing for multiple users and queries may lead to potential privacy leaks. Therefore, privacy protection techniques for LLM serving in multi-user scenarios, such as differential privacy [[241]](#b240), [[242]](#b241), [[243]](#b242), are worth further investigation.

## Hardware-aware Design

Recent hardware-aware optimizations for KV cache management span several key directions based on different hardware architectures and constraints. Single/Multi-GPU designs focus on optimizing memory access patterns, GPU kernel designs for efficient attention computation, and parallel processing with load balancing. IO-based designs optimize data movement across memory hierarchies through asynchronous I/O and intelligent prefetching mechanisms. Heterogeneous designs orchestrate computation and memory allocation across CPU-GPU tiers. SSD-based solutions have evolved from basic offloading approaches to more sophisticated designs, with InstInfer leveraging computational storage drives (CSDs) to perform in-storage attention computation, effectively bypassing PCIe bandwidth limitations. These approaches demonstrate how hardware-aware designs can significantly improve LLM inference efficiency by carefully considering and exploiting the characteristics of different hardware components and their interconnections.

## Single/Multi-GPU Design

Based on these works focusing on GPU-oriented designs, we can categorize the approaches into several key strategies for KV cache optimization. First, shared prefix optimization approaches like HydraGen [[226]](#b225) and DeFT [[227]](#b226) focus on efficient GPU memory utilization through batched prefix computations and tree-structured attention patterns. Rather than maintaining separate KV caches for each sequence with identical prefixes, HydraGen [[226]](#b225) decomposes attention computation to leverage a single shared KV cache for common prefixes across multiple requests. It enables efficient GPU memory utilization through two mechanisms: batched prefix KV cache access across sequences and separate handling of unique suffix KV caches. For DeFT [[227]](#b226), its core contributions are twofold: KV-Guided Grouping, which optimizes GPU memory access patterns by intelligently managing shared prefix KV caches to minimize redundant global-to-shared memory transfers, and Flattened Tree KV Splitting, which ensures balanced workload distribution across GPU compute units while minimizing computational redundancy.

Second, distributed processing frameworks exemplified by vLLM [[144]](#b143) and ORCA [[228]](#b227) optimize multi-GPU scenarios through sophisticated memory management and synchronization mechanisms. vLLM [[144]](#b143) also implements a KV cache manager that coordinates memory allocation across distributed GPU workers in model-parallel deployments, where each GPU handles a subset of attention heads while sharing the same logical-to-physical block mapping. This GPU-aware design enables efficient memory utilization through near-zero fragmentation and flexible KV cache sharing, while supporting Megatron-LM style tensor parallelism where GPUs execute in SPMD fashion with synchronized block-wise matrix operations. The scheduler broadcasts control messages containing input tokens and block tables to GPU workers, allowing them to independently process their assigned attention heads while maintaining memory coherence through all-reduce operations, effectively eliminating redundant memory management synchronization overhead and maximizing GPU utilization across distributed resources.

ORCA [[228]](#b227) distributes model layers across GPUs using both intra-layer and inter-layer parallelism, where each worker process manages multiple GPU-controlling threads and coordinates KV cache access through an Attention KV manager. ORCA's GPU-aware design minimizes CPU-GPU synchronization overhead by separating control message communication from tensor data transfer (via NCCL), allowing each GPU thread to efficiently access KV cache memory using request IDs and token indices.

Third, phase-aware designs like DistServe [[229]](#b228) separate prefill and decoding phases across GPU resources to optimize their distinct memory access patterns. Novel batching strategies are represented by Multi-Bin Batching [[230]](#b229), which focuses on length-aware request grouping for improved GPU utilization, while advanced parallel computation frameworks like Tree Attention [[231]](#b230) introduce sophisticated reduction algorithms for efficient attention computation across multiple GPUs. DistServe [[229]](#b228) recognizes that prefill and decoding phases have distinct KV cache

TABLE 12 Comparison of Hardware-aware Design Approaches for KV Cache Optimization.

## Method Single/Multi-GPU I/O-aware Heterogeneous SSD-based

Bifurcated Attention [[222]](#b221) ✓ Cake [[224]](#b223) ✓ DeFT [[227]](#b226) ✓ DistServe [[229]](#b228) ✓ FastDecode [[217]](#b216) ✓ FastSwitch [[225]](#b224) ✓ FlexGen [[96]](#b95) ✓ FlexInfer [[218]](#b217) ✓ FlashAttention [[145]](#b144) ✓ ✓ HCache [[223]](#b222) ✓ HydraGen [[226]](#b225) ✓ InfiniGen [[139]](#b138) ✓ InstInfer [[215]](#b214) Multi-Bin Batching [[230]](#b229) ✓ NEO [[216]](#b215) ✓ ORCA [[228]](#b227) ✓ PartKVRec [[221]](#b220) ✓ Pensieve [[219]](#b218) ✓ Tree Attention [[231]](#b230) ✓ vLLM [[144]](#b143) ✓ utilization characteristics and memory access patterns: prefill requires intensive computation with growing KV cache sizes for processing input tokens, while decoding maintains a fixed KV cache size for generating output tokens. By physically separating these phases onto different GPUs, DistServe enables optimized GPU memory management and KV cache access patterns specific to each phase, eliminating interference between prefill's bursty memory access patterns and decoding's steady-state KV cache utilization. Multi-Bin Batching [[230]](#b229) introduces a length-aware batching strategy helps minimize GPU idle time and memory fragmentation that typically when processing requests of varying lengths in the same batch, as it ensures that the KV cache memory allocated for each batch is utilized more uniformly across all requests. Tree Attention [[231]](#b230) implements a tree-based reduction algorithm that fundamentally changes how attention values are computed and aggregated across GPUs, enabling more efficient handling of KV cache data through partial reductions that significantly reduce memory bandwidth requirements and peak memory usage. These approaches can collectively demonstrate how hardware-aware designs can significantly improve the LLM efficiency by carefully considering GPU architecture characteristics and memory hierarchy constraints.

## I/O-based Design

Recent I/O-focused optimizations for KV cache management span several key dimensions, targeting different levels of the memory hierarchy. At the GPU level, approaches like FlashAttention [[145]](#b144) and Bifurcated Attention [[222]](#b221) optimize data movement between HBM and SRAM through sophisticated tiling strategies and split attention computations, while CPU-GPU data movement optimizations are addressed by systems like PartKVRec [[221]](#b220), which tackles PCIe bandwidth bottlenecks through hybrid recomputation and transfer strategies, and HCache [[223]](#b222), which optimizes intermediate activation storage and restoration.

FlashAttention [[145]](#b144) employs a tiling strategy that carefully manages KV cache access patterns, reducing redundant memory operations by keeping frequently accessed portions of the KV cache in fast SRAM while systematically fetching and evicting data blocks to minimize HBM accesses. Bifurcated Attention [[222]](#b221) presents an I/O-aware approach to optimize KV cache access patterns during sharedcontext batch decoding by strategically splitting attention computations into two distinct GEMM operations. It specifically targets the memory bandwidth bottleneck in highbatch scenarios with long contexts by minimizing repeated KV cache accesses, maintaining the same computational FLOPs while drastically reducing memory I/O operations. For PartKVRec [[221]](#b220), its key innovation lies in its hybrid strategy of partial KV cache recomputation on the GPU while simultaneously transferring the remaining cache data from CPU memory, effectively hiding PCIe transfer latency. The implementation employs a sophisticated I/O-aware scheduling system that analyzes input characteristics and hardware capabilities to determine the optimal balance between recomputation and data transfer, dynamically managing KV cache movement to maximize PCIe bandwidth utilization while minimizing GPU idle time. HCache [[223]](#b222) strategically stores and restores intermediate activations instead of complete KV cache states, implementing a bubblefree restoration scheduler that carefully balances computation and I/O operations to maximize bandwidth utilization. A key innovation is its chunk-based storage manager that addresses the I/O pattern mismatch between saving (layerbefore-token) and restoration (token-before-layer) operations, optimizing data layout and access patterns to reduce I/O overhead. Cake [[224]](#b223) addresses the fundamental I/O bottleneck in loading cached KV states from disk to GPU memory. It introduces a bidirectional parallelized strategy that simultaneously leverages both computational and I/O resources. This hybrid approach dynamically balances between loading cached KV states from storage and computing them on GPUs, adapting automatically to varying system conditions without manual parameter tuning.

Context management optimizations are exemplified by FastSwitch [[225]](#b224), which implements efficient context switching mechanisms for multi-user scenarios through granular memory management policies. FastSwitch [[225]](#b224) addresses I/O inefficiencies in traditional block-based KV cache approaches by implementing a more granular and continuous memory management policy that minimizes I/O overhead during preemption and context switching.

These approaches demonstrate how careful consideration of I/O patterns and memory hierarchy characteristics can significantly improve LLM inference efficiency by minimizing movement and maximizing bandwidth utilization across different storage tiers.

## Heterogeneous Design

Recent heterogeneous computing approaches for KV Cache demonstrate diverse strategies for optimizing CPU-GPU collaboration. Systems like NEO [[216]](#b215) and FastDecode [[217]](#b216) implement strategic workload distribution through CPU offloading of attention computations, while FlexInfer [[218]](#b217) introduces virtual memory abstractions for optimal resource coordination.

NEO [[216]](#b215) advances heterogeneous computing for LLM inference by implementing strategic CPU offloading of attention computations and KV cache states. Through asymmetric GPU-CPU pipelining and load-aware scheduling, it optimally balances workloads across both computing platforms, enabling larger GPU batch sizes without latency penalties. For FastDecode [[217]](#b216), its key contribution lies in its strategic offloading of memory-bound KV cache operations to distributed CPU resources, leveraging the aggregate memory capacity and computing power of multiple CPU nodes rather than treating CPUs as mere storage devices. By utilizing CPUs for KV cache computations and storage while keeping compute-intensive operations on GPUs, it creates an efficient pipeline that maximizes resource utilization across the heterogeneous infrastructure, enabling larger batch sizes and higher throughput. FlexInfer [[218]](#b217) orchestrates CPU-GPU resource utilization for LLM inference by introducing the virtual memory-based abstraction vTensor.

Advanced caching and prefetching mechanisms are exemplified by InfiniGen [[139]](#b138), which employs speculative prefetching for KV cache entries, and Pensieve [[219]](#b218), which implements multi-tier caching for conversation states. For InfiniGen [[139]](#b138), its key innovation lies in its prediction mechanism that operates across the heterogeneous architecture, using partial computation of attention inputs and modified query-key weights to identify and prefetch only the most relevant KV cache entries from CPU memory to GPU. Pensieve [[219]](#b218) introduces a heterogeneous computing architecture specifically designed for multi-turn conversation LLM serving by implementing a sophisticated multi-tier caching strategy across GPU and CPU resources. This stateful approach manages KV cache data across the heterogeneous memory hierarchy, maintaining conversation history states across multiple hardware tiers rather than recomputing them for each interaction.

Sophisticated scheduling and preemption strategies are demonstrated by FastServe [[220]](#b219), which focuses on tokenlevel preemption and proactive memory management, and PartKVRec [[221]](#b220), which balances data transfer and recomputation through dynamic scheduling. For FastServe [[220]](#b219), its token-level preemption capability is supported by a sophisticated heterogeneous memory management system that proactively coordinates KV cache data movement between GPU and host memory. It implements a skip-join Multi-Level Feedback Queue scheduler that manages computational resources across the CPU-GPU boundary, optimizing both computation scheduling and data movement. PartKVRec [[221]](#b220) employs a scheduler that dynamically optimizes the distribution of tasks across the heterogeneous hardware platform, using a profiler to analyze both hardware capabilities and workload characteristics.

These approaches collectively showcase how heterogeneous architectures can be effectively leveraged to overcome single-device limitations while maintaining efficient resource utilization and minimizing communication overhead between CPU and GPU resources.

## Solid-state Disk (SSD)-based Design

Recent SSD-based approaches for KV cache management demonstrate an evolution in storage utilization strategies, from traditional extension of the memory hierarchy to computational storage innovations. FlexGen [[96]](#b95) introduces an SSD-based approach to KV cache management that extends the memory hierarchy across GPU, CPU memory, and disk storage, optimizing high-throughput LLM inference on resource-constrained hardware through intelligent tensor storage and access pattern optimization determined by linear programming. The system's key innovations include coordinated data placement across all three storage tiers, optimized access patterns to minimize SSD latency impact, aggressive 4-bit compression for both model weights and attention cache, and efficient utilization of SSD storage as a memory hierarchy extension for KV cache management. InstInfer [[215]](#b214) introduces a more revolutionary approach by leveraging computational storage drives (CSDs) to perform attention computations directly within the storage layer, transforming SSDs from passive storage devices into active computational units and utilizing the high internal bandwidth of flash memory channels to bypass traditional PCIe bandwidth limitations.

These approaches demonstrate how storage devices can be effectively integrated into LLM inference systems, either as memory hierarchy extensions or as computational resources, to enable efficient processing of large models and long sequences in resource-constrained environments. Tab.12 compares hardware-aware design approaches for KV cache optimization across four key features: Single/Multi-GPU support, I/O-awareness, heterogeneous computing, and SSD-based design.

## Summary and Future Directions

Recent advancements in hardware-aware designs for KV cache management emphasize optimizing performance based on specific hardware architectures and constraints, demonstrating significant enhancements in large language model inference efficiency. Approaches like HydraGen [[226]](#b225) and vLLM [[144]](#b143) in single and multi-GPU designs focus on efficient memory access patterns and load balancing, while I/O-based strategies such as FlashAttention [[145]](#b144) and PartKVRec [[221]](#b220) tackle data movement bottlenecks through intelligent prefetching and scheduling mechanisms. Additionally, heterogeneous designs exemplified by NEO [[216]](#b215) and FastDecode [[217]](#b216) effectively leverage CPU-GPU collaboration to maximize resource utilization.

Future work should advance this research through multiple interconnected directions: exploring novel architectural designs that combine specialized hardware accelerators with optimized memory hierarchies, investigating hybrid systems that leverage computational storage drives and processing-in-memory capabilities, developing selfadaptive algorithms that dynamically optimize resource allocation based on workload patterns, researching advanced compression techniques that maintain model fidelity while reducing memory requirements, and designing intelligent scheduling mechanisms that efficiently coordinate heterogeneous computing resources including CPUs, GPUs, and custom accelerators. These improvements, working in concert, would enhance both the performance and scalability of LLM inference systems across diverse deployment scenarios, from edge devices to data centers, while maintaining adaptability to emerging hardware innovations and varying computational demands.

## TEXT AND MULTI-MODAL DATASETS

In this section, we introduce the text and multi-modal datasets used to evaluate LLM efficiency.

## Text Dataset

We collect a lot of long-context datasets from state-of-theart benchmark frameworks and various papers, including L-Eval [[244]](#b243), M4LE [[245]](#b244), BAMBOO [[246]](#b245), LongBench [[247]](#b246), LRA [[248]](#b247), SCROLLS [[249]](#b248), ZEROSCROLLS [[250]](#b249), LooGLE [[251]](#b250), LongEval [[252]](#b251), and StreamingEval [[136]](#b135). Specifically, we categorize these datasets into different tasks, including question answering, text summarization, text reasoning, text retrieval, and text generation.

## Question Answering (QA) Task

Dataset for this task usually consist of question-answer pairs, and documents that contains the answer to the question. For a model to run such task, documents and questions are usually used as the model input, while the output can differ greatly. Some datasets' answers are closed-ended, meaning that the model should only output its answer in designated form, typically multiple choice answers, while the open-ended answers take a more free form. According to the number of documents involved in a question-answer pair, we can categorize QA task datasets into single-doc QA(QA-SG) and multiple-doc QA(QA-MT). The detailed statistics of the datasets for question answering are provided in Table [13](#tab_13).

• Qasper [[260]](#b259) consists of 5049 questions based on 1585 papers on NLP. Question is from NLP practitioners that only have read the abstract and title of a paper, then another set of practitioners answer these questions by reading through the whole paper. The supporting evidences is provided correspondingly. Each instance of the dataset consists of a question, an answer, corresponding paper and supporting evidence. Instances built by Long-Bench [[247]](#b246) doesn't require evidence.

• HotpotQA [[261]](#b260) is a typical for a multi-doc QA dataset.

It's built based on Wikipedia, and each instance consists of multiple documents, a question, an answer and supporting facts. Supporting facts is a set of paragraph indexes, annotated manually.

• AltQA [[253]](#b252) is based on google's NQ [[258]](#b257) dataset. The answer are all numerical. The original document is "altered" so that each occurrences of the numerical answer is different from the original document, so as to avoid data contamination from pretraining. This dataset is also used in BAMBOO [[246]](#b245) benchmark. • PaperQA and MeetingQA from BAMBOO [[246]](#b245) benchmark are question answering tasks in the form of multiple-choice. Each instance of the two datasets consists of question , evidence, answer and corresponding content.

• NarrativeQA [[259]](#b258) uses complex narratives that are selfcontained as input documents. Both books and movie scripts are used. For question construction, annotators are only given a story summary, and are asked to write questions based on it. For each story(1572 stories in total), about 30 question-answer pairs are constructed from each summary-story pair. Notably, because of the consistency in story context, the task can be simplified to selecting a correct answer from all answers that relates to the story.

• MultifieldQA [[247]](#b246) is an original dataset from Longbench. Its contents covers scientific papers, legal documents, government reports and google results. The dataset has both Chinese and English version, and each instance consists of context built on documents, and a question-answer pair.

• 2WikiMultihopQA [[263]](#b262) is a multi-document QA dataset built on Wikipedia and Wikidata. WikiData is a Knowledge Graph database, from which the author was able to extract the (subject entity, property, object entity) triple that corresponds to a Wikipidia document. These triples are used as evidences in each QA pair, as a way for model to show its inference process. The dataset consists of 192,606 questions in total.

• Musique [[264]](#b263) is also a multi-document dataset(or multihop dataset, as the paper refers to). Its data is extracted from existing single-hop QA datasets. These single-hop QAs are then composed into multi-hop QA pairs. In addition, Musique add some unanswerable QA pairs in order to further test model's ability. There are 24,814 answerable questions in Musique, and each answerable question corresponds to an unanswerable question.

• DuReader [[265]](#b264) is a multi-document QA dataset, whose data is based on Baidu search results. It consists of 200,000 questions, 1,000,000 documents and 420,000 an- swers. Each instance contains a question, multiple possible answers(also possible to be empty), and multiple documents.

• TriviaQA [[254]](#b253) is a multi-document reading comprehension QA dataset. All QA pairs are from 14 trivia websites, written by trivia enthusiasts. For each QA pair, 6 supporting documents(evidence) are provided, collected from Bing search API as well as Wikipedia. The total number of QA pairs is 95,956, with a total of 662,659 supporting documents, the average length of each document is 2895 words.

• TOEFL(L-Eval) [[244]](#b243) collect lectures from the TOEFL Practice Online as context . Each instance consists of a long input of lectures, multiple instructions(questions) and corresponding answers. • Coursera(L-Eval) [[244]](#b243) is a dataset built on Coursera website. Similar to TOFEL, Each instance consists of a long input of lectures, multiple instructions and corresponding answers.

• SFiction(L-Eval) [[244]](#b243) is based on scientific fictions, in which context real-world principles don't apply. The questions contained in the documents ask the model to answer it based on either contextual information or realworld knowledge, as a way to test model hallucination.

• LongFQA(L-Eval) [[244]](#b243) is an open-ended QA dataset on finance based on earnings call transcripts. • CUAD(L-Eval) [[244]](#b243) is drawn from the CUAD [[257]](#b256) dataset, which use legal contract as its context.

• QuALITY [[262]](#b261) is a multiple-choice single-document QA dataset. It uses science fictions, magazine articles and nonfiction articles as input documents. The question is written by those that have read the full document. Each instance contains a document, a multiple-choice questions and corresponding answers. Notably, part of the questions are unanswerable.

• NewsQA [[245]](#b244) and DuoRC [[245]](#b244) are English QA datasets, constructed from news and movie plots, respectively. • C3 [[245]](#b244) is a multiple-choice QA dataset, based on second- language Chinese exams.

• NQ [[258]](#b257) is a QA dataset based on Wikipedia pages. Each instance(or example, as referred to in original paper) consists of a question, corresponding wikipedia page, a long answer and a short answer.

## Text Summarization Task

A summarization dataset is a curated collection of texts and their corresponding summaries. They typically include diverse content, such as news articles, scientific papers, or conversational data, paired with concise and accurate summaries. The detailed statistics of the datasets for text summarization are listed in Table [14](#tab_14).

• CNN/Dailymail [[266]](#b265), GovReport [[270]](#b269), and XSum [[267]](#b266) include a document and its corresponding summary in each instance. CNN/Dailymail is based on over 300,000 news articles, GovReport is based on 14,466 long government reports, and XSum is based on BBC news. • MultiNews [[269]](#b268) is a multi-doc summary dataset, each instance consists of multiple news and a summary.

• Loogle [[251]](#b250) is based on papers, WikiPedia, movie and TV scripts. Each long input text corresponds to muti-ple question-answer-summary triad. In total there are 776 documents and 6,448 questions. Average document length is 19.367 words.

• VCSUM [[271]](#b270) is based on real-world Chinese meeting transcripts. Each meeting tarnscript corresponds to a headline, segmentation summaries and an overall summary. There're 239 meetings in total.

• SummScreenFD [[272]](#b271) is based on TV transcripts.

Each instance consists of a TV transcript containing conversations, scenes and actor actions, and a summary(recapitulation, as referred to in original paper).

• BigPatent [[273]](#b272) is based on 1,341,362 patent documents.

The highlight of this dataset is that important information is distributed evenly in patent documents, compared to other types of documents. Each instance contains a document and its corresponding summary(human written abstract).

• SPACE [[274]](#b273) is based on reviews of 50 hotels. The highlight of the dataset is that the summaries are written in 6 different aspects, based on the hotel's review. Each hotel constructs an instance, containing the hotel's name, multiple reviews, summaries of different aspects and an overall summary.

• SQuality [[275]](#b274) is based on the same stories domain as QuALITY [[262]](#b261) dataset. It's a query-based summarization dataset. Each instance contains a story, multiple summarization questions, and multiple summarizations that corresponds to each questions. There are 625 QA pairs in total.

• CNNNews(M4LE) [[245]](#b244) is based on CNN English news.

Each instance of the dataset is paired with a multisentence summary.

• CEPSUM(M4LE) [[245]](#b244) is based on product information from Chinese e-commerce platform. Each instance contains a product description and corresponding summary. • LCSTS(M4LE) [[245]](#b244) is a summarization dataset in Chinese. It consists of over 2 million posts from a Chinese micro-blogging website, each post is paired with a summary. M4LE selects instances whose article has over 30 words.

• NCLS(M4LE) [[245]](#b244) is a summarization dataset with articles and corresponding summaries in different language, which highlights model's cross-lingual ability. Original NCLS is constructed from CNNNews and LCSTS. • WikiHow(M4LE) [[245]](#b244) is based on procedural descriptions on Wikipedia. Each article is entitled with a beginning of "How to...". Each paragraph of the article describes one step in the procedure, and corresponds to short summary. These summaries are then put together as the suymmary of the article. • News2016(M4LE) [[245]](#b244) is based on ove 2 million news articles in Chinese. For each article, its title is used as golden summary. M4LE remove instances whose length is less than 200 words or over 800 words. • PubMed(M4LE) [[245]](#b244) is based on medical papers. In M4LE, each paper's abstract is used as the summary of the paper. • BookSum(M4LE) [[245]](#b244) is a dataset containing 405 English books, whose contents covers plays, novels and short stories. Each chapter of the content corresponds to a human-written summary. • CNewsum(M4LE) [[245]](#b244) is based on 304,307 news articles in Chinese. Each article corresponds to a human-written summary.

• CLTS+(M4LE) [[245]](#b244) is based on CLTS [[280]](#b279). CLTS contains over 180,000 Chinese articles, and CLTS+ uses back translation to make summaries more abstractive. M4LE selects part of these instances for benchmark. • Arxiv(M4LE) [[245]](#b244) is based on papers collected from arXiv.org. For each paper, its abstract is used as golden summary.

## Text Reasoning Task

A reasoning task involves the ability of a model to draw logical conclusions, make inferences, or solve problems based on given information. It requires understanding relationships, patterns, or rules within the data to arrive at accurate and coherent outcomes.Natural Language Inference(NLI) can be considered a subset of reasoning. It highlights model's ability to perform logical inference instructed by natural language.In an NLI task, the typical goal is to determine the relationship between two pieces of text: a premise and a hypothesis. The detailed statistics of the datasets for text reasoning are listed in Tab. 15.

• Long Listops [[248]](#b247) is a mathematical reasoning dataset. It inputs an listop expression, instructing the model to perform calculation and output the exact numeric answer. A listop expression has a hierarchical structure that involves a set of simple mathematical operators. The final answer is a number in 0-9, described in original paper as "a tenway classification task".

• GSM [[278]](#b277) is a mathematcal reasoning dataset, which describes mathematical problems in natural language and ask the model to solve it.

• ContractNLI [277] uses contracts as context, and provides hypothesis, answer, and added evidence to each instance as well. The task requires model to judge the relationship between the hypothesis and context. Each instance contains 607 contracts, each contract has 17 annotated hypothesis and corresponding answers. • LSHT(LongBench) [247] is a Chinese classification dataset. It's based on Xinhua News. The model is asked to classify the input news articles into different categories. • SenHallu [246] and AbsHallu [246]use content and a related hypothesis as model's input, and instruct the model to determine whether the hypothesis is true based on the content. The false hypothesis(hallucination, as referred to by original paper) is generated by GPT. • MNDS News [279] is a classification dataset consisting of 10.917 news articles. The news articles have 17 first level categories and 109 second-level categories. 7.1.4 Text Retrieval Task A retrieval task in LLM benchmarks evaluates a model's ability to retrieve relevant information from a large collection of data based on a given query. It tests the model's

TABLE 16

Text Dataset-Retrieval. In the Avg. Len: average length, W: words. Particularly, LongEval, StreamingEval and TopicRet is more of a data generation method, which makes their length and instance number flexible, denoted by '-'. In the Metric column, Acc: Accuracy. F1 [[281]](#b280) calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words.

Task Name Source Instances Avg Len Metric Lang. CLS/RET TREC(LongBench) [247] Web Question 200 5177 W Acc EN RET LongEval [252] Conversations --Acc EN RET StreamingEval [136] LongChat [252] --Acc EN RET TopicRet(L-Eval) [244] LongChat [252] --Acc EN RET DRCD(M4LE) [245] Wiki -3617 W Acc ZH CLS+RET MARC [245] E-Commerce 2200 3543 W F1 EN,ZH CLS+RET Online Shopping(M4LE) [245] E-Commerce 2200 3714 W F1 ZH CLS+RET MNDS News(M4LE) [245] MNDS News [279] -3805 W Acc EN CLS+RET THUCNews(M4LE) [245] News -3721 W Acc ZH

understanding of the query, semantic matching, and efficiency in identifying the most relevant documents or pieces of information. The detailed statistics of the datasets for text retrieval are listed in Table [16](#tab_0).

• LongChat [[252]](#b251) has two subtask dataset for retrieval.

Coarse-grained Topic Retrieval dataset use a long document that talk about a number of different topics, and instrutct the model to retrieve the first topic of the document. Fine-grained Line retrieval, on the other hand, is more challenging, which present the model with multiple lines that contain a diffrernt number and label, with similar line patterns. The model is asked to retrieve the number of a specific labeled line.Notably, such dataset can be easily constructed or generated, so it's easy to create an ultra long dataset of this type. Because the dataset is easily constructed by definition, the length of the dataset and the number of instances is indefinite.

• StreamingEval [[136]](#b135) construct a line retrieval task based on LongChat, which makes a query in every 10 lines, with its answer about 20 lines above, so as to evaluate the streaming conversation scenario.

• TopicRet [[244]](#b243) on the other hand, is based on the coarsegrained topic retrieval task, but ask about the second or third topic instead of the first one, so as to make the task more challenging. • DRCD(M4LE) [[245]](#b244) is a reading comprehension dataset.

In M4LE, DRCD is constructed into two subset, one(DRCD explicit) require model to return the articles' IDs related to a given topic, and another subset(DRCD semantic) requires the model to answer specific questions given multiple paragraphs.

• MARC [[245]](#b244) consists of bilingual(namely English and Chinese) reviews. The model is asked to identify all positive reviews and retrieve them. • Online Shopping(M4LE) [[245]](#b244) is based on 60K product reviews on Chinese e-commerce platforms. Reviews are categorized into positive and negative.

## Text Generation Task

Generation tasks require model to generate contents based on the given instructions and context. The detailed statistics of the datasets for text generation are listed in Table [17](#tab_19).

• MultiDoc2Dial [[283]](#b282) gives model a dialogue history and all involved documents, and instruct model to generate the next turn of the dialogue. • OpenReview(L-Eval) [[244]](#b243), which is based on ASAP-Review [[285]](#b284), provides LLM with a paper and instruct it to generate a review. • ShowsPred and MeetingPred [[246]](#b245) use dialogue history as input, and ask model to infer which role said the last turn of the conversation. Apart from natural language context, code generation is also an important implementation for LLMs.

• LCC [[282]](#b281) gives model long code snippets as context, and instruct model to generate the following line of code. • RepoBench-P [[286]](#b285) requires model to retrieve toe most relevant code snippets from a long input, and then generate code according to the instruction.

• PrivateEval [[246]](#b245) use API documents and a code snippet as input, and instruct the model to generate code acccordingly. Notably, to avoid data contamination caused by pre-training, the keywords in API documents are modified, making the document "private".

• CodeU [[246]](#b245) use the same practice of modifying keyword, only that it uses modified source code of public library, rather than API document, as an input.

## Aggregation Task

Aggregation task involves understanding and aggregating information from the whole input to answer complex instructions, such as calculating the percentage of positive comments given a set of comments of different attitudes.

The detailed statistics of the datasets for text aggregation are listed in Table [18](#tab_20).

• SpaceDigest [[250]](#b249) give the model a set of hotel reviews, and ask the model to output the percentage of positive reviews in the context. • BookSumSort [[250]](#b249), ReportSumSort [[246]](#b245), and Shows-Sort [[246]](#b245) use shuffled paragraphs from book summaries, TV transcripts or government reports as context, and ask the model to sort them in the correct order.

• PassageCount [[247]](#b246) selects multiple passage, duplicates some of the paragraphs, and put all those paragraphs into an instance after shuffling. The model is then asked to determine how many documents are used to construct this instance. • PassageRetrieval [[247]](#b246), on the other hand, selects 30 wikipedia passages, and use GPT-3.5-Turbo to write a summary for one of them. Then these passages and the generated summary are used as the model input. The model is then instructed to tell which passage was the summary generated from.

## Evaluation Metric for Text Datasets

General evaluation metrics used by text datasets mentioned above include Exact Match [[287]](#b286), Partial Match, Accuracy, Recall, Precision, F1, BLEU [[288]](#b287), SacreBLEU [[289]](#b288), Rouge [[290]](#b289), METEOR [[291]](#b290), BERT [[292]](#b291), Edit Similarity, Pass@k [[293]](#b292) , Exponential Similarity, Concordance Index, Mean Reciprocal Rank. In addition to general evaluation metrics, some more specific metrics are used in particular benchmarks. For datasets from L-Eval [[244]](#b243), the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. ∆L is the length difference between answer length and ground truth. For LooGLE [[251]](#b250), it utilizes GPT-4 for its QA and summarization task, using it for answer's semantic judgment.

• Exact Match (EM) [[287]](#b286) is a metric used to evaluate the accuracy of models in tasks like question answering or text generation. It measures the percentage of predictions that exactly match the ground truth answer, considering both the content and format. • Partial Match (PM) metric evaluates the similarity between a model's output and the reference by allowing partial credit for partially correct answers. Unlike strict metrics like Exact Match (EM), PM accounts for overlaps or shared elements, such as keywords or phrases, making it more flexible in assessing performance. • Accuracy is a metric used to evaluate the overall performance of a model by measuring the proportion of correctly predicted instances (both positive and negative) out of the total instances. • Recall is a metric used to evaluate a model's ability to retrieve all relevant instances in a dataset. It is calculated as the ratio of correctly retrieved relevant items to the total number of relevant items, emphasizing completeness. • Precision is a metric used to evaluate the accuracy of a model by measuring the proportion of correctly predicted positive instances out of all predicted positive instances. • F1 is a performance measure that combines Precision and Recall into a single score using their harmonic mean. It provides a balanced evaluation, especially useful in datasets with imbalanced classes, by considering both false positives and false negatives.

• BLEU [[288]](#b287), is a widely used metric for evaluating the quality of machine-generated text, especially in machine translation. It works by comparing n-grams in the generated output with reference texts to measure overlap, while applying penalties for overly short outputs to ensure fluency.

• SacreBLEU [[289]](#b288) is a standardized version of the BLEU metric used to evaluate machine translation quality. It simplifies BLEU's implementation by fixing preprocessing steps like reference handling to ensure consistent and reproducible results across different systems.

• Rouge [[290]](#b289) and its variants measure model's performance by calculating overlap between model output and reference answer with unigram(Rouge-1), bigram(Rouge-2), LCS(Rouge-L), etc. Gold Rouge-1 in VCSUM dataset refers to using high-quality reference summaries (gold standards) for evaluation, ensuring reliable and meaningful comparisons.

• METEOR [[291]](#b290) (Metric for Evaluation of Translation with Explicit ORdering) is a text evaluation metric designed to assess the quality of machine translation.

• BERT [[292]](#b291) metric, often referred to as BERTScore, is a text evaluation metric that uses contextual embeddings from the BERT model to compare similarity between generated and reference texts. • Edit Similarity is a metric that measures the similarity between two text sequences based on the minimum number of edit operations required to transform one sequence into another. It is derived from the concept of edit distance such as Levenshtein distance.

• Pass@k [[293]](#b292) evaluates the performance of a model by measuring the percentage that at least one of the top k generated outputs contains a correct solution. In datasets we surveyed, only Pass@1 is used. • Exponential Similarity is a metric that measures the similarity between two items by exponentially weighting their differences, giving more importance to smaller discrepancies.

• Concordance Index is a metric used to evaluate the predictive accuracy of models, particularly in survival analysis or ranking tasks. • Mean Reciprocal Rank (MRR) is an evaluation metric commonly used in information retrieval and recommendation systems to measure the quality of ranked results. It calculates the reciprocal of the rank of the first relevant item in a result list and averages it across all queries.

## Multimodal Datasets and Evaluation Metric

## Multimodal Datasets

Multimodal datasets have emerged to address the need for a comprehensive understanding of the complex real world by integrating diverse data types such as text, images, audio, and video. These datasets drive advancements in AI, particularly in machine learning and deep learning, by offering rich and diverse data to train more robust and versatile models. We analyze the multimodal benchmarks listed in Table [19](#tab_21), highlighting their distinct focuses. Each benchmark is built upon one or more multimodal datasets, involving their collection, processing, and the use of specific validation metrics. Below, we provide a detailed introduction and description of each multimodal benchmark.

• LLaVA-Bench [[294]](#b293) The benchmark is structured around image-ground-truth textual description-question-answer triplets, segmented across COCO and In-The-Wild datasets. It assesses a model's proficiency in multimodal instruction adherence and visual reasoning. By employing a suite of tasks and metrics, it quantifies the model's ability to comprehend and act on visual-language directives, articulate comprehensive descriptions, and engage in intricate reasoning processes.

• MMBench [[295]](#b294) This benchmark serves as a bilingual multimodal benchmark, facilitating a comparative analysis of VLM performance across English and Chinese linguistic contexts. It distinctively assesses multimodal models using a hierarchical taxonomy of abilities, stringent quality assurance measures, and a dual-language evaluation framework. Unlike other benchmarks, MM-Bench [[295]](#b294) incorporates the CircularEval strategy for comprehensive evaluation and utilizes LLMs for precise extraction of choices, setting it apart from its counterparts.

• MileBench [[296]](#b295) evaluates the multi-modal long-context capabilities of LLMs, including both diagnostic and realistic evaluation sets. It emphasizes long-context and multi-image tasks. This unique focus allows it to capture the complexity and diversity of real-world multimodal challenges, setting it apart from existing benchmarks. The dataset in MileBench [[296]](#b295) is characterized by its inclusion of long texts integrated with multiple images, reflecting real-world scenarios where context is key. It contains a diverse range of tasks that require both comprehension and generation.

• MLVU [[297]](#b296) is a holistic benchmark, designed to gauge the capabilities of multi-modal LLMs in comprehending video content, transcends the constraints of its predecessors by significantly increasing video durations, encompassing diverse video genres, and crafting a spectrum of assessment tasks. This benchmark offers an extensive array of tasks and video genres to evaluate the comprehensive competencies of MLLMs. It highlights the substantial potential for enhancement in current methodologies and emphasizes the critical factors of context length, image comprehension quality, and the selection of LLM architecture for future progress.

• LongVideoBench [[298]](#b297) This benchmark offers an extensive benchmarking framework aimed at assessing the capacity of large multimodal models (LMMs) to comprehend lengthy videos with subtitles, extending up to an hour. It places a strong focus on the retrieval and reasoning capabilities over extended, interwoven video and language data streams, tackling the challenge of singleframe bias and underscoring its proficiency in evaluating multimodal comprehension in long contexts. • Video-MME [[299]](#b298) A benchmark for comprehensive evaluation, it assesses the proficiency of Multi-modal Large Language Models (MLLMs) in analyzing videos. This dataset comprises a wide array of 900 videos spanning diverse domains and subfields, ensuring extensive scenario coverage. It encompasses videos with lengths ranging from 11 seconds to 1 hour to gauge model flexibility across various time frames. Furthermore, it incorporates various data modalities, including subtitles and audio tracks, to evaluate the comprehensive competencies of MLLMs. The benchmark aims to test the models' capacity for sequential visual data comprehension, with an emphasis on temporal reasoning and the processing of multimodal inputs. • NExT-QA [[300]](#b299) Advancing video comprehension from mere description to explanation of causal, temporal, and descriptive actions, a video question answering (VideoQA) benchmark has been established. This benchmark boasts a dataset with 5,440 videos and approxi-  [[301]](#b300) Featuring a substantial dataset, the benchmark comprises 200 multiple-choice questionanswer (QA) pairs for each of the 20 temporal understanding tasks, amassing a total of 4,000 QA pairs. It draws from a variety of videos across 11 public datasets, spanning diverse domains and scenes, thereby testing models' abilities to comprehend temporal sequences. The benchmark automates the generation of multiple-choice QA pairs from existing video annotations, minimizing human involvement and ensuring a fair evaluation process. • MSVD-QA [[302]](#b301) The MSVD dataset is a collection of 1,970 video clips with descriptive captions, initially for video captioning. It features diverse real-world scenarios and assesses multimodal learning models' capabilities in understanding video content and generating natural language descriptions. • MSRVTT-QA [[302]](#b301) The MSR-VTT dataset comprises 10,000 video clips with 20 human-transcribed sentences each, focusing on connecting video content with language descriptions. It evaluates multimodal learning models' ability to comprehend video information and translate it into coherent captions, testing their video understanding and language generation skills in a more complex and diverse environment.

## Evaluation Metric for Multimodal Datasets

The evaluation metrics for multimodal datasets include Relative Score, Accuracy, ROUGE-L, M-Avg, G-Avg, WUPS. Several common metrics, including Accuracy, ROUHE-L, have been introduced in Sec. 7.1.7. Here, we only introduce the special metrics of multimodal datasets, which include Relativa Score, M-Avg, G-Avg, WUPS as follows:

• Relative Score This metric is used in LLaVA-Bench to evaluate the performance of multimodal models by comparing their outputs to a reference model, typically textbased GPT-4. It is calculated as the percentage ratio of the candidate model's score to the reference model's score, based on dimensions such as helpfulness, relevance, accuracy, and level of detail. • M-Avg Multiple-Choice Average is calculated as the mean accuracy across all multiple-choice tasks in the MLVU benchmark. The accuracy for each task is determined by the proportion of correctly predicted answers compared to the total number of questions within that task. • G-Axg Generation Average s calculated as the mean score across all generation tasks in the MLVU benchmark. Each task is evaluated on multiple dimensions (e.g., Accuracy, Relevance, Completeness, and Reliability) using GPT-4, with scores ranging from 1 to 5. The overall score for each task is the average of these dimensions, and G-Avg is the mean of these task-level scores.

• WUPS [[303]](#b302) Wu-Palmer Similarity measures the semantic similarity between two words based on their positions in a taxonomy (e.g., WordNet). It calculates how closely related two words are by considering their least common ancestor (LCS).

## CONCLUSION

Advancements in LLMs have driven significant progress on various fields, but their high computational and memory demands during inference pose challenges, especially for long-context and real-time applications. KV cache management offers an effective solution by optimizing memory, reducing redundant computation, and improving performance. This survey reviews KV cache management strategies across token-level, model-level, and system-level optimizations. Token-level optimizations focus on fine-grained control of KV cache through selection, budget allocation, merging, quantization, and low-rank decomposition, enabling efficient resource allocation without altering model architectures. Model-level optimizations leverage architectural innovations, such as attention grouping and nontransformer designs, to enhance the efficiency of KV reuse. System-level optimizations further complement these efforts by employing advanced memory management, scheduling techniques, and hardware-aware designs to optimize resource utilization across diverse computing environments. Despite the progress made, substantial opportunities remain for future exploration. Key areas include the development of real-time, task-specific budget allocation strategies, dynamic workload handling, advanced distributed coordination for KV cache in multi-node systems, and hardwareaware innovations to leverage emerging architectures like computational storage and processing-in-memory. Additionally, integrating reinforcement learning and adaptive algorithms could enable more intelligent and responsive KV cache management, further enhancing LLM efficiency across diverse deployment scenarios.

![Fig. 1. The decoder-only Transformer for LLMs.]()

![Fig.4. Three types of quantization. Then matrix X ∈ R T ×C , where T is the number of tokens and C is the feature dimension.]()

![Positional encodingQ i , K i , V iQuery, Key, and Value matricesd k , dvQuery/Key and Value dimensionW Q i , W K i , W V i Weight matrices for computing Q i , K i , V i .]()

![The summary of existing KV Cache merging approaches.]()

![The summary of existing mixed-precision quantization models.]()

![The summary of Model-based Intra-layer approaches.]()

![Comparison of Memory Management Techniques for KV CacheOptimization.]()

![Text question answering (QA) dataset. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Q: questions, Inst: instructions. Particularly, AltQA, PaperQA and MeetingQA have two datasets with different length levels, and is separated with /.Particularly, for datasets from L-Eval, the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. ∆L is the length difference between answer length and ground truth. For NarrativeQA, MRR: Mean Reciprocal Rank .]()

![Dataset-Summarization. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Q: questions, Inst: instructions. Particularly, SPACE has the concept of 'Entity', and R/Ent stands for reviews per entity. Sum stands for summary. In the Metric column, EM: Exact Match. PM: Partial Match. Acc: Accuracy. For MultiNews, Rouge-SU skip bigrams when having a distance larger than 4 words. Particularly, LooGLE utilizes GPT-4 for its QA and summarization task, using it for answer's semantic judgement.]()

![Reasoning/Classification Datasets. CLS: Classification. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Inst: instructions. In the Metric column, EM: Exact Match. Acc: Accuracy.]()

![Dataset-Generation.In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Inst: instructions. In the Metric column, EM: Exact Match. Acc: Accuracy.]()

![Dataset-Aggregation. In the Avg. Len: average length, Tok: tokens; W: words. In the Instances column, Doc: documents, Inst: instructions.In the Metric column, Acc: Accuracy. ES: Exponential Similarity, CI: Concordance Index]()

![Dataset. Specfically, for data type, Img: Image; T: text; V: Video. For task abbreviation, Conv: conversation task; Desc: description task; Reas: reasoning task; Perc: perception task; Pred: prediction; NTH: needle in the haystack; SUMM: summary. For instance and average column, Q: questions; W: words; s: seconds. For example, 54 Img, 150 Q denote that there are 54 images with 150 questions.annotated question-answer pairs, sorted into causal, temporal, and descriptive categories. It poses a challenge to QA models to engage in reasoning about causal and temporal actions and to decipher complex object interactions within daily activities. Distinguished from other video benchmarks, this benchmark specifically focuses on causal and temporal action reasoning within realistic videos that are rich in object interactions. It stands as one of the largest manually annotated VideoQA datasets, offering support for both multiple-choice and open-ended questions, and includes a variety of videos that mirror real-life scenarios.]()

