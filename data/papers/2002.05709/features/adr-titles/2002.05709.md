- Choice of contrastive learning framework (SimCLR)
- Data augmentation strategies and their composition
- Selection of neural network architecture (ResNet)
- Design of the projection head (MLP with one hidden layer)
- Definition of the contrastive loss function (NT-Xent)
- Use of large batch sizes for training
- Implementation of the LARS optimizer
- Aggregation of batch normalization statistics across devices
- Evaluation protocol for learned representations
- Default settings for training parameters (learning rate, weight decay, etc.)
- Impact of data augmentation on representation quality
- Decision to avoid memory banks in training
- Choice of datasets for empirical studies (ImageNet, CIFAR-10)
- Linear evaluation protocol for assessing representation quality
- Use of temperature parameter in contrastive loss
- Exploration of different data augmentation techniques (color distortion, Gaussian blur, etc.)
- Training duration and epoch settings
- Handling of positive and negative pairs in contrastive learning
- Analysis of the effect of deeper and wider networks on performance
- Consideration of unsupervised vs. supervised learning benefits