<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distillation Scaling Laws</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-12">12 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amitis</forename><surname>Shidani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Floris</forename><surname>Weers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gunesh</forename><surname>Dhekane</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Angeliki</forename><surname>Giannou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Goli≈Ñski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Gunter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Skyler</forename><surname>Seto</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Barry</forename><surname>Theobald</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vimal</forename><surname>Thilak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Zappella</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaungfei</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Okan</forename><surname>Akalin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Babaie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Bukowinski</surname></persName>
						</author>
						<author>
							<persName><roleName>Mubarak</roleName><forename type="first">Denise</forename><surname>Hui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Seyed</forename><surname>Ibrahim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Koski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cindy</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cesar</forename><forename type="middle">Lopez</forename><surname>Nataren</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rajat</forename><surname>Phull</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Evan</forename><surname>Samanas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Seguin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Swann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shang-Chen</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joe</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kelvin</forename><surname>Zou</surname></persName>
						</author>
						<title level="a" type="main">Distillation Scaling Laws</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-12">12 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">3093D1DD6B24028BE594194B681EAF01</idno>
					<idno type="arXiv">arXiv:2502.08606v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The study of scaling laws <ref type="bibr" target="#b44">(Hestness et al., 2017;</ref><ref type="bibr" target="#b84">Rosenfeld et al., 2020;</ref><ref type="bibr" target="#b54">Kaplan et al., 2020;</ref><ref type="bibr" target="#b26">Hoffmann et al., 2022)</ref> revealed that previously trained Language Models (LMs) could have been more capable if they had followed a compute optimal training paradigm, which determines the model size and the number of training tokens that give the best performing model under a given compute budget. Many subsequent works have followed compute optimal training <ref type="bibr">(Dey et al., 2023;</ref><ref type="bibr">Muennighoff et al., 2023b)</ref>.</p><p>The size of compute optimal models grows with compute <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref>, which makes them challenging to use due to the growth in inference costs. In practice, this means compute optimal models are slow, expensive to serve, consume more battery life, provide high barriers Preprint. to entry for academic study, and have a significant carbon footprint. With inference volume up to billions of tokens per day <ref type="bibr">(OpenAI &amp; Pilipiszyn, 2021)</ref>, the inference cost of an LM is typically significantly larger than its pretraining cost <ref type="bibr" target="#b22">(Chien et al., 2023;</ref><ref type="bibr">Wu et al., 2024a)</ref> and is going to further increase in an era of test-time compute scaling <ref type="bibr">(Snell et al., 2024;</ref><ref type="bibr" target="#b16">Brown et al., 2024;</ref><ref type="bibr">Wu et al., 2024b)</ref>.</p><p>Unsustainable inference costs have led to an alternative training paradigm, overtraining <ref type="bibr">(Gadre et al., 2024)</ref>, where the amount of training data used is much greater than in the compute optimal case, enabling small, capable models. Overtrained models better satisfy compute optimality when compute is measured over a model's lifetime, rather than just the pretraining cost <ref type="bibr" target="#b86">(Sardana et al., 2024)</ref>. As supervised scaling laws follow power laws in model size and training data, diminishing returns in performance oc-cur much sooner than in the compute-optimal case. To achieve reasonable capabilities, these models need to be trained on many trillions of tokens, <ref type="bibr">(Snell et al., 2024;</ref><ref type="bibr" target="#b16">Brown et al., 2024;</ref><ref type="bibr">Wu et al., 2024b)</ref>, which is expensive and time-consuming.</p><p>We seek models that match the performance of small overtrained models but at lower training cost. A popular candidate is distillation <ref type="bibr" target="#b45">(Hinton et al., 2015)</ref>, where a capable teacher LM produces targets for a smaller student LM. When distillation is used for LM pretraining, we will call this distillation pretraining. There are many explanations for why distillation works, from dark knowledge transfer, where information is contained in the ratio of probabilities of incorrect classes <ref type="bibr" target="#b45">(Hinton et al., 2015)</ref>, to being a form of regularization <ref type="bibr" target="#b67">(Mobahi et al., 2020)</ref>, or reducing noise in the learning process <ref type="bibr" target="#b64">(Menon et al., 2020)</ref>, among many other explanations. Despite a lack of consensus for why distillation works, distillation pretraining has produced more capable models than supervised pretraining in the Gemma and Gemini <ref type="bibr">(Rivi√®re et al., 2024)</ref>, Minitron <ref type="bibr">(Muralidharan et al., 2024;</ref><ref type="bibr">Sreenivas et al., 2024)</ref> and AFM <ref type="bibr">(Gunter et al., 2024)</ref> families of LMs in terms of both pretraining loss and downstream evaluations. Yet, at the same time, <ref type="bibr" target="#b59">Liu et al. (2024)</ref> reported that distillation produces less capable models than supervised pretraining does.</p><p>With such significant compute resources being devoted to distillation pretraining of LMs, it is essential to understand how to correctly allocate these resources, to produce the most capable models possible, and to have an understanding if any gains are even possible compared to supervised pretraining when both methods have access to the same resources <ref type="bibr">(Dehghani et al., 2021)</ref>.</p><p>To close this knowledge gap, we perform an extensive controlled study of distillation, with students and teachers ranging from 143M to 12.6B parameters, trained on data of a few billion tokens, up to 512B tokens. These experiments result in our distillation scaling law, which estimates student performance as a function of resources (the teacher, the student size, and the amount of data used for distillation), resolving questions about when distillation is and is not effective in terms of producing models of a desired capability under resource constraints of interest. We find:</p><p>1. The cross entropy of a student of size N S distilled on D S tokens from a teacher of size N T trained on D T tokens can be predicted using our distillation scaling law (Equation <ref type="formula">8</ref>).</p><p>2. The teacher size N T and number of teacher training tokens D T determines the student cross-entropy only through their determination of the teacher's crossentropy L T = L T (N T , D T ) (Figure <ref type="figure" target="#fig_2">3b</ref>).</p><p>3. The influence of the teacher cross-entropy upon the student loss follows a power law which transitions between two behaviors depending on the relative learning capacities of student and the teacher, reflecting a phenomenon in distillation called the capacity gap, where a stronger teacher produces a worse student.</p><p>Our parameterization resolves outstanding questions about the capacity gap, showing that it is a gap in learning capacity (both hypothesis space and ability to optimize) between the teacher and student, and not only about their relative sizes, which is a special case.</p><p>Our results show that distillation can not produce lower model cross-entropies than supervised learning when both learning processes are given enough data or compute. However, distillation is more efficient than supervised learning if both of the following are true:</p><p>1. The total compute or tokens used for the student is not larger than student size-dependent threshold given by our scaling law (Section 5.1).</p><p>2. A teacher already exists, or the teacher to be trained has uses beyond a single distillation (Section 5.3).</p><p>We hope the laws and analyses we provide will guide the community to produce even more capable models with lower inference cost and lower lifetime compute costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Predicting model performance is essential when scaling as it lets us understand i) the value of increasing the available compute (C), and ii) how that compute should be distributed, typically between model parameters (N ) and data (D), in order to achieve a model with desired properties. These properties may be predicting the data distribution sufficiently well, measured in cross-entropy (L), or achieving a level of performance on downstream tasks of interest.</p><p>Fortunately, cross-entropy is predictable, with substantial empirical and theoretical evidence that L follows a powerlaw in parameters N and data D (measured in tokens)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L(N, D)</head><p>Model Cross-Entropy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= E</head><p>Irreducible Error</p><formula xml:id="formula_0">+ A N Œ± + B D Œ≤ Œ≥ Model ability to mimic data ,<label>(1)</label></formula><p>where {E, A, B, Œ±, Œ≤, Œ≥} are task-specific positive coefficients<ref type="foot" target="#foot_2">foot_2</ref> estimated from n training runs {(N i , D i , L i )} n i=1 . The choice of runs is critical; not all experiments enable identifying the coefficients of Equation <ref type="formula" target="#formula_0">1</ref> This is tempting, as for a total experiment budget, compute optimal models offer the largest loss variation. Unfortunately, compute optimal models have a constant token to parameter ratio M ‚â° D/N = const. <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref>, removing a degree of freedom.</p><p>To achieve reliable identification of scaling coefficients, <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref> uses two training strategies:</p><p>1. <ref type="bibr">(Fixed model, varied data)</ref> The number of training tokens is varied for a fixed family of models.</p><p>2. (IsoFLOP profiles) Model size and training tokens are both varied subject to a total compute constraint.</p><p>Data from both strategies is then combined for the fit. See Appendix B for an extended background.</p><p>The goal of this paper is predict the cross-entropy L S of a student produced by distillation. This will tell us the value of increasing the compute for distillation and, crucially, which distillation produces the student of a given size with the lowest cross-entropy for a given compute budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Notation For a sequence x, x (i:j) = (x (i) , x (i+1) , . . . , x (j) ) returns a slice of the sequence, and x (&lt;i) =</p><p>x (1:i-1) = (x (1) , . . . , x (i-1) ) is the context of x (i) . We use the shorthand X * = ‚à™ n‚ààN X n to denote the set of sequences with arbitrary length n ‚àà N = {1, 2, . . .}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language modeling</head><p>We focus on the LM setting where the training objective is to model the probability of sequences x of tokens x i drawn from a vocabulary V = {1, 2, . . . , V }. Let f : V * √ó Œò ‚Üí R V be a next-token classifier parameterized by Œ∏ ‚àà Œò whose outputs define a predictive categorical distribution over V given a context x (&lt;i)   p(x (i) = a|x (&lt;i) ; Œ∏) = œÉ a (f (x (&lt;i) ; Œ∏)) = œÉ a (z (i) ), (3)</p><p>where œÉ a (z) = exp(z a )/ b exp(z b ) is the softmax function. The next-token classifier outputs z (i) = f (x (&lt;i) ; Œ∏) are the logits.<ref type="foot" target="#foot_3">foot_3</ref> Autoregressive LMs produce sequence likelihoods through p(x; Œ∏) = L i=1 p(x (i) |x (&lt;i) ; Œ∏) and are trained to maximize this likelihood on observed data through the Next Token Prediction (NTP) loss L NTP (x (i) , z (i) ) = -V a=1 e(x (i) ) a log œÉ a (z (i) ),</p><p>where e(i) is the i-th basis vector. It is common to also use the following token-level Z-loss to improve training stability <ref type="bibr" target="#b116">(Chowdhery et al., 2023;</ref><ref type="bibr">Wortsman et al., 2023</ref>)</p><formula xml:id="formula_2">L Z (z (i) ) = || log Z(z (i) )|| 2 2 = log V a=1 exp(z (i) a ) 2 2 . (<label>5</label></formula><formula xml:id="formula_3">)</formula><p>Distillation In distillation, a teacher with predicted nexttoken distribution pT (x (i) |x (&lt;i) ; Œ∏ T ) and corresponding logits z</p><formula xml:id="formula_4">(i)</formula><p>T replaces the one-hot basis vector in Equation 4 and is used as the target for a student predicted next-token distribution qS (x (i) |x (&lt;i) ; Œ∏ S ) and corresponding logits</p><formula xml:id="formula_5">z (i)</formula><p>S . The resulting knowledge distillation loss is used to optimize the student parameters</p><formula xml:id="formula_6">L KD (z (i) T ,z (i) S )=-œÑ 2 V a=1 œÉ a z (i) T œÑ logœÉ a z (i) S œÑ ,<label>(6)</label></formula><p>and is equivalent to optimizing the Kullback-Leibler Divergence (KLD) between the teacher and student predictions. œÑ &gt; 0 is the distillation temperature. Combining the losses together results in a total token-level loss for the student:</p><formula xml:id="formula_7">L S (x (i) , z<label>(i)</label></formula><p>T , z</p><p>S ) = (1Œª) L NTP (x (i) , z</p><p>S ) + Œª L KD (z</p><formula xml:id="formula_10">(i) T , z (i) S ) + Œª Z L Z (z (i) S ).</formula><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distillation Scaling Laws</head><p>Here we outline the steps taken to arrive at our distillation scaling law. First we describe the experimental setting (Section 4.1) and the experiments needed to determine the scaling coefficients (Section 4.2). Given the empirical observations, we discuss the form our distillation scaling law takes (Section 4.3), find the coefficients, and verify the law under extrapolation (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>All models are based on <ref type="bibr">Gunter et al. (2024)</ref> and use decoupled weight decay <ref type="bibr" target="#b61">Loshchilov &amp; Hutter (2019)</ref> for regularization, as well as a simplified version of ¬µP <ref type="bibr" target="#b104">(Yang &amp; Hu, 2021;</ref><ref type="bibr">Yang &amp; Littwin, 2023;</ref><ref type="bibr" target="#b31">Yang et al., 2022;</ref><ref type="bibr">Wortsman et al., 2023;</ref><ref type="bibr" target="#b114">Yang et al., 2023)</ref>, following ¬µP (simple) in <ref type="bibr" target="#b100">(Wortsman et al., 2024)</ref>. ¬µP simplifies the scaling law experimental setup as it enables hyperparameter transfer of the learning rate across model sizes. We validate that ¬µP functions as expected for distillation in Appendix G.3. DS The number of tokens the student is distilled on. M ‚â° D/N The tokens per parameter ratio, or M -ratio. In <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref>, M takes a compute optimal value M * ‚âà 20 which is the Chinchilla rule of thumb. L ‚âà L(N, D) The model cross-entropy, which is the model validation cross entropy under data, estimated by the supervised scaling law for a model with N parameters trained on D tokens. (Equation <ref type="formula" target="#formula_0">1</ref>). LT ‚âà L(NT , DT ) The teacher cross-entropy, which is the teacher validation cross entropy under data, estimated by the supervised scaling law for a teacher with NT parameters trained on DT tokens. LS ‚âà LS(NS, DS, LT ) The student cross-entropy, which is the student validation cross entropy under data, estimated by our distillation scaling law for a student with NS parameters distilled on DS tokens using a teacher with pretraining loss LT (Equation <ref type="formula">8</ref>).</p><p>LS ‚âà L(NS, DS) The student supervised cross-entropy, which is the student validation cross entropy under data if the student had been trained in a supervised way, estimated by the supervised scaling law for a student with NS parameters trained on DS tokens.</p><p>Models have sizes which range from 143M to 12.6B parameters, and we allow the teacher to be smaller or larger than the student. Multi-headed attention (MHA) is used, with Pre-Normalization <ref type="bibr">(Nguyen &amp; Salazar, 2019)</ref> using RMSNorm <ref type="bibr" target="#b111">(Zhang &amp; Sennrich, 2019)</ref>. We train all models with a sequence length of 4096, with Rotary Position Embedding (RoPE) <ref type="bibr">(Su et al., 2024)</ref>. We use the Englishonly subset of the C4 dataset <ref type="bibr">(Raffel et al., 2020)</ref> for all experiments. For all distillation trainings, the teacher is trained on a different split from the student. Except for the largest models, all Chinchilla-optimal models do not repeat data. Full hyperparameters and details can be found in Appendix I. As our goal is to understand the role of the teacher in the distillation process we distill in the pure distillation case (Œª = 1, Equation <ref type="formula">7</ref>) to avoid confounding coming from the data, as was done in <ref type="bibr" target="#b92">Stanton et al. (2021)</ref>. We verify the choice Œª = 1 produces results statistically similar to the optimal Œª * (see Appendix G.1). Similarly, all experiments use distillation temperature (œÑ = 1), as we found this produces the best performing students (see Appendix G.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Distillation Scaling Law Experiments</head><p>Here we discuss the experiments that produce the data for fitting our distillation scaling law. The distillation scaling law will estimate student cross-entropy L S 3 , which in general depends on the student parameters N S , number of distillation tokens D S , the teacher parameters N T and the number of teacher training tokens N T : L S ‚âà L S (N S , D S , N T , D T ). As discussed in Section 2, only certain combinations of data support reliable identification of scaling law coefficients. We combine three experimental 3 By cross-entropy, we always mean with respect to data, not the teacher. We summarize our scaling law notation in Table <ref type="table" target="#tab_0">1</ref>. protocols to produce data for our distillation scaling law fit.</p><p>100M 300M 1B 3B 7B 2.25 2.50 Student Cross-Entropy L S Teacher N T =975M 100M 300M 1B 3B 7B</p><p>Teacher N T =7.75B Student Parameters N S Student FLOPs 3 √ó 10 19 10 20 3 √ó 10 20 10 21 3 √ó 10 21 Figure 2. Fixed M Teacher/Student IsoFLOP profiles. Two (of a total of six) teachers with MT = DT /NT ‚âà 20 are distilled into students with four IsoFLOP profiles, and a small number with CS = 3 √ó 10 21 . Horizontal and vertical dashed lines indicate teacher cross entropy LT and size NT respectively. See Appendix E.4, Figure 38a for all six profiles.</p><p>Fixed M Teachers/Student IsoFLOPs To simplify the experimental protocol we make the following assumption: Training a student (N S , D S ) on the signal provided by a teacher (N T , D T ) is qualitatively similar to training that student on fixed dataset. As power law behavior has been observed in a wide variety of datasets and domains <ref type="bibr" target="#b43">(Henighan et al., 2020)</ref>, it is expected that there should be a power law behavior in (N S , D S ) given a fixed teacher.</p><p>To identify these coefficients correctly, a similar protocol to the Chinchilla protocol described in Section 2 should be performed. However, we cannot only do this for only one teacher, as the way student size and tokens affects downstream performance may be different for different teachers, just as the scaling laws are different for different domains and dataset. For distillation we anticipate this is the case so that different teachers produce different students. To produce the widest range of teachers for a compute budget, we train six Chinchilla-optimal (M T = D T /N T ‚âà 20) teachers ranging from 198M to 7.75B parameters. <ref type="foot" target="#foot_4">4</ref> For each of those teachers, we distill into students with four IsoFLOP profiles, taking only the standard training cost into account. The resulting student cross-entropies are in Figure <ref type="figure">2</ref>. We note that in some cases, the student is able to outperform the teacher, i.e. exhibits weak-to-stronggeneralization <ref type="bibr" target="#b19">(Burns et al., 2024;</ref><ref type="bibr">Ildiz et al., 2024)</ref> and investigate this further in Appendix E.7. IsoFLOP Teachers/Fixed M Students The fixed-M teacher IsoFLOP student protocol is insufficient to identify how N T and D T independently influence student crossentropy. To ensure our experiment can detect this influence, 100M 300M 1B 3B 7B Teacher Parameters N T 2.3 2.4 2.5 2.6 Student Cross-Entropy L S Student: 1.82B</p><p>Teacher FLOPs 3 √ó 10 19 10 20 3 √ó 10 20 10 21 (a) One teacher IsoFLOP set. 2.2 2.4 2.6 Teacher Cross-Entropy L T 2.3 2.4 2.5 2.6 2.7 Student Cross-Entropy L S Student Parameters NS 198M 546M 975M 1.82B (b) All teacher IsoFLOPs. we perform experiments where the student (N S , D S ) is fixed, and vary N T and D T subject to a compute constraint, i.e. a teacher IsoFLOP. We perform distillations into four Chinchilla-optimal (M S = D S /N S ‚âà 20) students ranging from 198M to 1.82B parameters from teachers trained according to four IsoFLOP profiles. The resulting student cross-entropies are in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Fixed M Teachers/Fixed M Students Finally, although not necessary for fitting our distillation scaling law, it is instructive to see how student cross entropies vary over as large a range as possible. To achieve this, we train fixed-M teacher fixed-M student combinations, with ten teachers with M T ‚âà 20, and students of five sizes, with at least four choices of M S per student. The resulting student cross-entropies for two of the students are in Figure <ref type="figure" target="#fig_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Capacity gap</head><p>In Figure <ref type="figure" target="#fig_3">4</ref>, we observe the capacity gap, where improving teacher performance does not always improve student performance, and even reduces student performance eventually. The capacity gap has been observed often in distillation (see <ref type="bibr">Appendix B.3</ref>). The KLD between teacher and student is an increasing function of teacher capability in all cases (see Appendix E.3), which means as the teacher improves its own performance, the student finds the teacher more challenging to model, eventually preventing the student from taking advantage of teacher gains. We use calibration metrics to investigate aspects that the student finds challenging to model in Appendix E.8. In Appendices C.1 and C.2 we offer a simple explanation in a kernel regression and synthetic Multi-Layer Perceptron (MLP) setting and, to the best our knowledge, are the first controlled demonstrations of the capacity gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distillation Scaling Law Functional Form</head><p>We need to determine the functional form of the distillation scaling law. First, we observe that contributions from teacher size N T and pretraining tokens D T are summarized by the teacher cross-entropy L T . This can be seen from Figures 1 and 3b which contains the IsoFLOP Teacher/Fixed M Students of Figure <ref type="figure" target="#fig_2">3</ref>, yet only smooth dependence as a function of L T is observed. Next, the distillation scaling law should reflect the following properties:</p><p>1. An infinitely capable student should be able to model any teacher:</p><formula xml:id="formula_11">lim N S ,D S ‚Üí‚àû L S (N S , D S , L T ) ‚Üí L T .</formula><p>2. A random teacher produces random students independent of how capable those students are:</p><formula xml:id="formula_12">lim L T ‚Üí‚àû L S (N S , D S , L T ) ‚Üí L T .</formula><p>3. There is a capacity gap: making a teacher too capable eventually reduces the student performance.</p><p>A transition between two power law regions: i) where the student is a stronger learner than the teacher, and ii) where the student is a weaker learner than the teacher is described by a broken power law <ref type="bibr" target="#b20">(Caballero et al., 2023)</ref>. Together, we propose that student cross-entropy follows a broken power law in L T and a power law in N S and D S :</p><formula xml:id="formula_13">L S (N S ,D S ,L T ) Student cross-entropy = L T Teacher cross-entropy + 1 L c0 T 1+ L T L S d 1 1/f1 -c1f1 A N Œ± ‚Ä≤ S + B D Œ≤ ‚Ä≤ S Œ≥ ‚Ä≤</formula><p>Student ability to mimic teacher <ref type="bibr">(8)</ref> where {c 0 , c 1 , d 1 , f 1 , Œ± ‚Ä≤ , Œ≤ ‚Ä≤ , Œ≥ ‚Ä≤ } are positive coefficients to be fitted following the procedure outlined in Appendix F.2 on the data produced in Section 4.2. The first two properties of our distillation scaling law can be readily checked.</p><p>For the third, recall, L S = L(N S , D S ) is the cross-entropy a student would have achieved if it had been trained in a supervised way (Table <ref type="table" target="#tab_0">1</ref>), and is determinable from the supervised scaling law (Equation <ref type="formula" target="#formula_0">1</ref>). The capacity gap behavior follows from a transition based on the ratio of the algorithmic learning capacities of the student and teacher, when L T / L S ‚â° L(N T , D T )/L(N S , D S ) = d 1 , which can be interpreted as measure of the relative learning abilities of the teacher and the student on a reference task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Distillation Scaling Law Parameteric Fit</head><p>We use the teachers (N T , D T ) for fitting our supervised scaling law (Appendix E.2), and all the data for fitting our distillation scaling law (Equation <ref type="formula">8</ref>). Our fitting procedure is described in detail in Appendix F and resulting scaling coefficients are presented in Appendix F.3. Our supervised and distillation scaling laws fit the observations at the level of ‚â≤ 1% relative prediction error, including when extrapolated from weaker to stronger models (see Figure <ref type="figure" target="#fig_5">5b</ref>).</p><p>1.8 2.0 2.2 2.4 2.6 2.8 Predicted Cross-Entropy L Extrapolation Fit data All L &gt; 2.2 1.8 2.0 2.2 2.4 2.6 2.8 Cross-Entropy L -1.0 -0.5 0.0 0.5 1.0 Prediction Error (%) (a) Supervised. 2.0 2.2 2.4 2.6 2.8 Predicted Student Cross-Entropy L S Extrapolation Fit data All LS &gt; 2.3 2.0 2.2 2.4 2.6 2.8  As a further verification, we confirm that for a fixed model size, distillation in the infinite data regime is consistent with supervised learning on infinite data (Appendix E.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Distillation scaling law applications</head><p>Here, we apply our distillation scaling law (Equation <ref type="formula">8</ref>) and investigate scenarios of interest. Typically, the resources in distillation pretraining include a compute budget, or a dataset containing a number of tokens. For a distillation process, the compute cost can be approximated by where Œ¥ Lgt T , Œ¥ Pre T ‚àà [0, 1] indicate if we account for the cost of teacher logit inference for the student targets<ref type="foot" target="#foot_5">foot_5</ref> , and teacher pretraining cost in the total compute budget (see Table <ref type="table" target="#tab_6">2</ref>). F (N ) is the number of Floating Operations (FLOPs) a model with N parameters performs during a forward pass. F (N ) ‚âà 2N is often used, giving supervised FLOPs ‚âà 6N D. We cannot use the 2N approximation, as i) using non-embedding parameters N induces systematic errors <ref type="bibr" target="#b81">(Porian et al., 2024)</ref>, and ii) we are interested in small models with large context sizes where the FLOP contribution from attention is significant. To resolve these issues. we derive a simple expression F (N ) ‚âà 2N (1+c 1 N -1/3 +c 2 N -2/3 ) for fixed-aspect ratio models Appendix H.1, and recommend the scaling community to consider adopting this hyperparameter setting.</p><formula xml:id="formula_14">FLOPs‚âà3F (N S )D S Student Training +F (N T )(Œ¥ Lgt T D S Teacher Logits +Œ¥ Pre T 3D T Teacher Training ) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fixed tokens or compute (best case)</head><p>To build intuition for when distillation may (and may not) be beneficial, we ask how well can distillation do in the best case scenario, compared with supervised learning? We superimpose the data of Figures 2 and 3 onto contours of distilled cross-entropy L S compared to a supervised model with the same resources L S (Figure <ref type="figure" target="#fig_6">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised learning always outperforms distillation</head><p>given enough student compute or tokens. For a modest token budget, distillation is favorable, however, when a large number of tokens are available, supervised learning outperforms distillation. This is expected; in the large data regime, supervised learning can find the best solution limited by model size N (Equation <ref type="formula" target="#formula_0">1</ref>), whereas distillation only finds this solution for the optimal teacher L * T (see Appendix E.6), and is otherwise limited by the distillation process. This finding appears to contradict the patient teacher finding of <ref type="bibr" target="#b10">Beyer et al. (2022)</ref>. A comment on this contradiction is provided in Appendix D.1. Student compute constrained version of Figure <ref type="figure" target="#fig_6">6</ref> and IsoFLOP Teacher/Fixed M student contours are provided in Appendix D.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Fixed tokens or compute (teacher inference)</head><p>Next, we focus on the common scenario of planning to distill, and trying to decide between an existing set of teachers</p><formula xml:id="formula_15">{(L (i) T , N<label>(i)</label></formula><p>T )} n i=1 . A larger teacher may provide a better learning signal (lower cross-entropy) but will also be more expensive to use because of the teacher logits cost (Equation 9, Œ¥ Lgt T = 1), inducing a trade-off. Given a target student size N S and budget D S or C Total , the only degree of freedom is the choice of teacher.</p><p>For a fixed data budget, as the student size increases, teacher cross-entropy should be decreased as a power law. Here, the compute cost from N T is not relevant as we are considering a token budget. Student cross-entropy at different distillation token budgets is shown in Figure <ref type="figure">7</ref>. An equivalent plot for different student sizes whilst varying tokens is shown in Appendix D.3. We see that the optimal teacher loss L * T (red line) decreases as a power law with student size N S until L S matches L * T , when there is an inflection point in L * T , causing the decrease of teacher loss to sharpen with N S . This generalizes the observation of <ref type="bibr">Zhang et al. (2023a)</ref>, that "Optimal teacher scale almost consistently follows a linear scaling with the student scale across different model architectures and data scales." which is a special case of our finding when the teachers are compute optimal (Figure <ref type="figure" target="#fig_30">36a</ref>). Note that our findings consistently show that teacher cross-entropy L T determines student cross-entropy L S , not N T itself (which leads to a</p><p>1.5 2.0 2.5 Student D S =250B 1. 75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2 .4 0 2.40 2.45 2.50 Student D S =1T 1.70 1.75 1.8 0 1.85 1.90 1.95 2.00 2.05 2.10 2. 15 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2 .4 0 2.40 2.45 2.50 1B 10B 100B 1.5 2.0 2.5 Student D S =4T 1.65 1.7 0 1.7 5 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2. 15 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2.40 2.45 2.50 1B 10B 100B Student D S =16T 1.6 5 1. 70 1.75 1. 80 1.85 1.90 1.95 2.00 2.05 2. 10 2.10 2 .1 5 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2.40 2.45 2.50 Student Parameters N S Teacher Loss L T Figure 7. Students given a teacher and token budget. For four distillation token budgets the student cross-entropy for a range of students and teachers. The red line indicates the optimal teacher cross-entropy L * T producing the lowest student cross-entropy.</p><p>given L T ). We investigate a fixed compute budget setting for teacher inference only in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Compute Optimal Distillation</head><p>We extend the analysis of <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref> to distillation, giving compute optimal distillation, determining how to produce the student of a desired size N S with the lowest cross-entropy given a compute budget</p><formula xml:id="formula_16">C D * S , N * T , D * T = arg min D S ,N T ,D T L S (N S , D S , N T , D T ) s.t. FLOPs = C,<label>(10)</label></formula><p>To present the best and worst case for incorporating teacher inference into the compute constraints, we consider all scenarios presented in Table <ref type="table" target="#tab_6">2</ref>. We also compare against the optimal supervised performance. To find the minima in Equation 10 we perform constrained numerical minimization using Sequential Least SQuares Programming (SLSQP) <ref type="bibr" target="#b57">(Kraft, 1988)</ref> in SciPy <ref type="bibr" target="#b97">(Virtanen et al., 2019)</ref>.</p><p>Supervised learning always matches optimal distillation at sufficient compute budget, with the intersection favoring supervised learning increasing as student size grows. In Figure <ref type="figure" target="#fig_7">8</ref> we see that supervised learning always matches the best case distillation setting at some total compute budget, as anticipated from the asymptotic analysis in Figure <ref type="figure" target="#fig_36">40</ref>. The compute transition point when supervised learning becomes preferable to distillation increases as a function of student size. See also Figure <ref type="figure" target="#fig_6">6</ref>. We also observe that smaller models are more likely to benefit from supervised pretraining, whereas larger models are more likely to benefit from distillation.</p><p>When teacher training is included in the compute, the best student cross-entropy is always higher than in the supervised setting. This means that if your only aim is to produce the best possible model of a target size and you do not have access to a teacher, then you should choose supervised learning, instead of training a teacher and then distilling. Conversely, if the intention is to distill into a family of models, or use the teacher as a server model, distillation may be more computationally beneficial than supervised learning. On reflection, this finding should be expected, otherwise it would imply that for a total amount of compute, distillation can outperform direct maximum likelihood optimization. A detailed discussion of the compute 2.3 2.4 2.5 Student N S =300M 2.2 2.4 Student N S =1B 10 20 10 22 10 24 10 26 2.0 2.2 2.4 2.6 Student N S =3B 10 20 10 22 10 24 10 26 1.8 2.0 2.2 2.4 2.6 Student N S =10B Total Compute (FLOPs) Student Cross Entropy L S Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Supervised For four student sizes , the best cross-entropy each student can achieve the five scenarios considered as total compute is varied. optimal configurations that produce (N * S , N * T , D * T ) for all scenarios is discussed in Appendix D.4.</p><p>To build intuition for how quantities play off against eachother, we take the most complex scenario, teacher pretraining + inference. A view of the optimal distillation setup as compute varies is presented in Figure <ref type="figure">9</ref>. Student and teacher tokens scale as a power law, with student tokens at a faster rate. Optimal teacher size increases initially until it is slightly larger than the student, after which it plateaus. This plateau occurs because inference with large teachers is expensive, and with the increase in number of student tokens, overtraining the teacher becomes more efficient.</p><p>The values in Figure <ref type="figure">9</ref> can be recombined to produce the compute terms in Equation 9 as shown in Appendix D.4, Figure <ref type="figure">29</ref>. We summarize the trend in Table <ref type="table" target="#tab_10">3</ref>.</p><p>10 10 10 12 10 14 10 16 Student N S =300M Student N S =1B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =3B 10 20 10 22 10 24 10 26 Student N S =10B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 9. Teacher pretraining + inference. For four student sizes, the optimal student and teacher configurations when teacher logit inference and teacher pretraining cost is accounted for. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. We then used our law to study practical distillation scenarios of interest, and showed that distillation is only more efficient than supervised learning if: i) the total compute or tokens used for distillation is not larger than a student size-dependent threshold, and ii) a teacher already exists, or the teacher to be trained has uses beyond single distillation. Moreover, we use this law to determine optimal distillation scenarios that are able to outperform supervised learning, enabling practitioners to select the best teacher for their use case. This work represents the largest controlled empirical study of distillation we are aware of, with systematic ablations of common distillation techniques. Just as supervised scaling has mitigated risks in supervised pretraining, our findings offer a roadmap for producing smaller, more powerful models with lower inference costs, reducing carbon footprints, and enhancing the feasibility of test-time scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This work shows how to apply the framework of scaling laws to the distillation setting, investigating distillation as a viable alternative to the overtraining paradigm for producing capable language models. The work explains when distillation should and should not be performed, from a compute efficiency perspective, compared to supervised learning. There are a number of benefits to this:</p><p>1. As compute-optimal recipes for distillation are now known, there is greater opportunity for producing powerful models with lower inference costs. Lowering inference costs lower the largest component of language model training carbon footprint.</p><p>2. When combined with other known scaling laws, there is a larger space of models for which we know compute-optimal configurations. To produce models with a given capability, the compute, hardware and climate costs have now been reduced compared to before, as the optimal recipe is known.</p><p>3. Our distillation scaling law lowers compute usage through removing unnecessary experimentation over various hyperparameters and distillation settings. We now understand that the primary driver of student cross-entropy is teacher cross-entropy, and so teacher size and tokens can be discarded as axes to search over.</p><p>4. Small powerful models democratize the study of models with significant capabilities, enabling the involvement of a greater number of perspectives to study model capabilities and safety aspects.</p><p>There are however, potential negative consequences:</p><p>1. Using distillation as part of a training pipeline introduces new sources of bias. Teacher models may contain bias from their pretraining data. Even if a student is distilled on data that is unbiased, the bias of the teacher will be inherited by the student.</p><p>2. Small powerful language models are more efficient during inference, reducing the amount of resources needed for bad actors to achieve their goals, such as generating targeted misinformation at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A Limitations B Extended background B.1 Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Neural Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 The Knowledge Distillation Capacity Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C Teacher Student Capacity Gaps C.1 Kernel Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.2 Distilling the Teacher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.3 U-shape in the student error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 MLPs on the Mapping Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.2 Experimental Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D Distillation scaling law applications (additional results) D.1 A contradiction with patient teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Fixed tokens or compute (best case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Fixed size or compute (teacher inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Compute optimal distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.2 Cross-entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.3 Distillation (best case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations</head><p>This work has several limitations that we are aware of:</p><p>‚Ä¢ Our work is performed in the language modeling setting only. Although there is good evidence that the functional form of scaling laws applies across domains <ref type="bibr" target="#b43">(Henighan et al., 2020)</ref>, we cannot be absolutely certain that distillation behaves in the way we describe in this work in all domains.</p><p>‚Ä¢ We perform our analysis on the English subset of C4 dataset (see Appendix I). This means that for our larger token runs, data has been repeated. Although it was shown in <ref type="bibr">Muennighoff et al. (2023b)</ref> that on the C4 dataset, repeating data up to 4 times has negligible impact to loss compared to having unique data, this was shown in the supervised setting, and we cannot be absolutely certain that the same applies in the distillation setting.</p><p>‚Ä¢ A second downside of using the C4 dataset is that we are limited in our ability to analyze downstream evaluations of students resulting from distillation. Our performance over standard English language downstream tasks closely follows cross-entropy, however, C4 is not as well suited for pretraining in order to probe aspects like reasoning performance (see Appendix E.1).</p><p>‚Ä¢ We focused on distillation as originally defined in <ref type="bibr" target="#b45">Hinton et al. (2015)</ref>, where the teacher produces a full probability distribution for the student to target. More colloquially, distillation has become used to describe the more general process of using a teacher in order to produce a student. One popular approach for training language models is Sequence-Level Knowledge Distillation <ref type="bibr" target="#b55">(Kim &amp; Rush, 2016)</ref> where the teacher is sampled, e.g. with beam search, in order to produce sequences for training the student on in a supervised way. This technique, also called synthetic data or hard distillation has been employed to great effect in the LLaMA families <ref type="bibr">(Touvron et al., 2023a)</ref> and most recently, the smaller models distilled from DeepSeek-R1 <ref type="bibr">(DeepSeek-AI et al., 2024)</ref>. While we anticipate that our broader findings also apply in the Sequence-Level Knowledge Distillation, we cannot be absolutely sure. We suggest that verifying the scaling properties of Sequence-Level Knowledge Distillation in a controlled, resource constrained manner as we have done here is important for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extended background</head><p>This section reviews related work on knowledge distillation, capacity gap phenomena, neural scaling laws, and foundation models, highlighting their relevance to our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Knowledge Distillation</head><p>Bucila <ref type="bibr">et al. (2006)</ref> provided strong evidence that the knowledge gained by a large ensemble of models can be effectively transferred to a single smaller model. Later, <ref type="bibr" target="#b45">Hinton et al. (2015)</ref> introduced knowledge distillation, where a smaller student network learns from a larger teacher network by mimicking its softened output probabilities, improving efficiency and generalization. Building on this, <ref type="bibr" target="#b92">Stanton et al. (2021)</ref> studied both fidelity and student generalization, showing that while knowledge distillation often improves generalization, it frequently fails to achieve high fidelity, as student models do not fully match the teacher's predictive distribution. We study fidelity in terms of calibration in Appendix E.8, and show that when the learning signal is consistent with the calibration measure, then the student in our setup is well-calibrated both with respect to the teacher and the actual data. Addressing this, <ref type="bibr" target="#b10">Beyer et al. (2022)</ref> demonstrated that knowledge distillation is most effective when the teacher is patient and consistent, providing stable targets over prolonged training to improve student generalization and fidelity. Our Language Model (LM) setup automatically satisfies consistency: both the teacher and student see the same data during the student's training. However, our conclusions differ from those of <ref type="bibr" target="#b10">Beyer et al. (2022)</ref> in that although distilling a student for longer does improve its performance, unless the teacher is chosen perfectly, distillation becomes less effective than supervised learning in the patient setting, see Appendix D.2 for a discussion.  <ref type="formula">2024</ref>) and studied in LMs, was further analyzed by <ref type="bibr">Ildiz et al. (2024)</ref>, who extended the theoretical analysis to high-dimensional data and over-parameterized regression. Their findings show that distillation can provably outperform training with strong labels under the same data budget but does not improve the data scaling law. Our distillation scaling law (Equation <ref type="formula">8</ref>) confirms this finding, which for a fixed teacher cross-entropy does not improve the scaling law compared to the supervised one in Equation <ref type="formula" target="#formula_0">1</ref>. Moreover, in many previous works, distillation happens with repeated data, that is, the student sees the same data as the teacher does during its training. In our setup, we do not repeat the data between teacher training and distillation, which allows us to examine only the effect of distillation rather than the possible diminishing returns of repeated data; see <ref type="bibr">Muennighoff et al. (2023a)</ref> for more details on the effect of repeating data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Neural Scaling Laws</head><p>Predictable scaling trends in neural networks were first empirically observed by Hestness et al. ( <ref type="formula">2017</ref>) and later by <ref type="bibr" target="#b54">Kaplan et al. (2020)</ref> who established empirical scaling laws for language model performance based on cross-entropy, which led to <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref> and the pursuit of compute-optimal training. Beyond the empirical studies, there have been many theoretical works which provide explanations for why scaling laws should exist <ref type="bibr" target="#b6">(Bahri et al., 2021;</ref><ref type="bibr" target="#b76">Paquette et al., 2024;</ref><ref type="bibr">Havrilla &amp; Liao, 2024)</ref>. More recent works explore scaling laws across different distributions, closely related to knowledge distillation. <ref type="bibr" target="#b44">Hernandez et al. (2021)</ref> derived a scaling law for transfer learning, analyzing effective data transfer in low-data regimes and diminishing returns in high-data regimes. Similarly, <ref type="bibr" target="#b7">Barnett (2024)</ref> empirically studied pretraining on one distribution for optimizing downstream performance on another, showing that when the transfer gap is low, pretraining is a cost-effective strategy. Finally, Jain et al. ( <ref type="formula">2024</ref>) theoretically analyze how additional data from a surrogate model affects generalization, demonstrating that surrogate data can reduce test error-even when unrelated-due to Stein's paradox <ref type="bibr" target="#b93">(Stein, 1956)</ref>, with test error following a scaling law. This setup is related to tuning the coefficient Œª in our case, where we also observe a U-shape behavior depending on the teacher and student sizes (see Figure <ref type="figure" target="#fig_5">51a</ref>). However, we are interested in studying the effect of distillation only (Œª = 1.0), which differs from their setup. While these works are closely related to knowledge distillation-since one can compare the distribution of the teacher logits to that of the student-they do not establish a distillation scaling law. Moreover, their setup differs from practical knowledge distillation, as it does not involve training a new student model using a teacher but instead studies the effect of transferring training knowledge to a downstream task. Our work is the first to determine and verify a distillation scaling law and examine the regions where one should distill as well as the regions where supervised pretraining outperforms distillation; see Figures 6, 7 and 14 in Appendix D.2 and Section 5.2. Finally, for improving inference cost at a given model capability, the scaling behavior of Mixture of Experts (MoE) <ref type="bibr" target="#b88">(Shazeer et al., 2017;</ref><ref type="bibr">Jelassi et al., 2024)</ref> have been investigated in the context of scaling laws <ref type="bibr" target="#b26">(Clark et al., 2022;</ref><ref type="bibr" target="#b62">Ludziejewski et al., 2024;</ref><ref type="bibr" target="#b2">Abnar et al., 2025)</ref> as one alternative to knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. The Knowledge Distillation Capacity Gap</head><p>Despite extensive research on knowledge distillation, a persistent challenge is the curse of capacity gap, where a larger teacher does not necessarily produce a superior student compared to a smaller teacher. This occurs because a large gap in model capacity makes it harder for the student to effectively learn from the teacher's outputs. As a result, there exists an optimal teacher size along the scaling trajectory that maximizes student performance. Our distillation scaling law in Equation 8 confirms this, revealing a u-shaped trend in the scaling law and validating the existence of an optimal teacher. However, our results further indicate that the capacity gap is influenced not only by the size of the teacher but also by its training tokens and, more generally, its loss. A theoretical analysis in the kernel regression setup (Appendix C) supports these findings. <ref type="bibr" target="#b63">Lukasik et al. (2022)</ref> showed that distillation gains are not uniform and can even degrade performance when small teacher errors are amplified by the student. Similarly, <ref type="bibr" target="#b72">Nagarajan et al. (2023)</ref> found that deviations in predictive probabilities cause students to exaggerate the teacher's confidence levels. Several works <ref type="bibr">(Peng et al., 2024;</ref><ref type="bibr">Zhang et al., 2023a;</ref><ref type="bibr" target="#b81">Rawat et al., 2024)</ref> observed the capacity gap in pre-training distillation for Large Language Model (LLM)s, affecting both large-to-small and small-to-large distillation. Notably, <ref type="bibr">Zhang et al. (2023a)</ref> proposed an empirical law of the capacity gap, showing that the optimal teacher scale follows an approximately linear relationship with the student's scale. However, our findings suggest that scaling alone is insufficient-one must account for the complexity of the effective hypothesis space (Equation <ref type="formula">8</ref>) and we show that <ref type="bibr">Zhang et al. (2023a)</ref> is a special case of our work when the teachers are compute-optimal from a supervised perspective (see Section 5.3). To address this issue, various strategies have been explored. <ref type="bibr">Yuan et al. (2024)</ref> studied temperature scaling, which simplifies the teacher's output into more learnable representations, aiding student generalization. We analyzed the effect of temperature and learning rate in distillation (Figures <ref type="figure" target="#fig_54">52</ref> and <ref type="figure" target="#fig_55">53</ref>) and found that, contrary to existing literature, the optimal temperature is one. We hypothesize that this discrepancy arises because previous studies used repeated tokens, whereas our setup does not involve repeated data. Additionally, <ref type="bibr" target="#b23">Cho &amp; Hariharan (2019)</ref> found that early stopping of the teacher's training mitigates the capacity gap, while <ref type="bibr" target="#b66">Mirzadeh et al. (2020)</ref> proposed progressive distillation, where knowledge is transferred through intermediate models to improve student learning. From a theoretical perspective, <ref type="bibr" target="#b38">Harutyunyan et al. (2023)</ref> analyzed the capacity gap in distillation using supervision complexity in kernel classifiers. Their findings highlight a trade-off between teacher accuracy, student margin with respect to teacher predictions, and teacher complexity, explaining why some teachers are easier for the student to learn. Earlier, Lopez-Paz et al. ( <ref type="formula">2016</ref>) studied generalization error in distillation, proving that learning from a teacher can be beneficial under certain conditions, particularly when the teacher's capacity is small. Using similar techniques in LMs, <ref type="bibr">Zhang et al. (2023b)</ref> demonstrated that among students of different capacities distilled from the same teacher, smaller students suffer from higher generalization error and lower performance, while larger teachers provide lower generalization error, reinforcing the trade-off in teacher-student capacity. Our distillation scaling law (Equation <ref type="formula">8</ref>) also confirms this trend, and we observe the effect of capacity gap in our scaling law terms, see Section 4.3 for more details.</p><p>Foundation model pretraining Foundation models were initially undertrained <ref type="bibr" target="#b17">(Brown et al., 2020)</ref>, then followed the compute-optimal scaling law carefully <ref type="bibr" target="#b26">(Hoffmann et al., 2022;</ref><ref type="bibr" target="#b79">Pearce &amp; Song, 2024;</ref><ref type="bibr">Besiroglu et al., 2024)</ref>, and soon after started overtraining heavily <ref type="bibr" target="#b86">(Sardana et al., 2024;</ref><ref type="bibr">Bi et al., 2024;</ref><ref type="bibr">Hu et al., 2024;</ref><ref type="bibr">Mesnard et al., 2024;</ref><ref type="bibr" target="#b52">Jiang et al., 2023)</ref>. The LLaMA family <ref type="bibr">(Touvron et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr">Dubey et al., 2024)</ref> and Phi line <ref type="bibr">(Li et al., 2023;</ref><ref type="bibr">Abdin et al., 2024b;</ref><ref type="bibr">a)</ref> is following the same trend, where smaller models are overtrained according to the original Chinchilla scaling laws. In all these cases, the models are designed to be best possible foundation model that is still cheap and fast to run on lower end hardware. Besides overtraining, more recently, smaller foundation models tend to be distilled from larger models <ref type="bibr">(Gunter et al., 2024;</ref><ref type="bibr">Rivi√®re et al., 2024;</ref><ref type="bibr">Reid et al., 2024)</ref> to further increase performance. In these cases, the large model either specifically trained with the sole purpose of being a distillation teacher, or an existing model is re-used. In both these cases, there are no reports of how the exact teacher size is decided when taking total compute into account. Determining the optimal allocation of compute budget in the distillation setting is one of the primary contributions of our work (see Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Teacher Student Capacity Gaps</head><p>In this section, we examine the capacity gap in two settings: kernel regression and a synthetic example using Multi-Layer Perceptron (MLP) for a mapping problem. The kernel regression setup provides a theoretical and analytically tractable perspective on the capacity gap. The MLP-based synthetic example allows us to study the capacity gap in a more practical, learnable function approximation scenario. By analyzing these two setups, we aim to better understand the fundamental limitations of distillation when there is a significant mismatch between teacher and student capacities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Kernel Regression</head><p>One of our main contributions is that the student loss follows a broken power law, where the transition between the two power law regions occur when the student becomes a stronger learner than the teacher (Equation <ref type="formula">8</ref>). This implies that making the teacher too capable (relative to the student) reduces student performance. In this section we show how a capacity gap provably degrades student performance in the setting of kernel regression. While simple, we believe the underlying principle causing the student performance degradation in this case carry over to much more general settings involving neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1. SETUP</head><p>Let H denote a Hilbert space spanned by orthonormal bases functions</p><formula xml:id="formula_17">{œï i } ‚àû i=1 such that ‚ü®œï i , œï j ‚ü© H = Œ¥ ij . Let f * ‚àà H denote the target function, identified by a set of coefficients Œ± = {Œ± i } ‚àû i=1 ‚àà R, ‚à•Œ±‚à• = M &lt; ‚àû such that: f ‚ãÜ (x) = ‚àû i=1 Œ± i œï i (x).<label>(11)</label></formula><p>Let H m t , H n s denote the teacher and student Hilbert spaces respectively:</p><formula xml:id="formula_18">H m t = Span{œï 1 , œï 2 , ..., œï m },<label>(12)</label></formula><formula xml:id="formula_19">H n s = Span{œï 1 , œï 2 , ..., œï n },<label>(13)</label></formula><p>which are the hypothesis spaces of the teacher and student. Note that while the Hilbert space H is spanned by an infinite orthonormal basis, the teacher and student spaces are finite and spanned by m and n basis functions respectively, where |m -n| represents the teacher and student capacity gap.</p><p>The process of training the teacher and student models involves solving the following constrained optimization problems:</p><formula xml:id="formula_20">g ‚ãÜ = min g‚ààH m t ‚à•g -f ‚ãÜ ‚à• H s.t ‚à•g‚à• H ‚â§ T,<label>(14)</label></formula><formula xml:id="formula_21">h ‚ãÜ = min h‚ààH n s ‚à•h -g ‚ãÜ ‚à• H s.t ‚à•h‚à• H ‚â§ D,<label>(15)</label></formula><p>where g ‚ãÜ , h ‚ãÜ are the optimal teacher and student respectively, and D ‚â§ T &lt; M . Note that we assume the teacher and student are exposed to an infinite amount of training data, hence our analysis is carried over entirely in function space.</p><p>Lemma C.1. The optimal teacher g ‚ãÜ is given by:</p><formula xml:id="formula_22">g ‚ãÜ (x) = C(m, T ) m i=1 Œ± i œï i (x), C(m, T ) = 1 m i=1 Œ± 2 i ‚â§ T T ‚àö m i=1 Œ± 2 i otherwise. (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>The teacher error e ‚ãÜ teacher (m, T ) is given by:</p><formula xml:id="formula_24">e ‚ãÜ teacher (m, T ) = ‚à•g ‚ãÜ -f ‚ãÜ ‚à• H = (C(m, T ) -1) 2 m i=1 Œ± 2 i + ‚àû i=m+1 Œ± 2 i . (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>Proof. By construction we may assume the teacher model takes the form g ‚ãÜ = m i=1 Œ≤ i œï i . where m i=1 Œ≤ 2 i ‚â§ T . We can write the error of g ‚ãÜ using:</p><formula xml:id="formula_26">e teacher (m, T, Œ≤) = m i=1 (Œ≤ i -Œ± i )œï i + ‚àû i=m+1 Œ± i œï i H = m i=1 (Œ≤ i -Œ± i ) 2 + ‚àû i=m+1 Œ± 2 i . (<label>18</label></formula><formula xml:id="formula_27">)</formula><p>Note that the minimizing coefficients Œ≤ ‚ãÜ of Equation 18 must take the form Œ≤ = CŒ± for some coefficient C. Considering the norm constraint on g, the constant C takes the form in Equation <ref type="formula" target="#formula_22">16</ref>. Plugging the resulting g ‚ãÜ into the expression for e teacher (m, T, Œ≤ ‚ãÜ ) completes the proof.</p><p>Notably and intuitively, the teacher error decreases monotonically as m, which represents the teacher model capacity, increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2. DISTILLING THE TEACHER</head><p>We now pick our student function h ‚ãÜ by mimicking the teacher subject to a norm constraint:</p><formula xml:id="formula_28">h ‚ãÜ (x) = min h‚ààH n t ‚à•h -g ‚ãÜ ‚à• H s.t. ‚à•h‚à• H ‚â§ D. (<label>19</label></formula><formula xml:id="formula_29">)</formula><p>Lemma C.2. Let k = min(m, n) be the smaller of the teacher and student capacities. The optimal student h ‚ãÜ is given by:</p><formula xml:id="formula_30">h ‚ãÜ = Q(m, k, T, D)C(m, T ) k i=1 Œ± i œï i (20) Q(m, k, T, D) = Ô£± Ô£≤ Ô£≥ 1 C(m, T ) k i=1 Œ± 2 i &lt; D D C(m,T ) ‚àö k i=1 Œ± 2 i otherwise. (<label>21</label></formula><formula xml:id="formula_31">)</formula><p>The student error with respect to the target function is then:</p><formula xml:id="formula_32">e student (m, n, T, D) = ‚à•h ‚ãÜ -f ‚ãÜ ‚à• H = (C(m, T )Q(m, k, T, D) -1) 2 k i=1 Œ± 2 i + ‚àû i=k+1 Œ± 2 i (22)</formula><p>Proof. The proof follows the exact same logic as in Lemma C.1. i.e, we can assume the optimal student is given by h ‚ãÜ = n i=1 Œ≥ i œï i . From the distillation loss, the optimal coefficients must match the teacher coefficients for the basis functions {œï i } n i=1 , perhaps rescaled due to the norm constraint n i=1 Œ≥ 2 i ‚â§ D. This rescaling then gives rise to the additional Q(m, k, T, D) multiplier in Equation <ref type="formula" target="#formula_30">21</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.3. U-SHAPE IN THE STUDENT ERROR</head><p>We will prove that the map m -‚Üí e student (m, n, T, D) is comprised of two distinct segments: i) where the student error monotonically decreases for m &lt; n, and ii) where it monotonically increases for m ‚â• n, establishing a U-shape in the student error echoing the trend seen in Figures <ref type="figure" target="#fig_2">3</ref> and <ref type="figure" target="#fig_3">4</ref>. In words, when m &lt; n, the error does not increase (and typically decreases) as the teacher capacity m increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>Let H m,T t ‚äÜ H m t denote the space of functions in H m t that are norm constrained by D. i.e:</p><formula xml:id="formula_33">H m,T t = {f ‚àà H m t : ‚à•f ‚à• H ‚â§ T }. (<label>23</label></formula><formula xml:id="formula_34">) Since H m,T t ‚äÜ H m+1,T t , it follows that g ‚ãÜ m ‚àà H m+1,T t</formula><p>, which implies that the teacher error cannot increase as m increases, hence it monotonically decreases. Now, let h ‚ãÜ m denote the optimal student given the teacher g ‚ãÜ m . Since D ‚â§ T , then for any m &lt; n, we can equivalently write the optimal student h ‚ãÜ m as the solution to the following optimization problem:</p><formula xml:id="formula_35">‚àÄ m‚â§n h ‚ãÜ m = min h‚ààH n s ‚à•h -g ‚ãÜ m ‚à• H s.t ‚à•h‚à• H ‚â§ D (24) = min h‚ààH m t ‚à•h -f ‚ãÜ ‚à• H s.t ‚à•h‚à• H ‚â§ D,<label>(25)</label></formula><p>which corresponds exactly to the objective of finding the optimal teacher with with a norm constraint set to D. Therefore, from the fact that the teacher error monotonically decreases we can conclude that the student error monotonically decreases as well in the regime m &lt; n.</p><p>Case 2: m ‚â• n. (Student error eventually increases in m)</p><p>Claim. For m ‚â• n: e student (m + 1, n, T, D) ‚â• e student (m, n, T, D). Hence once m exceeds n the student error cannot decrease any further, the error eventually starts to rise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>Let Œ≤ ‚ãÜ m = {Œ≤ 1 , ..., Œ≤ m } denote the coefficients of the optimal teacher g ‚ãÜ m . Note that in the regime m ‚â• n, as long as n i=1 Œ≤ 2 i ‚â• D (i.e the norm of the coefficients corresponding to the basis {œï 1 , ..., œï n } is smaller than D), we have from Equation <ref type="formula" target="#formula_30">21</ref>that Q(m, k, T, D) = 1, which means that the optimal student doesnt change, hence its error remains constant. If however n i=1 Œ≤ 2 i &lt; D, then we have from Equation <ref type="formula" target="#formula_30">21</ref>:</p><formula xml:id="formula_36">1 &gt; Q(m, k, T, D) ‚â• Q(m + 1, k, T, D),<label>(26)</label></formula><p>where the second inequality becomes strict if</p><formula xml:id="formula_37">Œ± 2 m+1 &gt; 0. A strict inequality (i.e Q(m, k, T, D) &gt; Q(m + 1, k, T, D))</formula><p>implies the optimal student is further scaled down due to the teacher having to "spread its capacity" to additional basis functions that are not learnable by the student, thereby strictly increasing its error. Hence for m ‚â• n, we get Therefore, as a function of m, the student error e student (m, n, T, D) first decreases and then increases (for m ‚â• n) (for m ‚â§ n), giving a u-shape in student error due to a capacity gap between the teacher and the student.</p><p>We present an empirical verification of these conclusions in Figure <ref type="figure" target="#fig_12">10</ref>. As can be seen, the student error exhibits a U shaped error curve as predicted by the theory, where the error starts to increase when m ‚â• n. The black solid line indicates the teacher error, which always decreases with increasing m.</p><p>The above theoretical analysis points to an intuitive interpretation of the potentially adverse effect of a large teacher-student capacity gap; the degradation in student performance is due to the teacher learning basis functions that are unreachable by the student, at the expense of basis functions that are reachable by the student. In the following we provide empirical evidence in support of this picture in a controlled yet more realistic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. MLPs on the Mapping Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1. PROBLEM DEFINITION</head><p>Here we show a synthetic setting which exhibits the U-shape phenomenon. Matching the kernel regression analysis (Appendix C.1), we find that the synthetic problem must include a class of problems that are easy for the student to learn, and ones that are harder, in order for the u-shape to appear.</p><p>The problem setting is the Mapping Problem, and is similar in spirit to Pointer Value Retrieval <ref type="bibr" target="#b112">(Zhang et al., 2021)</ref>, Here, the input is composed of small integers in {0,1,2}. The label for each sample is given by the code below, which shows the two cases: i) one where the label is simply given by a one-hot position, and ii) one where the label is given by the location of a matching element in the context portion of the input.</p><p>def find(vector, value): """Find locations of value in vector.""" return np.where(vector == value) <ref type="bibr" target="#b116">[0]</ref> def remove(vector, value): """Find value from vector.""" return np.delete(vector, find(vector, value))</p><p>def label(vector: np.ndarray, num_classes: int) -&gt; np.ndarray: """Return the label in [0, num_classes) for vector.""" assert len(vector</p><formula xml:id="formula_38">) == 2 * num_classes one_hot = vector[num_classes:] context = vector[:num_classes] i = find(one_hot, 1) if context[i] == 0: return i else: # remapping c = context[i] return remove(find(context, c), i)</formula><p>Examples: <ref type="figure">----------------------------</ref></p><formula xml:id="formula_39">- 2020210001000000, label = 1 context [2 0 2 0 2 1 0 0] one-hot [0 1 0 0 0 0 0 0] ----------------------------- 1210120000000100, label = 2 context [1 1 2 0 1 2 0 0] one-hot [0 0 0 0 0 1 0 0] ----------------------------- 0122221201000000, label = 6 context [0 1 2 2 2 2 1 2] one-hot [0 1 0 0 0 0 0 0] -----------------------------</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2. EXPERIMENTAL FINDINGS</head><p>We train MLPs with two hidden layers of equal width, all non-linearities are Rectified Linear Units (ReLUs). Teachers and students of different sizes are produced by varying the hidden layer width only.</p><p>All model are trained with Adam <ref type="bibr" target="#b56">(Kingma &amp; Ba, 2015)</ref> using a peak learning rate of 3 √ó 10 -4 , a single cycle cosine learning rate schedule with a linear warmup of 5% of the total training steps. A batch size of 512 is used for all models. Training samples are never repeated. Unless explicitly stated, model are trained on 500 √ó 512, or 20N samples, where N is the number of model parameters, whichever is larger.</p><p>In Figure <ref type="figure" target="#fig_13">11</ref>, we look at varying the size of the teacher. For the width 256 model, student performance improves as the teacher size increases to a point, and then student performance worsens. This is observable in both the student cross-entropy (Figure <ref type="figure" target="#fig_13">11a</ref>) and accuracy (Figure <ref type="figure" target="#fig_13">11b</ref>). Aligning with theory and large-scale experiments, the student cannot learn if it is too small, and can learns to match the teacher model when ther student is large enough. In the intermediate regime, where distillation is often used, we see an optimal teacher size and a capacity gap phenomenon. In Figure <ref type="figure" target="#fig_0">12</ref>, a similar effect can be seen, when a large teacher (d ffn = 512) is trained with on different amounts of data. This observation aligns with the idea that it is the teacher's completeness in modeling the problem that eventually harms the performance of a student with lesser capacity, and not only the teacher size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Fixed size or compute (teacher inference)</head><p>Fixed student size For a fixed student size, as the number of student tokens increases, the optimal teacher cross-entropy decreases slightly; see Figure <ref type="figure" target="#fig_5">15</ref>. This observation highlights an asymmetry between the growth of student size and student tokens (or their rates in the scaling law), as the behavior here differs from that observed in Section 5.1. Notably, when the student size is sufficiently large, such as N S = 30B, increasing the student tokens initially leads to a decrease in the teacher's loss, followed by a saturation point and a slow decrease in the optimal teacher's loss.</p><p>1.5 2.0 2.5 Student N S =1B 2.10 2.15 2.20 2.20 2.25 2.25 2.30 2.30 2.35 2.35 2.40 2.40 2.45 2.45 2.50 2.50 2.55 Student N S =3B 1.95 2.00 2.05 2.05 2.10 2.10 2.15 2.15 2.20 2.20 2.25 2.25 2.30 2.35 2.40 2.45 2.50 250B 1T 4T 16T 1.5 2.0 2.5 Student N S =10B 1.85 1.90 1.95 1.95 2.00 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 250B 1T 4T 16T Student N S =30B 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 Student Tokens D S Teacher Loss L T Figure 15. Student performance given a teacher varying distillation tokens. For four distillation student sizes NS ‚àà {1B, 3B, 10B, 30B} the validation loss achieved by a students distilled on DS ‚àà [250B, 16T ] tokens under a teacher with loss LT ‚àà [E, 2.5]. The red line indicates the value of the teacher loss resulting in the best performing student, and the vertical dashed line indicates the number of tokens at which supervised pretraining outperforms distillation.</p><p>Fixed compute budget Given an inference budget N S , a set of teachers {(L</p><formula xml:id="formula_40">(i) T , N<label>(i)</label></formula><p>T )} n i=1 and a total compute budget C Total , the number of distillation tokens is determined from Equation 9</p><formula xml:id="formula_41">D S = C Total /(3F (N S ) + Œ¥ T-Logits F (N T )),<label>(27)</label></formula><p>where F (N ) is the forward Floating Operations (FLOPs) per token of a model of size N (see Appendix H). If Œ¥ T-Logits = 0 then there is no price to pay for a larger teacher, and the conclusions are identical to those of the fixed token analysis of Section 5.2. In the worst case scenario, Œ¥ T-Logits = 1, then using a larger teacher will mean fewer distillation tokens are available for the student. Due to the capacity gap phenomenon, at small compute budgets, this means it is actually better to use a large weak teacher rather than a large strong teacher. Once compute is sufficient to allow enough distillation tokens, a stronger teacher can be used for all student sizes (see Figure <ref type="figure" target="#fig_6">16</ref>).  </p><formula xml:id="formula_42">D * S , N * T , D * T = arg min D S ,N T ,D T L S (N S , D S , N T , D T ) s.t. FLOPs(N S , D S , N T , D T ) = C,<label>(28)</label></formula><p>where L S (N S , D S , N T , D T ) is the distillation scaling law (Equation <ref type="formula">8</ref>), and</p><formula xml:id="formula_43">FLOPs(N S , D S , N T , D T ) ‚âà 3F (N S )D S Student Training +F (N T )(Œ¥ Lgt T D S Teacher Logits + Œ¥ Pre T 3D T Teacher Training )<label>(29)</label></formula><p>is the total number of floating operations performed in the entire distillation setup. F (N ) is the forward FLOPs per token of a model of size N (see Appendix H), and Œ¥ Lgt T , Œ¥ Pre T ‚àà [0, 1] indicate if we account for the cost of teacher logit inference for the student targets and teacher pretraining cost in the total compute budget. For convenience, we restate our compute scenarios of interest in Table <ref type="table" target="#tab_13">5</ref>). Constrained numerical minimization using Sequential Least SQuares Programming (SLSQP) <ref type="bibr" target="#b57">(Kraft, 1988)</ref> in SciPy <ref type="bibr" target="#b97">(Virtanen et al., 2019)</ref>. We allow numerical solutions for model sizes and tokens N T , D S , D T ‚àà [1M, 100P ]. While this token upper-limit is larger than available resources <ref type="bibr" target="#b34">(Epoch AI, 2023)</ref>, it simplifies discussions when comparing to supervised learning at large compute budgets, which otherwise, for smaller students, would only by using a fraction of the available compute.</p><p>We begin by looking at the student cross-entropy achievable in each compute scenarios alongside the corresponding teacher cross-entropies in Appendix D.4.2. We then investigate the compute-optimal distillation configurations for each scenario that produce those cross-entropies. We look at best case distillation in Appendix D.4.3, teacher inference in Appendix D.4.4, teacher pretraining in Appendix D.4.5, and teacher pretraining + inference in Appendix D.4.6. Finally, to aid comparisons across methods, we present the token and parameter configurations for all methods in Appendix D.4.7 and Appendix D.4.8 respectively. For completeness, in the following sections, some of the findings of Section 5.3 are restated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4.2. CROSS-ENTROPY</head><p>In Figure <ref type="figure" target="#fig_16">17</ref> we show the student cross-entropies achieved in the compute optimal case for each scenario in Table <ref type="table" target="#tab_13">5</ref>, and the teacher cross-entropies that enable those student cross-entropies in Figure <ref type="figure" target="#fig_7">18</ref>.</p><p>Distillation and supervised learning produce the same student at large compute. The first thing to note in Figure <ref type="figure" target="#fig_16">17</ref> is that at low compute, in the best case and teacher inference scenarios, distillation outperforms supervised learning, consistent with our expectations from distillation and the existing literature (see Appendix B.1). However, once enough the compute is large enough 6 , distillation and supervised learning produce models with the same cross-entropy, i.e. in general, distillation does not allow us to produce better models that supervised learning does, however, distillation does produce better models than supervised learning with modest resources. This behavior is consistent with the asymptotic analysis in Appendix E.6, and can be understood through noting that although distillation modifies the learning process the student undergoes, distillation does not alter the hypothesis space of the student, which is tied to the student size N S , is the same hypothesis space in the supervised and distillation settings, and can be explored in the limit of infinite compute or data.</p><p>2.3 2.4 2.5 Student N S =300M 2.2 2.3 2.4 2.5 Student N S =500M 2.2 2.4 Student N S =1B 2.0 2.2 2.4 2.6 Student N S =3B 10 20 10 22 10 24 10 26 2.0 2.2 2.4 2.6 Student N S =5B 10 20 10 22 10 24 10 26 1.8 2.0 2.2 2.4 2.6 Student N S =10B 10 20 10 22 10 24 10 26 2.0 2.5 3.0 Student N S =30B 10 20 10 22 10 24 10 26 2.0 2.5 3.0 Student N S =50B Total Compute (FLOPs) Student Cross Entropy L S Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Supervised The compute at which distillation and supervised learning produce similar models grows with student size. Continuing the previous observation, we see in Figure <ref type="figure" target="#fig_16">17</ref> that supervised cross-entropy approaches the best case and teacher inference student cross-entropies at a value of compute which increases with compute, meaning that larger students benefit from distillation for larger compute budgets than supervised learning. This implies that if your target student size is small and your compute budget is large, then supervised learning is more likely to be beneficial than if your target student size is larger. The phenomenon happens because larger supervised models saturate in performance at larger values of D (Equation <ref type="formula" target="#formula_0">1</ref>), and distillation accelerates progress towards this saturation with the correct choice of teacher (Equation <ref type="formula">8</ref>), with more capable teachers producing more gains per token.</p><p>Including teacher training in compute produces student cross-entropies higher than in the supervised setting. In Figure <ref type="figure" target="#fig_16">17</ref> supervised cross-entropy is always below the teacher pretraining and teacher pretraining + inference scenarios, except at very large compute budgets, when supervised learning and these distillation scenarios produce similar student cross-entropies. This means that if your only aim is to produce the model of a target size with the lowest cross-entropy and you do not have access to a teacher, then you should choose supervised learning, instead of training a teacher and then distilling. Conversely, if the intention is to distill into a family of models, or use the teacher as a server model, distillation may be more computationally beneficial than supervised learning. This finding aligns with expectations, the alternative implies distillation can outperform direct maximum likelihood optimization given fixed compute.</p><p>The optimal teacher cross-entropy decreases with increasing total compute. As shown in Figure <ref type="figure" target="#fig_7">18</ref>, the optimal teacher cross entropy loss has a decreasing trend with respect to the total compute. However, in the best case scenarios, at low compute for larger student, where the number of student tokens is lower than the Chinchilla rule of thumb, an inflection point happens in optimal teacher compute.</p><p>We now turn to investigating the optimal distillation configurations that achieve these student cross-entropies.</p><p>2.2 2.3 2.4 2.5 Student N S =300M 2.1 2.2 2.3 2.4 2.5 Student N S =500M 2.0 2.2 2.4 Student N S =1B 1.8 2.0 2.2 2.4 Student N S =3B 10 20 10 22 10 24 10 26 1.8 2.0 2.2 2.4 Student N S =5B 10 20 10 22 10 24 10 26 1.75 2.00 2.25 2.50 Student N S =10B 10 20 10 22 10 24 10 26 1.5 2.0 2.5 Student N S =30B 10 20 10 22 10 24 10 26 1.5 2.0 2.5 Student N S =50B Total Compute (FLOPs) Optimal Teacher Cross Entropy L * T Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining)</p><p>Figure <ref type="figure" target="#fig_7">18</ref>. Compute optimal distillation teacher cross-entropies. For eight student sizes, the optimal teacher validation loss L * T resulting in lowest student validation loss L * S in each of the distillation scenarios considered (Table <ref type="table" target="#tab_13">5</ref>) the total compute is varied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4.3. DISTILLATION (BEST CASE)</head><p>In the distillation (best case) scenario, Œ¥ Lgt T = Œ¥ Pre T = 0, which means that we only account for compute associated with the standard supervised learning case</p><formula xml:id="formula_44">FLOPs(N S , D S , N T , D T ) ‚âà 3F (N S )D S Student Training .<label>(30)</label></formula><p>We call this best case as the scenario reflects a freedom to choose the best distillation setting for a given student size N S , with all of the compute being put into training the student for as long as possible (maximal D S ). In this sense we can consider this the upper bound in performance for distillation in our experimental setting.</p><p>10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1.6 5 1. 70 1.7 5 1.80 1 .8 5 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2. 35 2.3 52.4 0 2.45 2.5 0 2.5 5 2.6 0 2.6 5 2.7 0 2.7 5 10 20 10 22 10 24 10 26 This scenario represents the setting where a teacher already exists, or we will use the teacher for another purpose, for example a server model. In these scenarios, we do not need to worry about the teacher pretraining cost. Additionally, this teacher may be used to produce the logits for many different students, or we may have saved the logits from the teacher during its training. In these cases, the cost for producing the student logits can also be ignored.</p><p>The optimal quantities (D * S , N * T , D * T ) giving rise to the cross entropies in Figure <ref type="figure" target="#fig_16">17</ref> are shown in Figures <ref type="figure" target="#fig_17">19</ref> and <ref type="figure" target="#fig_18">20</ref>. In the best case scenario, L * T is determined, however N * T and D * T are not determined because they do not enter into the compute constraint, yielding a one-dimensional family (N T (L * T , D T ), D T ) of valid solutions to the minimization problem (Equation <ref type="formula" target="#formula_42">28</ref>). To provide some guidance for producing L * T , in Figure <ref type="figure" target="#fig_7">18</ref> we present the supervised compute optimal (N T (L * T , D T ), D T ), i.e. the combination that minimizes FLOPs ‚àù F (N T )D T subject to L(N T , D T ) = L T .</p><p>10 10 10 12 10 14 10 16 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 20. Compute optimal configurations for distillation (best case). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for best case in Figure 17. (N * T , D * T ) are the supervised compute optimal combination giving rise to L * T in Figure <ref type="figure" target="#fig_7">18</ref>. This is a one-dimensional slice of Figure <ref type="figure" target="#fig_17">19</ref>.</p><p>In this scenario, all the compute goes into student tokens, and so in Figure <ref type="figure" target="#fig_18">20</ref> we see optimal student tokens D * S increases with compute at the same rate as we could for the supervised model, which is higher for smaller students. The optimal teacher parameters N * (31)</p><p>This scenario represents the setting where a teacher already exists, but logits for the distillation still need producing. The optimal quantities (D</p><p>* S , N * T , D * T ) giving rise to the cross entropies in Figure 17 are shown in Figures 21 and 22. 10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1 .7 0 1. 75 1.8 0 1 .8 5 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2. 40 2.40 2.4 5 2.5 0 2.5 5 2.6 0 2.6 5 2.7 0 2.7 5 10 20 10 22 10 24 10 26 The teacher should be overtrained. In the teacher inference scenario, D * T does not contribute directly to compute but instead indirectly N * T subject to L * T . To minimize N * T at a given L * T , the solution is to maximize D * T as is seen in Figure <ref type="figure">22</ref>;</p><p>10 10 10 12 10 14 10 16 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 22. Compute optimal configurations for distillation (teacher inference). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) producing the student cross entropies for teacher inference in Figure <ref type="figure" target="#fig_16">17</ref>. This is a one-dimensional slice of Figure <ref type="figure" target="#fig_19">21</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D *</head><p>T takes the largest value allowed in our numerical optimization, 10 17 tokens. Although not surprising, this demonstrates the benefit of producing overtrained teachers, instead of taking the tempting strategy of using compute optimal teachers followed by a long distillation process into a smaller student model.</p><p>As compute is increased, relatively less should be spent on student training, and more on teacher logit inference. The compute allocations resulting from the optimal combination are shown in Figure <ref type="figure" target="#fig_20">23</ref>. We see that in all cases, the student training term (blue) decreases as compute increases, whereas the teacher logits (orange) increases. This happens because as compute increases: i) optimal student tokens increases at a rate approximately independent of compute, ii) the teacher size increases with compute to provide a stronger signal, while iii) the student size is fixed (see Figure <ref type="figure">22</ref>). (32)</p><p>This scenario represents when we want to figure out which teacher to produce to distill into sufficiently many different students, storing the teacher logits for reuse, effectively ammortizing the cost of producing the logits. Here, contrary to the previous two scenarios <ref type="bibr">(Appendices D.4.3 and D.4.5)</ref>, the teacher size N T and teacher tokens D T contribute directly to the compute accounting (Equation <ref type="formula">32</ref>). The optimal quantities (D</p><p>* S , N * T , D * T ) giving rise to the cross entropies in Figure 17 are shown in Figures 24 and 25. 10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1 .7 0 1. 75 1.80 1.85 1. 90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.602.65 2.70 2.75 2.8 0 2.8 5 10 20 10 22 10 24 10 26 L * T 1.6 0 1.65 1. 70 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.60 2.65 10 20 10 22 10 24 10 26 D * S 1B 3B 10B 30B 10 0B 30 0B 1T 3T 10 T 30 T 100 T 30 0T 1P 3P 10P 10 20 10 22 10 24 10 26 N * T 500M 1B 2B 3B 5B 10B 20B 30B 50 B 100B 200 B 30 0B 50 0B 10 20 10 22 10 24 10 26 D * T 10B 30B 100B 30 0B 1T 3T Total Compute C Total Student Size N S Figure 24. Compute optimal configuration contours for distillation (teacher pretraining). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining in Figure 17. 10 10 10 12 10 14 10 16 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity Compute optimal configurations for distillation (teacher pretraining). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining in Figure <ref type="figure" target="#fig_16">17</ref>. This is a one-dimensional size of Figure <ref type="figure" target="#fig_8">24</ref>.</p><p>The compute optimal teacher for distillation is a supervised compute optimal teacher. In Figure <ref type="figure" target="#fig_5">25</ref> we see that the M T ‚â° D T /N T ratio of the teacher is constant for all values of compute, and can be compared to the ratio in Figure <ref type="figure" target="#fig_17">19</ref>. This can be understood as there is no inference cost to pay for making the teacher large; we are only minimizing the training compute budgets of two models, and the most efficient way to produce a teacher with a given cross-entropy L T is a teacher that is compute-optimal in a supervised sense. Note that this conclusion is the opposite to the finding in Appendix D.4.4. There, the inference is expensive, and so the teacher should be overtrained. Here, teacher training is expensive, so teacher training should be compute optimal.</p><p>As compute is increased, relatively less should be spent on teacher training, and more on student training. In Figure <ref type="figure" target="#fig_22">26</ref> we see the compute allocations for the configurations shown in Figure <ref type="figure" target="#fig_5">25</ref>, and see that student training relative compute (blue) increases with increasing compute budget, while the teacher training (green) decreases with increasing compute budget. This happens because, as in all compute scenarios, with increasing compute, the optimal student tokens N * S increases (Figure <ref type="figure" target="#fig_5">25</ref>). Teacher size and tokens are also increasing with increasing compute, providing a stronger signal for the student with more tokens to learn. However, this increase in teacher size and tokens plateaus, while the student tokens continues to increase. This is because here the teacher is compute optimal, and so the amount of compute needed to improve the learning signal for the student is much less than the amount of compute needed to train the student for to make use of that signal, due to the stronger diminishing returns with respect to D S at a fixed N S (Equation <ref type="formula">8</ref>).</p><p>0 25 50 75 100 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 0 25 50 75 100 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Compute Fraction (%) Compute Component Student Training Teacher Logits Teacher Training (</p><formula xml:id="formula_45">)<label>33</label></formula><p>This scenario can be thought of as the compute optimal worst case scenario for distillation, i.e. one teacher is trained only for the purposes of one student. As in Appendix D.4.4, teacher size N T and teacher tokens D T contribute directly to the compute accounting (Equation <ref type="formula" target="#formula_45">33</ref>). The optimal quantities (D * S , N * T , D * T ) giving rise to the cross entropies in Figure <ref type="figure" target="#fig_16">17</ref> are shown in Figures <ref type="figure">27</ref> and <ref type="figure" target="#fig_7">28</ref>.</p><p>Compute optimal teachers should be used for lower compute budgets and overtrained teachers should be used for larger compute budgets. In Figure <ref type="figure" target="#fig_7">28</ref> we see a teacher configuration that interpolates between the teacher pretraining (Appendix D.4.5) and teacher inference (Appendix D.4.4) compute scenarios. At low compute, the optimal number of student tokens D * S is not too large, this means there is little penalty to increasing the teacher size, resulting in an approximately supervised compute-optimal teacher given a teacher compute budget. Once the optimal number of student tokens becomes higher than the optimal number of teacher tokens, there is significant penalty to increasing the teacher size. At this point, the teacher solution starts to become the overtrained solution seen in teacher inference, the optimal teacher tokens continue to increase polynomially, but this is not followed with an increase in the teacher size. For sufficiently high compute, corresponding to a large number of student distillation tokens, the compute penalty for teacher size is so large that optimal teacher size decreases with compute.</p><p>10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1 .7 0 1 .7 5 1.80 1.8 5 1. 90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.602.6 5 2.70 2.75 2.8 0 2.8 5 10 20 10 22 10 24 10 26 L * T 1. 60 1 .6 5 1 .7 0 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.60 2.65 10 20 10 22 10 24 10 26 D * S 1B 3B 10B 30B 100 B 300 B 1T 3T 10 T 30 T 10 0T 30 0T 1P 3P 10P 10 20 10 22 10 24 10 26 N * T 500M 1B 1B 2B 3B 5B 10B 2 0 B 30B 50B 1 0 0 B 10 20 10 22 10 24 10 26 D * T 10B 30B 100B 300B 1T 3T 10T 30T 100T Total Compute C Total Student Size N S Figure 27. Compute optimal configuration contours for distillation (teacher pretraining + inference). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining + inference in Figure 17. 10 9 10 11 10 13 10 15 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 9 10 11 10 13 10 15 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 28. Compute optimal configurations for distillation (teacher pretraining + inference). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining + inference in Figure <ref type="figure" target="#fig_16">17</ref>. This is a one-dimensional size of Figure <ref type="figure">27</ref>.</p><p>For small students, as compute grows, more should be spent on training the student and producing logits for the student. In Figure <ref type="figure">29</ref> we see the compute allocations for the configurations shown in Figure <ref type="figure" target="#fig_7">28</ref>. Compute optimal smaller models tend to have smaller teachers, and optimal teacher tokens always grow at a slower rate than student tokens, and so teacher the training cost is relatively small. As compute grows, the student is distilled on more tokens, and the teacher always becomes slightly larger than the student, which gives rise to most compute being allocated to standard student training compute component and producing the logits for this training.</p><p>For large students, as compute grows, more should be spent on training the teacher, until a transition happens where more should be spent on training the student and producing logits for the student. The explanation for the phenomenon is as above, except that the larger students need a more capable teacher to learn from as compute grows, and so initially compute needs to bused to produce the teachers required. After a certain amount of compute, the large number of optimal student distillation tokens moves the optimal solution towards an overtrained teacher scenario, and more compute being allocated to student training and logit production.</p><p>0 25 50 75 100 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 0 25 50 75 100 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Compute Fraction (%) Compute Component Student Training Teacher Logits Teacher Training</p><p>Figure <ref type="figure">29</ref>. Compute optimal allocations for distillation (teacher pretraining). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation <ref type="formula" target="#formula_43">29</ref>for the compute optimal values in Figure <ref type="figure" target="#fig_7">28</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4.7. OPTIMAL TEACHER TRAINING AND STUDENT DISTILLATION TOKENS</head><p>To aid in comparing the different compute strategies presented in Appendices D.4.3 to D.4.6, we now present each compute optimal value for all strategies, including supervised. Here, we show compute-optimal distillation student tokens D * S in Figure <ref type="figure" target="#fig_24">31</ref> and compute-optimal teacher pretraining tokens D * T in Figure <ref type="figure" target="#fig_24">31</ref>.</p><formula xml:id="formula_46">1B 10B 100B 1T 10T 100T 1P 10P 100P Student N S =300M Student N S =500M Student N S =1B Student N S =3B</formula><p>10 20 10 22 10 24 10 26 1B 10B 100B 1T 10T 100T 1P 10P Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Student Tokens D * S Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Supervised In all scenarios, student tokens should be increased with compute similar to in the supervised case. We see in Figure <ref type="figure" target="#fig_23">30</ref> that, as in Chinchilla <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref>, supervised tokens are increased polynomially with compute. Dis-tillation (best case) follows the exact same allocation, as does distillation (pretraining) with asymptotically large compute. All other methods follow the same increase rate, but with scenario-dependent offsets.</p><p>1B 10B 100B 1T 10T 100T 1P 10P 100P Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 1B 10B 100B 1T 10T 100T 1P 10P 100P Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Teacher Tokens D * T Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Optimal teacher tokens interpolate between scenarios based on compute allocation. In Figure <ref type="figure" target="#fig_24">31</ref> we can see more clearly the interpolation behavior discussed in Appendix D.4.6. At low compute, teacher pretraining and teacher pretraining + inference share optimal solutions because the number of student tokens N * S is small. At high compute, teacher pretraining + inference approaches teacher inference, while teacher pretraining approaches best case, as N * S is large, and costs associated with teacher pretraining become less important. Optimal teacher size interpolate between scenarios based on compute allocation. As in the optimal teacher tokens N * T in Figure <ref type="figure" target="#fig_24">31</ref>, the same mechanism causes interpolation behavior in optimal teacher size (see Figure <ref type="figure" target="#fig_25">32</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4.8. OPTIMAL TEACHER SIZE</head><formula xml:id="formula_47">1B 10B 100B Student N S =300M Student N S =500M Student N S =1B Student N S =3B</formula><p>supervised learning based on a token or cross-entropy threshold, and ii) potentially increased importance of data mixtures (Œª ‚â§ 1, see Appendix G.1) when distilling with significant token and/or compute budgets. We leave this for future work.</p><p>In situations where teacher training is required, supervised learning is more efficient. As observed in Appendix D.4.2, for all student sizes, if teacher pretraining is included in the computational cost of producing a student, supervised learning is always more efficient than distilling. This can be seen from Figure <ref type="figure" target="#fig_2">33</ref> as the teacher pretraining (green) and teacher pretraining + inference (red) compute scenarios are above the grey dashed line, which means more compute is needed for distillation than supervised learning in those compute scenarios. This compute efficiency translates into data efficiency (see Figure <ref type="figure" target="#fig_8">34</ref>). 1.6 1.8 2.0 2.2 2.4 2.6 Student N S =10B 1.6 1.8 2.0 2.2 2.4 2.6 Student N S =30B 1.6 1.8 2.0 2.2 2.4 2.6 Student N S =50B Student Cross-Entropy L S Distillation Data / Supervised Data Compute Scenario Distillation (best case) Distillation (teacher pretraining) Distillation (teacher inference) Distillation (teacher pretraining + inference)</p><formula xml:id="formula_48">Break-even L(N = N S , D = ‚àû)</formula><p>Figure <ref type="figure" target="#fig_8">34</ref>. Compute optimal distillation data ratios. For eight student sizes, the number of tokens compute needed to produce a student of the indicated size and cross-entropy. The horizontal dashed line indicates the break-even point, when doing supervised leaning is as data efficient as the corresponding distillation compute scenario. Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing a model of the indicated size and cross-entropy. The vertical dashed line indicates the lowest cross-entropy achievable by that student.</p><p>Distillation is more efficient for larger students. In Figure <ref type="figure" target="#fig_2">33</ref> we see in the pretrain + inference scenario, producing a N S =500M student with a cross-entropy of 2.4 has roughly 3/4 the compute cost of producing the same model with supervised learning, whereas producing a N S =10B student with a cross-entropy of 2.2 has roughly 1/2 the compute cost of producing the same model with supervised learning. In terms of data (Figure <ref type="figure" target="#fig_8">34</ref>), the 500M and 10B configurations use roughly 2/3 and 1/2 the number of tokens of their supervised counterparts respectively. The efficiency gains from distillation are potentially greater for larger students when considering compute or data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Results</head><p>In this section, we provide an extensive list of studies, including downstream evaluations of distillation. We cover the models used as teachers, examine the Kullback-Leibler Divergence (KLD) between teacher and student in fixed token-tosize ratios, and present supplementary materials to Section 4.1. Additionally, we investigate the limiting behavior of our scaling law, weak-to-strong generalization, and conduct a model calibration study to assess fidelity. These analyses offer a comprehensive view of the factors influencing distillation performance and the behavior of our proposed scaling laws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Downstream evaluations</head><p>In all settings, we optimize for and predict model cross-entropy on the validaiton set. To confirm that the validation crossentropy L S is a good proxy for the downstream evaluation that we ultimately care about, we show how each downstream result is affected by the teacher and student loss. Figure <ref type="figure" target="#fig_28">35</ref> shows a set of English downstream evaluation tasks. ARC Easy <ref type="bibr" target="#b11">(Bhakthavatsalam et al., 2021)</ref>, ARC Challenge <ref type="bibr" target="#b11">(Bhakthavatsalam et al., 2021)</ref>, HellaSwag <ref type="bibr" target="#b110">(Zellers et al., 2019)</ref>, Piqa <ref type="bibr" target="#b13">(Bisk et al., 2020)</ref>, Sciq <ref type="bibr" target="#b98">(Welbl et al., 2017)</ref>, WinoGrande <ref type="bibr" target="#b85">(Sakaguchi et al., 2021)</ref> and Lambada OpenAI <ref type="bibr" target="#b75">(Paperno et al., 2016)</ref> are zero-shot tasks. TriviaQA <ref type="bibr" target="#b53">(Joshi et al., 2017)</ref> and WebQS <ref type="bibr" target="#b8">(Berant et al., 2013)</ref> are one-shot tasks. TriviaQA evaluation is on the larger and more challenging Web split. CoreEn is the average of both the zero-shot and one-shot tasks.</p><p>Finally, we have included GSM8K <ref type="bibr" target="#b28">(Cobbe et al., 2021)</ref> and MMLU <ref type="bibr">(Hendrycks et al., 2021b;</ref><ref type="bibr">a)</ref>. GSM8K is used in an 8-shot chain of thought setting, following LLaMA <ref type="bibr">(Touvron et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr">Dubey et al., 2024)</ref>. MMLU is used in a fiveshot setting. These perform near-random for most of the models, and only show a slightly upwards trend when decreasing student/teacher loss. This is due to the use of the C4 dataset in training, and we note that we do not aim for competitive downstream evaluation results.</p><p>All models are evaluated using an internal version of the open-source lm-evaluation-harness <ref type="bibr" target="#b19">(Gao et al., 2024)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Teachers used in distillation</head><p>In Figure <ref type="figure" target="#fig_30">36</ref> we show the cross-entropies of the models used as teachers in Section 4.2, and for fitting the supervised scaling law: i) eleven of fixed-M ratio models following the Chinchilla rule of thumb D/N = M * ‚âà 20 <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref>, ii) six models on D = 512B tokens (Figure <ref type="figure" target="#fig_30">36a</ref>), and iii) four IsoFLOP profiles (Figure <ref type="figure" target="#fig_30">36b</ref>). Together this produces 74 runs corresponding to tuples of (N, D, L).  Coefficient estimation (Appendix F.1) yields the scaling coefficients shown in Table <ref type="table">6</ref>, and a scaling law which has ‚â≤ 1% relative prediction error, including when extrapolated from weaker to stronger models (see Figure <ref type="figure" target="#fig_5">5a</ref>). In Figure <ref type="figure" target="#fig_2">37</ref>, the capacity gap in knowledge distillation can be seen. Improving a teacher's performance does not always improve a student's, and even reduces the performance after a certain point. The KLD between teacher and student is an increasing function of teacher size in all cases, which means as the teacher improves its own performance, the student finds the teacher more challenging to model, which eventually prevents the student from taking advantage of teacher gains. See Appendix E.8.2 for an investigation using calibration to understand where this mismatch occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Full distillation scaling law IsoFLOP profiles</head><p>In Figure <ref type="figure" target="#fig_33">38a</ref> we provide the full six fixed M Teacher/IsoFLOP Student profiles, only two of which were shown in Figure <ref type="figure">2</ref>. These experiments enable the reliable determination of Œ± ‚Ä≤ , Œ≤ ‚Ä≤ , Œ≥ ‚Ä≤ , A ‚Ä≤ and B ‚Ä≤ . In Figure <ref type="figure" target="#fig_33">38b</ref> we provide the full four IsoFLOP teacher/ fixed M student, only two of which were shown in Figure <ref type="figure" target="#fig_2">3</ref>. These experiments enable the reliable determination of c 0 , c 1 , f 1 and d 1 .</p><p>Strong-to-weak generalization occurs. For the weaker teachers (N T ‚â§ 2.72B), The horizontal dashed line in each pane shows the cross-entropy achieved by the teacher (Appendix E.2). we see that for students larger than the teacher (N S &gt; N T ) and for sufficiently large compute budgets, the student is able to outperform the teacher (see Appendix E.7 for a detailed one-dimensional slice).</p><p>A stronger teacher signal is needed in order for stronger students to outperfom the supervised baseline. The horizontal dashed line in each pane shows the cross-entropy achieved by the student if trained using supervised learning (Appendix E.2). We see that weaker students benefit more from distillation, as e.g. the 198M student has all observed data below this dashed line, meaning all distillations outperform the supervised baseline. However, for the 1.82B student, only 10 21 FLOP teachers produce distilled students that outperform the supervised baseline.</p><p>2.4 2.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.6</head><p>Teacher</p><formula xml:id="formula_49">N T =546M Teacher N T =975M 2.2 2.4 2.6</formula><p>Teacher</p><formula xml:id="formula_50">N T =1.82B Teacher N T =2.72B 100M 300M 1B 3B 7B 2.2 2.4 2.6</formula><p>Teacher N T =4.82B  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Distillation scaling law IsoFLOP optima</head><p>The optimal loss values of each IsoFLOP in Figure <ref type="figure" target="#fig_33">38a</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. Distillation with infinite data</head><p>From the supervised scaling law (Equation <ref type="formula" target="#formula_0">1</ref>) a model with N parameters has a cross-entropy lower bound</p><formula xml:id="formula_51">L(N ) ‚â° L(N, D = ‚àû) = E + (AN -Œ± ) Œ≥<label>(35)</label></formula><p>which represents the best solution to the training objective subject to constraints from that model's hypothesis space <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref> and is achieved when the number of training tokens is large (D ‚Üí ‚àû). As the hypothesis space of a model is independent of the procedure used to find the solutions, we anticipate that the student with N S parameters has a cross-entropy lower bound that is the same as the supervised one Equation <ref type="formula" target="#formula_51">35</ref>. However, it not immediately clear if this is true in practice, since</p><formula xml:id="formula_52">L S (N S ) ‚â° L S (N S , D S = ‚àû, L T = L * T ) (36) = L * T + (A ‚Ä≤ N -Œ± ‚Ä≤ S ) Œ≥ ‚Ä≤ (L * T ) c0 1 + L * T d -1 1 L(N S ) 1/f1 -c1f1 ,<label>(37)</label></formula><p>where L * T = arg min L (N S , D S = ‚àû, L T ) is the teacher cross-entropy that minimizes Equation <ref type="formula">8</ref>. Upon checking numerically, we do find that Equation 35 is consistent with Equation 37 for a range of models N, N S ‚àà [100M, 100B] (Figure <ref type="figure" target="#fig_36">40</ref>). We stress that unlike our three motivations for the equation properties (Section 4.3), this infinite data limit was imposed added by hand, and is only true for certain values scaling coefficients. This lower bound consistency is evidence that that our distillation scaling law has desired behavior far outside of observed models, at least along the data and teacher axes. We also note that only the optimal teacher for each student size produces a student cross-entropy lower bound that is consistent with the supervised one. Any other choice produces higher student cross-entropies, either because the teacher is too weak, or due to the capacity gap. For the optimal choice of teacher, the loss achieved by all student sizes under distillation is consistent with the loss achievable by supervised learning. This is not true for any choice of teacher, only the optimal one, which can be determined through numerical optimization of the provided distillation scaling laws (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7. Weak-to-strong generalization</head><p>In Figure <ref type="figure" target="#fig_37">41</ref> we see that weak-to-strong generalization <ref type="bibr" target="#b19">(Burns et al., 2024;</ref><ref type="bibr">Ildiz et al., 2024)</ref> occurs only in the finite distillation data regime, and when the number of tokens is sufficiently large, the student cross-entropy increases again, eventually matching the teacher cross-entropy. This can be understood in the following way: i) when the student is larger than the teacher, the student contains in its hypothesis space the function represented by the teacher, ii) when the student is shown the teacher outputs on enough of the data manifold, it eventually matches what the teacher does on the whole data manifold. We note this doesn't explain how and why the student outperforms its teacher, and only constrains its asymptotic (low and high distillation data) behaviors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8.2. 198M STUDENTS TRAINED ON 20N TOKENS</head><p>In this section we consider students trained on the teacher distribution, as in our main study. We also study students trained on the teacher top-1 distribution, as described in Appendix G.4, as the qualitative difference in behavior can be informative for student design.</p><p>Evaluating the calibration of a student can be done in a number of ways:</p><p>1. We can compare student outputs relative ground-truth data, as in Appendix E.8.1 for the teachers.</p><p>2. We can compare student outputs with the outputs of its teacher.</p><p>Calibration against ground-truth. First, let's consider comparison against ground truth data. In Figure <ref type="figure" target="#fig_39">43</ref> we show student calibration with respect to the dataset labels for both teacher distribution distillation and teacher top-1 distillation.</p><p>1. Distilled on the full teacher distribution. In Figure <ref type="figure" target="#fig_39">43a</ref>, we observe that the student is well-calibrated against ground truth data. Similar to the teacher's calibration plot in Figure <ref type="figure" target="#fig_8">42</ref>, we see a small discrepancy at very low and very high confidence values, and the ECE value is low.</p><p>2. Distilled on teacher top-1. In Figure <ref type="figure" target="#fig_39">43b</ref>, we see that a student trained only on its teacher's top-1 prediction, is not calibrated against ground truth data. The blue points below the dashed line indicate an overconfident student, i.e. , its predicted confidence is higher than the actual accuracy in that confidence range. This is because training the student on top-1 assigns the student to the most plausible outcome rather than all the plausible outcomes with correct frequencies. Confidence proportions are low for all bins that are not the most confident bin, and ECE is high, although decreases with increasing teacher size N T .</p><p>Figure <ref type="figure" target="#fig_39">43</ref> shows that training the student on the teacher's distribution results in a calibrated student, whereas training on the teacher top-1 does not. Indeed, optimizing against the teacher's top-1 is not a proper scoring metric, and that teacher top-1 is not an unbiased estimator for the data, while the teacher distribution is.  Calibration against teacher top-1. Next we investigate the first student calibration against the teacher. In Figure <ref type="figure" target="#fig_41">44</ref> we show student calibration with respect to the teacher's top-1 label. That is, the next-token label used for accuracy computation, and extract the students confidence is the most probable next-token according to the teacher, instead of the label from data. Here no next token labels are used at all. These teacher top-1 labels are also used for the ECE calculation, which is still computed using Equation <ref type="formula">38</ref>.</p><p>1. Distilled on the full teacher distribution. We see in Figure <ref type="figure" target="#fig_41">44a</ref> that when distilled from the full teacher distribution, the student is not calibrated against the teacher top-1. The blue points are above the dashed line, which means that the empirical accuracy is higher than the model's predicted confidence, i.e. with respect to the teacher top-1, the student is underconfident. This can be understood by noting that the top-1 objective is an easier objective than modeling the full vocabulary at each step.</p><p>2. Distilled on teacher top-1. In Figure <ref type="figure" target="#fig_41">44b</ref> we observe that a student is distilled from its teacher's top-1 is calibrated with respect to teacher's top-1.  Figure <ref type="figure" target="#fig_41">44</ref> shows that training the student on teacher top-1 results in calibration against teacher top-1, whereas a model trained on data, or distilled on the full teacher distribution is not calibrated against teacher top-1. As above, this can be understood as now teacher's top-1 is now a proper scoring metric, and teacher top-1 is an unbiased estimator for itself.</p><p>Calibration against teacher distribution. Here we develop a modified calibration measure that will help us understand if the student matches the teacher in a distributional sense. As we have two distributions to compare, we can ask, for a given teacher confidence, what is the expected student confidence. This leads to ECE Dist , a distributional form of ECE:</p><formula xml:id="formula_53">ECE Dist (A, B) = M m=1 |B m | N Samples |Confidence(B m ; A) -Confidence(B m ; B)| ,<label>(39)</label></formula><p>and is similar in spirit to divergence measures like KLD. B m , |B m |, and N Samples are defined as before, and Confidence S (B m ; A|B) is the average confidence of model A or B in bin m respectively. The bins G m are always witin the bins of confidence of model B. In the current evaluation, we take A as the teacher and B as the student, and we are measuring the average confidence of the teacher is measured within a student's confidence bin.</p><p>1. Distilled on the full teacher distribution. In Figure <ref type="figure" target="#fig_43">45a</ref>, we see that when the student is confident, it matches the teacher confidence. However, as the teacher model grows in size, when the student is less confident, it it systematically underestimates its confidence. This suggests that the student has not effectively learned low-probability outcomes, or that these outcomes are particularly challenging for the student to replicate. The underconfidence in these regions may be a result of the distillation process not providing sufficient learning signal for these difficult cases, or the inherent difficulty of capturing the uncertainty associated with low-confidence predictions. This observation of confidence mismatch helps indicate which parts of the distribution the student finds challenging to model, giving rise to the increasing KLD and capacity gap observed in Figure <ref type="figure" target="#fig_3">4</ref> and Appendix E.3.</p><p>2. Distilled on teacher top-1. In Figure <ref type="figure" target="#fig_43">45b</ref>, for small teachers, we observe student overconfidence. As the teacher increases in size, the student's overconfidence in low-confidence bins transitions to underconfidence. At the same time, the student's overconfidence in high-confidence bins improves, leading to an overall reduction in distributional ECE. This pattern of overconfidence in the student is similar to what we saw in Figure <ref type="figure" target="#fig_39">43b</ref>, but the change in behavior at low-confidence bins as the teacher's size varies is different. This shift in the student's calibration behavior, especially in low-confidence bins, aligns with findings from Figure <ref type="figure" target="#fig_43">45a</ref> and may highlight the difficulty the small student faces in learning rare events.  We can also inspect the student confidences within a bin of teacher confidences, and compute the distributional ECE (Equation <ref type="formula" target="#formula_53">39</ref>), swapping the roles of teacher and student (see Figure <ref type="figure" target="#fig_45">46</ref>).</p><p>1. Distilled on the full teacher distribution. In Figure <ref type="figure" target="#fig_43">45a</ref> we complete the picture from Figure <ref type="figure" target="#fig_43">45a</ref> and see that the part of the distribution the student struggles to model is actually the place where teacher is most confident.</p><p>2. Distilled on teacher top-1. In Figure <ref type="figure" target="#fig_43">45b</ref> we see that the student is systematically overconfident for all values of teaacher confidence, except for the largest teachers, where the student is underconfident when those teachers are most confident.  E.8.3. 198M STUDENTS TRAINED ON 128B TOKENS In this section, we study the effect of increasing the number distillation tokens in Appendix E.8.2 from D S ‚âà 20N S to D S ‚âà 512B. Here, we reserve discussion for the observed differences compared to Appendix E.8.2. 0.0 0.5 1.0 NT =198M ECE=0.1% NT =546M ECE=0.1% NT =975M ECE=0.2% NT =1.82B ECE=0.3% 0.0 0.5 1.0 0.0 0.5 1.0 NT =2.72B ECE=0.4% 0.0 0.5 1.0 NT =4.82B ECE=0.5% 0.0 0.5 1.0 NT =7.75B ECE=0.4% Confidence Proportion(confidence) Perfectly calibrated Student Confidence Student Accuracy (a) Train target: teacher distribution. 0.0 0.5 1.0 NT =198M ECE=42.1% NT =546M ECE=37.1% NT =975M ECE=34.2% NT =1.82B ECE=31.7% 0.0 0.5 1.0 0.0 0.5 1.0 NT =2.72B ECE=29.3% 0.0 0.5 1.0 NT =4.82B ECE=26.5% 0.0 0.5 1.0 NT =7.75B ECE=24.8% Confidence Proportion(confidence) Perfectly calibrated Student Confidence Student Accuracy (b) Train target: teacher Top 1. Calibration against ground-truth. As the number of distillation tokens increases, we observe a consistent decrease in the ECE when the student is trained on the teacher's distribution, as shown by the comparison between Figure <ref type="figure" target="#fig_46">47a</ref> and Figure <ref type="figure" target="#fig_39">43a</ref> across different teacher sizes. However, when the student is trained on the teacher's top-1 predictions, increasing the number of tokens negatively impacts ECE, as evidenced by the comparison between Figure <ref type="figure" target="#fig_46">47b</ref> and Figure <ref type="figure" target="#fig_39">43b</ref>. This suggests that the teacher's top-1 predictions are not a reliable, unbiased estimator of the actual data, and increasing the number of training tokens only exacerbates this issue. See Appendix G.4 for further discussion.</p><p>Calibration against teacher top-1. Increasing the number of distillation tokens leads to worse calibration between the student and the teacher's top-1 predictions when the student is trained on the full distribution. This change primarily occurs in the low-confidence bins, and results in a higher ECE (compare Figure <ref type="figure" target="#fig_49">48a</ref> and Figure <ref type="figure" target="#fig_41">44a</ref>). However, when comparing the ECEs for the student trained on the teacher's top-1 predictions (Figures <ref type="figure" target="#fig_41">44b</ref> and <ref type="figure" target="#fig_49">48b</ref>), there is an improvement across all teacher sizes. When the student is trained and evaluated using the same metric, increasing the training tokens helps improve calibration, demonstrating consistency between the learning objective and the evaluation metric.   Calibration against teacher distribution. A comparison between Figure <ref type="figure" target="#fig_51">49a</ref> and Figure <ref type="figure" target="#fig_43">45a</ref> shows that when the student is trained on the teacher's full distribution and evaluated against the full distribution using Equation <ref type="formula" target="#formula_53">39</ref>, increasing the number of training tokens consistently improves calibration across all teacher sizes. However, when the student is trained on the teacher's top-1 predictions, a quick comparison between Figure <ref type="figure" target="#fig_51">49b</ref> and Figure <ref type="figure" target="#fig_43">45b</ref>   Similarly, when comparing within teacher confidence bins (Figure <ref type="figure" target="#fig_53">50</ref>) increasing the number of distillation tokens from 20N to 128B primarily amplifies the observed phenomena at lower distillation token budgets, and improving calibration in cases where there is a proper scoring metric present (Figure <ref type="figure" target="#fig_53">50a</ref>).  In general, increasing the number of training tokens has a positive effect when the training metric is an unbiased estimator of the actual data or the measured calibration quantities (see Figures <ref type="figure" target="#fig_49">47a, 48b</ref> and <ref type="figure" target="#fig_51">49a</ref>) and reduces the ECE, while it has a negative impact when there is a mismatch between the learned and measured quantities (see Figures <ref type="figure" target="#fig_49">47b, 48a</ref> and <ref type="figure" target="#fig_51">49b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Scaling coefficients</head><p>In this section, we analyze the process of deriving the coefficients for our scaling law. We follow the procedure outlined in <ref type="bibr" target="#b26">(Hoffmann et al., 2022;</ref><ref type="bibr">Besiroglu et al., 2024)</ref>, while incorporating our modified scaling laws F.1. Supervised scaling law coefficient estimation First, let's tackle the supervised scaling law Equation 1 restated for convenience</p><formula xml:id="formula_54">L(N, D) = E + A N Œ± + B D Œ≤ Œ≥ .<label>(40)</label></formula><p>To aid numerical stability, we write this expression in log space. First note that for a, b &gt; 0</p><formula xml:id="formula_55">log(a + b) = log (exp log a + exp log b) = LSE(log a, log b),<label>(41)</label></formula><p>where LSE is the log-sum-exp operator. We can now proceed to write the supervised scaling law in log form</p><formula xml:id="formula_56">log L(N, D; A, B, E, Œ±, Œ≤) = log E + A N Œ± + B D Œ≤ Œ≥ (42) = LSE log E, Œ≥ log A N Œ± + B D Œ≤ (43) = LSE [log E, Œ≥ LSE (log A -Œ±N, log B -Œ±D)] .<label>(44)</label></formula><p>We make no assumptions about the relationships between the values (i.e. no parameter tying) and optimize</p><formula xml:id="formula_57">(A * , B * , E * , Œ± * , Œ≤ * , Œ≥ * ) = arg min {A,B,E,Œ±,Œ≤,Œ≥} i Huber Œ¥ log L(N (i) , D (i) ; A, B, E, Œ±, Œ≤) -L (i)<label>(45)</label></formula><p>with a Huber Œ¥ = 10 -4 , where N (i) , D (i) and L (i) are the model size, number of training tokens and loss achieved by the i-th run. We fit on 73 samples over a grid of L-BFGS-B initializations given by: log A ‚àà {0., 5., 10., 15., 20.}, log B ‚àà {0., 5., 10., 15., 20.}, log E ‚àà {-1., -0.5., 0., 0.5, 1., 1.5.}, Œ± ‚àà {0., 0.5, 1., 1.5}, Œ≤ ‚àà {0., 0.5, 1., 1.5}, Œ≥ ‚àà {0., 0.5, 1., 1.5}. The L ‚â• 2.2 case corresponds to 48 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Distillation scaling law coefficient estimation</head><p>Next, let's address the distillation scaling law Equation 8 restated for convenience</p><formula xml:id="formula_58">L S (N S , D S , L T ) = L T + 1 L c0 T 1 + L T L S d 1 1/f1 -c1 * f1 A ‚Ä≤ N Œ± ‚Ä≤ S + B ‚Ä≤ D Œ≤ ‚Ä≤ S Œ≥ ‚Ä≤ . (<label>46</label></formula><formula xml:id="formula_59">)</formula><p>As in Appendix F.1, to aid numerical stability during optimization, we write this in log space</p><formula xml:id="formula_60">log L S (N S , D S , L T ; Œ∏) = log Ô£Æ Ô£∞ L T + 1 L c0 T 1 + L T L S d 1 1/f1 -c1 * f1 A ‚Ä≤ N Œ± ‚Ä≤ S + B ‚Ä≤ D Œ≤ ‚Ä≤ S Œ≥ ‚Ä≤ Ô£π Ô£ª (47) = LSE log L T , -c 0 log L T -c 1 f 1 log 1 + L T d 1 L S 1/f1 + Œ≥ log A ‚Ä≤ N Œ± S + B ‚Ä≤ D Œ≤ S (48) = LSE log L T , -c 0 log(L T ) -c 1 f 1 LSE 0, 1 f 1 log L T -log L S -log d 1 + Œ≥ LSE (log A ‚Ä≤ -Œ± ‚Ä≤ log N S , log B ‚Ä≤ -Œ≤ ‚Ä≤ log D S ) ,<label>(49)</label></formula><p>We examine various Œª values across different teacher-student configurations in Figure <ref type="figure" target="#fig_5">51a</ref> and find that while the optimal mixing coefficients Œª * vary based on the specific teacher-student combinations (Figure <ref type="figure" target="#fig_5">51b</ref>), the student cross-entropy L S remains mostly flat for choices of Œª &gt; 0.5, with lower values of Œª only preferred in the cases where the teacher is particularly weak and where the supervised signal is more informative. From Figure <ref type="figure" target="#fig_5">51a</ref> it is also possible to get a sense of when distillation Œª &gt; 0 generally outperforms supervised learning Œª = 0 under the same token budget.</p><p>To guide practitioners, Figure <ref type="figure" target="#fig_5">51b</ref> shows empirically derived optimal mixing coefficients, Œª * , though the simplicity and robustness of pure distillation makes it a reliable default choice for practical use and study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Temperature (œÑ ) sensitivity analysis</head><p>In distillation, the temperature œÑ controls the entropy of teacher predictions by scaling logits z (i)</p><p>T /œÑ and z (i) S /œÑ in the knowledge distillation loss L KD (Equations 7 and 53). This scaling modulates the transfer of dark knowledge <ref type="bibr" target="#b45">(Hinton et al., 2015)</ref> -the log-probability ratios between incorrect categories encode the teacher's understanding of relationships between those categories. Our analysis across œÑ ‚àà [0.5, 10] (Figure <ref type="figure" target="#fig_54">52</ref>) reveals that higher temperatures (œÑ &gt; 3) reduces performance by attenuating these ratios in œÉ a (z (i) T /œÑ ), particularly harming smaller students that rely heavily on this signal. Lower temperatures (œÑ &lt; 1) similarly reduce effectiveness by concentrating probability mass on argmax tokens, diminishing the transfer of relationships between lower-ranked predictions.</p><p>We find optimal performance at œÑ = 1 across all model scales, suggesting this temperature best preserves log-probability structure. Unlike the original distillation setting, which relied on dark knowledge to represents hierarchical relationships between incorrect classification predictions in the presence of a true label, language modeling is inherently ambiguous and complex, with many valid continuations. It is precisely the understanding of the ambiguity of language we want to transfer to the student, which is supported by our finding that maintaining the teacher's original probability ratios (œÑ = 1) produces the lowest student cross-entropies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Learning rate (Œ∑) sensitivity analysis, verification of ¬µP for distillation</head><p>The peak learning rate Œ∑ determines the scale of student parameter updates in distillation. In our experiments we use a simplified version of ¬µP <ref type="bibr" target="#b104">(Yang &amp; Hu, 2021;</ref><ref type="bibr">Yang &amp; Littwin, 2023;</ref><ref type="bibr" target="#b31">Yang et al., 2022;</ref><ref type="bibr">Wortsman et al., 2023;</ref><ref type="bibr" target="#b114">Yang et al., 2023)</ref>, described as ¬µP (simple) in <ref type="bibr" target="#b100">(Wortsman et al., 2024)</ref>.</p><p>In the supervised case, in addition to improving the performance lower bound compared to the standard parameterization, ¬µP simplifies experimental settings as it enables hyperparameter transfer; the optimal peak learning rate Œ∑ and initialization scales found for a reference model size can be reused when changing model size<ref type="foot" target="#foot_7">foot_7</ref> .</p><p>Here we validate that the optimal peak learning rate Œ∑ * = 0.01 determined in the supervised case transfers to the distillation setting. Sweeping values Œ∑ ‚àà [0.001, 0.1] (Figure <ref type="figure" target="#fig_55">53</ref>) reveals that ¬µP achieves optimal performance at Œ∑ = 0.01 uniformly across all configurations, from 198M to 1.82B parameter students and 546M to 7.75B parameter teachers, consistent with the optimal peak learning rate in the supervised setting.</p><p>Performance varies smoothly and modestly around this optimum, with cross-entropy changing by less than 0.1 nats over one order of magnitude in learning rate. This consistency validates ¬µP's guarantee of scale-invariant training dynamics for distillation, confirming that our experimental setting for determining our distillation scaling law operates at the optimal learning rate or sufficiently close to it in all of our settings. The observed moderate learning sensitivity in distillation partially alleviates the requirement for careful learning rate tuning, showing that in practice the reference learning rate found in the supervised setting can be safely reused in the distillation setting. We investigate how the truncation of the teacher distributions affects student performance. For these methods, when the teacher produces a distribution pT (x (i) = a|x (&lt;i) ), a ‚àà {1, . . . , V } over the vocabulary for the student to match, only some entries in the distribution are used. This is done primarily to reduce repeated inference and storage costs in the case teacher outputs are being stored for re-use in the multiple distillations scenario discussed in Section 5.3. In our case, the vocabulary size V = 32168, so assuming storage in float32, means each token requires 32168 √ó 4 bytes ‚âà 129KB, and storing all of C4 (approximately 2T tokens) would take approximately 260 Petabytes, a significant amount of data, roughly the total amount collected during the first ten years of the Large Hadron Collider (LHC) <ref type="bibr" target="#b21">(CERN, 2018)</ref>.</p><p>Given a truncation method M, can a truncated teacher output p(M)</p><p>T can be stored whilst still achieving the gains of distillation? Concretely, the truncation p (M) (x|c) of a distribution p(x|c) with a truncation method M is </p><p>where S M (p( ‚Ä¢ |c)) represents the set of retained categories (i.e. non-zero probabilities) in the truncated distribution, which then undergoes renormalization over the retained categories.</p><p>We explore two complementary approaches: Top-k and Top-p (nucleus) sampling. As in all of our settings, we evaluate the student cross-entropy against the data distribution with all categories, as this is the model property we are most interested in (a model can trivially match the target distribution if all categories except one are removed). For Top-k, we zero-out all but the largest k probabilities, and Top-p, we zero-out all but the smallest set of probabilities that sum to at least p. </p><p>As the truncation parameters increase (k ‚Üí V or p ‚Üí 1), both methods approach the full teacher distribution, and the student's cross-entropy converges to the baseline using the entire pT . Conversely, aggressive truncation (small k or p) induces quantization that preserves only high-probability tokens while discarding information in the tail of the distribution.</p><p>Our empirical analysis (Figure <ref type="figure" target="#fig_57">54</ref>) reveals that both truncation methods directly correlate with reduced evaluation likelihoods. However, this performance degradation can be effectively mitigated through a combination of truncated distributions and ground truth next-token prediction using a mixing coefficient Œª ‚àà (0, 1) (Equation <ref type="formula">7</ref>). Specifically, with k = 128 and Œª = 0.7, we achieve validation losses statistically indistinguishable from those obtained using the complete teacher distribution. For large-scale distillation scenarios where maintaining multiple models in memory is prohibitive, particularly with large teacher models, storing only the Top-k teacher predictions (with Œª &gt; 0) enables efficient post-hoc distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5. Forward and reverse KL divergence</head><p>We investigate both forward (mode spreading) and reverse (mode seeking) Kullback-Leibler divergences for distillation from N T = 1.82B to N S = 546M. The forward KLD D KL (p T ||q S ) (Equation <ref type="formula">7</ref>), minimizes L forward = H(p T , qS ) -H(p T ), where H(p T ) is dropped during optimization as it depends on only fixed teacher parameters. In contrast, the reverse KLD D KL (q S ||p T ) requires explicitly computing the student's entropy, L reverse = H(q S , pT ) -H(q S ).</p><p>The forward KL achieves a lower data cross-entropy compared to the reverse KL (Table <ref type="table" target="#tab_28">7</ref>), with an average improvement of 0.28 nats. This suggests that explicitly regularizing with respect to the student's entropy during training may not provide additional benefits for distillation quality. Given both the improved performance and reduced computational overhead of forward KL (which avoids computing student entropy), we recommend using standard forward KL for distillation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Parameters and Floating Operation Estimation</head><p>Here we outline the number of parameters (Appendix H.2) and the number of FLOPs per token (Appendix H.3) for our experimental settings. The symbol notation is provided in Table <ref type="table" target="#tab_29">8</ref>. For our scaling laws, we find, as in <ref type="bibr" target="#b54">Kaplan et al. (2020)</ref> using that the number of non-embedding-parameters provides the cleanest fit and extrapolation behavior.</p><p>Our expressions for approximate compute (FLOPs per token) differ from prior work in that we are interested in small models that are capable. This means we are unable to ignore the context-dependent term that arises from the quadratic computational complexity of the attention mechanism. As our architectures are fixed aspect ratio, there is a modified approximation we can use. This expression is discussed in Appendix H.1</p><p>For ease of reference, we provide a comparison of the expressions we use to commonly used existing expressions <ref type="bibr" target="#b54">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b26">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b72">Narayanan et al., 2021)</ref>, and provide comments for significant differences. 8 It was shown in <ref type="bibr" target="#b81">Porian et al. (2024)</ref> that ignoring the embedding parameters and FLOPs can lead to systematic estimation bias for small models, and is one of the primary drivers between different exponents reported in <ref type="bibr" target="#b54">Kaplan et al. (2020) and</ref><ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref>. We find that the the non-embedding parameters gives a tighter scaling behavior. However, in the fixed-aspect-ratio setting, we are able to use both the non-embedding parameters in the scaling law and the approximate total compute simultaneously, removing estimation bias. Indeed, in the supervised setting, our coefficients a and b are consistent with those from Hoffmann et al. ( <ref type="formula">2022</ref>) (see Table <ref type="table">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Model architecture</head><p>All models are based on <ref type="bibr">Gunter et al. (2024)</ref> and are trained using AXLearn <ref type="bibr" target="#b4">(Apple, 2023)</ref>. All models use decoupled weight decay <ref type="bibr" target="#b61">Loshchilov &amp; Hutter (2019)</ref> of 10 -4 for regularization, as well as a simplified version of ¬µP <ref type="bibr" target="#b104">(Yang &amp; Hu, 2021;</ref><ref type="bibr">Yang &amp; Littwin, 2023;</ref><ref type="bibr" target="#b31">Yang et al., 2022;</ref><ref type="bibr">Wortsman et al., 2023;</ref><ref type="bibr" target="#b114">Yang et al., 2023)</ref>, following what is described as ¬µP (simple) in <ref type="bibr" target="#b100">(Wortsman et al., 2024)</ref>. Because of ¬µP (simple), we fix the learning rate to 1e-2 across all model sizes. Multiheaded attention (MHA) is used (g size = 1), with Pre-Normalization <ref type="bibr">(Nguyen &amp; Salazar, 2019)</ref> using RMSNorm <ref type="bibr" target="#b111">(Zhang &amp; Sennrich, 2019)</ref>. We train all models with a sequence length of n ctx = 4096, with RoPE <ref type="bibr">(Su et al., 2024)</ref> positional embeddings (base frequency set to 500k). All model architectures in this work are presented in Table <ref type="table" target="#tab_10">13</ref>, have a fixed aspect ratio d model = 128 and a fixed ffn ratio œÅ ffn = 8/3 coupled with gated linear activation (n ffn = 3).</p><p>Table <ref type="table" target="#tab_10">13</ref>. The models used in this work. The different parameter values and FLOPs per token are shown in billions. N is the number of non-embedding parameters and isthe value we use in our scaling laws. Ntotal counts all parameters in the model.Cfwd is the total number of forward FLOPs per token given by the fulltotal in Tables <ref type="table" target="#tab_0">11</ref> and <ref type="table" target="#tab_6">12</ref>.C fwd-approx(2N ) is the estimated value of forward FLOPs per tokenbased on the 2N approximation, and is accompanied by its relative error.C fwd-approx(2N +œÉ) is the estimated value of forward FLOPs per tokenbased on the approximation given in Equation <ref type="formula">69</ref>, and is accompanied by its relative error.</p><p>The C fwd-approx(2N +œÉ) is the one we use in this work. Name N (B) N total (B) n layers d model d ff C fwd (B) C fwd-approx(2N ) (B) C fwd-approx(2N +œÉ) (B) 103M 0.1028 0.1363 8 1024 2816 0.3411 0.2056 (-39.74%) 0.3398 (-0.39%) 143M 0.1434 0.1811 9 1152 3072 0.4487 0.2867 (-36.10%) 0.4471 (-0.34%) 198M 0.1983 0.2402 10 1280 3456 0.587 0.3965 (-32.44%) 0.5853 (-0.29%) 266M 0.2657 0.3118 11 1408 3840 0.7524 0.5314 (-29.38%) 0.7505 (-0.25%) 340M 0.3398 0.3901 12 1536 4096 0.9333 0.6796 (-27.19%) 0.9312 (-0.22%) 435M 0.4348 0.4893 13 1664 4480 1.158 0.8695 (-24.91%) 1.156 (-0.19%) 546M 0.546 0.6047 14 1792 4864 1.417 1.092 (-22.96%) 1.415 (-0.17%) 664M 0.6636 0.7265 15 1920 5120 1.692 1.327 (-21.54%) 1.689 (-0.15%) 810M 0.8096 0.8767 16 2048 5504 2.025 1.619 (-20.03%) 2.022 (-0.14%) 975M 0.9755 1.047 17 2176 5888 2.4 1.951 (-18.69%) 2.397 (-0.12%) 1.15B 1.147 1.222 18 2304 6144 2.787 2.293 (-17.72%) 2.784 (-0.11%) 1.35B 1.355 1.434 19 2432 6528 3.25 2.709 (-16.65%) 3.247 (-0.10%) 1.59B 1.586 1.67 20 2560 6912 3.763 3.172 (-15.70%) 3.759 (-0.09%) 1.82B 1.821 1.909 21 2688 7168 4.284 3.642 (-14.99%) 4.28 (-0.09%) 2.1B 2.102 2.194 22 2816 7552 4.899 4.203 (-14.21%) 4.895 (-0.08%) 2.41B 2.41 2.506 23 2944 7936 5.571 4.819 (-13.49%) 5.567 (-0.07%) 2.72B 2.718 2.819 24 3072 8192 6.246 5.436 (-12.96%) 6.241 (-0.07%) 3.08B 3.082 3.187 25 3200 8576 7.034 6.165 (-12.36%) 7.03 (-0.06%) 3.48B 3.478 3.587 26 3328 8960 7.887 6.956 (-11.81%) 7.883 (-0.06%) 3.87B 3.87 3.983 27 3456 9216 8.736 7.74 (-11.40%) 8.731 (-0.05%) 4.33B 4.329 4.446 28 3584 9600 9.72 8.658 (-10.93%) 9.715 (-0.05%) 4.82B 4.823 4.944 29 3712 9984 10.78 9.646 (-10.49%) 10.77 (-0.05%) 5.31B 5.309 5.434 30 3840 10240 11.82 10.62 (-10.16%) 11.81 (-0.05%) 5.87B 5.873 6.003 31 3968 10624 13.02 11.75 (-9.78%) 13.01 (-0.04%) 6.48B 6.476 6.611 32 4096 11008 14.3 12.95 (-9.43%) 14.29 (-0.04%) 7.07B 7.066 7.204 33 4224 11264 15.56 14.13 (-9.16%) 15.55 (-0.04%) 7.75B 7.747 7.889 34 4352 11648 17 15.49 (-8.85%) 16.99 (-0.04%) 8.47B 8.47 8.617 35 4480 12032 18.52</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Extrapolations of the Distillation Scaling Law. The distillation scaling law (Equation8) is fitted on weak students (LS &gt; 2.3) for a range of teachers with losses LT . Solid lines represent predicted model behavior for unseen teachers for a given student configuration (interpolation), and dashed lines represent predicted model behavior outside of seen teachers and for the strong student region (LS ‚â§ 2.3). As shown, the student can outperform the teacher (see Figures2, 3and 41 for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>. One could use compute optimal models whose size parameters N * and number of training tokens D * gives the lowest crossentropy subject to a compute constraint C N * , D * = arg min N,D L(N, D) s.t. FLOPs(N, D) = C. (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. IsoFLOP Teacher/Fixed M Students. (a) One (of four) student sizes trained with a MS = DS/NS = 20 are distilled from teachers with four IsoFLOP profiles. See Appendix E.4, Figure 38b for all profiles. (b) All profiles against teacher cross-entropy. Horizontal (vertical) dashed lines show student supervised cross entropy LS (student size NS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Fixed M Teacher/Fixed M Student. Students of two sizes trained with different MS = DS/NS = 20 ratios are distilled from teachers with MT = DT /NT ‚âà 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Scaling law fits. (a) The supervised scaling law (Equation 1) applied to the data in Figure 36a. (b) Our distillation scaling law (Equation 8) applied to the data in Figures 2 to 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Fixed-M Teacher/IsoFLOP students (data). For a student size NS and token budget DS, the cross-entropy difference between best case distillation and supervised learning. Blue indicates distillation outperforms supervised learning, red otherwise. The white horizontal dashed line indicates teacher size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure8. Compute optimal distillation student performance. For four student sizes , the best cross-entropy each student can achieve the five scenarios considered as total compute is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>D. 4</head><label>4</label><figDesc>.4 Distillation (teacher inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.5 Distillation (teacher pretraining) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.6 Distillation (teacher pretraining + inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.7 Optimal teacher training and student distillation tokens . . . . . . . . . . . . . . . . . . . . . . . D.4.8 Optimal teacher size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Compute and data efficiency gains for distillation compared to supervised learning . . . . . . . . . . . . E Additional Results E.1 Downstream evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Teachers used in distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Fixed-M teacher/fixed-M students and the capacity gap . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Full distillation scaling law IsoFLOP profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Distillation scaling law IsoFLOP optima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Distillation with infinite data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 Weak-to-strong generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8 Model calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.1 Teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.2 198M students trained on 20N tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.3 198M Students trained on 128B tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F Scaling coefficients F.1 Supervised scaling law coefficient estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Distillation scaling law coefficient estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Scaling law coefficients parameteric fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G Distilling language models in practice G.1 Mixing coefficient (Œª) sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Temperature (œÑ ) sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Learning rate (Œ∑) sensitivity analysis, verification of ¬µP for distillation . . . . . . . . . . . . . . . . . . . G.4 Distribution truncation methods: Top-k and Top-p sensitivity . . . . . . . . . . . . . . . . . . . . . . . . G.5 Forward and reverse KL divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H Parameters and Floating Operation Estimation H.1 Alternative approximation for FLOPs per token as a function of N . . . . . . . . . . . . . . . . . . . . . H.2 Model parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.3 FLOPs per token . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>Beyond empirical insights,<ref type="bibr" target="#b64">Menon et al. (2020)</ref> established a bias-variance tradeoff for the student, quantifying how access to teacher logits can significantly enhance learning. Meanwhile,Pareek et al. (2024)  investigated self-distillation, where the student and teacher share the same architecture and size, to assess the potential gains from repeatedly applying knowledge distillation. While most studies assume the teacher is a larger model, recent work explores weak-to-strong generalization, where a weaker model distills knowledge into a stronger one. This concept, introduced byBurns et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Case 1: m &lt; n. (Student error is non-increasing in m) Claim. For 1 ‚â§ m &lt; n, we have e student (m + 1, n, T, D) ‚â§ e student (m, n, T, D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>e</head><figDesc>student (m + 1, n, T, D) ‚â• e student (m, n, T, D), demonstrating that the error increases monotonically with m once m ‚â• n. Conclusion (U-shaped trend). Combining these two cases: For 1 ‚â§ m &lt; n : e student (m, n, T, D) monotonically decreasing in m, For m ‚â• n : e student (m, n, T, D) monotonically increasing in m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Distillation in kernel regression. We randomly sample the Œ± = {Œ±1, ..., Œ±1000} coefficients of the target function uniformly in the range [-1, 1]. We fix T = 5, D = 4.5 and compute the optimal student and teacher errors according to Lemmas C.1 and C.2 for various values of n (dashed curves), and for m ‚àà [1...1000].As can be seen, the student error exhibits a U shaped error curve as predicted by the theory, where the error starts to increase when m ‚â• n. The black solid line indicates the teacher error, which always decreases with increasing m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Student performance when varying teacher width. (a) Student cross-entropy as teacher width dffn is varied. (b) Student accuracy as teacher width dffn is varied. Bands show the (25%,75%) values across four trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .Figure 13 .Figure 14 .</head><label>121314</label><figDesc>Figure 12. Student performance when varying teacher training data. (a) Student cross-entropy as teacher training data is varied. (b) Student accuracy as teacher training data is is varied. Bands show the (25%,75%) values across four trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 .</head><label>17</label><figDesc>Figure17. Compute optimal distillation student cross-entropies. For eight student sizes, the optimal student validation cross-entropy L * S in each of the distillation scenarios considered as the total compute is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Compute optimal configuration contours for distillation (best case). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for best case in Figure 17 for a range of student sizes. (N * T , D * T ) are the supervised compute optimal combination giving rise to L * T in Figure 18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>T= 0 ,</head><label>0</label><figDesc>and tokens D * T move together to produce the L * T in Figure 18. Again, the exact values of N * T , D * T in Figure 20 represent the supervised compute optimal solution for producing the L * T , but are not the only solution in this compute scenario, since N * T , D * T are not uniquely determined by the compute constraint. D.4.4. DISTILLATION (TEACHER INFERENCE) In the distillation (teacher inference) scenario, Œ¥ Lgt T = 1 , Œ¥ Pre T which means that we account for compute associated with the standard supervised learning case as well as the cost for producing the logits for the student FLOPs(N S , D S , N T , D T ) ‚âà 3F (N S )D S Student Training + F (N T )D S Teacher Logits .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. Compute optimal configuration contours for distillation (teacher inference). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher inference in Figure 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 .</head><label>23</label><figDesc>Figure23. Compute optimal allocations for distillation (teacher inference). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation29for the compute optimal values in Figure22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><figDesc>Figure25. Compute optimal configurations for distillation (teacher pretraining). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining in Figure17. This is a one-dimensional size of Figure24.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 26 .</head><label>26</label><figDesc>Figure26. Compute optimal allocations for distillation (teacher pretraining). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation29for the compute optimal values in Figure25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 30 .</head><label>30</label><figDesc>Figure30. Compute optimal distillation student tokens. For eight student sizes, the compute optimal student tokens D * S giving rise to the student cross-entropies for all compute scenarios, including supervised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 31 .</head><label>31</label><figDesc>Figure31. Compute optimal distillation teacher tokens. For eight student sizes, the compute optimal teacher tokens D * T giving rise to the student cross-entropies for all compute scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 32 .</head><label>32</label><figDesc>Figure32. Compute optimal distillation teacher size. For eight student sizes, the compute optimal teacher size N * T giving rise to the student cross-entropies for all compute scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 35 .</head><label>35</label><figDesc>Figure 35. All student downstream evaluations. For a discussion of the individual metrics and datasets, see Appendix E.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 36 .</head><label>36</label><figDesc>Figure 36. Supervised IsoFLOPs. (a) The cross-entropy of supervised models trained with either a Chinchilla optimal M = D/N ‚âà 20 or on 512B tokens. (b) The cross-entropy supervised models trained with four ISOFLOP profiles C ‚àà {3 √ó 10 19 , 10 20 , 3 √ó 10 20 , 10 21 }. (c) The optimal supervised parameters N * (C) = arg min N L(C) for each IsoFLOP profile, and the loss L * (C) achieved by that model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>E. 3 .Figure 37 .</head><label>337</label><figDesc>Figure 37. Fixed M Teacher/Fixed M Student. Students of three sizes trained with different MS = DS/NS = 20 ratios are distilled from teachers with MT = DT /NT ‚âà 20. This is a more complete version of Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><figDesc>Teacher/Fixed M Student profiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 38 .</head><label>38</label><figDesc>Figure 38. Supervised IsoFLOPs. (a) Teachers of six sizes with MT = DT /NT ‚âà 20 are distilled into Students with four IsoFLOP profiles, and a small number with CS = 3 √ó 10 21 . The horizontal grey and vertical black dashed lines indicate teacher cross entropy LT and size NT respectively. (b) Students of four sizes trained with a M = DS/NS = 20 are distilled from teachers with four IsoFLOP profiles. Horizontal (vertical) dashed lines indicate student supervised cross entropy LS (student size NS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><figDesc>Fixed M -Ratio Student/Teacher ISOFlop optima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 39 .</head><label>39</label><figDesc>Figure 39. ISOFlop optima. a) The optimal student parameters N * S = arg min N S L(NS) that give the lowest student validation loss for each teacher-student combination shown in Figure 38a. The dashed lines correspond to the validation loss of the optimal supervised models trained with the four corresponding compute budget. b) The optimal teacher parameters N * T = arg min N T L(TS) that give the lowest student validation loss for each teacher-student combination shown in Figure 3. The black dashed line correspond to the validation loss of a M = D/N = 20 supervised model of the indicated student size. In both figures, the shaded region corresponds to where weak to strong generalization may occur, as NS &gt; NT (see Appendix E.7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 40 .</head><label>40</label><figDesc>Figure40. Scaling behavior in the infinite data regime. For the optimal choice of teacher, the loss achieved by all student sizes under distillation is consistent with the loss achievable by supervised learning. This is not true for any choice of teacher, only the optimal one, which can be determined through numerical optimization of the provided distillation scaling laws (see Section 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 41 .</head><label>41</label><figDesc>Figure 41. Fixed M-Ratio Teacher varying student data. We look at strong to weak generalization (left) and weak to strong (right) distillation, varying distillation tokens DS ‚àà [8B, 512B].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 43 .</head><label>43</label><figDesc>Figure 43. Student calibration (data). Calibration of the student with respect to the actual data labels, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate student overconfidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><figDesc>Distillation target: teacher top-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 44 .</head><label>44</label><figDesc>Figure 44. Student calibration (teacher top-1). Calibration of the student with respect to the teacher's top 1, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For axis definitions and the figure legend, refer to Figure 42. Blue points above the dashed line indicate the student is underconfident.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><figDesc>Train target: teacher top 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 45 .</head><label>45</label><figDesc>Figure 45. Student calibration (teacher distribution). Calibration of the student with respect to the teacher's distribution, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For ECE calculation on the full distribution, see Equation 39. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate student overconfidence, while points above the dashed line indicate underconfidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 46 .</head><label>46</label><figDesc>Figure 46. Student calibration (under teacher confidence bins). Calibration of the student with respect to the teacher's confidence bins, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For ECE calculation on the full distribution, see Equation 39. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate the teacher is less confident than the student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 47 .</head><label>47</label><figDesc>Figure 47. Student calibration (data). Calibration of the student with respect to the actual data labels with increased training tokens. Compare to Figure 43 for the effect of tokens and refer to Figure 42 for legend and axis explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><figDesc>Train target: teacher distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><figDesc>Train target: teacher top 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 48 .</head><label>48</label><figDesc>Figure 48. Student calibration (teacher top 1). Calibration of the student with respect to the teacher's top 1 when the training tokens have increased. Compare to Figure 44 for the effect of tokens and refer to Figure 42 for legend and axis explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><figDesc>reveals worse calibration uniformly across all confidence bins.Train target: teacher Top-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Figure 49 .</head><label>49</label><figDesc>Figure 49. Student calibration (teacher distribution). Calibration of the student with respect to the teacher's distribution as the number of training tokens increases. Compare to Figure 45 for the effect of tokens and refer to Figure 42 for legend and axis explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figure 50 .</head><label>50</label><figDesc>Figure 50. Student calibration (teacher distribution). Calibration of the student with respect to the teacher' confidence bins distribution as the number of training tokens increases. Compare to Figure 46 for the effect of tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Figure 52 .</head><label>52</label><figDesc>Figure 52. Temperature œÑ Sensitivity Analysis. Students of four sizes NS ‚àà {198M, 546M, 975M, 1.82B} trained with a M = DS/NS = 20 ratio are distilled from teachers of sizes NT ‚àà {546M, 1.82B, 4.82B, 7.75B} trained with a M = DT /NT = 20 ratio with different distillation temperatures œÑ ‚àà [0.5, 10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Figure 53 .</head><label>53</label><figDesc>Figure 53. Learning Rate Œ∑ Sensitivity Analysis. Students of four sizes NS ‚àà {198M, 546M, 975M, 1.82B} trained with a M = DS/NS = 20 ratio are distilled from teachers of sizes NT ‚àà {546M, 1.82B, 4.82B, 7.75B} trained with a M = DT /NT = 20 ratio with different learning rates Œ∑ ‚àà [0.001, 0.1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 54 .</head><label>54</label><figDesc>Figure 54. Distribution truncation analysis. Top-k (left) and Top-p (right) truncation of teacher logits z (i) T for student-teacher pairs with NS in {198M, 546M, 1.82B} and corresponding NT in {7.75B, 1.82B, 546M}. Standard truncation degrades performance: at k = 128, validation loss increases by 0.11 nats compared to full distillation (k = 32768), while Top-p with p = 0.9 degrades by 0.13 nats versus p = 1.0. Using Œª = 0.7 with k = 128 maintains performance within 0.01 nats while enabling efficient post-hoc training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><figDesc>The set defintions S M for Top-k and Top-p areS k (p) = Top(p, k), S p (p) = {a : b‚ààsort‚Üì( p,a)p ‚â§ p}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Expressions related to scaling laws used in this work. In each case, S always refers to student and not supervised. NS / NT The number of model/student/teacher non-embedding parameters. Whenever we mention parameters in text, we always mean non-embedding parameters unless explicitly stated otherwise. See Appendix H.2 for more details. D / DT The number of tokens the model/teacher is pretrained on.</figDesc><table><row><cell>Expression Meaning</cell></row><row><cell>N /</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Scenarios considered in our scaling law applications.</figDesc><table><row><cell>Compute Scenario</cell><cell>Œ¥ Lgt T</cell><cell>Œ¥ Pre T</cell><cell>Description</cell></row><row><cell>Best case (fully amortized teacher) Teacher inference Teacher pretraining Teacher pretraining + inference</cell><cell>0 1 0 1</cell><cell>0 0 1 1</cell><cell>The teacher produces no additional FLOPs and so we are free to choose the teacher L  *  T that mini-mizes the student cross-entropy. We don't account for the teacher cost because the teacher already exists, or we intend to use the teacher as e.g. a server model. We still need to pay to use it for distilling a student. The teacher needs training, but we store the logits for re-use, either during training, or after training for distilling into sufficiently many students. The teacher needs training and we pay for distill-ing into one student, the worst case scenario.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 .</head><label>3</label><figDesc>Optimal compute allocation trends.</figDesc><table><row><cell cols="2">Student size Compute (FLOPs) Allocation</cell></row><row><cell>Small (‚â≤ 3B) Small (‚â≤ 3B) Large (‚â≥ 10B) Large (‚â≥ 10B)</cell><cell>Small (‚â≤ 10 21 ) Mostly teacher pretraining. Large (‚â≥ 10 25 ) Evenly divided between student training and teacher inference, much less on teacher pretraining. Small (‚â≤ 10 21 ) Mostly standard student training. Large (‚â≥ 10 25 ) Equally divided between student training and teacher inference and teacher pretraining.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 .</head><label>5</label><figDesc>Fixed compute distillation strategy. The student performance obtained for four total compute budgets C Total ‚àà {10 21 , 10 22 , 10 23 , 10 24 } FLOPs and four student sizes NS ‚àà {1B, 3B, 10B, 30B} under a teacher of size NT ‚àà [1B, 1T ] and teacher loss LT ‚àà [E, 2.5]. The red line indicates the value of teacher loss L * T (NT ) that results in the best student performance for each teacher size NT . Scenarios considered in our scaling law applications. Same as Table2.The solutions resulting in the losses give guidance on how to scale depending on the use case, and are the result of constrained optimization</figDesc><table><row><cell></cell><cell></cell><cell>C Total = 10 22 2.15 2.20 2.30 2.35 2.40 2.45 2.50</cell><cell>C Total = 10 23 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50</cell><cell>C Total = 10 24 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50</cell><cell>1B</cell></row><row><cell>Teacher Loss L</cell><cell>1.5 2.0 2.5 1.5 2.0 2.5 1B 10B 100B 1T 2.1 5 2.20 2.25 2.3 0 2.35 2.40 2.4 5 2.50 2.5 5 2 .6 0 2. 65 2.70 2.75 2 . 8 0 2.85 2.9 0 2. 95 3 .0 0 3 .0 5 3 .1 0 3 .1 5 3 .2 0 3 .2 5 3 .3 0 3 .3 5 3 .4 0 3 .4 5 3 .5 0 3 .5 5 2.15 2.20 2.25 2.30 2.3 5 2.40 2.45 2.50 2.5 5 2. 60 2 .6 5 2.7 0 2.7 5 2.80 2 .8 5 2 .9 0 2 .9 5 3 .0 0 3 .0 5 3 .1 0 3 .1 5 3 .2 0 3 .2 5 3 .3 0 3 .3 5 3 .4 0 1.5 2.0 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.5 5 2 .6 0 2.6 5 2.7 0 2 . 7 5 2.80 2 .8 5 2. 90 2 .9 5 3 .0 0 3 .0 5 3 .1 0 3 .1 5 3 .2 0 3 .2 5 3 .3 0 3 .3 5 3 .4 0 2.5 3 .4 5</cell><cell>2.25 2.50 2.05 2.15 2.25 2.50 2.00 1B 10B 100B 1T 2.55 2.60 2.6 5 2 .7 0 2 .7 5 2 .8 0 2.10 2.20 2.30 2.35 2 .4 0 2.40 2 . 4 5 2.45 2 .5 0 2 . 3 0 1.95 2 .2 5 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 1.95 2.05 2.10 2.15 2. 20 2.25 2.30 2.35 2.40 2.45 2.50</cell><cell>2.25 2.50 2.55 2.00 2.15 2.20 2.45 1.90 1B 10B 100B 1T 2.30 2.35 2.40 2.4 5 2.60 2.05 2.10 2.15 2.20 2.25 2.25 2. 30 2.30 2.35 2.40 2.50 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 1.85 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50</cell><cell cols="2">N S = 2.55 3B 2.40 2.45 2.25 S = N 1B 10B 100B 1T 2.15 2.20 2.25 2.30 2.35 2.40 2.50 2.10 2.20 2.30 2.50 2.45 2.35 2.15 2.05 2.00 1.95 2.05 2.10 2.15 2.20 1. 80 N S = 30B 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 1.95 1.90 N S = 10B 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45</cell></row><row><cell></cell><cell></cell><cell cols="2">Teacher Parameters N T</cell><cell></cell></row></table><note><p>T Figure 16.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 7 .</head><label>7</label><figDesc>Forward vs Reverse KL Divergence for NT = 1.82B to NS = 546M distillation. Reverse KL is slightly more expensive with respect to vocabulary size V due to the entropy calculation.</figDesc><table><row><cell>Method</cell><cell cols="2">Cross-Entropy Computational Cost</cell></row><row><cell>Forward KL Reverse KL</cell><cell>2.42 2.70</cell><cell>O(V ) O(2V )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 8 .</head><label>8</label><figDesc>The notation we use for parameter and FLOPs estimation. Group size in Group Query Attention (GQA) nheads/nkv-heads gsize Model aspect ratio dmodel/nlayers œÅmodel Feed-forward ratio dffn/dmodel œÅffn H.1. Alternative approximation for FLOPs per token as a function of N From Table 10 and Equation 71 and Table 12 we can read our approximate values for non-embedding parameters and total compute (dropping contributions from normalization layers) as 8 ffn œÅ ffn + 2n layers n ctx d model (58) = 2N + 2n layers n ctx d model + 2n vocab d model .</figDesc><table><row><cell>Component</cell><cell></cell><cell></cell><cell></cell><cell>Notation</cell></row><row><cell>Sequence length/context size Vocabulary size Number of blocks/layers Number of query heads Number of key/value heads Model/embedding dimension Head dimension Feed-forward dimension Number of feed-forward linears</cell><cell></cell><cell></cell><cell></cell><cell>nctx nvocab nlayers nheads nkv-heads dmodel dhead dffn nffn</cell></row><row><cell>N = n layers d 2 model 2 +</cell><cell cols="2">2 g size</cell><cell cols="2">+ n ffn œÅ ffn</cell><cell>(57)</cell></row><row><cell cols="2">C Forward = 2n layers d 2 model 2 +</cell><cell cols="2">2 g size</cell><cell>+ n (59)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Apple</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>University of Oxford, UK. Work done during an internship at Apple. For a full breakdown of contributions see Appendix J. Correspondence to: Dan Busbridge &lt;dbusbridge@apple.com&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p><ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref> use Œ≥ = 1 whereas<ref type="bibr" target="#b54">Kaplan et al. (2020)</ref> use Œ≤ = 1. We observe a significantly better fit and extrapolation without coefficient tying, which may be due to our use of Maximal Update Parameterization (¬µP) (see Section 4.1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>We do not write this as z(&lt;i)  to avoid confusion with the sequence z (&lt;i) = (z (1) , . . . , z (i-1) ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>We generally refer to these as fixed-m models rather than Chinchilla-optimal models as we do not yet know whether M ‚âà 20 is a good choice in this specific setting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>Appendix G.4 evaluates distribution truncation via Top-p and Top-k to mitigate the overhead of computing these logits online.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>The level of compute at which this happens is larger for larger models, see Figure17for specific values.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>¬µP only guarantees learning rate optimality when varying widths. Empirically, the learning rate is also stable when changing the model depth within a reasonable range<ref type="bibr" target="#b31">(Yang et al., 2022)</ref>. To guarantee transfer across model depths one can additionally employ depth-¬µP<ref type="bibr" target="#b108">(Yang et al., 2024)</ref>, although we do not use depth-¬µP here.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Pierre Ablin</rs>, <rs type="person">Samira Abnar</rs>, <rs type="person">Samy Bengio</rs>, <rs type="person">Miguel Sarabia del Castillo</rs>, <rs type="person">Federico Danieli</rs>, Eeshan</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Distillation scaling law applications (additional results)</head><p>In this section, we present results referenced in Section 5. We explore the best-case scenario for distillation under fixed student tokens or compute, as well as under fixed teacher size or compute, while accounting for teacher inference. These results provide further insights into the optimal distillation strategies in different resource-constrained settings. D.1. A contradiction with patient teachers <ref type="bibr" target="#b10">Beyer et al. (2022)</ref> showed in computer vision that a good teacher is:</p><p>1. Patient. Distillation works best when training for a large number of epochs, and 2. Consistent. The teacher and the student see the same views of the data under an augmentation policy.</p><p>Our setting automatically satisfies consistency as there is no augmentation policy. There is a remaining question about patience, which in our scenario corresponds to the large D S limit. We observe that for a given student size:</p><p>1. If the teacher is optimally chosen for the student, distilling on a large number of tokens produces the same result as training the model in a supervised way on the same number of tokens (Appendix E.6).</p><p>2. Otherwise supervised learning outperforms distillation (Section 5.3).</p><p>The behavior we observe is expected if solutions a model can access are limited only by function space. Uncertain of the driver behind our conclusion differences, we note the differences between our experimental setups in Table <ref type="table">4</ref>.  <ref type="figure">13a</ref> and <ref type="figure">13b</ref>, when the teacher size, student size, and number of student tokens are held constant, increasing the number of teacher training tokens makes distillation more favorable than supervised learning. This advantage arises because the teacher, with access to more training tokens, can better learn the approximation of the language distribution. As a result, the teacher's learned distribution become more informative for the student to follow, thus improving the student's performance. Note that for a fixed student size and compute, the teacher must be sufficiently large and well-trained; otherwise, supervised learning will outperform distillation. Without adequate teacher size or training, the student may not benefit from the distillation process, leading to inferior performance compared to direct supervised learning.</p><p>We also see that the scatter data matches up well with the contour colors, despite these contour beings a difference of two scaling laws, providing a verification of our setup.</p><p>Supervised learning always outperforms distillation given enough student compute or tokens. The trend observed in Figure <ref type="figure">14</ref> mirrors that of Section 5.1. It demonstrates that, for a fixed teacher size and compute, supervised learning can outperform distillation when the student's compute is sufficiently large. With enough resources allocated to the student, it can learn more effectively from the data directly, making distillation less advantageous in comparison. This advantage only happens at a compute budget that grows with student size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Compute and data efficiency gains for distillation compared to supervised learning</head><p>In this final section, we use the compute-optimal strategies developed through Appendices D.4.3 to D.4.6 and understand, for each distillation compute scenario (Table <ref type="table">5</ref>) if it is more compute and/or data efficient to use distillation compared to supervised learning in order to produce a desired model (i.e. of a given size N S with a desired performance, measured in cross-entropy L S ).</p><p>In Figure <ref type="figure">33</ref> we show the amount of compute needed to distill a student of a given size to a given cross-entropy as a multiple of the compute that supervised learning needs to produce the same result. We do this for for each of the distillation compute scenarios, whose optimal configurations are given in Appendices D.4.3 to D.4.6. In Figure <ref type="figure">34</ref> we show the same, except we show the number of tokens needed to distill a student of a given size to a given cross-entropy as a multiple of the number of tokens that supervised learning needs to produce the same result. Our distillation token accounting depends on compute scenario:</p><p>i.e. we only count teacher tokens if the teacher pretraining cost is also included in the compute cost (see Equation <ref type="formula">29</ref>). </p><p>Figure <ref type="figure">33</ref>. Compute optimal distillation compute ratios. For eight student sizes, the amount of supervised compute needed to produce a student of the indicated size and cross-entropy. The horizontal dashed line indicates the break-even point, when doing supervised leaning is as computationally efficient as the corresponding distillation compute scenario. Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing a model of the indicated size and cross-entropy. The vertical dashed line indicates the lowest cross-entropy achievable by that student.</p><p>When teacher training is discounted, distillation is often more efficient. In Figure <ref type="figure">33</ref>, the base case (blue) and teacher inference (orange) compute scenarios are below the grey dashed line for cross-entropies slightly above the lowest possible cross-entropy (vertical grey dashed line), meaning less compute is needed for distillation than supervised learning. This compute efficiency translates into data efficiency (see Figure <ref type="figure">34</ref>).</p><p>To produce the strongest student possible, supervised learning is more efficient. In Figures <ref type="figure">33</ref> and <ref type="figure">34</ref>, the base case (blue) and teacher inference (orange) compute scenarios attain values larger than one as the target cross-entropy L S approaches the limiting value L(N = N S , D = ‚àû) for each student size N S , (vertical dashed line). This suggests i) the existence of a more efficient training strategy where distillation is used as an initial training stage, with a transition to Distillation Scaling Laws</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8. Model calibration</head><p>Calibration in LMs refers to the alignment between the model's confidence in its predictions and the actual correctness of those predictions. Well-calibrated models provide confidence scores that accurately reflect their probability of correctness, enabling more decision-making. Expected Calibration Error (ECE) is a common metric to quantify miscalibration, and measures the difference between predicted confidence and actual accuracy across multiple confidence intervals</p><p>where To measure ECE, we use M = 21 bins uniformly partitioned across the output probability space. Accuracy and confidence are computed in the standard manner: the predicted label is determined via the argmax over the output probabilities for each prediction, and the confidence is defined as the maximum probability assigned to the predicted label. Accuracy is then measured as the proportion of instances where the predicted label matches the ground truth. Notably, this approach focuses solely on the maximum probability prediction, disregarding the calibration of lower-probability predictions. To assess calibration across the entire output distribution rather than just the top prediction, alternative metrics could be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8.1. TEACHERS</head><p>In Figure <ref type="figure">42</ref>, we see that the ECE value across different sizes of teachers. For all models, the ECE ranges between 0.4% and 0.6%, suggesting that the models' confidence estimates closely align with their actual accuracies. We also observe that for each plot, the blue points, i.e. , the teacher's actual accuracy for predictions falling into specific confidence intervals, closely follow the diagonal, which shows that the models are well-calibrated. There is a small deviation at low and high confidence values denoted by the orange points. where</p><p>We make no assumptions about the relationships between the values and optimize</p><p>with a Huber Œ¥ = 10 -4 , where</p><p>T and L (i)</p><p>S are the student model size, number of training distillation tokens, the teacher pretraining loss and the student validation loss on the data achieved by the i-th run. We fit on 697 samples over a grid of L-BFGS-B initializations given by: log A ‚Ä≤ ‚àà {0., 5., 10., 15., 20.}, log B ‚Ä≤ ‚àà {0., 5., 10., 15., 20.}, Œ± ‚Ä≤ ‚àà {0., 0.5, 1.}, Œ≤ ‚Ä≤ ‚àà {0., 0.5, 1.}, Œ≥ ‚Ä≤ ‚àà {0., 0.5, 1.}, c 0 ‚àà {0., 0.5, 1., 1.5}, c 1 ‚àà {0., 0.5, 1., 1.5}, f 1 ‚àà {0., 0.5, 1., 1.5}, log d 1 ‚àà {-1., -0.5, 0., 0.5, 1.}. The L S ‚â• 2.3 case corresponds to 551 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Scaling law coefficients parameteric fit</head><p>The fitting procedure outlined in Appendices F.1 and F.2 applied to data described in Section 4.2 yields the scaling coefficients and associated confidence intervals shown in Table <ref type="table">6</ref>. Note in the supervised case, our values of a and b are consistent with those of <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref>.</p><p>Table <ref type="table">6</ref>. Scaling law parameter estimates accompanied by 90% confidence intervals obtained by bootstrapping (4096 resamples) following the procedure of <ref type="bibr">Besiroglu et al. (2024)</ref>. a = Œ≤/(Œ± + Œ≤) and b = Œ≤/(Œ± + Œ≤) are the supervised compute optimal scaling estimates for N and D respectively <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised</head><p>Distillation We also note that our irreducible error term is lower than the one in <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref>. We suspect this is due to our use of ¬µP <ref type="bibr" target="#b104">(Yang &amp; Hu, 2021;</ref><ref type="bibr">Yang &amp; Littwin, 2023;</ref><ref type="bibr" target="#b31">Yang et al., 2022;</ref><ref type="bibr">Wortsman et al., 2023;</ref><ref type="bibr" target="#b114">Yang et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Distilling language models in practice</head><p>In the following analyses, we explore the sensitivity of student performance under modification of distillation hyperparameters. We demonstrate that the pure distillation setting (Œª = 1, Appendix G.1), unit temperature (œÑ = 1, Appendix G.2), and learning rate Œ∑ = 0.01 (Appendix G.3) under ¬µP <ref type="bibr" target="#b104">(Yang &amp; Hu, 2021;</ref><ref type="bibr">Yang &amp; Littwin, 2023;</ref><ref type="bibr" target="#b31">Yang et al., 2022;</ref><ref type="bibr">Wortsman et al., 2023;</ref><ref type="bibr" target="#b114">Yang et al., 2023)</ref> provides robust performance across model scales, while distribution truncation methods (Top-k, Top-p) degrade performance unless combined with ground-truth next-token prediction (Appendix G.4). Finally, we verify that forward KL divergence distillation, D KL (p T ||q S ), consistently outperforms reverse KL (Appendix G.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation Scaling Laws</head><p>For ease of reference, we restate the components of the token-level loss for the student:</p><p>T , z</p><p>See Section 2 for a discussion of each of the terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Mixing coefficient (Œª) sensitivity analysis</head><p>The distillation process combines two loss components: knowledge transfer from the teacher, ŒªL KD (z</p><p>S ), and direct learning from data, (1Œª)L NTP (x (i) , z (i) S ), weighted by the mixing coefficient Œª (Equation <ref type="formula">7</ref>). Our distillation scaling law analysis is performed in the pure distillation setting (Œª = 1). Here we show this simple choice provides robust performance across a wide range of configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation Scaling Laws</head><p>Typically the term 2n layers n ctx d model would be dropped, and the embedding parameters included into the total parameters <ref type="bibr" target="#b26">(Hoffmann et al., 2022)</ref> or discarded <ref type="bibr" target="#b54">(Kaplan et al., 2020)</ref> yielding the expression C Forward and the familiar expression C = 6N D <ref type="bibr" target="#b54">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b26">Hoffmann et al., 2022)</ref>. For our investigation we are interested in small, capable models, which may have a large context, and so both of these terms cannot be ignored in general at the peril of making a systematic error in the region of configuration space we are most interested in. Fortunately, we will see that our choice of fixed aspect ratio œÅ model = d model /n layers architectures allows us a simple to use, more precise estimate. The trick will be to use this fixed aspect ratio to come up with an approximation for n layers and d model as a function of N and œÅ model . With these approximated, the term 2n layers n ctx d model can be represented as a function of N . First define 9</p><p>œâ ‚â° 2 + 2 g size + n ffn œÅ ffn (61) so that</p><p>Then we can substitute in œÅ model ‚â° d model /n layers so that</p><p>and solve for n layers and d model</p><p>The C Forward term can then be represented as a function of N . The context-dependent term becomes</p><p>where</p><p>The vocabulary projection term becomes</p><p>where</p><p>In total</p><p>where œÉ 1 and œÉ 2 are independent of model and context size. In the large N limit, or the small n ctx small n vocab limit this becomes the familiar C Forward = 2N . The backward FLOPS per token is taken as twice the forward FLOPs <ref type="bibr" target="#b15">(Blondel &amp; Roulet, 2024</ref>)</p><p>Given the simplicity of the compute expression as a function of N , the better tightness of fit in the scaling law, the improved intuition that the model size more directly corresponds to work being done by the model, and the predictability of hyperparameters at larger scales, we recommend the scaling law community consider adopting fixed aspect ratio models. 9 In our setting (Appendix I) œâ takes values</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Model parameters</head><p>In Table <ref type="table">9</ref> we present our parameter counting compared to commonly used existing expressions <ref type="bibr" target="#b54">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b26">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b72">Narayanan et al., 2021)</ref>. We present a convenient substitution in Table <ref type="table">10</ref> which can be easier to work with analytically. Our total expressions match the architecture we are using, which includes only gains for the normalization layers, whereas while (Narayanan et al., 2021) has both weights and biases. We account for potential use of <ref type="bibr" target="#b3">(Ainslie et al., 2023)</ref> as well as use of gated linear attention mechanisms which are becoming prevalent in modern architectures <ref type="bibr" target="#b87">(Shazeer, 2020)</ref> including the one used in this work (Appendix I).</p><p>Table <ref type="table">9</ref>. Parameter counts for embedding projector, a single transformer layer, final normalization and output layer. Ours indicates the expressions we use in the paper for the total number of parameters (note that the quantity N that appears in our scaling laws is the number of non-embedding parameters, but still includes parameters associated with normalization layers). Approx. indicates taking the within-section total and dropping all terms that are not at least quadratic in one of dmodel, nvocab, and will be used for estimating the FLOPs per token from a given model size (Appendix H.1), and does not differ significantly from the number of non-embedding parameters.</p><p>Parameters <ref type="bibr" target="#b54">(Kaplan et al., 2020)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3. FLOPs per token</head><p>In Table <ref type="table">11</ref> we present our counting of the total number of FLOPs per token performed per token during a forward pass compared to commonly used existing expressions <ref type="bibr" target="#b54">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b26">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b72">Narayanan et al., 2021)</ref>. We present a convenient substitution in Table <ref type="table">12</ref> which can be easier to work with analytically.</p><p>Beyond the potential accounting for gated linear layers and grouped query attention, the most important discrepancy across methods is how the attention mechanism is handled. As was also noted in <ref type="bibr" target="#b81">Porian et al. (2024)</ref>, the expression used in <ref type="bibr" target="#b54">Kaplan et al. (2020)</ref> is consistent with efficiently computing a causal attention mechanism <ref type="bibr">(Dao et al., 2022;</ref><ref type="bibr" target="#b29">Dao, 2024)</ref> whereas <ref type="bibr" target="#b26">Hoffmann et al. (2022)</ref>; Narayanan et al. ( <ref type="formula">2021</ref>) are consistent with counting attention FLOPs for a bidirectional (non-causal) attention mechanism, where the masked component of the attention matrix (zero by construction) is still being computed. We adopt the efficient expression of assuming a causal computation as this more closely reflects best practice.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C T</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saarikivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.08905</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2412.08905" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Phi-4 technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C T</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kurilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perez-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radmilac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saarikivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Santacroce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Witte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.14219</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.14219" />
		<imprint/>
	</monogr>
	<note>Phi-3 technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024b</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M E</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Thilak</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.12370" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GQA: training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lebr√≥n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.298</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.emnlp-main.298" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023. 2023</date>
			<biblScope unit="page" from="4895" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The axlearn library for deep learning</title>
		<author>
			<persName><surname>Apple</surname></persName>
		</author>
		<ptr target="https://github.com/apple/axlearn" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Accessed</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2025" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explaining neural scaling laws</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sharma</surname></persName>
		</author>
		<idno>CoRR, abs/2102.06701</idno>
		<ptr target="https://arxiv.org/abs/2102" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical study of scaling laws for transfer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barnett</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.16947</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.16947" />
		<imprint>
			<date type="published" when="2024">CoRR, abs/2408.16947, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D13-1160/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">2013. 18-21 October 2013. 2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chinchilla scaling: A replication attempt</title>
		<author>
			<persName><forename type="first">T</forename><surname>Besiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.10102</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.10102" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01065</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.01065" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">June 18-24, 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2022</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhakthavatsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<idno>CoRR, abs/2102.03315</idno>
		<ptr target="https://arxiv.org/abs/2102.03315" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llm</forename><surname>Deepseek</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.02954</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.02954" />
		<imprint/>
	</monogr>
	<note>scaling open-source language models with longtermism. CoRR, abs/2401.02954, 2024</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PIQA: reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1609/AAAI.V34I05</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<idno type="DOI">10.1609/aaai.v34i05.6239</idno>
		<ptr target="https://doi.org/10.1609/aaai" />
		<title level="m">URL</title>
		<imprint>
			<biblScope unit="page">6239</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The elements of differentiable programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roulet</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.14606</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.14606" />
		<imprint>
			<date type="published" when="2024">CoRR, abs/2403.14606, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large language monkeys: Scaling inference compute with repeated sampling</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.21787</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.21787" />
		<imprint>
			<date type="published" when="2024">CoRR, abs/2407.21787, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/1457" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems c0d6bfcb4967418bfb8ac142f64a-Abstract</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bucila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150464</idno>
		<ptr target="https://doi.org/10.1145/1150402.1150464" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</editor>
		<meeting>the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">August 20-23, 2006. 2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weak-to-strong generalization: Eliciting strong capabilities with weak supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aschenbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ghNRg2mEgN" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Broken neural scaling laws</title>
		<author>
			<persName><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=sckjveqlCZ" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cern data centre: Key information</title>
		<author>
			<persName><surname>Cern</surname></persName>
		</author>
		<ptr target="http://information-technology.web.cern.ch/sites/information-technology.web.cern.ch/files/CERNDataCentre_KeyInformation_02" />
		<imprint>
			<date type="published" when="2018-03">March 2018. March2018V1</date>
			<biblScope unit="page" from="2025" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reducing the carbon impact of generative AI inference (today and in 2035)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wijayawardana</surname></persName>
		</author>
		<idno type="DOI">10.1145/3604930.3605705</idno>
		<ptr target="https://doi.org/10.1145/3604930.3605705" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Sustainable Computer Systems, HotCarbon 2023</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Eilam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Josephson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</editor>
		<meeting>the 2nd Workshop on Sustainable Computer Systems, HotCarbon 2023<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023-07-09">9 July 2023</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00489</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00489" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="page" from="4793" to="4801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<ptr target="https://jmlr.org/papers/v24/22-1144.html" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="240" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unified scaling laws for routed language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Szepesv√°ri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="4057" to="4086" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/clark22a.html" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>CoRR, abs/2110.14168</idno>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=mZn2Xyh9Ec" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R√©</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">In</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belgrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oh ; Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.19437</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2412.19437" />
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><surname>Html</surname></persName>
		</editor>
		<editor>
			<persName><surname>Deepseek-Ai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. NeurIPS 2022. November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>302cb81d36e40d5-Abstract W. Deepseekv3 technical report. CoRR, abs/2412.19437, 2024</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khachane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pathria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.03208</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2304.03208" />
	</analytic>
	<monogr>
		<title level="m">The efficiency misnomer. CoRR, abs/2110.12894, 2021</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sravankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinsvark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gregerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spataru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozi√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caucheteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Touret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Allonsius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Albadawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lobanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Korevaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vranes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Billock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spisak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Plawiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.21783</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.21783" />
		<imprint/>
	</monogr>
	<note>The llama 3 herd of models. CoRR, abs/2407.21783, 2024</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Key trends and figures in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Epoch</surname></persName>
		</author>
		<ptr target="https://epoch.ai/trends.Ac-cessed" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2025" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language models scale reliably with over-training and on downstream tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nezhurina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.08540</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.08540" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://zenodo.org/records/12608602" />
		<imprint>
			<biblScope unit="page" from="7" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Keunebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bencomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vanderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Anupama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maalouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Erdenebileg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kotek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shahdadpuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rachapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tantawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davarnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sirovica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.21075</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.21075" />
		<imprint/>
	</monogr>
	<note>W. Apple intelligence foundation language models. CoRR, abs/2407.21075, 2024</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervision complexity and its role in knowledge distillation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net id=8jU7wy7N7mA</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding scaling laws with statistical and approximation theory for transformer neural networks on intrinsically low-dimensional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Havrilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2411.06646</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2411.06646" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aligning AI with shared human values</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Critch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=dNy_RKzJacY" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=d7KBjmI3GmQ" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scaling laws for autoregressive generative modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno>CoRR, abs/2010.14701</idno>
		<ptr target="https://arxiv.org/abs/2010.14701" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep learning scaling is predictable, empirically</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1712.00409</idno>
		<ptr target="http://arxiv.org/abs/1712.00409" />
		<imprint>
			<date type="published" when="2017">2021. 2017</date>
		</imprint>
	</monogr>
	<note>Scaling laws for transfer</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02531</idno>
		<ptr target="http://arxiv.org/abs/1503.02531" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.15556</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.15556" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unveiling the potential of small language models with scalable training strategies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Minicpm</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.06395</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.06395" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">High-dimensional analysis of knowledge distillation: Weak-to-strong generalization and scaling laws</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Ildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Gozeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Taga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mondelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.18837</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2410.18837" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Scaling laws for learning with real and surrogate data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sasoglu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.04376</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2402.04376" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Mixture of parrots: Experts improve memorization more than reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jelassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.19034</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2410.19034" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Sayed</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.06825</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.06825" />
		<imprint>
			<date type="published" when="2023">7b. CoRR, abs/2310.06825, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30">2017. July 30 -August 4. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno>CoRR, abs/2001.08361</idno>
		<ptr target="https://arxiv.org/abs/2001.08361" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1139</idno>
		<ptr target="https://doi.org/10.18653/v1/d16-1139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</editor>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deutsche Forschungs-und Versuchsanstalt f√ºr Luft-und Raumfahrt K√∂ln: Forschungsbericht</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kraft</surname></persName>
		</author>
		<ptr target="https://books.google.co.uk/books?id=4rKaGwAACAAJ" />
	</analytic>
	<monogr>
		<title level="j">Wiss. Berichtswesen d. DFVLR</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>A Software Package for Sequential Quadratic Programming</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Textbooks are all you need II: phi-1.5 technical report</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.05463</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.05463" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mobilellm: Optimizing sub-billion parameter language models for ondevice use cases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=EIGbXbxcUQ" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.03643" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. May 2-4, 2016. 2016</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scaling laws for fine-grained mixture of experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ludziejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Adamczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pi√≥ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krutul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antoniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ciebiera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kr√≥l</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Odrzyg√≥zdz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cygan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaszczur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=yoqdlynCRs" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Teacher&apos;s pet: understanding and mitigating biases in distillation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ph3AYXpwEb" />
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Why distillation helps: a statistical perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno>CoRR, abs/2005.10419</idno>
		<ptr target="https://arxiv.org/abs/2005.10419" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tafti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>H√©liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crepy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rozhdestvenskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.08295</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.08295" />
		<title level="m">Open models based on gemini research and technology</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i04.5963</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i04.5963" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5191" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Selfdistillation amplifies regularization in hilbert space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/2288" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>Balcan f691b58edecadcc9a8691762b4fd-Abstract</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.16264</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2305.16264" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>d89448b63ce1e2e8dc7af72c984c196-Abstract-Conferen html</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Compact language models via pruning and knowledge distillation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Sreenivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.14679</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.14679" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on GPU clusters using megatron-lm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458817.3476209</idno>
		<ptr target="https://doi.org/10.1145/3458817.3476209" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Supinski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Gamblin</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA; SC; St. Louis, Missouri, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-11-14">December 10 -16, 2023, 2023. 2021. November 14-19, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>286282e1be5431ea05262a21f415c-Abstract-Conferen html On student-teacher deviations in distillation: does it pay to disobey International Conference for High Performance Computing, Networking, Storage and Analysis</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">OpenAI and Pilipiszyn, A. Gpt-3 powers the next generation of apps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>St√ºker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="http://website-url.com" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Spoken Language Translation</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</editor>
		<meeting>the 16th International Conference on Spoken Language Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. Novem-ber 2-3, 2019. 2019. 2021. Jan 19, 2025</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fern√°ndez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1144</idno>
		<ptr target="https://doi.org/10.18653/v1/p16-1144" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. The Association for Computer Linguistics</publisher>
			<date type="published" when="2016">August 7-12, 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">+3 phases of compute-optimal neural scaling laws</title>
		<author>
			<persName><forename type="first">E</forename><surname>Paquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.15074</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.15074" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Understanding the gains from repeated self-distillation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2407</idno>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<idno type="DOI">10.48550/arXiv.2407.04600</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.04600" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Reconciling kaplan and chinchilla scaling laws</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.12907</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2406.12907" />
		<imprint>
			<date type="published" when="2024">CoRR, abs/2406.12907, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Pre-training distillation for large language models: A design space exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.16215</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2410.16215" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Porian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmon</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Sadhanala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.18779</idno>
		<idno>doi: 10.48550/ARXIV.2410.18779</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2410.18779" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">2024</biblScope>
			<date type="published" when="2020">CoRR, abs/2406.19146, 2024. 19146. 2020</date>
		</imprint>
	</monogr>
	<note>Resolving discrepancies in compute-optimal scaling of language models A little help goes a long way: Efficient LLM training by leveraging small lms</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcilroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haykal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sezener</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.05530</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.05530" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ram√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tafti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Casbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Momchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdagic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Royal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sinopalnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rogozinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herbison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Senter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eltyshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rasskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klimczak-Plucinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dhand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iljazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lipschultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Newlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Badola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sodhia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Sj√∂sund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Usui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heuermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><surname>Mc-Nealus</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.00118</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.00118" />
	</analytic>
	<monogr>
		<title level="j">L. Gemma</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Improving open language models at a practical size. CoRR, abs/2408.00118, 2024</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A constructive prediction of the generalization error across scales</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryenvpEKDr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Winogrande: an adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474381</idno>
		<ptr target="https://doi.org/10.1145/3474381" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Beyond chinchilla-optimal: Accounting for inference in language model scaling laws</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Portes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doubov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=0bmXrtTDUu" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">GLU variants improve transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/2002.05202</idno>
		<ptr target="https://arxiv.org/abs/2002.05202" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1701.06538</idno>
		<ptr target="http://arxiv.org/abs/1701.06538" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Scaling LLM test-time compute optimally can be more effective than scaling model parameters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.03314</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.03314" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">LLM pruning and distillation in practice: The minitron approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Sreenivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.11796</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.11796" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Does knowledge distillation really work</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Vaughan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/376" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="6906" to="6919" />
		</imprint>
	</monogr>
	<note>c6b9ff3bedbbea56751a84fffc10c-Abstract</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Inadmissibility of the usual estimator for the mean of a multivariate normal distribution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1956">1956</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2023.127063</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2023.127063" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozi√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.13971</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.13971" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.09288</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.09288" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Scipy 1.0-fundamental algorithms for scientific computing in python</title>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Mulbregt</surname></persName>
		</author>
		<author>
			<persName><surname>Scipy</surname></persName>
		</author>
		<idno>CoRR, abs/1907.10121</idno>
		<ptr target="http://arxiv.org/abs/1907.10121" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-4413</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-4413" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Derczynski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</editor>
		<meeting>the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-07">September 7, 2017. 2017</date>
			<biblScope unit="page" from="94" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.14322</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.14322" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>id=d8w0pmvXbZ</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Beyond efficiency: Scaling AI sustainably</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hazelwood</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2024.3409275</idno>
		<ptr target="https://doi.org/10.1109/MM.2024.3409275" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">An empirical analysis of compute-optimal inference for problem-solving with language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.00724</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.00724" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Tensor programs IV: feature learning in infinite-width neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/yang21c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11727" to="11737" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Tensor programs ivb: Adaptive optimization in the infinite-width limit</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Littwin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2308.01814</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2308.01814" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.03466</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.03466" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">A spectral condition for feature learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.17813</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.17813" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Tensor programs VI: feature learning in infinite depth neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=17pVDnpwwl" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Student-friendly knowledge distillation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Quan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2024.111915</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2024.111915" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hellaswag</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1472</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>M√†rquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="12360" to="12371" />
		</imprint>
	</monogr>
	<note>e8a19426224ca89e83cef47f1e7f53b-Abstract</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/2107.12580</idno>
		<ptr target="https://arxiv.org/abs/2107.12580" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Towards the law of capacity gap in distilling language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2311.07052</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2311.07052" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Lifting the curse of capacity gap in distilling language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.249</idno>
		<idno>16.94 (-8.55%) 18.52 (-0.03</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.acl-long.249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023. 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4535" to="4553" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">We rescale the gradients, such that the maximum of the global norm is 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Research direction led by Dan Busbridge, with research framing, question identification, and prioritization done by all authors. Scaling law experiments Fixed aspect ratio models (Appendix I) FLOP counting methods (Appendix H.1), and model implementation done by Dan Busbridge, Amitis Shidani, and Floris Weers. Dataset preparation done by Floris Weers. IsoFLOP experimental design (Section 4.1) done by Dan Busbridge. Teacher training and distillations done by Dan Busbridge, Amitis Shidani, and Floris Weers. Longer training duration (512B token) teachers and students trained by Floris Weers. Scaling law analysis Original scaling law fitting code based on Besiroglu et al. (2024) developed by Amitis Shidani. Generalized, JAX Just In Time (JIT) compilation compatible scaling law fitting code, and numerical minimization approaches for compute optimal analysis (Section 5 and Appendix D) done by Dan Busbridge. Functional form (Equation 8) developed by Dan Busbridge, in collaboration with Jason Ramapuram, Amitis Shidani, Russ Webb, and Floris Weers. Scaling law downstream metrics Implementations of calibration Appendix E.8, Cumulative Distribution Function (CDF) and top-k metrics done by Amitis Shidani. Downstream model evaluations (Appendix E.1) done by Floris Weers. Teacher student capacity gaps Kernel regression demonstration of the capacity gap phenomenon (Appendix C.1) done by Etai Littwin. MLP synthetic demonstration of the capacity gap phenomenon (Appendix C.2) done by Russ Webb. Distilling language models in practice Mixing coefficient sensitivity analysis (Appendix G.1) done by Dan Busbridge and Jason Ramapuram. Temperature (Appendix G.2) and learning rate</title>
		<author>
			<persName><surname>Chowdhery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A cosine learning rate schedule is used with warmup</title>
		<imprint>
			<date type="published" when="2000">2000. 2023. 2020</date>
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
	<note>with a final learning rate of one thousandths of the peak learning rate. A Z-loss For all experiments, the English-only subset of the C4 dataset is used for stability, slightly decreasing norm growth at the end of the training is used. The C4 dataset was chosen because of its wide usage in the research community. While C4 is big enough for larger-scale experiments, it is small enough to allow for reproduction of experiments For all distillation trainings, the teacher is trained on a different split as the student. The C4 dataset has roughly 180B tokens in total, which results in 90B unique tokens for the teacher training and 90B unique tokens for the student training Except for the largest models, all Chinchilla-optimal models do not repeat data. Models that overtrain on more than 90B tokens will have data repetition too. Muennighoff et al. (2023b) has shown (on the C4 dataset) that repeating data up to 4 times has negligible impact to loss compared to having unique data. J. Contributions All authors contributed to writing this paper, designing the experiments, discussing results at each stage of the project. Writing and framing Majority of writing done by Dan Busbridge, Jason Ramapruam, and Amitis Shidani Figure 53) sensitivity analyses done by Dan Busbridge. Top-k and top-p distribution truncation (Appendix G.4) implementation and analyses done by Jason Ramapuram. Mixing coefficient combined with truncation analysis (Appendix G.4) done by Jason Ramapuram. Reverse KL divergence Appendix G.5 implementation and analysis done by Jason Ramapuram</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
