# Distillation Scaling Laws

## Abstract

## 

We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.

## Introduction

The study of scaling laws [(Hestness et al., 2017;](#b44)[Rosenfeld et al., 2020;](#b84)[Kaplan et al., 2020;](#b54)[Hoffmann et al., 2022)](#b26) revealed that previously trained Language Models (LMs) could have been more capable if they had followed a compute optimal training paradigm, which determines the model size and the number of training tokens that give the best performing model under a given compute budget. Many subsequent works have followed compute optimal training [(Dey et al., 2023;](#)[Muennighoff et al., 2023b)](#).

The size of compute optimal models grows with compute [(Hoffmann et al., 2022)](#b26), which makes them challenging to use due to the growth in inference costs. In practice, this means compute optimal models are slow, expensive to serve, consume more battery life, provide high barriers Preprint. to entry for academic study, and have a significant carbon footprint. With inference volume up to billions of tokens per day [(OpenAI & Pilipiszyn, 2021)](#), the inference cost of an LM is typically significantly larger than its pretraining cost [(Chien et al., 2023;](#b22)[Wu et al., 2024a)](#) and is going to further increase in an era of test-time compute scaling [(Snell et al., 2024;](#)[Brown et al., 2024;](#b16)[Wu et al., 2024b)](#).

Unsustainable inference costs have led to an alternative training paradigm, overtraining [(Gadre et al., 2024)](#), where the amount of training data used is much greater than in the compute optimal case, enabling small, capable models. Overtrained models better satisfy compute optimality when compute is measured over a model's lifetime, rather than just the pretraining cost [(Sardana et al., 2024)](#b86). As supervised scaling laws follow power laws in model size and training data, diminishing returns in performance oc-cur much sooner than in the compute-optimal case. To achieve reasonable capabilities, these models need to be trained on many trillions of tokens, [(Snell et al., 2024;](#)[Brown et al., 2024;](#b16)[Wu et al., 2024b)](#), which is expensive and time-consuming.

We seek models that match the performance of small overtrained models but at lower training cost. A popular candidate is distillation [(Hinton et al., 2015)](#b45), where a capable teacher LM produces targets for a smaller student LM. When distillation is used for LM pretraining, we will call this distillation pretraining. There are many explanations for why distillation works, from dark knowledge transfer, where information is contained in the ratio of probabilities of incorrect classes [(Hinton et al., 2015)](#b45), to being a form of regularization [(Mobahi et al., 2020)](#b67), or reducing noise in the learning process [(Menon et al., 2020)](#b64), among many other explanations. Despite a lack of consensus for why distillation works, distillation pretraining has produced more capable models than supervised pretraining in the Gemma and Gemini [(Rivière et al., 2024)](#), Minitron [(Muralidharan et al., 2024;](#)[Sreenivas et al., 2024)](#) and AFM [(Gunter et al., 2024)](#) families of LMs in terms of both pretraining loss and downstream evaluations. Yet, at the same time, [Liu et al. (2024)](#b59) reported that distillation produces less capable models than supervised pretraining does.

With such significant compute resources being devoted to distillation pretraining of LMs, it is essential to understand how to correctly allocate these resources, to produce the most capable models possible, and to have an understanding if any gains are even possible compared to supervised pretraining when both methods have access to the same resources [(Dehghani et al., 2021)](#).

To close this knowledge gap, we perform an extensive controlled study of distillation, with students and teachers ranging from 143M to 12.6B parameters, trained on data of a few billion tokens, up to 512B tokens. These experiments result in our distillation scaling law, which estimates student performance as a function of resources (the teacher, the student size, and the amount of data used for distillation), resolving questions about when distillation is and is not effective in terms of producing models of a desired capability under resource constraints of interest. We find:

1. The cross entropy of a student of size N S distilled on D S tokens from a teacher of size N T trained on D T tokens can be predicted using our distillation scaling law (Equation [8](#)).

2. The teacher size N T and number of teacher training tokens D T determines the student cross-entropy only through their determination of the teacher's crossentropy L T = L T (N T , D T ) (Figure [3b](#fig_2)).

3. The influence of the teacher cross-entropy upon the student loss follows a power law which transitions between two behaviors depending on the relative learning capacities of student and the teacher, reflecting a phenomenon in distillation called the capacity gap, where a stronger teacher produces a worse student.

Our parameterization resolves outstanding questions about the capacity gap, showing that it is a gap in learning capacity (both hypothesis space and ability to optimize) between the teacher and student, and not only about their relative sizes, which is a special case.

Our results show that distillation can not produce lower model cross-entropies than supervised learning when both learning processes are given enough data or compute. However, distillation is more efficient than supervised learning if both of the following are true:

1. The total compute or tokens used for the student is not larger than student size-dependent threshold given by our scaling law (Section 5.1).

2. A teacher already exists, or the teacher to be trained has uses beyond a single distillation (Section 5.3).

We hope the laws and analyses we provide will guide the community to produce even more capable models with lower inference cost and lower lifetime compute costs.

## Background

Predicting model performance is essential when scaling as it lets us understand i) the value of increasing the available compute (C), and ii) how that compute should be distributed, typically between model parameters (N ) and data (D), in order to achieve a model with desired properties. These properties may be predicting the data distribution sufficiently well, measured in cross-entropy (L), or achieving a level of performance on downstream tasks of interest.

Fortunately, cross-entropy is predictable, with substantial empirical and theoretical evidence that L follows a powerlaw in parameters N and data D (measured in tokens)

## L(N, D)

Model Cross-Entropy

## = E

Irreducible Error

$+ A N α + B D β γ Model ability to mimic data ,(1)$where {E, A, B, α, β, γ} are task-specific positive coefficients[foot_2](#foot_2) estimated from n training runs {(N i , D i , L i )} n i=1 . The choice of runs is critical; not all experiments enable identifying the coefficients of Equation [1](#formula_0) This is tempting, as for a total experiment budget, compute optimal models offer the largest loss variation. Unfortunately, compute optimal models have a constant token to parameter ratio M ≡ D/N = const. [(Hoffmann et al., 2022)](#b26), removing a degree of freedom.

To achieve reliable identification of scaling coefficients, [Hoffmann et al. (2022)](#b26) uses two training strategies:

1. [(Fixed model, varied data)](#) The number of training tokens is varied for a fixed family of models.

2. (IsoFLOP profiles) Model size and training tokens are both varied subject to a total compute constraint.

Data from both strategies is then combined for the fit. See Appendix B for an extended background.

The goal of this paper is predict the cross-entropy L S of a student produced by distillation. This will tell us the value of increasing the compute for distillation and, crucially, which distillation produces the student of a given size with the lowest cross-entropy for a given compute budget.

## Preliminaries

Notation For a sequence x, x (i:j) = (x (i) , x (i+1) , . . . , x (j) ) returns a slice of the sequence, and x (<i) =

x (1:i-1) = (x (1) , . . . , x (i-1) ) is the context of x (i) . We use the shorthand X * = ∪ n∈N X n to denote the set of sequences with arbitrary length n ∈ N = {1, 2, . . .}.

## Language modeling

We focus on the LM setting where the training objective is to model the probability of sequences x of tokens x i drawn from a vocabulary V = {1, 2, . . . , V }. Let f : V * × Θ → R V be a next-token classifier parameterized by θ ∈ Θ whose outputs define a predictive categorical distribution over V given a context x (<i)   p(x (i) = a|x (<i) ; θ) = σ a (f (x (<i) ; θ)) = σ a (z (i) ), (3)

where σ a (z) = exp(z a )/ b exp(z b ) is the softmax function. The next-token classifier outputs z (i) = f (x (<i) ; θ) are the logits.[foot_3](#foot_3) Autoregressive LMs produce sequence likelihoods through p(x; θ) = L i=1 p(x (i) |x (<i) ; θ) and are trained to maximize this likelihood on observed data through the Next Token Prediction (NTP) loss L NTP (x (i) , z (i) ) = -V a=1 e(x (i) ) a log σ a (z (i) ),

where e(i) is the i-th basis vector. It is common to also use the following token-level Z-loss to improve training stability [(Chowdhery et al., 2023;](#b116)[Wortsman et al., 2023](#))

$L Z (z (i) ) = || log Z(z (i) )|| 2 2 = log V a=1 exp(z (i) a ) 2 2 . (5$$)$Distillation In distillation, a teacher with predicted nexttoken distribution pT (x (i) |x (<i) ; θ T ) and corresponding logits z

$(i)$T replaces the one-hot basis vector in Equation 4 and is used as the target for a student predicted next-token distribution qS (x (i) |x (<i) ; θ S ) and corresponding logits

$z (i)$S . The resulting knowledge distillation loss is used to optimize the student parameters

$L KD (z (i) T ,z (i) S )=-τ 2 V a=1 σ a z (i) T τ logσ a z (i) S τ ,(6)$and is equivalent to optimizing the Kullback-Leibler Divergence (KLD) between the teacher and student predictions. τ > 0 is the distillation temperature. Combining the losses together results in a total token-level loss for the student:

$L S (x (i) , z(i)$T , z

S ) = (1λ) L NTP (x (i) , z

S ) + λ L KD (z

$(i) T , z (i) S ) + λ Z L Z (z (i) S ).$(7)

## Distillation Scaling Laws

Here we outline the steps taken to arrive at our distillation scaling law. First we describe the experimental setting (Section 4.1) and the experiments needed to determine the scaling coefficients (Section 4.2). Given the empirical observations, we discuss the form our distillation scaling law takes (Section 4.3), find the coefficients, and verify the law under extrapolation (Section 4.4).

## Experimental setup

All models are based on [Gunter et al. (2024)](#) and use decoupled weight decay [Loshchilov & Hutter (2019)](#b61) for regularization, as well as a simplified version of µP [(Yang & Hu, 2021;](#b104)[Yang & Littwin, 2023;](#)[Yang et al., 2022;](#b31)[Wortsman et al., 2023;](#)[Yang et al., 2023)](#b114), following µP (simple) in [(Wortsman et al., 2024)](#b100). µP simplifies the scaling law experimental setup as it enables hyperparameter transfer of the learning rate across model sizes. We validate that µP functions as expected for distillation in Appendix G.3. DS The number of tokens the student is distilled on. M ≡ D/N The tokens per parameter ratio, or M -ratio. In [Hoffmann et al. (2022)](#b26), M takes a compute optimal value M * ≈ 20 which is the Chinchilla rule of thumb. L ≈ L(N, D) The model cross-entropy, which is the model validation cross entropy under data, estimated by the supervised scaling law for a model with N parameters trained on D tokens. (Equation [1](#formula_0)). LT ≈ L(NT , DT ) The teacher cross-entropy, which is the teacher validation cross entropy under data, estimated by the supervised scaling law for a teacher with NT parameters trained on DT tokens. LS ≈ LS(NS, DS, LT ) The student cross-entropy, which is the student validation cross entropy under data, estimated by our distillation scaling law for a student with NS parameters distilled on DS tokens using a teacher with pretraining loss LT (Equation [8](#)).

LS ≈ L(NS, DS) The student supervised cross-entropy, which is the student validation cross entropy under data if the student had been trained in a supervised way, estimated by the supervised scaling law for a student with NS parameters trained on DS tokens.

Models have sizes which range from 143M to 12.6B parameters, and we allow the teacher to be smaller or larger than the student. Multi-headed attention (MHA) is used, with Pre-Normalization [(Nguyen & Salazar, 2019)](#) using RMSNorm [(Zhang & Sennrich, 2019)](#b111). We train all models with a sequence length of 4096, with Rotary Position Embedding (RoPE) [(Su et al., 2024)](#). We use the Englishonly subset of the C4 dataset [(Raffel et al., 2020)](#) for all experiments. For all distillation trainings, the teacher is trained on a different split from the student. Except for the largest models, all Chinchilla-optimal models do not repeat data. Full hyperparameters and details can be found in Appendix I. As our goal is to understand the role of the teacher in the distillation process we distill in the pure distillation case (λ = 1, Equation [7](#)) to avoid confounding coming from the data, as was done in [Stanton et al. (2021)](#b92). We verify the choice λ = 1 produces results statistically similar to the optimal λ * (see Appendix G.1). Similarly, all experiments use distillation temperature (τ = 1), as we found this produces the best performing students (see Appendix G.2).

## Distillation Scaling Law Experiments

Here we discuss the experiments that produce the data for fitting our distillation scaling law. The distillation scaling law will estimate student cross-entropy L S 3 , which in general depends on the student parameters N S , number of distillation tokens D S , the teacher parameters N T and the number of teacher training tokens N T : L S ≈ L S (N S , D S , N T , D T ). As discussed in Section 2, only certain combinations of data support reliable identification of scaling law coefficients. We combine three experimental 3 By cross-entropy, we always mean with respect to data, not the teacher. We summarize our scaling law notation in Table [1](#tab_0). protocols to produce data for our distillation scaling law fit.

100M 300M 1B 3B 7B 2.25 2.50 Student Cross-Entropy L S Teacher N T =975M 100M 300M 1B 3B 7B

Teacher N T =7.75B Student Parameters N S Student FLOPs 3 × 10 19 10 20 3 × 10 20 10 21 3 × 10 21 Figure 2. Fixed M Teacher/Student IsoFLOP profiles. Two (of a total of six) teachers with MT = DT /NT ≈ 20 are distilled into students with four IsoFLOP profiles, and a small number with CS = 3 × 10 21 . Horizontal and vertical dashed lines indicate teacher cross entropy LT and size NT respectively. See Appendix E.4, Figure 38a for all six profiles.

Fixed M Teachers/Student IsoFLOPs To simplify the experimental protocol we make the following assumption: Training a student (N S , D S ) on the signal provided by a teacher (N T , D T ) is qualitatively similar to training that student on fixed dataset. As power law behavior has been observed in a wide variety of datasets and domains [(Henighan et al., 2020)](#b43), it is expected that there should be a power law behavior in (N S , D S ) given a fixed teacher.

To identify these coefficients correctly, a similar protocol to the Chinchilla protocol described in Section 2 should be performed. However, we cannot only do this for only one teacher, as the way student size and tokens affects downstream performance may be different for different teachers, just as the scaling laws are different for different domains and dataset. For distillation we anticipate this is the case so that different teachers produce different students. To produce the widest range of teachers for a compute budget, we train six Chinchilla-optimal (M T = D T /N T ≈ 20) teachers ranging from 198M to 7.75B parameters. [4](#foot_4) For each of those teachers, we distill into students with four IsoFLOP profiles, taking only the standard training cost into account. The resulting student cross-entropies are in Figure [2](#). We note that in some cases, the student is able to outperform the teacher, i.e. exhibits weak-to-stronggeneralization [(Burns et al., 2024;](#b19)[Ildiz et al., 2024)](#) and investigate this further in Appendix E.7. IsoFLOP Teachers/Fixed M Students The fixed-M teacher IsoFLOP student protocol is insufficient to identify how N T and D T independently influence student crossentropy. To ensure our experiment can detect this influence, 100M 300M 1B 3B 7B Teacher Parameters N T 2.3 2.4 2.5 2.6 Student Cross-Entropy L S Student: 1.82B

Teacher FLOPs 3 × 10 19 10 20 3 × 10 20 10 21 (a) One teacher IsoFLOP set. 2.2 2.4 2.6 Teacher Cross-Entropy L T 2.3 2.4 2.5 2.6 2.7 Student Cross-Entropy L S Student Parameters NS 198M 546M 975M 1.82B (b) All teacher IsoFLOPs. we perform experiments where the student (N S , D S ) is fixed, and vary N T and D T subject to a compute constraint, i.e. a teacher IsoFLOP. We perform distillations into four Chinchilla-optimal (M S = D S /N S ≈ 20) students ranging from 198M to 1.82B parameters from teachers trained according to four IsoFLOP profiles. The resulting student cross-entropies are in Figure [3](#fig_2).

Fixed M Teachers/Fixed M Students Finally, although not necessary for fitting our distillation scaling law, it is instructive to see how student cross entropies vary over as large a range as possible. To achieve this, we train fixed-M teacher fixed-M student combinations, with ten teachers with M T ≈ 20, and students of five sizes, with at least four choices of M S per student. The resulting student cross-entropies for two of the students are in Figure [4](#fig_3). 

## Capacity gap

In Figure [4](#fig_3), we observe the capacity gap, where improving teacher performance does not always improve student performance, and even reduces student performance eventually. The capacity gap has been observed often in distillation (see [Appendix B.3](#)). The KLD between teacher and student is an increasing function of teacher capability in all cases (see Appendix E.3), which means as the teacher improves its own performance, the student finds the teacher more challenging to model, eventually preventing the student from taking advantage of teacher gains. We use calibration metrics to investigate aspects that the student finds challenging to model in Appendix E.8. In Appendices C.1 and C.2 we offer a simple explanation in a kernel regression and synthetic Multi-Layer Perceptron (MLP) setting and, to the best our knowledge, are the first controlled demonstrations of the capacity gap.

## Distillation Scaling Law Functional Form

We need to determine the functional form of the distillation scaling law. First, we observe that contributions from teacher size N T and pretraining tokens D T are summarized by the teacher cross-entropy L T . This can be seen from Figures 1 and 3b which contains the IsoFLOP Teacher/Fixed M Students of Figure [3](#fig_2), yet only smooth dependence as a function of L T is observed. Next, the distillation scaling law should reflect the following properties:

1. An infinitely capable student should be able to model any teacher:

$lim N S ,D S →∞ L S (N S , D S , L T ) → L T .$2. A random teacher produces random students independent of how capable those students are:

$lim L T →∞ L S (N S , D S , L T ) → L T .$3. There is a capacity gap: making a teacher too capable eventually reduces the student performance.

A transition between two power law regions: i) where the student is a stronger learner than the teacher, and ii) where the student is a weaker learner than the teacher is described by a broken power law [(Caballero et al., 2023)](#b20). Together, we propose that student cross-entropy follows a broken power law in L T and a power law in N S and D S :

$L S (N S ,D S ,L T ) Student cross-entropy = L T Teacher cross-entropy + 1 L c0 T 1+ L T L S d 1 1/f1 -c1f1 A N α ′ S + B D β ′ S γ ′$Student ability to mimic teacher [(8)](#) where {c 0 , c 1 , d 1 , f 1 , α ′ , β ′ , γ ′ } are positive coefficients to be fitted following the procedure outlined in Appendix F.2 on the data produced in Section 4.2. The first two properties of our distillation scaling law can be readily checked.

For the third, recall, L S = L(N S , D S ) is the cross-entropy a student would have achieved if it had been trained in a supervised way (Table [1](#tab_0)), and is determinable from the supervised scaling law (Equation [1](#formula_0)). The capacity gap behavior follows from a transition based on the ratio of the algorithmic learning capacities of the student and teacher, when L T / L S ≡ L(N T , D T )/L(N S , D S ) = d 1 , which can be interpreted as measure of the relative learning abilities of the teacher and the student on a reference task.

## Distillation Scaling Law Parameteric Fit

We use the teachers (N T , D T ) for fitting our supervised scaling law (Appendix E.2), and all the data for fitting our distillation scaling law (Equation [8](#)). Our fitting procedure is described in detail in Appendix F and resulting scaling coefficients are presented in Appendix F.3. Our supervised and distillation scaling laws fit the observations at the level of ≲ 1% relative prediction error, including when extrapolated from weaker to stronger models (see Figure [5b](#fig_5)).

1.8 2.0 2.2 2.4 2.6 2.8 Predicted Cross-Entropy L Extrapolation Fit data All L > 2.2 1.8 2.0 2.2 2.4 2.6 2.8 Cross-Entropy L -1.0 -0.5 0.0 0.5 1.0 Prediction Error (%) (a) Supervised. 2.0 2.2 2.4 2.6 2.8 Predicted Student Cross-Entropy L S Extrapolation Fit data All LS > 2.3 2.0 2.2 2.4 2.6 2.8  As a further verification, we confirm that for a fixed model size, distillation in the infinite data regime is consistent with supervised learning on infinite data (Appendix E.6).

## Distillation scaling law applications

Here, we apply our distillation scaling law (Equation [8](#)) and investigate scenarios of interest. Typically, the resources in distillation pretraining include a compute budget, or a dataset containing a number of tokens. For a distillation process, the compute cost can be approximated by where δ Lgt T , δ Pre T ∈ [0, 1] indicate if we account for the cost of teacher logit inference for the student targets[foot_5](#foot_5) , and teacher pretraining cost in the total compute budget (see Table [2](#tab_6)). F (N ) is the number of Floating Operations (FLOPs) a model with N parameters performs during a forward pass. F (N ) ≈ 2N is often used, giving supervised FLOPs ≈ 6N D. We cannot use the 2N approximation, as i) using non-embedding parameters N induces systematic errors [(Porian et al., 2024)](#b81), and ii) we are interested in small models with large context sizes where the FLOP contribution from attention is significant. To resolve these issues. we derive a simple expression F (N ) ≈ 2N (1+c 1 N -1/3 +c 2 N -2/3 ) for fixed-aspect ratio models Appendix H.1, and recommend the scaling community to consider adopting this hyperparameter setting.

$FLOPs≈3F (N S )D S Student Training +F (N T )(δ Lgt T D S Teacher Logits +δ Pre T 3D T Teacher Training ) (9)$
## Fixed tokens or compute (best case)

To build intuition for when distillation may (and may not) be beneficial, we ask how well can distillation do in the best case scenario, compared with supervised learning? We superimpose the data of Figures 2 and 3 onto contours of distilled cross-entropy L S compared to a supervised model with the same resources L S (Figure [6](#fig_6)).

## Supervised learning always outperforms distillation

given enough student compute or tokens. For a modest token budget, distillation is favorable, however, when a large number of tokens are available, supervised learning outperforms distillation. This is expected; in the large data regime, supervised learning can find the best solution limited by model size N (Equation [1](#formula_0)), whereas distillation only finds this solution for the optimal teacher L * T (see Appendix E.6), and is otherwise limited by the distillation process. This finding appears to contradict the patient teacher finding of [Beyer et al. (2022)](#b10). A comment on this contradiction is provided in Appendix D.1. Student compute constrained version of Figure [6](#fig_6) and IsoFLOP Teacher/Fixed M student contours are provided in Appendix D.2. 

## Fixed tokens or compute (teacher inference)

Next, we focus on the common scenario of planning to distill, and trying to decide between an existing set of teachers

${(L (i) T , N(i)$T )} n i=1 . A larger teacher may provide a better learning signal (lower cross-entropy) but will also be more expensive to use because of the teacher logits cost (Equation 9, δ Lgt T = 1), inducing a trade-off. Given a target student size N S and budget D S or C Total , the only degree of freedom is the choice of teacher.

For a fixed data budget, as the student size increases, teacher cross-entropy should be decreased as a power law. Here, the compute cost from N T is not relevant as we are considering a token budget. Student cross-entropy at different distillation token budgets is shown in Figure [7](#). An equivalent plot for different student sizes whilst varying tokens is shown in Appendix D.3. We see that the optimal teacher loss L * T (red line) decreases as a power law with student size N S until L S matches L * T , when there is an inflection point in L * T , causing the decrease of teacher loss to sharpen with N S . This generalizes the observation of [Zhang et al. (2023a)](#), that "Optimal teacher scale almost consistently follows a linear scaling with the student scale across different model architectures and data scales." which is a special case of our finding when the teachers are compute optimal (Figure [36a](#fig_30)). Note that our findings consistently show that teacher cross-entropy L T determines student cross-entropy L S , not N T itself (which leads to a

1.5 2.0 2.5 Student D S =250B 1. 75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2 .4 0 2.40 2.45 2.50 Student D S =1T 1.70 1.75 1.8 0 1.85 1.90 1.95 2.00 2.05 2.10 2. 15 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2 .4 0 2.40 2.45 2.50 1B 10B 100B 1.5 2.0 2.5 Student D S =4T 1.65 1.7 0 1.7 5 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2. 15 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2.40 2.45 2.50 1B 10B 100B Student D S =16T 1.6 5 1. 70 1.75 1. 80 1.85 1.90 1.95 2.00 2.05 2. 10 2.10 2 .1 5 2.15 2 .2 0 2.20 2 .2 5 2.25 2 .3 0 2.30 2 .3 5 2.35 2.40 2.45 2.50 Student Parameters N S Teacher Loss L T Figure 7. Students given a teacher and token budget. For four distillation token budgets the student cross-entropy for a range of students and teachers. The red line indicates the optimal teacher cross-entropy L * T producing the lowest student cross-entropy.

given L T ). We investigate a fixed compute budget setting for teacher inference only in Appendix D.3.

## Compute Optimal Distillation

We extend the analysis of [Hoffmann et al. (2022)](#b26) to distillation, giving compute optimal distillation, determining how to produce the student of a desired size N S with the lowest cross-entropy given a compute budget

$C D * S , N * T , D * T = arg min D S ,N T ,D T L S (N S , D S , N T , D T ) s.t. FLOPs = C,(10)$To present the best and worst case for incorporating teacher inference into the compute constraints, we consider all scenarios presented in Table [2](#tab_6). We also compare against the optimal supervised performance. To find the minima in Equation 10 we perform constrained numerical minimization using Sequential Least SQuares Programming (SLSQP) [(Kraft, 1988)](#b57) in SciPy [(Virtanen et al., 2019)](#b97).

Supervised learning always matches optimal distillation at sufficient compute budget, with the intersection favoring supervised learning increasing as student size grows. In Figure [8](#fig_7) we see that supervised learning always matches the best case distillation setting at some total compute budget, as anticipated from the asymptotic analysis in Figure [40](#fig_36). The compute transition point when supervised learning becomes preferable to distillation increases as a function of student size. See also Figure [6](#fig_6). We also observe that smaller models are more likely to benefit from supervised pretraining, whereas larger models are more likely to benefit from distillation.

When teacher training is included in the compute, the best student cross-entropy is always higher than in the supervised setting. This means that if your only aim is to produce the best possible model of a target size and you do not have access to a teacher, then you should choose supervised learning, instead of training a teacher and then distilling. Conversely, if the intention is to distill into a family of models, or use the teacher as a server model, distillation may be more computationally beneficial than supervised learning. On reflection, this finding should be expected, otherwise it would imply that for a total amount of compute, distillation can outperform direct maximum likelihood optimization. A detailed discussion of the compute 2.3 2.4 2.5 Student N S =300M 2.2 2.4 Student N S =1B 10 20 10 22 10 24 10 26 2.0 2.2 2.4 2.6 Student N S =3B 10 20 10 22 10 24 10 26 1.8 2.0 2.2 2.4 2.6 Student N S =10B Total Compute (FLOPs) Student Cross Entropy L S Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Supervised For four student sizes , the best cross-entropy each student can achieve the five scenarios considered as total compute is varied. optimal configurations that produce (N * S , N * T , D * T ) for all scenarios is discussed in Appendix D.4.

To build intuition for how quantities play off against eachother, we take the most complex scenario, teacher pretraining + inference. A view of the optimal distillation setup as compute varies is presented in Figure [9](#). Student and teacher tokens scale as a power law, with student tokens at a faster rate. Optimal teacher size increases initially until it is slightly larger than the student, after which it plateaus. This plateau occurs because inference with large teachers is expensive, and with the increase in number of student tokens, overtraining the teacher becomes more efficient.

The values in Figure [9](#) can be recombined to produce the compute terms in Equation 9 as shown in Appendix D.4, Figure [29](#). We summarize the trend in Table [3](#tab_10).

10 10 10 12 10 14 10 16 Student N S =300M Student N S =1B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =3B 10 20 10 22 10 24 10 26 Student N S =10B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 9. Teacher pretraining + inference. For four student sizes, the optimal student and teacher configurations when teacher logit inference and teacher pretraining cost is accounted for. 

## Conclusion

We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. We then used our law to study practical distillation scenarios of interest, and showed that distillation is only more efficient than supervised learning if: i) the total compute or tokens used for distillation is not larger than a student size-dependent threshold, and ii) a teacher already exists, or the teacher to be trained has uses beyond single distillation. Moreover, we use this law to determine optimal distillation scenarios that are able to outperform supervised learning, enabling practitioners to select the best teacher for their use case. This work represents the largest controlled empirical study of distillation we are aware of, with systematic ablations of common distillation techniques. Just as supervised scaling has mitigated risks in supervised pretraining, our findings offer a roadmap for producing smaller, more powerful models with lower inference costs, reducing carbon footprints, and enhancing the feasibility of test-time scaling.

## Impact Statement

This work shows how to apply the framework of scaling laws to the distillation setting, investigating distillation as a viable alternative to the overtraining paradigm for producing capable language models. The work explains when distillation should and should not be performed, from a compute efficiency perspective, compared to supervised learning. There are a number of benefits to this:

1. As compute-optimal recipes for distillation are now known, there is greater opportunity for producing powerful models with lower inference costs. Lowering inference costs lower the largest component of language model training carbon footprint.

2. When combined with other known scaling laws, there is a larger space of models for which we know compute-optimal configurations. To produce models with a given capability, the compute, hardware and climate costs have now been reduced compared to before, as the optimal recipe is known.

3. Our distillation scaling law lowers compute usage through removing unnecessary experimentation over various hyperparameters and distillation settings. We now understand that the primary driver of student cross-entropy is teacher cross-entropy, and so teacher size and tokens can be discarded as axes to search over.

4. Small powerful models democratize the study of models with significant capabilities, enabling the involvement of a greater number of perspectives to study model capabilities and safety aspects.

There are however, potential negative consequences:

1. Using distillation as part of a training pipeline introduces new sources of bias. Teacher models may contain bias from their pretraining data. Even if a student is distilled on data that is unbiased, the bias of the teacher will be inherited by the student.

2. Small powerful language models are more efficient during inference, reducing the amount of resources needed for bad actors to achieve their goals, such as generating targeted misinformation at scale.

## Appendices

A Limitations B Extended background B.1 Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Neural Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 The Knowledge Distillation Capacity Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C Teacher Student Capacity Gaps C.1 Kernel Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.2 Distilling the Teacher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.3 U-shape in the student error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 MLPs on the Mapping Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.2 Experimental Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D Distillation scaling law applications (additional results) D.1 A contradiction with patient teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Fixed tokens or compute (best case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Fixed size or compute (teacher inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Compute optimal distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.2 Cross-entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.3 Distillation (best case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

## A. Limitations

This work has several limitations that we are aware of:

• Our work is performed in the language modeling setting only. Although there is good evidence that the functional form of scaling laws applies across domains [(Henighan et al., 2020)](#b43), we cannot be absolutely certain that distillation behaves in the way we describe in this work in all domains.

• We perform our analysis on the English subset of C4 dataset (see Appendix I). This means that for our larger token runs, data has been repeated. Although it was shown in [Muennighoff et al. (2023b)](#) that on the C4 dataset, repeating data up to 4 times has negligible impact to loss compared to having unique data, this was shown in the supervised setting, and we cannot be absolutely certain that the same applies in the distillation setting.

• A second downside of using the C4 dataset is that we are limited in our ability to analyze downstream evaluations of students resulting from distillation. Our performance over standard English language downstream tasks closely follows cross-entropy, however, C4 is not as well suited for pretraining in order to probe aspects like reasoning performance (see Appendix E.1).

• We focused on distillation as originally defined in [Hinton et al. (2015)](#b45), where the teacher produces a full probability distribution for the student to target. More colloquially, distillation has become used to describe the more general process of using a teacher in order to produce a student. One popular approach for training language models is Sequence-Level Knowledge Distillation [(Kim & Rush, 2016)](#b55) where the teacher is sampled, e.g. with beam search, in order to produce sequences for training the student on in a supervised way. This technique, also called synthetic data or hard distillation has been employed to great effect in the LLaMA families [(Touvron et al., 2023a)](#) and most recently, the smaller models distilled from DeepSeek-R1 [(DeepSeek-AI et al., 2024)](#). While we anticipate that our broader findings also apply in the Sequence-Level Knowledge Distillation, we cannot be absolutely sure. We suggest that verifying the scaling properties of Sequence-Level Knowledge Distillation in a controlled, resource constrained manner as we have done here is important for future study.

## B. Extended background

This section reviews related work on knowledge distillation, capacity gap phenomena, neural scaling laws, and foundation models, highlighting their relevance to our study.

## B.1. Knowledge Distillation

Bucila [et al. (2006)](#) provided strong evidence that the knowledge gained by a large ensemble of models can be effectively transferred to a single smaller model. Later, [Hinton et al. (2015)](#b45) introduced knowledge distillation, where a smaller student network learns from a larger teacher network by mimicking its softened output probabilities, improving efficiency and generalization. Building on this, [Stanton et al. (2021)](#b92) studied both fidelity and student generalization, showing that while knowledge distillation often improves generalization, it frequently fails to achieve high fidelity, as student models do not fully match the teacher's predictive distribution. We study fidelity in terms of calibration in Appendix E.8, and show that when the learning signal is consistent with the calibration measure, then the student in our setup is well-calibrated both with respect to the teacher and the actual data. Addressing this, [Beyer et al. (2022)](#b10) demonstrated that knowledge distillation is most effective when the teacher is patient and consistent, providing stable targets over prolonged training to improve student generalization and fidelity. Our Language Model (LM) setup automatically satisfies consistency: both the teacher and student see the same data during the student's training. However, our conclusions differ from those of [Beyer et al. (2022)](#b10) in that although distilling a student for longer does improve its performance, unless the teacher is chosen perfectly, distillation becomes less effective than supervised learning in the patient setting, see Appendix D.2 for a discussion.  [2024](#)) and studied in LMs, was further analyzed by [Ildiz et al. (2024)](#), who extended the theoretical analysis to high-dimensional data and over-parameterized regression. Their findings show that distillation can provably outperform training with strong labels under the same data budget but does not improve the data scaling law. Our distillation scaling law (Equation [8](#)) confirms this finding, which for a fixed teacher cross-entropy does not improve the scaling law compared to the supervised one in Equation [1](#formula_0). Moreover, in many previous works, distillation happens with repeated data, that is, the student sees the same data as the teacher does during its training. In our setup, we do not repeat the data between teacher training and distillation, which allows us to examine only the effect of distillation rather than the possible diminishing returns of repeated data; see [Muennighoff et al. (2023a)](#) for more details on the effect of repeating data.

## B.2. Neural Scaling Laws

Predictable scaling trends in neural networks were first empirically observed by Hestness et al. ( [2017](#)) and later by [Kaplan et al. (2020)](#b54) who established empirical scaling laws for language model performance based on cross-entropy, which led to [Hoffmann et al. (2022)](#b26) and the pursuit of compute-optimal training. Beyond the empirical studies, there have been many theoretical works which provide explanations for why scaling laws should exist [(Bahri et al., 2021;](#b6)[Paquette et al., 2024;](#b76)[Havrilla & Liao, 2024)](#). More recent works explore scaling laws across different distributions, closely related to knowledge distillation. [Hernandez et al. (2021)](#b44) derived a scaling law for transfer learning, analyzing effective data transfer in low-data regimes and diminishing returns in high-data regimes. Similarly, [Barnett (2024)](#b7) empirically studied pretraining on one distribution for optimizing downstream performance on another, showing that when the transfer gap is low, pretraining is a cost-effective strategy. Finally, Jain et al. ( [2024](#)) theoretically analyze how additional data from a surrogate model affects generalization, demonstrating that surrogate data can reduce test error-even when unrelated-due to Stein's paradox [(Stein, 1956)](#b93), with test error following a scaling law. This setup is related to tuning the coefficient λ in our case, where we also observe a U-shape behavior depending on the teacher and student sizes (see Figure [51a](#fig_5)). However, we are interested in studying the effect of distillation only (λ = 1.0), which differs from their setup. While these works are closely related to knowledge distillation-since one can compare the distribution of the teacher logits to that of the student-they do not establish a distillation scaling law. Moreover, their setup differs from practical knowledge distillation, as it does not involve training a new student model using a teacher but instead studies the effect of transferring training knowledge to a downstream task. Our work is the first to determine and verify a distillation scaling law and examine the regions where one should distill as well as the regions where supervised pretraining outperforms distillation; see Figures 6, 7 and 14 in Appendix D.2 and Section 5.2. Finally, for improving inference cost at a given model capability, the scaling behavior of Mixture of Experts (MoE) [(Shazeer et al., 2017;](#b88)[Jelassi et al., 2024)](#) have been investigated in the context of scaling laws [(Clark et al., 2022;](#b26)[Ludziejewski et al., 2024;](#b62)[Abnar et al., 2025)](#b2) as one alternative to knowledge distillation.

## B.3. The Knowledge Distillation Capacity Gap

Despite extensive research on knowledge distillation, a persistent challenge is the curse of capacity gap, where a larger teacher does not necessarily produce a superior student compared to a smaller teacher. This occurs because a large gap in model capacity makes it harder for the student to effectively learn from the teacher's outputs. As a result, there exists an optimal teacher size along the scaling trajectory that maximizes student performance. Our distillation scaling law in Equation 8 confirms this, revealing a u-shaped trend in the scaling law and validating the existence of an optimal teacher. However, our results further indicate that the capacity gap is influenced not only by the size of the teacher but also by its training tokens and, more generally, its loss. A theoretical analysis in the kernel regression setup (Appendix C) supports these findings. [Lukasik et al. (2022)](#b63) showed that distillation gains are not uniform and can even degrade performance when small teacher errors are amplified by the student. Similarly, [Nagarajan et al. (2023)](#b72) found that deviations in predictive probabilities cause students to exaggerate the teacher's confidence levels. Several works [(Peng et al., 2024;](#)[Zhang et al., 2023a;](#)[Rawat et al., 2024)](#b81) observed the capacity gap in pre-training distillation for Large Language Model (LLM)s, affecting both large-to-small and small-to-large distillation. Notably, [Zhang et al. (2023a)](#) proposed an empirical law of the capacity gap, showing that the optimal teacher scale follows an approximately linear relationship with the student's scale. However, our findings suggest that scaling alone is insufficient-one must account for the complexity of the effective hypothesis space (Equation [8](#)) and we show that [Zhang et al. (2023a)](#) is a special case of our work when the teachers are compute-optimal from a supervised perspective (see Section 5.3). To address this issue, various strategies have been explored. [Yuan et al. (2024)](#) studied temperature scaling, which simplifies the teacher's output into more learnable representations, aiding student generalization. We analyzed the effect of temperature and learning rate in distillation (Figures [52](#fig_54) and [53](#fig_55)) and found that, contrary to existing literature, the optimal temperature is one. We hypothesize that this discrepancy arises because previous studies used repeated tokens, whereas our setup does not involve repeated data. Additionally, [Cho & Hariharan (2019)](#b23) found that early stopping of the teacher's training mitigates the capacity gap, while [Mirzadeh et al. (2020)](#b66) proposed progressive distillation, where knowledge is transferred through intermediate models to improve student learning. From a theoretical perspective, [Harutyunyan et al. (2023)](#b38) analyzed the capacity gap in distillation using supervision complexity in kernel classifiers. Their findings highlight a trade-off between teacher accuracy, student margin with respect to teacher predictions, and teacher complexity, explaining why some teachers are easier for the student to learn. Earlier, Lopez-Paz et al. ( [2016](#)) studied generalization error in distillation, proving that learning from a teacher can be beneficial under certain conditions, particularly when the teacher's capacity is small. Using similar techniques in LMs, [Zhang et al. (2023b)](#) demonstrated that among students of different capacities distilled from the same teacher, smaller students suffer from higher generalization error and lower performance, while larger teachers provide lower generalization error, reinforcing the trade-off in teacher-student capacity. Our distillation scaling law (Equation [8](#)) also confirms this trend, and we observe the effect of capacity gap in our scaling law terms, see Section 4.3 for more details.

Foundation model pretraining Foundation models were initially undertrained [(Brown et al., 2020)](#b17), then followed the compute-optimal scaling law carefully [(Hoffmann et al., 2022;](#b26)[Pearce & Song, 2024;](#b79)[Besiroglu et al., 2024)](#), and soon after started overtraining heavily [(Sardana et al., 2024;](#b86)[Bi et al., 2024;](#)[Hu et al., 2024;](#)[Mesnard et al., 2024;](#)[Jiang et al., 2023)](#b52). The LLaMA family [(Touvron et al., 2023a;](#)[b;](#)[Dubey et al., 2024)](#) and Phi line [(Li et al., 2023;](#)[Abdin et al., 2024b;](#)[a)](#) is following the same trend, where smaller models are overtrained according to the original Chinchilla scaling laws. In all these cases, the models are designed to be best possible foundation model that is still cheap and fast to run on lower end hardware. Besides overtraining, more recently, smaller foundation models tend to be distilled from larger models [(Gunter et al., 2024;](#)[Rivière et al., 2024;](#)[Reid et al., 2024)](#) to further increase performance. In these cases, the large model either specifically trained with the sole purpose of being a distillation teacher, or an existing model is re-used. In both these cases, there are no reports of how the exact teacher size is decided when taking total compute into account. Determining the optimal allocation of compute budget in the distillation setting is one of the primary contributions of our work (see Section 5.3).

## C. Teacher Student Capacity Gaps

In this section, we examine the capacity gap in two settings: kernel regression and a synthetic example using Multi-Layer Perceptron (MLP) for a mapping problem. The kernel regression setup provides a theoretical and analytically tractable perspective on the capacity gap. The MLP-based synthetic example allows us to study the capacity gap in a more practical, learnable function approximation scenario. By analyzing these two setups, we aim to better understand the fundamental limitations of distillation when there is a significant mismatch between teacher and student capacities.

## C.1. Kernel Regression

One of our main contributions is that the student loss follows a broken power law, where the transition between the two power law regions occur when the student becomes a stronger learner than the teacher (Equation [8](#)). This implies that making the teacher too capable (relative to the student) reduces student performance. In this section we show how a capacity gap provably degrades student performance in the setting of kernel regression. While simple, we believe the underlying principle causing the student performance degradation in this case carry over to much more general settings involving neural networks.

## C.1.1. SETUP

Let H denote a Hilbert space spanned by orthonormal bases functions

${ϕ i } ∞ i=1 such that ⟨ϕ i , ϕ j ⟩ H = δ ij . Let f * ∈ H denote the target function, identified by a set of coefficients α = {α i } ∞ i=1 ∈ R, ∥α∥ = M < ∞ such that: f ⋆ (x) = ∞ i=1 α i ϕ i (x).(11)$Let H m t , H n s denote the teacher and student Hilbert spaces respectively:

$H m t = Span{ϕ 1 , ϕ 2 , ..., ϕ m },(12)$$H n s = Span{ϕ 1 , ϕ 2 , ..., ϕ n },(13)$which are the hypothesis spaces of the teacher and student. Note that while the Hilbert space H is spanned by an infinite orthonormal basis, the teacher and student spaces are finite and spanned by m and n basis functions respectively, where |m -n| represents the teacher and student capacity gap.

The process of training the teacher and student models involves solving the following constrained optimization problems:

$g ⋆ = min g∈H m t ∥g -f ⋆ ∥ H s.t ∥g∥ H ≤ T,(14)$$h ⋆ = min h∈H n s ∥h -g ⋆ ∥ H s.t ∥h∥ H ≤ D,(15)$where g ⋆ , h ⋆ are the optimal teacher and student respectively, and D ≤ T < M . Note that we assume the teacher and student are exposed to an infinite amount of training data, hence our analysis is carried over entirely in function space.

Lemma C.1. The optimal teacher g ⋆ is given by:

$g ⋆ (x) = C(m, T ) m i=1 α i ϕ i (x), C(m, T ) = 1 m i=1 α 2 i ≤ T T √ m i=1 α 2 i otherwise. (16$$)$The teacher error e ⋆ teacher (m, T ) is given by:

$e ⋆ teacher (m, T ) = ∥g ⋆ -f ⋆ ∥ H = (C(m, T ) -1) 2 m i=1 α 2 i + ∞ i=m+1 α 2 i . (17$$)$Proof. By construction we may assume the teacher model takes the form g ⋆ = m i=1 β i ϕ i . where m i=1 β 2 i ≤ T . We can write the error of g ⋆ using:

$e teacher (m, T, β) = m i=1 (β i -α i )ϕ i + ∞ i=m+1 α i ϕ i H = m i=1 (β i -α i ) 2 + ∞ i=m+1 α 2 i . (18$$)$Note that the minimizing coefficients β ⋆ of Equation 18 must take the form β = Cα for some coefficient C. Considering the norm constraint on g, the constant C takes the form in Equation [16](#formula_22). Plugging the resulting g ⋆ into the expression for e teacher (m, T, β ⋆ ) completes the proof.

Notably and intuitively, the teacher error decreases monotonically as m, which represents the teacher model capacity, increases.

## C.1.2. DISTILLING THE TEACHER

We now pick our student function h ⋆ by mimicking the teacher subject to a norm constraint:

$h ⋆ (x) = min h∈H n t ∥h -g ⋆ ∥ H s.t. ∥h∥ H ≤ D. (19$$)$Lemma C.2. Let k = min(m, n) be the smaller of the teacher and student capacities. The optimal student h ⋆ is given by:

$h ⋆ = Q(m, k, T, D)C(m, T ) k i=1 α i ϕ i (20) Q(m, k, T, D) =    1 C(m, T ) k i=1 α 2 i < D D C(m,T ) √ k i=1 α 2 i otherwise. (21$$)$The student error with respect to the target function is then:

$e student (m, n, T, D) = ∥h ⋆ -f ⋆ ∥ H = (C(m, T )Q(m, k, T, D) -1) 2 k i=1 α 2 i + ∞ i=k+1 α 2 i (22)$Proof. The proof follows the exact same logic as in Lemma C.1. i.e, we can assume the optimal student is given by h ⋆ = n i=1 γ i ϕ i . From the distillation loss, the optimal coefficients must match the teacher coefficients for the basis functions {ϕ i } n i=1 , perhaps rescaled due to the norm constraint n i=1 γ 2 i ≤ D. This rescaling then gives rise to the additional Q(m, k, T, D) multiplier in Equation [21](#formula_30).

## C.1.3. U-SHAPE IN THE STUDENT ERROR

We will prove that the map m -→ e student (m, n, T, D) is comprised of two distinct segments: i) where the student error monotonically decreases for m < n, and ii) where it monotonically increases for m ≥ n, establishing a U-shape in the student error echoing the trend seen in Figures [3](#fig_2) and [4](#fig_3). In words, when m < n, the error does not increase (and typically decreases) as the teacher capacity m increases.

## Proof.

Let H m,T t ⊆ H m t denote the space of functions in H m t that are norm constrained by D. i.e:

$H m,T t = {f ∈ H m t : ∥f ∥ H ≤ T }. (23$$) Since H m,T t ⊆ H m+1,T t , it follows that g ⋆ m ∈ H m+1,T t$, which implies that the teacher error cannot increase as m increases, hence it monotonically decreases. Now, let h ⋆ m denote the optimal student given the teacher g ⋆ m . Since D ≤ T , then for any m < n, we can equivalently write the optimal student h ⋆ m as the solution to the following optimization problem:

$∀ m≤n h ⋆ m = min h∈H n s ∥h -g ⋆ m ∥ H s.t ∥h∥ H ≤ D (24) = min h∈H m t ∥h -f ⋆ ∥ H s.t ∥h∥ H ≤ D,(25)$which corresponds exactly to the objective of finding the optimal teacher with with a norm constraint set to D. Therefore, from the fact that the teacher error monotonically decreases we can conclude that the student error monotonically decreases as well in the regime m < n.

Case 2: m ≥ n. (Student error eventually increases in m)

Claim. For m ≥ n: e student (m + 1, n, T, D) ≥ e student (m, n, T, D). Hence once m exceeds n the student error cannot decrease any further, the error eventually starts to rise.

## Proof.

Let β ⋆ m = {β 1 , ..., β m } denote the coefficients of the optimal teacher g ⋆ m . Note that in the regime m ≥ n, as long as n i=1 β 2 i ≥ D (i.e the norm of the coefficients corresponding to the basis {ϕ 1 , ..., ϕ n } is smaller than D), we have from Equation [21](#formula_30)that Q(m, k, T, D) = 1, which means that the optimal student doesnt change, hence its error remains constant. If however n i=1 β 2 i < D, then we have from Equation [21](#formula_30):

$1 > Q(m, k, T, D) ≥ Q(m + 1, k, T, D),(26)$where the second inequality becomes strict if

$α 2 m+1 > 0. A strict inequality (i.e Q(m, k, T, D) > Q(m + 1, k, T, D))$implies the optimal student is further scaled down due to the teacher having to "spread its capacity" to additional basis functions that are not learnable by the student, thereby strictly increasing its error. Hence for m ≥ n, we get Therefore, as a function of m, the student error e student (m, n, T, D) first decreases and then increases (for m ≥ n) (for m ≤ n), giving a u-shape in student error due to a capacity gap between the teacher and the student.

We present an empirical verification of these conclusions in Figure [10](#fig_12). As can be seen, the student error exhibits a U shaped error curve as predicted by the theory, where the error starts to increase when m ≥ n. The black solid line indicates the teacher error, which always decreases with increasing m.

The above theoretical analysis points to an intuitive interpretation of the potentially adverse effect of a large teacher-student capacity gap; the degradation in student performance is due to the teacher learning basis functions that are unreachable by the student, at the expense of basis functions that are reachable by the student. In the following we provide empirical evidence in support of this picture in a controlled yet more realistic setting.

## C.2. MLPs on the Mapping Problem

## C.2.1. PROBLEM DEFINITION

Here we show a synthetic setting which exhibits the U-shape phenomenon. Matching the kernel regression analysis (Appendix C.1), we find that the synthetic problem must include a class of problems that are easy for the student to learn, and ones that are harder, in order for the u-shape to appear.

The problem setting is the Mapping Problem, and is similar in spirit to Pointer Value Retrieval [(Zhang et al., 2021)](#b112), Here, the input is composed of small integers in {0,1,2}. The label for each sample is given by the code below, which shows the two cases: i) one where the label is simply given by a one-hot position, and ii) one where the label is given by the location of a matching element in the context portion of the input.

def find(vector, value): """Find locations of value in vector.""" return np.where(vector == value) [[0]](#b116) def remove(vector, value): """Find value from vector.""" return np.delete(vector, find(vector, value))

def label(vector: np.ndarray, num_classes: int) -> np.ndarray: """Return the label in [0, num_classes) for vector.""" assert len(vector

$) == 2 * num_classes one_hot = vector[num_classes:] context = vector[:num_classes] i = find(one_hot, 1) if context[i] == 0: return i else: # remapping c = context[i] return remove(find(context, c), i)$Examples: [----------------------------](#)

$- 2020210001000000, label = 1 context [2 0 2 0 2 1 0 0] one-hot [0 1 0 0 0 0 0 0] ----------------------------- 1210120000000100, label = 2 context [1 1 2 0 1 2 0 0] one-hot [0 0 0 0 0 1 0 0] ----------------------------- 0122221201000000, label = 6 context [0 1 2 2 2 2 1 2] one-hot [0 1 0 0 0 0 0 0] -----------------------------$
## C.2.2. EXPERIMENTAL FINDINGS

We train MLPs with two hidden layers of equal width, all non-linearities are Rectified Linear Units (ReLUs). Teachers and students of different sizes are produced by varying the hidden layer width only.

All model are trained with Adam [(Kingma & Ba, 2015)](#b56) using a peak learning rate of 3 × 10 -4 , a single cycle cosine learning rate schedule with a linear warmup of 5% of the total training steps. A batch size of 512 is used for all models. Training samples are never repeated. Unless explicitly stated, model are trained on 500 × 512, or 20N samples, where N is the number of model parameters, whichever is larger.

In Figure [11](#fig_13), we look at varying the size of the teacher. For the width 256 model, student performance improves as the teacher size increases to a point, and then student performance worsens. This is observable in both the student cross-entropy (Figure [11a](#fig_13)) and accuracy (Figure [11b](#fig_13)). Aligning with theory and large-scale experiments, the student cannot learn if it is too small, and can learns to match the teacher model when ther student is large enough. In the intermediate regime, where distillation is often used, we see an optimal teacher size and a capacity gap phenomenon. In Figure [12](#fig_0), a similar effect can be seen, when a large teacher (d ffn = 512) is trained with on different amounts of data. This observation aligns with the idea that it is the teacher's completeness in modeling the problem that eventually harms the performance of a student with lesser capacity, and not only the teacher size. 

## D.3. Fixed size or compute (teacher inference)

Fixed student size For a fixed student size, as the number of student tokens increases, the optimal teacher cross-entropy decreases slightly; see Figure [15](#fig_5). This observation highlights an asymmetry between the growth of student size and student tokens (or their rates in the scaling law), as the behavior here differs from that observed in Section 5.1. Notably, when the student size is sufficiently large, such as N S = 30B, increasing the student tokens initially leads to a decrease in the teacher's loss, followed by a saturation point and a slow decrease in the optimal teacher's loss.

1.5 2.0 2.5 Student N S =1B 2.10 2.15 2.20 2.20 2.25 2.25 2.30 2.30 2.35 2.35 2.40 2.40 2.45 2.45 2.50 2.50 2.55 Student N S =3B 1.95 2.00 2.05 2.05 2.10 2.10 2.15 2.15 2.20 2.20 2.25 2.25 2.30 2.35 2.40 2.45 2.50 250B 1T 4T 16T 1.5 2.0 2.5 Student N S =10B 1.85 1.90 1.95 1.95 2.00 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 250B 1T 4T 16T Student N S =30B 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 Student Tokens D S Teacher Loss L T Figure 15. Student performance given a teacher varying distillation tokens. For four distillation student sizes NS ∈ {1B, 3B, 10B, 30B} the validation loss achieved by a students distilled on DS ∈ [250B, 16T ] tokens under a teacher with loss LT ∈ [E, 2.5]. The red line indicates the value of the teacher loss resulting in the best performing student, and the vertical dashed line indicates the number of tokens at which supervised pretraining outperforms distillation.

Fixed compute budget Given an inference budget N S , a set of teachers {(L

$(i) T , N(i)$T )} n i=1 and a total compute budget C Total , the number of distillation tokens is determined from Equation 9

$D S = C Total /(3F (N S ) + δ T-Logits F (N T )),(27)$where F (N ) is the forward Floating Operations (FLOPs) per token of a model of size N (see Appendix H). If δ T-Logits = 0 then there is no price to pay for a larger teacher, and the conclusions are identical to those of the fixed token analysis of Section 5.2. In the worst case scenario, δ T-Logits = 1, then using a larger teacher will mean fewer distillation tokens are available for the student. Due to the capacity gap phenomenon, at small compute budgets, this means it is actually better to use a large weak teacher rather than a large strong teacher. Once compute is sufficient to allow enough distillation tokens, a stronger teacher can be used for all student sizes (see Figure [16](#fig_6)).  

$D * S , N * T , D * T = arg min D S ,N T ,D T L S (N S , D S , N T , D T ) s.t. FLOPs(N S , D S , N T , D T ) = C,(28)$where L S (N S , D S , N T , D T ) is the distillation scaling law (Equation [8](#)), and

$FLOPs(N S , D S , N T , D T ) ≈ 3F (N S )D S Student Training +F (N T )(δ Lgt T D S Teacher Logits + δ Pre T 3D T Teacher Training )(29)$is the total number of floating operations performed in the entire distillation setup. F (N ) is the forward FLOPs per token of a model of size N (see Appendix H), and δ Lgt T , δ Pre T ∈ [0, 1] indicate if we account for the cost of teacher logit inference for the student targets and teacher pretraining cost in the total compute budget. For convenience, we restate our compute scenarios of interest in Table [5](#tab_13)). Constrained numerical minimization using Sequential Least SQuares Programming (SLSQP) [(Kraft, 1988)](#b57) in SciPy [(Virtanen et al., 2019)](#b97). We allow numerical solutions for model sizes and tokens N T , D S , D T ∈ [1M, 100P ]. While this token upper-limit is larger than available resources [(Epoch AI, 2023)](#b34), it simplifies discussions when comparing to supervised learning at large compute budgets, which otherwise, for smaller students, would only by using a fraction of the available compute.

We begin by looking at the student cross-entropy achievable in each compute scenarios alongside the corresponding teacher cross-entropies in Appendix D.4.2. We then investigate the compute-optimal distillation configurations for each scenario that produce those cross-entropies. We look at best case distillation in Appendix D.4.3, teacher inference in Appendix D.4.4, teacher pretraining in Appendix D.4.5, and teacher pretraining + inference in Appendix D.4.6. Finally, to aid comparisons across methods, we present the token and parameter configurations for all methods in Appendix D.4.7 and Appendix D.4.8 respectively. For completeness, in the following sections, some of the findings of Section 5.3 are restated.

## D.4.2. CROSS-ENTROPY

In Figure [17](#fig_16) we show the student cross-entropies achieved in the compute optimal case for each scenario in Table [5](#tab_13), and the teacher cross-entropies that enable those student cross-entropies in Figure [18](#fig_7).

Distillation and supervised learning produce the same student at large compute. The first thing to note in Figure [17](#fig_16) is that at low compute, in the best case and teacher inference scenarios, distillation outperforms supervised learning, consistent with our expectations from distillation and the existing literature (see Appendix B.1). However, once enough the compute is large enough 6 , distillation and supervised learning produce models with the same cross-entropy, i.e. in general, distillation does not allow us to produce better models that supervised learning does, however, distillation does produce better models than supervised learning with modest resources. This behavior is consistent with the asymptotic analysis in Appendix E.6, and can be understood through noting that although distillation modifies the learning process the student undergoes, distillation does not alter the hypothesis space of the student, which is tied to the student size N S , is the same hypothesis space in the supervised and distillation settings, and can be explored in the limit of infinite compute or data.

2.3 2.4 2.5 Student N S =300M 2.2 2.3 2.4 2.5 Student N S =500M 2.2 2.4 Student N S =1B 2.0 2.2 2.4 2.6 Student N S =3B 10 20 10 22 10 24 10 26 2.0 2.2 2.4 2.6 Student N S =5B 10 20 10 22 10 24 10 26 1.8 2.0 2.2 2.4 2.6 Student N S =10B 10 20 10 22 10 24 10 26 2.0 2.5 3.0 Student N S =30B 10 20 10 22 10 24 10 26 2.0 2.5 3.0 Student N S =50B Total Compute (FLOPs) Student Cross Entropy L S Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Supervised The compute at which distillation and supervised learning produce similar models grows with student size. Continuing the previous observation, we see in Figure [17](#fig_16) that supervised cross-entropy approaches the best case and teacher inference student cross-entropies at a value of compute which increases with compute, meaning that larger students benefit from distillation for larger compute budgets than supervised learning. This implies that if your target student size is small and your compute budget is large, then supervised learning is more likely to be beneficial than if your target student size is larger. The phenomenon happens because larger supervised models saturate in performance at larger values of D (Equation [1](#formula_0)), and distillation accelerates progress towards this saturation with the correct choice of teacher (Equation [8](#)), with more capable teachers producing more gains per token.

Including teacher training in compute produces student cross-entropies higher than in the supervised setting. In Figure [17](#fig_16) supervised cross-entropy is always below the teacher pretraining and teacher pretraining + inference scenarios, except at very large compute budgets, when supervised learning and these distillation scenarios produce similar student cross-entropies. This means that if your only aim is to produce the model of a target size with the lowest cross-entropy and you do not have access to a teacher, then you should choose supervised learning, instead of training a teacher and then distilling. Conversely, if the intention is to distill into a family of models, or use the teacher as a server model, distillation may be more computationally beneficial than supervised learning. This finding aligns with expectations, the alternative implies distillation can outperform direct maximum likelihood optimization given fixed compute.

The optimal teacher cross-entropy decreases with increasing total compute. As shown in Figure [18](#fig_7), the optimal teacher cross entropy loss has a decreasing trend with respect to the total compute. However, in the best case scenarios, at low compute for larger student, where the number of student tokens is lower than the Chinchilla rule of thumb, an inflection point happens in optimal teacher compute.

We now turn to investigating the optimal distillation configurations that achieve these student cross-entropies.

2.2 2.3 2.4 2.5 Student N S =300M 2.1 2.2 2.3 2.4 2.5 Student N S =500M 2.0 2.2 2.4 Student N S =1B 1.8 2.0 2.2 2.4 Student N S =3B 10 20 10 22 10 24 10 26 1.8 2.0 2.2 2.4 Student N S =5B 10 20 10 22 10 24 10 26 1.75 2.00 2.25 2.50 Student N S =10B 10 20 10 22 10 24 10 26 1.5 2.0 2.5 Student N S =30B 10 20 10 22 10 24 10 26 1.5 2.0 2.5 Student N S =50B Total Compute (FLOPs) Optimal Teacher Cross Entropy L * T Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining)

Figure [18](#fig_7). Compute optimal distillation teacher cross-entropies. For eight student sizes, the optimal teacher validation loss L * T resulting in lowest student validation loss L * S in each of the distillation scenarios considered (Table [5](#tab_13)) the total compute is varied.

## D.4.3. DISTILLATION (BEST CASE)

In the distillation (best case) scenario, δ Lgt T = δ Pre T = 0, which means that we only account for compute associated with the standard supervised learning case

$FLOPs(N S , D S , N T , D T ) ≈ 3F (N S )D S Student Training .(30)$We call this best case as the scenario reflects a freedom to choose the best distillation setting for a given student size N S , with all of the compute being put into training the student for as long as possible (maximal D S ). In this sense we can consider this the upper bound in performance for distillation in our experimental setting.

10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1.6 5 1. 70 1.7 5 1.80 1 .8 5 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2. 35 2.3 52.4 0 2.45 2.5 0 2.5 5 2.6 0 2.6 5 2.7 0 2.7 5 10 20 10 22 10 24 10 26 This scenario represents the setting where a teacher already exists, or we will use the teacher for another purpose, for example a server model. In these scenarios, we do not need to worry about the teacher pretraining cost. Additionally, this teacher may be used to produce the logits for many different students, or we may have saved the logits from the teacher during its training. In these cases, the cost for producing the student logits can also be ignored.

The optimal quantities (D * S , N * T , D * T ) giving rise to the cross entropies in Figure [17](#fig_16) are shown in Figures [19](#fig_17) and [20](#fig_18). In the best case scenario, L * T is determined, however N * T and D * T are not determined because they do not enter into the compute constraint, yielding a one-dimensional family (N T (L * T , D T ), D T ) of valid solutions to the minimization problem (Equation [28](#formula_42)). To provide some guidance for producing L * T , in Figure [18](#fig_7) we present the supervised compute optimal (N T (L * T , D T ), D T ), i.e. the combination that minimizes FLOPs ∝ F (N T )D T subject to L(N T , D T ) = L T .

10 10 10 12 10 14 10 16 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 20. Compute optimal configurations for distillation (best case). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for best case in Figure 17. (N * T , D * T ) are the supervised compute optimal combination giving rise to L * T in Figure [18](#fig_7). This is a one-dimensional slice of Figure [19](#fig_17).

In this scenario, all the compute goes into student tokens, and so in Figure [20](#fig_18) we see optimal student tokens D * S increases with compute at the same rate as we could for the supervised model, which is higher for smaller students. The optimal teacher parameters N * (31)

This scenario represents the setting where a teacher already exists, but logits for the distillation still need producing. The optimal quantities (D

* S , N * T , D * T ) giving rise to the cross entropies in Figure 17 are shown in Figures 21 and 22. 10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1 .7 0 1. 75 1.8 0 1 .8 5 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2. 40 2.40 2.4 5 2.5 0 2.5 5 2.6 0 2.6 5 2.7 0 2.7 5 10 20 10 22 10 24 10 26 The teacher should be overtrained. In the teacher inference scenario, D * T does not contribute directly to compute but instead indirectly N * T subject to L * T . To minimize N * T at a given L * T , the solution is to maximize D * T as is seen in Figure [22](#);

10 10 10 12 10 14 10 16 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 22. Compute optimal configurations for distillation (teacher inference). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) producing the student cross entropies for teacher inference in Figure [17](#fig_16). This is a one-dimensional slice of Figure [21](#fig_19).

## D *

T takes the largest value allowed in our numerical optimization, 10 17 tokens. Although not surprising, this demonstrates the benefit of producing overtrained teachers, instead of taking the tempting strategy of using compute optimal teachers followed by a long distillation process into a smaller student model.

As compute is increased, relatively less should be spent on student training, and more on teacher logit inference. The compute allocations resulting from the optimal combination are shown in Figure [23](#fig_20). We see that in all cases, the student training term (blue) decreases as compute increases, whereas the teacher logits (orange) increases. This happens because as compute increases: i) optimal student tokens increases at a rate approximately independent of compute, ii) the teacher size increases with compute to provide a stronger signal, while iii) the student size is fixed (see Figure [22](#)). (32)

This scenario represents when we want to figure out which teacher to produce to distill into sufficiently many different students, storing the teacher logits for reuse, effectively ammortizing the cost of producing the logits. Here, contrary to the previous two scenarios [(Appendices D.4.3 and D.4.5)](#), the teacher size N T and teacher tokens D T contribute directly to the compute accounting (Equation [32](#)). The optimal quantities (D

* S , N * T , D * T ) giving rise to the cross entropies in Figure 17 are shown in Figures 24 and 25. 10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1 .7 0 1. 75 1.80 1.85 1. 90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.602.65 2.70 2.75 2.8 0 2.8 5 10 20 10 22 10 24 10 26 L * T 1.6 0 1.65 1. 70 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.60 2.65 10 20 10 22 10 24 10 26 D * S 1B 3B 10B 30B 10 0B 30 0B 1T 3T 10 T 30 T 100 T 30 0T 1P 3P 10P 10 20 10 22 10 24 10 26 N * T 500M 1B 2B 3B 5B 10B 20B 30B 50 B 100B 200 B 30 0B 50 0B 10 20 10 22 10 24 10 26 D * T 10B 30B 100B 30 0B 1T 3T Total Compute C Total Student Size N S Figure 24. Compute optimal configuration contours for distillation (teacher pretraining). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining in Figure 17. 10 10 10 12 10 14 10 16 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 10 10 12 10 14 10 16 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity Compute optimal configurations for distillation (teacher pretraining). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining in Figure [17](#fig_16). This is a one-dimensional size of Figure [24](#fig_8).

The compute optimal teacher for distillation is a supervised compute optimal teacher. In Figure [25](#fig_5) we see that the M T ≡ D T /N T ratio of the teacher is constant for all values of compute, and can be compared to the ratio in Figure [19](#fig_17). This can be understood as there is no inference cost to pay for making the teacher large; we are only minimizing the training compute budgets of two models, and the most efficient way to produce a teacher with a given cross-entropy L T is a teacher that is compute-optimal in a supervised sense. Note that this conclusion is the opposite to the finding in Appendix D.4.4. There, the inference is expensive, and so the teacher should be overtrained. Here, teacher training is expensive, so teacher training should be compute optimal.

As compute is increased, relatively less should be spent on teacher training, and more on student training. In Figure [26](#fig_22) we see the compute allocations for the configurations shown in Figure [25](#fig_5), and see that student training relative compute (blue) increases with increasing compute budget, while the teacher training (green) decreases with increasing compute budget. This happens because, as in all compute scenarios, with increasing compute, the optimal student tokens N * S increases (Figure [25](#fig_5)). Teacher size and tokens are also increasing with increasing compute, providing a stronger signal for the student with more tokens to learn. However, this increase in teacher size and tokens plateaus, while the student tokens continues to increase. This is because here the teacher is compute optimal, and so the amount of compute needed to improve the learning signal for the student is much less than the amount of compute needed to train the student for to make use of that signal, due to the stronger diminishing returns with respect to D S at a fixed N S (Equation [8](#)).

0 25 50 75 100 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 0 25 50 75 100 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Compute Fraction (%) Compute Component Student Training Teacher Logits Teacher Training (

$)33$This scenario can be thought of as the compute optimal worst case scenario for distillation, i.e. one teacher is trained only for the purposes of one student. As in Appendix D.4.4, teacher size N T and teacher tokens D T contribute directly to the compute accounting (Equation [33](#formula_45)). The optimal quantities (D * S , N * T , D * T ) giving rise to the cross entropies in Figure [17](#fig_16) are shown in Figures [27](#) and [28](#fig_7).

Compute optimal teachers should be used for lower compute budgets and overtrained teachers should be used for larger compute budgets. In Figure [28](#fig_7) we see a teacher configuration that interpolates between the teacher pretraining (Appendix D.4.5) and teacher inference (Appendix D.4.4) compute scenarios. At low compute, the optimal number of student tokens D * S is not too large, this means there is little penalty to increasing the teacher size, resulting in an approximately supervised compute-optimal teacher given a teacher compute budget. Once the optimal number of student tokens becomes higher than the optimal number of teacher tokens, there is significant penalty to increasing the teacher size. At this point, the teacher solution starts to become the overtrained solution seen in teacher inference, the optimal teacher tokens continue to increase polynomially, but this is not followed with an increase in the teacher size. For sufficiently high compute, corresponding to a large number of student distillation tokens, the compute penalty for teacher size is so large that optimal teacher size decreases with compute.

10 20 10 22 10 24 10 26 300M 500M 1B 3B 5B 10B 30B 50B L * S 1 .7 0 1 .7 5 1.80 1.8 5 1. 90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.602.6 5 2.70 2.75 2.8 0 2.8 5 10 20 10 22 10 24 10 26 L * T 1. 60 1 .6 5 1 .7 0 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.55 2.60 2.65 10 20 10 22 10 24 10 26 D * S 1B 3B 10B 30B 100 B 300 B 1T 3T 10 T 30 T 10 0T 30 0T 1P 3P 10P 10 20 10 22 10 24 10 26 N * T 500M 1B 1B 2B 3B 5B 10B 2 0 B 30B 50B 1 0 0 B 10 20 10 22 10 24 10 26 D * T 10B 30B 100B 300B 1T 3T 10T 30T 100T Total Compute C Total Student Size N S Figure 27. Compute optimal configuration contours for distillation (teacher pretraining + inference). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining + inference in Figure 17. 10 9 10 11 10 13 10 15 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 10 9 10 11 10 13 10 15 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Value Optimal Quantity N * S D * S N * T D * T Figure 28. Compute optimal configurations for distillation (teacher pretraining + inference). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining + inference in Figure [17](#fig_16). This is a one-dimensional size of Figure [27](#).

For small students, as compute grows, more should be spent on training the student and producing logits for the student. In Figure [29](#) we see the compute allocations for the configurations shown in Figure [28](#fig_7). Compute optimal smaller models tend to have smaller teachers, and optimal teacher tokens always grow at a slower rate than student tokens, and so teacher the training cost is relatively small. As compute grows, the student is distilled on more tokens, and the teacher always becomes slightly larger than the student, which gives rise to most compute being allocated to standard student training compute component and producing the logits for this training.

For large students, as compute grows, more should be spent on training the teacher, until a transition happens where more should be spent on training the student and producing logits for the student. The explanation for the phenomenon is as above, except that the larger students need a more capable teacher to learn from as compute grows, and so initially compute needs to bused to produce the teachers required. After a certain amount of compute, the large number of optimal student distillation tokens moves the optimal solution towards an overtrained teacher scenario, and more compute being allocated to student training and logit production.

0 25 50 75 100 Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 0 25 50 75 100 Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Compute Fraction (%) Compute Component Student Training Teacher Logits Teacher Training

Figure [29](#). Compute optimal allocations for distillation (teacher pretraining). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation [29](#formula_43)for the compute optimal values in Figure [28](#fig_7).

## D.4.7. OPTIMAL TEACHER TRAINING AND STUDENT DISTILLATION TOKENS

To aid in comparing the different compute strategies presented in Appendices D.4.3 to D.4.6, we now present each compute optimal value for all strategies, including supervised. Here, we show compute-optimal distillation student tokens D * S in Figure [31](#fig_24) and compute-optimal teacher pretraining tokens D * T in Figure [31](#fig_24).

$1B 10B 100B 1T 10T 100T 1P 10P 100P Student N S =300M Student N S =500M Student N S =1B Student N S =3B$10 20 10 22 10 24 10 26 1B 10B 100B 1T 10T 100T 1P 10P Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Student Tokens D * S Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Supervised In all scenarios, student tokens should be increased with compute similar to in the supervised case. We see in Figure [30](#fig_23) that, as in Chinchilla [(Hoffmann et al., 2022)](#b26), supervised tokens are increased polynomially with compute. Dis-tillation (best case) follows the exact same allocation, as does distillation (pretraining) with asymptotically large compute. All other methods follow the same increase rate, but with scenario-dependent offsets.

1B 10B 100B 1T 10T 100T 1P 10P 100P Student N S =300M Student N S =500M Student N S =1B Student N S =3B 10 20 10 22 10 24 10 26 1B 10B 100B 1T 10T 100T 1P 10P 100P Student N S =5B 10 20 10 22 10 24 10 26 Student N S =10B 10 20 10 22 10 24 10 26 Student N S =30B 10 20 10 22 10 24 10 26 Student N S =50B Total Compute (FLOPs) Optimal Teacher Tokens D * T Distillation (best case) Distillation (teacher inference) Distillation (teacher pretraining + inference) Distillation (teacher pretraining) Optimal teacher tokens interpolate between scenarios based on compute allocation. In Figure [31](#fig_24) we can see more clearly the interpolation behavior discussed in Appendix D.4.6. At low compute, teacher pretraining and teacher pretraining + inference share optimal solutions because the number of student tokens N * S is small. At high compute, teacher pretraining + inference approaches teacher inference, while teacher pretraining approaches best case, as N * S is large, and costs associated with teacher pretraining become less important. Optimal teacher size interpolate between scenarios based on compute allocation. As in the optimal teacher tokens N * T in Figure [31](#fig_24), the same mechanism causes interpolation behavior in optimal teacher size (see Figure [32](#fig_25)).

## D.4.8. OPTIMAL TEACHER SIZE

$1B 10B 100B Student N S =300M Student N S =500M Student N S =1B Student N S =3B$supervised learning based on a token or cross-entropy threshold, and ii) potentially increased importance of data mixtures (λ ≤ 1, see Appendix G.1) when distilling with significant token and/or compute budgets. We leave this for future work.

In situations where teacher training is required, supervised learning is more efficient. As observed in Appendix D.4.2, for all student sizes, if teacher pretraining is included in the computational cost of producing a student, supervised learning is always more efficient than distilling. This can be seen from Figure [33](#fig_2) as the teacher pretraining (green) and teacher pretraining + inference (red) compute scenarios are above the grey dashed line, which means more compute is needed for distillation than supervised learning in those compute scenarios. This compute efficiency translates into data efficiency (see Figure [34](#fig_8)). 1.6 1.8 2.0 2.2 2.4 2.6 Student N S =10B 1.6 1.8 2.0 2.2 2.4 2.6 Student N S =30B 1.6 1.8 2.0 2.2 2.4 2.6 Student N S =50B Student Cross-Entropy L S Distillation Data / Supervised Data Compute Scenario Distillation (best case) Distillation (teacher pretraining) Distillation (teacher inference) Distillation (teacher pretraining + inference)

$Break-even L(N = N S , D = ∞)$Figure [34](#fig_8). Compute optimal distillation data ratios. For eight student sizes, the number of tokens compute needed to produce a student of the indicated size and cross-entropy. The horizontal dashed line indicates the break-even point, when doing supervised leaning is as data efficient as the corresponding distillation compute scenario. Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing a model of the indicated size and cross-entropy. The vertical dashed line indicates the lowest cross-entropy achievable by that student.

Distillation is more efficient for larger students. In Figure [33](#fig_2) we see in the pretrain + inference scenario, producing a N S =500M student with a cross-entropy of 2.4 has roughly 3/4 the compute cost of producing the same model with supervised learning, whereas producing a N S =10B student with a cross-entropy of 2.2 has roughly 1/2 the compute cost of producing the same model with supervised learning. In terms of data (Figure [34](#fig_8)), the 500M and 10B configurations use roughly 2/3 and 1/2 the number of tokens of their supervised counterparts respectively. The efficiency gains from distillation are potentially greater for larger students when considering compute or data.

## E. Additional Results

In this section, we provide an extensive list of studies, including downstream evaluations of distillation. We cover the models used as teachers, examine the Kullback-Leibler Divergence (KLD) between teacher and student in fixed token-tosize ratios, and present supplementary materials to Section 4.1. Additionally, we investigate the limiting behavior of our scaling law, weak-to-strong generalization, and conduct a model calibration study to assess fidelity. These analyses offer a comprehensive view of the factors influencing distillation performance and the behavior of our proposed scaling laws.

## E.1. Downstream evaluations

In all settings, we optimize for and predict model cross-entropy on the validaiton set. To confirm that the validation crossentropy L S is a good proxy for the downstream evaluation that we ultimately care about, we show how each downstream result is affected by the teacher and student loss. Figure [35](#fig_28) shows a set of English downstream evaluation tasks. ARC Easy [(Bhakthavatsalam et al., 2021)](#b11), ARC Challenge [(Bhakthavatsalam et al., 2021)](#b11), HellaSwag [(Zellers et al., 2019)](#b110), Piqa [(Bisk et al., 2020)](#b13), Sciq [(Welbl et al., 2017)](#b98), WinoGrande [(Sakaguchi et al., 2021)](#b85) and Lambada OpenAI [(Paperno et al., 2016)](#b75) are zero-shot tasks. TriviaQA [(Joshi et al., 2017)](#b53) and WebQS [(Berant et al., 2013)](#b8) are one-shot tasks. TriviaQA evaluation is on the larger and more challenging Web split. CoreEn is the average of both the zero-shot and one-shot tasks.

Finally, we have included GSM8K [(Cobbe et al., 2021)](#b28) and MMLU [(Hendrycks et al., 2021b;](#)[a)](#). GSM8K is used in an 8-shot chain of thought setting, following LLaMA [(Touvron et al., 2023a;](#)[b;](#)[Dubey et al., 2024)](#). MMLU is used in a fiveshot setting. These perform near-random for most of the models, and only show a slightly upwards trend when decreasing student/teacher loss. This is due to the use of the C4 dataset in training, and we note that we do not aim for competitive downstream evaluation results.

All models are evaluated using an internal version of the open-source lm-evaluation-harness [(Gao et al., 2024)](#b19).  

## E.2. Teachers used in distillation

In Figure [36](#fig_30) we show the cross-entropies of the models used as teachers in Section 4.2, and for fitting the supervised scaling law: i) eleven of fixed-M ratio models following the Chinchilla rule of thumb D/N = M * ≈ 20 [(Hoffmann et al., 2022)](#b26), ii) six models on D = 512B tokens (Figure [36a](#fig_30)), and iii) four IsoFLOP profiles (Figure [36b](#fig_30)). Together this produces 74 runs corresponding to tuples of (N, D, L).  Coefficient estimation (Appendix F.1) yields the scaling coefficients shown in Table [6](#), and a scaling law which has ≲ 1% relative prediction error, including when extrapolated from weaker to stronger models (see Figure [5a](#fig_5)). In Figure [37](#fig_2), the capacity gap in knowledge distillation can be seen. Improving a teacher's performance does not always improve a student's, and even reduces the performance after a certain point. The KLD between teacher and student is an increasing function of teacher size in all cases, which means as the teacher improves its own performance, the student finds the teacher more challenging to model, which eventually prevents the student from taking advantage of teacher gains. See Appendix E.8.2 for an investigation using calibration to understand where this mismatch occurs.

## E.4. Full distillation scaling law IsoFLOP profiles

In Figure [38a](#fig_33) we provide the full six fixed M Teacher/IsoFLOP Student profiles, only two of which were shown in Figure [2](#). These experiments enable the reliable determination of α ′ , β ′ , γ ′ , A ′ and B ′ . In Figure [38b](#fig_33) we provide the full four IsoFLOP teacher/ fixed M student, only two of which were shown in Figure [3](#fig_2). These experiments enable the reliable determination of c 0 , c 1 , f 1 and d 1 .

Strong-to-weak generalization occurs. For the weaker teachers (N T ≤ 2.72B), The horizontal dashed line in each pane shows the cross-entropy achieved by the teacher (Appendix E.2). we see that for students larger than the teacher (N S > N T ) and for sufficiently large compute budgets, the student is able to outperform the teacher (see Appendix E.7 for a detailed one-dimensional slice).

A stronger teacher signal is needed in order for stronger students to outperfom the supervised baseline. The horizontal dashed line in each pane shows the cross-entropy achieved by the student if trained using supervised learning (Appendix E.2). We see that weaker students benefit more from distillation, as e.g. the 198M student has all observed data below this dashed line, meaning all distillations outperform the supervised baseline. However, for the 1.82B student, only 10 21 FLOP teachers produce distilled students that outperform the supervised baseline.

2.4 2.5

## 2.6

Teacher

$N T =546M Teacher N T =975M 2.2 2.4 2.6$Teacher

$N T =1.82B Teacher N T =2.72B 100M 300M 1B 3B 7B 2.2 2.4 2.6$Teacher N T =4.82B  

## E.5. Distillation scaling law IsoFLOP optima

The optimal loss values of each IsoFLOP in Figure [38a](#fig_33)

## E.6. Distillation with infinite data

From the supervised scaling law (Equation [1](#formula_0)) a model with N parameters has a cross-entropy lower bound

$L(N ) ≡ L(N, D = ∞) = E + (AN -α ) γ(35)$which represents the best solution to the training objective subject to constraints from that model's hypothesis space [(Hoffmann et al., 2022)](#b26) and is achieved when the number of training tokens is large (D → ∞). As the hypothesis space of a model is independent of the procedure used to find the solutions, we anticipate that the student with N S parameters has a cross-entropy lower bound that is the same as the supervised one Equation [35](#formula_51). However, it not immediately clear if this is true in practice, since

$L S (N S ) ≡ L S (N S , D S = ∞, L T = L * T ) (36) = L * T + (A ′ N -α ′ S ) γ ′ (L * T ) c0 1 + L * T d -1 1 L(N S ) 1/f1 -c1f1 ,(37)$where L * T = arg min L (N S , D S = ∞, L T ) is the teacher cross-entropy that minimizes Equation [8](#). Upon checking numerically, we do find that Equation 35 is consistent with Equation 37 for a range of models N, N S ∈ [100M, 100B] (Figure [40](#fig_36)). We stress that unlike our three motivations for the equation properties (Section 4.3), this infinite data limit was imposed added by hand, and is only true for certain values scaling coefficients. This lower bound consistency is evidence that that our distillation scaling law has desired behavior far outside of observed models, at least along the data and teacher axes. We also note that only the optimal teacher for each student size produces a student cross-entropy lower bound that is consistent with the supervised one. Any other choice produces higher student cross-entropies, either because the teacher is too weak, or due to the capacity gap. For the optimal choice of teacher, the loss achieved by all student sizes under distillation is consistent with the loss achievable by supervised learning. This is not true for any choice of teacher, only the optimal one, which can be determined through numerical optimization of the provided distillation scaling laws (see Section 5).

## E.7. Weak-to-strong generalization

In Figure [41](#fig_37) we see that weak-to-strong generalization [(Burns et al., 2024;](#b19)[Ildiz et al., 2024)](#) occurs only in the finite distillation data regime, and when the number of tokens is sufficiently large, the student cross-entropy increases again, eventually matching the teacher cross-entropy. This can be understood in the following way: i) when the student is larger than the teacher, the student contains in its hypothesis space the function represented by the teacher, ii) when the student is shown the teacher outputs on enough of the data manifold, it eventually matches what the teacher does on the whole data manifold. We note this doesn't explain how and why the student outperforms its teacher, and only constrains its asymptotic (low and high distillation data) behaviors. 

## E.8.2. 198M STUDENTS TRAINED ON 20N TOKENS

In this section we consider students trained on the teacher distribution, as in our main study. We also study students trained on the teacher top-1 distribution, as described in Appendix G.4, as the qualitative difference in behavior can be informative for student design.

Evaluating the calibration of a student can be done in a number of ways:

1. We can compare student outputs relative ground-truth data, as in Appendix E.8.1 for the teachers.

2. We can compare student outputs with the outputs of its teacher.

Calibration against ground-truth. First, let's consider comparison against ground truth data. In Figure [43](#fig_39) we show student calibration with respect to the dataset labels for both teacher distribution distillation and teacher top-1 distillation.

1. Distilled on the full teacher distribution. In Figure [43a](#fig_39), we observe that the student is well-calibrated against ground truth data. Similar to the teacher's calibration plot in Figure [42](#fig_8), we see a small discrepancy at very low and very high confidence values, and the ECE value is low.

2. Distilled on teacher top-1. In Figure [43b](#fig_39), we see that a student trained only on its teacher's top-1 prediction, is not calibrated against ground truth data. The blue points below the dashed line indicate an overconfident student, i.e. , its predicted confidence is higher than the actual accuracy in that confidence range. This is because training the student on top-1 assigns the student to the most plausible outcome rather than all the plausible outcomes with correct frequencies. Confidence proportions are low for all bins that are not the most confident bin, and ECE is high, although decreases with increasing teacher size N T .

Figure [43](#fig_39) shows that training the student on the teacher's distribution results in a calibrated student, whereas training on the teacher top-1 does not. Indeed, optimizing against the teacher's top-1 is not a proper scoring metric, and that teacher top-1 is not an unbiased estimator for the data, while the teacher distribution is.  Calibration against teacher top-1. Next we investigate the first student calibration against the teacher. In Figure [44](#fig_41) we show student calibration with respect to the teacher's top-1 label. That is, the next-token label used for accuracy computation, and extract the students confidence is the most probable next-token according to the teacher, instead of the label from data. Here no next token labels are used at all. These teacher top-1 labels are also used for the ECE calculation, which is still computed using Equation [38](#).

1. Distilled on the full teacher distribution. We see in Figure [44a](#fig_41) that when distilled from the full teacher distribution, the student is not calibrated against the teacher top-1. The blue points are above the dashed line, which means that the empirical accuracy is higher than the model's predicted confidence, i.e. with respect to the teacher top-1, the student is underconfident. This can be understood by noting that the top-1 objective is an easier objective than modeling the full vocabulary at each step.

2. Distilled on teacher top-1. In Figure [44b](#fig_41) we observe that a student is distilled from its teacher's top-1 is calibrated with respect to teacher's top-1.  Figure [44](#fig_41) shows that training the student on teacher top-1 results in calibration against teacher top-1, whereas a model trained on data, or distilled on the full teacher distribution is not calibrated against teacher top-1. As above, this can be understood as now teacher's top-1 is now a proper scoring metric, and teacher top-1 is an unbiased estimator for itself.

Calibration against teacher distribution. Here we develop a modified calibration measure that will help us understand if the student matches the teacher in a distributional sense. As we have two distributions to compare, we can ask, for a given teacher confidence, what is the expected student confidence. This leads to ECE Dist , a distributional form of ECE:

$ECE Dist (A, B) = M m=1 |B m | N Samples |Confidence(B m ; A) -Confidence(B m ; B)| ,(39)$and is similar in spirit to divergence measures like KLD. B m , |B m |, and N Samples are defined as before, and Confidence S (B m ; A|B) is the average confidence of model A or B in bin m respectively. The bins G m are always witin the bins of confidence of model B. In the current evaluation, we take A as the teacher and B as the student, and we are measuring the average confidence of the teacher is measured within a student's confidence bin.

1. Distilled on the full teacher distribution. In Figure [45a](#fig_43), we see that when the student is confident, it matches the teacher confidence. However, as the teacher model grows in size, when the student is less confident, it it systematically underestimates its confidence. This suggests that the student has not effectively learned low-probability outcomes, or that these outcomes are particularly challenging for the student to replicate. The underconfidence in these regions may be a result of the distillation process not providing sufficient learning signal for these difficult cases, or the inherent difficulty of capturing the uncertainty associated with low-confidence predictions. This observation of confidence mismatch helps indicate which parts of the distribution the student finds challenging to model, giving rise to the increasing KLD and capacity gap observed in Figure [4](#fig_3) and Appendix E.3.

2. Distilled on teacher top-1. In Figure [45b](#fig_43), for small teachers, we observe student overconfidence. As the teacher increases in size, the student's overconfidence in low-confidence bins transitions to underconfidence. At the same time, the student's overconfidence in high-confidence bins improves, leading to an overall reduction in distributional ECE. This pattern of overconfidence in the student is similar to what we saw in Figure [43b](#fig_39), but the change in behavior at low-confidence bins as the teacher's size varies is different. This shift in the student's calibration behavior, especially in low-confidence bins, aligns with findings from Figure [45a](#fig_43) and may highlight the difficulty the small student faces in learning rare events.  We can also inspect the student confidences within a bin of teacher confidences, and compute the distributional ECE (Equation [39](#formula_53)), swapping the roles of teacher and student (see Figure [46](#fig_45)).

1. Distilled on the full teacher distribution. In Figure [45a](#fig_43) we complete the picture from Figure [45a](#fig_43) and see that the part of the distribution the student struggles to model is actually the place where teacher is most confident.

2. Distilled on teacher top-1. In Figure [45b](#fig_43) we see that the student is systematically overconfident for all values of teaacher confidence, except for the largest teachers, where the student is underconfident when those teachers are most confident.  E.8.3. 198M STUDENTS TRAINED ON 128B TOKENS In this section, we study the effect of increasing the number distillation tokens in Appendix E.8.2 from D S ≈ 20N S to D S ≈ 512B. Here, we reserve discussion for the observed differences compared to Appendix E.8.2. 0.0 0.5 1.0 NT =198M ECE=0.1% NT =546M ECE=0.1% NT =975M ECE=0.2% NT =1.82B ECE=0.3% 0.0 0.5 1.0 0.0 0.5 1.0 NT =2.72B ECE=0.4% 0.0 0.5 1.0 NT =4.82B ECE=0.5% 0.0 0.5 1.0 NT =7.75B ECE=0.4% Confidence Proportion(confidence) Perfectly calibrated Student Confidence Student Accuracy (a) Train target: teacher distribution. 0.0 0.5 1.0 NT =198M ECE=42.1% NT =546M ECE=37.1% NT =975M ECE=34.2% NT =1.82B ECE=31.7% 0.0 0.5 1.0 0.0 0.5 1.0 NT =2.72B ECE=29.3% 0.0 0.5 1.0 NT =4.82B ECE=26.5% 0.0 0.5 1.0 NT =7.75B ECE=24.8% Confidence Proportion(confidence) Perfectly calibrated Student Confidence Student Accuracy (b) Train target: teacher Top 1. Calibration against ground-truth. As the number of distillation tokens increases, we observe a consistent decrease in the ECE when the student is trained on the teacher's distribution, as shown by the comparison between Figure [47a](#fig_46) and Figure [43a](#fig_39) across different teacher sizes. However, when the student is trained on the teacher's top-1 predictions, increasing the number of tokens negatively impacts ECE, as evidenced by the comparison between Figure [47b](#fig_46) and Figure [43b](#fig_39). This suggests that the teacher's top-1 predictions are not a reliable, unbiased estimator of the actual data, and increasing the number of training tokens only exacerbates this issue. See Appendix G.4 for further discussion.

Calibration against teacher top-1. Increasing the number of distillation tokens leads to worse calibration between the student and the teacher's top-1 predictions when the student is trained on the full distribution. This change primarily occurs in the low-confidence bins, and results in a higher ECE (compare Figure [48a](#fig_49) and Figure [44a](#fig_41)). However, when comparing the ECEs for the student trained on the teacher's top-1 predictions (Figures [44b](#fig_41) and [48b](#fig_49)), there is an improvement across all teacher sizes. When the student is trained and evaluated using the same metric, increasing the training tokens helps improve calibration, demonstrating consistency between the learning objective and the evaluation metric.   Calibration against teacher distribution. A comparison between Figure [49a](#fig_51) and Figure [45a](#fig_43) shows that when the student is trained on the teacher's full distribution and evaluated against the full distribution using Equation [39](#formula_53), increasing the number of training tokens consistently improves calibration across all teacher sizes. However, when the student is trained on the teacher's top-1 predictions, a quick comparison between Figure [49b](#fig_51) and Figure [45b](#fig_43)   Similarly, when comparing within teacher confidence bins (Figure [50](#fig_53)) increasing the number of distillation tokens from 20N to 128B primarily amplifies the observed phenomena at lower distillation token budgets, and improving calibration in cases where there is a proper scoring metric present (Figure [50a](#fig_53)).  In general, increasing the number of training tokens has a positive effect when the training metric is an unbiased estimator of the actual data or the measured calibration quantities (see Figures [47a, 48b](#fig_49) and [49a](#fig_51)) and reduces the ECE, while it has a negative impact when there is a mismatch between the learned and measured quantities (see Figures [47b, 48a](#fig_49) and [49b](#fig_51)).

## F. Scaling coefficients

In this section, we analyze the process of deriving the coefficients for our scaling law. We follow the procedure outlined in [(Hoffmann et al., 2022;](#b26)[Besiroglu et al., 2024)](#), while incorporating our modified scaling laws F.1. Supervised scaling law coefficient estimation First, let's tackle the supervised scaling law Equation 1 restated for convenience

$L(N, D) = E + A N α + B D β γ .(40)$To aid numerical stability, we write this expression in log space. First note that for a, b > 0

$log(a + b) = log (exp log a + exp log b) = LSE(log a, log b),(41)$where LSE is the log-sum-exp operator. We can now proceed to write the supervised scaling law in log form

$log L(N, D; A, B, E, α, β) = log E + A N α + B D β γ (42) = LSE log E, γ log A N α + B D β (43) = LSE [log E, γ LSE (log A -αN, log B -αD)] .(44)$We make no assumptions about the relationships between the values (i.e. no parameter tying) and optimize

$(A * , B * , E * , α * , β * , γ * ) = arg min {A,B,E,α,β,γ} i Huber δ log L(N (i) , D (i) ; A, B, E, α, β) -L (i)(45)$with a Huber δ = 10 -4 , where N (i) , D (i) and L (i) are the model size, number of training tokens and loss achieved by the i-th run. We fit on 73 samples over a grid of L-BFGS-B initializations given by: log A ∈ {0., 5., 10., 15., 20.}, log B ∈ {0., 5., 10., 15., 20.}, log E ∈ {-1., -0.5., 0., 0.5, 1., 1.5.}, α ∈ {0., 0.5, 1., 1.5}, β ∈ {0., 0.5, 1., 1.5}, γ ∈ {0., 0.5, 1., 1.5}. The L ≥ 2.2 case corresponds to 48 samples.

## F.2. Distillation scaling law coefficient estimation

Next, let's address the distillation scaling law Equation 8 restated for convenience

$L S (N S , D S , L T ) = L T + 1 L c0 T 1 + L T L S d 1 1/f1 -c1 * f1 A ′ N α ′ S + B ′ D β ′ S γ ′ . (46$$)$As in Appendix F.1, to aid numerical stability during optimization, we write this in log space

$log L S (N S , D S , L T ; θ) = log   L T + 1 L c0 T 1 + L T L S d 1 1/f1 -c1 * f1 A ′ N α ′ S + B ′ D β ′ S γ ′   (47) = LSE log L T , -c 0 log L T -c 1 f 1 log 1 + L T d 1 L S 1/f1 + γ log A ′ N α S + B ′ D β S (48) = LSE log L T , -c 0 log(L T ) -c 1 f 1 LSE 0, 1 f 1 log L T -log L S -log d 1 + γ LSE (log A ′ -α ′ log N S , log B ′ -β ′ log D S ) ,(49)$We examine various λ values across different teacher-student configurations in Figure [51a](#fig_5) and find that while the optimal mixing coefficients λ * vary based on the specific teacher-student combinations (Figure [51b](#fig_5)), the student cross-entropy L S remains mostly flat for choices of λ > 0.5, with lower values of λ only preferred in the cases where the teacher is particularly weak and where the supervised signal is more informative. From Figure [51a](#fig_5) it is also possible to get a sense of when distillation λ > 0 generally outperforms supervised learning λ = 0 under the same token budget.

To guide practitioners, Figure [51b](#fig_5) shows empirically derived optimal mixing coefficients, λ * , though the simplicity and robustness of pure distillation makes it a reliable default choice for practical use and study.

## G.2. Temperature (τ ) sensitivity analysis

In distillation, the temperature τ controls the entropy of teacher predictions by scaling logits z (i)

T /τ and z (i) S /τ in the knowledge distillation loss L KD (Equations 7 and 53). This scaling modulates the transfer of dark knowledge [(Hinton et al., 2015)](#b45) -the log-probability ratios between incorrect categories encode the teacher's understanding of relationships between those categories. Our analysis across τ ∈ [0.5, 10] (Figure [52](#fig_54)) reveals that higher temperatures (τ > 3) reduces performance by attenuating these ratios in σ a (z (i) T /τ ), particularly harming smaller students that rely heavily on this signal. Lower temperatures (τ < 1) similarly reduce effectiveness by concentrating probability mass on argmax tokens, diminishing the transfer of relationships between lower-ranked predictions.

We find optimal performance at τ = 1 across all model scales, suggesting this temperature best preserves log-probability structure. Unlike the original distillation setting, which relied on dark knowledge to represents hierarchical relationships between incorrect classification predictions in the presence of a true label, language modeling is inherently ambiguous and complex, with many valid continuations. It is precisely the understanding of the ambiguity of language we want to transfer to the student, which is supported by our finding that maintaining the teacher's original probability ratios (τ = 1) produces the lowest student cross-entropies. 

## G.3. Learning rate (η) sensitivity analysis, verification of µP for distillation

The peak learning rate η determines the scale of student parameter updates in distillation. In our experiments we use a simplified version of µP [(Yang & Hu, 2021;](#b104)[Yang & Littwin, 2023;](#)[Yang et al., 2022;](#b31)[Wortsman et al., 2023;](#)[Yang et al., 2023)](#b114), described as µP (simple) in [(Wortsman et al., 2024)](#b100).

In the supervised case, in addition to improving the performance lower bound compared to the standard parameterization, µP simplifies experimental settings as it enables hyperparameter transfer; the optimal peak learning rate η and initialization scales found for a reference model size can be reused when changing model size[foot_7](#foot_7) .

Here we validate that the optimal peak learning rate η * = 0.01 determined in the supervised case transfers to the distillation setting. Sweeping values η ∈ [0.001, 0.1] (Figure [53](#fig_55)) reveals that µP achieves optimal performance at η = 0.01 uniformly across all configurations, from 198M to 1.82B parameter students and 546M to 7.75B parameter teachers, consistent with the optimal peak learning rate in the supervised setting.

Performance varies smoothly and modestly around this optimum, with cross-entropy changing by less than 0.1 nats over one order of magnitude in learning rate. This consistency validates µP's guarantee of scale-invariant training dynamics for distillation, confirming that our experimental setting for determining our distillation scaling law operates at the optimal learning rate or sufficiently close to it in all of our settings. The observed moderate learning sensitivity in distillation partially alleviates the requirement for careful learning rate tuning, showing that in practice the reference learning rate found in the supervised setting can be safely reused in the distillation setting. We investigate how the truncation of the teacher distributions affects student performance. For these methods, when the teacher produces a distribution pT (x (i) = a|x (<i) ), a ∈ {1, . . . , V } over the vocabulary for the student to match, only some entries in the distribution are used. This is done primarily to reduce repeated inference and storage costs in the case teacher outputs are being stored for re-use in the multiple distillations scenario discussed in Section 5.3. In our case, the vocabulary size V = 32168, so assuming storage in float32, means each token requires 32168 × 4 bytes ≈ 129KB, and storing all of C4 (approximately 2T tokens) would take approximately 260 Petabytes, a significant amount of data, roughly the total amount collected during the first ten years of the Large Hadron Collider (LHC) [(CERN, 2018)](#b21).

Given a truncation method M, can a truncated teacher output p(M)

T can be stored whilst still achieving the gains of distillation? Concretely, the truncation p (M) (x|c) of a distribution p(x|c) with a truncation method M is 

where S M (p( • |c)) represents the set of retained categories (i.e. non-zero probabilities) in the truncated distribution, which then undergoes renormalization over the retained categories.

We explore two complementary approaches: Top-k and Top-p (nucleus) sampling. As in all of our settings, we evaluate the student cross-entropy against the data distribution with all categories, as this is the model property we are most interested in (a model can trivially match the target distribution if all categories except one are removed). For Top-k, we zero-out all but the largest k probabilities, and Top-p, we zero-out all but the smallest set of probabilities that sum to at least p. 

As the truncation parameters increase (k → V or p → 1), both methods approach the full teacher distribution, and the student's cross-entropy converges to the baseline using the entire pT . Conversely, aggressive truncation (small k or p) induces quantization that preserves only high-probability tokens while discarding information in the tail of the distribution.

Our empirical analysis (Figure [54](#fig_57)) reveals that both truncation methods directly correlate with reduced evaluation likelihoods. However, this performance degradation can be effectively mitigated through a combination of truncated distributions and ground truth next-token prediction using a mixing coefficient λ ∈ (0, 1) (Equation [7](#)). Specifically, with k = 128 and λ = 0.7, we achieve validation losses statistically indistinguishable from those obtained using the complete teacher distribution. For large-scale distillation scenarios where maintaining multiple models in memory is prohibitive, particularly with large teacher models, storing only the Top-k teacher predictions (with λ > 0) enables efficient post-hoc distillation.

## G.5. Forward and reverse KL divergence

We investigate both forward (mode spreading) and reverse (mode seeking) Kullback-Leibler divergences for distillation from N T = 1.82B to N S = 546M. The forward KLD D KL (p T ||q S ) (Equation [7](#)), minimizes L forward = H(p T , qS ) -H(p T ), where H(p T ) is dropped during optimization as it depends on only fixed teacher parameters. In contrast, the reverse KLD D KL (q S ||p T ) requires explicitly computing the student's entropy, L reverse = H(q S , pT ) -H(q S ).

The forward KL achieves a lower data cross-entropy compared to the reverse KL (Table [7](#tab_28)), with an average improvement of 0.28 nats. This suggests that explicitly regularizing with respect to the student's entropy during training may not provide additional benefits for distillation quality. Given both the improved performance and reduced computational overhead of forward KL (which avoids computing student entropy), we recommend using standard forward KL for distillation. 

## H. Parameters and Floating Operation Estimation

Here we outline the number of parameters (Appendix H.2) and the number of FLOPs per token (Appendix H.3) for our experimental settings. The symbol notation is provided in Table [8](#tab_29). For our scaling laws, we find, as in [Kaplan et al. (2020)](#b54) using that the number of non-embedding-parameters provides the cleanest fit and extrapolation behavior.

Our expressions for approximate compute (FLOPs per token) differ from prior work in that we are interested in small models that are capable. This means we are unable to ignore the context-dependent term that arises from the quadratic computational complexity of the attention mechanism. As our architectures are fixed aspect ratio, there is a modified approximation we can use. This expression is discussed in Appendix H.1

For ease of reference, we provide a comparison of the expressions we use to commonly used existing expressions [(Kaplan et al., 2020;](#b54)[Hoffmann et al., 2022;](#b26)[Narayanan et al., 2021)](#b72), and provide comments for significant differences. 8 It was shown in [Porian et al. (2024)](#b81) that ignoring the embedding parameters and FLOPs can lead to systematic estimation bias for small models, and is one of the primary drivers between different exponents reported in [Kaplan et al. (2020) and](#b54)[Hoffmann et al. (2022)](#b26). We find that the the non-embedding parameters gives a tighter scaling behavior. However, in the fixed-aspect-ratio setting, we are able to use both the non-embedding parameters in the scaling law and the approximate total compute simultaneously, removing estimation bias. Indeed, in the supervised setting, our coefficients a and b are consistent with those from Hoffmann et al. ( [2022](#)) (see Table [6](#)).

## I. Model architecture

All models are based on [Gunter et al. (2024)](#) and are trained using AXLearn [(Apple, 2023)](#b4). All models use decoupled weight decay [Loshchilov & Hutter (2019)](#b61) of 10 -4 for regularization, as well as a simplified version of µP [(Yang & Hu, 2021;](#b104)[Yang & Littwin, 2023;](#)[Yang et al., 2022;](#b31)[Wortsman et al., 2023;](#)[Yang et al., 2023)](#b114), following what is described as µP (simple) in [(Wortsman et al., 2024)](#b100). Because of µP (simple), we fix the learning rate to 1e-2 across all model sizes. Multiheaded attention (MHA) is used (g size = 1), with Pre-Normalization [(Nguyen & Salazar, 2019)](#) using RMSNorm [(Zhang & Sennrich, 2019)](#b111). We train all models with a sequence length of n ctx = 4096, with RoPE [(Su et al., 2024)](#) positional embeddings (base frequency set to 500k). All model architectures in this work are presented in Table [13](#tab_10), have a fixed aspect ratio d model = 128 and a fixed ffn ratio ρ ffn = 8/3 coupled with gated linear activation (n ffn = 3).

Table [13](#tab_10). The models used in this work. The different parameter values and FLOPs per token are shown in billions. N is the number of non-embedding parameters and isthe value we use in our scaling laws. Ntotal counts all parameters in the model.Cfwd is the total number of forward FLOPs per token given by the fulltotal in Tables [11](#tab_0) and [12](#tab_6).C fwd-approx(2N ) is the estimated value of forward FLOPs per tokenbased on the 2N approximation, and is accompanied by its relative error.C fwd-approx(2N +σ) is the estimated value of forward FLOPs per tokenbased on the approximation given in Equation [69](#), and is accompanied by its relative error.

The C fwd-approx(2N +σ) is the one we use in this work. Name N (B) N total (B) n layers d model d ff C fwd (B) C fwd-approx(2N ) (B) C fwd-approx(2N +σ) (B) 103M 0.1028 0.1363 8 1024 2816 0.3411 0.2056 (-39.74%) 0.3398 (-0.39%) 143M 0.1434 0.1811 9 1152 3072 0.4487 0.2867 (-36.10%) 0.4471 (-0.34%) 198M 0.1983 0.2402 10 1280 3456 0.587 0.3965 (-32.44%) 0.5853 (-0.29%) 266M 0.2657 0.3118 11 1408 3840 0.7524 0.5314 (-29.38%) 0.7505 (-0.25%) 340M 0.3398 0.3901 12 1536 4096 0.9333 0.6796 (-27.19%) 0.9312 (-0.22%) 435M 0.4348 0.4893 13 1664 4480 1.158 0.8695 (-24.91%) 1.156 (-0.19%) 546M 0.546 0.6047 14 1792 4864 1.417 1.092 (-22.96%) 1.415 (-0.17%) 664M 0.6636 0.7265 15 1920 5120 1.692 1.327 (-21.54%) 1.689 (-0.15%) 810M 0.8096 0.8767 16 2048 5504 2.025 1.619 (-20.03%) 2.022 (-0.14%) 975M 0.9755 1.047 17 2176 5888 2.4 1.951 (-18.69%) 2.397 (-0.12%) 1.15B 1.147 1.222 18 2304 6144 2.787 2.293 (-17.72%) 2.784 (-0.11%) 1.35B 1.355 1.434 19 2432 6528 3.25 2.709 (-16.65%) 3.247 (-0.10%) 1.59B 1.586 1.67 20 2560 6912 3.763 3.172 (-15.70%) 3.759 (-0.09%) 1.82B 1.821 1.909 21 2688 7168 4.284 3.642 (-14.99%) 4.28 (-0.09%) 2.1B 2.102 2.194 22 2816 7552 4.899 4.203 (-14.21%) 4.895 (-0.08%) 2.41B 2.41 2.506 23 2944 7936 5.571 4.819 (-13.49%) 5.567 (-0.07%) 2.72B 2.718 2.819 24 3072 8192 6.246 5.436 (-12.96%) 6.241 (-0.07%) 3.08B 3.082 3.187 25 3200 8576 7.034 6.165 (-12.36%) 7.03 (-0.06%) 3.48B 3.478 3.587 26 3328 8960 7.887 6.956 (-11.81%) 7.883 (-0.06%) 3.87B 3.87 3.983 27 3456 9216 8.736 7.74 (-11.40%) 8.731 (-0.05%) 4.33B 4.329 4.446 28 3584 9600 9.72 8.658 (-10.93%) 9.715 (-0.05%) 4.82B 4.823 4.944 29 3712 9984 10.78 9.646 (-10.49%) 10.77 (-0.05%) 5.31B 5.309 5.434 30 3840 10240 11.82 10.62 (-10.16%) 11.81 (-0.05%) 5.87B 5.873 6.003 31 3968 10624 13.02 11.75 (-9.78%) 13.01 (-0.04%) 6.48B 6.476 6.611 32 4096 11008 14.3 12.95 (-9.43%) 14.29 (-0.04%) 7.07B 7.066 7.204 33 4224 11264 15.56 14.13 (-9.16%) 15.55 (-0.04%) 7.75B 7.747 7.889 34 4352 11648 17 15.49 (-8.85%) 16.99 (-0.04%) 8.47B 8.47 8.617 35 4480 12032 18.52

![Figure 1. Extrapolations of the Distillation Scaling Law. The distillation scaling law (Equation8) is fitted on weak students (LS > 2.3) for a range of teachers with losses LT . Solid lines represent predicted model behavior for unseen teachers for a given student configuration (interpolation), and dashed lines represent predicted model behavior outside of seen teachers and for the strong student region (LS ≤ 2.3). As shown, the student can outperform the teacher (see Figures2, 3and 41 for details).]()

![. One could use compute optimal models whose size parameters N * and number of training tokens D * gives the lowest crossentropy subject to a compute constraint C N * , D * = arg min N,D L(N, D) s.t. FLOPs(N, D) = C. (2)]()

![Figure 3. IsoFLOP Teacher/Fixed M Students. (a) One (of four) student sizes trained with a MS = DS/NS = 20 are distilled from teachers with four IsoFLOP profiles. See Appendix E.4, Figure 38b for all profiles. (b) All profiles against teacher cross-entropy. Horizontal (vertical) dashed lines show student supervised cross entropy LS (student size NS).]()

![Figure 4. Fixed M Teacher/Fixed M Student. Students of two sizes trained with different MS = DS/NS = 20 ratios are distilled from teachers with MT = DT /NT ≈ 20.]()

![Figure 5. Scaling law fits. (a) The supervised scaling law (Equation 1) applied to the data in Figure 36a. (b) Our distillation scaling law (Equation 8) applied to the data in Figures 2 to 4.]()

![Figure 6. Fixed-M Teacher/IsoFLOP students (data). For a student size NS and token budget DS, the cross-entropy difference between best case distillation and supervised learning. Blue indicates distillation outperforms supervised learning, red otherwise. The white horizontal dashed line indicates teacher size.]()

![Figure8. Compute optimal distillation student performance. For four student sizes , the best cross-entropy each student can achieve the five scenarios considered as total compute is varied.]()

![.4 Distillation (teacher inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.5 Distillation (teacher pretraining) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.6 Distillation (teacher pretraining + inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.7 Optimal teacher training and student distillation tokens . . . . . . . . . . . . . . . . . . . . . . . D.4.8 Optimal teacher size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Compute and data efficiency gains for distillation compared to supervised learning . . . . . . . . . . . . E Additional Results E.1 Downstream evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Teachers used in distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Fixed-M teacher/fixed-M students and the capacity gap . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Full distillation scaling law IsoFLOP profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Distillation scaling law IsoFLOP optima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Distillation with infinite data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 Weak-to-strong generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8 Model calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.1 Teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.2 198M students trained on 20N tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.3 198M Students trained on 128B tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F Scaling coefficients F.1 Supervised scaling law coefficient estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Distillation scaling law coefficient estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Scaling law coefficients parameteric fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G Distilling language models in practice G.1 Mixing coefficient (λ) sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Temperature (τ ) sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Learning rate (η) sensitivity analysis, verification of µP for distillation . . . . . . . . . . . . . . . . . . . G.4 Distribution truncation methods: Top-k and Top-p sensitivity . . . . . . . . . . . . . . . . . . . . . . . . G.5 Forward and reverse KL divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H Parameters and Floating Operation Estimation H.1 Alternative approximation for FLOPs per token as a function of N . . . . . . . . . . . . . . . . . . . . . H.2 Model parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.3 FLOPs per token . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .]()

![Beyond empirical insights,Menon et al. (2020) established a bias-variance tradeoff for the student, quantifying how access to teacher logits can significantly enhance learning. Meanwhile,Pareek et al. (2024) investigated self-distillation, where the student and teacher share the same architecture and size, to assess the potential gains from repeatedly applying knowledge distillation. While most studies assume the teacher is a larger model, recent work explores weak-to-strong generalization, where a weaker model distills knowledge into a stronger one. This concept, introduced byBurns et al. (]()

![Case 1: m < n. (Student error is non-increasing in m) Claim. For 1 ≤ m < n, we have e student (m + 1, n, T, D) ≤ e student (m, n, T, D).]()

![student (m + 1, n, T, D) ≥ e student (m, n, T, D), demonstrating that the error increases monotonically with m once m ≥ n. Conclusion (U-shaped trend). Combining these two cases: For 1 ≤ m < n : e student (m, n, T, D) monotonically decreasing in m, For m ≥ n : e student (m, n, T, D) monotonically increasing in m.]()

![Figure 10. Distillation in kernel regression. We randomly sample the α = {α1, ..., α1000} coefficients of the target function uniformly in the range [-1, 1]. We fix T = 5, D = 4.5 and compute the optimal student and teacher errors according to Lemmas C.1 and C.2 for various values of n (dashed curves), and for m ∈ [1...1000].As can be seen, the student error exhibits a U shaped error curve as predicted by the theory, where the error starts to increase when m ≥ n. The black solid line indicates the teacher error, which always decreases with increasing m.]()

![Figure 11. Student performance when varying teacher width. (a) Student cross-entropy as teacher width dffn is varied. (b) Student accuracy as teacher width dffn is varied. Bands show the (25%,75%) values across four trials.]()

![Figure 12. Student performance when varying teacher training data. (a) Student cross-entropy as teacher training data is varied. (b) Student accuracy as teacher training data is is varied. Bands show the (25%,75%) values across four trials.]()

![Figure17. Compute optimal distillation student cross-entropies. For eight student sizes, the optimal student validation cross-entropy L * S in each of the distillation scenarios considered as the total compute is varied.]()

![Figure 19. Compute optimal configuration contours for distillation (best case). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for best case in Figure 17 for a range of student sizes. (N * T , D * T ) are the supervised compute optimal combination giving rise to L * T in Figure 18.]()

![and tokens D * T move together to produce the L * T in Figure 18. Again, the exact values of N * T , D * T in Figure 20 represent the supervised compute optimal solution for producing the L * T , but are not the only solution in this compute scenario, since N * T , D * T are not uniquely determined by the compute constraint. D.4.4. DISTILLATION (TEACHER INFERENCE) In the distillation (teacher inference) scenario, δ Lgt T = 1 , δ Pre T which means that we account for compute associated with the standard supervised learning case as well as the cost for producing the logits for the student FLOPs(N S , D S , N T , D T ) ≈ 3F (N S )D S Student Training + F (N T )D S Teacher Logits .]()

![Figure 21. Compute optimal configuration contours for distillation (teacher inference). The compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher inference in Figure 17.]()

![Figure23. Compute optimal allocations for distillation (teacher inference). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation29for the compute optimal values in Figure22.]()

![Figure25. Compute optimal configurations for distillation (teacher pretraining). For eight student sizes, the compute optimal quantities (D * S , N * T , D * T ) giving rise to the student cross entropies for teacher pretraining in Figure17. This is a one-dimensional size of Figure24.]()

![Figure26. Compute optimal allocations for distillation (teacher pretraining). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation29for the compute optimal values in Figure25.]()

![Figure30. Compute optimal distillation student tokens. For eight student sizes, the compute optimal student tokens D * S giving rise to the student cross-entropies for all compute scenarios, including supervised.]()

![Figure31. Compute optimal distillation teacher tokens. For eight student sizes, the compute optimal teacher tokens D * T giving rise to the student cross-entropies for all compute scenarios.]()

![Figure32. Compute optimal distillation teacher size. For eight student sizes, the compute optimal teacher size N * T giving rise to the student cross-entropies for all compute scenarios.]()

![Figure 35. All student downstream evaluations. For a discussion of the individual metrics and datasets, see Appendix E.1.]()

![Figure 36. Supervised IsoFLOPs. (a) The cross-entropy of supervised models trained with either a Chinchilla optimal M = D/N ≈ 20 or on 512B tokens. (b) The cross-entropy supervised models trained with four ISOFLOP profiles C ∈ {3 × 10 19 , 10 20 , 3 × 10 20 , 10 21 }. (c) The optimal supervised parameters N * (C) = arg min N L(C) for each IsoFLOP profile, and the loss L * (C) achieved by that model.]()

![Figure 37. Fixed M Teacher/Fixed M Student. Students of three sizes trained with different MS = DS/NS = 20 ratios are distilled from teachers with MT = DT /NT ≈ 20. This is a more complete version of Figure 3.]()

![Teacher/Fixed M Student profiles.]()

![Figure 38. Supervised IsoFLOPs. (a) Teachers of six sizes with MT = DT /NT ≈ 20 are distilled into Students with four IsoFLOP profiles, and a small number with CS = 3 × 10 21 . The horizontal grey and vertical black dashed lines indicate teacher cross entropy LT and size NT respectively. (b) Students of four sizes trained with a M = DS/NS = 20 are distilled from teachers with four IsoFLOP profiles. Horizontal (vertical) dashed lines indicate student supervised cross entropy LS (student size NS).]()

![Fixed M -Ratio Student/Teacher ISOFlop optima.]()

![Figure 39. ISOFlop optima. a) The optimal student parameters N * S = arg min N S L(NS) that give the lowest student validation loss for each teacher-student combination shown in Figure 38a. The dashed lines correspond to the validation loss of the optimal supervised models trained with the four corresponding compute budget. b) The optimal teacher parameters N * T = arg min N T L(TS) that give the lowest student validation loss for each teacher-student combination shown in Figure 3. The black dashed line correspond to the validation loss of a M = D/N = 20 supervised model of the indicated student size. In both figures, the shaded region corresponds to where weak to strong generalization may occur, as NS > NT (see Appendix E.7).]()

![Figure40. Scaling behavior in the infinite data regime. For the optimal choice of teacher, the loss achieved by all student sizes under distillation is consistent with the loss achievable by supervised learning. This is not true for any choice of teacher, only the optimal one, which can be determined through numerical optimization of the provided distillation scaling laws (see Section 5).]()

![Figure 41. Fixed M-Ratio Teacher varying student data. We look at strong to weak generalization (left) and weak to strong (right) distillation, varying distillation tokens DS ∈ [8B, 512B].]()

![Figure 43. Student calibration (data). Calibration of the student with respect to the actual data labels, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate student overconfidence.]()

![Distillation target: teacher top-1.]()

![Figure 44. Student calibration (teacher top-1). Calibration of the student with respect to the teacher's top 1, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For axis definitions and the figure legend, refer to Figure 42. Blue points above the dashed line indicate the student is underconfident.]()

![Train target: teacher top 1.]()

![Figure 45. Student calibration (teacher distribution). Calibration of the student with respect to the teacher's distribution, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For ECE calculation on the full distribution, see Equation 39. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate student overconfidence, while points above the dashed line indicate underconfidence.]()

![Figure 46. Student calibration (under teacher confidence bins). Calibration of the student with respect to the teacher's confidence bins, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teacher's top-1. For ECE calculation on the full distribution, see Equation 39. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate the teacher is less confident than the student.]()

![Figure 47. Student calibration (data). Calibration of the student with respect to the actual data labels with increased training tokens. Compare to Figure 43 for the effect of tokens and refer to Figure 42 for legend and axis explanations.]()

![Train target: teacher distribution.]()

![Train target: teacher top 1.]()

![Figure 48. Student calibration (teacher top 1). Calibration of the student with respect to the teacher's top 1 when the training tokens have increased. Compare to Figure 44 for the effect of tokens and refer to Figure 42 for legend and axis explanations.]()

![reveals worse calibration uniformly across all confidence bins.Train target: teacher Top-1.]()

![Figure 49. Student calibration (teacher distribution). Calibration of the student with respect to the teacher's distribution as the number of training tokens increases. Compare to Figure 45 for the effect of tokens and refer to Figure 42 for legend and axis explanations.]()

![Figure 50. Student calibration (teacher distribution). Calibration of the student with respect to the teacher' confidence bins distribution as the number of training tokens increases. Compare to Figure 46 for the effect of tokens.]()

![Figure 52. Temperature τ Sensitivity Analysis. Students of four sizes NS ∈ {198M, 546M, 975M, 1.82B} trained with a M = DS/NS = 20 ratio are distilled from teachers of sizes NT ∈ {546M, 1.82B, 4.82B, 7.75B} trained with a M = DT /NT = 20 ratio with different distillation temperatures τ ∈ [0.5, 10].]()

![Figure 53. Learning Rate η Sensitivity Analysis. Students of four sizes NS ∈ {198M, 546M, 975M, 1.82B} trained with a M = DS/NS = 20 ratio are distilled from teachers of sizes NT ∈ {546M, 1.82B, 4.82B, 7.75B} trained with a M = DT /NT = 20 ratio with different learning rates η ∈ [0.001, 0.1].]()

![Figure 54. Distribution truncation analysis. Top-k (left) and Top-p (right) truncation of teacher logits z (i) T for student-teacher pairs with NS in {198M, 546M, 1.82B} and corresponding NT in {7.75B, 1.82B, 546M}. Standard truncation degrades performance: at k = 128, validation loss increases by 0.11 nats compared to full distillation (k = 32768), while Top-p with p = 0.9 degrades by 0.13 nats versus p = 1.0. Using λ = 0.7 with k = 128 maintains performance within 0.01 nats while enabling efficient post-hoc training.]()

![The set defintions S M for Top-k and Top-p areS k (p) = Top(p, k), S p (p) = {a : b∈sort↓( p,a)p ≤ p}.]()

![Expressions related to scaling laws used in this work. In each case, S always refers to student and not supervised. NS / NT The number of model/student/teacher non-embedding parameters. Whenever we mention parameters in text, we always mean non-embedding parameters unless explicitly stated otherwise. See Appendix H.2 for more details. D / DT The number of tokens the model/teacher is pretrained on.]()

![Scenarios considered in our scaling law applications.]()

![Optimal compute allocation trends.]()

![Fixed compute distillation strategy. The student performance obtained for four total compute budgets C Total ∈ {10 21 , 10 22 , 10 23 , 10 24 } FLOPs and four student sizes NS ∈ {1B, 3B, 10B, 30B} under a teacher of size NT ∈ [1B, 1T ] and teacher loss LT ∈ [E, 2.5]. The red line indicates the value of teacher loss L * T (NT ) that results in the best student performance for each teacher size NT . Scenarios considered in our scaling law applications. Same as Table2.The solutions resulting in the losses give guidance on how to scale depending on the use case, and are the result of constrained optimization]()

![Forward vs Reverse KL Divergence for NT = 1.82B to NS = 546M distillation. Reverse KL is slightly more expensive with respect to vocabulary size V due to the entropy calculation.]()

![The notation we use for parameter and FLOPs estimation. Group size in Group Query Attention (GQA) nheads/nkv-heads gsize Model aspect ratio dmodel/nlayers ρmodel Feed-forward ratio dffn/dmodel ρffn H.1. Alternative approximation for FLOPs per token as a function of N From Table 10 and Equation 71 and Table 12 we can read our approximate values for non-embedding parameters and total compute (dropping contributions from normalization layers) as 8 ffn ρ ffn + 2n layers n ctx d model (58) = 2N + 2n layers n ctx d model + 2n vocab d model .]()

Apple

University of Oxford, UK. Work done during an internship at Apple. For a full breakdown of contributions see Appendix J. Correspondence to: Dan Busbridge <dbusbridge@apple.com>.

[Hoffmann et al. (2022)](#b26) use γ = 1 whereas[Kaplan et al. (2020)](#b54) use β = 1. We observe a significantly better fit and extrapolation without coefficient tying, which may be due to our use of Maximal Update Parameterization (µP) (see Section 4.1).

We do not write this as z(<i)  to avoid confusion with the sequence z (<i) = (z (1) , . . . , z (i-1) ).

We generally refer to these as fixed-m models rather than Chinchilla-optimal models as we do not yet know whether M ≈ 20 is a good choice in this specific setting.

Appendix G.4 evaluates distribution truncation via Top-p and Top-k to mitigate the overhead of computing these logits online.

The level of compute at which this happens is larger for larger models, see Figure17for specific values.

µP only guarantees learning rate optimality when varying widths. Empirically, the learning rate is also stable when changing the model depth within a reasonable range[(Yang et al., 2022)](#b31). To guarantee transfer across model depths one can additionally employ depth-µP[(Yang et al., 2024)](#b108), although we do not use depth-µP here.

