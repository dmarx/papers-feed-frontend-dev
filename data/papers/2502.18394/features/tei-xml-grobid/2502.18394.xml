<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The FFT Strikes Back: An Efficient Alternative to Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jacob</forename><surname>Fein-Ashley</surname></persName>
							<email>feinashl@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The FFT Strikes Back: An Efficient Alternative to Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">29E6891C748765DB93303A7DF6488337</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-26T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional self-attention mechanisms incur quadratic complexity, limiting their scalability on long sequences. We introduce FFTNet, an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to achieve global token mixing in O(n log n) time. By transforming inputs into the frequency domain, FFTNet exploits the orthogonality and energy preservation guaranteed by Parseval's theorem to capture long-range dependencies efficiently. A learnable spectral filter and modReLU activation dynamically emphasize salient frequency components, providing a rigorous and adaptive alternative to traditional self-attention. Experiments on the Long Range Arena and ImageNet benchmarks validate our theoretical insights and demonstrate superior performance over fixed Fourier and standard attention models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conventional self-attention mechanisms capture global interactions through explicit pairwise computations, which results in a quadratic computational complexity that can be prohibitive for long sequences. In contrast, our work introduces an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to perform global token mixing with a mathematically elegant and scalable approach.</p><p>Our method begins by transforming the input sequence into the frequency domain, where orthogonal frequency components naturally encode long-range dependencies. This not only reduces the computational complexity to O(n log n) but also preserves the energy of the original signal, as ensured by Parseval's theorem. Such a transformation facilitates efficient global interactions without the need for exhaustive pairwise comparisons.</p><p>A central innovation of our framework is the integration of a learnable spectral filter. This adaptive component modulates the Fourier coefficients based on a global context vector, enabling the model to dynamically emphasize salient frequency bands that are critical for capturing complex patterns. Furthermore, the application of nonlinear activations to both the real and imaginary parts of the filtered signal enhances the model's expressivity, allowing it to represent higher-order interactions that go beyond the scope of linear transformations.</p><p>In essence, our adaptive spectral filtering framework combines the computational efficiency of FFT-based transformations with adaptive, context-sensitive filtering and nonlinear processing. This synthesis offers a rigorous and expressive alternative to traditional self-attention, providing a robust solution for modeling long-range dependencies in sequence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review existing methods aimed at improving the efficiency of sequence models. We first discuss the complexity issues inherent in self-attention (Section 2.1), then highlight Fourierbased approaches (Section 2.2) and other approximation techniques (Section 2.3). We then examine orthogonal matrix decomposition methods (Section 2.4), and finally position our adaptive spectral filtering method within this landscape (Section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-Attention Complexity</head><p>The original Transformer architecture <ref type="bibr" target="#b10">Vaswani et al. [2017]</ref> uses pairwise dot-product attention, incurring a computational and memory cost of O(n 2 ), where n is the sequence length. As n grows, this quadratic complexity quickly becomes infeasible for long sequences in tasks such as language modeling and long-context document understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fourier-Based Mixing</head><p>Fourier-based approaches leverage the Fast Fourier Transform (FFT) <ref type="bibr" target="#b2">Cooley and Tukey [1965]</ref> to achieve more efficient global mixing of tokens. FNet <ref type="bibr" target="#b5">Lee-Thorp et al. [2022]</ref>, for example, replaces the self-attention sublayer with a fixed Fourier transform, drastically lowering computational overhead. However, the use of a static transform limits its capacity to adapt to varying inputs or highlight task-specific frequency components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linear, Sparse, and Low-Rank Approximations</head><p>Beyond Fourier methods, several alternative strategies aim to reduce the cost of self-attention. Performer <ref type="bibr" target="#b1">Choromanski et al. [2021]</ref> and linear transformer variants <ref type="bibr" target="#b3">Katharopoulos et al. [2020]</ref> approximate the softmax attention matrix to achieve linear or near-linear complexity. Meanwhile, Reformer <ref type="bibr" target="#b4">Kitaev et al. [2020]</ref>, Linformer <ref type="bibr" target="#b11">Wang et al. [2020]</ref>, and BigBird <ref type="bibr" target="#b13">Zaheer et al. [2020]</ref> employ sparse or low-rank approximations, extending the effective context length without paying the full quadratic price. Other approaches like Synthesizer <ref type="bibr" target="#b7">Tay et al. [2020]</ref> and MLP-Mixer <ref type="bibr" target="#b9">Tolstikhin et al. [2021]</ref> avoid explicit token-pair interactions, replacing them with fixed or learned mixing operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Orthogonal Matrix Decomposition Methods</head><p>Orthogonal (or unitary) transformations provide a powerful avenue for stable and efficient sequence modeling. A key advantage of orthogonal decompositions is their norm-preserving property, which can mitigate issues such as vanishing or exploding gradients <ref type="bibr" target="#b12">Wisdom et al. [2016]</ref>. In the context of RNNs, unitary or orthonormal recurrent weights have been shown to preserve long-term dependencies while keeping representations stable <ref type="bibr" target="#b0">Arjovsky et al. [2016]</ref>, <ref type="bibr" target="#b6">Lezcano-Casado and Martínez-Rubio [2019]</ref>. From another perspective, the discrete Fourier transform (DFT) itself is an orthonormal transformation (up to scaling) that can mix tokens globally without explicit pairwise attention.</p><p>The FFT-based approaches discussed above can be viewed as a special class of such orthonormal transforms, where the matrix is structured by the DFT. More general orthogonal transformationswhether learned or hand-crafted-have also been proposed to reduce complexity or enhance stability in modern architectures. These include fast variants of orthonormal transforms, often parameterized in ways that ensure orthogonality is preserved throughout training <ref type="bibr" target="#b6">Lezcano-Casado and Martínez-Rubio [2019]</ref>. Within Transformers, adopting orthogonal or unitary blocks has been explored to stabilize training and capture global structure, although these methods may not always achieve the same O(n log n) cost as the FFT. Nonetheless, they highlight a broad paradigm wherein structured or parameterized orthonormal decompositions serve as efficient global mixing mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Adaptive Spectral Filtering in Context</head><p>Our work diverges from both fixed Fourier-based schemes and the various attention approximations by incorporating a learnable filter in the frequency domain. This adaptive mechanism leverages the theoretical underpinnings of FFT-based transformations-including energy preservation via Parseval's theorem-while permitting dynamic reweighting of salient frequency bands. Thus, our method maintains an O(n log n) complexity yet provides richer expressivity than fixed spectral mixing approaches. In contrast to purely approximate or sparse attention mechanisms, adaptive spectral filtering offers a direct and theoretically grounded route to capture long-range dependencies efficiently.</p><p>Overall, while approaches such as FNet, Performer, and sparse transformers demonstrate that either fixed or approximate token mixing can reduce computational overhead, our adaptive spectral filtering strategy uniquely merges the efficiency of the FFT with a learnable, input-dependent spectral filter. This provides a compelling combination of scalability and adaptability, which is crucial for complex sequence modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive Spectral Filtering Method</head><p>In this section, we present an adaptive spectral filtering framework that eliminates the need for explicit pairwise interactions in global token mixing. Instead of relying on dot-product-based self-attention, we employ the discrete Fourier transform (DFT) to capture long-range dependencies efficiently. By adaptively modulating the resulting frequency components and then applying an inverse transform, our method strikes an effective balance between expressive power and computational cost, supported by strong theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Standard attention mechanisms compute pairwise interactions between tokens, incurring a quadratic cost in sequence length. As the number of tokens grows, this approach rapidly becomes prohibitive. In contrast, the Fourier transform decomposes a sequence into frequency components, inherently encoding global interactions in O(n log n) time. To enhance representational power, we introduce a learnable filter in the frequency domain, enabling the model to emphasize salient frequency bands while retaining computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method Description</head><p>Let X ∈ R n×d be the input sequence of length n and embedding dimension d. Our method comprises four steps:</p><p>1. Fourier Transform. We first apply the discrete Fourier transform along the token dimension:</p><formula xml:id="formula_0">F = FFT(X) ∈ C n×d .</formula><p>This operation represents each embedding across orthogonal frequency components, enabling global interactions without explicit pairwise comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Adaptive Spectral Filtering.</head><p>To selectively emphasize important frequencies, we use a learnable filter. First, compute a global context vector:</p><formula xml:id="formula_1">c = 1 n n i=1 X i ,</formula><p>and pass it through a multi-layer perceptron (MLP) to obtain a modulation tensor:</p><formula xml:id="formula_2">∆W = MLP(c) ∈ R n×d .</formula><p>We define the final filter as</p><formula xml:id="formula_3">W = W base + ∆W,</formula><p>where W base is a fixed base filter (often initialized to all ones). The adaptive filtering step is then</p><formula xml:id="formula_4">F = F ⊙ W,</formula><p>which reweights the Fourier coefficients element-wise according to the global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Nonlinear Activation (modReLU).</head><p>To capture higher-order relationships in the complex frequency domain, we apply the modReLU <ref type="bibr" target="#b0">Arjovsky et al. [2016]</ref> activation, defined for a complex number z = re iθ (with r = |z| and θ = arg(z)) as:</p><formula xml:id="formula_5">modReLU(z) = (r + b) e iθ , if r + b &gt; 0, 0, otherwise,</formula><p>where b is a learnable bias. This operation applies a ReLU-like threshold to the magnitude while preserving the phase, making it well suited for frequency-domain representations. The element-wise modReLU is then:</p><formula xml:id="formula_6">F = modReLU F .</formula><p>4. Inverse Fourier Transform. Finally, we return to the token domain via the inverse Fourier transform, retaining the real component of the reconstructed signal:</p><formula xml:id="formula_7">Y = IFFT F ∈ R n×d .</formula><p>This yields a globally mixed representation that incorporates adaptive filtering and nonlinear transformations in the frequency domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical Justification for FFT over Self-Attention</head><p>Our FFT-based adaptive spectral filtering approach offers several advantages over conventional self-attention:</p><p>• Efficient Global Mixing: By decomposing inputs into frequency components, the FFT provides global interactions in O(n log n) time-much more scalable than the O(n 2 ) of self-attention. • Implicit, Adaptive Attention: The adaptive spectral filter effectively learns a frequencydomain mask informed by the global context vector. This mask reweights crucial frequency bands similarly to attention weights but avoids explicit pairwise computations. • Greater Expressivity via Nonlinearity: While the Fourier transform itself is linear, applying modReLU to the complex coefficients enriches representational capacity. This allows the model to capture intricate, higher-order patterns that purely linear operations may miss. • Energy Preservation and Stability: Parseval's theorem ensures the input signal's norm (energy) is preserved by the FFT. Thus, our method maintains stability by avoiding accidental loss of crucial information.</p><p>By leveraging frequency-domain global mixing, learnable spectral filtering, and the modReLU nonlinear activation, our method is both theoretically grounded and computationally efficient, serving as a robust alternative to self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computational Complexity</head><p>The central operations in our method are the Fast Fourier Transform (FFT) and its inverse, each costing O(n log n) per channel. In contrast, self-attention requires O(n 2 ) time for pairwise computations.</p><p>The adaptive filtering and activation steps introduce only a linear overhead O(n), keeping the overall complexity at O(n log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary</head><p>In essence, adaptive spectral filtering transforms the input into the frequency domain, applies a learnable nonlinear modulation (modReLU), and then inverts the transform to obtain a globally mixed representation. This provides:</p><p>• A favorable O(n log n) complexity.</p><p>• Robust mechanisms for modeling long-range dependencies.</p><p>• An efficient, expressive alternative to self-attention.</p><p>The code is available here: <ref type="url" target="https://github.com/jacobfa/fft">https://github.com/jacobfa/fft</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Proof of Computational Complexity</head><p>Fourier and Inverse Fourier Transforms. For X ∈ R n×d , computing the FFT and IFFT requires O(n log n) operations per channel, totaling O(d • n log n).</p><p>Adaptive Spectral Filtering.</p><p>• Summation over n tokens for the context vector c takes O(n).</p><p>• Computing ∆W from c via a small MLP adds O(1) per channel.</p><p>• Element-wise filtering on F is O(n) per channel.</p><p>Overall, filtering costs O(d • n).</p><p>Nonlinear Activation.</p><p>Applying modReLU on all complex elements is O(n) per channel, or O(d • n) in total. Overall Complexity. Since O(d • n log n) (for FFT/IFFT) dominates O(d • n) (for filtering and activation), the total cost is O(d • n log n). For most practical settings where d is not far larger than n, this effectively behaves as O(n log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Proofs and Theoretical Guarantees</head><p>Below, we show key theoretical properties that justify our method as an efficient surrogate for self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.1">Global Mixing via Orthogonal Decomposition</head><p>Let X ∈ R n×d be the input and F = FFT(X). A unitary DFT matrix F n ∈ C n×n satisfies F * n F n = nI, where F * n is the conjugate transpose and I is the identity. This orthogonality preserves inner products:</p><formula xml:id="formula_8">⟨X i , X j ⟩ = 1 n ⟨F i , F j ⟩,</formula><p>implying that each token influences every frequency component. Consequently, the transformation encodes global interactions much like self-attention, without O(n 2 ) pairwise computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.2">Energy Preservation via Parseval's Theorem</head><p>Parseval's theorem states:</p><formula xml:id="formula_9">∥X∥ 2 2 = 1 n ∥F∥ 2 2 .</formula><p>After adaptive filtering and activation, the output is</p><formula xml:id="formula_10">Y = IFFT modReLU F ⊙ W .</formula><p>Provided the filter does not excessively amplify particular frequencies, the energy of Y remains close to that of X, ensuring that key information is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.3">Approximation of Self-Attention Mechanism</head><p>Self-attention can be seen as a weighted sum of tokens via</p><formula xml:id="formula_11">A ij = exp(q ⊤ i k j ) n j=1 exp(q ⊤ i k j )</formula><p>, leading to Y attn = A X. Our method replaces explicit pairwise interactions with frequency-domain multiplication. By the convolution theorem, this frequency-domain multiplication is equivalent to a convolution in the token domain. Under mild conditions on W, this global, data-dependent convolution can approximate self-attention at significantly lower computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.4">Role of Nonlinear Activation in Enhancing Expressivity</head><p>While the Fourier transform is inherently linear, real-world data often exhibit nonlinear patterns. By using modReLU directly on the complex coefficients, we capture higher-order interactions that would otherwise necessitate more complex mechanisms in the token domain. The phase is preserved, and a threshold is applied to the magnitude, helping the model learn highly expressive frequency-based features.</p><p>3.8 Proof of Expressivity: FFT as an Approximation of Self-Attention Theorem 1. Let X ∈ R n×d be an input sequence, and let the self-attention mechanism be</p><formula xml:id="formula_12">Y attn = AX, where A ij = exp(q ⊤ i k j ) n j=1 exp(q ⊤ i k j )</formula><p>.</p><p>Consider the adaptive spectral filtering operation</p><formula xml:id="formula_13">Y = IFFT modReLU FFT(X) ⊙ W ,</formula><p>where W is a learnable frequency-domain filter and modReLU is a complex-valued nonlinear activation. Then, under mild regularity conditions on W and the activation, there exists a parameterization such that Y ≈ Y attn , and the presence of the nonlinear activation extends the expressive capacity beyond that of purely linear self-attention.</p><p>Proof. We outline the argument in several steps:</p><p>1) Unitary Transformation and Energy Preservation. The DFT matrix F n satisfies the unitary property (up to scaling), F * n F n = nI, implying Parseval's theorem, which preserves the norm of X across the frequency transform.</p><p>2) Frequency-Domain Filtering and Convolution Equivalence. The element-wise multiplication FFT(X) ⊙ W corresponds to a convolution in the token domain due to the convolution theorem. Defining w = IFFT(W), we get</p><formula xml:id="formula_14">IFFT FFT(X) ⊙ W = X * w.</formula><p>3) Approximating Self-Attention via Convolution Kernels. Self-attention can be interpreted as a learnable, global aggregation function. Convolution kernels, particularly those conditioned on input features, can approximate a wide variety of such functions. Hence, suitable choices of W allow X * w to approximate Y attn . 4) Nonlinear Activation for Enhanced Expressivity. Because data often require nonlinear modeling, we use modReLU on the complex coefficients:</p><formula xml:id="formula_15">z = re iθ → modReLU(z) = (r + b)e iθ , if r + b &gt; 0, 0, otherwise.</formula><p>This nonlinearity, when applied element-wise in the frequency domain, enriches the effective convolution kernel beyond what a purely linear approach can achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Conclusion</head><p>. By choosing W and the bias b in modReLU appropriately, we can approximate the self-attention operation with an FFT-based, O(n log n) method while also harnessing the additional expressive potential afforded by nonlinear activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Comparison with FNet</head><p>FNet <ref type="bibr" target="#b5">Lee-Thorp et al. [2022]</ref> also employs the DFT to mix tokens, removing all learnable parameters for mixing. However, it lacks adaptation to specific input distributions. Our method departs from that approach in key ways:</p><p>• Adaptive Filtering: Instead of relying on a fixed DFT, we introduce a learnable filter conditioned on a global context vector, enabling input-dependent emphasis on certain frequencies. • Nonlinear Activation: We incorporate a complex-domain activation (modReLU) to capture higher-order phenomena that lie beyond the scope of linear transforms. • Strong Theoretical Basis: Our analysis uses energy preservation (Parseval's theorem) and the convolution theorem to formally relate frequency-domain multiplication to global token mixing. • Practical Scalability: Although both methods achieve O(n log n) complexity, our adaptive filtering and nonlinear activation introduce minimal overhead and substantially boost expressivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.1">Computational Efficiency</head><p>Both FFT and IFFT run in O(n log n) per channel. Our adaptive filtering and modReLU steps each add only O(n), so the overall cost remains dominated by the FFT and IFFT. This results in an efficient, scalable method for handling long sequences, standing in contrast to O(n 2 ) self-attention mechanisms.</p><p>In summary, adaptive spectral filtering:</p><p>• Achieves global token mixing via the Fourier transform's orthogonality.</p><p>• Preserves signal energy for stable representations.</p><p>• Approximates self-attention through a frequency-domain convolution.</p><p>• Employs modReLU for higher-order interactions in the complex domain.</p><p>• Operates in O(n log n) time, offering a significant advantage over O(n 2 ) attention-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed method, FFTNet, comparing it to FNet and standard self-attention-based Transformers. We present results on the Long Range Arena (LRA) benchmark <ref type="bibr" target="#b8">Tay et al. [2021]</ref> and the ImageNet classification task. We also provide ablation studies to highlight the contributions of each FFTNet component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Long Range Arena (LRA) Benchmark</head><p>We evaluate on six tasks in LRA: ListOps, Text, Retrieval, Image, Pathfinder, and Path-X. Table <ref type="table" target="#tab_1">1</ref> reports the accuracy (%) on each task, as well as the average performance across all tasks. Our FFTNet model achieves higher accuracy on most tasks, including a 37.65% accuracy on ListOps (compared to 36.06% for the standard Transformer and 35.33% for FNet). Overall, FFTNet slightly outperforms both baselines on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageNet Classification</head><p>Next, we evaluate our FFTNetViT variants on the ImageNet classification task, comparing them to standard ViT (self-attention). Table 2: Comparison of FFTNetViT vs. Standard ViT on ImageNet, grouped by variant. FLOPs are in GFLOPs and Params in millions. FFTNetViT scales fastear than standard self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We further investigate the contributions of individual FFTNet components by conducting ablation experiments on the ImageNet classification task (Base variant). In particular, we evaluate the following variants:</p><p>• FFTNet (full): The complete model with spectral gating, the adaptive module, and FFTbased filtering.</p><p>• FFTNet without spectral gating: The spectral gating mechanism is removed.</p><p>• FFTNet without adaptive module: The adaptive module is omitted.</p><p>• FFTNet with convolutional replacement: The FFT layer is replaced with a standard convolutional layer. Table 3: Ablation study on the ImageNet classification task (Base variant). Each variant removes or modifies one component of the full FFTNet model. Overall, these experiments indicate:</p><p>• FFTNet surpasses FNet on both LRA and ImageNet, demonstrating improved accuracy and efficiency.</p><p>• Compared to standard self-attention, FFTNetViT often has lower FLOPs for similar or better performance.</p><p>• Ablations confirm the importance of each FFTNet component (spectral gating, adaptive module).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented FFTNet, a novel approach that overcomes the inherent limitations of self-attention through adaptive spectral filtering. Our method transforms inputs into the frequency domain, leveraging Fourier theory to ensure energy preservation and efficient global mixing. The integration of a learnable spectral filter with a modReLU activation allows the model to dynamically focus on critical frequency bands, reducing complexity to O(n log n) while maintaining expressive power. Extensive evaluations on LRA and ImageNet confirm that FFTNet not only achieves competitive accuracy but also significantly improves computational efficiency compared to both fixed Fourier approaches and standard self-attention. These results underscore the potential of merging rigorous theoretical foundations with adaptive learning strategies for scalable sequence modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Latency comparison of FFTNetViT vs. Standard ViT for varying batch sizes on ImageNet. FFTNetViT scales fastear than standard self-attention.</figDesc><graphic coords="8,187.20,343.51,237.60,184.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Table2presents the FLOPs, parameter counts, and both Top-1 and Top-5 accuracy for each variant (Base, Large, Huge). We omit latency from the table for clarity and show it in a separate figure. Notably, FFTNetViT often achieves lower FLOPs than ViT for comparable model sizes, while maintaining strong accuracy. In most experiments, FNet lags behind Accuracy (%) on the Long Range Arena (LRA) benchmark. OOM indicates out-of-memory and FAIL indicates the model could not process the dataset. Our FFTNet obtains the best average accuracy.both methods on ImageNet, so we focus on comparing our approach to the stronger self-attention baseline.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="7">ListOps Text Retrieval Image Pathfinder Path-X Avg.</cell></row><row><cell cols="2">Transformer</cell><cell>36.06</cell><cell>61.54</cell><cell>59.67</cell><cell>41.51</cell><cell>80.38</cell><cell>OOM</cell><cell>55.83</cell></row><row><cell>FNet</cell><cell></cell><cell>35.33</cell><cell>65.11</cell><cell>59.61</cell><cell>38.67</cell><cell>77.80</cell><cell>FAIL</cell><cell>55.32</cell></row><row><cell cols="2">FFTNet (ours)</cell><cell>37.65</cell><cell>66.01</cell><cell>60.21</cell><cell>42.02</cell><cell>80.71</cell><cell>83.25</cell><cell>58.31</cell></row><row><cell>Variant</cell><cell></cell><cell cols="2">FFTNetViT</cell><cell></cell><cell></cell><cell></cell><cell>ViT</cell><cell></cell></row><row><cell></cell><cell>FLOPs</cell><cell>Params</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell><cell>FLOPs</cell><cell>Params</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell>Base</cell><cell>22.64</cell><cell>76.33</cell><cell>79.6</cell><cell>94.9</cell><cell>36.65</cell><cell>86.57</cell><cell>79.4</cell><cell>94.8</cell></row><row><cell>Large</cell><cell>79.92</cell><cell>267.89</cell><cell>82.1</cell><cell>96.2</cell><cell>127.18</cell><cell>304.33</cell><cell>81.8</cell><cell>96.0</cell></row><row><cell>Huge</cell><cell>166.14</cell><cell>539.96</cell><cell>83.2</cell><cell>96.8</cell><cell>261.39</cell><cell>632.20</cell><cell>82.9</cell><cell>96.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>reports the Top-1 accuracy for each variant. As shown, each component contributes positively to the overall performance, with the convolutional replacement yielding the largest degradation.</figDesc><table><row><cell>Variant</cell><cell cols="2">Top-1 Acc (%) Observation</cell></row><row><cell>FFTNet (full)</cell><cell>79.6</cell><cell>Full model with spectral gating, adap-</cell></row><row><cell></cell><cell></cell><cell>tive module, and FFT-based filtering.</cell></row><row><cell>-without spectral gating</cell><cell>78.3</cell><cell>Removing the spectral gating mecha-</cell></row><row><cell></cell><cell></cell><cell>nism leads to a noticeable accuracy</cell></row><row><cell></cell><cell></cell><cell>drop.</cell></row><row><cell>-without adaptive module</cell><cell>77.8</cell><cell>Omitting the adaptive module further</cell></row><row><cell></cell><cell></cell><cell>reduces performance.</cell></row><row><cell>-FFT replaced with convolution</cell><cell>77.2</cell><cell>Replacing the FFT layer with a convolu-</cell></row><row><cell></cell><cell></cell><cell>tion results in the largest degradation.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.06464" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fnet: Mixing tokens with fourier transforms</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105.03824" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Casado</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Martínez-Rubio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08428</idno>
		<title level="m">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.00743" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scott T Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Les</forename><forename type="middle">E</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><surname>Atlas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00035</idno>
		<title level="m">Full-capacity unitary recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
