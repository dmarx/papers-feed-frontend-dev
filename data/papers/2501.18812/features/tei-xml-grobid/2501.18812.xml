<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating the Probability of Sampling a Trained Neural Network at Random</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-31">31 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adam</forename><surname>Scherlis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nora</forename><surname>Belrose</surname></persName>
						</author>
						<title level="a" type="main">Estimating the Probability of Sampling a Trained Neural Network at Random</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-31">31 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">1F13C6DF62B54EE27932E903654CAAD4</idno>
					<idno type="arXiv">arXiv:2501.18812v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There is a long line of research which finds that flat minima in a neural network parameter space, defined as weight vectors surrounded by large regions "with the property that each weight vector from that region leads to similar small error" generalize better than sharp minima <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997)</ref>. While there are counterexamples to this tendency <ref type="bibr" target="#b5">(Dinh et al., 2017)</ref>, it seems to be empirically and theoretically fairly robust, and has inspired the development of optimizers that explicitly search for flatter minima <ref type="bibr">(Foret et al., 2021)</ref>.</p><p>In a related line of work, <ref type="bibr" target="#b4">Chiang et al. (2022)</ref> put forward the volume hypothesis, which states that "...the implicit bias of neural networks may arise from the volume disparity of different basins in the loss landscape, with good hypothesis classes occupying larger volumes." They evaluate simple gradient-free learning algorithms, such as the "Guess &amp; Check" optimizer which randomly samples parameters until it stumbles upon a network that achieves training loss under some threshold, and find that these methods have similar generalization behavior to gradient descent, at least on the very simple tasks they tested. <ref type="bibr" target="#b17">Teney et al. (2024)</ref> find that randomly initialized networks represent very simple functions, which would explain the simplicity bias of deep learning if SGD behaves similarly to Guess &amp; Check.</p><p>Additionally, <ref type="bibr" target="#b16">Mingard et al. (2021)</ref> provide evidence that SGD may be an approximate Bayesian sampler, where the prior distribution over functions is equal to the distribution over functions represented by randomly initialized networks. Since networks are usually initialized using a uniform or Gaussian distribution, the Bayesian sampling hypothesis makes similar predictions to the volume hypothesis. Finally, recent work suggests that singular learning theory <ref type="bibr" target="#b19">(Watanabe, 2009)</ref>, originally developed to analyze the learning dynamics of overparameterized Bayesian models, can be profitably used to understand deep learning <ref type="bibr" target="#b10">(Hoogland et al., 2024;</ref><ref type="bibr" target="#b13">Lau et al., 2024)</ref>.</p><p>In this work, we propose an efficient algorithm for estimating the probability that a network from some behaviorallydefined region would be sampled from a Gaussian or uniform prior. This is equivalent to the volume of the region, if the prior is uniform. We define the local volume of a weight vector θ ∈ R N relative to a cost function C : R N → [0, ∞), threshold ϵ &gt; 0, and measure µ to be the size according to µ of the star domain S anchored at θ containing points θ ′ such that C(θ ′ ) &lt; ϵ, which we call the neighborhood of θ. This is equivalent to the probability of sampling a network inside S from a prior proportional to the measure µ. While prior work has assumed µ to be the Lebesgue measure (i.e. volume), we also consider the probability measure used to initialize the network before training, which guarantees that the measure of any neighborhood must be finite. We find empirically that some realworld neighborhood actually have infinite Lebesgue volume, creating difficulties for analysis.</p><p>We use the term "cost" rather than "loss" intentionally be-cause we are interested in a broader class of functions than just the loss function used to train the model. In particular, for networks that output a predictive probability distribution we find it useful to consider the expected KL cost function E x [D KL (f (x; θ)||f (x; θ ′ )], which measures how behaviorally different θ ′ is from θ, independent of any ground truth labels. This cost also has the benefit that it is always zero at the minimum point θ ′ = θ.</p><p>We estimate that the probability of randomly sampling the trained Pythia 31M language model from its initialization distribution, within an accuracy of 0.01 nats, is about Pr(language model) ≈ 1 10 3.6×10 8</p><p>(1) or one in 1 followed by 360 million zeros. For comparison, there are about 10 80 atoms in the observable universe, so this is about the same as the probability of correctly guessing a specific atom 4.5 million times in a row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>As mentioned in the introduction, one major motivation behind this work is to test the volume hypothesis: the idea that the relative volumes of different regions in parameter space strongly shape the kinds of networks that are produced by gradient descent. There are many variants of this hypothesis, and we detail two of them below.</p><p>The Bayesian volume hypothesis. Here is a simplistic version of the volume hypothesis which we think is likely false for real-world neural networks, but which may be helpful as an intuition pump.</p><p>Consider Bayesian inference with prior density ρ(θ) and likelihood function -L. The posterior distribution is proportional to ρ(θ) exp(-L(θ)). Since many neural-network losses can be interpreted as negative log-likelihoods, we can think of L as the loss function of a neural net and ρ as a prior related to initialization and regularization of the network. If neural-net training were perfectly Bayesian, the probability density for obtaining some parameter θ from training would depend only on the prior and the loss. This is of interest because it attributes generalization entirely to the architecture and loss function: under this hypothesis, the only way for one low-training-loss solution to be favored over another is if it simply occupies more of parameter space. In effect, the architecture imposes a sophisticated inductive prior (on top of the simple prior ρ) by overrepresenting "good", well-generalizing functions, and underrepresenting "bad" ones.</p><p>This hypothesis is true for stochastic gradient Langevin dynamics <ref type="bibr" target="#b20">(Welling &amp; Teh, 2011)</ref>, which is an efficient Bayes sampler for deep neural networks, but only with unrealisti-cally long mixing times.</p><p>Quadratic toy model. Consider a quadratic loss function with Hessian H. If the initialization distribution µ 0 has covariance matrix I, then at timestep t the covariance is exp(-Ht) exp(-Ht) T . Assuming µ 0 is a zero-mean Gaussian, the log density of parameters θ at time t is proportional to θ exp(2Ht)θ T , which is in general not proportional to the loss<ref type="foot" target="#foot_0">foot_0</ref> 2 θHθ T . The probability mass becomes concentrated along directions of higher curvature (larger Hessian eigenvalues) exponentially faster than along directions of lower curvature.</p><p>If we introduce isotropic noise and solve the resulting Fokker-Planck equation, it can be shown that the logdensity instead converges to something proportional to the loss. However, if the noise is not isotropic -in particular if it is stronger in more steeply-curved directions, as is true in practice -then this fails <ref type="bibr" target="#b14">(Mandt et al., 2018)</ref>. This shows that, within basins of non-isotropic curvature, the posterior density of popular optimizers does not satisfy the Bayesian volume hypothesis. We can, however, restrict the hypothesis to apply only between different basins.</p><p>The basin volume hypothesis. Let λ be the Lebesgue measure on R N , and let µ 0 be the probability measure on R N from which the initial network parameters θ 0 are sampled, usually a uniform distribution on a compact set or a Gaussian. Let µ t be the distribution over network parameters at timestep t in training, and let f t (x) = dµt dµ0 be the probability density of parameters x at time t. 1 We can decompose the posterior probability of behaviorally distinct regions of parameter space, such as basins of low loss with differing degrees of generalization, as follows.</p><p>Let A ⊂ R N and B ⊂ R N be two disjoint regions of parameter space, perhaps defined by their performance on a held-out test set. The probability that training will yield an element of A can be decomposed as</p><formula xml:id="formula_0">log P(θ ∈ A) = log µ 0 (A) • 1 µ 0 (A) A f t dµ 0 (2) = log µ 0 (A) volume + log E x∼Unif(A) f t (x)</formula><p>mean density</p><p>(3) and the log probability ratio is</p><formula xml:id="formula_1">log P(θ ∈ A) P(θ ∈ B) = log µ 0 (A) µ 0 (B) volume ratio + log E x∼µ0|A f t (x) E x∼µ0|B f t (x) density ratio ,<label>(4)</label></formula><p>where µ 0 |A denotes the restriction of µ 0 to A. 2 Note that at t = 0 we have f 0 (x) = dµ0 dµ0 (x) = 1 for any x, so that at early times t the density ratio term in Eq. 4 should be small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A restricted form of the volume hypothesis is as follows:</head><p>Even at the end of training, the volume ratio term in Eq.4 should be larger than the density ratio term for suitable choices of A and B.</p><p>Of course, if the networks in A and the networks in B differ significantly in terms of their performance on the training set, the density ratio term must become very large as t → ∞, since a well-tuned optimizer is guaranteed to bring the loss close to a local minimum. When analyzing generalization, then, we should select A and B such that are both contained in a low-loss manifold <ref type="bibr" target="#b2">(Benton et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Minimum description length</head><p>Basin volume can be connected directly to generalization using the notion of minimum description length (MDL). The idea is that a statistical model is more likely to generalize if it compresses its training data effectively, while not being too complex itself. Since we are assuming that all networks in the neighborhood perform similarly, we will treat the neighborhood itself as an ensemble over networks, and use it as our statistical model. In Bayesian terms, our posterior is a uniform distribution over the neighborhood, and we assume that our receiver is using the initialization distribution µ 0 as a prior. The bits-back argument <ref type="bibr" target="#b8">(Hinton &amp; Van Camp, 1993)</ref> shows that the MDL of this model plus the training data</p><formula xml:id="formula_2">x 1:n is KL Unif(A)||µ 0 + E θ∼Unif(A) n i=i log 2 p θ (x i ) , (5)</formula><p>where A ⊂ R N is the neighborhood, and p θ (x i ) is the probability that the network with parameters θ assigns to datapoint x i .</p><p>In practice, µ 0 is either a uniform distribution over a simple polytope S ⊂ R N , or a (possibly truncated) Gaussian N (0, Σ) with diagonal covariance. In the former case, the KL term simplifies to log λ(S)-log λ(A), and in the latter, it simplifies to</p><formula xml:id="formula_3">n 2 log(2π)+ 1 2 log |Σ|+ 1 2 E θ∼Unif(A) [θ T Σ -1 θ]-log λ(A),</formula><p>which only depends on A is through its volume and its mean Mahalanobis distance from the origin. Neighborhoods with large Lebesgue volume and small average Mahalanobis norm will have lower description length than neighborhoods with smaller volume or higher Mahalanobis norm.</p><p>then µ0|A = Unif(A ∩ S). If µ0 is a Gaussian, then µ0|A is a truncated Gaussian with support A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Singular Learning Theory and the Local Learning Coefficient</head><p>The local learning coefficient (LLC) was introduced by Lau et al. ( <ref type="formula">2024</ref>), extending concepts from singular learning theory <ref type="bibr" target="#b19">(Watanabe, 2009)</ref>, and has proved to be useful as a measure of the complexity of neural networks and their components <ref type="bibr" target="#b10">(Hoogland et al., 2024;</ref><ref type="bibr" target="#b18">Wang et al., 2024)</ref>.</p><p>Consider a local minimum θ * in the loss landscape L(θ).</p><p>Consider the volume V (c) of the "basin" of nearby parameters θ with loss L(θ) ≤ L(θ * ) + c. Under some fairly general smoothness assumptions, V (c) → 0 as c → 0, with some asymptotic scaling of the form</p><formula xml:id="formula_4">V (c) ∼ c λ (6)</formula><p>The LLC is defined as the exponent λ. Note that λ = N 2 whenever the Hessian is full-rank. In the context of singular learning theory, this is derived from a Bayesian perspective on deep learning, somewhat along the lines of the Bayesian volume hypothesis described above, albeit with much more mathematical sophistication.</p><p>Our measure is derived from somewhat similar considerations, and takes a similar form, with some key differences:</p><p>• We are interested in the behavior of V (c) away from the c → 0 limit.</p><p>• We want to compare the value V (c) across different regions, such as better-or worse-generalizing networks, or multiple training checkpoints.</p><p>• We want to apply this framework to cost functions other than the loss, allowing us to study neural nets far to local minima, without a localizing term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Predictions</head><p>The considerations above lead us to expect the following:</p><p>• Among trained networks with low training loss, better-generalizing networks (lower validation loss) should have larger KL neighborhoods (shorter description lengths) than worse-generalizing ones.</p><p>• During training, KL local volume should tend to decrease (description length should increase), with possible exceptions when networks consolidate their knowledge (as seen for LLC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method builds on the work of <ref type="bibr" target="#b11">Huang et al. (2020)</ref>, who define 'basin' as "the set of points in a neighborhood of the minimizer that have loss value below a cutoff." This definition is ambiguous because it leaves the notion of "neighborhood" undefined. We will show below that their method in fact estimates the volume of the star domain anchored at the minimizer such that all networks in the domain have loss value (or more generally, cost) below a cutoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Naïve approach</head><p>Recall that a star domain S ⊆ R N is a set containing an anchor s 0 such that for all s ∈ S, the line segment from s 0 to s lies in S. This property allows us to define S in terms of a radial function r : S N -1 → [0, ∞) which takes in a unit vector u and outputs a non-negative number corresponding to the "radius" of S along u, or the length of the line segment from s 0 to the boundary of S along the direction u. Given this parameterization, the volume of S can be written as</p><formula xml:id="formula_5">vol(S) = S N -1 r(u) 0 r n-1 drdΩ (7) = 1 n S N -1 r(u) n dΩ (8) = |S N -1 | n E u∼Unif(S N -1 ) [r(u) n ],<label>(9)</label></formula><p>where</p><formula xml:id="formula_6">|S n-1 | = 2π n/2 Γ(n/2)</formula><p>is the surface area of a unit N -ball. We can estimate this using k Monte Carlo samples:<ref type="foot" target="#foot_2">foot_2</ref> </p><formula xml:id="formula_7">vol(S) ≈ vol(S) = |S n-1 | nk k i=1 r(u i ) n (<label>10</label></formula><formula xml:id="formula_8">)</formula><p>Equation 10 is an unbiased estimator for the volume. It is also, very reliably, millions of orders of magnitude too small in practice. Below, we explain the source of this phenomenon and a method for ameliorating it.</p><p>Bias of the log-estimator. In practice, we estimate log vol(S), rather than vol(S) itself, to prevent numerical overflow or underflow. Jensen's inequality tells us that the logarithm of an unbiased estimator is a downwardly biased estimator for the logarithm of the population parameter:</p><formula xml:id="formula_9">log vol(S) ≥ E[log vol(S)],<label>(11)</label></formula><p>with equality if and only if the estimator is constant. This "Jensen gap" is especially large when the variance of the log-estimator is large. For example, if the log-estimator is normally distributed with standard deviation σ, the gap is σ 2 /2.</p><p>Smooth maximum. In practice, n will be extremely large, ranging from 10 6 to 10 12 parameters. It is therefore worth considering the limit of our estimator as n tends to infinity. First note that</p><formula xml:id="formula_10">E[log vol(S)] ∝ E[log k i=i exp n log r(u i ) ].<label>(12)</label></formula><p>LogSumExp is sometimes used as a continuous relaxation of the max function, because for any fixed set of values {x 1 , . . . , x k } we have:</p><formula xml:id="formula_11">lim n→∞ 1 n log k i=i exp nx i = max({x 1 , . . . , x k }). (<label>13</label></formula><formula xml:id="formula_12">)</formula><p>This suggests that, in the large-n limit, the normalized log volume estimate 1 n E[log vol(S)] will be proportional to the maximum of our log-radius samples. Empirically, we find that this is already very nearly true for tiny networks of a few thousand parameters (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Markov's inequality. Since our estimator is a nonnegative random variable, we can use Markov's inequality to show that with high probability, our estimate of the logvolume will not significantly overestimate the true value:</p><formula xml:id="formula_13">P log vol(S) -log vol(S) ≥ log k ≤ 1 k<label>(14)</label></formula><p>That is, the probability that we overestimate the true volume by m &gt; 0 orders of magnitude is at most one in 10 m . Empirically, since our Monte Carlo samples vary over thousands or millions of orders of magnitude, we can view our estimate as a high-confidence approximate lower bound on the true log-volume, with error ≪ Var(log vol(S)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preconditioning</head><p>We propose to reduce the variance of the volume estimator with importance sampling. We still begin by sampling isotropic unit vectors u. However, we then multiply these by a positive-definite preconditioner P with unit determinant, to obtain vectors v = Pu. We then unit-normalize these to obtain unit vectors v, and use the estimator</p><formula xml:id="formula_14">vol(S) = |S N -1 | nk k i=1 r(v i ) n |v| n (15)</formula><p>where the denominator is the usual importance-sampling correction. Under the stated conditions on P, this is still unbiased.</p><p>The purpose of P is to more aggressively sample directions that are flatter. We can interpret the formula above as our original estimator under a change of coordinates by P, with the unit-determinant condition ensuring that the volume of the neighborhood is unchanged in the new coordinates. For a good choice of P, the neighborhood will be more spherical in the new coordinates. <ref type="foot" target="#foot_3">4</ref> With this in mind, we refer to the matrix P as a preconditioner.</p><p>In the case where the neighborhood is perfectly ellipsoid, a perfect choice of P would have eigenvectors aligned with principal axes and eigenvalues proportional to the lengths of those axes. This would result in an estimator with zero variance, returning the exact volume every time. Note that for a quadratic cost function, this is proportional to the inverse square root of the Hessian,</p><formula xml:id="formula_15">P ∝ H -1 2 = VD -1 2 V T (16)</formula><p>where V, D are the eigenvectors and eigenvalues of H.<ref type="foot" target="#foot_4">foot_4</ref> </p><p>For very small neural nets, we use a form of this Hessian preconditioner that is modified to ensure positivedefiniteness:</p><formula xml:id="formula_16">P ∝ V 1 |D| 1 2 + ϵ V T<label>(17)</label></formula><p>We can further economize by using the Hessian diagonal:</p><formula xml:id="formula_17">P ∝ 1 |diag(H)| 1 2 + ϵ<label>(18)</label></formula><p>where diag(H) is a matrix equal to H along its diagonal and zero elsewhere. While exactly computing the Hessian diagonal is no more computationally efficient than computing the entire Hessian, in practice we use the HesScale approximation <ref type="bibr" target="#b6">(Elsayed &amp; Mahmood, 2022)</ref>, which is deterministic, highly efficient, and empirically very accurate.</p><p>Finally, for arbitrarily large networks we can use Adam's second moment buffers to estimate diag(H). In general, we can use any vector or matrix in place of H and its diagonal, and can optionally replace 1 2 with another exponent to obtain a better preconditioner.</p><p>Because of the Markov-inequality bound above, we can test preconditioners very easily: larger numbers are always more accurate, so long as the preconditioner is unitdeterminant. This also gives us, retroactively, a lower bound on how badly the naive (un-preconditioned) estimator undershoots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gaussian volume</head><p>Behaviorally defined neighborhoods can often have infinite Lebesgue volume, making them hard to analyze. If there is any direction along which perturbations have precisely zero effect on the model's behavior on the validation set, that direction will have an infinite radius. There are often many of these. As an example, we find that several pixel locations are never used in the digits validation set, so the corresponding input weight parameters in any network will have no effect.</p><p>If we view neural network training as Bayesian inference, it is natural to think of the distribution used to initialize the parameters as a prior, and in practice this is often a Gaussian distribution. We therefore replace the Lebesgue measure with the Gaussian initialization measure with PDF ρ. Our preconditioned volume estimator becomes</p><formula xml:id="formula_18">vol(S) = |S N -1 | k k i=1 r(v i ) 0 ρ(s 0 + ru i )r n-1 dr |V| n (19)</formula><p>Note that the integrand is of the form exp(quadratic(r) + n log r) and varies rapidly when n is large. We evaluate these integrals numerically using an approximation similar to Lagrange's method, expanding the exponent to second-order and performing a Gaussian integral using a numerically-stable implementation of the error function.</p><p>In practice, the error from the approximation is less than floating-point rounding error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">From loss to KL divergence</head><p>The forms of the volume hypothesis above deal with the training loss. In practice, however, the Hessian of the loss function is closely related to the Fisher information matrix for the model <ref type="bibr" target="#b15">(Martens &amp; Grosse, 2015)</ref>, which is the Hessian of KL divergence. We therefore use KL from the anchor point as our cost function, as well as our defining criterion for our neighborhoods. This has the added benefit of putting the anchor point at a global minimum by definition, simplifying some of the practical challenges. It can also be interpreted within an MDL framework as the description length of the network, as shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Poisoned networks</head><p>We produce "poisoned" ConvNeXt networks on CIFAR-10 using the methodology of <ref type="bibr" target="#b11">(Huang et al., 2020)</ref>, where the standard training loss is augmented with a term encouraging the model to perform poorly on a held-out "poison" set. These networks generalize worse than the unpoisoned ones, while still achieving low train loss. Our hypothesis is that poisoned networks should have smaller local volumes than unpoisoned ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We test our method in three settings: a small MLP (4810 parameters) trained on the UCI handwritten digits dataset <ref type="bibr" target="#b0">(Alpaydin &amp; Kaynak, 1998)</ref>, a variant of<ref type="foot" target="#foot_5">foot_5</ref> the ConvNeXt Atto model <ref type="bibr" target="#b21">(Woo et al., 2023)</ref> (3.4M parameters) trained on <ref type="bibr">CIFAR-10 (Krizhevsky &amp; Hinton, 2009)</ref>, and checkpoints from the Pythia 31M language model <ref type="bibr" target="#b3">(Biderman et al., 2023)</ref>.</p><p>We compute KL divergence on held-out sets consisting of 773 images from digits, 1024 images from CIFAR-10, and 20 text sequences (10926 tokens) from the Pile, respectively. Except where otherwise specified, all results are for k = 100 samples per data point and with a KL cutoff of 10 -2 nats. In the plots that follow, note that the base-ten logarithms of the probability estimates are themselves on the order of -10 6 or -10 8 , as shown by the "×10 6 " and "×10 8 " annotations on the x-axis labels. We estimate the local volume for our small MLP, using the various preconditioners described above, as shown in 1. We show a histogram of the individual samples, with vertical dashed lines for the aggregated estimate. Note that, on a log scale, the estimate is extremely close to the largest individual sample.</p><p>Interestingly, results when preconditioning with the Hessian of the KL (the Fisher matrix) are very similar to the unpreconditioned ones. The diag(H), HesScale, and Adam second-moment (ν) preconditioners perform much better, and very similarly to each other. The Adam first-moment (µ) preconditioner is somewhere in between.</p><p>The hyperparameter ϵ is tuned separately for each of these, to obtain the largest (hence most accurate) result. We find that ϵ = 0.1 works best for the Hessian, while ϵ = 0.01 is best for diag(H) and HesScale and ϵ = 0.001 is best for both Adam preconditioners.</p><p>We find it surprising that the full Hessian performs so poorly, especially given the success of diag(H) and its approximations. This may be some form of overfitting, if the locally-flattest directions are slightly misaligned with the longest directions of the neighborhood, but if so, it is unclear why constraining to axis-aligned directions helps so much.</p><p>We also use the second-moment Adam preconditioner for Pythia and ConvNeXt, where it shows both a clear improvement in the value of the estimates and a smaller sample variance (Figures <ref type="figure">2</ref> and <ref type="figure">3</ref>). The improvement is several standard deviations above most of the naïve estimates, suggesting that it would be infeasible to merely increase the sample size to try to get the same result. <ref type="foot" target="#foot_6">7</ref>For ConvNeXt, we find that the poisoned network has a smaller local volume (with or without preconditioning), in agreement with the results of <ref type="bibr" target="#b11">Huang et al. (2020)</ref> on small networks and in line with our expectation from the MDL and compression perspective. As expected, local volume tends to decrease during training, as the network learns more and its description length increases. For Pythia, this decrease is smooth and approximately exponential after an rapid drop early in training (Figure <ref type="figure" target="#fig_2">4</ref>). In this case, the Adam preconditioner yields modestly larger local volume estimates than the unprecon- For ConvNeXt, the poisoned network actually has larger local volume for much of training, and then drops below the unpoisoned network around 30,000 steps, which is also when the val-set and poison-set losses diverge strongly from each other. This makes sense: early in training, the poisoned loss is just holding back the network (worse loss across all three datasets), slowing the decrease in local volume. Later in training, the network overfits, decreasing its local volume to below the unpoisoned network's. This corresponds to a larger description length for the poisoned (overfit, poorly-generalizing) network. When varying the cutoff, we see a roughly power-law trend for local volume on Pythia (6). The log-log slope is consistent with n/2, where n is the model dimension; this is what would be expected for a purely quadratic cost function. A line with this slope is shown in black for comparison. At very high cutoffs (≥ 10 nats), the Adam preconditioner begins to fail, although raising ϵ partially counteracts this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Across cutoffs</head><p>For ConvNeXt, the result is similar for cutoffs between 10 -5 and 10 -2 . The preconditioner again fails at high cutoff, sooner than for Pythia. At very low cutoffs (10 -6 nats), the poisoned network's local volume suddenly plummets.</p><p>We have not investigated this phenomenon in detail, but we suspect it may have to do with floating point rounding error.</p><p>The slope of n/2 is somewhat in conflict with empirical results for the Local Learning Coefficient <ref type="bibr" target="#b10">(Hoogland et al., 2024;</ref><ref type="bibr" target="#b18">Wang et al., 2024)</ref>, which we suspect may be an artifact of the Monte Carlo local volume estimator. In particular, at low k, our estimator is nearly insensitive to very flat (long) directions, unless they are adequately corrected by the preconditioner. If the steeper (shorter) directions are close to quadratic in shape, this could cause the cost function to appear more quadratic than it really is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduced an efficient algorithm for estimating the probability that a network from some behaviorally-defined region would be sampled from a Gaussian or uniform prior, or equivalently, the network's local volume. While the method is demonstrably more accurate than prior state of the art, it is still unclear how close our estimates are to the ground truth. Nevertheless, we find that our estimated local volume decreases with training time, and is smaller for networks that overfit than for generalizing networks, suggesting that it at least correlates with the true local volume.</p><p>Our results are broadly consistent with a weak form of the volume hypothesis. As expected, poisoned networks were observed to have smaller local volumes than unpoisoned ones. That said, more research is needed to confirm or refute any specific version of the volume hypothesis.</p><p>One promising direction for future work may be to use stochastic gradient Langevin dynamics (SGLD) to propose directions along which to measure the neighborhood's radius. We also are excited to see practical applications of local volume estimation. We think it may be useful for predicting generalization performance. More speculatively, if we define the cost function to be the model's behavior on a relatively narrow distribution-say, a set of math problems fed to a large language model-the local volume may tell us something about how "difficult" these problems are for the model, or how hard it is "thinking."</p><p>Another possible direction may be to estimate the probability measure of neighborhoods around initializations that lead to a given final behavior after training, which corresponds almost exactly to the probability of SGD producing that trained behavior. This would allow for a precise quantitative evaluation of the basin volume hypothesis, and could potentially be accomplished via the training Jacobian <ref type="bibr" target="#b1">(Belrose &amp; Scherlis, 2024)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Results (k = 3000) for various preconditioners on a MLP. Vertical dashed lines indicate the aggregated logvolume estimate, which is very close to the maximum sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Results (k = 1000) with and without Adam preconditioner on Pythia 31M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Local volume decrease while training Pythia 31M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Local volume decrease while training ConvNeXt V2 Atto, and training metrics across datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 6. Results for various cutoffs on Pythia 31M</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Formally, the Radon-Nikodym derivative of µt w.r.t. µ0. This quantity exists if µt is absolutely continuous w.r.t. µ0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For example, if µ0 = Unif(S) for some compact S ⊂ R N ,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For each sample, the radial function is computed via binary search in a uniformly-random direction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The denominator, in this interpretation, can be seen as resulting from differing notions of "unit length" in the original and new coordinates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>P is "proportional to" this quantity because it must be normalized to determinant 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We changed the default patch size, which was optimized for ImageNet, from 4 × 4 to 1 × 1. This significantly improves accuracy on smaller images like those in CIFAR-10.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Note, however, that the naïve estimate for the unpoisoned ConvNeXt network has a large outlier sample that completely dominates the aggregated estimate, nearly reaching the bulk of the preconditioned estimates.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions and Acknowledgements</head><p>Adam Scherlis came up with the main ideas of this paper, wrote the code and performed all experiments and data analysis, and did much of the writing. Nora Belrose proposed the idea of using expected KL divergence as a cost function in lieu of training loss, pointed out the connection with minimum description length, proposed a last-minute reframing of the paper in terms of "sampling trained networks at random," and wrote various parts of the paper.</p><p>Adam and Nora are funded by a grant from Open Philanthropy. We thank Coreweave for computing resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the science of deep learning. At this early stage of research the social impacts are uncertain and indirect, although we hope that future research building on this work may be used to enhance generalization and reliability of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Variance of the log-estimator</head><p>The variance of our local volume estimator is large when S contains outlier directions which have a large effect on the volume. Intuitively, it is difficult to estimate the volume of a needle or pancake by measuring its size along uniformly sampled directions. Most samples will be far closer to the minimum than to the maximum radius. We can formalize this intuition in the following way.</p><p>Consider an ellipsoid S = {x ∈ R N : x T Ax ≤ 1} for some p.s.d. matrix A. Assume also that our anchor s 0 is equal to the centroid of S. Now the radial function has the closed form:</p><p>It turns out that the variance of this quadratic form, assuming u is uniformly distributed on the unit sphere, is</p><p>where Var(λ) is the variance of the eigenvalues of A. Using a Taylor expansion around the mean, the variance of the log radial function is roughly half the squared coefficient of variation of the spectrum. As a result, the variance of the log-estimator is approximately:</p><p>If the spectrum of A has any significant variance, as is empirically often the case for neural network Hessians, this variance will be extremely large.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optical Recognition of Handwritten Digits</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<idno type="DOI">10.24432/C50P49</idno>
		<ptr target="https://doi.org/10.24432/C50P49" />
	</analytic>
	<monogr>
		<title level="j">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding gradient descent through the training jacobian</title>
		<author>
			<persName><forename type="first">N</forename><surname>Belrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scherlis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.07003" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Loss surface simplexes for mode connecting volumes and fast ensembling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="769" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Loss landscapes are all you need: Neural network generalization can be explained without the implicit bias of gradient descent</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><surname>-Y</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><surname>Hesscale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11639</idno>
		<title level="m">Scalable computation of hessian diagonals</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on Computational learning theory</title>
		<meeting>the sixth annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Flat minima. Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hoogland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrugia-Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murfet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02364</idno>
		<title level="m">The developmental landscape of in-context learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding generalization through visualizations</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A S</forename><surname>Emam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=pxqYT_7gToV" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020 workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The local learning coefficient: A singularity-aware complexity measure</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Furman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murfet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.12108" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Stochastic gradient descent as approximate bayesian inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.04289" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page" from="2408" to="2417" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In International conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Is sgd a bayesian sampler? well, almost</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mingard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valle-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skalse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">79</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural redshift: Random networks are not random functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Nicolicioiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4786" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Differentiation and specialization of attention heads via the refined local learning coefficient</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoogland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Wingerden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Furman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murfet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.02984" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Algebraic geometry and statistical learning theory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convnext v2: Co-designing and scaling convnets with masked autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="16133" to="16142" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
