# Estimating the Probability of Sampling a Trained Neural Network at Random

## Abstract

## 

We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.

## Introduction

There is a long line of research which finds that flat minima in a neural network parameter space, defined as weight vectors surrounded by large regions "with the property that each weight vector from that region leads to similar small error" generalize better than sharp minima [(Hochreiter & Schmidhuber, 1997)](#b9). While there are counterexamples to this tendency [(Dinh et al., 2017)](#b5), it seems to be empirically and theoretically fairly robust, and has inspired the development of optimizers that explicitly search for flatter minima [(Foret et al., 2021)](#).

In a related line of work, [Chiang et al. (2022)](#b4) put forward the volume hypothesis, which states that "...the implicit bias of neural networks may arise from the volume disparity of different basins in the loss landscape, with good hypothesis classes occupying larger volumes." They evaluate simple gradient-free learning algorithms, such as the "Guess & Check" optimizer which randomly samples parameters until it stumbles upon a network that achieves training loss under some threshold, and find that these methods have similar generalization behavior to gradient descent, at least on the very simple tasks they tested. [Teney et al. (2024)](#b17) find that randomly initialized networks represent very simple functions, which would explain the simplicity bias of deep learning if SGD behaves similarly to Guess & Check.

Additionally, [Mingard et al. (2021)](#b16) provide evidence that SGD may be an approximate Bayesian sampler, where the prior distribution over functions is equal to the distribution over functions represented by randomly initialized networks. Since networks are usually initialized using a uniform or Gaussian distribution, the Bayesian sampling hypothesis makes similar predictions to the volume hypothesis. Finally, recent work suggests that singular learning theory [(Watanabe, 2009)](#b19), originally developed to analyze the learning dynamics of overparameterized Bayesian models, can be profitably used to understand deep learning [(Hoogland et al., 2024;](#b10)[Lau et al., 2024)](#b13).

In this work, we propose an efficient algorithm for estimating the probability that a network from some behaviorallydefined region would be sampled from a Gaussian or uniform prior. This is equivalent to the volume of the region, if the prior is uniform. We define the local volume of a weight vector θ ∈ R N relative to a cost function C : R N → [0, ∞), threshold ϵ > 0, and measure µ to be the size according to µ of the star domain S anchored at θ containing points θ ′ such that C(θ ′ ) < ϵ, which we call the neighborhood of θ. This is equivalent to the probability of sampling a network inside S from a prior proportional to the measure µ. While prior work has assumed µ to be the Lebesgue measure (i.e. volume), we also consider the probability measure used to initialize the network before training, which guarantees that the measure of any neighborhood must be finite. We find empirically that some realworld neighborhood actually have infinite Lebesgue volume, creating difficulties for analysis.

We use the term "cost" rather than "loss" intentionally be-cause we are interested in a broader class of functions than just the loss function used to train the model. In particular, for networks that output a predictive probability distribution we find it useful to consider the expected KL cost function E x [D KL (f (x; θ)||f (x; θ ′ )], which measures how behaviorally different θ ′ is from θ, independent of any ground truth labels. This cost also has the benefit that it is always zero at the minimum point θ ′ = θ.

We estimate that the probability of randomly sampling the trained Pythia 31M language model from its initialization distribution, within an accuracy of 0.01 nats, is about Pr(language model) ≈ 1 10 3.6×10 8

(1) or one in 1 followed by 360 million zeros. For comparison, there are about 10 80 atoms in the observable universe, so this is about the same as the probability of correctly guessing a specific atom 4.5 million times in a row.

## Motivation

As mentioned in the introduction, one major motivation behind this work is to test the volume hypothesis: the idea that the relative volumes of different regions in parameter space strongly shape the kinds of networks that are produced by gradient descent. There are many variants of this hypothesis, and we detail two of them below.

The Bayesian volume hypothesis. Here is a simplistic version of the volume hypothesis which we think is likely false for real-world neural networks, but which may be helpful as an intuition pump.

Consider Bayesian inference with prior density ρ(θ) and likelihood function -L. The posterior distribution is proportional to ρ(θ) exp(-L(θ)). Since many neural-network losses can be interpreted as negative log-likelihoods, we can think of L as the loss function of a neural net and ρ as a prior related to initialization and regularization of the network. If neural-net training were perfectly Bayesian, the probability density for obtaining some parameter θ from training would depend only on the prior and the loss. This is of interest because it attributes generalization entirely to the architecture and loss function: under this hypothesis, the only way for one low-training-loss solution to be favored over another is if it simply occupies more of parameter space. In effect, the architecture imposes a sophisticated inductive prior (on top of the simple prior ρ) by overrepresenting "good", well-generalizing functions, and underrepresenting "bad" ones.

This hypothesis is true for stochastic gradient Langevin dynamics [(Welling & Teh, 2011)](#b20), which is an efficient Bayes sampler for deep neural networks, but only with unrealisti-cally long mixing times.

Quadratic toy model. Consider a quadratic loss function with Hessian H. If the initialization distribution µ 0 has covariance matrix I, then at timestep t the covariance is exp(-Ht) exp(-Ht) T . Assuming µ 0 is a zero-mean Gaussian, the log density of parameters θ at time t is proportional to θ exp(2Ht)θ T , which is in general not proportional to the loss[foot_0](#foot_0) 2 θHθ T . The probability mass becomes concentrated along directions of higher curvature (larger Hessian eigenvalues) exponentially faster than along directions of lower curvature.

If we introduce isotropic noise and solve the resulting Fokker-Planck equation, it can be shown that the logdensity instead converges to something proportional to the loss. However, if the noise is not isotropic -in particular if it is stronger in more steeply-curved directions, as is true in practice -then this fails [(Mandt et al., 2018)](#b14). This shows that, within basins of non-isotropic curvature, the posterior density of popular optimizers does not satisfy the Bayesian volume hypothesis. We can, however, restrict the hypothesis to apply only between different basins.

The basin volume hypothesis. Let λ be the Lebesgue measure on R N , and let µ 0 be the probability measure on R N from which the initial network parameters θ 0 are sampled, usually a uniform distribution on a compact set or a Gaussian. Let µ t be the distribution over network parameters at timestep t in training, and let f t (x) = dµt dµ0 be the probability density of parameters x at time t. 1 We can decompose the posterior probability of behaviorally distinct regions of parameter space, such as basins of low loss with differing degrees of generalization, as follows.

Let A ⊂ R N and B ⊂ R N be two disjoint regions of parameter space, perhaps defined by their performance on a held-out test set. The probability that training will yield an element of A can be decomposed as

$log P(θ ∈ A) = log µ 0 (A) • 1 µ 0 (A) A f t dµ 0 (2) = log µ 0 (A) volume + log E x∼Unif(A) f t (x)$mean density

(3) and the log probability ratio is

$log P(θ ∈ A) P(θ ∈ B) = log µ 0 (A) µ 0 (B) volume ratio + log E x∼µ0|A f t (x) E x∼µ0|B f t (x) density ratio ,(4)$where µ 0 |A denotes the restriction of µ 0 to A. 2 Note that at t = 0 we have f 0 (x) = dµ0 dµ0 (x) = 1 for any x, so that at early times t the density ratio term in Eq. 4 should be small.

## A restricted form of the volume hypothesis is as follows:

Even at the end of training, the volume ratio term in Eq.4 should be larger than the density ratio term for suitable choices of A and B.

Of course, if the networks in A and the networks in B differ significantly in terms of their performance on the training set, the density ratio term must become very large as t → ∞, since a well-tuned optimizer is guaranteed to bring the loss close to a local minimum. When analyzing generalization, then, we should select A and B such that are both contained in a low-loss manifold [(Benton et al., 2021)](#b2).

## Minimum description length

Basin volume can be connected directly to generalization using the notion of minimum description length (MDL). The idea is that a statistical model is more likely to generalize if it compresses its training data effectively, while not being too complex itself. Since we are assuming that all networks in the neighborhood perform similarly, we will treat the neighborhood itself as an ensemble over networks, and use it as our statistical model. In Bayesian terms, our posterior is a uniform distribution over the neighborhood, and we assume that our receiver is using the initialization distribution µ 0 as a prior. The bits-back argument [(Hinton & Van Camp, 1993)](#b8) shows that the MDL of this model plus the training data

$x 1:n is KL Unif(A)||µ 0 + E θ∼Unif(A) n i=i log 2 p θ (x i ) , (5)$where A ⊂ R N is the neighborhood, and p θ (x i ) is the probability that the network with parameters θ assigns to datapoint x i .

In practice, µ 0 is either a uniform distribution over a simple polytope S ⊂ R N , or a (possibly truncated) Gaussian N (0, Σ) with diagonal covariance. In the former case, the KL term simplifies to log λ(S)-log λ(A), and in the latter, it simplifies to

$n 2 log(2π)+ 1 2 log |Σ|+ 1 2 E θ∼Unif(A) [θ T Σ -1 θ]-log λ(A),$which only depends on A is through its volume and its mean Mahalanobis distance from the origin. Neighborhoods with large Lebesgue volume and small average Mahalanobis norm will have lower description length than neighborhoods with smaller volume or higher Mahalanobis norm.

then µ0|A = Unif(A ∩ S). If µ0 is a Gaussian, then µ0|A is a truncated Gaussian with support A.

## Singular Learning Theory and the Local Learning Coefficient

The local learning coefficient (LLC) was introduced by Lau et al. ( [2024](#)), extending concepts from singular learning theory [(Watanabe, 2009)](#b19), and has proved to be useful as a measure of the complexity of neural networks and their components [(Hoogland et al., 2024;](#b10)[Wang et al., 2024)](#b18).

Consider a local minimum θ * in the loss landscape L(θ).

Consider the volume V (c) of the "basin" of nearby parameters θ with loss L(θ) ≤ L(θ * ) + c. Under some fairly general smoothness assumptions, V (c) → 0 as c → 0, with some asymptotic scaling of the form

$V (c) ∼ c λ (6)$The LLC is defined as the exponent λ. Note that λ = N 2 whenever the Hessian is full-rank. In the context of singular learning theory, this is derived from a Bayesian perspective on deep learning, somewhat along the lines of the Bayesian volume hypothesis described above, albeit with much more mathematical sophistication.

Our measure is derived from somewhat similar considerations, and takes a similar form, with some key differences:

• We are interested in the behavior of V (c) away from the c → 0 limit.

• We want to compare the value V (c) across different regions, such as better-or worse-generalizing networks, or multiple training checkpoints.

• We want to apply this framework to cost functions other than the loss, allowing us to study neural nets far to local minima, without a localizing term.

## Predictions

The considerations above lead us to expect the following:

• Among trained networks with low training loss, better-generalizing networks (lower validation loss) should have larger KL neighborhoods (shorter description lengths) than worse-generalizing ones.

• During training, KL local volume should tend to decrease (description length should increase), with possible exceptions when networks consolidate their knowledge (as seen for LLC).

## Method

Our method builds on the work of [Huang et al. (2020)](#b11), who define 'basin' as "the set of points in a neighborhood of the minimizer that have loss value below a cutoff." This definition is ambiguous because it leaves the notion of "neighborhood" undefined. We will show below that their method in fact estimates the volume of the star domain anchored at the minimizer such that all networks in the domain have loss value (or more generally, cost) below a cutoff.

## Naïve approach

Recall that a star domain S ⊆ R N is a set containing an anchor s 0 such that for all s ∈ S, the line segment from s 0 to s lies in S. This property allows us to define S in terms of a radial function r : S N -1 → [0, ∞) which takes in a unit vector u and outputs a non-negative number corresponding to the "radius" of S along u, or the length of the line segment from s 0 to the boundary of S along the direction u. Given this parameterization, the volume of S can be written as

$vol(S) = S N -1 r(u) 0 r n-1 drdΩ (7) = 1 n S N -1 r(u) n dΩ (8) = |S N -1 | n E u∼Unif(S N -1 ) [r(u) n ],(9)$where

$|S n-1 | = 2π n/2 Γ(n/2)$is the surface area of a unit N -ball. We can estimate this using k Monte Carlo samples:[foot_2](#foot_2)

$vol(S) ≈ vol(S) = |S n-1 | nk k i=1 r(u i ) n (10$$)$Equation 10 is an unbiased estimator for the volume. It is also, very reliably, millions of orders of magnitude too small in practice. Below, we explain the source of this phenomenon and a method for ameliorating it.

Bias of the log-estimator. In practice, we estimate log vol(S), rather than vol(S) itself, to prevent numerical overflow or underflow. Jensen's inequality tells us that the logarithm of an unbiased estimator is a downwardly biased estimator for the logarithm of the population parameter:

$log vol(S) ≥ E[log vol(S)],(11)$with equality if and only if the estimator is constant. This "Jensen gap" is especially large when the variance of the log-estimator is large. For example, if the log-estimator is normally distributed with standard deviation σ, the gap is σ 2 /2.

Smooth maximum. In practice, n will be extremely large, ranging from 10 6 to 10 12 parameters. It is therefore worth considering the limit of our estimator as n tends to infinity. First note that

$E[log vol(S)] ∝ E[log k i=i exp n log r(u i ) ].(12)$LogSumExp is sometimes used as a continuous relaxation of the max function, because for any fixed set of values {x 1 , . . . , x k } we have:

$lim n→∞ 1 n log k i=i exp nx i = max({x 1 , . . . , x k }). (13$$)$This suggests that, in the large-n limit, the normalized log volume estimate 1 n E[log vol(S)] will be proportional to the maximum of our log-radius samples. Empirically, we find that this is already very nearly true for tiny networks of a few thousand parameters (Figure [1](#fig_0)).

Markov's inequality. Since our estimator is a nonnegative random variable, we can use Markov's inequality to show that with high probability, our estimate of the logvolume will not significantly overestimate the true value:

$P log vol(S) -log vol(S) ≥ log k ≤ 1 k(14)$That is, the probability that we overestimate the true volume by m > 0 orders of magnitude is at most one in 10 m . Empirically, since our Monte Carlo samples vary over thousands or millions of orders of magnitude, we can view our estimate as a high-confidence approximate lower bound on the true log-volume, with error ≪ Var(log vol(S)).

## Preconditioning

We propose to reduce the variance of the volume estimator with importance sampling. We still begin by sampling isotropic unit vectors u. However, we then multiply these by a positive-definite preconditioner P with unit determinant, to obtain vectors v = Pu. We then unit-normalize these to obtain unit vectors v, and use the estimator

$vol(S) = |S N -1 | nk k i=1 r(v i ) n |v| n (15)$where the denominator is the usual importance-sampling correction. Under the stated conditions on P, this is still unbiased.

The purpose of P is to more aggressively sample directions that are flatter. We can interpret the formula above as our original estimator under a change of coordinates by P, with the unit-determinant condition ensuring that the volume of the neighborhood is unchanged in the new coordinates. For a good choice of P, the neighborhood will be more spherical in the new coordinates. [4](#foot_3) With this in mind, we refer to the matrix P as a preconditioner.

In the case where the neighborhood is perfectly ellipsoid, a perfect choice of P would have eigenvectors aligned with principal axes and eigenvalues proportional to the lengths of those axes. This would result in an estimator with zero variance, returning the exact volume every time. Note that for a quadratic cost function, this is proportional to the inverse square root of the Hessian,

$P ∝ H -1 2 = VD -1 2 V T (16)$where V, D are the eigenvectors and eigenvalues of H.[foot_4](#foot_4)

For very small neural nets, we use a form of this Hessian preconditioner that is modified to ensure positivedefiniteness:

$P ∝ V 1 |D| 1 2 + ϵ V T(17)$We can further economize by using the Hessian diagonal:

$P ∝ 1 |diag(H)| 1 2 + ϵ(18)$where diag(H) is a matrix equal to H along its diagonal and zero elsewhere. While exactly computing the Hessian diagonal is no more computationally efficient than computing the entire Hessian, in practice we use the HesScale approximation [(Elsayed & Mahmood, 2022)](#b6), which is deterministic, highly efficient, and empirically very accurate.

Finally, for arbitrarily large networks we can use Adam's second moment buffers to estimate diag(H). In general, we can use any vector or matrix in place of H and its diagonal, and can optionally replace 1 2 with another exponent to obtain a better preconditioner.

Because of the Markov-inequality bound above, we can test preconditioners very easily: larger numbers are always more accurate, so long as the preconditioner is unitdeterminant. This also gives us, retroactively, a lower bound on how badly the naive (un-preconditioned) estimator undershoots.

## Gaussian volume

Behaviorally defined neighborhoods can often have infinite Lebesgue volume, making them hard to analyze. If there is any direction along which perturbations have precisely zero effect on the model's behavior on the validation set, that direction will have an infinite radius. There are often many of these. As an example, we find that several pixel locations are never used in the digits validation set, so the corresponding input weight parameters in any network will have no effect.

If we view neural network training as Bayesian inference, it is natural to think of the distribution used to initialize the parameters as a prior, and in practice this is often a Gaussian distribution. We therefore replace the Lebesgue measure with the Gaussian initialization measure with PDF ρ. Our preconditioned volume estimator becomes

$vol(S) = |S N -1 | k k i=1 r(v i ) 0 ρ(s 0 + ru i )r n-1 dr |V| n (19)$Note that the integrand is of the form exp(quadratic(r) + n log r) and varies rapidly when n is large. We evaluate these integrals numerically using an approximation similar to Lagrange's method, expanding the exponent to second-order and performing a Gaussian integral using a numerically-stable implementation of the error function.

In practice, the error from the approximation is less than floating-point rounding error.

## From loss to KL divergence

The forms of the volume hypothesis above deal with the training loss. In practice, however, the Hessian of the loss function is closely related to the Fisher information matrix for the model [(Martens & Grosse, 2015)](#b15), which is the Hessian of KL divergence. We therefore use KL from the anchor point as our cost function, as well as our defining criterion for our neighborhoods. This has the added benefit of putting the anchor point at a global minimum by definition, simplifying some of the practical challenges. It can also be interpreted within an MDL framework as the description length of the network, as shown below.

## Poisoned networks

We produce "poisoned" ConvNeXt networks on CIFAR-10 using the methodology of [(Huang et al., 2020)](#b11), where the standard training loss is augmented with a term encouraging the model to perform poorly on a held-out "poison" set. These networks generalize worse than the unpoisoned ones, while still achieving low train loss. Our hypothesis is that poisoned networks should have smaller local volumes than unpoisoned ones.

## Results

We test our method in three settings: a small MLP (4810 parameters) trained on the UCI handwritten digits dataset [(Alpaydin & Kaynak, 1998)](#b0), a variant of[foot_5](#foot_5) the ConvNeXt Atto model [(Woo et al., 2023)](#b21) (3.4M parameters) trained on [CIFAR-10 (Krizhevsky & Hinton, 2009)](#), and checkpoints from the Pythia 31M language model [(Biderman et al., 2023)](#b3).

We compute KL divergence on held-out sets consisting of 773 images from digits, 1024 images from CIFAR-10, and 20 text sequences (10926 tokens) from the Pile, respectively. Except where otherwise specified, all results are for k = 100 samples per data point and with a KL cutoff of 10 -2 nats. In the plots that follow, note that the base-ten logarithms of the probability estimates are themselves on the order of -10 6 or -10 8 , as shown by the "×10 6 " and "×10 8 " annotations on the x-axis labels. We estimate the local volume for our small MLP, using the various preconditioners described above, as shown in 1. We show a histogram of the individual samples, with vertical dashed lines for the aggregated estimate. Note that, on a log scale, the estimate is extremely close to the largest individual sample.

Interestingly, results when preconditioning with the Hessian of the KL (the Fisher matrix) are very similar to the unpreconditioned ones. The diag(H), HesScale, and Adam second-moment (ν) preconditioners perform much better, and very similarly to each other. The Adam first-moment (µ) preconditioner is somewhere in between.

The hyperparameter ϵ is tuned separately for each of these, to obtain the largest (hence most accurate) result. We find that ϵ = 0.1 works best for the Hessian, while ϵ = 0.01 is best for diag(H) and HesScale and ϵ = 0.001 is best for both Adam preconditioners.

We find it surprising that the full Hessian performs so poorly, especially given the success of diag(H) and its approximations. This may be some form of overfitting, if the locally-flattest directions are slightly misaligned with the longest directions of the neighborhood, but if so, it is unclear why constraining to axis-aligned directions helps so much.

We also use the second-moment Adam preconditioner for Pythia and ConvNeXt, where it shows both a clear improvement in the value of the estimates and a smaller sample variance (Figures [2](#) and [3](#)). The improvement is several standard deviations above most of the naïve estimates, suggesting that it would be infeasible to merely increase the sample size to try to get the same result. [7](#foot_6)For ConvNeXt, we find that the poisoned network has a smaller local volume (with or without preconditioning), in agreement with the results of [Huang et al. (2020)](#b11) on small networks and in line with our expectation from the MDL and compression perspective. As expected, local volume tends to decrease during training, as the network learns more and its description length increases. For Pythia, this decrease is smooth and approximately exponential after an rapid drop early in training (Figure [4](#fig_2)). In this case, the Adam preconditioner yields modestly larger local volume estimates than the unprecon- For ConvNeXt, the poisoned network actually has larger local volume for much of training, and then drops below the unpoisoned network around 30,000 steps, which is also when the val-set and poison-set losses diverge strongly from each other. This makes sense: early in training, the poisoned loss is just holding back the network (worse loss across all three datasets), slowing the decrease in local volume. Later in training, the network overfits, decreasing its local volume to below the unpoisoned network's. This corresponds to a larger description length for the poisoned (overfit, poorly-generalizing) network. When varying the cutoff, we see a roughly power-law trend for local volume on Pythia (6). The log-log slope is consistent with n/2, where n is the model dimension; this is what would be expected for a purely quadratic cost function. A line with this slope is shown in black for comparison. At very high cutoffs (≥ 10 nats), the Adam preconditioner begins to fail, although raising ϵ partially counteracts this.

## Across cutoffs

For ConvNeXt, the result is similar for cutoffs between 10 -5 and 10 -2 . The preconditioner again fails at high cutoff, sooner than for Pythia. At very low cutoffs (10 -6 nats), the poisoned network's local volume suddenly plummets.

We have not investigated this phenomenon in detail, but we suspect it may have to do with floating point rounding error.

The slope of n/2 is somewhat in conflict with empirical results for the Local Learning Coefficient [(Hoogland et al., 2024;](#b10)[Wang et al., 2024)](#b18), which we suspect may be an artifact of the Monte Carlo local volume estimator. In particular, at low k, our estimator is nearly insensitive to very flat (long) directions, unless they are adequately corrected by the preconditioner. If the steeper (shorter) directions are close to quadratic in shape, this could cause the cost function to appear more quadratic than it really is.

## Conclusion

In this work, we introduced an efficient algorithm for estimating the probability that a network from some behaviorally-defined region would be sampled from a Gaussian or uniform prior, or equivalently, the network's local volume. While the method is demonstrably more accurate than prior state of the art, it is still unclear how close our estimates are to the ground truth. Nevertheless, we find that our estimated local volume decreases with training time, and is smaller for networks that overfit than for generalizing networks, suggesting that it at least correlates with the true local volume.

Our results are broadly consistent with a weak form of the volume hypothesis. As expected, poisoned networks were observed to have smaller local volumes than unpoisoned ones. That said, more research is needed to confirm or refute any specific version of the volume hypothesis.

One promising direction for future work may be to use stochastic gradient Langevin dynamics (SGLD) to propose directions along which to measure the neighborhood's radius. We also are excited to see practical applications of local volume estimation. We think it may be useful for predicting generalization performance. More speculatively, if we define the cost function to be the model's behavior on a relatively narrow distribution-say, a set of math problems fed to a large language model-the local volume may tell us something about how "difficult" these problems are for the model, or how hard it is "thinking."

Another possible direction may be to estimate the probability measure of neighborhoods around initializations that lead to a given final behavior after training, which corresponds almost exactly to the probability of SGD producing that trained behavior. This would allow for a precise quantitative evaluation of the basin volume hypothesis, and could potentially be accomplished via the training Jacobian [(Belrose & Scherlis, 2024)](#b1).

![Figure 1. Results (k = 3000) for various preconditioners on a MLP. Vertical dashed lines indicate the aggregated logvolume estimate, which is very close to the maximum sample.]()

![Figure 2. Results (k = 1000) with and without Adam preconditioner on Pythia 31M]()

![Figure 4. Local volume decrease while training Pythia 31M]()

![Figure 5. Local volume decrease while training ConvNeXt V2 Atto, and training metrics across datasets]()

![Figure 6. Results for various cutoffs on Pythia 31M]()

Formally, the Radon-Nikodym derivative of µt w.r.t. µ0. This quantity exists if µt is absolutely continuous w.r.t. µ0.

For example, if µ0 = Unif(S) for some compact S ⊂ R N ,

For each sample, the radial function is computed via binary search in a uniformly-random direction.

The denominator, in this interpretation, can be seen as resulting from differing notions of "unit length" in the original and new coordinates.

P is "proportional to" this quantity because it must be normalized to determinant 1.

We changed the default patch size, which was optimized for ImageNet, from 4 × 4 to 1 × 1. This significantly improves accuracy on smaller images like those in CIFAR-10.

Note, however, that the naïve estimate for the unpoisoned ConvNeXt network has a large outlier sample that completely dominates the aggregated estimate, nearly reaching the bulk of the preconditioned estimates.

