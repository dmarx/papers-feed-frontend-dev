<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LANGUAGE MODELS ARE HIDDEN REASONERS: UNLOCKING LATENT REASONING CAPABILITIES VIA SELF-REWARDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-21">21 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haolin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuxin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiran</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akshara</forename><surname>Prabhakar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shelby</forename><surname>Heinecke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricky</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phil</forename><surname>Mui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
							<email>huan.wang@salesforce.com.1</email>
							<affiliation key="aff0">
								<orgName type="department">Salesforce AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LANGUAGE MODELS ARE HIDDEN REASONERS: UNLOCKING LATENT REASONING CAPABILITIES VIA SELF-REWARDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-21">21 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">501711E3EB6C49936A8F0217318F4A74</idno>
					<idno type="arXiv">arXiv:2411.04282v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at <ref type="url" target="https://github.com/SalesforceAIResearch/LaTRO">https://github.com/SalesforceAIResearch/LaTRO</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The development of large language models (LLMs) with enhanced reasoning capabilities has emerged as a crucial area of research. Despite their impressive advances, the inherent next-token prediction mechanism of LLMs makes it challenging for these models to solve complex problems requiring multiple reasoning steps <ref type="bibr" target="#b43">(Wang et al., 2022;</ref><ref type="bibr" target="#b17">Huang et al., 2023)</ref>. For instance, LLMs often struggle to directly provide accurate solutions to mathematical problems or even simple puzzles like counting specific letters in a word. Consequently, researchers have explored various prompting strategies that guide LLMs to generate reasoning trajectories or rationales-sequences of tokens that build a step-by-step progression toward an answer. Techniques such as Chain-of-Thought (CoT) <ref type="bibr" target="#b45">(Wei et al., 2022)</ref>, Tree-of-Thought (ToT) <ref type="bibr" target="#b50">(Yao et al., 2024)</ref>, and Program-of-Thought (PoT) <ref type="bibr" target="#b6">(Chen et al., 2023)</ref> prompting methods exemplify this approach.</p><p>Recent progress has also focused on inference-time techniques to enhance the reasoning abilities of LLMs <ref type="bibr" target="#b48">(Wu et al., 2024;</ref><ref type="bibr" target="#b5">Brown et al., 2024)</ref>, as observed in the OpenAI o1 model <ref type="bibr" target="#b29">(OpenAI, 2024)</ref>. These methods have demonstrated remarkable performance in diverse reasoning tasks, including mathematics <ref type="bibr">(Cobbe et al., 2021b;</ref><ref type="bibr" target="#b39">Trinh et al., 2024;</ref><ref type="bibr" target="#b27">Luo et al., 2024)</ref>, coding <ref type="bibr" target="#b19">(Jimenez et al., 2023;</ref><ref type="bibr" target="#b11">Guo et al., 2024;</ref><ref type="bibr" target="#b54">Zhang et al., 2024)</ref>, and scientific problem-solving <ref type="bibr" target="#b34">(Rein et al., 2023)</ref>. Notable inference-time methods, such as CoT with Self-Consistency (CoT-SC) <ref type="bibr" target="#b44">(Wang et al., 2023)</ref> and CoT-Decoding <ref type="bibr">(Wang &amp; Zhou, 2024)</ref>, extend the CoT approach by generating multiple reasoning paths and selecting the most consistent one. Additionally, techniques like ReAct <ref type="bibr">(Yao et al., 2023a)</ref> and Reflexion <ref type="bibr" target="#b36">(Shinn et al., 2023)</ref> integrate reasoning into LLM agent loops, further enhancing their problem-solving capabilities.</p><p>Despite the promising results at inference time, improving the reasoning abilities of LLMs during their training phase remains a challenging problem. Several obstacles impede progress in this area. Firstly,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question x x x</head><p>Language Model π θ Sampled Rationale z z z1, z z z2, . . . , z z z K Self-reward: Compute the likelihood of π θ generating y y y after observing x x x and z z z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Groundtruth y y y</head><p>Language Model π θ update Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts does it take?</p><p>Groundtruth: The answer is 3. Sampled Rationale 1 (correct , higher likelihood): It takes 2/2 = 1 bolt of white fiber. 2 + 1 = 3. So, it takes a total of 3 bolts of fiber. Sampled Rationale 2 (incorrect , lower likelihood): We need 2 bolts of blue and 2 bolts of white fiber. In total, it is 2 + 2 = 4.</p><p>Figure <ref type="figure">1</ref>: Overview of LaTRO with an example question from GSM8K <ref type="bibr">(Cobbe et al., 2021b)</ref>. LaTRO treats reasoning trajectories as latent variables and optimizes the underlying distribution through self-rewarding. Given a question, the language model generates multiple reasoning rationales, evaluates their likelihood of producing the correct answer, and updates its parameters to favor high-quality rationales. This iterative process allows the model to improve both its ability to generate good reasoning paths and to evaluate the quality of those paths.</p><p>there is a scarcity of high-quality reasoning data for complex problems, limiting the applicability of traditional supervised fine-tuning (SFT) approaches <ref type="bibr" target="#b52">(Zelikman et al., 2022)</ref>. Moreover, when such data is available, SFT on deterministic reasoning paths may result in a lack of diversity in problem-solving strategies, potentially causing over-confidence issues and performance degradation <ref type="bibr">(Cobbe et al., 2021b)</ref>, especially in domains needing multiple valid approaches, such as mathematical proofs and coding. Alternatively, improving reasoning through reinforcement learning from human feedback (RLHF) presents its own challenges <ref type="bibr" target="#b12">(Havrilla et al., 2024;</ref><ref type="bibr" target="#b27">Luo et al., 2024)</ref>. Developing a reward model that accurately evaluates the quality and validity of reasoning paths is a formidable task, susceptible to distribution shifts and biased evaluations.</p><p>Self-improvement approaches like STaR (Self-Taught Reasoner) <ref type="bibr" target="#b52">(Zelikman et al., 2022)</ref> and Quiet-STaR <ref type="bibr" target="#b53">(Zelikman et al., 2024)</ref> have shown promise in enhancing language models' reasoning capabilities without external feedback. However, STaR relies on task-specific few-shot examples to bootstrap its reasoning process, which can limit its generalizability across diverse tasks. While Quiet-STaR attempts to overcome this by inferring implicit rationales across arbitrary text, it does not directly optimize the reasoning process itself. Through these findings, we observe that pretrained LLMs already possess innate reasoning capabilities but just have not been fully activated or utilized, inspiring us to propose our approach.</p><p>Our proposed method, LaTent Reasoning Optimization (LaTRO), addresses the limitations of previous approaches by formulating reasoning as sampling from a latent distribution and optimizing it through a principled variational framework. As illustrated in Fig. <ref type="figure">1</ref>, LaTRO enables language models to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring task-specific few-shot examples or external reward models. Key contributions of LaTRO include:</p><p>1. A theoretical formulation connecting LLM reasoning optimization to latent variable models; 2. A self-rewarding mechanism leveraging the model's own probability estimates; 3. Significant performance gains across multiple model architectures and reasoning tasks, demonstrating LaTRO's effectiveness in unlocking latent reasoning capabilities of language models.</p><p>Our findings suggest that pre-trained LLMs are not only capable reasoners but also possess the potential to act as explicit reward models for evaluating reasoning paths. We term this approach of utilizing explicit reward functions induced by LLMs themselves as "self-rewarding." Empirically, LaTRO outperforms both baseline models and supervised fine-tuning approaches on reasoning tasks like GSM8K, while also demonstrating the capacity to compress reasoning processes and shift computational burdens from inference to training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Prompt-based LLM Reasoning Prompt-based reasoning methods prove to be effective across various domains, such as math problem-solving <ref type="bibr" target="#b31">(Polu &amp; Sutskever, 2020;</ref><ref type="bibr" target="#b13">Hendrycks et al., 2021;</ref><ref type="bibr">Cobbe et al., 2021a)</ref>, logical reasoning <ref type="bibr" target="#b37">(Sprague et al., 2024)</ref> and agentic tasks <ref type="bibr">(Yao et al., 2023a;</ref><ref type="bibr" target="#b36">Shinn et al., 2023;</ref><ref type="bibr">Yao et al., 2023b)</ref>. Chain-of-Thoughts or CoT <ref type="bibr" target="#b45">(Wei et al., 2022)</ref> is the pioneering work that prompts LLMs to decompose challenging tasks into smaller reasoning steps. After that, two primary research directions further improved reasoning capabilities during inference. One direction searched over the reasoning trajectories against a process-based verifier, or reward model <ref type="bibr" target="#b50">(Yao et al., 2024;</ref><ref type="bibr" target="#b4">Besta et al., 2024;</ref><ref type="bibr" target="#b24">Lightman et al., 2023)</ref>. For example, tree-of-thoughts <ref type="bibr" target="#b50">(Yao et al., 2024)</ref> explored over thoughts by depth-first search (DFS), breadth-first search (BFS) or beam search. The other approach used a critic model to provide verbal feedback, iteratively refining the responses with that feedback <ref type="bibr" target="#b35">(Saunders et al., 2022;</ref><ref type="bibr" target="#b36">Shinn et al., 2023;</ref><ref type="bibr">Yao et al., 2023b;</ref><ref type="bibr" target="#b28">Madaan et al., 2023)</ref>.</p><formula xml:id="formula_0">Self</formula><p>-Rewarding for LLM Reasoning Reasoning capabilities in LLMs can be enhanced in posttraining through self-rewarding and reinforcement learning. The Self-Taught Reasoner, or STaR (Zelikman et al., 2022) introduced a bootstrapping technique that allows LLMs to generate rationales and fine-tune itself with self-generated reasoning paths. Quiet-STaR (Zelikman et al., 2024) extended this by training LLMs to infer implicit rationales across arbitrary text, enhancing both reasoning and predictive abilities without task-specific fine-tuning. Reinforced Fine-Tuning, or ReFT (Trung et al., 2024) took this further by leveraging reinforcement learning to improve generalization in reasoning tasks like math problem-solving, enabling LLMs to learn from multiple reasoning paths. Self-correction capabilities in LLMs can also be reinforced through self-generated data (Kumar et al., 2024). Lastly, Hoffman et al. (2024); Hu et al. (2023) formulated the reasoning process as latent variable models, aligning LLMs towards more accurate reasoning with fewer annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND ANMOTIVULATION</head><p>We start by briefly introducing reasoning techniques (e.g., chain-of-thought <ref type="bibr" target="#b45">(Wei et al., 2022)</ref>, ReAct <ref type="bibr">(Yao et al., 2023a), etc)</ref>. Given a user query x x x, the standard procedure to sample the response y y y is to leverage an autoregressive pretrained LLMs π θ (parameterized by θ): y y y ∼ π θ (• | x x x). As for prompt-based reasoning techniques such as chain-of-thought <ref type="bibr" target="#b45">(Wei et al., 2022)</ref>, the LLM π θ is firstly asked to generate thoughts (a.k.a reasoning rationale) before generating the answers to the response:</p><formula xml:id="formula_1">x x x ′ := Reason(x x x) = x x x ⊕ Prompt Template of Thought, z z z ∼ π θ (• | x x x ′ ), y y y ∼ π θ (• | x x x ′ ⊕ z z z) ,</formula><p>where z z z is the thought or the reasoning rationale path, ⊕ indicates the concatenate operator, and the prompt template of the thought can be some hint prompt such as "Let's think s tep by step"<ref type="foot" target="#foot_0">foot_0</ref> .Empirically, people observe that there is a higher chance for the LLM π θ to generate the desired answer y y y following the above procedure than directly sampling the response y y y ∼ π θ (• | x x x).</p><p>From a statistical perspective, we hypothesize that good reasoning rationales can significantly improve the probability of generating good answers y y y: ∃ z z z, s.t. π θ (y y y| x x x ⊕ z z z) ≫ π θ (y y y| x x x).</p><p>Mistral-7B-Instruct-v0. To validate the hypothesis, we check the probability of the correct answers y y y on pretrained LLMs with or without reasoning rationales. In Figure <ref type="figure" target="#fig_0">2</ref>, we visualize the negative log probability of the correct answers on three different LLMs on GSM8K dataset <ref type="bibr">(Cobbe et al., 2021b)</ref>.</p><p>We have observed that when the LLMs are conditioned on the reasoning rationales, the probability of the correct answer is remarkably larger than without reasoning rationales. This suggests that good reasoning rationales help the LLMs generate desired answers for complex tasks by increasing their probability, giving them a higher chance of generating the correct answers.</p><p>The above observation inspires us that we can potentially further improve the reasoning ability of existing LLMs. One may find some surrogate objective to enhance the quality of the reasoning rationales or improve the ability of LLMs to leverage good reasoning rationales. In the following Proposition 1, we show that Self-Consistency Chain-of-Thought (CoT-SC) <ref type="bibr" target="#b44">(Wang et al., 2023)</ref>, which takes a majority vote of multiple reasoning rationales to improve the reasoning ability, approximates some surrogate objective.</p><p>Proposition 1. Denote the user query, model response, and reasoning rationale by x x x, y y y, z z z, respectively. The distribution of the majority vote answer of the K reasoning rationales obtained by CoT-SC approximates p M (y y y|x x x)</p><formula xml:id="formula_2">:= E z z z∼π θ (•| x x x) [π θ (y y y| x x x ⊕ z z z)], as K → ∞.</formula><p>Proof. Given a user query x x x, CoT-SC essentially follows the procedure: 1) Sample K i.i.d reasoning rationales together with model responses:</p><formula xml:id="formula_3">(z z z k , y y y k ) ∼ π θ (•|x x x), 1 ≤ k ≤ K.</formula><p>2) Take the majority vote of (y y y 1 , . . . , y y y K ). For a specific response y y y, its frequency can be calculated as F (y y y) := 1 K K k=1 1{y y y k = y y y}, where 1 is the indicator function. Then the expectation of F (y y y) is</p><formula xml:id="formula_4">E y y y1,...,y y y K F (y y y) = 1 K K k=1 E y y yi 1{y y y i = y} = 1 K K i=1 P y y yi∼π θ (•|x x x⊕z z zi) [y y y i = y y y] = 1 K K i=1 π θ (y y y|x x x ⊕ z z z i ) K→∞ -→ E z z z∼π θ (•|x x x) π θ (y y y|x x x ⊕ z z z).</formula><p>CoT-SC essentially leverages p M (y y y|x x x) := π θ (z z z|x x x) π θ (•| x x x ⊕ z z z)dz z z to obtain reasoning rationales and produce final correct answers. Inspired by the conclusion, we could leverage surrogate objectives like E z z z∼π θ (•| x x x) [ϕ(π θ (y y y| x x x ⊕ z z z))] to further enhance the reasoning ability of LLMs, where ϕ is some monotonic transform such as logarithm (log(•)). Further, we could also optimize the parameters of LLMs to enhance the reasoning abilities of existing LLMs during the training, so we can obtain LLMs with better reasoning abilities with the same inference time budget. In the following sections, we introduce the idea of optimizing LLMs to improve reasoning abilities without external feedback, by proposing a principled framework containing the surrogate objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZING THE REASONING PROCESS</head><p>In this section, we describe how to optimize the reasoning rationale without external feedback. Specifically, we introduce the objective for optimizing the reasoning rationale in Section 4.1 from a variational perspective of LLM training; we derive the gradient estimation for the new objective in Section 4.2, and discuss the sampling procedure together with reward shaping in Section 4.3. We summarize the proposed algorithm, LaTent Reasoning Optimization (LaTRO) in Algorithm 1, and illustrate the overall procedure in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LATENT REASONING OPTIMIZATION: A VARIATIONAL APPROACH</head><p>Suppose we are given a golden dataset D Gold := {(x x x i , y y y i )} N i=1 consisting of N query and answer pairs, where (x x x, y y y) denotes the query and the answer respectively. A standard finetuning procedure to fit the LLM π θ to the dataset D Gold is by likelihood maximization:</p><formula xml:id="formula_5">max θ E (x x x,y y y)∼DGold [log π θ (y y y | x x x)] ,<label>(1)</label></formula><p>where θ are the parameters of the LLM π θ to optimize. Based on the discussion in Section 3, it is more feasible to optimize π θ with additional reasoning rationale path z z z, compared with standard finetuning objective in Equation ( <ref type="formula" target="#formula_5">1</ref>). Hence, we can introduce another "reasoner" q(z z z|x x x) to sample the latent reasoning rationales that can help the optimization procedure of π θ . This is achievable by optimizing the following lower bound:</p><formula xml:id="formula_6">log π θ (y y y|x x x) = log π θ (y y y | x x x ⊕ z z z)π 0 (z z z | x x x)dz z z = log π θ (y y y | x x x ⊕ z z z) q(z z z|x x x) q(z z z|x x x) π 0 (z z z|x x x)dz z z ≥ max q(z z z|x x x) E q(z z z|x x x) log π θ (y y y| x x x ⊕ z z z) -D KL [q(z z z|x x x)||π 0 (z z z|x x x)] ,<label>(2)</label></formula><p>where π 0 is a prior reference LLM that regularizes the "reasoner" q(z z z|x x x), and the lower bound is achieved via Jensen's inequality <ref type="bibr" target="#b14">(Higgins et al., 2017)</ref>. Based on the literature of variational Bayes (Kingma, 2013), one can either learn and optimize q(z z z|x x x) via variational Expectation Maximization (EM) <ref type="bibr" target="#b1">(Abdolmaleki et al., 2018;</ref><ref type="bibr" target="#b26">Liu et al., 2022)</ref>, or introduce another parameterized LLM q ϕ (z z z|x x x) and optimize ϕ to amortize the cost. Additionally, from the discussion in Section 3, we know π θ itself can also serve as a naive "reasoner", since π θ is an autoregressive LLM.</p><p>To simplify the learning procedure, we propose to use π θ as the "reasoner" q(z z z|x x x). As a result, we can jointly learn one single LLM π θ , that is capable of generating good reasoning rationale together with providing correct answers given the query and its own generated reasoning rationale. To be more specific, we can define the learning objective as follows:</p><formula xml:id="formula_7">max θ J(θ) := E (x x x,y y y)∼DGold E z z z∼π θ (•|x x x) log π θ (y y y| x x x ⊕ z z z) R θ (z z z,y y y,x x x) -D KL [π θ (z z z|x x x)||π 0 (z z z|x x x)] ,<label>(3)</label></formula><p>where we specify the reference LLM π 0 to be the original π θ before the optimization. Furthermore, log π θ (y y y| x x x ⊕ z z z) in Equation ( <ref type="formula" target="#formula_7">3</ref>) can be viewed as the reward function R θ (z z z, y y y, x x x) to evaluate the quality of the rationale z z z given the pair (x x x, y y y), since the reasoning rationale z z z with higher likelihood log π θ (y y y| x x x ⊕ z z z) indicates that it would provide a higher probability for the model to answer the question correctly.</p><p>Remark By substituting log π θ (y y y| x x x ⊕ z z z) with R θ (z z z, y y y, x x x), Equation (3) exactly recovers the standard optimization objective defined in offline RL <ref type="bibr" target="#b23">(Levine et al., 2020)</ref>, RLHF <ref type="bibr" target="#b30">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b32">Rafailov et al., 2024)</ref> literature. Though Equation (3) unifies the learning procedure of the "reasoner" π θ (z z z|x x x) and the "reward" function R θ (z z z, y y y, x x x) := log π θ (y y y| x x x ⊕ z z z), we can break down these two procedures to analyze them separately. When we fix R θ (z z z, y y y, x x x) and optimize the "reasoner" π θ (z z z|x x x), the procedure can be interpreted as self-improvement learning, where we improve π θ (z z z|x x x) on self-generated synthetic reasoning rationale. When we fix π θ (z z z|x x x) and optimize R θ (z z z, y y y, x x x), the procedure can be interpreted as self-reward learning, where we learn the self-reward function log π θ (y y y| x x x ⊕ z z z). The procedure can also be considered finetuning optimization given the learned reasoning rationale and query. Fortunately, we can naturally enjoy the benefits of these two selflearning procedures with the new reasoning finetuning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GRADIENT ESTIMATION FOR LATRO</head><p>From previous RL literature, we know that estimating ∇ θ J(θ) in Equation ( <ref type="formula" target="#formula_7">3</ref>) involves the use of policy gradient methods, which usually suffers from high variances with the naive REINFORCE estimators <ref type="bibr" target="#b46">(Williams, 1992)</ref>. Inspired by the recent work on policy gradient for LLMs <ref type="bibr" target="#b2">(Ahmadian et al., 2024)</ref>, we also leverage the REINFORCE Leave-One-Out (RLOO) <ref type="bibr" target="#b21">(Kool et al., 2019)</ref> to optimize the "reasoner" π θ (z z z|x x x), where we can achieve lower variances of gradient estimation by sampling multiple rationales. We summarize the empirical gradient estimation for solving LaTRO in Proposition 2. Proposition 2. (LaTRO Gradient Estimation) Suppose we are given a set of training data D Gold := {x x x i , y y y i } N i=1 , we sample K i.i.d reasoning rationales z z z</p><formula xml:id="formula_8">(i) 1 , z z z (i) 2 , . . . , z z z (i) K ∼ π θ (•|x x x i )</formula><p>for each query and answer pair (x x x i , y y y i ). The empirical gradient estimator for ∇ θ J(θ) is expressed as</p><formula xml:id="formula_9">∇ θ J(θ) := 1 N K N i=1 K k=1 ∇ θ log π θ (z z z (i) k | x x x i ) • A (i) k + ∇ θ log π θ (y y y i | x x x i ⊕ z z z (i) k ) ,<label>(4)</label></formula><p>with</p><formula xml:id="formula_10">A (i) k = r(z z z (i) k ) - 1 K -1 K j̸ =k r(z z z (i) j ) , r(z z z (i) k ) := log π θ (y y y i | x x x i ⊕ z z z (i) k ) -β log π θ (z z z (i) k | x x x i ) π 0 (z z z (i) k | x x x i )</formula><p>, Algorithm 1: LaTent Reasoning Optimization (LaTRO) Input: Language model π θ , learning rate η, KL penalty factor β, MC sample size K, maximum generation length L, sample temperature T , number of epochs M , training dataset D Gold . Output: An optimized language model π θ .</p><p>def generate(π, x x x, K, L, T ):</p><p>Given an autoregressive language model π, input x, sample K sequences of length L from the distribution π(•|x x x) at temperature T . return K sampled sequences Intialize reference language model π 0 as π θ for epoch in range(M ):</p><p>for x x x i , y y y i in D Gold : z z z</p><formula xml:id="formula_11">(i) 1 , . . . , z z z (i) K ← generate(π θ , x x x i , K, L, T ) Estimate ∇ θ J(θ) with Proposition 2 θ ← θ + η∇ θ J(θ) return π θ</formula><p>where β ≥ 0 is the coefficient to control the KL penalty. The proof can be found in Appendix A.1.</p><p>The first gradient term in Equation ( <ref type="formula" target="#formula_9">4</ref>) serves as policy gradient to improve the ability of the LLM π θ to generate high-quality reasoning rationales, and log π θ (y y y|x x x ⊕ z z z) can be viewed as the evaluator for reasoning rationale, which is further used to calculate the advantages. The second gradient term in Equation ( <ref type="formula" target="#formula_9">4</ref>), which is the gradient of supervised finetuning loss, essentially helps the LLM π θ to leverage the reasoning rationales to produce correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PRACTICAL CONSIDERATIONS</head><p>To reduce computation overhead and better control the sampling of reasoning rationales during training, we limit their maximum token length to L. The rationale ends either at the EOS token or at the start of a predefined answer template (e.g., "The answer is"). We then use the truncated rationale z z z, along with the query x x x and the answer z z z, for further computation.</p><p>We also encourage the LLM to finish its reasoning process with L tokens. Inspired by the implementation of the RLOO trainer in the TRL library <ref type="bibr" target="#b41">(von Werra et al., 2020)</ref> , we introduce a constant penalty for rationales truncated by the maximum token length L. This penalty encourages the generation of rationales that fit within the specified token limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SETUP</head><p>We evaluate the performance of the proposed method across two datasets: a mathematical reasoning dataset (GSM8K, <ref type="bibr">Cobbe et al. (2021b)</ref>) and a logical reasoning dataset (ARC-Challenge, <ref type="bibr" target="#b38">Talmor et al. (2019)</ref>). The sizes of the datasets are listed in Table <ref type="table" target="#tab_1">1</ref>. Training. For each dataset, we fine-tune three base models: Phi-3.5-mini-instruct (Abdin et al., 2024), Mistral-7B-Instruct-v0.3 <ref type="bibr">(Jiang et al., 2023)</ref>, and Meta-Llama-3.1-8B-Instruct <ref type="bibr">(Dubey et al., 2024)</ref>, abbreviated as Phi-3.5, Mistral-7B, and Llama-3.1-8B, respectively. We provide two baseline comparisons: the base model and the supervised fine-tuned (SFT) model. For GSM8K, LaTRO fine-tuning excludes golden rationales from the solutions in the training set, while the SFT model is trained using golden rationales. For ARC-Challenge, as suggested in <ref type="bibr" target="#b55">(Zheng et al., 2024)</ref>, the model is trained to generate answers to the text of multiple-choice questions rather than selecting labels. Since no golden rationales are available for ARC-Challenge, the SFT model is trained to directly generate answers.</p><p>Evaluation. For GSM8K, we evaluate all models with CoT prompting, and for ARC-Challenge, we evaluate the SFT baseline with direct answer generation, while the base model and the LaTRO fine-tuned model with CoT prompting. All evaluations are conducted with zero-shot prompts. We report both greedy decoding (GD) results and self-consistency (with temperature T = 1) results. We choose a self-consistency sample size k = 8 (maj@8) in Table <ref type="table" target="#tab_2">2</ref> after observing that more than 8 samples did not bring further performance improvement (see Figure <ref type="figure">3</ref> (b) for details).</p><p>Implementation. LaTRO is implemented on the high level as in Algorithm 1, with additional engineering techniques as discussed in section 4.3. LaTRO is implemented using the widely recognized transformers <ref type="bibr" target="#b47">(Wolf et al., 2020)</ref> and TRL (von Werra et al., 2020) libraries, with PyTorch <ref type="bibr" target="#b3">(Ansel et al., 2024)</ref> as backend. DeepSpeed ZeRO <ref type="bibr" target="#b33">(Rasley et al., 2020)</ref> is used in stage 3, along with Flash Attention 2 <ref type="bibr" target="#b9">(Dao et al., 2022)</ref> to enhance training efficiency. The models were trained on a machine equipped with 8xH100 80GB GPUs, using bfloat16 precision.</p><p>Hyperparameters. AdamW optimizer with a learning rate of 5 × 10 -7 , no warm-up steps, and a linear decay strategy is used. The Monte Carlo (MC) sample size K = 16 and the batch size of the data loader 3 are predetermined, resulting in an effective batch size of 48. Gradient accumulation steps and training batch size are subsequently adjusted to prevent out-of-memory errors during training. A temperature of T = 1 is used for MC sampling, and a penalty factor γ = 2 is applied for incomplete rationales. The KL penalty is set at β = 0.05 for GSM8K and 0.25 for ARC-Challenge.</p><p>Except for the results presented in Section 5.3, the maximum generation length is maintained at L = 500. We train all models up to six epochs for GSM8K, and 12 epochs for ARC-Challenge. The checkpoint with best test accuracy is chosen.</p><p>For the SFT baseline experiments, we use a batch size of 32 and adjust the learning rate to ensure that the evaluation loss decreases and finally converges. All SFT baselines are trained for a maximum of 12 epochs. The checkpoint with the best test accuracy is selected.</p><p>In addition to the main quantitative results, we conduct ablation studies on two factors: 1. The maximum generation length L, where we study the effects of tuning L in both training and inference times; 2. The self-consistency samples k, where we explore to what extent LaTRO can still benefit from inference-time scaling.</p><p>The main quantitative results, qualitative analysis of sample responses, and results of the ablation study are presented in Sections 5.2 to 5.4, respectively. Additional details on our prompt templates and more samples can be found in Appendices B and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RESULTS</head><p>In this subsection, we present evaluation results that demonstrate how effectively LaTRO enhances the reasoning abilities of LLMs on downstream datasets. The detailed results are provided inTable 2.</p><p>For the GSM8K dataset, LaTRO fine-tuned models outperform all base models by up to 19.5% (Mistral-7B, 47.8% → 67.3%) and show an average improvement of 12.5% across the three models examined with greedy decoding. The greatest improvement margin is observed for Mistral-7B, while the smallest is seen for Llama-3.1-8B, consistent with our initial findings in Figure <ref type="figure" target="#fig_0">2</ref>, where Mistral-7B exhibited the lowest log probability for directly answering questions and Llama-3.1-8B exhibited the highest. With self-consistency, the improvements are by up to 16.5% (Phi-3.5, 74.0% → 90.5%) and the average improvement is 13.1%. Furthermore, LaTRO models demonstrate superior performance relative to SFT baselines, with an average improvement of 9.6% for greedy decoding and 13.2% for self-consistency. It is worth noting that for the SFT baseline of Llama-3.1-8B, overfitting on the test set is still observed after tuning the learning rate.</p><p>For ARC-Challenge, LaTRO fine-tuned models still outperform the baselines, though with a smaller margin. When using greedy decoding, the improvements over the base models are up to 1.6% with an average increase of 1%. We see more increment with self-consistency, where the improvement margins are on average 2.4%. Comparing to SFT baslines, we find that all three models are very sensitive when fine-tuning to directly generate the answer of ARC-Challenge questions. They perform even inferior to the unoptimized base models. When using greedy decoding, the improvements of LaTRO fine-tuned models over the SFT baselines are on an average of 5.2%, and by up to 6% (Llama-3.1-8B). In the case of self-consistency, LaTRO performs better than the base models by an average of 2.4%, and surpasses the SFT models by an average of 8.1%. On the less surprising results compared to GSM8K, we conjecture that for ARC-Challenge, the models are already good at producing the answer either directly or through CoT prompting. Hence, further optimization of the reasoning process did not yield significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ABLATION STUDY</head><p>In this subsection, we present our ablation study on the effect of different parameters in LaTRO. For consistency, we fix the base model to Phi-3.5 and the dataset to GSM8K throughout the ablation experiments.</p><p>How many tokens are enough? <ref type="bibr" target="#b25">Liu et al. (2024)</ref> demonstrated that when the input length is n, a transformer model with a hidden size of O(log n) can solve problems equivalent to Boolean circuits of size m, using m CoT steps. However, the empirical determination of sufficient CoT tokens for optimal performance remains underexplored. In this section, we report zero-shot accuracy with generation length L ranging from 200 to 1000 tokens at inference time. Additionally, a Phi-3.5 model is fine-tuned with L = 200 for comparison. We distinguish two LaTRO fine-tuned models, referred to as LaTRO and LaTRO 200 . As shown in Figure <ref type="figure">3</ref>(a) accuracy gains plateau when L ≥ 500, suggesting 500 tokens might suffice for grade school math problems. In contrast, limiting L to 200 reduces accuracy, unless the model is trained accordingly. Interestingly, LaTRO significantly improves performance under this constraint by training the model to generate more concise rationales. Figure <ref type="figure">3</ref>: Ablation study results on GSM8K with base model Phi-3.5. In (a), the x-axis represents various maximum length L of reasoning rationales, y-axis is the accuracy, and the plot shows the zero-shot performance v.s. various maximum token lengths for different methods. In (b), the x-axis represents the # of sampled reasoning rationales, the y-axis is the accuracy, and the plot shows the zero-shot performance v.s. the # of reasoning rationales used in the majority vote.</p><p>This finding implies LaTRO can also train LLMs to condense rationales for limited inference-time compute. A sample from LaTRO 200 is provided in Appendix C for qualitative analysis.</p><p>Is inference-time scaling still beneficial? While LaTRO shifts much of the reasoning computation to training time, it can still benefit from the inference-time scaling. To explore this, We evaluate the models again using self-consistency with the number of samples k ranging from 1 to 32. As shown in Figure <ref type="figure">3</ref> (b), with k = 8 rationales and majority voting, LaTRO achieves 90% accuracy on GSM8K. This indicates that LaTRO equipped with self-consistency can further enhance performance, even with LaTRO's focus on training-time optimization.</p><p>Question: Frankie and Binkie went bowling together. Frankie's score was 15 better more than twice as high as Binkie's. If Binkie bowled a score of 90, what was Frankie's score? Let's think step by step. Response (Base model): Let's break this problem down into steps: 1. We know that the difference between Frankie's score (F) and Binkie's score (B) is 15. So we can express Frankie's score as F = B + 15 2. We also know that Frankie's score is twice as high as Binkie's score. So we can write another equation: F = 2B 3. Now, we have two equations with two unknowns. Let's solve for Frankie's score (F) by setting the two equations equal to each other: -2B = B + 15 -Multiply both sides by 1 to get rid of the fraction: 2B -B = 15 -Simplify: B = 15 (since subtracting B from 2B results in 15) 4. Now that we know Binkie's score, we can find Frankie's score using either of the equations we created earlier. Let's use the first one (F = B + 15): -F = 15 + 15 -Frankie's score is 30 more than Binkie's score, which is 15 in this case. So Frankie's score is 15 + 15 = 30 + 15 = 45 The final answer is that Frankie's score was 45. Reponse (SFT): Twice as many as Binkie's 90 score is equal to 2*90= «2*90=180»180. Therefore, 180-(180-15)=165 was the number Frankie bowled. The answer is 165 Response (LaTRO): Here's how to solve the problem: 1. We know that Binkie's score is 90. 2. The problem states that Frankie's score was 15 better more than twice as high as Binkie's score. This means that Frankie's score is 15 + (2 * 90) = 15 + 180 = 195. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CASE STUDY</head><p>We take a closer look at the responses generated by the LaTRO fine-tuned models. We select a question from GSM8K and compare the responses from the base, the SFT model, and the LaTRO finetuned model. We choose the set of responses from the Mistral-7B models that we evaluated. As can be seen in Figure <ref type="figure" target="#fig_2">4</ref>, the base model not only generates a lengthy response, it also makes a logical mistake at the first step, where the correct equation to establish here is "F = 2B + 15". The SFT model simplifies the answer and makes the first step correct. However, in the second step it first makes a wrong equation, then makes an arithmetic error when evaluating this equation. Further, LaTRO can give a concise and correct answer. We include more sample responses in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In conclusion, this work introduces LaTRO, a principled framework for optimizing language models' reasoning capabilities without external feedback or reward models. By formulating reasoning as sampling from a latent distribution and leveraging self-rewarding, LaTRO enables models to concurrently improve both their reasoning process and ability to evaluate reasoning quality. Our extensive experiments across multiple model architectures and tasks demonstrate significant performance gains, with LaTRO outperforming baseline models and supervised fine-tuning approaches. These findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked through our proposed optimization approach, representing a significant step towards creating more intelligent systems that can self-evolve their problem-solving capabilities.</p><p>While LaTRO shows promising results, there are some limitations to consider. The computational cost of sampling multiple rationales during training could be prohibitive for very large models. Future work could explore ways to reduce this computational overhead, such as using more efficient sampling techniques or adaptive rationale generation. Other promising directions include investigating the applicability of LaTRO to a wider range of reasoning tasks beyond math and science problems, and exploring how to conduct multi-step reasoning learning to to enhance reasoning capabilities further. Despite these limitations, our contributions advance both the state-of-the-art in LLM reasoning capabilities and provide valuable insights into the nature of LLM alignment and its potential for self-improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL DETAILS ON OUR THEORETICAL FRAMEWORK</head><p>A.1 PROOF OF PROPOSITION 2</p><p>Proof. We restate the objective as follows:</p><formula xml:id="formula_12">J(θ) := E (x x x,y y y)∼DGold E z z z∼π θ (•|x x x) log π θ (y y y| x x x ⊕ z z z) -βD KL [π θ (z z z|x x x)||π 0 (z z z|x x x)] , = E (x x x,y y y)∼DGold E π θ (z z z|x x x) [log π θ (y y y|x x x ⊕ z z z) -β log π θ (z z z|x x x) + β log π 0 (z z z|x x x)] ,</formula><p>where β &gt; 0 is a positive coefficient to control the regularization strength. We take the gradient w.r.t θ at each sample pair (x x x, y y y), and we get</p><formula xml:id="formula_13">∇ θ J(θ; x x x, y y y) := ∇ θ (π θ (z z z|x x x))(log π θ (y y y|x x x ⊕ z z z) -β log π θ (z z z|x x x) + β log π 0 (z z z|x x x))dz z z = E π θ (z z z|x x x) ∇ θ log π θ (z z z|x x x) log π θ (y y y|x x x ⊕ z z z) -β log π θ (z z z|x x x) π 0 (z z z|x x x) + E π θ (z z z|x x x) [∇ θ log π θ (y y y|x x x ⊕ z z z) -β∇ θ log π θ (z z z|x x x)] .</formula><p>We further define r(z z z) := log π θ (y y y|x x x ⊕ z z z) -β log π θ (z z z|x x x) π0(z z z|x x x) , and use the fact that</p><formula xml:id="formula_14">E π θ (z z z|x x x) [∇ θ log π θ (z z z|x x x)] = π θ (z z z|x x x) ∇ θ π θ (z z z|x x x)</formula><p>π θ (z z z|x x x) dz z z = ∇ θ π θ (z z z|x x x)dz z z = 0. we obtain the final gradient as</p><formula xml:id="formula_15">∇ θ J(θ; x x x, y y y) = E π(z z z|x x x) [∇ θ log π θ (z z z|x x x) • r(z z z) + ∇ θ log π θ (y y y|x x x ⊕ z z z)] .</formula><p>And when we use RLOO estimator with empirical samples, we can replace above gradient estimation with empirical samples, which gives us the following result:</p><formula xml:id="formula_16">∇ θ J(θ) := 1 N K N i=1 K k=1 ∇ θ log π θ (z z z (i) k | x x x i ) • A (i) k + ∇ θ log π θ (y y y i | x x x i ⊕ z z z (i) k ) , with A (i) k = r(z z z (i) k ) - 1 K -1 K j̸ =k r(z z z (i) j ) , r(z z z (i) k ) := log π θ (y y y i | x x x i ⊕ z z z (i) k ) -β log π θ (z z z (i) k | x x x i ) π 0 (z z z (i) k | x x x i )</formula><p>, which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROMPT TEMPLATES</head><p>In this section, we list all prompt templates used in our experiments. For the GSM8K dataset, we used a minimum prompt template. For the ARC-Challenge dataset, we use a CoT template for the base model and LaTRO fine-tuning, and another template direct answer generation in SFT. {question} Let's think step by step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SAMPLE RESPONSES</head><p>Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Let's think step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response (Base model):</head><p>Step 1: Determine the cost of one regular glass and one discounted glass. One regular glass costs $5. Every second glass costs 60% of the price of a regular glass, so: 60% of 5 = 0.60 * 5 = $3</p><p>Step 2: Calculate the total number of regular and discounted glasses. Kylar wants to buy 16 glasses in total. Since every second glass is discounted, we can divide the total number of glasses by 2 to find the number of discounted glasses: 16 glasses / 2 = 8 discounted glasses Now, subtract the number of discounted glasses from the total number of glasses to find the number of regular glasses: 16 glasses -8 discounted glasses = 8 regular glasses Step (did not finish) Response (LaTRO 200 ): Let's break it down: 1. Kylar needs to buy 16 glasses. 2. Every second glass costs 60% of the original price ($5), which is $3. 3. So, for every pair of glasses (one full price and one discounted), Kylar pays $5 + $3 = $8. 4. Since he needs 16 glasses, he buys 8 pairs. 5. Therefore, Kylar needs to pay 8 pairs * $8 per pair = $64. So, Kylar needs to pay $64 for the 16 glasses. The answer is 64. Question: Water, ice, and steam all have different temperatures. What is the order from coldest to hottest? Options: <ref type="bibr">['ice, water, steam', 'ice, steam, water','steam, ice, water','steam, water, ice']</ref> Response (Base model): To determine the order from coldest to hottest, let's evaluate each option.</p><p>-Ice: It is the solid form of water and is typically around 0°C (32°F).</p><p>-Water: It is the liquid form of water and is usually around 100°C (212°F) or 0°C (32°F) at standard pressure and temperature conditions, respectively. -Steam: It is the gaseous form of water and is typically at a temperature equal to or greater than 100°C (212°F) at standard pressure conditions, depending on the pressure and other factors. Option 1: 'ice, water, steam' -This order seems incorrect since steam is typically hotter than water at standard conditions. Option 2: 'ice, steam, water' -This order is incorrect because ice is the coldest, but the relative order of water and steam is opposite of what we know. Option 3:'steam, ice, water' -This order is incorrect since ice is colder than steam. Option 4:'steam, water, ice' -This order seems the most plausible, considering that steam is hotter than water and water is hotter than ice. The answer is 'steam, water, ice'. Response (SFT): water, ice, steam Response (LaTRO): To determine the order from coldest to hottest, let's analyze each option: 1. Ice has a solid form and a fixed temperature, which is typically around 0°C (32°F) at standard pressure. 2. Water is liquid and exists at a temperature of 0°C (32°F) at standard pressure, melting point of ice. 3. Steam is the gaseous state of water and is typically around 100°C (212°F) at standard pressure. Based on this information, we can now order them from coldest to hottest: First, we have ice, as it is the coldest. Then, we have water at 0°C, which is the middle temperature. Lastly, we have steam, which is the hottest. So, the order from coldest to hottest is 'ice, water, steam'. The answer is <ref type="bibr">['ice, water, steam']</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average negative log probabilities of LLMs to generate correct responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Zero-shot accuracy with different L.(b) Zero-shot maj@k accuracy with different k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample responses of a GSM8K question from Mistral-7B models, the errors are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 7 :</head><label>57</label><figDesc>Figure 5: CoT template for GSM8K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sample responses of a GSM8K question, from Phi-3.5 models, maximum generation length L = 200. The base model does not finish the generation, while the LaTRO model generates a short and correct answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Sample responses of an ARC-Challenge question, from Llama-3.1 models, the errors are highlighted in red. The base model shows knowledge about basic physics but makes a logical error on the order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Size of the datasets</figDesc><table><row><cell>Name</cell><cell cols="2">Training Evaluation</cell></row><row><cell>GSM8K</cell><cell>7473</cell><cell>1319</cell></row><row><cell>ARC-Challenge</cell><cell>1119</cell><cell>1172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot accuracy (%) comparison between LaTRO and the baselines on GSM8K and ARC-Challenge datasets. The models are fine-tuned on corresponding training datasets. The base model are marked with "N/A" in the training method. GD stands for greedy decoding at inference time and maj@8 stands for self-consistency with 8 samples. The models are evaluated by default using CoT, except that † indicates the direct answer generation is applied during evaluation.</figDesc><table><row><cell>Base Model</cell><cell cols="4">Training Method Inference Method GSM8K ARC-Challenge</cell></row><row><cell></cell><cell>N/A</cell><cell>GD maj@8</cell><cell>72.9 74.0</cell><cell>85.1 86.0</cell></row><row><cell>Phi-3.5</cell><cell>SFT</cell><cell>GD maj@8</cell><cell>75.8 77.1</cell><cell>81.0  † 80.5  †</cell></row><row><cell></cell><cell>LaTRO</cell><cell>GD maj@8</cell><cell>87.6 90.5</cell><cell>86.4 87.5</cell></row><row><cell></cell><cell>N/A</cell><cell>GD maj@8</cell><cell>47.8 58.2</cell><cell>74.1 74.1</cell></row><row><cell>Mistral-7B</cell><cell>SFT</cell><cell>GD maj@8</cell><cell>57.2 59.9</cell><cell>70.0  † 70.6  †</cell></row><row><cell></cell><cell>LaTRO</cell><cell>GD maj@8</cell><cell>67.3 73.8</cell><cell>74.3 78.9</cell></row><row><cell></cell><cell>N/A</cell><cell>GD</cell><cell>76.8</cell><cell>81.4</cell></row><row><cell></cell><cell></cell><cell>maj@8</cell><cell>79.7</cell><cell>84.4</cell></row><row><cell>Llama-3.1-8B</cell><cell>SFT</cell><cell>GD maj@8</cell><cell>73.2 74.7</cell><cell>77.0  † 76.4  †</cell></row><row><cell></cell><cell>LaTRO</cell><cell>GD maj@8</cell><cell>80.1 87.0</cell><cell>83.0 85.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We omit the difference between x ′ and x for convenience in the latter notation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">Ade</forename><surname>Marah I Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Bahree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><forename type="middle">S</forename><surname>Bakhtiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parul</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emman</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junheng</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwoo</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahoud</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Kurilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weishung</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barun</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Perez-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyang</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corby</forename><surname>Radmilac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sambudha</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><surname>Saarikivi ; Sonali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengruidong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Lyna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiren</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.14219</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2404.14219" />
		<editor>Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu,</editor>
		<imprint/>
	</monogr>
	<note>Phi-3 technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06920</idno>
		<title level="m">Maximum a posteriori policy optimisation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14740</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Voznesensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geeta</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Chourdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherlock</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshiteej</forename><surname>Kalambarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark-Albert</forename><surname>Saroufim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">Yukio</forename><surname>Siraichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eikan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">268794728</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nyczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v38i16.29720</idno>
		<ptr target="https://doi.org/10.1609/aaai.v38i16.29720" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Wooldridge</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sriraam</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><surname>Natarajan</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014-02-20">2014. February 20-27, 2024</date>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large language monkeys: Scaling inference compute with repeated sampling</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21787</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YfZ4ZPt8zd" />
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>CoRR, abs/2110.14168</idno>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/67" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sanmi Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022, 2022</date>
		</imprint>
	</monogr>
	<note>d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The llama 3 herd of models</title>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aobo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archi</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archie</forename><surname>Sravankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Hinsvark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austen</forename><surname>Gregerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ava</forename><surname>Spataru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Biron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobbie</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Caucheteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaya</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Touret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinne</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Allonsius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Pintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehab</forename><surname>Albadawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elina</forename><surname>Lobanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabrielle</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Lewis Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Nail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Korevaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arrieta</forename><surname>Imanol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">M</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Geffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Vranes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeet</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelmer</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Van Der Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Billock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Spisak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Rocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Johnstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Vasuden Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Plawiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.21783</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.21783" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.14196</idno>
		<title level="m">Deepseek-coder: When the large language model meets programming-the rise of code intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Havrilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharath</forename><surname>Chandra Raparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoforos</forename><surname>Nalmpantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Zhuravinskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04642</idno>
		<title level="m">Teaching large language models to reason with reinforcement learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training chain-of-thought via latent-variable inference</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Douglas Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younesse</forename><surname>Elmoznino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Kaddar</surname></persName>
		</author>
		<author>
			<persName><surname>Lajoie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04363</idno>
		<title level="m">Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large language models cannot self-correct reasoning yet</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Huaixiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinying</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01798</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><surname>Sayed</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.06825</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.06825" />
	</analytic>
	<monogr>
		<title level="j">Mistral</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">E</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06770</idno>
		<title level="m">Swe-bench: Can language models resolve real-world github issues? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Buy 4 REINFORCE samples, get a baseline for free! In Deep Reinforcement Learning Meets Structured Prediction</title>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lgTGL5DE" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 Workshop</title>
		<meeting><address><addrLine>New Orleans, Louisiana, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">May 6, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training language models to self-correct via reinforcement learning</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Baumli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shariq</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colton</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12917</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01643</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Hunter Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.20050</idno>
		<title level="m">Ilya Sutskever, and Karl Cobbe. Let&apos;s verify step by step</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chain of thought empowers transformers to solve inherently serial problems</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=3EWTEy9MTM" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Constrained variational policy optimization for safe reinforcement learning</title>
		<author>
			<persName><forename type="first">Zuxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhepeng</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladislav</forename><surname>Isenbaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13644" to="13668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improve mathematical reasoning in language models by automated process supervision</title>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06592</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhisattwa</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2023/file/91" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="46534" to="46594" />
		</imprint>
	</monogr>
	<note>edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to reason with LLMs</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/learning-to-reason-with-llms/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generative language modeling for automated theorem proving</title>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03393</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3406703</idno>
		<ptr target="https://doi.org/10.1145/3394486.3406703" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Rajesh</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
		</editor>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">August 23-27, 2020. 2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Rein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betty</forename><forename type="middle">Li</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Dirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12022</idno>
		<title level="m">Gpqa: A graduate-level google-proof q&amp;a benchmark</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05802</idno>
		<title level="m">Self-critiquing models for assisting human evaluators</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reflexion: language agents with verbal reinforcement learning</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>b44b878bb782e6954cd888628510e90-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">To cot or not to cot? chainof-thought helps mainly on math and symbolic reasoning</title>
		<author>
			<persName><forename type="first">Zayne</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangcong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manya</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasann</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno>CoRR, abs/2409.12183</idno>
		<ptr target="http://arxiv.org/abs/2409.12183" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1421</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1421" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Solving olympiad geometry without human demonstrations</title>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Trieu H Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="issue">7995</biblScope>
			<biblScope unit="page" from="476" to="482" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reft: Reasoning with reinforced fine-tuning</title>
		<author>
			<persName><forename type="first">Luong</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoran</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7601" to="7614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Younes</forename><surname>Leandro Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyi</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/trl" />
	</analytic>
	<monogr>
		<title level="j">Trl: Transformer reinforcement learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Chain-of-thought reasoning without prompting</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10200</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=1PL1NIMMrw" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2023</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sanmi Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An empirical analysis of compute-optimal inference for problem-solving with language models</title>
		<author>
			<persName><forename type="first">Yangzhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00724</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=WE_vluYUL-X" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelby</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rithesh</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02151</idno>
		<title level="m">Retrospective large language agents with policy gradient optimization</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Star: Bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15476" to="15488" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Quiet-star: Language models can teach themselves to think before speaking</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georges</forename><surname>Harik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varuna</forename><surname>Jayasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09629</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Diversity empowers intelligence: Integrating expertise of software engineering agents</title>
		<author>
			<persName><forename type="first">Kexun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rithesh</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.07060</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Large language models are not robust multiple choice selectors</title>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=shr9PXz7T0" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
