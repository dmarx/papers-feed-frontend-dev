<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Platonic Representation Hypothesis</title>
				<funder ref="#_sDDET38">
					<orgName type="full">MIT Quest for Intelligence, NSF</orgName>
				</funder>
				<funder ref="#_razAkRk">
					<orgName type="full">ONR MURI</orgName>
				</funder>
				<funder ref="#_SuXSuMr">
					<orgName type="full">Packard Fellowship</orgName>
				</funder>
				<funder ref="#_9c564As">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_F64ZK4s #_puDn9Kt #_2Q69e2f #_6JdpPat #_ntVTBxD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Brains, Minds, and Machines</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA Machine Common Sense</orgName>
					<orgName type="abbreviated">MCS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-25">25 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Cheung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
						</author>
						<title level="a" type="main">The Platonic Representation Hypothesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-25">25 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">A6FA1D581BBA8724037E4FC4313C86E7</idno>
					<idno type="arXiv">arXiv:2405.07987v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>phillipi</term>
					<term>github</term>
					<term>io/prh github</term>
					<term>com/minyoungg/platonic-rep</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>AI systems are rapidly evolving into highly multifunctional entities. For example, whereas in the past we had specialpurpose solutions for different language processing tasks (e.g., sentiment analysis, parsing, dialogue), modern large language models (LLMs) are competent at all these tasks using a single set of weights <ref type="bibr" target="#b116">(Srivastava et al., 2022)</ref>. Unified systems are also being built across data modalities: instead of using a different architecture for processing images versus text, recent models, such as GPT4-V (Ope-nAI, 2023), Gemini <ref type="bibr" target="#b35">(Google, 2023)</ref>, and LLaVA <ref type="bibr" target="#b69">(Liu et al., 2023)</ref>, handle both modalities with a combined architecture. More and more systems are built off of general-purpose pretrained backbones, sometimes called foundation models <ref type="bibr" target="#b13">(Bommasani et al., 2021)</ref>, that support a large range of tasks, including robotics <ref type="bibr" target="#b28">(Driess et al., 2023;</ref><ref type="bibr" target="#b14">Brohan</ref> Proceedings of the 41 st International Conference on Machine <ref type="bibr">Learning, Vienna, Austria. PMLR 235, 2024.</ref> Copyright 2024 by the author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Platonic Representation Hypothesis</head><p>Neural networks, trained with different objectives on different data and modalities, are converging to a shared statistical model of reality in their representation spaces.</p><p>Figure <ref type="figure">1</ref>. The Platonic Representation Hypothesis: Images (X) and text (Y ) are projections of a common underlying reality (Z). We conjecture that representation learning algorithms will converge on a shared representation of Z, and scaling model size, as well as data and task diversity, drives this convergence. <ref type="bibr">et al., 2023)</ref>, bioinformatics <ref type="bibr" target="#b76">(Ma et al., 2024)</ref>, and healthcare <ref type="bibr" target="#b117">(Steinberg et al., 2021)</ref>. In short, AI systems are becoming increasingly homogeneous in both their architectures and their capabilities. This paper explores one aspect of this trend: representational convergence. We argue that there is a growing similarity in how datapoints are represented in different neural network models. This similarity spans across different model architectures, training objectives, and even data modalities.</p><p>What has led to this convergence? Will it continue? And ultimately, where does it end? Our central hypothesis, stated above in Figure <ref type="figure">1</ref>, is that there is indeed an endpoint to this convergence and a principle that drives it: different models are all trying to arrive at a representation of reality, meaning a representation of the joint distribution over events in the world that generate the data we observe. Figure <ref type="figure">1</ref> conveys this hypothesis: there exists a real world (labeled Z), which we measure with various sensors, such as the camera shown to the left (X). Other projections of these measurements, such as the textual description shown, can be produced from the first set of measurements or mediated by some other set of measurements, e.g., touch or other camera views (dotted arrow from X to Y )<ref type="foot" target="#foot_0">foot_0</ref> . Representation learning algorithms find vector embeddings that statistically model the various measurements and projections. The resulting vector embeddings are all derived from the underlying reality in Z and thereby become aligned. As models are trained on more data and for more tasks, they require representations that capture more and more information about Z, and hence alignment toward Z increases toward a convergent point as a function of scale.</p><p>We call this converged hypothetical representation the "platonic representation" in reference to Plato's Allegory of the Cave <ref type="bibr">(Plato,</ref><ref type="bibr">c. 375 BC)</ref>, and his idea of an ideal reality that underlies our sensations. The training data for our algorithms are shadows on the cave wall, yet, we hypothesize, models are recovering ever better representations of the actual world outside the cave. This idea is not unique to Plato; our hypothesis is also related to the notion of "convergent realism" <ref type="bibr" target="#b86">(Newton-Smith, 1981;</ref><ref type="bibr" target="#b98">Putnam, 1982;</ref><ref type="bibr" target="#b25">Doppelt, 2007;</ref><ref type="bibr" target="#b42">Hardin &amp; Rosenberg, 1982)</ref> in the philosophy of science (i.e., that science is converging on truth), and to many arguments that have been put forth in the representation learning literature (e.g., <ref type="bibr">Tian et al. (2020a)</ref>; <ref type="bibr" target="#b143">Zimmermann et al. (2021)</ref>; <ref type="bibr" target="#b103">Richens &amp; Everitt (2024)</ref>; <ref type="bibr" target="#b15">Cao &amp; Yamins (2024)</ref>).</p><p>Also closely related to our hypothesis is the "Anna Karenina scenario" described by <ref type="bibr" target="#b7">Bansal et al. (2021)</ref>, referring to the possibility that all well-performing neural nets represent the world in the same way. We discuss the evidence they give for this possibility in Section 2<ref type="foot" target="#foot_1">foot_1</ref> . The platonic representation hypothesis refers to the situation where we are in an Anna Karenina scenario and the "happy representation" that is converged upon is one that reflects a statistical model of the underlying reality. We discuss the potential nature of this statistical model in more detail in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Representations are converging</head><p>Preliminaries We restrict our attention to representations that are vector embeddings. We characterize such a repre-sentation by the similarity structure it induces, referred to as its kernel. Kernels are commonly used to assess representations <ref type="bibr" target="#b60">(Kornblith et al., 2019;</ref><ref type="bibr" target="#b58">Klabunde et al., 2023)</ref>; this can be justified by the fact that they capture the relative structures among data samples, which are also the learning signal for many machine learning algorithms <ref type="bibr" target="#b3">(Aronszajn, 1950;</ref><ref type="bibr" target="#b111">Smola &amp; Schölkopf, 1998)</ref>. Following prior literature, we define representational alignment as a measure of the similarity of the similarity structures induced by two representations, i.e., a similarity metric over kernels. We give the mathematical definition of these concepts below:</p><p>• A representation is a function f : X → R n that assigns a feature vector to each input in some data domain X . • A kernel, K : X × X → R, characterizes how a representation measures distance/similarity between datapoints. K(x i , x j ) = ⟨f (x i ), f (x j )⟩, where ⟨ • , • ⟩ denotes inner product, x i , x j ∈ X and K ∈ K. • A kernel-alignment metric, m : K × K → R, measures the similarity between two kernels, i.e., how similar is the distance measure induced by one representation to the distance measure induced by another. Examples include Centered Kernel Distance (CKA) <ref type="bibr" target="#b60">(Kornblith et al., 2019)</ref>, SVCCA <ref type="bibr" target="#b102">(Raghu et al., 2017)</ref>, and nearest-neighbor metrics <ref type="bibr" target="#b58">(Klabunde et al., 2023)</ref>.</p><p>In our experiments, we use a mutual nearest-neighbor metric that measures the mean intersection of the k-nearest neighbor sets induced by two kernels, K 1 and K 2 , normalized by k. This metric is a variant of those proposed in <ref type="bibr" target="#b96">Park et al. (2024)</ref>, <ref type="bibr" target="#b58">Klabunde et al. (2023)</ref> and <ref type="bibr" target="#b94">Oron et al. (2017)</ref>. See Appendix A for the exact definition and Appendix B for comparisons with alternative alignment metrics.</p><p>Next, we explore several ways in which representations are converging. First, we argue that different neural networks are converging to aligned representations. Then, we show that this continues to hold across modalities, where image embeddings in vision models align with text embeddings in language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Different models, with different architectures and objectives, can have aligned representations</head><p>One indication of representational convergence is the rising number of systems built on top of pre-trained foundation models. These models are becoming standard backbones across a growing spectrum of tasks. Their versatility across numerous applications implies a level of universality in the way they represent data.</p><p>While this trend implies convergence toward a relatively small set of foundation models, it does not imply that different foundation models will arrive at the same representation.</p><p>Yet that is what has been observed by several recent papers. <ref type="bibr" target="#b64">Lenc &amp; Vedaldi (2015)</ref> conducted one such study, in which We measure alignment among 78 models using mutual nearestneighbors on Places-365 <ref type="bibr" target="#b141">(Zhou et al., 2017)</ref>, and evaluate their performance on downstream tasks from the Visual Task Adaptation Benchmark (VTAB; <ref type="bibr" target="#b139">Zhai et al. (2019)</ref>). LEFT: Models that solve more VTAB tasks tend to be more aligned with each other. Error bars show standard error. RIGHT: We use UMAP to embed models into a 2D space, based on distance ≜ -log(alignment). More competent and general models (blue) have more similar representations.</p><p>they measured representational similarity through a technique called model stitching. Given two models, f and g, each composed of multiple layers (f</p><formula xml:id="formula_0">= f 1 • • • • • f n , g = g 1 • • • • • g m ),</formula><p>an intermediate representation from f is integrated into g via a learned affine stitching layer h, resulting in a new stitched model</p><formula xml:id="formula_1">F = f 1 •• • ••f k •h•g k+1 •• • ••g m .</formula><p>If F has good performance, it indicates that f and g have compatible representations at layer k, up to the transform h.</p><p>In their study, <ref type="bibr" target="#b64">Lenc &amp; Vedaldi (2015)</ref> made two notable findings: (1) A vision model trained on ImageNet <ref type="bibr" target="#b105">(Russakovsky et al., 2015)</ref> can be aligned with a model trained on Places-365 <ref type="bibr" target="#b141">(Zhou et al., 2017)</ref> while maintaining good performance;</p><p>(2) The early layers of these convolutional networks are more interchangeable than later layers. The first finding illustrates a level of data independence where distinct image datasets lead to similar representations. The second finding agrees with extensive research that oriented Gabor-like filters are common in both artificial and biological vision systems. This suggests a convergence to a similar initial layer of representation across various neural network architectures <ref type="bibr" target="#b89">(Olshausen &amp; Field, 1996;</ref><ref type="bibr" target="#b62">Krizhevsky et al., 2017)</ref>. <ref type="bibr" target="#b7">Bansal et al. (2021)</ref> expanded on the idea of model stitching, showing that models trained using self-supervised objectives align closely with their supervised counterparts. <ref type="bibr" target="#b83">Moschella et al. (2022)</ref> further demonstrated the feasibility of "zero-shot" model stitching without learning a stitching layer. Despite the fact that different text models were trained on different modalities, they found that the models often embed data in remarkably similar ways. In particular, they considered the kernel K defined by learned representations and showed that K serves as a bridge between models, allowing an encoder trained in one language, like English, to work effectively with a decoder in another, like French. <ref type="bibr" target="#b27">Dravid et al. (2023)</ref> extended this idea to individual neurons, and found "Rosetta Neurons" that are activated by the same pattern across a range of vision models. Such neurons form a common dictionary independently discovered by all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Alignment increases with scale and performance</head><p>Kornblith et al. (2019) and Roeder et al. (2021) observed model alignment not only exists but also increases with model scale and dataset size. On CIFAR-10 classification, <ref type="bibr" target="#b61">Krizhevsky et al. (2009)</ref> found that larger models exhibit greater alignment with each other compared to smaller ones. Theoretically, <ref type="bibr" target="#b6">Balestriero &amp; Baraniuk (2018)</ref> showed that models with similar outputs (e.g., as a result of having high performance) also have similar internal activations. With the continuing trend of models scaling up, this suggests model alignment will increase over time -we might expect that the next generation of bigger, better models will be even more aligned with each other.</p><p>We expand upon this observation by evaluating the transfer performance of 78 vision models. These models were trained with varying architectures, training objectives, and datasets (detailed in Appendix C.1). In Figure <ref type="figure" target="#fig_0">2</ref> (left), we bin these models based on their average transfer performance on the VTAB dataset <ref type="bibr" target="#b139">(Zhai et al., 2019)</ref>, and then measure the average kernel alignment of the models within each bin. The results indicate that models with high transfer performance form a tightly clustered set of representations, while models with weak performance have more variable representations. We further visualize this structure with UMAP <ref type="bibr" target="#b78">(McInnes et al., 2018)</ref> over models representation in Figure <ref type="figure" target="#fig_0">2</ref> (right). This suggests that models that are competent all represent data in a similar way. Echoing <ref type="bibr" target="#b7">Bansal et al. (2021)</ref> and Tolstoy (1877), we might say: all strong models are alike, each weak model is weak in its own way.</p><p>The discussion so far indicates that various models are aligning toward a unified representation. But does the convergence extend to model weights? While models with different architectures might not have compatible weight spaces, there exists ample evidence that models with the same architecture will often converge to the same basin of weights <ref type="bibr" target="#b84">(Nagarajan &amp; Kolter, 2019;</ref><ref type="bibr" target="#b30">Garipov et al., 2018;</ref><ref type="bibr" target="#b75">Lubana et al., 2023)</ref>. This holds even for models with different initializations, up to permutations over weight space (Ainsworth  <ref type="bibr" target="#b115">(Srinivasan et al., 2021)</ref>. The x-axis is the language model performance measured over 4M tokens from the OpenWebText dataset <ref type="bibr" target="#b33">(Gokaslan &amp; Cohen, 2019)</ref> (see Appendix B for plots with model names). We measure performance using 1bits-per-byte, where bits-per-byte normalizes the cross-entropy by the total bytes in the input text string. The results show a linear relationship between language-vision alignment and language modeling score, where a general trend is that more capable language models align better with more capable vision models. We find that CLIP models, which are trained with explicit language supervision, exhibit a higher level of alignment. However, this alignment decreases after being fine-tuned on ImageNet classification (labeled CLIP (I12K ft)). <ref type="bibr">et al., 2022)</ref>. Because of this, it is possible to merge separately trained models of the same architecture, and achieve some of the capabilities of all models in the mixture <ref type="bibr" target="#b118">(Stoica et al., 2023;</ref><ref type="bibr" target="#b54">Jordan et al., 2022;</ref><ref type="bibr" target="#b134">Wortsman et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Representations are converging across modalities</head><p>Do models trained on different data modalities also converge? Several works indicate that the answer is yes. <ref type="bibr" target="#b79">Merullo et al. (2022)</ref> extended model stitching to the crossmodal setting, finding that a single linear projection is sufficient to stitch a vision model to an LLM and achieve good performance on visual question answering and image captioning. <ref type="bibr" target="#b59">Koh et al. (2023)</ref> showed that linear stitching can also work in the opposite direction, aligning text inputs to visual outputs. In fact, many recent language-vision models stitch pre-trained language and vision models together. For example, LLaVA <ref type="bibr" target="#b69">(Liu et al., 2023)</ref> demonstrated state-ofthe-art results by projecting visual features into a language model with a 2-layer MLP.</p><p>Other works show further kinds of evidence of cross-modal synergy. <ref type="bibr">OpenAI (2023)</ref> found that jointly training a language model with a vision model improves performance on language tasks, compared to training the language model on its own. <ref type="bibr" target="#b114">Sorscher et al. (2022)</ref> show a setting in which word embeddings of visual concept names can be isometrically mapped to image embeddings for those same concepts. In work concurrent to ours, <ref type="bibr" target="#b77">Maniparambil et al. (2024)</ref> show well-trained vision encoders on large datasets exhibit high semantic similarity with language encoders regardless of the training paradigm (supervised, self-supervised, or language-supervised). <ref type="bibr" target="#b108">Sharma et al. (2024)</ref> probed the visual knowledge of LLMs trained only on language data, by converting images into code that an LLM can process. They found that LLMs have rich knowledge of visual structures, to the extent that decent visual representations can be trained on images generated solely by querying an LLM to produce code and rendering the response. In visual generation, LLMs show abilities to augment captions with visual structures (e.g., bounding boxes) and improve generation quality <ref type="bibr" target="#b11">(Betker et al., 2023;</ref><ref type="bibr">Lian et al., 2023a;</ref><ref type="bibr" target="#b10">b;</ref><ref type="bibr" target="#b135">Wu et al., 2023)</ref>. Over other modalities, <ref type="bibr" target="#b88">Ngo &amp; Kim (2024)</ref> showed auditory models are also roughly aligned with LLMs up to a linear transformation, and <ref type="bibr" target="#b87">Ng et al. (2023)</ref> demonstrated the effectiveness of using pre-trained LLMs for facial motion prediction.</p><p>We set out to address these claims in a broader scope to determine whether models are indeed learning an increasingly modality-agnostic representation of the world. We sampled a variety of models trained either solely on vision or solely on language, and compared their representations as they became larger and more competent over many tasks.</p><p>In Figure <ref type="figure" target="#fig_1">3</ref>, we assess alignment between a suite of language models and vision models. So far we have only defined alignment for two kernels defined over the same input space. To measure cross-modal alignment, we use paired datasets to bridge the two modalities. For vision and text, we use the Wikipedia captions dataset {(x i , y i )} i <ref type="bibr" target="#b115">(Srinivasan et al., 2021)</ref>, composed of images from Wikipedia (x i ) and their</p><p>0.14 0.16 0.18 0.20 0.22 0.24 0.26 Alignment to VISION (DINOv2) 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 Performance on Hellaswag llama-33b bloom-1.7b llama-65b mistral-7b openllama-7b mixtral-8x7b openllama-3b gemma-7b bloom-3b llama3-8b bloom-7.1b bloom-1.1b olmo-1b bloom-560m gemma-2b llama-7b olmo-7b openllama-13b llama3-70b llama-13b 0.14 0.16 0.18 0.20 0.22 0.24 0.26 Alignment to VISION (DINOv2) 0.0 0.2 0.4 0.6 0.8 Performance on GSM8K (5 shot) llama-33b bloom-1.7b llama-65b mistral-7b openllama-7b mixtral-8x7b openllama-3b gemma-7b bloom-3b llama3-8b bloom-7.1b bloom-1.1b olmo-1b bloom-560m gemma-2b llama-7b olmo-7b openllama-13b llama3-70b llama-13b</p><p>Figure <ref type="figure">4</ref>. Alignment predicts downstream performance: We visualize correlation between LLM alignment score to DINOv2 <ref type="bibr" target="#b93">(Oquab et al., 2023)</ref> and downstream task performance on Hellaswag (common-sense) <ref type="bibr" target="#b138">(Zellers et al., 2019)</ref> and GSM8K (math) <ref type="bibr" target="#b19">(Cobbe et al., 2021)</ref>. LLMs are plotted with radii proportional to the size of the model, and color-coded by their rank order in language modeling scores (1bits-per-byte). We observe that models aligned more closely with vision also show better performance on downstream language tasks. For Hellaswag, there is a linear relationship with alignment score, while GSM8K exhibits an "emergence"-esque trend.</p><p>corresponding captions (y i ). We then measure alignment between a language model f text and a vision model f img as the alignment of the two following kernels:</p><formula xml:id="formula_2">K img (i, j) = ⟨f img (x i ), f img (x j )⟩ (1) K text (i, j) = ⟨f text (y i ), f text (y j )⟩.</formula><p>(2)</p><p>Using this analysis, we find that the better an LLM is at language modeling, the more it tends to aligns with vision models, as shown in Figure <ref type="figure" target="#fig_1">3</ref>. The converse effect also holds: the better a vision models is, the more it tends to align with LLMs. See Appendix C.2 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Models are increasingly aligning to brains</head><p>Neural networks also show substantial alignment with biological representations in the brain <ref type="bibr" target="#b137">(Yamins et al., 2014)</ref>. This commonality may be due to similarities in the task and data constraints both systems are confronted with. Even though the mediums may differ -silicon transistors versus biological neurons -the fundamental problem faced by brains and machines is the same: efficiently extracting and understanding the underlying structure in images, text, sounds, etc. <ref type="bibr" target="#b10">(Barlow et al., 1961;</ref><ref type="bibr" target="#b90">Olshausen &amp; Field, 1997)</ref>. <ref type="bibr" target="#b114">Sorscher et al. (2022)</ref> developed a theoretical framework for how the efficient extraction of novel concepts occurs for both the human visual system and deep networks. The tasks that the human visual system has been honed to perform through evolution -like segmentation, detection, and whole-image classification -are also the ones that we train our neural nets to perform. <ref type="bibr" target="#b137">Yamins et al. (2014)</ref> went as far as to title their work in the spirit that performance over such tasks implies brain alignment. <ref type="bibr" target="#b2">Antonello &amp; Huth (2024)</ref> posited that it is less the particular task and more the generality of the representations that explain their alignment with biological representations. Further, <ref type="bibr" target="#b20">Conwell et al. (2022)</ref> showed that training data plays a large role in alignment.</p><p>Psychophysical studies have also shown agreement between how humans perceive visual similarity and how models do, even when the models are trained on tasks, such as self-supervised prediction, that are seemingly unrelated to mimicking human perception <ref type="bibr" target="#b140">(Zhang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Does alignment predict downstream performance?</head><p>If models are converging towards a more accurate representation of reality, we expect that alignment should correspond to improved performance on downstream tasks. Figure <ref type="figure">4</ref> supports this hypothesis by demonstrating improved performance on commonsense reasoning (Hellaswag; <ref type="bibr" target="#b138">Zellers et al. (2019)</ref>) and mathematical problem solving (GSM8K; <ref type="bibr" target="#b19">Cobbe et al. (2021)</ref>) as alignment improves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Why are representations converging?</head><p>Modern machine learning models are generally trained to minimize the empirical risk with possible implicit and/or explicit regularization:</p><formula xml:id="formula_3">trained model f * = arg min f ∈ F function class E x∼ dataset [ training objective L (f, x)] + R regularization (f )</formula><p>In the following sections, we lay out how each colored component in this optimization process potentially plays a role in facilitating representational convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convergence via Task Generality</head><p>Each training datapoint and objective (task) places an additional constraint on the model. As data and tasks scale, the volume of representations that satisfy these constraints must proportionately grow smaller, as visualized in Figure <ref type="figure" target="#fig_24">6</ref> and stated below: Solves task 1    </p><formula xml:id="formula_4">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 1 N X Q k D y p c G / A + W t w X q m 2 W e y f W o = " &gt; A A A B 9 H i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I z G x I n c 0 W h J t L D H K R w I X s r f s w Y a 9 2 3 N 3 j o R c + B 0 2 F h p j 6 4 + x 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 j E o 1 4 0 2 m p N K d g B o u R c y b K F D y T q I 5 j Q L J 2 8 H 4 d u 6 3 J 1 w b o e J H n C b c j + g w F q F g F K 3 k P y g 5 4 Y Y g N W P i 9 c s V t + o u Q N a J l 5 M K 5 G j 0 y 1 + 9 g W J p x G N k k h r T 9 d w E / Y x q F E z y W a m X G p 5 Q N q Z D 3 r U 0 p h E 3 f r Y 4 e k Y u r D I g o d K 2 Y i Q L 9 f d E R i N j p l F g O y O K I 7 P q z c X / v G 6 K 4 b W f i T h J k c d s u S h M J U F F 5 g m Q g d C c o Z x a Q p k W 9 l b C R l R T h j a n k g 3 B W 3 1 5 n b R q V c + t e v e 1 S v 0 m j 6 M I Z 3 A O l + D B F d T h D h r Q B A Z P 8 A y v 8 O Z M n B f n 3 f l Y t h a c f O Y U / s D 5 / A E M H 5 G a &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F W 6 a g l l 4 4 / + c H G y W R 3 U L X Y f d B V s = " &gt; A A A C G X i c j V C 7 T s M w F L 3 h W c q r w M h i U S E x V U k X G C t Y G E H Q h 9 R G l e M 6 r V X H C f Z N p S r q d z C w 8 C s s C D H C x N / g t h m g Z e B I l o 7 O O V f X 9 w S J F A Z d 9 8 t Z W V 1 b 3 9 g s b B W 3 d 3 b 3 9 k s H h w 0 T p 5 r x O o t l r F s B N V w K x e s o U P J W o j m N A s m b w f B q 6 j d H X B s R q 3 s c J 9 y P a F + J U D C K V v L v Y j n i h i A 1 Q + J 1 S 2 W 3 4 s 5 A l o m X k z L k + F + 8 W / r o 9 G K W R l w h k 9 S Y t u c m 6 G d U o 2 C S T 4 q d 1 P C E s i H t 8 7 a l i k b c + N n s s g k 5 t U q P h L G 2 T y G Z q T 8 n M h o Z M 4 4 C m 4 w o D s y i N x X / 8 t o p h h d + J l S S I l d s v i h M J c G Y T G s i P a E 5 Q z m 2 h D I t 7 F 8 J G 1 B N G d o y i / Z 0 b / H Q Z d K o V j y 3 4 t 1 W y 7 X L v L M C H M M J n I E H 5 1 C D a 7 i B O j B 4 g E d 4 h l f n y X l x 3 p z 3 e X T F y W e O 4 B e c z 2 8 O b 5 k T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F W 6 a g l l 4 4 / + c H G y W R 3 U L X Y f d B V s = " &gt; A A A C G X i c j V C 7 T s M w F L 3 h W c q r w M h i U S E x V U k X G C t Y G E H Q h 9 R G l e M 6 r V X H C f Z N p S r q d z C w 8 C s s C D H C x N / g t h m g Z e B I l o 7 O O V f X 9 w S J F A Z d 9 8 t Z W V 1 b 3 9 g s b B W 3 d 3 b 3 9 k s H h w 0 T p 5 r x O o t l r F s B N V w K x e s o U P J W o j m N A s m b w f B q 6 j d H X B s R q 3 s c J 9 y P a F + J U D C K V v L v Y j n i h i A 1 Q + J 1 S 2 W 3 4 s 5 A l o m X k z L k + F + 8 W / r o 9 G K W R l w h k 9 S Y t u c m 6 G d U o 2 C S T 4 q d 1 P C E s i H t 8 7 a l i k b c + N n s s g k 5 t U q P h L G 2 T y G Z q T 8 n M h o Z M 4 4 C m 4 w o D s y i N x X / 8 t o p h h d + J l S S I l d s v i h M J c G Y T G s i P a E 5 Q z m 2 h D I t 7 F 8 J G 1 B N G d o y i / Z 0 b / H Q Z d K o V j y 3 4 t 1 W y 7 X L v L M C H M M J n I E H 5 1 C D a 7 i B O j B 4 g E d 4 h l f n y X l x 3 p z 3 e X T F y W e O 4 B e c z 2 8 O b 5 k T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F W 6 a g l l 4 4 / + c H G y W R 3 U L X Y f d B V s = " &gt; A A A C G X i c j V C 7 T s M w F L 3 h W c q r w M h i U S E x V U k X G C t Y G E H Q h 9 R G l e M 6 r V X H C f Z N p S r q d z C w 8 C s s C D H C x N / g t h m g Z e B I l o 7 O O V f X 9 w S J F A Z d 9 8 t Z W V 1 b 3 9 g s b B W 3 d 3 b 3 9 k s H h w 0 T p 5 r x O o t l r F s B N V w K x e s o U P J W o j m N A s m b w f B q 6 j d H X B s R q 3 s c J 9 y P a F + J U D C K V v L v Y j n i h i A 1 Q + J 1 S 2 W 3 4 s 5 A l o m X k z L k + F + 8 W / r o 9 G K W R l w h k 9 S Y t u c m 6 G d U o 2 C S T 4 q d 1 P C E s i H t 8 7 a l i k b c + N n s s g k 5 t U q P h L G 2 T y G Z q T 8 n M h o Z M 4 4 C m 4 w o D s y i N x X / 8 t o p h h d + J l S S I l d s v i h M J c G Y T G s i P a E 5 Q z m 2 h D I t 7 F 8 J G 1 B N G d o y i / Z 0 b / H Q Z d K o V j y 3 4 t 1 W y 7 X L v L M C H M M J n I E H 5 1 C D a 7 i B O j B 4 g E d 4 h l f n y X l x 3 p z 3 e X T F</formula><formula xml:id="formula_5">b R m W a 8 R Z T U u l u S A 2 X I u E t F C h 5 N 9 W c x q H k n X B 8 N / c 7 E 6 6 N U M k T T l M e x H S Y i E g w i l b y H 5 W c c E O Q m n G / U n V r 7 g J k n X g F q U K B Z r / y 1 R s o l s U 8 Q S a p M b 7 n p h j k V K N g k s / K v c z w l L I x H X L f 0 o T G 3 A T 5 4 u Q Z u b T K g E R K 2 0 q Q L N T f E z m N j Z n G o e 2 M K Y 7 M q j c X / / P 8 D K O b I B d J m i F P 2 H J R l E m C i s z / J w O h O U M 5 t Y Q y L</formula><formula xml:id="formula_6">i O R 0 v Y l S O b q z 4 k J j Y 0 Z x 6 F N x h Q H Z t m b i X 9 5 f o b R Z T A R S Z o h T 9 h i U Z R J g o r M S i I 9 o T l D O b a E M i 3 s X w k b U E 0 Z 2 i p L 9 n R v + d B V 0 q x V P b f q 3 d U q 9 a u</formula><formula xml:id="formula_7">d J x + d H x G x H z s U U d y q X K S E = " &gt; A A A C F 3 i c j V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r t p t A z a W C q a B 2 y W M D u Z T Y b M 7 i w z d w M h 5 D M s b P w V G x F b 7 f w b J 8 k W m l h 4 Y O B w z r n c u S d M p T D o u l 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j W N y j T j D a a k 0 u 2 Q G i 5 F w h s o U P J 2 q j m N Q 8 l b 4 f B 6 5 r d G X B u h k g c c p z y I a T 8 R k W A U r e T f K z n i h i A 1 w 2 6 5 4 l b d O c g q 8 X J S g R z / i 3 f L n 5 2 e Y l n M E 2 S S G u N 7 b o r B h G o U T P J p q Z M Z n l I 2 p H 3 u W 5 r Q m J t g M r 9 r S s 6 s 0 i O R 0 v Y l S O b q z 4 k J j Y 0 Z x 6 F N x h Q H Z t m b i X 9 5 f o b R Z T A R S Z o h T 9 h i U Z R J g o r M S i I 9 o T l D O b a E M i 3 s X w k b U E 0 Z 2 i p L 9 n R v + d B V 0 q x V P b f q 3 d U q 9 a u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y q 8 / 5 g 8 N c Y 6 c y 0  b N E = " &gt; A A A C E n i c j V C 7 T s M w F L 0 p r 1 J e B U Y W i w q J q U q 6 w F j B w g i C P q Q 2 q h z 3 p j W 1 k 8 h 2 k K q o / 8 D A w q + w I b N E = " &gt; A A A C E n i c j V C 7 T s M w F L 0 p r 1 J e B U Y W i w q J q U q 6 w F j B w g i C P q Q 2 q h z 3 p j W 1 k 8 h 2 k K q o / 8 D A w q + w I b N E = " &gt; A A A C E n i c j V C 7 T s M w F L 0 p r 1 J e B U Y W i w q J q U q 6 w F j B w g i C P q Q 2 q h z 3 p j W 1 k 8 h 2 k K q o / 8 D A w q + w I </p><formula xml:id="formula_8">A D Z K i G U G r j 1 Y Y = " &gt; A A A B 6 H i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I z G x I n c 0 U h J t L C G R j w Q u Z G + Z g 5 W 9 v c v u n g m 5 8 A t s L D T G 1 p 9 k 5 7 9 x g S s U f M k k L + / N Z G Z e k A i u j e t + O 4 W t 7 Z 3 d v e J + 6 e D w 6 P i k f H r W 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 b u F 3 n 1 B p H s s H M 0 v Q j + h Y 8 p A z a q z U q g 3 L F b f q L k E 2 i Z e T C u R o D s t f g 1 H M 0 g i l Y Y J q 3 f f c x P g Z V Y Y z g f P S I N W Y U D a l Y + x b K m m E 2 s + W h 8 7 J l V V G J I y V L W n I U v 0 9 k d F I 6 1 k U 2 M 6 I m o l e 9 x b i f 1 4 / N W H d z 7 h M U o O S r R a F q S A m J o u v y Y g r Z E b M L K F M c X s r Y R O q K D M 2 m 5 I N w V t / e Z N 0 a l X P r X q t W q V x m 8 d R h A u 4 h G v w 4 A Y a c A 9 N a A M D h G d 4 h T f n 0 X l x 3 p 2 P V W v B y W f O 4 Q + c z x 9 7 g Y y 0 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 1 1 z A F H k y S / / e h o 1 X 0 C n g 1 E O x t U = " &gt; A A A C D X i c j V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r t p T B m 0 s T R g H p A s Y X Z y N x k y O 7 v M z A p h y R d Y 2 P g r N i K 2 9 n b + j Z N k C 0 0 s P D B w O O d c 7 t w T J I J r 4 7 p f T m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m f u d B 1 S a x / L e T B P 0 I z q S P O S M G i s 1 a 4 N y x a 2 6 C 5 B 1 4 u W k A j n + F x + U P / v D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n p X 6 q M a F s Q k f Y s 1 T S C L W f L a 6 Z k Q u r D E k Y K / u k I Q v 1 5 0 R G I 6 2 n U W C T E T V j v e r N x b + 8 X m r C u p 9 x m a Q G J V s u C l N B T E z m 1 Z A h V 8 i M m F p C m e L 2 r 4 S N q a L M 2 A J L 9 n R v 9 d B 1 0 q 5 V P b f q N W u V x n X e W R H O 4 B w u w Y M r a M A t 3 E E L G C A 8 w j O 8 O k / O i / P m v C + j B S e f O Y V f c D 6 + A d g g l C 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 1 1 z A F H k y S / / e h o 1 X 0 C n g 1 E O x t U = " &gt; A A A C D X i c j V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r t p T B m 0 s T R g H p A s Y X Z y N x k y O 7 v M z A p h y R d Y 2 P g r N i K 2 9 n b + j Z N k C 0 0 s P D B w O O d c 7 t w T J I J r 4 7 p f T m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m f u d B 1 S a x / L e T B P 0 I z q S P O S M G i s 1 a 4 N y x a 2 6 C 5 B 1 4 u W k A j n + F x + U P / v D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n p X 6 q M a F s Q k f Y s 1 T S C L W f L a 6 Z k Q u r D E k Y K / u k I Q v 1 5 0 R G I 6 2 n U W C T E T V j v e r N x b + 8 X m r C u p 9 x m a Q G J V s u C l N B T E z m 1 Z A h V 8 i M m F p C m e L 2 r 4 S N q a L M 2 A J L 9 n R v 9 d B 1 0 q 5 V P b f q N W u V x n X e W R H O 4 B w u w Y M r a M A t 3 E E L G C A 8 w j O 8 O k / O i / P m v C + j B S e f O Y V f c D 6 + A d g g l C 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 1 1 z A F H k y S / / e h o 1 X 0 C n g 1 E O x t U = " &gt; A A A C D X i c j V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r t p T B m 0 s T R g H p A s Y X Z y N x k y O 7 v M z A p h y R d Y 2 P g r N i K 2 9 n b + j Z N k C 0 0 s P D B w O O d c 7 t w T J I J r 4 7 p f T m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m f u d B 1 S a x / L e T B P 0 I z q S P O S M G i s 1 a 4 N y x a 2 6 C 5 B 1 4 u W k A j n + F x + U P / v D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n p X 6 q M a F s Q k f Y s 1 T S C L W f L a 6 Z k Q u r D E k Y K / u k I Q v 1 5 0 R G I 6 2 n U W C T E T V j v e r N x b + 8 X m r C u p 9 x m a Q G J V s u C l N B T E z m 1 Z A h V 8 i M m F p C m e L 2 r 4 S N q a L M 2 A J L 9 n R v 9 d B 1 0 q 5 V P b f q N W u V x n X e W R H O 4 B w u w Y M r a M A t 3 E E L G C A 8 w j O 8 O k / O i / P m v C + j B S e f O Y V f c D 6 + A d g g l C 0 = &lt; / l a t e x i t &gt; Hypothesis space &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t L m I l j F W h X s / B u 7 + r 0 W b g / j 8 T V U = " &gt; A A A C o X i c f V F N b 9 N A E N 2 Y r x K + U j h y s b C Q E E K R 3 Q s c K + B Q D o i A m r S S 1 4 r G m 3 G y 6 n 5 Y u + O C Z f m f c I X / x L 9 h n Q a J t o i R V n r 7 5 s 3 u m 5 m y V t J T m v 4 a R T d u 3 r p 9 Z + / u + N 7 9 B w 8 f T f Y f L 7 x t n M C 5 s M q 6 0 x I 8 K m l w T p I U n t Y O Q Z c K T 8 q z d 0 P + 5 B y d l 9 Y c U 1 t j o W F t Z C U F U K C W k 8 l R W 1 v a o J c + 9 j U I X E 6 S d J p u I 7 4 O s h 1 I 2 C 5 m y / 1 R y 1 d W N B o N C Q X e 5 1 l a U 9 G B I y k U 9 m P e e A w v n 8 E a 8 w A N a P R F t 7 X e x 8 8 D s 4 o r 6 8 I x F G / Z v y s 6 0 N 6 3 u g x K D b T x V 3 M D + a 9 c 3 l D 1 p u i k q R t C I y 4 + q h o V k 4 2 H O c Q r 6 V C Q a g M A 4 W T w G o s N O B A U p j U e c 4 N f h d U a z K r j x j r d 5 1 n R c Y U V c b V A R 0 n G n V x v i L v h F r p 8 j 6 F 7 h x + D k 0 8 1 O i D r X n Y c 3 F p L 0 4 d p r P m r A f 1 P C N / + C A O 6 b I E c G F 9 b j 3 3 H t 8 1 W 3 X H f h 2 V l V 1 d z H S w O p l k 6 z T 4 f J I d v d 2 v b Y 0 / Z M / a C Z e w 1 O 2 R H b M b m T L B z 9 p 3 9 Y D + j J P o Q z a I v F 9 J o t K t 5 w i 5 F l P 8 G k Z 7 T f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / p y n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9 Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / p y n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9 Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / p y n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9 Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d</formula><formula xml:id="formula_9">V v U I F 9 Z 2 h D + L D e F q i r 1 + T O 8 Q w = " &gt; A A A B 7 X i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Y M 2 l h H N B y R H 2 N v M J W t 2 9 4 7 d P S G E / A c b C 0 V s / T 9 2 / h s 3 y R W a + G D g 8 d 4 M M / O i V H B j f f / b W 1 v f 2 N z a L u w U d / f 2 D w 5 L R 8 d N k 2 S a Y Y M l I t H t i B o U X G H D c i u w n W q k M h L Y i k Y 3 M 7 / 1 h N r w R D 3 Y c Y q h p A P F Y 8 6 o d V L z n s t U Y K 9 U 9 i v + H G S V B D k p Q 4 5 6 r / T V 7 S c s k 6 g s E 9 S Y T u C n N p x Q b T k T O C 1 2 M 4 M p Z S M 6 w I 6 j i k o 0 4 W R + 7 Z S c O 6 V P 4 k S 7 U p b M 1 d 8 T E y q N G c v I d U p q h 2 b Z m 4 n / e Z 3 M x l f h h K s 0 s 6 j Y Y l G c C W I T M n u d 9 L l G Z s X Y E c o 0 d 7 c S N q S a M u s C K r o Q g u W X V 0 m z W g n 8 S n</formula><formula xml:id="formula_10">M T K x M b f 4 L Y Z o G X g S J a O z j l X 1 / c E i e D a u O 6 X U 1 h Z X V v f K G 6 W t r Z 3 d v f K + w d N H a e K Y Y P F I l b t g G o U P M K G 4 U Z g O 1 F I Z S C w F Y w u p 3 7 r A Z X m c X R n x g n 6 k g 4 i H n J G j Z W a t 1 w m A n v l i l t 1 Z y D L x M t J B X L 8 L 9 4 r f 3 b 7 M U s l R o Y J q n X H c x P j Z 1 Q Z z g R O S t 1 U Y 0 L Z i A 6 w Y 2 l E J W o / m 5 0 0 I S d W 6 Z M w V v Z F h s z U n x M Z l V q P Z W C T k p q h X v S m 4 l 9 e J z X h u Z / x K E k N R m y + K E w F M T G Z 9 k P 6 X C E z Y m w J Z Y</formula><formula xml:id="formula_11">M T K x M b f 4 L Y Z o G X g S J a O z j l X 1 / c E i e D a u O 6 X U 1 h Z X V v f K G 6 W t r Z 3 d v f K + w d N H a e K Y Y P F I l b t g G o U P M K G 4 U Z g O 1 F I Z S C w F Y w u p 3 7 r A Z X m c X R n x g n 6 k g 4 i H n J G j Z W a t 1 w m A n v l i l t 1 Z y D L x M t J B X L 8 L 9 4 r f 3 b 7 M U s l R o Y J q n X H c x P j Z 1 Q Z z g R O S t 1 U Y 0 L Z i A 6 w Y 2 l E J W o / m 5 0 0 I S d W 6 Z M w V v Z F h s z U n x M Z l V q P Z W C T k p q h X v S m 4 l 9 e J z X h u Z / x K E k N R m y + K E w F M T G Z 9 k P 6 X C E z Y m w J Z Y</formula><formula xml:id="formula_12">M T K x M b f 4 L Y Z o G X g S J a O z j l X 1 / c E i e D a u O 6 X U 1 h Z X V v f K G 6 W t r Z 3 d v f K + w d N H a e K Y Y P F I l b t g G o U P M K G 4 U Z g O 1 F I Z S C w F Y w u p 3 7 r A Z X m c X R n x g n 6 k g 4 i H n J G j Z W a t 1 w m A n v l i l t 1 Z y D L x M t J B X L 8 L 9 4 r f 3 b 7 M U s l R o Y J q n X H c x P j Z 1 Q Z z g R O S t 1 U Y 0 L Z i A 6 w Y 2 l E J W o / m 5 0 0 I S d W 6 Z M w V v Z F h s z U n x M Z l V q P Z W C T k p q h X v S m 4 l 9 e J z X h u Z / x K E k N R m y + K E w F M T G Z 9 k P 6 X C E z Y m w J Z Y</formula><formula xml:id="formula_13">O i Z O N W V t G o t Y 9 0 J i m O C K t S 2 3 g v U S z Y g M B e u G 0 9 u 8 3 n 1 i 2 v B Y P d h Z w g J J x o p H n B L r r M c o V T Q H M 6 z W v L q 3 E F 4 H v 4 A a F G o N q 1 + D U U x T y Z S l g h j T 9 7 3 E B h n R l l P B 5 p V B a l h C 6 J S M W d + h I p K Z I F s s P M c X z h n h K N b u K Y s X 7 u + J j E h j Z j J 0 n Z L Y i V m t 5 e Z / t X 5 q o + s g 4 y p J L V N 0 + V G U C m x j n F + P R 1 w z a s X M A a G a u 1 0 x n R B N q H U Z V V w I / u r J 6 9 B p 1 H 2 v 7 t 8 3 a s 2 b I o 4 y n M E 5 X I I P V 9 C E O 2 h B G y h I e I Z X e E M a v a B 3 9 L F s L a F i 5 h T + C H 3 + A D u b k K s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L 1 h f g X b k 9 H O Q P + E H + u D 2 l O y E q i o = " &gt; A A A C F X i c j V D L S g M x F L 3 x W e u r 6 t J N s A i u y k w 3 u i y 6 c a l g H 9 I O J Z N m 2 t A k M y Q Z o Q z 9 C h d u / B U 3 I m 4 F d / 6 N m X Y W 2 r r w Q O B w z r n c 3 B M m g h v r e V 9 o Z X V t f W O z t F X e 3 t n d 2 6 8 c H L Z M n G r K m j Q W s e 6 E x D D B F W t a b g X r J J o R G Q r W D s d X u d 9 + Y N r w W N 3 Z S c I C S Y a K R 5 w S 6 6 T 7 K F U 0 J 6 Z f q X o 1 b w a 8 T P y C V K H A / + L 9 y m d v E N N U M m W p I M Z 0 f S + x Q U a 0 5 V S w a b m X G p Y Q O i Z D 1 n V U E c l M k M 2 u m u J T p w x w F G v 3 l M U z 9 e d E R q Q x E x m 6 p C R 2 Z B a 9 X P z L 6 6 Y 2 u g g y r p L U M k X n i 6 J U Y B v j v C I 8 4 J p R K y a O E K q 5 + y u m I 6 I J t a 7 I s j v d X z x 0 m b T q N d + r + b f 1 a u O y 6 K w E x 3 A C Z + D D O T T g G m 6 g C R Q k P M I z v K I n 9 I L e 0 P s 8 u o K K m S P 4 B f T x D S C U m C Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L 1 h f g X b k 9 H O Q P + E H + u D 2 l O y E q i o = " &gt; A A A C F X i c j V D L S g M x F L 3 x W e u r 6 t J N s A i u y k w 3 u i y 6 c a l g H 9 I O J Z N m 2 t A k M y Q Z o Q z 9 C h d u / B U 3 I m 4 F d / 6 N m X Y W 2 r r w Q O B w z r n c 3 B M m g h v r e V 9 o Z X V t f W O z t F X e 3 t n d 2 6 8 c H L Z M n G r K m j Q W s e 6 E x D D B F W t a b g X r J J o R G Q r W D s d X u d 9 + Y N r w W N 3 Z S c I C S Y a K R 5 w S 6 6 T 7 K F U 0 J 6 Z f q X o 1 b w a 8 T P y C V K H A / + L 9 y m d v E N N U M m W p I M Z 0 f S + x Q U a 0 5 V S w a b m X G p Y Q O i Z D 1 n V U E c l M k M 2 u m u J T p w x w F G v 3 l M U z 9 e d E R q Q x E x m 6 p C R 2 Z B a 9 X P z L 6 6 Y 2 u g g y r p L U M k X n i 6 J U Y B v j v C I 8 4 J p R K y a O E K q 5 + y u m I 6 I J t a 7 I s j v d X z x 0 m b T q N d + r + b f 1 a u O y 6 K w E x 3 A C Z + D D O T T g G m 6 g C R Q k P M I z v K I n 9 I L e 0 P s 8 u o K K m S P 4 B f T x D S C U m C Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L 1 h f g X b k 9 H O Q P + E H + u D 2 l O y E q i o = " &gt; A A A C F X i c j V D L S g M x F L 3 x W e u r 6 t J N s A i u y k w 3 u i y 6 c a l g H 9 I O J Z N m 2 t A k M y Q Z o Q z 9 C h d u / B U 3 I m 4 F d / 6 N m X Y W 2 r r w Q O B w z r n c 3 B M m g h v r e V 9 o Z X V t f W O z t F X e 3 t n d 2 6 8 c H L Z M n G r K m j Q W s e 6 E x D D B F W t a b g X r J J o R G Q r W D s d X u d 9 + Y N r w W N 3 Z S c I C S Y a K R 5 w S 6 6 T 7 K F U 0 J 6 Z f q X o 1 b w a 8 T P y C V K H A / + L 9 y m d v E N N U M m W p I M Z 0 f S + x Q U a 0 5 V S w a b m X G p Y Q O i Z D 1 n V U E c l M k M 2 u m u J T p w x w F G v 3 l M U z 9 e d E R q Q x E x m 6 p C R 2 Z B a 9 X P z L 6 6 Y 2 u g g y r p L U M k X n i 6 J U Y B v j v C I 8 4 J p R K y a O E K q 5 + y u m I 6 I J t a 7 I s j v d X z x 0 m b T q N d + r + b f 1 a u O y 6 K w E x 3 A C Z + D D O T T g G m 6 g C R Q k P M I z v K I n 9 I L e 0 P s 8 u o K K m S P 4 B f T x D S C U m C Q = &lt; / l a t e x i t &gt; Functions that solve &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q Y 5 B R e h 1 7 p 7 4 a 3 n 3 n J b o s 5 l x 5 K k = " &gt; A A A B / X i c b Z D L S g M x F I Y z 9 V b r r V 5 2 b o J F c F V m u t F l U R C X F e w F 2 q F k 0 k w b m k m G 5 E y h D s V X c e N C E b e + h z v f x k w 7 C 2 3 9 I f D x n 3 M 4 J 3 8 Q C 2 7 A d b + d w t r 6 x u Z W c b u 0 s 7 u 3 f 1 A + P G o Z l W j K m l Q J p T s B M U x w y Z r A Q b B O r B m J A s H a w f g m q 7 c n T B u u 5 A N M Y + Z H Z C h 5 y C k B a / X L J 7 e J p B k a D C M C 2 C g x Y f 1 y x a 2 6 c + F V 8 H K o o F y N f v m r N 1 A 0 i Z g E K o g x X c + N w U + J B k 4 F m 5 V 6 i W E x o W M y Z F 2 L k k T M + O n 8 + h k + t 8 4 A h 0 r b J w H P 3 d 8 T K Y m M m U a B 7 Y w I j M x y L T P / q 3 U T C K / 8 l M s 4 A S b p Y l G Y C A w K Z 1 H g A d e M g p h a I F R z e y u m I 6 I J B R t Y y Y b g L X 9 5 F V q 1 q u d W v f t a p X 6 d x 1 F E p + g M X S A P X a I 6 u k M N 1 E Q U P a J n 9 I r e n C f n x X l 3 P h a t B S e f O U Z / 5 H z + A I 0 i l U Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I d Z u o / Q J S V V 7 D 8 P p l h 4 p + d r N 7 1 E = " &gt; A A A C I n i c j V D L S g M x F M 3 U V 6 2 v 8 b F z E y y C q z L T j S 6 L g r h U s A 9 o h 5 J J M 2 1 o J h m S O 4 V a + i 8 u 3 P g r b k R d C X 6 M m X Y W 2 r r w Q O B w z r n c 3 B M m g h v w v E + n s L K 6 t r 5 R 3 C x t b e / s 7 r n 7 B w 2 j U k 1 Z n S q h d C s k h g k u W R 0 4 C N Z K N C N x K F g z H F 5 l f n P E t O F K 3 s M 4 Y U F M + p J H n B K w U t c 9 u k 4 l z a j B M C C A j R I j 1 n X L X s W b A S 8 T P y d l l O N / 8 a 7 7 3 u k p m s Z M A h X E m L b v J R B M i A Z O B Z u W O q l h C a F D 0 m d t S y W J m Q k m s x O n + N Q q P R w p b Z 8 E P F N / T k x I b M w 4 D m 0 y J j A w i 1 4 m / u W 1 U 4 g u g g m X S Q p M 0 v m i K B U Y F M 7 6 w j 2 u G Q U x t o R Q z e 1 f M R 0 Q T S j Y V k v 2 d H / x 0 G X S q F Z 8 r + L f V c u 1 y 7 y z I j p G J + g M + e g c 1 d A N u k V 1 R N E D e k T P 6 N V 5 c l 6 c N + d j H i 0 4 + c w h + g X n 6</formula><p>x s L X p y 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_14">I d Z u o / Q J S V V 7 D 8 P p l h 4 p + d r N 7 1 E = " &gt; A A A C I n i c j V D L S g M x F M 3 U V 6 2 v 8 b F z E y y C q z L T j S 6 L g r h U s A 9 o h 5 J J M 2 1 o J h m S O 4 V a + i 8 u 3 P g r b k R d C X 6 M m X Y W 2 r r w Q O B w z r n c 3 B M m g h v w v E + n s L K 6 t r 5 R 3 C x t b e / s 7 r n 7 B w 2 j U k 1 Z n S q h d C s k h g k u W R 0 4 C N Z K N C N x K F g z H F 5 l f n P E t O F K 3 s M 4 Y U F M + p J H n B K w U t c 9 u k 4 l z a j B M C C A j R I j 1 n X L X s W b A S 8 T P y d l l O N / 8 a 7 7 3 u k p m s Z M A h X E m L b v J R B M i A Z O B Z u W O q l h C a F D 0 m d t S y W J m Q k m s x O n + N Q q P R w p b Z 8 E P F N / T k x I b M w 4 D m 0 y J j A w i 1 4 m / u W 1 U 4 g u g g m X S Q p M 0 v m i K B U Y F M 7 6 w j 2 u G Q U x t o R Q z e 1 f M R 0 Q T S j Y V k v 2 d H / x 0 G X S q F Z 8 r + L f V c u 1 y 7 y z I j p G J + g M + e g c 1 d A N u k V 1 R N E D e k T P 6 N V 5 c l 6 c N + d j H i 0 4 + c w h + g X n 6</formula><p>x s L X p y 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_15">I d Z u o / Q J S V V 7 D 8 P p l h 4 p + d r N 7 1 E = " &gt; A A A C I n i c j V D L S g M x F M 3 U V 6 2 v 8 b F z E y y C q z L T j S 6 L g r h U s A 9 o h 5 J J M 2 1 o J h m S O 4 V a + i 8 u 3 P g r b k R d C X 6 M m X Y W 2 r r w Q O B w z r n c 3 B M m g h v w v E + n s L K 6 t r 5 R 3 C x t b e / s 7 r n 7 B w 2 j U k 1 Z n S q h d C s k h g k u W R 0 4 C N Z K N C N x K F g z H F 5 l f n P E t O F K 3 s M 4 Y U F M + p J H n B K w U t c 9 u k 4 l z a j B M C C A j R I j 1 n X L X s W b A S 8 T P y d l l O N / 8 a 7 7 3 u k p m s Z M A h X E m L b v J R B M i A Z O B Z u W O q l h C a F D 0 m d t S y W J m Q k m s x O n + N Q q P R w p b Z 8 E P F N / T k x I b M w 4 D m 0 y J j A w i 1 4 m / u W 1 U 4 g u g g m X S Q p M 0 v m i K B U Y F M 7 6 w j 2 u G Q U x t o R Q z e 1 f M R 0 Q T S j Y V k v 2 d H / x 0 G X S q F Z 8 r + L f V c u 1 y 7 y z I j p G J + g M + e g c 1 d A N u k V 1 R N E D e k T P 6 N V 5 c l 6 c N + d j H i 0 4 + c w h + g X n 6</formula><p>x s L X p y 9 &lt; / l a t e x i t &gt; the tasks</p><formula xml:id="formula_16">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k H N B G I L 9 X h / D + 2 b T V 1 f y N l w d k T Y = " &gt; A A A B 8 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 7 S a B m 0 s Y x g P i Q 5 w t 5 m k y z Z 3 T t 2 5 4 R w 5 F f Y W C h i 6 8 + x 8 9 + 4 S a 7 Q x A c D j / d m m J k X J V J Y 9 P 1 v r 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 9 a N k 4 N 4 0 0 W y 9 h 0 I m q 5 F J o 3 U a D k n c R w q i L J 2 9 H k d u 6 3 n 7 i x I t Y P O E 1 4 q O h I i 6 F g F J 3 0 i G N O k N q J 7 Z c r f t V f g K y T I C c V y N H o l 7 9 6 g 5 i l i m t k k l r b D f w E w 4 w a F E z y W a m X W p 5 Q N q E j 3 n V U U 8 V t m C 0 O n p E L p w z I M D a u N J K F + n s i o 8 r a q Y p c p 6 I 4 t q v e X P z P 6 6 Y 4 v A 4 z o Z M U u W b L R c N U E o z J / H s y E I Y z l F N H K D P C 3 U r Y m B r K 0 G V U c i E E q y + v k 1 a t G v j V 4 L 5 W q d / k c R T h D M 7 h E g K 4 g j r c Q Q O a w E D B M 7 z C m 2 e 8 F + / d + 1 i 2 F r x 8 5 h T + w P v 8 A b 3 B k F k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H Q z q c Z X P y Q I U E Q 3 C 4 1 b l E r 9 0 t 9 E = " &gt; A A A C F X i c j V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y T e N l k E b S w X z k G Q J s 5 P Z Z M j M 7 D J z V w g h X 2 F h 4 6 / Y i N g K d v 6 N k 2 Q L T S w 8 M H A 4 5 1 z u 3 B O l U l j 0 / S 9 v Z X V t f W O z s F X c 3 t n d 2 y 8 d H D Z s k h n G 6 y y R i W l F 1 H I p N K + j Q M l b q e F U R Z I 3 o + H V 1 G 8 + c G N F o u 9 w l P J Q 0 b 4 W s W A U n X S P A 0 6 Q 2 q H t l s p + x Z + B L J M g J 2 X I 8 b 9 4 t / T Z 6 S U s U 1 w j k 9 T a d u C n G I 6 p Q c E k n x Q 7 m e U p Z U P a 5 2 1 H N V X c h u P Z V R N y 6 p Q e i R P j n k Y y U 3 9 O j K m y d q Q i l 1 Q U B 3 b R m 4 p / e e 0 M 4 4 t w L H S a I d d s v i j O J M G E T C s i P W E 4 Q z l y h D I j 3 F 8 J G 1 B D G b o i i + 7 0 Y P H Q Z d K o V g K / E t x W y 7 X L v L M C H M M J n E E A 5 1 C D a 7 i B O j B Q 8 A j P 8 O o 9 e S / e m / c + j 6 5 4 + c w R / I L 3 8 Q 2 W 4 J f S &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H Q z q c Z X P y Q I U E Q 3 C 4 1 b l E r 9 0 t 9 E = " &gt; A A A C F X i c j V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y T e N l k E b S w X z k G Q J s 5 P Z Z M j M 7 D J z V w g h X 2 F h 4 6 / Y i N g K d v 6 N k 2 Q L T S w 8 M H A 4 5 1 z u 3 B O l U l j 0 / S 9 v Z X V t f W O z s F X c 3 t n d 2 y 8 d H D Z s k h n G 6 y y R i W l F 1 H I p N K + j Q M l b q e F U R Z I 3 o + H V 1 G 8 + c G N F o u 9 w l P J Q 0 b 4 W s W A U n X S P A 0 6 Q 2 q H t l s p + x Z + B L J M g J 2 X I 8 b 9 4 t / T Z 6 S U s U 1 w j k 9 T a d u C n G I 6 p Q c E k n x Q 7 m e U p Z U P a 5 2 1 H N V X c h u P Z V R N y 6 p Q e i R P j n k Y y U 3 9 O j K m y d q Q i l 1 Q U B 3 b R m 4 p / e e 0 M 4 4 t w L H S a I d d s v i j O J M G E T C s i P W E 4 Q z l y h D I j 3 F 8 J G 1 B D G b o i i + 7 0 Y P H Q Z d K o V g K / E t x W y 7 X L v L M C H M M J n E E A 5 1 C D a 7 i B O j B Q 8 A j P 8 O o 9 e S / e m / c + j 6 5 4 + c w R / I L 3 8 Q 2 W 4 J f S &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H Q z q c Z X P y Q I U E Q 3 C 4 1 b l E r 9 0 t 9 E = " &gt; A A A C F X i c j V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y T e N l k E b S w X z k G Q J s 5 P Z Z M j M 7 D J z V w g h X 2 F h 4 6 / Y i N g K d v 6 N k 2 Q L T S w 8 M H A 4 5 1 z u 3 B O l U l j 0 / S 9 v Z X V t f W O z s F X c 3 t n d 2 y 8 d H D Z s k h n G 6 y y R i W l F 1 H I p N K + j Q M l b q e F U R Z I 3 o + H V 1 G 8 + c G N F o u 9 w l P J Q 0 b 4 W s W A U n X S P A 0 6 Q 2 q H t l s p + x Z + B L J M g J 2 X I 8 b 9 4 t / T Z 6 S U s U 1 w j k 9 T a d u C n G I 6 p Q c E k n x Q 7 m e U p Z U P a 5 2 1 H N V X c h u P Z V R N y 6 p Q e i R P j n k Y y U 3 9 O j K m y d q Q i l 1 Q U B 3 b R m 4 p / e e 0 M 4 4 t w L H S a I d d s v i j O J M G E T C s i P W E 4 Q z l y h D I j 3 F 8 J G 1 B D G b o i i + 7 0 Y P H Q Z d K o V g K / E t x W y 7 X L v L M C H M M J n E E A 5 1 C D a 7 i B O j B Q 8 A j P 8 O o 9 e S / e m / c + j 6 5 4 + c w R / I L 3 8 Q 2 W 4 J f S &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis space</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t L m I l j F W h X s / B u 7 + r 0 W b g / j 8 T             </p><formula xml:id="formula_17">V U = " &gt; A A A C o X i c f V F N b 9 N A E N 2 Y r x K + U j h y s b C Q E E K R 3 Q s c K + B Q D o i A m r S S 1 4 r G m 3 G y 6 n 5 Y u + O C Z f m f c I X / x L 9 h n Q a J t o i R V n</formula><formula xml:id="formula_18">u i k q R t C I y 4 + q h o V k 4 2 H O c Q r 6 V C Q a g M A 4 W T w G o s N O B A U p j U e c 4 N f h d U a z K r j x j r d 5 1 n R c Y U V c b V A R 0 n G n V x v i L v h F r p 8 j 6 F 7 h x + D k 0 8 1 O i D r X n Y c 3 F p L 0 4 d p r P m r A f 1 P C N / + C A O 6 b I E c G F 9 b j 3 3 H t 8 1 W 3 X H f h 2 V l V 1 d z H S w O p l k 6 z T 4 f J I d v d 2 v b Y 0 / Z M / a C Z e w 1 O 2 R H b M b m T L B z 9 p 3 9 Y D + j J P o Q z a I v F 9 J o t K t 5 w i 5 F l P 8 G k Z 7 T f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / p y n G g S U</formula><formula xml:id="formula_19">Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y</formula><formula xml:id="formula_20">Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y</formula><formula xml:id="formula_21">Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y</formula><formula xml:id="formula_22">K V s Q k e 8 Z 2 l M F T f B d H H 4 j F x Y Z U i i R N u K k S z U 3 x N T q o z J V W g 7 F c W x W f X m 4 n 9 e L 8 P o O p i K O M 2 Q x 2 y 5 K M o k w Y T M U y B D o T l D m V t C m R b 2 V s L G V F O G N q u y D c F f f X m d t O</formula><formula xml:id="formula_23">W y E R 3 Q 2 q 4 F D F v o U D J u 6 n m V I W S d 8 L J 7 d z v P H J t R B I / Y J 7 y Q N F R L C L B K F p p 4 F a M U H Y P E 5 i T U F A z c K t e z V u A r B O / I F U o 0 B y 4 X / 1 h w j L F Y 2 S S G t P z v R S D K d U o m O S z c j 8 z P K V s Q k e 8 Z 2 l M F T f B d H H 4 j F x Y Z U i i R N u K k S z U 3 x N T q o z J V W g 7 F c W x W f X m 4 n 9 e L 8 P o O p i K O M 2 Q x 2 y 5 K M o k w Y T M U y B D o T l D m V t C m R b 2 V s L G V F O G N q u y D c F f f X m d t O s 1 3 6 v 5 9 / V q 4 6 a I o w R n c A 6 X 4 M M V N O A O m t A C B</formula><formula xml:id="formula_24">Y i 1 A w N F Z 6 M K j H d K g w E D w 2 d F C p O j V n D r p K 3 I J U o U B z U P n q B w n L I h t m E r X u u U 5 q v B y V E U z y a b m f a Z 4 i G + O Q 9 y y N M e L a y + d X T + m 5 V Q I a J s o + u 3 y u / k 7 k G G k 9 i X w 7 G a E Z 6 W V v J v 7 n 9 T I T X n m 5 i N P M 8 J g t F o W Z p C a h s w p o I B R n R k 4 s Q a a E v Z W y E S p k x h Z V t i W 4 y 1 9 e J e 1 6 z X V q 7 l 2 9 2 r g u 6 i j B K Z z B B b h w C Q 2 4 h S a 0 g I G C Z 3 i F N / J E X s g 7 + V i M r p E i c w J / Q D 5 / A C p l k k U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 l G 3 s U B k K 4 E R Y d L 3 h Q H 7 d + r Y 7 j A = " &gt; A A A C G n i c j V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Y M 2 l g r m A 5 I z z O 3 t J U t 2 7 4 7 d P S E c + R 8 W N v 4 V G x E 7 s f H f u E m u 0 M T C B w O P 9 2 a Y e R O k g m v j u l / O y u r a + s Z m a a u 8 v b O 7 t 1 8 5 O G z p J F O U N W k i E t U J U D P B Y 9 Y 0 3 A j W S R V D G Q j W D k Z X U 7 / 9 w J T m S X x n x i n z J Q 5 i H n G K x k r 3 B v W I D B S G n M W G 9 C t V t + b O Q J a J V 5 A q F P h f e 7 / y 0 Q s T m k m 7 g Q r U u u u 5 q f F z V I Z T w S b l X q Z Z i n S E A 9 a 1 N E b J t J / P o k 3 I q V V C E i X K l</formula><formula xml:id="formula_25">G 3 s U B k K 4 E R Y d L 3 h Q H 7 d + r Y 7 j A = " &gt; A A A C G n i c j V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Y M 2 l g r m A 5 I z z O 3 t J U t 2 7 4 7 d P S E c + R 8 W N v 4 V G x E 7 s f H f u E m u 0 M T C B w O P 9 2 a Y e R O k g m v j u l / O y u r a + s Z m a a u 8 v b O 7 t 1 8 5 O G z p J F O U N W k i E t U J U D P B Y 9 Y 0 3 A j W S R V D G Q j W D k Z X U 7 / 9 w J T m S X x n x i n z J Q 5 i H n G K x k r 3 B v W I D B S G n M W G 9 C t V t + b O Q J a J V 5 A q F P h f e 7 / y 0 Q s T m k m 7 g Q r U u u u 5 q f F z V I Z T w S b l X q Z Z i n S E A 9 a 1 N E b J t J / P o k 3 I q V V C E i X K l</formula><formula xml:id="formula_26">G 3 s U B k K 4 E R Y d L 3 h Q H 7 d + r Y 7 j A = " &gt; A A A C G n i c j V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Y M 2 l g r m A 5 I z z O 3 t J U t 2 7 4 7 d P S E c + R 8 W N v 4 V G x E 7 s f H f u E m u 0 M T C B w O P 9 2 a Y e R O k g m v j u l / O y u r a + s Z m a a u 8 v b O 7 t 1 8 5 O G z p J F O U N W k i E t U J U D P B Y 9 Y 0 3 A j W S R V D G Q j W D k Z X U 7 / 9 w J T m S X x n x i n z J Q 5 i H n G K x k r 3 B v W I D B S G n M W G 9 C t V t + b O Q J a J V 5 A q F P h f e 7 / y 0 Q s T m k m 7 g Q r U u u u 5 q f F z V I Z T w S b l X q Z Z i n S E A 9 a 1 N E b J t J / P o k 3 I q V V C E i X K l</formula><formula xml:id="formula_27">O G z p J F O U N W k i E t U J U D P B Y 9 Y 0 3 A j W S R V D G Q j W D k Z X U 7 / 9 w J T m S X x n x i n z J Q 5 i H n G K x k r 3 B v W I D B S G n M W G 9 C t V t + b O Q J a J V 5 A q F P h f e 7 / y 0 Q s T m k m 7 g Q r U u u u 5 q f F z V I Z T w S b l X q Z Z i n S E A 9 a 1 N E b J t J / P o k 3 I q V V C E i X K l</formula><formula xml:id="formula_28">O G z p J F O U N W k i E t U J U D P B Y 9 Y 0 3 A j W S R V D G Q j W D k Z X U 7 / 9 w J T m S X x n x i n z J Q 5 i H n G K x k r 3 B v W I D B S G n M W G 9 C t V t + b O Q J a J V 5 A q F P h f e 7 / y 0 Q s T m k m 7 g Q r U u u u 5 q f F z V I Z T w S b l X q Z Z i n S E A 9 a 1 N E b J t J / P o k 3 I q V V C E i X K l</formula><formula xml:id="formula_29">O G z p J F O U N W k i E t U J U D P B Y 9 Y 0 3 A j W S R V D G Q j W D k Z X U 7 / 9 w J T m S X x n x i n z J Q 5 i H n G K x k r 3 B v W I D B S G n M W G 9 C t V t + b O Q J a J V 5 A q F P h f e 7 / y 0 Q s T m k m 7 g Q r U u u u 5 q f F z V I Z T w S b l X q Z Z i n S E A 9 a 1 N E b J t J / P o k 3 I q V V C E i X K l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Multitask Scaling Hypothesis</head><p>There are fewer representations that are competent for N tasks than there are for M &lt; N tasks. As we train more general models that solve more tasks at once, we should expect fewer possible solutions.</p><p>This has been previously termed as the Contravariance principle by <ref type="bibr" target="#b15">Cao &amp; Yamins (2024)</ref>, which states that the set of solutions to an easy goal is large, while the set of solutions to a challenging goal is comparatively smaller. Moreover, we argue that this narrower solution set also generalizes better. As data scales, models that optimize the empirical risk E x∼ dataset [L(f, x)] also improve on the population risk E x∼ reality [L(f, x)], and become better at capturing statistical structures of the true data generating process (reality).</p><p>Recent work has demonstrated a power law relationship between data scale and model performance <ref type="bibr" target="#b46">(Hestness et al., 2017)</ref>. This implies that with enough data (e.g., consisting of the entire internet and all offline scientific measurements) one ought to converge to a very small solution set with irreducible error -the inherent epistemic uncertainty of the world. As more models are trained on internet-scale data, the set of solutions that satisfies all data constraints must become relatively small.</p><p>In addition to data-scaling, many modern representation learning objectives L (f, x) directly optimize for multitask solving. Contrastive learning finds a distance structure over data samples that optimizes many classification tasks <ref type="bibr">(Arora et al., 2019b;</ref><ref type="bibr">Wang &amp; Isola, 2020;</ref><ref type="bibr">Tian et al., 2020b)</ref>. Masked Autoencoders <ref type="bibr" target="#b44">(He et al., 2021)</ref> optimize randomly sampled reconstruction tasks. In fact, autoregressive language modeling can also be seen as optimizing a diverse set of tasks <ref type="bibr" target="#b100">(Radford et al., 2019)</ref>. Such multi-task objectives may be more effective than single-task ones (e.g., ImageNet classification) due to the fact that they impose more task constraints on the representation, leading to a smaller and higher-quality solution space <ref type="bibr" target="#b18">(Chen et al., 2020;</ref><ref type="bibr" target="#b43">He et al., 2020;</ref><ref type="bibr" target="#b99">Radford et al., 2017;</ref><ref type="bibr">2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convergence via Model Capacity</head><p>Suppose there is a globally optimal representation for standard learning objectives. Then, under sufficient data, scaling a model (i.e., using larger function classes F ), as well as improved optimization , should be more effective at finding better approximations to this optimum, as illustrated in Figure <ref type="figure" target="#fig_2">5</ref>. With the same training objective, larger models, even of different architectures, will thus tend to converge toward this optimum. When different training objectives share similar minimizers, larger models are better at finding these minimizers, and will train to similar solutions over the training tasks. We summarize this hypothesis as follows:</p><p>Figure <ref type="figure">7</ref>. The Simplicity Bias Hypothesis: Larger models have larger coverage of all possible ways to fit the same data. However, the implicit simplicity biases of deep networks encourage larger models to find the simplest of these solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Capacity Hypothesis</head><p>Bigger models are more likely to converge to a shared representation than smaller models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convergence via Simplicity Bias</head><p>Arriving at the same mapping on the training data does not prohibit the models from developing distinct internal representations. It is not unreasonable to posit that the representations used to detect a dog in a 1M parameter model could be quite different than that used by a 1B parameter model. What would stop a billion-parameter (and counting) model from learning an overly complicated and distinct representation? One key factor might be simplicity bias:</p><p>The Simplicity Bias Hypothesis</p><p>Deep networks are biased toward finding simple fits to the data, and the bigger the model, the stronger the bias. Therefore, as models get bigger, we should expect convergence to a smaller solution space.</p><p>Such simplicity bias could be coming from explicit regularization R(f ) commonly used in deep learning (e.g., weight decay and dropout). However, even in the absence of external influences, deep networks naturally adhere to Occam's razor, implicitly favoring simple solutions that fit the data <ref type="bibr" target="#b112">(Solomonoff, 1964;</ref><ref type="bibr" target="#b38">Gunasekar et al., 2018;</ref><ref type="bibr">Arora et al., 2019a;</ref><ref type="bibr" target="#b129">Valle-Perez et al., 2019;</ref><ref type="bibr" target="#b48">Huh et al., 2023;</ref><ref type="bibr" target="#b24">Dingle et al., 2018;</ref><ref type="bibr" target="#b34">Goldblum et al., 2023)</ref>. Figure <ref type="figure">7</ref> visualizes how simplicity bias can drive convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">What representation are we converging to?</head><p>By now, we hope to have convinced the reader that task and data pressures, combined with increasing model capacity, can lead to convergence. We next turn our attention to what exactly is the endpoint of all this convergence.</p><p>Our central hypothesis, stated in Figure <ref type="figure">1</ref>, is that the representation we are converging toward is a statistical model of the underlying reality that generates our observations. Consistent with the multitask scaling hypothesis, such a representation would naturally be useful toward many tasks (or at least toward any task grounded in reality). Additionally, this representation might be relatively simple, assuming that scientists are correct in suggesting that the fundamental laws of nature are indeed simple functions <ref type="bibr" target="#b31">(Gell-Mann, 1995)</ref>, in line with the simplicity bias hypothesis.</p><p>But what exactly do we mean by "a statistical model of the underlying reality." In this section, we formalize one definition with concrete mathematical statements. Importantly, this section should be read as just one concrete candidate for the form of the platonic representation; other candidates could be arrived at from other modeling assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">An idealized world</head><p>We consider a world that works as follows, consistent with the cartoon in Figure <ref type="figure">1</ref>. The world consists of a sequence of T discrete events, denoted as Z ≜ [z 1 , . . . , z T ], sampled from some unknown distribution P(Z). Each event can be observed in various ways. An observation is a bijective, deterministic function obs : Z → • that maps events to an arbitrary measurement space, such as pixels, sounds, mass, force, torque, words, etc. Later, in Section 6, we discuss limitations and potential extensions to continuous and unbounded worlds, and stochastic observations, that could yield a model that better reflects real learning scenarios.</p><p>One can think of an event as corresponding to the state of the world at some point in time<ref type="foot" target="#foot_2">foot_2</ref> , but it is also fine to simply consider an event as any variable that indexes observations, with no further physical meaning<ref type="foot" target="#foot_3">foot_3</ref> .</p><p>In this idealized world, knowing P(Z) would be useful for many kinds of predictions; this would constitute a world model over the events that cause our observations <ref type="bibr" target="#b131">(Werbos, 1987;</ref><ref type="bibr" target="#b40">Ha &amp; Schmidhuber, 2018;</ref><ref type="bibr" target="#b103">Richens &amp; Everitt, 2024)</ref>. We will next show that a particular representation of P(Z) is recovered by certain contrastive learners.</p><p>discovered that color distances in learned language representations, when trained to predict cooccurrences in text <ref type="bibr" target="#b22">(Devlin et al., 2018)</ref>, closely mirror human perception of these distances, which we reproduce in Figure <ref type="figure">8</ref> with both contrastive and predictive models. Interestingly, they noted an increasing similarity as models scale larger and become better at modeling text cooccurrences. In Figure <ref type="figure">8</ref>, we also learn representations of color based on K PMI from cooccurrences in images. Indeed, learning cooccurrence statistics in either domain recovers roughly the same perceptual representation. Details of this experiment are described in Appendix D.</p><p>We believe that our simple model encapsulates essential aspects of complex real-world systems, and offers a path toward understanding the representation that models are converging to-a unified model that is proficient across various domains and modalities, grounded in the statistical properties of the underlying world. Section 6 further elaborates some limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">What are the implications of convergence?</head><p>Scaling is sufficient, but not necessarily efficient Our arguments are roughly in line with the claim that "scale is all you need" to reach high levels of intelligence. We have argued that as resources are scaled (# parameters, # datapoints, # flops), representations are converging, regardless of other modeling choices and even data modality. Does this mean that scale is all that matters? Not quite: different methods can scale with different levels of efficiency <ref type="bibr" target="#b46">(Hestness et al., 2017;</ref><ref type="bibr" target="#b57">Kaplan et al., 2020)</ref>, and successful methods must still satisfy some general requirements (e.g., be a consistent estimator, model pairwise statistics of P(Z)).</p><p>Training data can be shared across modalities Suppose you have access to N images and M sentences, and want to learn the best representation. If there is indeed a modality-agnostic platonic representation, then both image and language data should help find it. The implication is that if you want to train the best vision model, you should train not just on N images but also on M sentences. This is already becoming common practice <ref type="bibr">(OpenAI, 2023;</ref><ref type="bibr" target="#b101">Radford et al., 2021)</ref>. Many vision models are finetuned from pre-trained LLMs. The other direction is less common, but also is implied by our hypothesis: if you want to build the best LLM, you should also train on image data. Indeed, OpenAI (2023) showed that training on images improved performance on text. In theory, there should be some conversion ratio: a pixel is worth a words for training LLMs, and a word is worth b pixels for training vision models.</p><p>Ease of translation and adaptation across modalities When two representations are aligned, transitioning from one to the other should be a simple function that's easily obtained. Our hypothesis could explain the phenomenon that conditional generation is easier than unconditional <ref type="bibr" target="#b82">(Mirza &amp; Osindero, 2014;</ref><ref type="bibr" target="#b70">Liu et al., 2020;</ref><ref type="bibr" target="#b106">Sauer et al., 2022)</ref>, as the data we condition on may have the same platonic structure as the data we are generating. In line with this, recent work has found that representation-conditioning is even easier <ref type="bibr" target="#b65">(Li et al., 2023)</ref>. Similarly, representational convergence could act as a bridge that lets us find mappings between domains even without paired data; this may underlie the success of unpaired translation in vision <ref type="bibr" target="#b142">(Zhu et al., 2017;</ref><ref type="bibr" target="#b110">Shi et al., 2024;</ref><ref type="bibr" target="#b136">Xie et al., 2022)</ref> and language <ref type="bibr" target="#b126">(Tran et al., 2017;</ref><ref type="bibr" target="#b63">Lample et al., 2018)</ref>. We emphasize that this doesn't mean that models trained on a single modality (e.g., language) can immediately process raw data from another (e.g., vision). What makes them adaptable to the new modalities is that they share a common modality-agnostic representation, and can readily process representations of new modalities. Furthermore, this implies that language models would achieve some notion of grounding in the visual domain even in the absence of cross-modal data<ref type="foot" target="#foot_4">foot_4</ref> . The primary advantage of cross-modal data could then simply be sample efficiency.</p><p>Scaling may reduce hallucination and bias A prominent shortcoming of current LLMs is their propensity to hallucinate, or output false statements. If models are indeed converging toward an accurate model of reality, and scale powers this convergence, then we may expect hallucinations to decrease with scale. Of course, our hypothesis is conditioned on the training data for future models constituting a sufficiently lossless and diverse set of measurements. This may not come to pass, but it is an implication of our hypothesis worth pointing out. A similar argument can be made about certain kinds of bias. It has been shown that large models can exacerbate existing biases present in their training data <ref type="bibr" target="#b41">(Hall et al., 2022)</ref>. Our hypothesis implies that, while this may be true, we should expect larger models to amplify bias less. This does not mean bias will be removed, rather that the model's biases will more accurately reflect the data's biases, rather than exacerbating them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Counterexamples and limitations</head><p>Different modalities may contain different information One immediate objection to our hypothesis is: what about the information that is unique to a given modality? Can language really describe the ineffable experience of watching 5 words 10 words 20 words 30 words DCI Caption density 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 Alignment to vision ImageNet21K MAE DINOv2 CLIP CLIP (I12K ft) Figure 9. Increasing caption density improves alignment: We vary caption length using the Densely-Captioned-Images (DCI) dataset <ref type="bibr" target="#b128">(Urbanek et al., 2023)</ref>. Starting from a dense caption, we used LLaMA3-8B-Instruct (Meta, 2024) to summarize and generate coarse-grained captions. We compute the average alignment score across all vision and language models with standard deviation measured over the language models we evaluated. With denser captions, the mapping may become more bijective, leading to improved language-vision alignment scores.</p><p>a total solar eclipse? Or, how could an image convey the a concept like "I believe in the freedom of speech," which is easy to write in English? Two different models cannot converge to the same representation if they have access to fundamentally different information.</p><p>More precisely, our mathematical argument in Section 4 only strictly holds for bijective projections of Z, so that the information in all the projections is equivalent to the information in the underlying world. This will not hold true for either lossy or stochastic observation functions. Nonetheless, similar arguments have been made theoretically and empirically that cooccurrence relations are learned by practical contrastive <ref type="bibr">(Wang &amp; Isola, 2020;</ref><ref type="bibr" target="#b143">Zimmermann et al., 2021)</ref> and predictive learners <ref type="bibr" target="#b95">(Papyan et al., 2020;</ref><ref type="bibr" target="#b104">Roeder et al., 2021)</ref>. <ref type="bibr">Lu et al. (2021)</ref> and <ref type="bibr" target="#b81">Mirchandani et al. (2023)</ref> also showed that models trained to autoregressively generate text also capture statistical relations in many other modalities, including symbolic reasoning, vision, protein folding, and robotics.</p><p>A more nuanced version of our hypothesis will need to be developed to handle the case of non-bijective observations and abstract concepts. A starting point could be: different models will converge to the same representation when the input signals are sufficiently high information and the models are sufficiently high capacity; when they are not, the lower-information representation will only align with the higher-information one up to a level capped by the mutual information between the input signals and by the capacity of each model. This cap might or might not be practically important. Popular representations like CLIP are explicitly optimized to only capture the shared information between vision and language, yet are highly successful on many pure vision tasks. We perform a preliminary test of the effect of information level in Figure <ref type="figure">9</ref> (detailed in Appendix E), and find that the more descriptive (higher information) a caption is, the better its LLM representation aligns with the visual representation of the corresponding image.</p><p>Not all representations are presently converging Our argument has mainly focused on two modalities: vision and language. While we do expect other modalities will follow similar trends, we have yet to see the same level of convergence across all domains. For example, in robotics there is not yet a standardized approach to representing world states in the same way as there is for representing images and text. One limitation lies in the hardware used in robotics, which is often expensive and slow. This creates a bottleneck in the quantity and diversity of training data.</p><p>Sociological bias in producing AI models Researcher bias and collective preferences within the AI community have shaped the trajectory of model development. There is often an explicit or implicit goal of designing AI systems that mimic human reasoning and performance, and this could lead to convergence toward human-like representations even if other kinds of intelligence are in fact possible. Additionally, the "hardware lottery" <ref type="bibr" target="#b47">(Hooker, 2021)</ref> suggests that the success of AI models can also depend on the compatibility of their design with available computational architectures, further contributing to convergent trends.</p><p>Special-purpose intelligences might not converge Different intelligent systems can be designed to accomplish different tasks. For instance: A bioinformatics systems might predict protein structure; an autonomous vehicle might follow lanes on highways. It's possible that not much is shared between these two narrow tasks. Our argument only holds for intelligences that are optimized to perform well on many tasks. We have argued that a representation of reality is a structure that is useful across many tasks, but for any special purpose there may be shortcuts, or even effective representations detached from reality. Such shortcuts may be more efficient and necessary for continued improvements in specific domains. This will become more relevant if continued scaling comes up against boundary conditions around resources like energy and compute.</p><p>How do we measure alignment? We focused on one particular alignment measure, mutual nearest-neighbor, in our experiments, and cited experiments using several others. However, there is active debate on the merits and deficiencies of all these ways of measuring alignment <ref type="bibr" target="#b7">(Bansal et al., 2021;</ref><ref type="bibr" target="#b119">Sucholutsky et al., 2023)</ref>. We discuss our choice and show results for other alignment metrics in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lots left to explain</head><p>We have shown results where different models arrive at similar but not the same representations. For example, in Figure <ref type="figure" target="#fig_1">3</ref>, alignment clearly increases but only reaches a score of 0.16, according to our mutual nearest-neighbor metric. The maximum theoretical value for this metric is 1. Is a score of 0.16 indicative of strong alignment with the remaining gap being "noise" or does it signify poor alignment with major differences left to explain? We leave this as an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mutual k-Nearest Neighbor Alignment Metric</head><p>For two models with representations f , g the mutual k-nearest neighbor metric measures the average overlap of their respective nearest neighbor sets. In this section, we refer to this metric as m NN , which we will formally define below.</p><p>For cross-modal domains, define (x i , y i ) ∈ X as a sample from the data distribution X (e.g. image-caption dataset). For the single domain alignment measurements, the samples are equivalent x i = y i (e.g., images for vision, and text for language). Let {x i , y i } b i=1 be the corresponding mini-batch sampled from this data distribution. Then given two model representations f and g the corresponding features are: ϕ i = f (x i ) and ψ i = g(y i ), where the collection of these features are denoted as Φ = {ϕ 1 , . . . , ϕ b } and Ψ = {ψ 1 , . . . , ψ b }. Then for each feature pair (ϕ i , ψ i ), we compute the respective nearest neighbor sets S(ϕ i ) and S(ψ i ).</p><formula xml:id="formula_30">d knn (ϕ i , Φ \ ϕ i ) = S(ϕ i ) (9) d knn (ψ i , Ψ \ ψ i ) = S(ψ i )<label>(10)</label></formula><p>where d knn returns the set of indices of its k-nearest neighbors. Then we measure its average intersection via</p><formula xml:id="formula_31">m NN (ϕ i , ψ i ) = 1 k |S(ϕ i ) ∩ S(ψ i )|<label>(11)</label></formula><p>where | • | is the size of the intersection.</p><p>The choice to use mutual nearest-neighbors Our initial efforts to measure alignment with CKA revealed a very weak trend of alignment between models, even when comparing models within their own modality. This has also been observed by <ref type="bibr" target="#b7">(Bansal et al., 2021)</ref>, which had relied on alternative metrics such as model-stitching as it "reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot" <ref type="bibr" target="#b7">(Bansal et al., 2021)</ref>.</p><p>We chose to use nearest-neighbor as a metric, as methods like CKA has a very strict definition of alignment, which may not fit our current needs. For instance, understanding the precise similarity between unrelated items, such as an orange and Bill Gates, may not be critical.</p><p>Relationship between CKA and Mutual Nearest-Neighbors Let ϕ i ∈ R n and ψ i ∈ R m be vectorized features of two models (e.g. language and vision models). Let K ij = κ(ϕ i , ϕ j ) and L ij = κ(ψ i , ψ j ) be the kernel matrices computed from a dataset using some kernel-function κ. Using an inner-product kernel, the ij-th entry of the centered counterpart of these Kernel matrices is:</p><formula xml:id="formula_32">Kij = ⟨ϕ i , ϕ j ⟩ -E l [⟨ϕ i , ϕ l ⟩] Lij = ⟨ψ i , ψ j ⟩ -E l [⟨ψ i , ψ l ⟩]<label>(12)</label></formula><p>Then, the cross-covariance of K and L is given by:</p><formula xml:id="formula_33">HSIC(K, L) = 1 (n -1) 2 Trace( KL )<label>(13)</label></formula><p>which serves as an empirical estimator of the Hilbert-Schmidt Independence Criterion <ref type="bibr" target="#b36">(Gretton et al., 2005)</ref>. The Centered Kernel Alignment (CKA) <ref type="bibr" target="#b60">(Kornblith et al., 2019)</ref> is then its normalized counterpart:</p><formula xml:id="formula_34">CKA(K, L) = HSIC(K, L) HSIC(K, K)HSIC(L, L)<label>(14)</label></formula><p>CKA measures the congruence between two random variables, with a maximum alignment of 1 and a minimum of 0. It is invariant to isotropic scaling and offers a strict notion of alignment, measuring alignment across all samples. Hence, the CKA score reflects the global similarities of the models. This can be illustrated by expanding the trace term in HSIC:</p><formula xml:id="formula_35">Trace( KL ) = i j (⟨ϕ i , ϕ j ⟩ -E l [⟨ϕ i , ϕ l ⟩]) (⟨ψ i , ψ j ⟩ -E l [⟨ψ i , ψ l ⟩])<label>(15)</label></formula><p>One can modify the definition of alignment to restrict the cross-covariance measurement to samples considered to be nearest neighbors of the current sample i. This emphasizes similarity over dissimilarity, biasing the measure toward local alignment:</p><formula xml:id="formula_36">Align knn (K, L) = i j α(i, j) • (⟨ϕ i , ϕ j ⟩ -E l [⟨ϕ i , ϕ l ⟩]) (⟨ψ i , ψ j ⟩ -E l [⟨ψ i , ψ l ⟩])<label>(16)</label></formula><p>where</p><formula xml:id="formula_37">α(i, j) = 1[ϕ j ∈ knn(ϕ i ) ∧ ψ j ∈ knn(ψ i ) ∧ i ̸ = j]<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Consistency across various metrics</head><p>We describe the metrics in Table <ref type="table">11</ref> and their corresponding properties. The symmetric property implies that the metric is symmetric with respect to the data points d(x, y) = d(y, x). The global property means all samples are used to compute the distance with respect to every sample. The ordinal property is when the ordering of the distance is taken into consideration. For example, mutual nearest neighbor is not ordinal since the nearest neighbors {a, b, c} and {c, a, b} are treated equally. The batchable property is a computational property that makes it feasible to compute in a reasonable time frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-vision comparison</head><p>In Figure <ref type="figure" target="#fig_0">12</ref>, we evaluate Spearman's rank correlation among different metrics and hyperparameters over 78 vision models (details in Appendix C.1). We find most metrics highly correlated with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-modal comparison</head><p>We measure vision-language alignment using a range of alternative metrics. We visualize the corresponding alignment results in Figure <ref type="figure" target="#fig_1">13</ref> and Figure <ref type="figure">14</ref>. Our findings indicate that alignment sensitivity not only depends on the metric used to compute it but also varies according to the specific tasks on which the vision models are trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Property</head><p>Description symmetric global ordinal batchable CKA ✓ ✓ ✓ ✓ Centered Kernel Alignment (CKA; Kornblith et al. (2019)) measures the similarity of neural networks by comparing the alignment of their kernel induced by their feature spaces. Unbiased CKA ✓ ✓ ✓ ✓ Unbiased estimator of CKA that corrects for sample bias in HSIC (Song et al., 2012). SVCCA ✓ ✓ ✓ ✓ Singular Value Canonical Correlation Analysis (SVCCA; Raghu et al. (2017)) compares neural networks by decomposing their activities into singular vectors and measuring correlation. Mutual k-NN ✓ ✓ Measures the intersection over union (IoU) of nearest neighbors between two models. CKNNA ✓ ✓ * ✓ ✓ Modified CKA measure that computes the kernel alignment only for its nearest neighbors. See Appendix A. Cycle k-NN ✓ Measures whether the nearest neighbor in one domain also considers the original sample as its nearest neighbor in the other domain. Edit k-NN ✓ ✓ * ✓ Computes the edit distance required to match the nearest neighbors between two datasets. The score is normalized by the maximum edit distance. LCS k-NN ✓ ✓ * ✓ Calculates the longest common subsequence of nearest neighbors and is normalized by the sequence length. Figure 11. Comparative analysis of neural network similarity metrics. ✓ * indicates the metric is global and still meaningful when the nearest neighbor k is set to maximum batch-size k = |X |. bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.30 0.33 0.35 0.38 0.40 0.43 0.45 0.48 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.24 0.26 0.28 0.30 0.32 0.34 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.30 0.33 0.35 0.38 0.40 0.43 0.45 0.48 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.33 0.35 0.38 0.40 0.43 0.45 0.48 0.50 0.53 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (a) CKA bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.20 0.22 0.24 0.26 0.28 0.30 0.32 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.20 0.22 0.24 0.26 0.28 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.28 0.30 0.32 0.34 0.36 0.38 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.32 0.34 0.36 0.38 0.40 0.42 0.44 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.24 0.26 0.28 0.30 0.32 0.34 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (b) Unbiased CKA bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.34 0.36 0.38 0.40 0.42 0.44 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.32 0.34 0.36 0.38 0.40 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.38 0.40 0.42 0.44 0.46 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.42 0.44 0.46 0.48 0.50 0.52 0.54 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.36 0.38 0.40 0.42 0.44 0.46 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (c) SVCCA bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.09 0.10 0.11 0.12 0.13 0.14 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.06 0.07 0.08 0.09 0.10 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.12 0.14 0.16 0.18 0.20 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (d) Mutual k-NN (k = 10) Figure 13. Cross-modal alignment for various metrics bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.10 0.12 0.14 0.16 0.18 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.06 0.07 0.08 0.09 0.10 0.11 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.10 0.12 0.14 0.16 0.18 0.20 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.14 0.16 0.18 0.20 0.22 0.24 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.12 0.14 0.16 0.18 0.20 0.22 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (a) CKNNA (k = 10) bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.45 0.48 0.50 0.53 0.55 0.58 0.60 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.50 0.55 0.60 0.65 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.55 0.58 0.60 0.62 0.65 0.68 0.70 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.50 0.55 0.60 0.65 0.70 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (b) Cycle k-NN (k = 10) bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.75 0.75 0.75 0.75 0.75 0.76 0.76 0.76 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.75 0.75 0.75 0.75 0.75 0.76 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.75 0.76 0.76 0.76 0.76 0.76 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.76 0.76 0.76 0.76 0.76 0.76 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.75 0.76 0.76 0.76 0.76 0.76 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft (c) Edit-distance k-NN (k = 10) bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.16 0.18 0.20 0.22 0.24 0.26 0.28 Alignment to ImageNet21K vit tiny vit small vit base vit large bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.12 0.14 0.16 0.18 Alignment to MAE mae base mae large mae huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.20 0.22 0.24 0.26 0.28 0.30 0.32 Alignment to DINOv2 dino small dino base dino large dino giant bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 Alignment to CLIP clip base clip large clip huge bloom0.56b bloom1.1b bloom1.7b bloom3b bloom7b openllama3b openllama7b openllama13b llama7b llama13b llama33b llama65b 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 Alignment to CLIP (I12K ft) clip base i1k ft clip large i1k ft clip huge i1k ft </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Color Cooccurrence Experiment</head><p>Here we describe the details of how we created the four color representations visualized in Figure <ref type="figure">8</ref>, from left to right.</p><p>Perceptual representation from CIELAB color space We embed pixels taken from the CIFAR-10 image dataset <ref type="bibr" target="#b61">(Krizhevsky et al., 2009;</ref><ref type="bibr" target="#b124">Torralba et al., 2008)</ref> based on the CIELAB color space, which is designed as a perceptually uniform space that changes numerical values correspond to similar perceived changes in color.</p><p>Three representations from cooccurrence in VISION and LANGUAGE For these three representations, we first obtain a dissimilarity matrix over colors (in different ways detailed below), then use multidimensional scaling <ref type="bibr" target="#b109">(Shepard, 1980)</ref> to find a 3-dimensional embedding in which Euclidean distance between the embeddings for A and B, z A and z B , best matches this dissimilarity matrix. We use 1,000 fits and take the best match. Afterward, we visually align it with the CIELAB space by finding the best rotation, translation, scaling, and flipping, by running the Kabsch-Umeyama algorithm <ref type="bibr" target="#b55">(Kabsch, 1976;</ref><ref type="bibr" target="#b56">1978;</ref><ref type="bibr" target="#b127">Umeyama, 1991)</ref> twice, once on z and once on -z, to account for flipping. The dissimilarity matrix we used in each case is described as following:</p><p>• VISION: Pixel cooccurrence. We collect color cooccurrence statistics from the CIFAR-10 dataset, and estimate a joint distribution p(A, B) over 300,000 randomly sampled pixel colors A and B that occur within a radius of at most 4 pixels of one another. Colors are quantized on a grid in RGB space and represented as discrete variables, and p(A, B) is modeled as a table of normalized counts, from which we compute the empirical pointwise mutual information matrix K PMI (A, B). Quantization ensures that there is no bias from how color distances are represented in RGB space. Dissimilarity matrix is defined as -K PMI (A, B) + c, where c = max A,B K PMI (A, B) is an offset to ensure non-negativity (similar to the constant in Section 4.2 and Proposition F.1 that ensures neural networks can express K PMI ).</p><p>• LANGUAGE. We used an approach similar to <ref type="bibr" target="#b0">Abdou et al. (2021)</ref>.</p><p>-We take 20 pairs of (color, word) appeared in the dataset collected by <ref type="bibr" target="#b68">Lindsey &amp; Brown (2014)</ref>, where 51 participants were asked to free name each of the 330 colors from the Munsell Color Chart. We filtered words that appeared less than 100 times, and computed each word's associate color by taking the centroid in CIELAB space. Our filtering process followed <ref type="bibr" target="#b0">Abdou et al. (2021)</ref> exactly, but resulted in 20 colors, a slightly different set than the 18 colors they claimed.</p><p>-For each of the 20 color words &lt;col&gt;, we construct three sentences:</p><p>The color &lt;col&gt;.</p><p>This color is &lt;col&gt;.</p><p>The color of this thing is &lt;col&gt;.</p><p>and obtain the average sentence embedding from the language encoder, as the embedding for &lt;col&gt; (details below). We find this approach more effective than <ref type="bibr" target="#b0">Abdou et al. (2021)</ref>, which uses object names that potentially have color biases, even though the objects may appear in multiple colors.</p><p>-Unlike <ref type="bibr" target="#b0">Abdou et al. (2021)</ref>, we did not perform linear regression from language embedding to CIELAB space, which distorts distances and easily overfits with only 20 samples. Instead, we used multidimensional scaling to best preserve distances, as described above.</p><p>-Masked language contrastive learning (SimCSE) embedding: We used sentence embedding from the unsupervised SimCSE RoBERTa-L <ref type="bibr">(Gao et al., 2021)</ref> to encode the above sentences into 1024-dimensional embeddings, and used the pairwise Euclidean distances among &lt;col&gt; embeddings as the dissimilarity matrix.</p><p>-Masked language predictive learning (RoBERTa) embedding: We concatenated hidden states of the last four layers of RoBERTa-L <ref type="bibr" target="#b71">(Liu et al., 2019)</ref>, following <ref type="bibr" target="#b22">(Devlin et al., 2018)</ref>. We averaged across token dimensions, and obtained a 4096-dimensional embedding for each of the above sentences, and used the pairwise Euclidean distances among &lt;col&gt; embeddings as the dissimilarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Caption Density Experiments</head><p>We use LLaMA3-8B-Instruct (Meta, 2024) to generate summary captions at various densities for images in the Densely Captioned Images dataset <ref type="bibr" target="#b128">(Urbanek et al., 2023)</ref> from the train split. Following <ref type="bibr" target="#b128">Urbanek et al. (2023)</ref>, we prompt the language model with the following instructions to generate captions at differing granularity:</p><p>system: You are given a full-text description of an image. You should summarize it into about &lt;num_words&gt; words, being sure to include as much salient visual information as possible given the &lt;num_words&gt; word constraint, especially information from the start of the original description. The new description should apply for the original image. Respond with only the summary, in one line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>user: &lt;original_caption&gt;</head><p>We measure the alignment with this generated caption to test our hypothesis that denser captations would result in higher alignment scores. In Figure <ref type="figure">9</ref>, we find that the alignment score also improves as caption length increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analysis of Contrastive Learners</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Contrastive objectives learn pointwise mutual information</head><p>There are two widely used forms of contrastive objectives. We now discuss each form in detail and show how they both are minimized by the pointwise mutual information (PMI) as stated in Equation (5). To simplify notation, we consider learning the bivariate model g(x a , x b ) ∈ R. In Section 4, such g is optimized within the family of {g = ⟨f X , f X ⟩ : f X ∈ F X }.</p><p>Recall that our positive pairs are sampled from (x, x + ) ∼ P coor , and that the negative pairs are sampled independently from its marginals which we denote as (x, x -)</p><p>i.i.d.</p><p>∼ P where P (x) = x+ P coor (x, x + ).</p><p>1. The binary NCE loss <ref type="bibr" target="#b39">(Gutmann &amp; Hyvärinen, 2010)</ref> is defined with a certain prior over sampling positive vs. negative pairs. Let p pos be the probability of sampling a positive pair. Then the loss is given by 2. The InfoNCE loss <ref type="bibr" target="#b91">(Oord et al., 2018)</ref> is defined with randomly sampling one positive pair along with K negative ones. With some hyperparameter τ &gt; 0, the loss is given by</p><formula xml:id="formula_38">L InfoNCE (g) ≜ E (x,x+)∼Pcoor (x (1) -,x<label>(2)</label></formula><p>-,...,x (K) -)</p><p>i.i.d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∼ P</head><p>log e g(x,x+)/τ e g(x,x+)/τ + K i=1 e g(x,x (i)</p><formula xml:id="formula_39">-)/τ . (<label>26</label></formula><formula xml:id="formula_40">)</formula><p>The Bayes optimal solution is given by e g(x,x+)/τ e g(x,x+)/τ + K i=1 e g(x,x (i) -)/τ = P coor (x + | x) j P (x .</p><p>(28)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. VISION models converge as COM-PETENCE increases: We measure alignment among 78 models using mutual nearestneighbors on Places-365<ref type="bibr" target="#b141">(Zhou et al., 2017)</ref>, and evaluate their performance on downstream tasks from the Visual Task Adaptation Benchmark (VTAB;<ref type="bibr" target="#b139">Zhai et al. (2019)</ref>). LEFT: Models that solve more VTAB tasks tend to be more aligned with each other. Error bars show standard error. RIGHT: We use UMAP to embed models into a 2D space, based on distance ≜ -log(alignment). More competent and general models (blue) have more similar representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. LANGUAGE and VISION models align: We measure alignment using mutual nearest-neighbor on the Wikipedia caption dataset (WIT)<ref type="bibr" target="#b115">(Srinivasan et al., 2021)</ref>. The x-axis is the language model performance measured over 4M tokens from the OpenWebText dataset<ref type="bibr" target="#b33">(Gokaslan &amp; Cohen, 2019)</ref> (see Appendix B for plots with model names). We measure performance using 1bits-per-byte, where bits-per-byte normalizes the cross-entropy by the total bytes in the input text string. The results show a linear relationship between language-vision alignment and language modeling score, where a general trend is that more capable language models align better with more capable vision models. We find that CLIP models, which are trained with explicit language supervision, exhibit a higher level of alignment. However, this alignment decreases after being fine-tuned on ImageNet classification (labeled CLIP (I12K ft)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The Capacity Hypothesis: If an optimal representation exists in function space, larger hypothesis spaces are more likely to cover it. LEFT: Two small models might not cover the optimum and thus find different solutions (marked by outlined ☆). RIGHT: As the models become larger, they cover the optimum and converge to the same solution (marked by filled ⋆).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>y W e O 4 B e c z 2 8 O b 5 k T &lt; / l a t e x i t &gt;Solves task&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v H e 9 m N 3 f l A + x m k c J N F X c K e X 8 k H A = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k v e i x 6 8 V j R f k A a y m a 7 a Z d u s m F 3 U i i h P 8 O L B 0 W 8 + m u 8 + W / c t j l o 6 4 O B x 3 s z z M w L U y k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>e y t h I 2 o p g x t S m U b g r f 6 8 j p p 1 2 u e W / M e 6 t X G b R F H C c 7 h A q 7 A g 2 t o w D 0 0 o Q U M F D z D K 7 w 5 6 L w 4 7 8 7 H s n X D K W b O 4 A + c z x 8 + n J E 1 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4= " o L u S o E d J x + d H x G x H z s U U d y q X K S E = " &gt; A A A C F 3 i c j V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r t p t A z a W C q a B 2 y W M D u Z T Y b M 7 i w z d w M h 5 D M s b P w V G x F b 7 f w b J 8 k W m l h 4 Y O B w z r n c u S d M p T D o u l 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j W N y j T j D a a k 0 u 2 Q G i 5 F w h s o U P J 2 q j m N Q 8 l b 4 f B 6 5 r d G X B u h k g c c p z y I a T 8 R k W A U r e T f K z n i h i A 1 w 2 6 5 4 l b d O c g q 8 X J S g R z / i 3 f L n 5 2 e Y l n M E 2 S S G u N 7 b o r B h G o U T P J p q Z M Z n l I 2 p H 3 u W 5 r Q m J t g M r 9 r S s 6 s 0 i O R 0 v Y l S O b q z 4 k J j Y 0 Z x 6 F N x h Q H Z t m b i X 9 5 f o b R Z T A R S Z o h T 9 h i U Z R J g o r M S i I 9 o T l D O b a E M i 3 s X w k b U E 0 Z 2 i p L 9 n R v + d B V 0 q x VP b f q 3 d U q 9 a u 8 s y K c w C m c g w c X U I c b u I U G M F D w C M / w 6 j w 5 L 8 6 b 8 7 6 I F p x 8 5 h h + w f n 4 B j T t m K 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4= " o L u S o E d J x + d H x G x H z s U U d y q X K S E = " &gt; A A A C F 3 i c j V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r t p t A z a W C q a B2 y W M D u Z T Y b M 7 i w z d w M h 5 D M s b P w V G x F b 7 f w b J 8 k W m l h 4 Y O B w z r n c u S d M p T D o u l 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j W N y j T j D a a k 0 u 2 Q G i 5 F w h s o U P J 2 q j m N Q 8 l b 4 f B 6 5 r d G X B u h k g c c p z y I a T 8 R k W A U r e T f K z n i h i A 1 w 2 6 5 4 l b d O c g q 8 X J S g R z / i 3 f L n 5 2 e Y l n M E 2 S S G u N 7 b o r B h G o U T P J p q Z M Z n l I 2 p H 3 u W 5 r Q m J t g M r 9 r S s 6 s 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>8 s y K c w C m c g w c X U I c b u I U G M F D w C M / w 6 j w 5 L 8 6 b 8 7 6 I F p x 8 5 h h + w f n 4 B j T t m K 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o L u S o E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>8 s y K c w C m c g w c X U I c b u I U G M F D w C M / w 6 j w 5 L 8 6 b 8 7 6 I F p x 8 5 h h + w f n 4 B j T t m K 4 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; Simple &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>B X L d e u 8 z g K c A p n c A E B X E I N b q E O D W D w C M / w C m 9 e 4 r 1 4 7 9 7 H o n X N y 2 d O 4 A + 8 z x + W + 4 8 e &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D 0 y K 0 w 0 W + m M u O E w U p m 4 g 6 2 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>r b v x I 2 p I o y Y 1 s s 2 d O 9 x U O X S b N W 9 d y q d 1 O r 1 C / y z o p w B M d w C h 6 c Q R 2 u 4 B o a w O A e H u E Z X p 0 n 5 8 V 5 c 9 7 n 0 Y K T z x z C L z g f 3 0 Z 6 l p c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D 0 y K 0 w 0 W + m M u O E w U p m 4 g 6 2 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>r b v x I 2 p I o y Y 1 s s 2 d O 9 x U O X S b N W 9 d y q d 1 O r 1 C / y z o p w B M d w C h 6 c Q R 2 u 4 B o a w O A e H u E Z X p 0 n 5 8 V 5 c 9 7 n 0 Y K T z x z C L z g f 3 0 Z 6 l p c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D 0 y K 0 w 0 W + m M u O E w U p m 4 g 6 2 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>r b v x I 2 p I o y Y 1 s s 2 d O 9 x U O X S b N W 9 d y q d 1 O r 1 C / y z o p w B M d w C h 6 c Q R 2 u 4 B o a w O A e H u E Z X p 0 n 5 8 V 5 c 9 7 n 0 Y K T z x z C L z g f 3 0 Z 6 l p c = &lt; / l a t e x i t &gt; functions &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h a 6 + c Y d M c I x v O C s 6 8 2 0 n v 4 v Y s 8 Q = " &gt; A A A B 8 H i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z b n R Z d O O y g r 1 I O 5 R M m m l D k 8 y Q Z I Q y 9 C n c u F D E r Y / j z r c x 0 8 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i 0 s b m 1 v V P e r e z t H x w e V Y 9 P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>r 7 5 s 3 u m 5 m y V t J T m v 4 a R T d u 3 r p 9 Z + / u + N 7 9 B w 8 f T f Y f L 7 x t n M C 5 s M q 6 0 x I 8 K m l w T p I U n t Y O Q Z c K T 8 q z d 0 P + 5 B y d l 9 Y c U 1 t j o W F t Z C U F U K C W k 8 l R W 1 v a o J c + 9 j U I X E 6 S d J p u I 7 4 O s h 1 I 2 C 5 m y / 1 R y 1 d W N B o N C Q X e 5 1 l a U 9 G B I y k U 9 m P e e A w v n 8 E a 8 w A N a P R F t 7 X e x 8 8 D s 4 o r 6 8 I x F G / Z v y s 6 0 N 6 3 u g x K D b T x V 3 M D + a 9 c 3 l D 1 p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " DE U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / py n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><figDesc>r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " DE U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / py n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; simplicity bias &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 h Y X 6 W j y 0 V H K y x p l o v o p / 5 + 4 + q Y = " &gt; A A A B + H i c b V B N S 8 N A E J 3 4 W e t H o x 6 9 L B b B U 0 l 6 0 W P R i 8 c K 9 g P a U D b b T b t 0 N w m 7 E y G W / h I v H h T x 6 k / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f z s b m 1 v b O b m m v v H 9 w e F R x j 0 / a J s k 0 4 y 2 W y E R 3 Q 2 q 4 F D F v o U D J u 6 n m V I W S d 8 L J 7 d z v P H J t R B I / Y J 7 y Q N F R L C L B K F p p 4 F a M U H Y P E 5 i T U F A z c K t e z V u A r B O / I F U o 0 B y 4 X / 1 h w j L F Y 2 S S G t P z v R S D K d U o m O S z c j 8 z P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><figDesc>s 1 3 6 v 5 9 / V q 4 6 a I o w R n c A 6 X 4 M M V N O A O m t A C B h k 8 w y u 8 O U / O i / P u f C x b N 5 x i 5 h T + w P n 8 A f m y k 0 U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0 x + A 4 o = " &gt; A A A C H X i c j V C 7 S g N B F L 3 r M 8 Z HV i 1 t B o N g F X b T a B m 0 s V Q w D 0 i W M D u Z T Y b M 7 C 4 z d 4 U l 5 E s s b P w V G x E L G / F v n C R b a G L h g Y H D O f d y 5 5 w w l c K g 5 3 0 5 a + s b m 1 v b p Z 3 y 7 t 7 + Q c U 9 P G q Z J N O M N 1 k i E 9 0 J q e F S x L y J A i X v p J p T F U r e D s f X M 7 / 9 w L U R S X y P e c o D R Y e x i A S j a K W + W z F C 2 T t M Y E 5 C Q U 3 f r X o 1 b w 6 y S v y C V K H A / 8 b 7 7 k d v k L B M 8 R i Z p M Z 0 f S / F Y E I 1 C i b 5 t N z L D E 8 p G 9 M h 7 1 o a U 8 V N M J m n m 5 I z q w x I l G j 7 Y i R z 9 e f G h C p j c h X a S U V x Z J a 9 m f i X 1 8 0 w u g w m I k 4 z 5 D F b H I o y S T A h s 6 r I Q G j O U O a W U K a F / S t h I 6 o p Q 1 t o 2 U b 3 l 4 O u k l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt;/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0x + A 4 o = " &gt; A A A C H X i c j V C 7 S g N B F L 3 r M 8 Z H V i 1 t B o N g F X b T a B m 0 s V Q w D 0 i W M D u Z T Y b M 7 C 4 z d 4 U l 5 E s s b P w V G x E L G / F v n C R b a G L h g Y H D O f d y 5 5 w w l c K g 5 3 0 5 a + s b m 1 v b p Z 3 y 7 t 7 + Q c U 9 P G q Z J N O M N 1 k i E 9 0 J q e F S x L y J A i X v p J p T F U r e D s f X M 7 / 9 w L U R S X y P e c o D R Y e x i A S j a K W + W z F C 2 T t M Y E 5 C Q U 3 f r X o 1 b w 6 y S v y C V K H A / 8 b 7 7 k d v k L B M 8 R i Z p M Z 0 f S / F Y E I 1 C i b 5 t N z L D E 8 p G 9 M h 7 1 o a U 8 V N M J mn m 5 I z q w x I l G j 7 Y i R z 9 e f G h C p j c h X a S U V x Z J a 9 m f i X 1 8 0 w u g w m I k 4 z 5 D F b H I o y S T A h s 6 r I Q G j O U O a W U K a F / S t h I 6 o p Q 1 t o 2 U b 3 l 4 O u k l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0x + A 4 o = " &gt; A A A C H X i c j V C 7 S g N B F L 3 r M 8 Z H V i 1 t B o N g F X b T a B m 0 s V Q w D 0 i W M D u Z T Y b M 7 C 4 z d 4 U l 5 E s s b P w V G x E L G / F v n C R b a G L h g Y H D O f d y 5 5 w w l c K g 5 3 0 5 a + s b m 1 v b p Z 3 y 7 t 7 + Q c U 9 P G q Z J N O M N 1 k i E 9 0 J q e F S x L y J A i X v p J p T F U r e D s f X M 7 / 9 w L U R S X y P e c o D R Y e x i A S j a K W + W z F C 2 T t M Y E 5 C Q U 3 f r Xo 1 b w 6 y S v y C V K H A / 8 b 7 7 k d v k L B M 8 R i Z p M Z 0 f S / F Y E I 1 C i b 5 t N z L D E 8 p G 9 M h 7 1 o a U 8 V N M J m n m 5 I z q w x I l G j 7 Y i R z 9 e f G h C p j c h X a S U V x Z J a 9 m f i X 1 8 0 w u g w m I k 4 z 5 D F b H I o y S T A h s 6 r I Q G j O U O a W U K a F / S t h I 6 o p Q 1 t o 2 U b 3 l 4 O u k l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt; / l a t e x i t &gt; simplicity bias &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 h Y X 6 W j y 0 V H K y x p l o v o p / 5 + 4 + q Y = " &gt; A A A B + H i c b V B N S 8 N A E J 3 4 W e t H o x 6 9 L B b B U 0 l 6 0 W P R i 8 c K 9 g P a U D b b T b t 0 N w m 7 E y G W / h I v H h T x 6 k / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f z s b m 1 v b O b m m v v H 9 w e F R x j 0 / a J s k 0 4 y 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><figDesc>h k 8 w y u 8 O U / O i / P u f C x b N 5 x i 5 h T + w P n 8 A f m y k 0 U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0 x + A 4 o = " &gt; A A A C H X i c j V C 7 S g N B F L 3 r M 8 Z HV i 1 t B o N g F X b T a B m 0 s V Q w D 0 i W M D u Z T Y b M 7 C 4 z d 4 U l 5 E s s b P w V G x E L G / F v n C R b a G L h g Y H D O f d y 5 5 w w l c K g 5 3 0 5 a + s b m 1 v b p Z 3 y 7 t 7 + Q c U 9 P G q Z J N O M N 1 k i E 9 0 J q e F S x L y J A i X v p J p T F U r e D s f X M 7 / 9 w L U R S X y P e c o D R Y e x i A S j a K W + W z F C 2 T t M Y E 5 C Q U 3 f r X o 1 b w 6 y S v y C V K H A / 8 b 7 7 k d v k L B M 8 R i Z p M Z 0 f S / F Y E I 1 C i b 5 t N z L D E 8 p G 9 M h 7 1 o a U 8 V N M J m n m 5 I z q w x I l G j 7 Y i R z 9 e f G h C p j c h X a S U V x Z J a 9 m f i X 1 8 0 w u g w m I k 4 z 5 D F b H I o y S T A h s 6 r I Q G j O U O a W U K a F / S t h I 6 o p Q 1 t o 2 U b 3 l 4 O u k l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0 x + A 4 o = " &gt; A A A C H X i c j V C 7 S g N B F L 3 r M 8 Z H V i 1 t B o N g F X b T a B m 0 s V Q w D 0 i W M D u Z T Y b M 7 C 4 z d 4 U l 5 E s s b P w V G x E L G / F v n C R b a G L h g Y H D O f d y 5 5 w w l c K g 5 3 0 5 a + s b m 1 v b p Z 3 y 7 t 7 + Q c U 9 P G q Z J N O M N 1 k i E 9 0 J q e F S x L y J A i X v p J p T F U r e D s f X M 7 / 9 w L U R S X y P e c o D R Y e x i A S j a K W + W z F C 2 T t M Y E 5 C Q U 3 f r X o 1 b w 6 y S v y C V K H A / 8 b 7 7 k d v k L B M 8 R i Z p M Z 0 f S / F Y E I 1 C i b 5 t N z L D E 8 p G 9 M h 7 1 o a U 8 V N M J m n m 5 I z q w x I l G j 7 Y i R z 9 e f G h C p j c h X a S U V x Z J a 9 m f i X 1 8 0 w u g w m I k 4 z 5 D F b H I o y S T A h s 6 r I Q G j O U O a W U K a F / S t h I 6 o p Q 1 t o 2 U b 3 l 4 O uk l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0x + A 4 o = " &gt; A A A C H X i c j V C 7 S g N B F L 3 r M 8 Z H V i 1 t B o N g F X b T a B m 0 s V Q w D 0 i W M D u Z T Y b M 7 C 4 z d 4 U l 5 E s s b P w V G x E L G / F v n C R b a G L h g Y H D O f d y 5 5 w w l c K g 5 3 0 5 a + s b m 1 v b p Z 3 y 7 t 7 + Q c U 9 P G q Z J N O M N 1 k i E 9 0 J q e F S x L y J A i X v p J p T F U r e D s f X M 7 / 9 w L U R S X y P e c o D R Y e x i A S j a K W + W z F C 2 T t M Y E 5 C Q U 3 f r X o 1 b w 6 y S v y C V K H A / 8 b 7 7 k d v k L B M 8 R i Z p M Z 0 f S / F Y E I 1 C i b 5 t N z L D E 8 p G 9 M h7 1 o a U 8 V N M J m n m 5 I z q w x I l G j 7 Y i R z 9 e f G h C p j c h X a S U V x Z J a 9 m f i X 1 8 0 w u g w m I k 4 z 5 D F b H I o y S T A h s 6 r I Q G j O U O a W U K a F / S t h I 6 o p Q 1 t o 2 U b 3 l 4 O u k l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt; / l a t e x i t &gt; task gradient &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 g X g Y d d U Q b J O g b 2 V b x i y 2 P Q + 7 d I = " &gt; A A A B 9 X i c b V C 7 S g N B F L 3 j M 8 Z X 1 N J m M A h W Y T e N l k E b y w j m A c k a 7 s 7 O J k N m H 8 z M K m H J f 9 h Y K G L r v 9 j 5 N 0 6 S L T T x w M D h n H u 4 d 4 6 f S q G N 4 3 y T t f W N z a 3 t 0 k 5 5 d 2 / / 4 L B y d N z W S a Y Y b 7 F E J q r r o + Z S x L x l h J G 8 m y q O k S 9 5 x x / f z P z O I 1 d a J P G 9 m a T c i 3 A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><figDesc>r 1 w p v 6 c y F F q P Z a B 7 Z R o h n r R m 4 p / e d 3 M R B d + z u M 0 M y y m 8 0 V R J o h J y P R P J O S K U S P G l i B V 3 N 5 K 6 B A V U m O / W b b R v c W g y 6 R V r 3 l u z b u t V x u X x c 9 K c A w n c A Y e n E M D r u E G m k B B w S M 8 w 6 v z 5 L w 4 b 8 7 7 v H X F K W a O 4 B e c z 2 9 E H 5 m + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><figDesc>r 1 w p v 6 c y F F q P Z a B 7 Z R o h n r R m 4 p / e d 3 M R B d + z u M 0 M y y m 8 0 V R J o h J y P R P J O S K U S P G l i B V 3 N 5 K 6 B A V U m O / W b b R v c W g y 6 R V r 3 l u z b u t V x u X x c 9 K c A w n c A Y e n E M D r u E G m k B B w S M 8 w 6 v z 5 L w 4 b 8 7 7 v H X F K W a O 4 B e c z 2 9 E H 5 m + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><figDesc>r 1 w p v 6 c y F F q P Z a B 7 Z R o h n r R m 4 p / e d 3 M R B d + z u M 0 M y y m 8 0 V R J o h J y P R P J O S K U S P G l i B V 3 N 5 K 6 B A V U m O / W b b R v c W g y 6 R V r 3 l u z b u t V x u X x c 9 K c A w n c A Y e n E M D r u E G m k B B w S M 8 w 6 v z 5 L w 4 b 8 7 7 v H X F K W a O 4 B e c z 2 9 E H 5 m + &lt; / l a t e x i t &gt; task gradient &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 gX g Y d d U Q b J O g b 2 V b x i y 2 P Q + 7 d I = " &gt; A A A B 9 X i c b V C 7 S g N B F L 3 j M 8 Z X 1 N J m M A h W Y T e N l k E b y w j m A c k a 7 s 7 O J k N m H 8 z M K m H J f 9 h Y K G L r v 9 j 5 N 0 6 S L T T x w M D h n H u 4 d 4 6 f S q G N 4 3 y T t f W N z a 3 t 0 k 5 5 d 2 / / 4 L B y d N z W S a Y Y b 7 F E J q r r o + Z S x L x l h J G 8 m y q O k S 9 5 x x / f z P z O I 1 d a J P G 9 m a T c i 3 A Y i 1 A w N F Z 6 M K j H d K g w E D w 2 d F C p O j V n D r p K 3 I J U o U B z U P n q B w n L I h t m E r X u u U 5 q v B y V E U z y a b m f a Z 4 i G + O Q 9 y y N M e L a y + d X T + m 5 V Q I a J s o + u 3 y u / k 7 k G G k 9 i X w 7 G a E Z 6 W V v J v 7 n 9 T I T X n m 5 i N P M 8 J g t F o W Z p C a h s w p o I B R n R k 4 s Q a a E v Z W y E S p k x h Z V t i W4 y 1 9 e J e 1 6 z X V q 7 l 2 9 2 r g u 6 i j B K Z z B B b h w C Q 2 4 h S a 0 g I G C Z 3 i F N / J E X s g 7 + V i M r p E i c w J / Q D 5 / A C p l k k U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 l G 3 s UB k K 4 E R Y d L 3 h Q H 7 d + r Y 7 j A = " &gt; A A A C G n i c j V A 9 S w N B E J 3 z M 8 a v q K X N Y h Cs w l 0 a L Y M 2 l g r m A 5 I z z O 3 t J U t 2 7 4 7 d P S E c + R 8 W N v 4 V G x E 7 s f H f u E m u 0 M T C B w O P 9 2 a Y e R O k g m v j u l / O y u r a + s Z m a a u 8 v b O 7 t 1 8 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><figDesc>r 1 w p v 6 c y F F q P Z a B 7 Z R o h n r R m 4 p / e d 3 M R B d + z u M 0 M y y m 8 0 V R J o h J y P R P J O S K U S P G l i B V 3 N 5 K 6 B A V U m O / W b b R v c W g y 6 R V r 3 l u z b u t V x u X x c 9 K c A w n c A Y e n E M D r u E G m k B B w S M 8 w 6 v z 5 L w 4 b 8 7 7 v H X F K W a O 4 B e c z 2 9 E H 5 m + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 l G 3 s U B k K 4 E R Y d L 3 h Q H 7 d + r Y 7 j A = " &gt; A A A C G n i c j V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Y M 2 l g r m A 5 I z z O 3 t J U t 2 7 4 7 d P S E c + R 8 W N v 4 V G x E 7 s f H f u E m u 0 M T C B w O P 9 2 a Y e R O k g m v j u l / O y u r a + s Z m a a u 8 v b O 7 t 1 8 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><figDesc>r 1 w p v 6 c y F F q P Z a B 7 Z R o h n r R m 4 p / e d 3 M R B d + z u M 0 M y y m 8 0 V R J o h J y P R P J O S K U S P G l i B V 3 N 5 K 6 B A V U m O / W b b R v c W g y 6 R V r 3 l u z b u t V x u X x c 9 K c A w n c A Y e n E M D r u E G m k B B w S M 8 w 6 v z 5 L w 4 b 8 7 7 v H X F K W a O 4 B e c z 2 9 E H 5 m + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 l G 3 s U B k K 4 E R Y d L 3 h Q H 7 d + r Y 7 j A = " &gt; A A A C G n i c j V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Y M 2 l g r m A 5 I z z O 3 t J U t 2 7 4 7 d P S E c + R 8 W N v 4 V G x E 7 s f H f u E m u 0 M T C B w O P 9 2 a Y e R O k g m v j u l / O y u r a + s Z m a a u 8 v b O 7 t 1 8 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 6 .</head><label>6</label><figDesc>Figure6. The Multitask Scaling Hypothesis: Models trained with an increasing number of tasks are subjected to pressure to learn a representation that can solve all the tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><figDesc>Figure 14. Cross-modal alignment measured with various metrics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>L</head><figDesc>binary-NCE (g) ≜ p pos • E (x,x+)∼Pcoor [-log σ(g(x, x + ))] + (1 -p pos ) • E σ(-g(x, x -))] . (20)The Bayes optimal solution is given byg(x a , x b ) = log P (pos | x a , x b ) 1 -P (pos | x a , x b ) (21) = log P (pos, x a , x b ) P (neg, x a , x b ) (22) = log p pos • P coor (x a , x b ) (1 -p pos )P (x a )P (x b ) (23) = log P coor (x a , x b ) P (x a )P (x b ) + log p pos 1 -p pos (24) = K PMI (x a , x b ) + c X .(25)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><figDesc>coor (x + | x) j P (x(j) -) + i P coor (x (i) -| x)P (x + ) j̸ =i P (x (j) -) (27) = P coor (x + | x)/P (x + ) P coor (x + | x)/P (x + ) + i P coor (x (i) -| x)/P (x (i) -)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="21,190.13,279.17,279.16,279.16" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Touch could convey the shapes in this example but not the colors. This is an important limitation to our hypothesis that we discuss at several points in the paper: different sensors and views might capture different information, which may limit their potential to converge to identical representations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Borrowed from Tolstoy (1877), similar analogies have been made in other domains, such as the "Anna Karenina principle" popularized by<ref type="bibr" target="#b23">Diamond (1998)</ref> to explain animal domestication.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Here we only analyze temporal sequences, but note that the same could be done with respect to events laid out in space instead.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This latter interpretation may be more consistent with Plato's intent. Scholars have argued that his allegory of the cave rejects any notion of a true world state(Nettleship, 1897). Instead, we could say that the joint distribution of observation indices is itself the platonic reality.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In 1688, William Molyneux asked if a person born blind, upon gaining sight, could distinguish shapes by vision alone<ref type="bibr" target="#b72">(Locke, 1690)</ref>. Our arguments suggest they could not do so immediately, but after some visual experience, they could easily map shapes to their prior touch-based representations. Empirical data supports this, showing that congenitally blind children given sight can quickly learn these abilities<ref type="bibr" target="#b45">(Held et al., 2011)</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Lindsey &amp; Brown</rs> for sharing their data for our experiments shown in Figure <ref type="figure">8</ref>. We thank the anonymous reviewers for helpful feedback, and for providing the counterexample on how to visually convey "I believe in the freedom of speech." Thanks for <rs type="person">Yonglong Tian</rs>, <rs type="person">Dilip Krishnan</rs>, <rs type="person">Anna Decker</rs>, <rs type="person">Yoon Kim</rs>, <rs type="person">Jyo Pari</rs>, <rs type="person">Ani Nrusimha</rs>, <rs type="person">Dave Epstein</rs>, <rs type="person">Victor Butoi</rs>, and <rs type="person">Seungwook Han</rs> for helpful discussions and suggestions. We thank <rs type="person">Mingzhong Sun</rs> for catching a typo. This work was supported by a <rs type="funder">Packard Fellowship</rs> and a <rs type="grantName">Sloan Research Fellowship</rs> to P.I., by the <rs type="institution">MIT-IBM Watson AI Lab</rs>, by <rs type="funder">ONR MURI</rs> grant <rs type="grantNumber">N00014-22-1-2740</rs>, by the <rs type="funder">Center for Brains, Minds, and Machines</rs>, the <rs type="funder">MIT Quest for Intelligence, NSF</rs> <rs type="grantName">STC award</rs> <rs type="grantNumber">CCF-1231216</rs>, the <rs type="funder">DARPA</rs> <rs type="programName">Knowledge Management at Scale and Speed (KMASS) program</rs>, and the <rs type="funder">DARPA Machine Common Sense (MCS)</rs> program.</p></div>
			</div>
			<div type="funding">
<div><p>c M h e V z e a F O x Q m 3 3 / 2 j l t U V m f i J c = " &gt; A A A B 8 X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X Y B E 8 l a Q X P R a 9 9 F j B f m A b y m Y 7 a Z d u d s P u R A i h / 8 K L B 0 W 8 + m + 8 + W / c t j l o 6 4 O B x 3 s z z M w L E 8 E N e t 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 R q W a Q Z s p o X Q v p A Y E l 9 B G j g J 6 i Q Y a h w K 6 4 f R u 7 n e f Q B u u 5 A N m C Q Q x H U s e c U b R S o / N L F E 4 A c P N s F L 1 a t 4 C 7 j r x C 1 I l B V r D y t d g p F g a g 0 Q m q D F 9 3 0 s w y K l G z g T M y o P U Q E L Z l I 6 h b 6 m k M Z g g X 1 w 8 c y + t M n I j p W 1 J d B f q 7 4 m c x s Z k c W g 7 Y 4 o T s + r N x f + 8 f o r R T Z B z m a Q I k i 0 X R a l w U b n z 9 9 0 R 1 8 B Q Z J Z Q p r m 9 1 W U T q i l D G 1 L Z h u C v v r x O O v W a 7 9 X 8 + 3 q 1 c V v E U S L n 5 I J c E Z 9 c k w Z p k h Z p E 0 Y k e S a v 5 M 0 x z o v z 7 n w s W z e c Y u a M / I H z + Q P n i p E M &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y <rs type="person">B I O Y D A W L G C X G S v e N L J F</rs> m h J r p f r n i V b 0 5 3 F X i 5 6 Q C O f 4 X 7 5 c / e w N J 0 x i F o Z x o 3 f W 9</p><p>x A Q T o g y j H K e l X q o x I X R M h t i 1 V J A Y d T C Z n z V 1 z 6 w y c C O p 7 B P G n a s / J y Y k 1 j q L Q 5 u M i R n p Z W 8 m / u V 1 U x N d B h M m k t S g o I t F U c p d I 9 1 Z R + 6 A K a S G Z 5 Y Q q p j 9 q 0 t H R B F q b J M l e 7 q / f O g q a d W q v l f 1 b 2 u V + l X e <rs type="programName">W R F O 4 B T O w Y c L q E M D b q A J F A Q 8 w j O 8 O k / O i / P m v C + i B S e f O Y Z f c D 6 + A d k 7 m I U = &lt; /</rs> l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y <rs type="person">B I O Y D A W L G C X G S v e N L J F</rs> m h J r p f r n i V b 0 5 3 F X i 5 6 Q C O f 4 X 7 5 c / e w N J 0 x i F o Z x o 3 f W 9</p><p>x A Q T o g y j H K e l X q o x I X R M h t i 1 V J A Y d T C Z n z V 1 z 6 w y c C O p 7 B P G n a s / J y Y k 1 j q L Q 5 u M i R n p Z W 8 m / u V 1 U x N d B h M m k t S g o I t F U c p d I 9 1 Z R + 6 A K a S G Z 5 Y Q q p j 9 q 0 t H R B F q b J M l e 7 q / f O g q a d W q v l f 1 b 2 u V + l X e <rs type="programName">W R F O 4 B T O w Y c L q E M D b q A J F A Q 8 w j O 8 O k / O i / P m v C + i B S e f O Y Z f c D 6 + A d k 7 m I U = &lt; /</rs> l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y B I O</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 8 Z j z a 7 9 / 9 b v 3 3 Y s e P I g O f Z K p y 8 = " &gt; A A A B 7 n i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 K r u 9 6 L H o x W M F + w H t U r L p b B u a Z J c k K 5 S l P 8 K L B 0 W 8 + n u 8 + W 9 M 2 z 1 o 6 4 P A 4 7 2 Z y c y L U s G N 9 f 1 v b 2 N z a 3 t n t 7 R X 3 j 8 4 P D q u n J y 2 T Z J p h i 2 W i E R 3 I 2 p Q c I U t y 6 3 A b q q R y k h g J 5 r c z f <ref type="table">m 9 e 6 r 1 4 7 9 7</ref> H s n T D K 3 r O 4 A + 8 z x + q 2 I 8 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P J 5 v U j t I 9 Z 3 N J</p><p>y X p w 3 5 3 0 x W n K K n W P 4 B e f j G 1 i b l p Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P J 5 v U j t I 9 Z 3 N J</p><p>l y X p w 3 5 3 0 x W n K K n W P 4 B e f j G 1 i b l p Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P J 5 v U j t I 9 Z 3 N J</p><p>space 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 7 U B o V Q a v r 2 1 Y M A A 6 C / n Q x P P 8 N c = " &gt; A A A B 7 n i c b V B N S w M x E J 3 4 W e t X 1 a O X Y B E 8 l d 1 e 9 F j 0 4 r G C / Y B 2 K d l 0 t g 3 N Z p c k K 5 S l P 8 K L B 0 W 8 + n u 8 + W 9 M 2 z 1 o 6 4 P A 4 7 2 Z y c w L U y m M 9 b x v s r G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 7 b J s k 0 x x Z P</p><p>r / U K 8 2 b o s 4 S n A O F 3 A F P l x D A + 6 h C S 3 g M I F n e I U 3 k p I X 8 k 4 + l q U b p O g 5 g z 8 g n z + s X I 8 c &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C + + 0 6 e t g m c d / s m P x Z Y w d q k <ref type="table">d 8 4 J U 8 G 1 8 b w v p 7 S 2 v r G 5 V d 6 u 7 O z u 7 R + 4 h 0</ref> </p><p>H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8</p><p>a v 5 M 0 x z o v z 7 n w s W z e c Y u a M / I H z + Q P n i p E M &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y B I O</p><p>e f O Y Z f c D 6 + A d k 7 m I U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y <rs type="person">B I O Y D A W L G C X G S v e N L J F</rs> m h J r p f r n i V b 0 5 3 F X i 5 6 Q C O f 4 X 7 5 c / e w N J 0</p><p>e f O Y Z f c D 6 + A d k 7 m I U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y <rs type="person">B I O Y D A W L G C X G S v e N L J F</rs> m h J r p f r n i V b 0 5 3 F X i 5 6 Q C O f 4 X 7 5 c / e w N J 0</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 8 Z j z a 7 9 / 9 b v 3 3 Y s e P I g O f Z K p y 8 = " &gt; A A A B 7 n i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 K r u 9 6 L H o x W M F + w H t U r L p b B u a Z J c k K 5 S l P 8 K L B 0 W 8 + n u 8 + W 9 M 2 z 1 o 6 4 P A 4 7 2 Z y c y L U s G N 9 f 1 v b 2 N z a 3 t n t 7 R X 3 j 8 4 P D q u n J y 2 T Z J p h i 2 W i E R 3 I 2 p Q c I U t y 6 3 A b q q R y k h g J 5 r c z f</p><p>m 9 e 6 r 1 4 7 9 7 H s n T D K 3 r O 4 A + 8 z x + q 2 I 8 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P J 5 v U j t I 9 Z 3 N J</p><p>l y X p w 3 5 3 0 x W n K K n W P 4 B e f j G 1 i b l p Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P J 5 v U j t I 9 Z 3 N J</p><p>l y X p w 3 5 3 0 x W n K K n W P 4 B e f j G 1 i b l p Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m P J 5 v U j t I 9 Z 3 N J</p><p>H 3 S k L n 6 c y O n s d a <rs type="person">T O L S T M T U</rs> j v e z N x L + 8 b m a i y y D n M s 0 M S r Y 4 F G W C m I T M C i I D r p A Z M b G E M s X t X w k b U U W Z s T V W b H R / O e g q a d V r v l f z 7 + r V x l <rs type="programName">X R W R l O 4 B T O w Y c L a M A N 3 E I T G I z h E Z 7 h 1</rs> X l y X p w 3 5 3 0 x W n K K n W P 4 B e f j G 1 i b l p Q = &lt; / l a t e x i t &gt; space 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 7 U B o V Q a v r 2 1 Y M A A 6 C / n Q x P P 8 N c = " &gt; A A A B 7 n i c b V B N S w M x E J 3 4 W e t X 1 a O X Y B E 8 l d 1 e 9 F j 0 4 r G C / Y B 2 K d l 0 t g 3 N Z p c k K 5 S l P 8 K L B 0 W 8 + n u 8 + W 9 M 2 z 1 o 6 4 P A 4 7 2 Z y c w L U y m M 9 b x v s r G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 7 b J s k 0 x x Z P Z K K 7 I T</p><p>r / U K 8 2 b o s 4 S n A O F 3 A F P l x D A + 6 h C S 3 g M I F n e I U 3 k p I X 8 k 4 + l q U b p O g 5 g z 8 g n z + s X I 8 c &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C + + 0 6 e t g m c d / s m P x Z Y w d q k</p><p>a v 5 M 0 x z o v z 7 n w s W z e c Y u a M / I H z + Q P n i p E M &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B + b / j F 1 g J b x A d e 3 3 G n B 6 5 3 x 9 w Z c = " &gt; A A A C F n i c j V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 G w C r t p t A z a p F Q w D 0 y W M D u 5 m w y Z n V l m Z o U l 5 C 8 s b P w V G x F b s f N v n C R b a G L h g Y H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8 K i l Z a o o N q n k U n V C o p E z g U 3 D D M d O o p D E I c d 2 O L 6 e + e 0 H V J p J c W e y B I O</p><p>H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8</p><p>H D O e d y 5 5 4 w 4 U w b z / t y C m v r G 5 t b x e 3 S z u 7 e / k H 5 8</p></div>
<div><head>Scale up</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c t w B W q 2 U 9 Q X a 7 D T N T r I a 0 u e u v e o = " &gt;</p><p>k 1 a t 6 r l V 7 7 5 W q d / k c R T h D M 7 h E j y 4 g j r c Q Q O a w E D A M 7 z C m z N x X p x 3 5 2 P Z W n D y m V P 4 A G 5 V d w u 7 e z u 7 R + U D 4 9 a O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f z f z 2 A y r N Y 3 l v J g n 6 E R 1 K H n J G j Z U 6 d 4 w K J G n S L 1 f c q j s H W S V e T i q Q 4 3 / x f v m z N 4 h Z G q E 0 T F C t u 5 6 b G D + j y n A m c F r q p R o T y s Z 0 i F 1 L J Y 1 Q + 9 n 8 q C k 5 s 8 q A h L G y T x o y V 3 9 O Z D T S e h I F N h l R M 9 L L 3 k z 8 y + u m J q z 7 G Z d J a l C y x a I w F c <rs type="person">T E Z N Y Q G X C F z I i J J Z Q</rs> p b v 9 K 2 I g q y o z t s W R P 9 5 Y P X S W t W t V z q 9 5 t r d K 4 z D s r w g m c w j l 4 c A E N u I Y b a A I D A Y / w D K / O k / P i v D n v i 2 j B y W e O 4 R e c j 2 G 5 V d w u 7 e z u 7 R + U D 4 9 a O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f z f z 2 A y r N Y 3 l v J g n 6 E R 1 K H n J G j Z U 6 d 4 w K J G n S L 1 f c q j s H W S V e T i q Q 4 3 / x f v m z N 4 h Z G q E 0 T F C t u 5 6 b G D + j y n A m c F r q p R o T y s Z 0 i F 1 L J Y 1 Q + 9 n 8 q C k 5 s 8 q A h L G y T x o y V 3 9 O Z D T S e h I F N h l R M 9 L L 3 k z 8 y + u m J q z 7 G Z d J a l C y x a I w F c <rs type="person">T E Z N Y Q G X C F z I i J J Z Q</rs> p b v 9 K 2 I g q y o z t s W R P 9 5 Y P X S W t W t V z q 9 5 t r d K 4 z D s r w g m c w j l 4 c A E N u I Y b a A I D A Y / w D K / O k / P i v D n v i 2 j B y W e O 4 R e c j 2 G 5 V d w u 7 e z u 7 R + U D 4 9 a O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f z f z 2 A y r N Y 3 l v J g n 6 E R 1 K H n J G j Z U 6 d 4 w K J G n S L 1 f c q j s H W S V e T i q Q 4 3 / x f v m z N 4 h Z G q E 0 T F C t u 5 6 b G D + j y n A m c F r q p R o T y s Z 0 i F 1 L J Y 1 Q + 9 n 8 q C k 5 s 8 q A h L G y T x o y V 3 9 O Z D T S e h I F N h l R M 9 L L 3 k z 8 y + u m J q z 7 G Z d J a l C y x a I w F c <rs type="person">T E Z N Y Q G X C F z I i J J Z Q</rs> p b v 9 K 2 I g q y o z t s W R P 9 5 Y P X S W t W t V z q 9 5 t r d K 4 z D s r w g m c w j l 4 c A E N u I Y b a A I D A Y / w D K / O k / P i v D n v i 2 j B y W e O 4 R e c j 2 9 u Z p c u &lt; / l a t e x i t &gt; architectures &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H x V N I k M H x 1 z Z k 5 + 9 X h <rs type="programName">E j o 2 d C r T 0 = " &gt; A A A B 9 X i c b V D L T g J B E O z 1 i</rs> f h C P X q Z S E w 8 k V 0 u e i R 6 8 Y i J P B J Y y e z Q w I T Z R 2 Z 6 N W T D f 3 j x o D F e / R d v / o 0 D 7 E H B S j q p V H W n u y t I l D T k u t / O 2 v r G 5 t Z 2 Y a e 4 u 7 d / c F g 6 O m 6 a O N U C G y J W s W 4 H 3 K C S E T Z I k s J 2 o p G H g c J W M L 6 Z + a 1 H 1 E b G 0 T 1 N E v R D P o z k Q A p O V n p g X I u R J B S U a j S 9 U t m t u H O w V e L l p A w 5 6 r 3 <rs type="programName">S V 7 c f i z T E i I T i x n Q</rs> 8 N y E / 4 5 q k U D g t d l O D C R d j P s S O p R E P 0 f j Z / O o p O 7 d K n w 1 i b S s i N l d / T 2 Q 8 N G Y S B r Y z 5 D Q y y 9 5 M / M / r p D S 4 8 j M Z J S l h J B a L B q l i F L N Z B K w v t X 1 Y T S z h Q k t 7 K x M j r r k g G 1 T R h u A t v 7 x K m t W K 5 1 a 8 u 2 q 5 d p 3 <rs type="person">H U Y B T O I M L 8 O A S</rs> a n A L d W i A A A 3 P 8 A p v z p P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A K m o k p o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 B w 8 Z U C 5 P E B H t A a J d k h 2 9 W J 9 G b</p><p>a N a 8 d y K d 1 s t 1 y 7 z z g p w D C d w B h 6 c Q w 2 u 4 Q b q I E D D I z z D q / P k v D h v z v s 8 u u L k M 0 f w C 8 7 n N 8 + r m h M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 B w 8 Z U C 5 P</p><p>a N a 8 d y K d 1 s t 1 y 7 z z g p w D C d w B h 6 c Q w 2 u 4 Q b q I E D D I z z D q / P k v D h v z v s 8 u u L k M 0 f w C 8 7 n N 8 + r m h M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 B w 8 Z U C 5 P</p><p><rs type="person">R Y W H M T E S</rs> u U 2 A 7 M a t e L v 7 n 9 V M b X 4 c Z k 0 l q q S T L R X H K k V U o f x y N m K b E 8 p k j m G j m b k V k g j U m 1 s V T c S E E q y + v k 0 6 j H v j 1 4 K F R a 9 4 U c Z T h D M 7 h E g K 4 g i b c Q Q v a Q G A C z / A K b 5 7 w X r x 3 7 2 P Z W v K K m V P 4 A + / z B x j R j k E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 Y x S A N 2 n l A i c W / w T m 7 P t f 3 o a z X c = " &gt; A A A C E H i c j V C 7 T s M w F L 3 m W c q r w M h i U S E x V U k X G C t Y G B h A o g + p j S r H d V q r j h 3 Z D l I V 9 R c Y W P g V F o R Y G d n 4 G 5 w 2 A 7 Q M H M n S 0 T n n 6 v q e M B H c W M / 7 Q i u r a + s b m 6 W t 8 v b O 7 t 5 + 5 e C w Z V S q K W t S J Z T u h M Q w w S V r W m 4 F 6 y S a k T g U r B 2 O r 3 K / / c C 0 4 U r e 2 0 n C g p g M J Y 8 4 J T a X b p Q x / U r V q 3 k z 4 G X i F 6 Q K B f 4 X 7 1 c + e w N F 0 5 h J S w U x p u t 7 i Q 0 y o i 2 n g k 3 L v d S w h N A x G b K u o 5 L E z A T Z 7 K A p P n X K A E d K u y c t n q k / J z I S G z O J Q 5 e M i R 2 Z R S 8 X / / K 6 q Y 0 u g o z L J L V M 0 v m i K B X Y K p y 3 g w d c M 2 r F x B F C N X d / x X R E N K H W d V h 2 p / u L h y 6 T V r 3 m e z X / r l 5 t X B a d l e A Y T u A M f <rs type="person">D i H B l z D L T S B</rs> w g g e 4 R l e 0 R N 6 Q W / o f R 5 d Q c X M E f w C + v g G q u q V u g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 Y x S A N 2 n l A i c W / w T m 7 P t f 3 o a z X c = " &gt; A A A C E H i c j V C 7 T s M w F L 3 m W c q r w M h i U S E x V U k X G C t Y G B h A o g + p j S r H d V q r j h 3 Z D l I V 9 R c Y W P g V F o R Y G d n 4 G 5 w 2 A 7 Q M H M n S 0 T n n 6 v q e M B H c W M / 7 Q i u r a + s b m 6 W t 8 v b O 7 t 5 + 5 e C w Z V S q K W t S J Z T u h M Q w w S V r W m 4 F 6 y S a k T g U r B 2 O r 3 K / / c C 0 4 U r e 2 0 n C g p g M J Y 8 4 J T a X b p Q x / U r V q 3 k z 4 G X i F 6 Q K B f 4 X 7 1 c + e w N F 0 5 h J S w U x p u t 7 i Q 0 y o i 2 n g k 3 L v d S w h N A x G b K u o 5 L E z A T Z 7 K A p P n X K A E d K u y c t n q k / J z I S G z O J Q 5 e M i R 2 Z R S 8 X / / K 6 q Y 0 u g o z L J L V M 0 v m i K B X Y K p y 3 g w d c M 2 r F x B F C N X d / x X R E N K H W d V h 2 p / u L h y 6 T V r 3 m e z X / r l 5 t X B a d l e A Y T u A M f <rs type="person">D i H B l z D L T S B</rs> w g g e 4 R l e 0 R N 6 Q W / o f R 5 d Q c X M E f w C + v g G q u q V u g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 Y x S A N 2 n l A i c W / w T m 7 P t f 3 o a z X c = " &gt; A A A C E H i c j V C 7 T s M w F L 3 m W c q r w M h i U S E x V U k X G C t Y G B h A o g + p j S r H d V q r j h 3 Z D l I V 9 R c Y W P g V F o R Y G d n 4 G 5 w 2 A 7 Q M H M n S 0 T n n 6 v q e M B H c W M / 7 Q i u r a + s b m 6 W t 8 v b O 7 t 5 + 5 e C w Z V S q K W t S J Z T u h M Q w w S V r W m 4 F 6 y S a k T g U r B 2 O r 3 K / / c C 0 4 U r e 2 0 n C g p g M J Y 8 4 J T a X b p Q x / U r V q 3 k z 4 G X i F 6 Q K B f 4 X 7 1 c + e w N F 0 5 h J S w U x p u t 7 i Q 0 y o i 2 n g k 3 L v d S w h N A x G b K u o 5 L E z A T Z 7 K A p P n X K A E d K u y c t n q k / J z I S G z O J Q 5 e M i R 2 Z R S 8 X / / K 6 q Y 0 u g o z L J L V M 0 v m i K B X Y K p y 3 g w d c M 2 r F x B F C N X d / x X R E N K H W d V h 2 p / u L h y 6 T V r 3 m e z X / r l 5 t X B a d l e A Y T u A M f <rs type="person">D i H B l z D L T S B</rs> w g g e 4 R l e 0 R N 6 Q W / o f R 5 d Q c X M E f w C</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SuXSuMr">
					<orgName type="grant-name">Sloan Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_razAkRk">
					<idno type="grant-number">N00014-22-1-2740</idno>
				</org>
				<org type="funding" xml:id="_sDDET38">
					<idno type="grant-number">CCF-1231216</idno>
					<orgName type="grant-name">STC award</orgName>
				</org>
				<org type="funding" xml:id="_9c564As">
					<orgName type="program" subtype="full">Knowledge Management at Scale and Speed (KMASS) program</orgName>
				</org>
				<org type="funding" xml:id="_F64ZK4s">
					<orgName type="program" subtype="full">W R F O 4 B T O w Y c L q E M D b q A J F A Q 8 w j O 8 O k / O i / P m v C + i B S e f O Y Z f c D 6 + A d k 7 m I U = &lt; /</orgName>
				</org>
				<org type="funding" xml:id="_puDn9Kt">
					<orgName type="program" subtype="full">W R F O 4 B T O w Y c L q E M D b q A J F A Q 8 w j O 8 O k / O i / P m v C + i B S e f O Y Z f c D 6 + A d k 7 m I U = &lt; /</orgName>
				</org>
				<org type="funding" xml:id="_2Q69e2f">
					<orgName type="program" subtype="full">X R W R l O 4 B T O w Y c L a M A N 3 E I T G I z h E Z 7 h 1</orgName>
				</org>
				<org type="funding" xml:id="_6JdpPat">
					<orgName type="program" subtype="full">E j o 2 d C r T 0 = &quot; &gt; A A A B 9 X i c b V D L T g J B E O z 1 i</orgName>
				</org>
				<org type="funding" xml:id="_ntVTBxD">
					<orgName type="program" subtype="full">S V 7 c f i z T E i I T i x n Q</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis space</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t L m I l j F W h X s / B u 7 + r 0 W b g / j 8 T V U = " &gt; A A A C o X i c f V F N b 9 N A E N 2 Y r x K + U j h y s b C Q E E K R 3 Q s c K + B Q D o i A m r S S 1 4 r G m 3 G y 6 n 5 Y u + O C Z f m f c I X / x L 9 h n Q a J t o i R V n r 7 5 s 3 u m 5 m y V t J T m v 4 a R T d u 3 r p 9 Z + / u + N 7 9 B w 8 f T f Y f L 7 x t n M C 5 s M q 6 0 x I 8 K m l w T p I U n t Y O Q Z c K T 8 q z d 0 P + 5 B y d l 9 Y c U 1 t j o W F t Z C U F U K C W k 8 l R W 1 v a o J c + 9 j U I X E 6 S d J p u I 7 4 O s h 1 I 2 C 5 m y / 1 R y 1 d W N B o N C Q X e 5 1 l a U 9 G B I y k U 9 m P e e A w v n 8 E a 8 w A N a P R F t 7 X e x 8 8 D s 4 o r 6 8 I x F G / Z v y s 6 0 N 6 3 u g x K D b T x V 3 M D + a 9 c 3 l D 1 p u i k q R t C I y 4 + q h o V k 4 2 H O c Q r 6 V C Q a g M A 4 W T w G o s N O B A U p j U e c 4 N f h d U a z K r j x j r d 5 1 n R c Y U V c b V A R 0 n G n V x v i L v h F r p 8 j 6 F 7 h x + D k 0 8 1 O i D r X n Y c 3 F p L 0 4 d p r P m r A f 1 P C N / + C A O 6 b I E c G F 9 b j 3 3 H t 8 1 W 3 X H f h 2 V l V 1 d z H S w O p l k 6 z T 4 f J I d v d 2 v b Y 0 / Z M / a C Z e w 1 O 2 R H b M b m T L B z 9 p 3 9 Y D + j J P o Q z a I v F 9 J o t K t 5 w i 5 F l P 8 G k Z 7 T f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / p y n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9 Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U Q t 5 K k 9 Q b D Y Q M J 1 v Z 1 4 + x L e y c = " &gt; A A A C x n i c j V H b a t t A E F 2 r t 8 S 9 O e 1 j X 0 R N o Z R i p L y 0 j 6 H t g 1 9 K W 4 i d g F e Y 0 X o k L 9 m L 2 B 0 l F U L Q D + l r v i Z f 0 L / p y n G g S U r p w M L Z M 2 e Y O T N 5 p a S n J P k 1 i O 7 c v X f / w c 7 u 8 O G j x 0 + e j v a e z b 2 t n c C Z s M q 6 4 x w 8 K m l w R p I U H l c O Q e c K j / K T j 3 3 + 6 B S d l 9 Y c U l N h p q E 0 s p A C K F D L 0 W j a V J b W 6 K W P f Q U C l 6 N x M k k 2 E d 8 G 6 R a M 2 T b + T 7 7 c G z R 8 Z U W t 0 Z B Q 4 P 0 i T S r K W n A k h c J u y G u P o f 0 J l L g I 0 I B G n 7 U b f 1 3 8 K j C r u L A u P E P x h v 2 z o g X t f a P z o N R A a 3 8 z 1 5 N / y y 1 q K t 5 n r T R V T W j E Z a O i V j H Z u F 9 W v J I O B a k m A B B O h l l j s Q Y H g s J K h 0 N u 8 E x Y r c G s W m 6 s 0 9 0 i z V q u s C C u 5 u h o n H I n y z V x 1 / + C y 0 8 Y 3 D v 8 H C b 5 U q E D s u 5 N y 8 G V W p o u b K P k b 3 v 0 L y F 8 v x I G d H 0 E c m B 8 Z T 1 2 L d + Y L d r D r g s n S m 8 e 5 D a Y 7 0 / S Z J J + 2 x 8 f f N j e d o e 9 Y C / Z a 5 a y d + y A T d l X N m O C n b K f 7 J x d R N P I R H V 0 d i m N B t u a 5 + x a R D 9 + A y 8 H 2 v Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D E U</p><p>Functions that solve &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q Y 5 B R e h <ref type="table">1 7 p 7 4 a 3 n 3 n J b o s 5 l x 5 K k</ref> </p><p>x s L X p y 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><p>/ e m / c + j 6 5 4 + c w R / I L 3 8 Q 2 W 4 J f S &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H Q z q c Z X P y</p><p>/ e m / c + j 6 5 4 + c w R / I L 3 8 Q 2 W 4 J f S &lt; / l a t e x i t &gt; simplicity bias &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 h Y X 6 W j y 0</p><p>/ l a t e x i t &gt; simplicity bias &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 h Y X 6 W j y 0 V H K y x p l o v o p / 5 + 4 + q Y = " &gt; A</p><p>x i 5 h T + w P n 8 A f m y k 0 U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0</p><p>k l a 9 5 n s 1 / 6 5 e b V w V n Z X g B E 7 h H H y 4 g A b c w C 0 0 g U E G j / A M r 8 6 T 8 + K 8 O e + L 0 T W n 2 D m G X 3 A + v w E 0 l J q + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0</p><p>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 c 5 H Z p t I F u L S I g 5 w R r H L e 0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A family of contrastive learners converge to a representation of P(Z)</head><p>Consider a contrastive learner that models observations that cooccur together. For simplicity, we ground our discussion with the following definition of the cooccurrence probability, P coor , of two observations x a and x b both occurring within some window T window :</p><p>Analogously, we can define P coor for Z and other observation modalities. Note that P coor is symmetric.</p><p>Consider positive pairs as two observations nearby in time (sampled from P coor ) and negative pairs as observations drawn from any point in time (sampled independently from the marginal). Our contrastive learner tries to classify if a pair is positive or negative by learning a representation f X : X → R d such that the dot-product kernel approximates the log odds ratio up to some offset:</p><p>where K PMI is the pointwise mutual information (PMI) kernel, and c X (x a ) is constant in x b . We note that this is a common setting for self-supervised contrastive learners with NCE objectives <ref type="bibr" target="#b39">(Gutmann &amp; Hyvärinen, 2010;</ref><ref type="bibr" target="#b91">Oord et al., 2018)</ref>, including SimCLR <ref type="bibr" target="#b18">(Chen et al., 2020)</ref> and SimCSE <ref type="bibr">(Gao et al., 2021)</ref>. (See <ref type="bibr" target="#b91">Oord et al. (2018)</ref> and Appendix F.1 for detailed derivations.)</p><p>Under mild conditions that the world is smooth enough (see Appendix F.2), a choice of f X can exactly represent K PMI :</p><p>where we observed that c X (x a ) from Equation ( <ref type="formula">5</ref>) must be a constant since both sides are symmetric.</p><p>Therefore, the contrastive learners we consider are minimized by a representation f X whose kernel is K PMI (up to a constant offset). With sufficient data and optimization, we will observe convergence to this point.</p><p>Thus we have convergence to a representation of the statistics of X, but what about Z? Recall that our idealized world consists of bijective observation functions, which, over discrete random variables, preserve probabilities. So we have:</p><p>where we use P coor and K PMI in a modality-agnostic way to emphasize that different modalities share the same these quantities.</p><p>All these arguments hold not just for X but also for Y (or any other bijective, discrete modality), implying:</p><p>Therefore, for any modality in our idealized world, we observe representational convergence to the same kernel, which represents certain pairwise statistics of P(Z).</p><p>This analysis suggests that certain representation learning algorithms may boil down to a simple rule: find an embedding in which similarity equals PMI. We note that this idea is consistent with prior works that have used PMI as a similarity measure for clustering in vision and language (e.g., <ref type="bibr" target="#b50">Isola et al. (2014)</ref>; <ref type="bibr" target="#b49">Isola (2015)</ref>; <ref type="bibr" target="#b51">Isola et al. (2016)</ref>; <ref type="bibr" target="#b17">Chambers &amp; Jurafsky (2008)</ref>).</p><p>A study in color We conduct a case study to verify that convergence does happen on real data. <ref type="bibr" target="#b0">Abdou et al. (2021)</ref> The Platonic Representation Hypothesis Where α(i, j) is a scalar weighting that assigns 1 if j is a mutual nearest neighbors to both ϕ i and ψ i , and 0 otherwise. We refer to this metric as the Centered Kernel Nearest-Neighbor Alignment (CKNNA) metric. As the number of nearest neighbors k → dim(K), we recover the original CKA metric.</p><p>We can further relax the metric to treat the cross-covariance term identically across all nearest-neighbor samples. This is equivalent to the assumption that all nearby samples have the same distance. This simplification leads us back to the mutual nearest neighbor metric:</p><p>By equating these metrics, we analyze the changes in alignment between language and vision models as we vary the number of neighbors k in Eqn. 18. In Figure <ref type="figure">10</ref>, we compute the average alignment score across all LLM models. For each k, we center the scores to the smallest vision model and divide by the standard deviation of the scores. We find that high values of k show less conclusive alignment across tasks while decreasing k shows a coherent trend across both models and tasks. 1.00 1.00 1.00 1.00 1.00 0.98 0.92 0.93 0.85 0.79 0.69 0.62 0.62 0.64 0.66 0.67 0.68 0.69 0.73 0.73 0.70 0.55 0.71 1.00 1.00 1.00 1.00 1.00 0.98 0.92 0.93 0.85 0.79 0.69 0.62 0.62 0.64 0.66 0.67 0.68 0.69 0.73 0.73 0.70 0.55 0.71 1.00 1.00 1.00 1.00 1.00 0.98 0.92 0.93 0.85 0.79 0.69 0.62 0.62 0.64 0.66 0.67 0.68 0.69 0.73 0.73 0.70 0.55 0.71 1.00 1.00 1.00 1.00 1.00 0.98 0.92 0.93 0.85 0.79 0.69 0.62 0.62 0.64 0.66 0.67 0.68 0.69 0.73 0.73 0.70 0.55 0.71 1.00 1.00 1.00 1.00 1.00 0.97 0.91 0.93 0.85 0.79 0.69 0.62 0.62 0.64 0.66 0.67 0.68 0.68 0.73 0.73 0.70 0.55 0.71 0.98 0.98 0.98 0.98 0.97 1.00 0.91 0.91 0.82 0.77 0.66 0.59 0.59 0.61 0.63 0.64 0.65 0.66 0.71 0.71 0.68 0.53 0.71 0.92 0.92 0.92 0.92 0.91 0.91 1.00 0.97 0.89 0.85 0.76 0.69 0.69 0.69 0.68 0.67 0.67 0.67 0.71 0.71 0.68 0.52 0.62 0.93 0.93 0.93 0.93 0.93 0.91 0.97 1.00 0.94 0.90 0.82 0.75 0.74 0.74 0.73 0.71 0.70 0.70 0.73 0.73 0.70 0.53 0.60 0.85 0.85 0.85 0.85 0.85 0.82 0.89 0.94 1.00 0.99 0.93 0.87 0.85 0.84 0.81 0.77 0.75 0.74 0.76 0.76 0.74 0.55 0.57 0.79 0.79 0.79 0.79 0.79 0.77 0.85 0.90 0.99 1.00 0.97 0.92 0.90 0.88 0.84 0.79 0.76 0.75 0.77 0.77 0.74 0.55 0.52 0.69 0.69 0.69 0.69 0.69 0.66 0.76 0.82 0.93 0.97 1.00 0.98 0.96 0.93 0.87 0.80 0.77 0.75 0.75 0.75 0.73 0.57 0.44 0.62 0.62 0.62 0.62 0.62 0.59 0.69 0.75 0.87 0.92 0.98 1.00 0.99 0.97 0.91 0.84 0.80 0.79 0.78 0.78 0.76 0.62 0.38 0.62 0.62 0.62 0.62 0.62 0.59 0.69 0.74 0.85 0.90 0.96 0.99 1.00 0.99 0.95 0.88 0.85 0.83 0.81 0.81 0.80 0.67 0.38 0.64 0.64 0.64 0.64 0.64 0.61 0.69 0.74 0.84 0.88 0.93 0.97 0.99 1.00 0.98 0.93 0.90 0.89 0.87 0.87 0.86 0.72 0.40 0.66 0.66 0.66 0.66 0.66 0.63 0.68 0.73 0.81 0.84 0.87 0.91 0.95 0.98 1.00 0.98 0.96 0.94 0.92 0.92 0.92 0.78 0.42 0.67 0.67 0.67 0.67 0.67 0.64 0.67 0.71 0.77 0.79 0.80 0.84 0.88 0.93 0.98 1.00 0.99 0.98 0.96 0.96 0.96 0.82 0.45 0.68 0.68 0.68 0.68 0.68 0.65 0.67 0.70 0.75 0.76 0.77 0.80 0.85 0.90 0.96 0.99 1.00 1.00 0.97 0.97 0.98 0.83 0.47 0.69 0.69 0.69 0.69 0.68 0.66 0.67 0.70 0.74 0.75 0.75 0.79 0.83 0.89 0.94 0.98 1.00 1.00 0.98 0.98 0.99 0.84 0.49 0.73 0.73 0.73 0.73 0.73 0.71 0.71 0.73 0.76 0.77 0.75 0.78 0.81 0.87 0.92 0.96 0.97 0.98 1.00 1.00 0.99 0.83 0.56 0.73 0.73 0.73 0.73 0.73 0.71 0.71 0.73 0.76 0.77 0.75 0.78 0.81 0.87 0.92 0.96 0.97 0.98 1.00 1.00 0.99 0.83 0.56  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Evaluating Alignment and Convergence</head><p>To demonstrate representational convergence, we take off-the-shelf models at multiple scales and multiple modalities and measure their representational alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Vision-Vision Alignment and Representation Quality</head><p>We consider 78 vision models in total:</p><p>• 17 ViT models ranging from ViT-tiny to ViT-giant, trained on tasks including ImageNet-21k <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020)</ref> classification, Masked Autoencoders <ref type="bibr" target="#b44">(He et al., 2021)</ref>, DINO <ref type="bibr" target="#b16">(Caron et al., 2021)</ref>, and CLIP <ref type="bibr" target="#b101">(Radford et al., 2021)</ref>, including some finetuned on ImageNet-12k.</p><p>• 1 randomly initialized ResNet-50.</p><p>• 11 ResNet-50 models trained with contrastive learning on ImageNet-1k, Places-365 <ref type="bibr" target="#b141">(Zhou et al., 2017;</ref><ref type="bibr" target="#b73">López-Cifuentes et al., 2020)</ref>, and 9 synthetic image datasets used in <ref type="bibr" target="#b9">Baradad et al. (2022)</ref>.</p><p>• 49 ResNet-18 models trained with Alignment and Uniformity contrastive loss <ref type="bibr">(Wang &amp; Isola, 2020)</ref> on ImageNet-100, Places-365, and 47 realistic and synthetic image datasets from <ref type="bibr" target="#b8">Baradad et al. (2021)</ref>.</p><p>To test representation quality, we evaluate linear probing performance on all 19 VTAB classification tasks <ref type="bibr" target="#b139">(Zhai et al., 2019)</ref>, which is a standard multi-task transfer learning benchmark containing structured, specialized, and natural datasets covering diverse domains. To reduce compute requirements, we subsample training and validation datasets to have at most 10,000 samples. We consider a representation solves a task if its performance is ≥ 80% of the best performance on that task across all 78 models.</p><p>To compute the alignment metric, we use k = 10 nearest neighbors over 1000 image representations computed on Places-365's validation dataset <ref type="bibr" target="#b141">(Zhou et al., 2017)</ref>. This dataset is disjoint from VTAB datasets, although both contain natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Cross-Modal Alignment</head><p>We compare the representation of an image in a vision model to the representation of a caption describing that image in a language model. The language model families we consider are BLOOM <ref type="bibr" target="#b12">(BigScience et al., 2022)</ref>, OpenLLaMA <ref type="bibr" target="#b32">(Geng &amp; Liu, 2023)</ref>, and LLaMA <ref type="bibr" target="#b125">(Touvron et al., 2023)</ref>. For Figure <ref type="figure">4</ref>, we included more recent model families such as OLMo <ref type="bibr" target="#b37">(Groeneveld et al., 2024)</ref>, LLaMA3 (Meta, 2024), Gemma <ref type="bibr" target="#b120">(Team et al., 2024)</ref>, and Mistral/Mixtral <ref type="bibr" target="#b52">(Jiang et al., 2023;</ref><ref type="bibr">2024)</ref>. These models were downloaded from Huggingface <ref type="bibr" target="#b133">(Wolf et al., 2019)</ref>.</p><p>For vision models, we consider ViT models <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020)</ref> of various sizes trained on various data and objectives. We mainly consider the popular vision models: classification on ImageNet-21K <ref type="bibr" target="#b105">(Russakovsky et al., 2015)</ref>, MAE <ref type="bibr" target="#b44">(He et al., 2021)</ref>, DINOv2 <ref type="bibr" target="#b93">(Oquab et al., 2023)</ref>, CLIP <ref type="bibr" target="#b101">(Radford et al., 2021)</ref>, and CLIP finetuned on ImageNet-12K. These models were downloaded from PyTorch Image Models (TIMM; <ref type="bibr" target="#b132">Wightman (2021)</ref>). This is a subset of the models used in vision-vision comparison.</p><p>To compute the alignment metric, we use k = 10 nearest neighbors over 1024 samples from WIT (Wikipedia-based Image Text; <ref type="bibr" target="#b115">Srinivasan et al. (2021)</ref>). For the vision model, we use class token of each layer, and for the language model, we average pool each layer to a single token. Since it is not trivial to determine where the alignment might occur, we draw inspiration from BrainScore <ref type="bibr" target="#b107">(Schrimpf et al., 2018)</ref> and compute pairwise alignment scores, then take the maximum. One of these pairwise comparisons also includes concatenated features. We apply l 2 normalization to the features before measuring the distance. As transformer architectures have "emergent outliers" <ref type="bibr" target="#b21">(Dettmers et al., 2022)</ref>, we truncate the elements in the features that are above the 95-th percentile.</p><p>Simply taking the last token did not show any strong alignment signal. We also experimented with prompting the language model and taking the last token representation. The prompt we used was</p><p>An image with the caption '&lt;caption&gt;'. This is an image of a &lt;fill&gt;</p><p>Using prompting showed similar trends to average pooling but had slightly lower alignment scores. </p><p>For the general τ ̸ = 1 case, we have g (and corresponding f X ) recovers K PMI up to an offset and a scale. Our main argument in Section 4 that f X recovers K PMI still holds.</p><p>F.2. Contrastive learners can represent K PMI exactly under smoothness conditions</p><p>We want to express K PMI + C using some representation function f X : X → R n so that ⟨f X (x a ), f X (x b )⟩ = K PMI (x a , x b ) + C, for some C.</p><p>For such an f X to exist, an equivalent criterion is that K PMI + C is positive semi-definite (PSD), as can be seen from eigendecomposition.</p><p>Proposition F.1. Suppose that the off-diagonal elements of K PMI are bounded within [log ρ min , log ρ min + δ] ∈ (-∞, 0].</p><p>We have K PMI + C is positive semi-definite (PSD) for some C if the joint distribution is sufficiently smooth:</p><p>Proof. Note that K PMI + C still only has non-positive off-diagonal elements if</p><p>For such C, it is diagonally dominant (and thus PSD) if, ∀i,</p><p>or equivalently, ∀i, N C + j K PMI (z i , z j ) ≥ 0.</p><p>The following choice of C readily satisfies the above Equation ( <ref type="formula">35</ref>):</p><p>Therefore, it remains to show that Equation (33) is true. Note that</p><p>Therefore, it suffices to have</p><p>Rearranging terms gives the desired condition</p><p>Remark F.2. Proposition F.1 is one example that a sufficiently smooth world or a sufficiently high sampling rate allows the PMI kernel K PMI to be exactly represented as inner products of a learned feature space (up to a scale). The condition here can be satisfied, for example, if the off-diagonal terms decay linearly with respect to N and stay sufficiently close to each other. While the condition is somewhat strict, it captures the essence that smoothness and continuity allow easier learning. Nonetheless, we note that exact representation is not necessary for convergence, and thus this requirement can likely be relaxed. Please see Section 6 for discussions on practical settings.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Can language models encode perceptual structure without grounding? a case study in color</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulmizev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hershcovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06129</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Git re-basin: Merging models modulo permutation symmetries</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.04836</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predictive coding or just feature discovery? an alternative account of why language models fit brain data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Antonello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Language</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="79" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American mathematical society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implicit regularization in deep matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09229</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A spline theory of deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="374" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting model stitching to compare neural representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to see by looking at noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Procedural image programs for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6450" to="6462" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Possible principles underlying the transformation of sensory messages</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensory communication</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving image generation with better captions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/papers/dall-e-3.pdf" />
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Scao</forename><surname>Bigscience</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rt-2: Vision-language-action models transfer web knowledge to robotic control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15818</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explanatory models in neuroscience: Part 2-constraint-based intelligibility</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Conwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2023" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30318" to="30332" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Gpt3. int8 (</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Guns, germs and steel: a short history of everybody for the last 13,000 years</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Diamond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Vintage London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Input-output maps are strongly biased towards simple outputs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dingle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">761</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstructing scientific realism to rebut the pessimistic meta-induction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doppelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="118" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rosetta neurons: Mining the common units in a model zoo</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dravid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1934" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<title level="m">Palm-e: An embodied multimodal language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Simcse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gell-Mann</surname></persName>
		</author>
		<title level="m">The Quark and the Jaguar: Adventures in the Simple and the Complex</title>
		<imprint>
			<publisher>Macmillan</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An open reproduction of LLaMA</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Openllama</surname></persName>
		</author>
		<ptr target="https://github.com/openlm-research/openllama" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<title level="m">Openwebtext corpus</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The no free lunch theorem, Kolmogorov complexity, and the role of inductive biases in machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05366</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gemini: a family of highly capable multimodal models</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring statistical dependence with hilbert-schmidt norms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on algorithmic learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Implicit bias of gradient descent on linear convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9461" to="9471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10122</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">World models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adcock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11706</idno>
		<title level="m">A systematic study of bias amplification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">In defense of convergent realism</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="615" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners. 2022 ieee</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll'ar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15979" to="15988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The newly sighted fail to match seen with felt</title>
		<author>
			<persName><forename type="first">R</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="551" to="553" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00409</idno>
		<title level="m">Deep learning scaling is predictable, empirically</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The hardware lottery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The low-rank simplicity bias in deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=bCiNWDmlY2" />
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<idno type="ISSN">2835-8856</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The discovery of perceptual structure from visual co-occurrences in space and time</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">MIT Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Crisp boundary detection using pointwise mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04088</idno>
		<title level="m">Mixtral of experts</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.08403</idno>
		<title level="m">Repair: Renormalizing permuted activations for interpolation repair</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A solution for the best rotation to relate two sets of vectors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diffraction, Theoretical and General Crystallography</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="922" to="923" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>Acta Crystallographica Section A: Crystal Physics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A discussion of the solution for the best rotation to relate two sets of vectors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="827" to="828" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Klabunde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strohmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lemmerich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06329</idno>
		<title level="m">Similarity of neural network models: A survey of functional and representational measures</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Grounding language models to images for multimodal inputs and outputs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="17283" to="17300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Phrase-based &amp; neural unsupervised machine translation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Return of unconditional generation: A self-supervised representation generation method</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03701</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">LLM-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13655</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">LLM-grounded video diffusion models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17444</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The color lexicon of american english</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="17" to="17" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Diverse image generation via self-conditioned GANs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">An Essay Concerning Human Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Locke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1690">1690</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semantic-aware scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>López-Cifuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Escudero-Vinolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bescós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>García-Martín</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">107256</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05247</idno>
		<title level="m">Pretrained transformers as universal computation engines</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mechanistic mode connectivity</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bigelow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22965" to="23004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Segment anything in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">654</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Do vision and language encoders represent the world similarly?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maniparambil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akshulakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A D</forename><surname>Djilali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El Amine Seddik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="14334" to="14343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><surname>Umap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Merullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Castricato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15162</idno>
		<title level="m">Linearly mapping from image to text space</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title/>
		<ptr target="https://ai.meta.com/blog/meta-llama-3/" />
	</analytic>
	<monogr>
		<title level="j">Meta. Meta LLaMA</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Large language models as general pattern machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04721</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Moschella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fumero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15430</idno>
		<title level="m">Relative representations enable zero-shot latent space communication</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Uniform convergence may be unable to explain generalization in deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Nettleship</surname></persName>
		</author>
		<title level="m">Lectures on the &apos;Republic&apos; of Plato</title>
		<imprint>
			<publisher>Macmillan</publisher>
			<biblScope unit="page">1897</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Newton-Smith</surname></persName>
		</author>
		<title level="m">The Rationality of Science. International Library of Philosophy, Psychology, and Scientific Method. Routledge &amp; Kegan Paul</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Can language models learn to listen?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10083" to="10093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">What do language models hear? probing for auditory representations in language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">OpenAI. GPT-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Dinov2: Learning robust visual features without supervision</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Best-buddies similarity-robust template matching using mutual nearest neighbors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1799" to="1813" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Prevalence of neural collapse during the terminal phase of deep learning training</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="24652" to="24663" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Quantifying representation reliability in self-supervised learning models</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ardeshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Azizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName><surname>Plato</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<pubPlace>BC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Three kinds of scientific realism. The Philosophical Quarterly</title>
		<author>
			<persName><forename type="first">H</forename><surname>Putnam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950. 1982</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><surname>Svcca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Robust agents learn causal world models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Everitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">On linear identifiability of learned representations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9030" to="9039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">StylegGAN-XL: Scaling StyleGAN to large diverse datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2022 conference proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Brain-score: Which artificial neural network for object recognition is most brain-like?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajalingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prescott-Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page">407007</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A vision check-up for language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rott Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Multidimensional scaling, tree-fitting, and clustering</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="issue">4468</biblScope>
			<biblScope unit="page" from="390" to="398" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Diffusion schrödinger bridge matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Learning with kernels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A formal theory of inductive inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">part i. Information and control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Feature selection via dependence maximization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Neural representational geometry underlies few-shot concept learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sorscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page">2200800119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2443" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A M</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Language models are an effective representation learning technique for electronic health record data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Corbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">103637</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bjorner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03053</idno>
		<title level="m">Zipit! merging models from different tasks without training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muttenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Groen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achterberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oktar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marjieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<title level="m">Getting aligned on representational alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Love</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<title level="m">Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV 16</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Anna</forename><surname>Tolstoy</surname></persName>
		</author>
		<author>
			<persName><surname>Karenina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Russian Messenger</title>
		<imprint>
			<biblScope unit="page">1877</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">LLaMA 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Feature-matching auto-encoders</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="376" to="380" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Astolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero-Soriano</surname></persName>
		</author>
		<title level="m">A picture is worth more than 77 text tokens: Evaluating CLIP-style models on dense captions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Deep learning generalizes because the parameter-function map is biased towards simple functions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valle-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Learning how the world works: Specifications for predictive networks in robots and brains</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>IEEE International Conference on Systems, Man and Cybernetics<address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>PyTorch image models</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23965" to="23998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16090</idno>
		<title level="m">Self-correcting LLM-controlled diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation with density changing regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="28545" to="28558" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hellaswag</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
		<ptr target="https://aclanthology.org/P19-1472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">The visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Contrastive learning inverts the data generating process</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12979" to="12990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
