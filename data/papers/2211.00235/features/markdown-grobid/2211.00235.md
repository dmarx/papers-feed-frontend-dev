# Efficient AlphaFold2 Training using Parallel Evoformer and Branch Parallelism

## Abstract

## 

The accuracy of AlphaFold2, a frontier end-to-end structure prediction system, is already close to that of the experimental determination techniques. Due to the complex model architecture and large memory consumption, it requires lots of computational resources and time to train AlphaFold2 from scratch. Efficient AlphaFold2 training could accelerate the development of life science. In this paper, we propose a Parallel Evoformer and Branch Parallelism to speed up the training of AlphaFold2. We conduct sufficient experiments on UniFold implemented in PyTorch and HelixFold implemented in PaddlePaddle, and Branch Parallelism can improve the training performance by 38.67% and 36.93%, respectively. We also demonstrate that the accuracy of Parallel Evoformer could be on par with AlphaFold2 on the CASP14 and CAMEO datasets. The source code is available on [https://github.com/PaddlePaddle/PaddleFleetX](https://github.com/PaddlePaddle/PaddleFleetX).

## Introduction

Proteins are exceptionally critical for life science, as it plays a wide range of functions in organisms. A protein comprises a chain of amino acid residues and folds into a 3D structure to play its functions. Since the 3D structure determines the protein's functions, studying the 3D structure helps to understand the mechanism of the protein's activities. However, it is time-consuming and complex to study protein structure determination through experimental technologies, e.g., Xray crystallography and nuclear magnetic resonance (NMR). Until now, the experimental methods have determined about two hundred thousand protein structures [(Sussman et al. 1998;](#b19)[Burley et al. 2020)](#b3), only a fairly small portion of hundreds of millions of publicly available amino acid sequences (The UniProt Consortium 2016). Therefore, efficient protein structure estimation methods are in great demand.

Many institutions [(Jumper et al. 2021;](#b8)[Yang et al. 2015;](#b22)[Du et al. 2021;](#b5)[Baek et al. 2021;](#b1)[Peng and Xu 2011)](#b16) made their efforts to develop AI-based protein structure prediction systems due to the efficiency and the capacity of the deep neural networks. In particular, thanks to the fantastic performance in the challenging 14th Critical Assessment of Protein Structure Prediction (CASP14) [(Kryshtafovych et](#)   2021a), AlphaFold2 [(Jumper et al. 2021](#b8)) from DeepMind has attracted lots of public attention. The accuracy of Al-phaFold2 approaches that of the experimental determination technologies. AlphaFold2 is an end-to-end protein estimation pipeline that directly estimates the 3D coordinates of all the atoms in the proteins. A novel and well-designed architecture is proposed to promote the estimation accuracy, which jointly models multiple sequence alignments (MSAs) for evolutionary relationships and pairwise relations between the amino acids to learn the spatial relations.

Although the accuracy of AlphaFold2 is satisfactory for protein structure prediction, it also takes 11 days to train To this end, this paper proposes two optimization techniques for two of the above three problems to achieve efficient AlphaFold2 training under the premise of fully aligning hyperparameters (network model configuration and total batchsize of 128 with 1 protein sample per device). First, inspired by AlphaFold-mutimer [(Evans et al. 2021)](#b6), we modify the two serial computing branches in the Evoformer block into a parallel computing structure, named Parallel Evoformer, as shown in Figure [1](#fig_1). Second, we propose a novel Branch Parallelism (BP) for Parallel Evoformer, which can break the barrier of parallel acceleration that cannot be scaled to more devices through data parallelism due to a batch size of 1 on each device.

The method proposed in this paper to efficiently train Al-phaFold2 models is general and not limited to deep learning frameworks and the version of re-implemented AlphaFold2. We perform extensive experimental verification on Uni-Fold implemented in PyTorch and HelixFold implemented in PaddlePaddle. Extensive experimental results show that Branch Parallelism can achieve similar training performance improvements on both UniFold and HelixFold, which are 38.67% and 36.93% higher, respectively. We also demonstrate that the accuracy of Parallel Evoformer could be on par with AlphaFold2 on the CASP14 and CAMEO datasets.

The main contributions of this paper can be summarized as follows:

• We improve the Evoformer in AlphaFold2 to Parallel Evoformer, which breaks the computational dependency of MSA and pair representation, and experiments show that this does not affect the accuracy. • We propose Branch Parallelism for Parallel Evoformer, which splits different computing branches across more devices in parallel to speed up training efficiency. This breaks the limitation of data parallelism in the official implementation of AlphaFold2. • We reduce the end-to-end training time of AlphaFold2 to 4.18 days on UniFold and 4.88 days on HelixFold, improving the training performance by 38.67% and 36.93%, respectively. It achieves efficient AlphaFold2 training, saving R&D economic costs for biocomputing research.

2 Background

## Overview of AlphaFold2

Comparing to the traditional protein structure prediction model which usually consists of multiple steps, AlphaFold2 processes the input protein sequence and predicts the 3D protein structure through an end-to-end procedure. In general, AlphaFold2 takes the amino acid sequence as input and then search against protein databases to obtain MSAs and similar templates. By using MSA information, we can detect correlations between the parts of similar sequences that are more likely to mutate. The templates with regards to the input sequence, on the other hand, provide structural information for the model to predict the final structure.

The overall framework of AlphaFold2 can be divided into five parts: Preprocess, Embedding, Encoder, Decoder, and Recycle, which is shown in Figure [2](#fig_2). The Preprocess part mainly parses the input raw sequence and generates MSArelated and template-related features via genetic database search and structure database search. The features are then embedded into MSA representation, pair representation and extra MSA representation during Embedding part. These representations contain sufficient co-evolutionary information among similar sequences and geometric information of residue pairs within the sequence. The third part consists of 48-layer Evoformer blocks that iteratively refine and exchange information between the MSA representation and the pair representation. After obtaining the refined representations, the 8-block Structure Module acts as a decoder to directly generate the final structure. Moreover, AlphaFold2 adopts Recycling technique to improve the accuracy of prediction by passing the representations through Evoformer and Structure Module repeatedly.

## Evoformer

Evoformer is the core module of AlphaFold2, which includes two tracks that handle MSA representation and pair representation, as well as the communication scheme between them. As shown in Figure [1](#fig_1)(a), MSA representation is processed with Row-wise gated self-attention with pair bias, Column-wise gated self-attention and Transition, while pair representation is further processed with Triangle update, Triangle self-attention and Transition. The outer product mean is used to pass the information between the two tracks.

3 Related Work

## Evoformer Improvment for AlphaFold2

Although many biological or pharmaceutical studies are based on AlphaFold2 after its open source in July 2021, few works focused on improving the main modules of Al-phaFold2. ParaFold [(Zhong et al. 2022](#b23)) aims to accelerate the inference pipeline of AlphaFold2 by performing MSA searches with multiprocessing and distributing different procedures within Evoformer and Structure Module to CPUs or GPUs, instead of optimizing the network structure of Al-phaFold2. In order to predict larger proteins, AlphaFold-Multimer [(Evans et al. 2021](#b6)) makes minor architectural modifications of AlphaFold2, including swapping the attention and triangular multiplicative update layers in the template stack which aligns the order in the Evoformer stack, and moving the outer product mean to the start of the Evoformer block which ensures MSA stack and pair stack can be processed in parallel. Recently, several works proposed new training schemes and improved main structures of Al-phaFold2 which enable the prediction of proteins accurately from single primary sequence alone. Meta's ESMFold [(Lin et al. 2022](#b12)) replaces the axial attention with a standard attention in order to adapt to Evoformer block. OmegaFold from HeliXon [(Wu et al. 2022)](#b21) optimizes Evoformer with simplified node attention and edge attention to capture complex interaction patterns among amino acids. HelixFold-Single [(Fang et al. 2022](#b7)) designs an adaptor to align the output of pretrained protein language model to Evoformer and the column-wise gated self-attention is removed due to no necessity of exchanging the messages within the MSAs.

## Distributed Parallel Strategy

To improve training speed, data parallelism (DP) [(Li et al. 2020)](#b11) is the most popular and efficient method in deep learning distributed training. Each worker has a copy of the model parameters, and parallelism operates in the data dimension. Each device accepts mini-batch samples while scaling to more devices at the expense of increasing the total batch size. DeepSpeed's ZeRO [(Rajbhandari et al. 2020)](#b17) and FairScale's Fully Sharded Data Parallel (Baines et al. 2021) reduce redundant storage of tensors through communication costs. Model parallelism (MP) (Narayanan et al. 2021) uses more accelerators to train large-scale models, which can be divided into pipeline model parallelism (PP) and tensor model parallelism (TP). PP splits the whole model to multiple devices at layer dimension and will introduce the idle time named pipeline bubble, and TP distributes the parameter in individual layers to multiple devices at the tensor dimension. Dynamic axial parallelism (DAP) (Cheng et al. 2022) is proposed to solve the inefficient problem of training the AlphaFold2 model with a small parameter shape and a large activation memory consumption. Our proposed branch parallelism is different from other parallel computing methods. For the feature of two computing branches in Evoformer block, BP splits the two computing branches of MSA stack and pair stack into different devices for parallel computing. The difference between the parallel methods is shown in Figure 3.

## Implementation

## Parallel Evoformer

Evoformer of AlphaFold2 has two computational branches with axial self-attention in the MSA stack; triangular multiplicative updates and triangular self-attention in the pair stack; and an information exchange mechanism that outer product mean and attention biasing to allow communication between the stacks, as shown in Figure [1](#fig_1) to evolve independently within a given with cross happening at the of the overall process across the Evoformer block is learnable, the position the outer product mean affect the of final prediction as the number the Evoformer blocks increases, which is further proved by our experiments shown in Figure [5](#fig_5).

## Branch Parallelism

In order for the two computing branches in Parallel Evoformer to be computed in parallel, we propose Branch Parallelism (BP), and Figures 4 shows the details. BP is a novel parallel technique, which can be applied not only to the Al-phaFold2 Evoformer model structure, but also to a model structure with multiple parallel computing branches and an approximate amount of computation. BP splits the calculation branch across multiple devices. One device calculates the MSA stack and the other calculates the pair stack. To preserve strict calculation semantics, BP inserts Broadcast to and AllReduce communication.

Specifically, in the forward stage, we do not need to split the input in each Evoformer block as the first device uses the MSA and pair representation to calculate the MSA stack and outer product mean branch then the outputs are broadcasted to the second device. The second device uses the pair representation to calculate the pair stack branch, adds the outer product mean broadcasted from the first device, and then broadcasts the output to the first device. In the backward stage, we insert Broadcast to synchronize the gradient of the outer product mean from the second device and AllReduce to accumulate the sum of the gradient of the input pair representation in each Evoformer block. At the end of backward propagation of the whole Evoformer module, we need an extra broadcast of the gradient of MSA representation. Finally, we adopt the AllReduce or Broadcast communication to synchronize the gradients of Evoformer model parameters.

BP does not split the intermediate activation for the Evoformer block dominated by small kernels, as a result, the same computational intensity is retained. At the same time, BP can compute the two branches of Evoformer completely in parallel. However, BP also has its limitations: it can be scaled up to as many devices as the number of computational branches and requires a similar amount of computation from each branch.

## Hybrid Parallelism

DAP [(Cheng et al. 2022)](#b4) splits the activation to reduce the memory footprint. In addition, DAP also has high parallel efficiency with a large shape of input, e.g. in the fine-tuning stage. Therefore, we can combine BP and DAP to train the AlphaFold2 model when CPU launch overheads are not the performance bottleneck. Since BP and DAP just split the computing branches and the activation across multiple devices respectively, the same protein is processed on each device. To improve communication efficiency, BP and DAP work within a single node where the inter-GPU communication bandwidth is high. With data parallelism, we can scale the total mini-batch to 128. We call this technique BP-DAP-DP hybrid parallelism.

## Experiments

## Experimental Setup

Third-party implementation of AlphaFold2 Since the original AlphaFold2 only open sourced the inference code but not the training code, multiple teams have reimplemented and optimized AlphaFold2 training on different deep learning frameworks. We perform experimental verification on UniFold [(Li et al. 2022)](#b11) implemented in Py-Torch [(Paszke et al. 2019)](#b15) and HelixFold [(Wang et al. 2022)](#b20) implemented in PaddlePaddle [(Ma et al. 2019)](#b13).

Datasets For training, we follow the settings reported in the paper of AlphaFold2 to collect the training data, including 25% of samples from RCSB PDB ([https://www. rcsb.org/](https://www.rcsb.org/)) [(Berman et al. 2000;](#b3)[Burley et al. 2020](#b3)) and 75% of self-distillation samples. For evaluation, we collect two datasets: CASP14 and CAMEO. We collect 87 domain targets from CASP14 ([https://predictioncenter.org/ casp14/index.cgi](https://predictioncenter.org/casp14/index.cgi)) [(Jumper et al. 2021;](#b8)[Kinch et al. 2021;](#)[Kryshtafovych et al. 2021b](#)). We also collect 371 protein targets from CAMEO ([https://www.cameo3d.org/](https://www.cameo3d.org/)) [(Robin et al. 2021)](#b18), ranging from 2021-09-04 to 2022-02-19.

## Settings of Model Architectures

We use two model settings to assess the AlphaFold2 model training speed improved by this paper. The model settings are shown in Table [1](#tab_2), with initial training setting corresponding to model 1 and fine-tuning setting corresponding to model 1.1.1 reported in the supplementary information of paper AlphaFold2.

Hyperparameter setting All our experiments are run on NVIDIA A100 (40G) and the mini-batch size is 1 on each device. We strictly follow the settings in AlphaFold2. As a feature in AlphaFold2, recycling iteration is to randomize a number from 1 to 4 in each step, performs the forward pass multiple times, and then performs the backward pass. To compare performance quickly and fairly, we firstly fix the random seed, then run 105 training steps, discard the first 5 steps, and finally calculate the average speed for the last  100 steps. After our extensive experimental verification, the average speed of 100 steps can get a similar global speed. We fix random seed = 32, then the random seed for each step is calculated by random seed + cur step. Unless otherwise specified, we use AMP for training, using the Float32 parameter and BFloat16 intermediate activation.

## Effectiveness of Parallel Evoformer

To demonstrate the effectiveness of the Evoformer block modification, we test 3 different Evoformer blocks shown in Figure [1](#fig_1). We train it from scratch on a single node with 8 A100 GPUs and the batch size is 1 per GPU. The training dataset consists of 120,000 proteins from the RCSB PDB, the learning rate is 5e-5, the global gradient clipping value is 0.1, the number of warm-up steps is 1000 and total training step is 40,000. Figure [5](#fig_5) shows the training loss, TM-score and lDDT-C α metrics on the CASP14 and CAMEO test sets. The results show that Parallel Evoformer can achieve competitive accuracy with Evoformer in AlphaFold2 and AlphaFold-Mutimer. The training speed of the 3 different Evoformer blocks is the same. We also report the computational overhead ratio for 52 (48+4) Evoformer blocks, as shown in Table [2](#tab_4). It means that the position of the outer product mean does not affect the accuracy and training speed of AlphaFold2.

## Training Performance

Performance Comparison of BP and DAP The author of FastFold (Cheng et al. 2022) has open-sourced DAP. To Training Process Evoformer s/step Evoformer Other Total (%) Initial training AlphaFold2 3.12 1.88 5.00 62.40% AlphaFold-Multimer 3.09 1.89 4.98 62.04% Parallel Evoformer 3.11 1.89 5.00 62.20% Fine-tuning AlphaFold2 12.81 3.69 16.50 77.63% AlphaFold-Multimer 12.86 3.65 16.51 77.89% Parallel Evoformer 12.83 3.67 16.50 77.75% Table 3: Performance improvements for Branch Parallelism on HelixFold and UniFold. The total batch size of the test is 128, with 1 protein sample per GPU. Fine-tuning performance is not reported because fine-tuning of Float32 data type can cause OOM on A100 40G. Table [5](#tab_8) shows the performance comparison of FastFold and PPFold. As can be seen in the table, DAP=2 uses 2 GPUs compared to DAP=1, but there is a performance drop in both FastFold and PPFold as the last two dimensions of the inputs are relatively small with a low computational intensity. DAP splits the input into smaller tensors, and the computational intensity is not improved. In addition, a lot of additional communication overhead is introduced, resulting in a decrease in performance. BP splits the computing branches across different GPUs, which can be calculated in parallel while maintaining computational intensity, and only introducing a small amount of communication overhead. Thus, the performance is improved by 67.45%. Similar performance is also observed in end-to-end training, see Table [6](#).

These require an extra declaration. This paper does not compare BP with other distributed parallelisms such as TP and PP. There are two reasons for this. First, FastFold has compared DAP, TP and PP on AlphaFold2. The experimental results show that the acceleration efficiency of DAP is higher than that of TP and PP. Second, implementing TP and PP on AlphaFold2 requires a lot of work.

Implementation Training Process Hardware Step Time (s) Protein/s Training Time (days) Total (days) (%) AlphaFold2-DP Initial training 128 × TPUv3 7.513 17.037 6.793 10.961 Fine-tuning 30.729 4.165 4.167 OpenFold-DP Initial training 128 × A100(40G) 8.9 14.382 8.047 10.849 Fine-tuning 20.657 6.196 2.801 UniFold-DP Initial training 128 × A100(40G) 4.16 30.76 3.761 5.798 -Fine-tuning 15.02 8.52 2.037 UniFold-BP Initial training 256 × A100(40G) 3.02 42.38 2.730 4.181 +38.67% Fine-tuning 256 × A100(40G) 10.70 11.96 1.451 HelixFold-DP † Initial training 128 × A100(40G) 4.925 25.989 4.453 6.685 -Fine-tuning 16.458 7.777 2.232 HelixFold-BP Initial training 256 × A100(40G) 3.555 36.005 3.214 4.882 +36.93% Fine-tuning 256 × A100(40G) 12.298 10.407 1.668  Table 6: Performance on HelixFold with hybrid parallelism. The total batch size is 128, with 1 protein sample per GPU.

Performance of Hybrid Parallelism To illustrate the effectiveness of BP combined with other parallel strategies, we conduct experiments with different configurations using hybrid parallelism on HelixFold. As shown in Table [6](#), in the initial training, where the dimensions involved in the computation are relatively small, the performance of DAP=2 drops compared to that of unused, showing a negative gain. When BP=2, the performance is improved by 38.51%, indicating a positive benefit. Conversely, in the fine-tuning, where the dimensions of MSA depth and the length of amino acid sequences increase, DAP achieves a higher performance improvement than BP by splitting larger activations across multiple GPUs for parallel computing. However, the hybrid parallelism of DAP=2 and BP=2 has the approximate throughput of DAP=4 and BP=1 while DAP=4 and BP=2 have higher throughput than DAP=8 and BP=1, demonstrating that when the activation is divided to a certain size, the parallelism efficiency of BP is higher than that of DAP.

## End-to-end training Training Performance

We make a comparison among AlphaFold2 [(Jumper et al. 2021)](#b8), Open-Fold [(Ahdritz et al. 2021)](#b0), HelixFold [(Wang et al. 2022)](#b20) and UniFold [(Li et al. 2022)](#b11), in terms of hardware, time cost of each step, training throughput and total training time as

shown in Table [4](#tab_7). We also report training performance using Branch Parallelism on UniFold and HelixFold. The *-DP method uses 128 accelerator cards only with data parallelism, such as TPUv3 core or A100, while the *-BP method uses 256 accelerator cards in combination with data parallelism and branch parallelism.

Compared with the original AphaFold2-DP, UniFold-DP has been optimized, and the training time has been reduced from 10.961 days to 5.798 days, with an increase of 89%. We add Branch Parallelism to UniFold to obtain UniFold-BP, which further shortens the training time to 4.181 days, and improves the training performance by 38.67%.

Similarly, HelixFold has been optimized by operator fusion and tensor fusion to improve the throughput to obtain the HelixFold-DP performance and the training time has been reduced from 10.961 days to 6.685 days. We use Branch Parallelism to further improve training throughput. The total training time was shortened from 6.685 days to 4.882 days, with an increase of 36.93%.

Sufficient experimental results show that Branch Parallelism has similar performance improvements in different deep learning frameworks and optimized AlphaFold2 implementations, which is enough to illustrate the effectiveness and generalization of Branch Parallelism.

## Conclusion

As the end-to-end training of AlphaFold2 takes lots of computational resources, it is a great burden for the individuals and institutions who are interested in applying AlphaFold2. This paper improves the Evoformer block of AlphaFold2 into Parallel Evoformer, which can be extended to more accelerators by the proposed Branch Parallelism to speed up training. After optimization, the training time was shortened to 4.18 days on UniFold and 4.88 days on HelixFold. We believe that the efficient AlphaFold2 training proposed in this paper can help accelerate research progress in the field of protein structure prediction.

![al.]()

![Figure 1: Various Evoformer block. (a) The original Evoformer block in AlphaFold2. (b) Modified Evoformer block in AlphaFold-Multimer. (c) The Parallel Evoformer block proposed in this paper. The main difference is that the outer product mean cross-communication happens at different position within the block.]()

![Figure 2: Overall framework of AlphaFold2. Dimension names: b: mini-batchs, s: clustered MSA sequences, s e : extra MSA sequences, r: residues, c: channels. The Extra MSA stack is composed of Evoformer, so AlphaFold2 has 52 Evoformer blocks.]()

![Figure 3: Various distributed parallel strategies.]()

![Figure 4: Branch parallelism implementation of Parallel Evoformer. Branch parallelism splits Parallel Evoformer's two computing branches across two devices. The two branches are calculated in parallel and the results are synchronized through the broadcast communication operator.]()

![Figure 5: Accuracy comparison of various Evoformer block on HelixFold.]()

![compare with the performance of DAP, we firstly use Pad-dlePaddle to reproduce the open-source code of FastFold, called PPFold. Then we add the implementation of BP on PPFold to compare the performance of DAP and BP.]()

![Training Process Model setting N templ N res N seq N extra seq Model settings for performance comparison.]()

![The effect of modification of the Evoformer block on training performance on HelixFold. The ratio of Evoformer block computation time to total time cost can help to understand the performance improvement of Branch Parallelism. The experiments are performed on 8 A100 GPUs, and CUDA synchronization operations are added.]()

![Complete end-to-end training performance. In the initial training stage, we train 10×10 6 samples (78125 steps), and in the fine-tuning stage, we continue to train 1.5 × 10 6 samples (11718 steps). *-DP and *-BP refer to using only data parallelism and a hybrid of data parallelism and branch parallelism, respectively. UniFold results are based on its GitHub commit 726480e.]()

![FastFold VS PPFold performance comparison. Compare the total time of forward computation and backward computation for each layer. 12-layer Evoformer, data type is Float16, head = 8, B = 1, N seq = 128, N res = 256, C m = 256, C z = 128, the settings are the same as Fast-Fold open source code. PPFold does not use asynchronous communication in DAP.The main reason is that the main module of the AlphaFold2 model is the Evoformer block, but there are other module computing overheads, as shown in Table2. Branch Parallelism is only calculated in parallel in the two branches of the Evoformer block.]()

