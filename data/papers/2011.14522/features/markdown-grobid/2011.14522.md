# Feature Learning in Infinite-Width Neural Networks

## Abstract

## 

As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the Tensor Programs technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.

## NTK type state city

Width 64 Width (Feature Learning)

* Work done partly during the Microsoft AI Residency Program

## 

Figure [1](#fig_8): PCA of Word2Vec embeddings of top US cities and states, for NTK, width-64, and width-∞ feature learning networks (Definition 5.1). NTK embeddings are essentially random, while cities and states get naturally separated in embedding space as width increases in the feature learning regime.

## Introduction

The study of infinite-width limits of neural networks, in particular the Neural Tangent Kernel (NTK), has recently solved many longstanding open problems on the optimization and generalization of overparametrized neural networks [26]. However, in the NTK limit, (last layer) features learned during pretraining are essentially the same as those from random initialization (Corollary 3.9 and Theorem H.13); this is verified empirically in Word2Vec in Fig. [1](#fig_8). As feature learning (e.g. Imagenet and BERT) lies at the core of deep learning's far-ranging impact so far [7, 13, 23], this insight amounts to a fatal weakness of the NTK theory as a model of neural networks in practice.

We seek to capture feature learning in overparametrized networks by considering other parametrizations and their infinite-width limits. By slightly modifying the standard parametrization (SP), in fact, we can enable feature learning that is maximal in a sense to be explained shortly. We describe how to compute this limit exactly (and rigorously) via the Tensor Programs technique developed in [49-52]. Feature Learning Infinite-Width Networks on Real Tasks We explicitly calculate this limit for the tasks of Word2Vec [32, 33] and few-shot learning on Omniglot via MAML [16], [2](#b1) two standard tasks relying crucially on feature learning. In Word2Vec, an important early instance of large-scale language pretraining, we must learn, in an unsupervised manner, word embeddings so that similar words have close embeddings. Then we test the learned embeddings on the word analogy task, which asks questions of the kind "what to a queen is as a man to a woman?" In few-shot learning, the model is asked to make predictions given only a handful (e.g. 5) of labeled examples. Metalearning/MAML makes this possible by having the model learn good representations of typical examples that can adapt quickly, via a small number of SGD steps, to new few-shot learning tasks. On both tasks, we find our feature learning infinite-width networks outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width performance as width increases.

Figure [right](#) shows this for one of our Word2Vec results. See Section 9 for our other experiments.

abc-Parametrizations This paper studies a natural class of parametrizations, which we call the abc-Parametrization and describe here. Consider an L-hidden-layer perceptron: For weight matrices W 1 ∈ R n×d and W[foot_0](#foot_0) , . . . , W L ∈ R n×n , and nonlinearity φ : R → R, such a neural network on input ξ ∈ R d is given by h 1 (ξ) = W 1 ξ ∈ R n , and

$x l (ξ) = φ(h l (ξ)) ∈ R n , h l+1 (ξ) = W l+1 x l (ξ) ∈ R n , for l = 1, . . . , L -1,(1)$and the network output (also called the logit(s)) is f (ξ) = W L+1 x L (ξ) for W L+1 ∈ R 1×n . An abc-parametrization is specified by a set of numbers {a l , b l } l ∪ {c} such that (a) We parametrize each weight as W l = n -a l w l for actual trainable parameter w l (b) We initialize each w l αβ ∼ N (0, n -2b l ), and (c) The SGD learning rate is ηn -c for some width-independent η. [3 4](#) Examples: The NTK parametrization (NTP) [26] has a 1 = 0 and a l = 1/2 for l ≥ 2; b l = 0 for all l; c = 0. When depth L = 1, the Mean Field parametrization (MFP) [11, 30, 43, 45] has a 1 = 0, a 2 = 1; b l = 0 for all l; c = -1. The standard parametrization (SP) available as the default setting in PyTorch [39]  [5](#b4) has a l = 0 for all l; b 1 = 0 and b l = 1/2 for l ≥ 2; c = 0. However, we shall see that c is too small (learning rate too large) in SP. We can define abc-parametrization and generalize our results to arbitrary neural architectures (Appendix C), but we shall focus on MLPs in the main text.

Dynamical Dichotomy For any abc-parametrization, if c is too small (i.e. learning rate too large), SGD can lead to blowup of preactivation and/or logits; we say this parametrization is unstable. In practice this translates to numerical issues. If c is too large (i.e. learning rate too small), then the function computed by the network does not change in finite time; we say this parametrization is trivial. We prove what we call the Dynamical Dichotomy theorem (Corollary 3.9):

Any nontrivial stable abc-parametrization yields a (discrete-time) infinite-width limit. This limit either 1) allows the embedding x L (ξ) to evolve nontrivially (Definition 3.5) or 2) is described by kernel gradient descent in function space (Definition 3.7), but not both.

We call the former kind a feature learning limit and the latter a kernel limit. For 1-hidden-layer MLPs, the former is exemplified by MFP, and the latter, NTP. This dichotomy implies that certain functional dynamics, such as higher order generalizations of the NTK dynamics, are not valid infinite-width limits (see Remark 3.12). In addition, the neural network function f (defined in Eq. ( [1](#formula_0))) in any feature learning limit must be identically 0 at initialization (see Corollary 3.10). [6](#b5)  Verifying Max Learning Rate for P and SP Standard Param. Does Not Learn Features We show that the SP (resp. NTP) can only allow O(1/width) (resp. O(1)) learning rate (i.e. c = 1, resp. c = 0), so as to avoid blowup, and yield kernel limits (Section 4). Instead, we propose a parametrization that has Θ(1) max learning rate and admits feature learning maximally: it allows every parameter to be updated maximally (in terms of scaling with width) without leading to blowup (Section 5). We thus call it the Maximal Update Parametrization (abbreviated MUP or µP). It is given by a 1 = -1/2, a L+1 = 1/2, and a l = 0 for all 2 ≤ l ≤ L; b l = 1/2 for all l; c = 0. In a 1-hidden-layer MLP, this specializes to MFP, up to symmetry (see Eq. ( [5](#formula_8))). The "feature learning limits" mentioned above in our main experiments are µP limits. Figure to the right: We empirically verify our max learning rate predictions on relu MLP with 2 hidden layers, trained with square loss on CIFAR10. We plot learning rate vs accuracy in each subplot. Each curve represents MLP with a specific width. The right edge of each curve indicates the max learning rate. The diagonal subplots scale the x-axes (log learning rate) in the correct width-scaling for the corresponding parametrizations. We see, indeed, max learning rate for SP scales like 1/width but is constant in µP. When width is large, every activation vector has roughly iid coordinates, at any time during training. Using Tensor Programs, we can recursively calculate such coordinate distributions, and consequently understand how the neural network function evolves.

The Tensor Programs technique was developed in a series of papers [49-52] that proved the architectural universality of the Neural Network-Gaussian Process (NNGP) Correspondence and the Neural Tangent Kernel (NTK) limits and showed how to compute the corresponding infinite-width kernels. In the Figure above, the NNGP kernel can be thought of as the "limit" of the first forward pass of a randomly initialized model; the NTK can be similarly thought of as the "limit" of its first backward pass. The mechanics of calculating such limits is 1) to write down the relevant neural network computation (e.g. the first forward pass in the NNGP case) as a principled composition of matrix multiplication and coordinatewise nonlinearities, called a Tensor Program, and 2) to recursively calculate the distribution of coordinates of each vector via what's called the Master Theorem. In this paper, we follow the exact same recipe, where in 1) we just write down the entire SGD training instead of only the first step. More generally,

To derive the infinite-width limit of any neural computation (e.g. SGD training), 1) express it as a Tensor Program, and 2) mechanically apply the Master Theorem.

For example, we easily recover the (discrete-time) 1-hidden-layer mean field limit (Theorem 6.1). It readily applies to practically any neural architecture (e.g. ResNet and Transformers) [7](#b6) as well as many common variants of SGD; however, in this paper, for pedagogical clarity, we only focus on multilayer perceptrons. The generality of our approach allows us to easily adapt to settings outside the traditional (CIFAR10-style) supervised classification, such as the Word2Vec and few-shot learning tasks in this paper, or reinforcement learning and image generation outside of our scope.

## Our Contributions

1. Formulate a natural space of NN parametrizations (abc-parametrizations).

2. Prove Dynamical Dichotomy: Any nontrivial stable abc-parametrization yields either feature learning or kernel limits, but not both.

3. Show both NTK and standard parametrizations yield kernel limits and propose the Maximal Update Parametrization (µP) , which admits maximal feature learning in a suitable sense.

4. Use Tensor Programs to derive the infinite-width limit of µP and, more generally, the limit of any abc-parametrization. We verify our theory using extensive experiments.

5. Show the µP limit outperforms both NNGP/NTK baselines and finite networks on 1) Word2Vec and 2) Omniglot few-shot learning, trained via first-order MAML.

Tensor Programs Series While this work is self-contained, it is positioned as the 4th paper in the series, following Yang [49, 51, 52]. We do not extend the Tensor Programs machinery further here, but instead extract the first major payoff of the foundation laid in the earlier works. In fact, this paper is the original motivation for this series; for a short history, see Appendix A.

## Related Works

Comparison with Mean Field Limits For 1-hidden-layer MLP, the mean field limit [11, 30, 43,  45] is equivalent to the µP limit modulo the symmetry of Eq. ( [5](#formula_8)) (see Section 3.1). Several works also proposed different versions of mean field frameworks for deeper MLPs [5, 15, 34, 35, 46]. However, they did not consider the typical Gaussian N (0, 1/n) random initialization (or the appropriately rescaled version in their respective parametrizations) [8](#b7) , which has a Central-Limit effect as opposed to a Law-of-Large-Numbers effect. For example, [5, 35] can cover the case of N (0, 1/n 2 ), instead of N (0, 1/n), initialization, which in fact causes the function to be stuck at initialization. See Appendix E for more explanations. Of these works, the mean field limit of [15] has the form most similar to what we derive here. There, as we do here, the coordinate distribution of each (pre)activation vector is tracked recursively. The main difference is, while [15] has an atypical initialization involving 2 regression, we consider the usual Gaussian N (0, 1/n) scheme. Such a (size n×n) Gaussian matrix in the middle of the network has a distinctly different effect, more similar to that of a Gaussian matrix in the usual NNGP/NTK calculation, [9](#b8) than the "mean field" matrices considered in [15] and previous works [5, 34, 35, 46], which has an "integral kernel" effect that is the straightforward generalization of matrices to function spaces. Nevertheless, discrete time versions of the 1-hidden-layer mean field limit and of many of the multilayer limits (such as [15, 35]) can be derived directly by writing the corresponding initialization and training inside a Tensor Program and applying the Master Theorem (Theorem 7.4).

Discrete-vs Continuous-Time Gradient Descent At a high level, there are two natural limits of neural networks training dynamics: large-width and continuous-time. Most prior works on infinite-width limits of neural networks also took the continuous-time limit simultaneously, e.g. [11, 26, 30, 43, 45]. In contrast, here we only take the large width limit, so that gradient descent stays discrete-time. Then the results of these prior works can be recovered by taking another continuoustime limit. From a practical perspective, the continuous-time limit is often unnatural, e.g. 1) because the step size is usually as large as possible to speed up training, 2) because of the task (such as reinforcement learning), or 3) because of the importance of hyperparameters like batch size that are hidden away in such limits. On the theory side, taking the continuous-time limit can create issues with 1) well-posedness and 2) existence and uniqueness of the resulting ODE/PDE. While they can sometimes be proved to hold, they are artifacts of the continuous-time limit, as the corresponding questions for the discrete time evolution are trivial, and thus not relevant to the behavior of real networks.

Technical Assumptions Earlier works on neural tangent or mean field limits (e.g. [11, 15, 26,  30, 35, 43, 45]) assume various forms of regularity conditions, such as 1) 0th, 1st, and/or 2nd order smoothness on the nonlinearity or other related functions, and 2) the support boundedness, subgaussianity, and/or PDF smoothness of initialization distributions. These are often either unnatural or difficult to check. In our work, the only assumption needed to rigorously obtain the infinite-width limit is that the nonlinearity φ has a polynomially bounded weak 2nd derivative and that the loss function has a continuous derivative w.r.t. the prediction (Assumption H.22). In particular, when we specialize to the 1-hidden-layer case and derive the discrete time version of the mean field limit, we cover the standard Gaussian initialization; in fact, we can allow any heavy-tailed initialization that can be written as the image of a Gaussian under a pseudo-Lipschitz function, which include nonsmooth PDFs and singular distributions. [10](#b9) This generosity of technical assumptions is due to that of the Tensor Programs Master Theorems proven in [49, 51, 52].

Training Time Many prior works (e.g. [4, 25, 30]) derived explicit time dependence of the convergence to infinite-width limit, so that a larger width can allow the network to stay close to the limit for longer. In this paper, our results only concern training time independent of width, since our primary objective is to investigate the limit itself and its feature learning capabilities. Moreover, recent evidence suggests that, given a fixed computational budget, it's always better to train a larger model for a shorter amount of time [29], which validates the practical relevance of our limit mode. Nevertheless, it is possible to prove a quantitative version of the Tensor Programs Master Theorem, by which one can straightforwardly allow training time to increase with width.

Classification of Parametrizations [10] pointed out that the weights move very little in the NTK limit, so that linearization approximately holds around the initial parameters, in contrast to the mean field limit (for 1-hidden-layer networks) where the weights move substantially. For this reason, they called the former "lazy training" and the latter "active training," which are classified nonrigorously by a multiplicative scaling factor of the logit (similar to n -a L+1 in this paper). While these terms are not formally defined, they intuitively correspond to the kernel and feature learning regimes in our paper. From a different perspective, [31] observed that the NTK and mean field limit can be thought of as short and long time-scale regimes of the mean field evolution equations. Neither of the above works attempted to formally classify natural parametrizations of neural networks. In contrast, [48] studied a toy class of neural networks in the context of implicit regularization due to the scale α of initialization (which is closely related to logit multiplier of [10] noted above). They identified the α → ∞ limit (of the scale α, not of width) with the "kernel regime" and the α → 0 limit with what they call the "rich regime". They showed that the former is implicitly minimizing an 2 risk while the latter, an 1 risk. They claim width allows the toy model to enter the kernel regime more naturally, but as we see in this work, both kernel and feature learning regimes are admissible in the large width limit of a standard MLP. Closer to our approach, [19] studied what amounts to a 2-dimensional subspace of the space of stable abc-parametrizations for L = 1. They proposed a notion of stability which is similar to the combination of stability and nontriviality in this paper. They characterized when the Neural Tangent Kernel, suitably generalized to any parametrization and playing a role similar to the feature kernel in this paper, evolves over time. However, to simplify the proofs, they assumed that the gradients for the different weight matrices are estimated using different inputs, a very unnatural condition. In contrast, here our results are for the usual SGD algorithm applied to MLPs of arbitrary depth. In all of the above works and most of existing literature, not much attention is paid to the feature learning capabilities of neural networks in the right parametrization, as opposed to our focus here. A notable exception is [12], which showed that the mean field limit, but not the NTK limit, can learn low dimension linear structure of the input distribution resulting in ambient-dimension-independent generalization bounds.

Other Related Works [27] proposed a toy model to study how large learning rate can induce a neural network to move out of the kernel regime in Ω(log(width)) time. Since our dichotomy result only concerns training for O(1) time (which, as we argue above, is more practically relevant), there is no contradiction. [47] also noted that standard parametrization leads to unstable training dynamics. They then injected constants in the NTK parametrization, such as α/ √ n instead of 1/ √ n and tuned α in the resulting kernel. [2, 3] also observed the lack of feature learning in NNGP and NTK limits but, in contrast to taking the exact limit of SGD training as we do here, they proposed a deep kernel process as a way of loosely mimicking feature learning in finite-width networks. [17]  empirically observed that wider networks achieve better downstream performance with linear transfer learning, even though on the original pretraining task there can be little difference. We fix the input dimension d in this work, but one can also consider varying d with width n, e.g. [36, 38]. [28] proved a complexity separation between NTK and finite-width networks by showing the latter approximates a sort of infinite-width feature learning network. In the literature surrounding NTK, often there are subtle differences in parametrization leading to subtle differences in conclusion (e.g. [4, 14, 57]). Our abc framework encapsulates all such parametrizations, and can easily tell when two ostensibly different parametrizations (e.g. [14, 57]) are actually equivalent or when they are really different (e.g. [4, 14]) via Eq. ( [5](#formula_8)).

## Feature Learning vs Kernel Behavior

In this section, we give a characterization of training procedures that induce feature learning vs kernel behavior; we will elaborate on what we mean by these two kinds of behavior below. We first motivate this discussion by reviewing the well-known tangent kernel and mean field limits of a shallow neural network.

## Motivating Examples: Neural Tangent Kernel and Mean Field Limits

For simplicity, define a shallow network f (ξ) with input/output dimension 1 by

$f (ξ) = V x(ξ) ∈ R, x(ξ) = φ(h(ξ)) ∈ R n , h(ξ) = U ξ ∈ R n . (2$$)$As a specialization of Eq. ( [1](#formula_0)), we parametrize weights

$V = n -av v ∈ R 1×n and U = n -au u ∈ R n×1 ,$where the width n should be thought of as tending to ∞, and v, u should be thought of as the actual trainable parameters. We will sample

$v α ∼ N (0, n -2bv ), u α ∼ N (0, n -2bu ) for α ∈ [n].$The learning rate is ηn -c for some η independent of n.

For example, in the Neural Tangent Parametrization (abbreviated NTP) [26],

$a u = b v = b u = 0, a v = 1/2, c = 0. The Mean Field Parametrization (abbreviated MFP) corresponds to a v = 1, a u = b u = b v = 0, c = -1;$however, as will be explained shortly, we will use the equivalent formulation

$a u = -1/2, a v = b u = b v = 1/2,$c = 0 in this section so c = 0 for both NTP and MFP. We remark that the GP limit, i.e. training only the last layer of a infinite-wide, randomly initialized network, is a special case of the NTK limit where the first layer is not trained. Everything we discuss below about the NTK limit specializes to the GP limit appropriately.

Given an input ξ, the gradient of f can be calculated as

$dx(ξ) = V, dh(ξ) = dx(ξ) φ (h(ξ)), dv(ξ) = n -av x(ξ), du(ξ) = n -au dh(ξ)ξ$where d • (ξ) is shorthand for ∇ • f (ξ) (however, note that later in Section 6, d • (ξ) will stand for n∇ • f (ξ)). For loss function L : R × R → R, the loss gradient on a pair (ξ, y) is then given by L (f (ξ), y)[dv(ξ), du(ξ)] (where L denotes derivative in first argument).

Note that one can keep the function f invariant while changing the magnitude of the gradient dv by changing a v , b v , holding a v + b v constant; likewise for du. Thus, the trajectory of f stays fixed if, for any θ ∈ R, we set )). With θ = -1/2, this explains why the two formulations of MFP above are equivalent. Then, for both NTP and MFP, we will consider the dynamics of f trained under stochastic gradient descent with learning rate η = 1 and batch size 1, where the network is fed the pair (ξ t , y t ) at time t, starting with t = 0. This simplicity is intended to intuitively illustrate our points below, but we shall state formal results regarding more common settings in Section 3.2.

$a u ← a u + θ, a v ← a v + θ, b u ← b u -θ, b v ← b v -θ, c ← c -2θ (also see Eq. (5$
## Notation and Setup

Below, when we say a (random [11](#b10) we mean v 2 /n = O(n a ) with high probability for large n. Intuitively, this means that each coordinate has a typical fluctuation of O(n a ). Likewise if O(n a ) is replaced with Θ(n a ) or Ω(n a ). See Definition H.2 for a formal definition. Let f t , h t , x t , U t , V t , dx t , dh t , dv t , du t denote the corresponding objects at time t, with t = 0 corresponding to random initialization. We also abuse notation and write x t = x t (ξ t ), i.e. applying the function x t specifically to tth input ξ t ; similarly for f t , h t , dx t , dh t , dv t , du t . These symbols will never appear by themselves to denote the corresponding function, so this should cause no confusion. Then SGD effectively updates U and V by

$) vector v ∈ R n has coordinate size O(n a ) (written v = O(n a )),$$U t+1 = U t -χ t n -au du t , V t+1 = V t -χ t n -av dv t . where χ t def = L (f t , y t ). Finally, let ∆• t def = • t -• 0 ,$for all • ∈ {f, h, x, U, V, dx, dh, dv, du}. For example, after 1 SGD update, we have, for any ξ ∈ R,

$∆h 1 (ξ) = h 1 (ξ) -h 0 (ξ) = -n -au χ 0 ξdu 0 = -n -2au χ 0 ξ 0 ξdh 0 = -n -2au χ 0 ξ 0 ξdx 0 φ (h 0 ) (3) ∆f 1 (ξ) = V 0 ∆x 1 (ξ) + ∆V 1 x 1 (ξ) = V 0 ∆x 1 (ξ) -n -av dv 0 x 1 (ξ) = V 0 ∆x 1 (ξ) -n -2av x 0 x 1 (ξ)(4)$
## Key Observations

Let's list a few characteristics of the NTK and MF limits in the context of the shallow network in Eq. ( [2](#formula_1)), and then discuss them in the general setting of deep MLP. We will keep our discussion intuitive to carry across the key ideas.

Feature Evolution For a generic ξ ∈ R, its embedding vector x 0 (ξ) has coordinates of Θ(1) size in both NTP and MFP. However, for any t ≥ 1 independent of n, ∆x t (ξ) generically has coordinate size

$Θ(1/ √ n) in NTP but Θ(1) in MFP.$Example for t = 1: By Eq. ( [3](#)), we have

$∆h 1 (ξ) = n -2au χ 0 ξ 0 ξdx 0 φ (h 0 ).$Plug in a u = 0 for NTP. Observe that ξ 0 , ξ, χ 0 = Θ(1), [12](#b11) so

$∆h 1 (ξ) = Θ(1) • dx 0 φ (h 0 ). (in NTP) In addition, φ (h 0 ) = Θ(1) because h 0 = Θ(1), so ∆h 1 (ξ) = Θ(1) • dx 0 Θ(1). (in NTP) Finally, dx 0 = V 0 = Θ(1/ √ n) in NTP. Altogether, this implies ∆h 1 (ξ) = Θ(1/ √ n) =⇒ ∆x 1 (ξ) ≈ φ (h 0 (ξ)) ∆h 1 (ξ) = Θ(1/ √ n) → 0, as n → ∞. (in NTP)$On the other hand, in MFP, the only thing different is a u = -1/2 and dx 0 = Θ(1/n), which implies

$∆h 1 (ξ) = Θ(n) • Θ(1/n) Θ(1) = Θ(1) =⇒ ∆x 1 (ξ) = Θ(1). (in MFP)$Feature Kernel Evolution Therefore the feature kernel F t (ξ, ζ) def = x t (ξ) x t (ζ)/n does not change in the NTK limit but it does in the MF limit, i.e. for any fixed t ≥ 1, [13](#b12) lim

$n→∞ F t (ξ, ζ) = lim n→∞ F 0 (ξ, ζ), in NTP, but lim n→∞ F t (ξ, ζ) = lim n→∞ F 0 (ξ, ζ), in MFP, in general.$Indeed, regardless of parametrization, we have

$F t (ξ, ζ) = 1 n x 0 (ξ) x 0 (ζ) + ∆x t (ξ) x 0 (ζ) + x 0 (ξ) ∆x t (ζ) + ∆x t (ξ) ∆x t (ζ) .$In NTP, because ∆x t (ξ) = Θ(1/ √ n) as noted above,

$1 n ∆x t (ξ) x 0 (ζ) = 1 n n α=1 ∆x t (ξ) α x 0 (ζ) α = 1 n n α=1 O(n -1/2 ) = O(n -1/2 ),$and likewise the other terms involving ∆x t will vanish as n → ∞. But in MFP, ∆x t (ξ) = Θ(1) will in general be correlated with

$x 0 (ζ) such that 1 n ∆x t (ξ) x 0 (ζ) = 1 n n α=1 Θ(1) = Θ(1)$. It may seem somewhat puzzling how the NTK limit induces change in f without feature or feature kernel evolution. We give some intuition in Appendix B.

Pretraining and Transfer Learning The simple fact above about the feature kernel K implies that the NTK limit is unable to perform linear transfer learning. By linear transfer learning, we mean the popular style of transfer learning where one discards the pretrained linear classifier layer and train a new one on top of the features (e.g. x in our example), which are fixed. Indeed, this is a linear problem and thus only depends on the kernel of the features. If this kernel is the same as the kernel at initialization, then the pretraining phase has had no effect on the outcome of this "transfer" learning.

In fact, a more sophisticated reasoning shows pretraining in the NTK limit is no better than random initialization for transfer learning even if finetuning is performed to the whole network, not just the classifier layer. This remains true if we replace the linear classifier layer by a new deep neural network. See Remark H.16 and Theorem H.17. The Word2Vec experiment we do in this paper is a linear transfer task.

In some other settings, such as some settings of metalearning, like the few-shot learning task in this paper, the last layer of the pretrained network is not discarded. This is called adaptation. Then the NTK limit does not automatically trivialize transfer learning. However, as will be seen in our experiments, the NTK limit still vastly underperforms the feature learning limit, which is exemplified by the MF limit here.

## Kernel Gradient Descent in Function Space

$In NTP, as n → ∞, ∇ U,V f 0 (ξ), ∇ U,V f 0 (ζ)$converges to some deterministic value K(ξ, ζ) such that K forms a kernel (the NTK). Then, in this limit, if the learning rate is η, the function f evolves according to kernel gradient descent f t+1 (ξ) = f t (ξ) -ηK(ξ, ξ t )χ t . However, this shouldn't be the case for the MF limit. For example, if φ is identity, then intuitively f t+1 (ξ) -f t (ξ) should be quadratic in η, not linear, because two layers are updated at the same time.

## abc-Parametrizations and Dynamical Dichotomy

In this section, we broaden our scope to the abc-parametrizations of deeper MLPs, defined by Eq. ( [1](#formula_0)), and their infinite-width limits. In Table [1](#), we summarize the {a l , b l } l ∪ {c} values of various abc-parametrizations in the literature. Assumption 3.1. Our main results in this section (and this section only) will assume φ is either tanh or a smooth version of relu called σ-gelu (see Definition H.1), for sufficiently small σ > 0 (which means σ-gelu approximates relu arbitrarily well).

Note this assumption is only needed for the classification of abc-parametrizations. For deriving the infinite-width limits, the much weaker Assumption H.22 suffices. We believe our results here will hold for generic nonlinearities, but making this precise is outside our scope. (See Remark H.15 for an overview on how Assumption 3.1 is used).

## Symmetries of abc-Parametrizations

As above, we can scale the parameter gradients ∇ w l f arbitrarily while keeping f fixed, if we vary a l , b l while fixing a l + b l : ∇ w l f is scaled by n -θ if a l ← a l + θ, b l ← b l -θ. In other words, changing a l , b l this way effectively gives w l a per-layer learning rate. If we apply this gradient with learning rate ηn -c , then the change in W l is scaled by ηn -c-2θ . Consequently, if c ← c -2θ, then W l is not affected by the change in a l , b l . In summary, ∀θ ∈ R : f t (ξ) stays fixed for all t and ξ if we set

$a l ← a l + θ, b l ← b l -θ, c ← c -2θ. (5)$Stable abc-Parametrizations We will only consider abc-parametrizations such that, as n → ∞, 1) the preactivations {h l } l and activations {x l } l have Θ(1) coordinates at initialization, and 2) their coordinates and the logit f (ξ) all stay O(1) throughout the course of SGD. [14](#b13) Otherwise, they tend to ∞ with n, eventually going out of floating point range. Indeed, this is an acute and real problem common in modern deep learning, where float16 is necessary to train large models. We call any such parametrization stable (see Definition H.4 for a formal definition). Thus unstable parametrizations are of no practical interest.

It turns out stable abc-parametrizations can be characterized by a set of inequalities on {a l , b l } l ∪ {c} (so that the stable ones form a polyhedron). To present these inequalities succinctly, it's useful to define Table [1](#): We summarize the abc values of SP (standard), NTP (Neural Tangent), MFP (Mean Field, for 1-hidden-layer nets), µP (Maximal Update, ours). We show the minimal value of c such that the parametrization is stable (Definition H.4). We also list the quantities r, 2a L+1 + c, a L+1 + b L+1 + r involved in stability, feature learning, and kernel regime properties of the parametrizations. Here we only focus on scaling with n and ignore dependence on input dimension. Recall the MLP definition:

$h 1 = W 1 ξ ∈ R n , x l = φ(h l ) ∈ R n , h l+1 = W l+1 x l ∈ R n , f (ξ) = W L+1 x L Definition SP (w/ LR 1 n ) NTP MFP (L = 1) µP (ours) a l W l = n -a l w l 0 0 l = 1 1 /2 l ≥ 2 0 l = 1 1 l = 2    -1 /2 l = 1 0 2 ≤ l ≤ L 1 /2 l = L + 1 b l w l αβ ∼ N (0, n -2b l ) 0 l = 1 1 /2 l ≥ 2 0 0 1 /2 c LR = ηn -c 1 0 -1 0 r Definition 3.2 1 /2 1 /2 0 0 2a L+1 + c 1 1 1 1 a L+1 + b L+1 + r 1 1 1 1 Nontrivial? Stable?$Feature Learning? Kernel Regime? Definition 3.2. For any abc-parametrization, we write r for the quantity

$r def = min(a L+1 + b L+1 , 2a L+1 + c) + c -1 + L min l=1 [2a l + I(l = 1)] .$For example, in NTP, r = 1/2, while in MFP (when L = 1), r = 0. Intuitively, r is the exponent such that ∆x L t (ξ) = Θ(n -r ). Thus, to avoid activation blowup, we want r ≥ 0; to perform feature learning, we want r = 0. Theorem 3.3 (Stability Characterization, c.f. Theorem H.6). An abc-parametrization is stable iff all of the following are true (with intuitions in parentheses):

1. ((pre)activations x l 0 , h l 0 at initialization are Θ(1) and logits f 0 are O(1))

$a 1 + b 1 = 0; a l + b l = 1/2, ∀l ∈ [2, L]; a L+1 + b L+1 ≥ 1/2.(6)$2. (features don't blowup, i.e. ∆x l t = O(1) for all l) r ≥ 0.

3. (logits don't blow up during training, i.e.

$∆W L+1 t x L t , W L+1 0 ∆x L t = O(1)) 2a L+1 + c ≥ 1; a L+1 + b L+1 + r ≥ 1.(8)$Nontrivial abc-Parametrizations Among stable abc-parametrizations, there are also those where f does not change throughout training in the infinite-width limit. We say such parametrizations are trivial. Our dichotomy result will only apply to nontrivial stable abc-parametrizations. [15](#b14) Nontrivial abc-parametrizations can also be described by a disjunction of equations on {a l , b l } l ∪ {c} (geometrically, they correspond to the union of two faces on the polyhedron of stable abcparametrizations).

Theorem 3.4. A stable abc-parametrization is nontrivial iff

$a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1.$Feature Learning Below, for brevity, we say training routine to mean the package of learning rate ηn -c , training sequence {(ξ t , y t )} t≥0 , [16](#b15) and a loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ). As above, we use • t to denote the object • after t steps of SGD. Definition 3.5 (c.f. Definitions H.9 and H.11). We say an abc-parametrization admits feature learning (resp. evolves the feature kernel) if, as n → ∞, ∆x L t (ξ) has Ω(1) coordinates (resp.

$1 n (x L t (ξ) x L t (ζ) -x L 0 (ξ) x L 0 (ζ)) = Ω(1)$) for some training routine, time t ≥ 1, and input ξ (resp. ξ, ζ). [17](#b16) MFP, in the 1-hidden-layer case, is an example of feature learning parametrization.

Intuitively, feature kernel evolution implies feature learning, but a priori it seems possible that the latter can occur without the former (akin to some kind of rotation of features). If so, then, e.g. in terms of linear transfer learning, the pretraining ultimately had no benefit. But, in fact, Theorem 3.6. A nontrivial stable abc-parametrization admits feature learning iff it evolves the feature kernel iff r = 0.

Kernel Regime While feature learning here is defined by looking at the embedding of an input ξ, we can also look at the dynamics of the function represented by the neural network. Definition 3.7 (c.f. Definition H.12). We say an abc-parametrization is in kernel regime if there exists a positive semidefinite kernel K such that, for any training routine, time t ≥ 0, and input ξ, in the n → ∞ limit,

$f t+1 (ξ) = f t (ξ) -ηK(ξ, ξ t )L (f t (ξ t ), y t ), ∀t ≥ 0.(9)$In other words, SGD reduces to kernel gradient descent in the large n limit. Theorem 3.8. A nontrivial stable abc-parametrization is in kernel regime iff r > 0.

NTP is a typical example of this, where r = 1/2 and K is given by the NTK.

## Kernel Regime

Neural Tangent

$Standard 𝐿𝑅 = Θ(1/𝑤𝑖𝑑𝑡ℎ) Maximal Update (ours)$
## Unstable

or Trivial

## Space of abc-Parametrizations

## Mean Field when depth=1

Standard 𝐿𝑅 = Θ(1) Figure [2](#): A Caricature of abc-Parametrizations. The nontrivial stable parametrizations form a high dimensional polyhedron. Those on a part of its boundary admit feature learning, while all others are in kernel regime. µP is a vertex in the former, while NTP, latter. See Fig. [5](#fig_11) for a more geometrically accurate depiction.

Dynamical Dichotomy Since a stable abc-parametrization has either r = 0 or r > 0 by Eq. ( [7](#formula_25)): Corollary 3.9. A nontrivial stable abc-parametrization either admits feature learning or is in kernel regime, but not both.

Note that kernel regime (Definition 3.7) is not defined as lack of feature learning, so Corollary 3.9 is not a trivial statement. In addition, Assumption 3.1 is necessary. For example, if φ is linear, then this dichotomy doesn't hold, as a 1-hidden-layer linear network where only the first layer is trained would both admit feature learning and is in kernel regime.

An interesting consequence of Dynamical Dichotomy is Corollary 3.10. Any nontrivial stable feature learning abcparametrization must have lim n→∞ f 0 (ξ) = 0 for all ξ, where the limit is almost sure. Theorems 3.6 and 3.8 and Corollary 3.10 are consequences of the more general classification theorem Theorem H.13, which in addition shows: 1) feature learning in layer l would imply the same for layers l, . . . , L; 2) in any feature learning parametrization, f t in the large n limit becomes deterministic, and thus is incompatible with any Bayesian perspective (in contrast to the NNGP limit).

Dynamical Dichotomy in the shallow perceptron case is illustrated by the NTK and MF limits, as presented in Section 3.1, which shows the NTK limit exemplifies Theorem 3.8 while the MF limit exemplifies Theorem 3.6. We present a simplified picture of abc-parametrizations in Fig. [2](#), but see Fig. [5](#fig_11) for a more geometrically accurate depiction.

The paragraph above Appendix H.2 gives a quick outline of the proof of Dynamical Dichotomy, and the beginning of each succeeding section outlines the logic of that section. Remark 3.11 (Function Space Picture). A kernel regime limit resides solely in the function space picture, i.e. the evolution of f at any time being solely determined by the function values {lim f t (ζ)} ζ themselves (as opposed to the internal activations of f as well) along with η, L, and (ξ t , y t ). Intuitively, this cannot be true for the feature learning limit, and therefore, at least informally, Dynamical Dichotomy is also a dichotomy over the sufficiency of the function space picture for determining the training evolution: We can construct two settings where {lim f t (ζ)} ζ , η, L, and (ξ t , y t ) are the same but f t+1 are different. 1) The first setting is at t = 0, where lim f t (ζ) = 0 for all input ζ by Corollary 3.10. Here a typical SGD will change f . 2) In the second setting, suppose φ is relu. Design a sequence of inputs such that training the MLP on them with very large learning rate will make all relu neurons saturated in the 0 region. Then f is everywhere 0, and an SGD step will not change that. Remark 3.12 (Not All Dynamics are Infinite-Width Limits). Accordingly, a nonlinear function space dynamics cannot be a valid infinite-width limit of some abc-parametrization. By nonlinear, we mean f t+1 (ξ) -f t (ξ) is nonlinear in L (f t (ξ t ), y t ). For example, any natural higher-order generalization of Eq. ( [9](#formula_29)) (perhaps derived from a Taylor expansion at initialization) is not a valid limit. [18](#b17) Pretraining and Transfer Learning As in the shallow examples, Corollary 3.9 says that any kernel regime parametrization (including NTP) trivializes pretraining and transfer learning [19](#b18) in the infinite-width limit.

By calculating r for the standard parametrization (SP), we can easily see that it cannot admit feature learning in the sense here without becoming unstable. However, in the next section, we will manually analyze the training dynamics in an SP MLP to give an intuition why this is the case. In turn, we then propose a simple modification of SP, the Maximal Update Parametrization (MUP or µP), which does admit feature learning and, in fact, does so maximally in a suitable sense. In the pedagogical spirit, we will focus on the key insights and stress the right heuristics without dwelling on formal aspects.

## Standard Parametrization

In this section, we give intuition for why gradient descent of neural network in standard parametrization (SP) will lead to logits blowup after 1 step, if the learning rate is ω(1/n), where n is the width. In addition, we will see why, with learning rate O(1/n), SP is in kernel regime. We first consider the simplest example and then state the general result at the end of the section.

To demonstrate the general principle in deep networks, it is necessary to consider the behavior of an n × n matrix in the middle of the network. Thus, the simplest case is a 2-hidden-layer linear MLP, i.e. Eq. ( [1](#formula_0)) with L = 2 and φ = id. The standard parametrization is given by

$a l = 0 ∀l, b 1 = 0, b l = 1/2 ∀l ≥ 2.$(SP)

We consider 1 step of SGD with learning rate n -c on a single data pair (ξ, y). Then we can without ambiguity suppress explicit dependence on ξ and write

$f = V h, h = W h, h = U ξ,(10)$where U αβ ∼ N (0, 1) and W αβ , V αβ ∼ N (0, 1/n) are the trainable parameters (simplifying the notation in Section 3). As in Section 3, we use • t to denote the quantity • after t step of SGD.

Because we only focus on the 1st step of SGD, we lighten notation and write • = • 0 .

Initialization Since U, W, V are independently sampled, a standard Central Limit argument would show that h, h, f all have roughly iid Gaussian coordinates of variance Θ(1).

First Gradient Now let's consider the gradients of f on the data pair (ξ, y), which are given by

$d h = V , dh = W d h, dV = h, dW = d h h = V h , dU = dh ξ .(11)$For simplicity, suppose we only update W by learning rate n -c (and leave U, V unchanged); our conclusion will not change in the general case where we train all layers. Then with χ denoting the loss derivative L (f, y), we can write

$W 1 = W -n -c χ dW.$We shall show now that c ≥ 1 or else f 1 blows up with the width n after this SGD step.

After First SGD Step At t = 1, h 1 = h since we did not update U , but

$h1 = W 1 h = h -n -c χ dW h = h -n -c χ • V h h(12)$$f 1 = V h1 = f -n -c χ V V h h.(13)$Now, as noted above, h has iid Θ(1) coordinates, so

$h h = Θ(n) ∈ R. Similarly, V ∈ R 1×n has Gaussian coordinates of variance Θ(1/n), so V V = Θ(1) ∈ R.$Finally, for typical loss function L like MSE or cross entropy, χ = L (f, y) is of order Θ(1) because f fluctuates on the order Θ(1). Altogether,

$f 1 = f -Θ(n 1-c ). Therefore, for f 1 to remain O(1), we must have c ≥ 1, i.e. the learning rate is O(1/n).$Kernel Regime and Lack of Feature Learning Consequently, the network cannot learn features in the large width limit if we would like the logits to not blow up. Indeed, this version of SGD where only W is updated can be seen to correspond to the limit where

$a 1 = θ, b 1 = -θ, a 2 = 0, b 2 = 1/2, a 3 = θ, b 3 = -θ + 1/2, θ → ∞.$With c = 1 as derived above, the parametrization is stable and nontrivial, as can be checked from Theorems 3.3 and 3.4. Then we get r = 1/2 > 0, so by Corollary 3.9, this parametrization is in kernel regime and does not admit feature learning. We can also see this directly from Eq. ( [12](#formula_35)): from our calculations above,

$h1 -h = O(n 1-c ) V = O(1) V$whose coordinates have size O(n -1/2 ) since V 's coordinates do, so there's no feature learning (at least in the first step). Finally, from Eq. ( [13](#formula_36)), because

$V V → 1 and n -c h h = n -1 h h → ξ 2 , we get 20 f 1 -f → -χK(ξ, ξ) def = -χ ξ 2 , i.$e. f evolves by kernel gradient descent with the linear kernel. Our derivations here only illustrate the first SGD step, but we can get the same conclusion from all steps of SGD similarly.

We summarize the general case below, which follows trivially from Theorem 3.3 and Corollary 3.9. Theorem 4.1. An L-hidden-layer MLP in standard parametrization (see Eq. (SP) and Table [1](#)) can only allow SGD learning rate of order O(1/n) if we require lim n→∞ E f t (ξ) 2 < ∞ for all training routine, time t, and input ξ. In this case, it is in kernel regime and does not admit feature learning.

## Maximal Update Parametrization

As shown in the last section, the standard parametrization does not admit a feature learning infinitewidth limit without blowing up logits. Here we propose simple modifications of the standard parametrization to make this possible while maintaining stability: 1) To enable feature learning, it suffices to divide the logits by √ n and use Θ(1) learning rate, i.e. set a L+1 = 1/2, c = 0 on top of Eq. (SP); 2) to allow every layer to perform feature learning, we should furthermore set a 1 = -1/2, b 1 = 1/2. We will see that this essentially means we update each weight matrix as much as possible without blowing up the logits or activations, so we call this the Maximal Update Parametrization (abbreviated MUP or µP). [20](#b19) Formally, these are almost sure convergences, but we suppress these details to emphasize on intuition.

## Dividing Logits by √ n

For example, in the 2-hidden-layer linear MLP example above, the network would compute

$f (ξ) = 1 √ n v h(ξ), h(ξ) = W h(ξ), h(ξ) = U ξ,(14)$where U αβ ∼ N (0, 1) and W αβ , v αβ ∼ N (0, 1/n) are the trainable parameters. Compared to SP (Eq. ( [10](#formula_32))), h(ξ), h(ξ) stays the same; only the logit f (ξ) is scaled down. Again, to simplify notation, we abbreviate • = • 0 and suppress explicit dependence on ξ. This has two consequences Logits at Initialization Converge to 0 since f has variance Θ(1/n) (compare to the GP limit of MLP in SP at initialization).

Θ(1) Learning Rate and Feature Learning Even though f → 0, the loss derivative χ = L (f, y) stays Θ(1) if y = 0. When we redo the calculation in Eq. ( [12](#formula_35)), we see

$h1 = h -n -c-1/2 χ v h h = h -Θ(n -c+1/2 )v(15)$$f 1 = f -n -c-1 χ vv h h = f -Θ(n -c ).$Because v has coordinates of size Θ(n -1/2 ), we see that h and f both change by Θ(1) coordinatewise if c = 0 (i.e. learning rate is Θ( [1](#formula_0))). This directly illustrates feature learning after just 1 step of SGD.

For general MLPs, we can also check a L+1 = 1/2, c = 0 on top of Eq. (SP) implies r = 0 and thus admits feature learning by Theorem 3.6.

Kernel Behavior or Lack Thereof The example we have here, where we only train the middle layer in a linear MLP, actually is in kernel regime. This does not violate Corollary 3.9, however, which assumes Assumption 3.1. If, for example, we have tanh nonlinearity, then it is easy to see the µP SGD dynamics does not have a kernel limit: If so, then f 1 -f is linear in the learning rate η. But note h1 -h is Θ(1) as n → ∞ and linear in η, as can be derived similarly to Eq. ( [15](#formula_43)). Because tanh is bounded, this cannot happen. Contrast this with SP or NTP, where h1 -h is Θ(1/ √ n) and thus "resides in the linear regime of tanh", allowing perfect scaling with η.

In addition, even in an linear MLP, if we train the middle layer and the last layer, then the dynamics intuitively will become quadratic in the weights, so will not have a kernel limit. Contrast this with SP or NTP, which suppress these higher order interactions because the learning rate is small, and a first order Taylor expansion heuristic holds.

How is this different from standard parametrization with learning rate 1/ √ n? As shown above, the logit f blows up like Θ( √ n) after 1 step of SGD with learning rate Θ(1/ √ n) in the standard parametrization, but remains Θ(1) in our parametrization here. The reason these two parametrizations seem similar is because in the 1st step, the weights receive the same updates modulo the loss derivative χ = L (f, y). Consequently,

$x L 1 -x L and h L 1 -h L are Θ(1) coordinatewise in both cases. However, this update makes x L 1 correlated with W L+1 1 , so that W L+1 1 x L 1 (and f 1 ) scales like Θ(n 1-a L+1 -b L+1 ) due to Law of Large Numbers. Thus only in our parametrization here (a L+1 = b L+1 = 1/2) is it Θ(1), while in standard parametrization (a L+1 = 0, b L+1 = 1/2) it blows up like Θ( √ n).$Contrast this with the behavior at initialization, where W L+1 and x L are independent and zero-mean, so W L+1 x L scales like Θ(n 1/2-a L+1 -b L+1 ) by Central Limit Theorem.

## First Layer Parametrization

While this now enables feature learning, the first layer preactivation h effectively stays fixed throughout training even if we were to train U . For example, if we update U in the linear MLP example Eq. ( [14](#formula_42)), then by Eq. ( [11](#formula_33)),

$U 1 = U -n -c χ dU = U -n -c χ dhξ h 1 = U 1 ξ = h -n -c χ dhξ ξ = h -Θ(n -c )dh since ξ ξ, χ = Θ(1). Now dh = W d h = W 1 √ n v has roughly iid Gaussian coordinates, each of size Θ(1/n), since 1$√ n v has coordinates of the same size. Therefore, even with c = 0, h changes by at most O(1/n) coordinatewise, which is dominated by its value at initialization. This O(1/n) change also induces a O(1/n) change in f , which would be dominated by the Θ(1) change due to W 's evolution, as seen in Eq. (15). We therefore propose to set a 1 = -1/2, b 1 = 1/2 on top of Section 5.1's parametrization. This implies the forward pass of f remains the same but U 's gradient is scaled up by n, so that h now changes by Θ(1) coordinatewise. In summary, we define Definition 5.1. The Maximal Update Parametrization (abbreviated MUP, or µP), in the context of an L-hidden-layer MLP (Eq. ( [1](#formula_0))), is given by

$c = 0, b l = 1/2 ∀l, a l =    -1/2 l = 1 0 2 ≤ l ≤ L 1/2 l = L + 1.$Notice that µP for a 1-hidden-layer perceptron is equivalent to the mean field parametrization by Eq. ( [5](#formula_8)). We also describe µP for any architecture in Appendix C.1.

## What is µP Maximal In?

For technical reasons, we adopt Assumption 3.1 again for the formal results of this section.

In an abc-parametrization, the change in weight W = W l t for any l ≥ 2 due to learning rate n -c is δW def = -n -c • n -2a dh x where we abbreviated x = x l-1 t , h = h l t , a = a l . (We will use δ to denote 1-step change, but ∆ to denote lifetime change). In the next forward pass, δW contributes δW x = -n 1-c-2a (x x/n)dh, where x is the new activation due to change in previous layers' weights. In general, x and x are strongly correlated. Then x x/n → R for some R = 0 by Law of Large Numbers (as they both have Θ(1) coordinates in a stable parametrization). One can heuristically see that dh has the same size as the last layer weights, which is

$Θ(n -(a L+1 +b L+1 ) + n -(2a L+1 +c) ) (where the first summand is from W L+1 0 and the other from ∆W L+1 t ). Thus, δW x is a vector with Θ(n -r l ) def = Θ((n -(a L+1 +b L+1 ) + n -(2a L+1 +c) )n 1-c-2a$) coordinates. If r l > 0, then δW x contributes vanishingly; if r l < 0, then δW x blows up. For l = 1, we get similar insights after accounting for the finite dimensionality of ξ. Definition 5.2. For l ∈ [L], we say W l is updated maximally if ∆W l t x l-1 t (ξ) has Θ(1) coordinates for some training routine [21](#b20) , time t ≥ 1, and input ξ. Proposition 5.3. In a stable abc-parametrization, for any l ∈ [L], W l is updated maximally iff

$r l def = min(a L+1 + b L+1 , 2a L+1 + c) + c -1 + 2a l + I(l = 1) = 0.$Note that r (Definition 3.2) is the minimum of r l over all l. In µP, we can calculate that r l = 0 for all l ∈ [L], so all W l , l ∈ [L], are updated maximally. Put another way, the final embedding x L (ξ) will have nonvanishing (nonlinear) contributions from ∆W l of all l. These contributions cause the logit f (ξ) to change via interactions with W L+1 0 and ∆W L+1 t . If both W L+1 0 and ∆W L+1 t are too small, then the logit is fixed to its initial value, so all of the feature learning would have been useless. [22](#b21) It's also possible for one to contribute vanishingly but not the other. [23](#b22) But both contribute in µP.

$Definition 5.4. We say W L+1 is updated maximally (resp. initialized maximally) if ∆W L+1 t x L t (ξ) = Θ(1) (resp. W L+1 0 ∆x L t (ξ) = Θ(1)$) for some training routine, time t ≥ 1, and input ξ.

Note Definition 5.4 is similar to Definition 5.2 except

$∆W L+1 t x L t (ξ) ∈ R but ∆W l t x l-1 t (ξ) ∈ R n . Proposition 5.5. In a stable abc-parametrization, W L+1 is 1) updated maximally iff 2a L+1 + c = 1, and 2) initialized maximally iff a L+1 + b L+1 + r = 1.$We remark that, by Theorem 3.4, a parametrization is nontrivial iff W L+1 is maximally updated or initialized. Using Propositions 5.3 and 5.5 and Theorem 3.3, we can now easily conclude Theorem 5.6. In µP, W l is updated maximally for every l ∈ [L + 1], and W L+1 is also initialized maximally. µP is the unique stable abc-parametrization with this property.

## Deriving Feature Learning Infinite-Width Limit: Intuition and Examples

We propose the Tensor Programs technique for deriving the infinite-width limit of any abcparametrization. This ultimately just requires the researcher to mechanically apply a set of rules to the computation graph underlying SGD. However, while operationally simple, this procedure would seem "too magical" at first. In this section, through a series of examples, we seek to build intuition for what is being automated by this procedure. Then, in the next section, we formally describe the Tensor Programs framework.

## Setup and Notation

For pedagogical simplicity, we only consider input dimension d = 1 and learning rate η = 1 here, but generalization to d > 1, η = 1 is straightforward. We consider SGD with a singleton minibatch {(ξ t , y t )} at time t = 0, 1, 2, . . ., where ξ t is the network input and y t is the label. We write W l t for the matrix W l after t steps of such training. For any network input ξ ∈ R, we write x l t (ξ) (resp. h l t (ξ), f t (ξ)) for the activation x l (resp. preactivation h l , logits f ) of the network after t steps of SGD. We denote the scaled gradient n∇

$x l t f t (ξ) (resp. n∇ h l t f t (ξ)) by dx l t (ξ) (resp. dh l t (ξ)).$For brevity, we abuse notation and use x l t (without being applied to ξ) to also denote the vector x l t (ξ t ) (applied specifically to ξ t ); likewise for h l t , dh l t , dx l t , f t . We will not use x l t on its own to denote the function ξ → x l t (ξ) so this should not cause confusion. The loss function is denoted L and the loss derivative L (logit, target) is in the first argument. We write χ t def = L (f t , y t ).

## 1-Hidden-Layer MLP

As mentioned above, for 1 hidden layer, the infinite-width µP limit is the same as the mean field limit of [11, 30, 43, 45]. Nevertheless, we present a slightly different derivation of this that is more consistent with the philosophy of Tensor Programs. Such a network on input ξ ∈ R is given by

$f (ξ) = V x(ξ), x(ξ) = φ(h(ξ)), h(ξ) = U ξ,(16)$for

$U ∈ R n×1 , V ∈ R 1×n parametrized like U = √ nu, V = 1$√ n v and with initialization u αβ , v αβ ∼ N (0, 1/n). [24](#b23) Then U 0 (the initial value of U ) has iid N (0, 1) coordinates. It will turn out to be convenient to represent each such coordinate distribution as a random variable Z U0 def = N (0, 1). Likewise, let Z nV0 def = N (0, 1), independent from Z U0 , represent the coordinate distribution of nV 0 (we do nV 0 instead of V 0 so that the Z random variable is always independent of n). We derive the µP limits of the first forward and backward passes manually before stating the general case. To lighten notation, we suppress the t = 0 subscript (e.g. U = U 0 , h = h 0 , f = f 0 , etc), as we will spend some time on the first SGD step.

## First Forward Pass After randomly initialization, the preactivation

$h = h(ξ) (where ξ = ξ 0 ∈ R is the first input) has iid coordinates, each a sample from Z h def = ξZ U ∈ R. Naturally, x = x(ξ) has iid coordinates as well, each a sample from Z x def = φ(Z h ). Finally, f = V x = 1 n n α=1 (nV ) α x α → f def = E Z nV Z$x by Law of Large Numbers as n → ∞. [25](#b24) In particular, f becomes deterministically 0 in this limit because V and U are independent. For a typical loss function L, the loss derivative

$χ def = L (f, y) then also become deterministic, χ → χ def = L ( f , y). First Backward Pass Similarly, dx = nV (recall dx t def = n∇ xt f t ) has coordinates distributed like Z dx def = Z nV and dh = dx φ (h) has coordinates distributed like Z dh def = Z dx φ (Z h ) = Z nV φ (Z h ).$Then SGD with learning rate 1 makes the following updates:

$v 1 = v -χx/$Since χ converges to a deterministic limit χ, the coordinates of these updates are roughly iid, corresponding to an update of Z random variables:

$Z nV1 = Z nV -χZ x , Z U1 = Z U -χξZ dh .$Second Forward Pass Thus V 1 and U 1 still have roughly iid coordinates after 1 SGD step. Then, in the second forward pass, h 1 has coordinates

$Z h1 def = ξ 1 Z U1 = ξ 1 Z U -ξ 1 χξZ dh = ξ 1 Z U -ξ 1 χξZ nV φ (Z h ), x 1 has coordinates Z x1 def = φ(Z h1$), and the output is

$f 1 = 1 n n α=1 (nV 1 ) α x α → f1 def = E Z nV1 Z x1 = E(Z nV -χZ x )Z x1(17)$as n → ∞. Then χ

$1 def = L (f 1 , y 1 ) → χ1 def = L ( f1 , y 1$) becomes deterministic. The gradient vectors have roughly iid coordinates by a similar logic.

tth Iteration Repeating the above reasoning shows that at any time t (independent of n), we obtain Theorem 6.1. Consider a 1-hidden-layer MLP in µP (Eq. ( [16](#formula_53))) and any training routine with learning rate 1. Suppose φ is pseudo-Lipschitz. [26](#b25) As n → ∞, for every input ξ, f t (ξ) converges almost surely to ft (ξ) defined as follows:

$f t (ξ) a.s. --→ ft (ξ) def = E Z nVt Z xt(ξ) , Z xt(ξ) def = φ(Z ht(ξ) ), Z ht(ξ) def = ξZ Ut ,(18)$$χt def = L ( ft , y t ), Z nVt+1 def = Z nVt -χt Z xt , Z Ut+1 def = Z Ut -χt ξ t Z nVt φ (Z ht ),(19)$with, as initial conditions, Z U0 and Z nV0 being independent standard Gaussians, where in Eq. (19)  we abbreviated ft = ft (ξ t ),

$x t = x t (ξ t ), h t = h t (ξ t ).$As aforementioned, this is a discrete time, minibatched version of the mean field limit of [11, 30,  43, 45]. [27](#b26) When φ is identity, it's easy to see that Z nVt and Z Ut are always (deterministic) linear combinations of Z nV0 and Z U0 , say Z nVt = A t Z nV0 + B t Z U0 and Z Ut = C t Z nV0 + D t Z U0 . Then the limit ft depends solely on A t , B t , C t , D t . By tracking their evolution, we get the following greatly simplified formula for an infinite-width µP linear network. Corollary 6.2. Consider a 1-hidden-layer linear MLP in µP (Eq. ( [16](#formula_53))) and any training routine with learning rate 1. As n → ∞, for every input ξ, f t (ξ) converges almost surely to ft (ξ) defined as follows:

$ft (ξ) = (A t C t + B t D t )ξ, χt = L ( ft , y t ), (A t+1 , B t+1 ) = (A t , B t ) -χt ξ t (C t , D t ), (C t+1 , D t+1 ) = (C t , D t ) -χt ξ t (A t , B t ), with initial condition A 0 = D 0 = 1, B 0 = C 0 = 0.$This can be easily generalized to larger input and output dimenions (see Appendix D.1). In a gist, such an infinite-width µP linear network with input dimension d and output dimension d o is equivalent to a width-(d + d o ) linear network with the same input/output dimensions but an "diagonal", instead of random, initialization. Our Word2Vec and MAML experiments will crucially rely on this simplifying observation. We remark that, in contrast to our approach, such an observation would be obscured by the PDE perspective of prior works [11, 30, 43, 45].

## 2-Hidden-Layer MLP: SGD with Partially Decoupled Backpropagation

A 2-hidden-layer MLP is given by f

$(ξ) = V x(ξ), x(ξ) = φ( h(ξ)), h(ξ) = W x(ξ), x(ξ) = φ(h(ξ)), h(ξ) = U ξ, for U ∈ R n×1 , W ∈ R n×n , V ∈ R 1×n parametrized like U = √ nu, V = 1 √ n v and with initial- ization u αβ , W αβ , v αβ ∼ N (0, 1/n).$The presence of the n × n Gaussian matrix W ("∞ × ∞" as opposed to "∞× finite" like U or "finite ×∞" like V ) is new and has two major effects on the infinite-width training dynamics: 1) A Central Limit effect from the random Gaussian nature of W and 2) a correlation effect between W and its transpose W . We isolate the first effect here by analyzing a slightly different version of backpropagation (which has a different limit than normal backpropagation), and then discuss the second effect in the next section. We abuse notation and abbreviate W = W 0 .

Partially Decoupled Backpropagation In this section, we analyze a version of SGD where the backpropagation weights are partially decoupled from the forward propagation weights. Here, we think of ∆W t as the trainable weights, initialized at 0, and think of the Gaussian W as untrainable "constants". The forward pass proceeds normally [28](#b27) with W t = W + ∆W t . But we sample and fix an iid copy W of W before training, and in the backward pass compute

$dx t = ( W + ∆W t )d ht instead of dx t = (W + ∆W t )d ht = W t d ht .(20)$In particular, at initialization, we would have dx 0 = W d h0 instead of dx 0 = W d h0 . Everything else stays the same in the backward pass [29](#b28) . Finally, each weight is still updated by SGD via the usual outer products: with

$χ t def = L (f t , y t ), v t+1 = v t -χ t x t / √ n, ∆w t+1 = ∆w t -χ t d ht x t /n, u t+1 = u t -χ t ξ t dh t / √ n. (21) Since V = v/ √ n, W = w, U = √$nu per µP, this causes the following changes in W s:

$V t+1 = V t -χ t x t /n, ∆W t+1 = ∆W t -χ t d ht x t /n, U t+1 = U t -χ t ξ t dh t(22)$Note here we update ∆w and ∆W instead of w and W .

Why This Decoupled SGD? The reasons we talk about this version of SGD is that it isolates the effect of having a Gaussian n × n matrix W in the backward pass, and we can derive its infinite-width limit relatively easily using Central Limit heuristics. In the normal version of SGD, W would equal W , and its correlation with W creates additional terms in the infinite-width dynamics, that are better explained on their own. Again, we walk through the first few forward and backward passes to gain some intuition for the infinite-width limit, before stating the general case.

First Forward Pass is similar to that in Section 6.1 and follows the usual calculations involved in deriving the NNGP [30](#b29) .

First Backward Pass is similar to that in Section 6.1 and to calculations involved in deriving Neural Tangent Kernel, except swapping W with W (which at this point has no visible effect, because of the Gradient Independence Phenomenon [51]; but the effect will become clear in the second forward pass) [31](#b30) . We end up with ∆W 1 = -χ 0 d h0 x 0 , as usual. [28](#b27)

$i.e. ft = Vt xt, xt = φ( ht), ht = (W + ∆Wt)xt, xt = φ(ht), ht = U ξt. 29 i.e. dxt = nV t , d ht = φ ( ht) dxt, dht = φ (ht) dxt 30 1) h0 is iid Gaussian with coordinates drawn from Z h 0 = ξ0Z U 0 ; 2) x0 has coordinates Z x 0 = φ(Z h 0 );$3) h0 = W x0 has roughly iid coordinates drawn from a zero-mean Gaussian Z h0 by a Central Limit heuristic, where Z h0 is correlated with Z h0 (ξ) for any ξ (including

$ξ = ξ0) with covariance Cov(Z h0 , Z h0 (ξ) ) = limn→∞ 1 n x 0 x0(ξ) = E Z x 0 Z x 0 (ξ) ; 4) x0 has coordinates Z x0 = φ(Z h0 ); 5) f0 = 1 n n α=1 (nV0)α x0α → f0 def = E Z nV 0 Z x0 by a Law of Large Number heuristic. 31 1) dx0 = nV 0 so Z dx 0 = Z nV 0 ; 2) Z d h0 = φ (Z h0 ) Z dx 0 ; 3) Z dx 0 = Z W d h0 is Gaussian with covariance Cov(Z dx 0 , Z dx 0 (ξ) ) = limn→∞ 1 n d h 0 d h0(ξ) = E Z d h0 Z d h0 (ξ)$for any input ξ; 4) Z dh 0 = φ (Z h 0 ) Z dx 0 . Since f converges to a deterministic number f0, we also generically have L (f, y0) → χ0 def = L ( f0, y0). Finally, the weights are updated like Eq. (22).

Second Forward Pass As usual, we have

$Z h1 = ξ 1 Z U1 = ξ 1 Z U0 -χ0 ξ 1 ξ 0 Z dh0 and Z x1 = φ(Z h1$), reflecting the coordinate distributions of h 1 and x 1 [32](#b31) . Next,

$h1 = W x 1 + ∆W 1 x 1 = W x 1 -χ 0 d h0 x 0 x 1 n .(23)$On one hand, 1)

$x 0 x1 n → E Z x1 Z x0$by a Law of Large Numbers heuristic. On the other hand, 2) by a Central Limit heuristic, W x 1 should roughly have Gaussian coordinates Z W x1 correlated with

$Z h0 = Z W x0 with Cov(Z W x1 , Z W x0 ) = lim x 0 x1 n = E Z x1 Z x0$. However, very importantly, this Central Limit heuristic is correct only because we used W in backprop instead of W ; otherwise, h 1 has a strong correlation with W through dh 0 = φ (h 0 ) (W d h0 ), and thus so does x 1 , so that W x 1 no longer has Gaussian coordinates. This is the "second major effect" referred to in the beginning of this section. See Section 6.3 for how to handle this correlation.

In any case, in our scenario here,

$Z h1 def = Z W x1 -cZ d h0 , where c = χ0 E Z x1 Z x0$, is a linear combination of a Gaussian variable and the gradient d h0 's coordinate random variable. Finally, Z x1 = φ(Z h1 ) and the logit is

$f 1 = 1 n n α=1 (nV 1 ) α x1α → f1 def = E Z nV1 Z x1 = E Z nV0 Z x1 -χ0 E Z x0 Z x1 .$Second Backward Pass Everything proceeds just like in the 1-hidden-layer case [33](#b32) except for the computation of

$dx 1 = W d h1 -∆W 1 d h1 = W d h1 -χ 0 x 0 d h 0 d h1 n .$Like in the computation of h1 in Eq. ( [23](#formula_73)),

$d h 0 d h1 n → E Z d h0 Z d h1$and W d h1 is roughly Gaussian (and correlated with W d h0 in the natural way). But again, for this Gaussian intuition to be correct, it is crucial that we use W here instead of W , or else dx 1 (and thus d h1 ) is strongly correlated with

$W (through x0 = φ(W x 0 ) inside n∆V 1 = -χ 0 x 0 ).$In any case, we have

$Z dx1 = Z W d h1 -cZ x0 , where c = χ0 E Z d h0 Z d h1 ,$is a sum of Gaussian Z W d h1 and a multiple of Z x0 . Then weights are updated according to Eq. (22).

tth Iteration For general t, we always have (true in normal SGD as well)

$∆W t = - 1 n t-1 s=0 χ s d hs x s$so that in the forward pass

$ht = W x t + ∆W t x t = W x t - t-1 s=0 χ s d hs x s x t n(24)$$Z ht def = Z W xt - t-1 s=0 χs Z d hs E Z xs Z xt .$Here

$Z W xt is Gaussian with covariance Cov(Z W xt , Z W xs ) = E Z xt Z xs for any s.$This means that Z ht and Z hs are correlated through Z W xt , Z W xs (but also through Z d hr , r ≤ min(t, s)). Likewise, in the backward pass,

$dx t = W d ht -∆W d ht = W d ht - t-1 s=0 χ s x s d h s d ht n Z dxt def = Z W d ht - t-1 s=0 χs Z xs E Z d hs Z d ht$32 Recall they abbreviate h1(ξ1) and x1(ξ1)

$33 dx1 = nV 1 , d h1 = dx1 φ ( h1), dh1 = dx1 φ (h1)$Here, Z W d ht is Gaussian with covariance Cov(Z W d ht , Z W d hs ) = E Z d ht Z d hs for any s. Thus, Z dxt and Z dxs are correlated through Z W d ht , Z W d hs (but also through Z xr , r ≤ min(t, s)). Again, the Gaussianity of Z W xt and Z W d ht depend crucially on the fact that we use W instead of W in backpropagation.

Other parts of the forward and backward propagations are similar to before. Our reasoning can be formalized via Tensor Programs to prove the following Theorem 6.3. Consider a 2-hidden-layer MLP in µP with partially decoupled backpropagation as in Eq. ( [20](#formula_67)) and any training routine with learning rate 1. Suppose φ is pseudo-Lipschitz. [34](#b33) As n → ∞, for every input ξ, f t (ξ) a.s.

--→ ft (ξ), where ft (ξ) is defined as follows:

$(forward pass) ft (ξ) def = E Z nVt Z xt(ξ) , Z xt(ξ) def = φ(Z ht(ξ) ), Z xt(ξ) def = φ(Z ht(ξ) ), Z ht(ξ) def = ξZ Ut Z ht(ξ) def = Z W xt(ξ) - t-1 s=0 χs Z d hs E Z xs Z xt(ξ) (25$$)$${Z W xt(ξ) } ξ,t centered, jointly Gaussian with Cov(Z W xt(ξ) , Z W xs(ζ) ) = E Z xt(ξ) Z xs(ζ)$(backward pass)

$χ t def = L ( ft , y t ), Z dxt def = Z nVt , Z d ht def = φ (Z ht )Z dxt Z dht def = φ (Z ht )Z dxt Z dxt def = Z W d ht - t-1 s=0 χs Z xs E Z d hs Z d ht (26) {Z W d ht } t centered, jointly Gaussian with Cov(Z W d ht , Z W d hs ) = E Z d ht Z d hs (U, V updates) Z nVt+1 def = Z nVt -χt Z xt Z Ut+1 def = Z Ut -χt ξ t Z dht$with Z U0 and Z nV0 being independent standard Gaussians as initial conditions, and by definition,

${Z W xt(ξ) } ξ,t , {Z W d ht } t , Z U0 ,$and Z nV0 are mutually independent sets of random variables. Here, if h t appears without argument, it means h t (ξ t ); likewise for ht , x t , xt , dh t , d ht , dx t , dx t , ft . 6.3 2-Hidden-Layer MLP: Normal SGD Finally, we dicuss normal SGD for 2-hidden-layer MLP, i.e. in backprop we compute

$dx t = W t d ht = (W + ∆W )d ht .$The first forward and backward passes are essentially the same as in the last section. However, as mentioned there, in the second forward pass, W x 1 (a part of h1 = W x 1 + ∆W 1 x 1 ) will no longer be approximately Gaussian because of the correlation between x 1 and W . Let's first get some intuition for why this is before stating the infinite-width limit formally.

Warmup: φ = id First, as warmup, suppose φ = id. In this case, W x 1 will actually still be Gaussian, but its variance will be different than what's predicted in the previous section. To lighten notation, we write x = x 1 in this section. Then unwinding the definition of x, we have

$x = h + aW z where we abbreviated h = ξ 1 U 0 , z = d h0 , a = -χ 0 ξ 0 ξ 1 . Then W x has coordinates (W x) α = (W h) α + a(W W z) α .$As derived in the first forward pass in Section 6.2, (W h) α is approximately Gaussian (particularly because W, U 0 are independent). This is true for (W W z) α as well here because we assumed φ = id, but not true generally. Indeed,

$(W W z) α = β,γ W αβ W γβ z γ = z α β (W αβ ) 2 + β γ =α W αβ W γβ z γ .$We will soon see the derivations of Section 6.2 correspond to ignoring the first term: In the second term, there are n summands of the form γ =α W αβ W γβ z γ that are approximately iid with variance ≈ z 2 /n 2 . Thus, the second term itself, by a Central Limit heuristic, should converge to N (0, lim n→∞ z 2 /n). On the other hand, the first term z α β (W αβ ) 2 → z α by Law of Large Numbers. Tying it all together, (W x) α is a linear combination of two Gaussian terms (W h) α and β γ =α W αβ W γβ z γ , as well as as z α (which is Gaussian in the case of φ = id, but not generally).

Note that, if we did (W W z) α instead of (W W z) α , as in the last section, then the same analysis would show the first term is z α β W αβ W βα → 0, while the second term converge in distribution to the same Gaussian. Thus, the effect of decoupling in Section 6.2 is killing the copy of z in (W x) α .

We can summarize our derivation here in terms of Z:

$For φ = id: Z W x def = Z W h + aZ W W z = Z W h + a( ẐW W z + Z z ),(27)$where

$ẐW W z def = N (0, E(Z z ) 2 ).$Note the Central Limit heuristic in the derivation of ẐW W z also shows ẐW W z is jointly Gaussian with Z W h with Cov( ẐW W z , Z W h ) = E Z W z Z h . So, to put Eq. ( [27](#formula_96)) in a form more suggestive of the general case, we will write

$Z W x = ẐW x + aZ z , where ẐW x = Z W h + a ẐW W z d = N (0, E(Z x ) 2 ). (28$$)$General φ Unwinding the definition of x, we have

$x = φ(h + aW z φ (h 0 )).(29)$By Taylor-expanding φ, we can apply a similar (though more tedious) argument as above to derive

$Z W x = ẐW x + cZ z(30)$where c = a E φ (Z h1 )φ (Z h0 ) and ẐW x d = N (0, E(Z x ) 2 ). In the case of φ = id, c reduces to a as above, recovering Eq. (28). For general φ, we can immediately see that Z W x is not Gaussian because Z z = Z dx0 φ (Z h0 ) is not. In the Tensor Programs framework formalized in Section 7, cZ z is denoted ŻW x .

Similarly, coordinates distribution of dx 1 = W 1 d h1 will also change in the backward pass.

General t For general t, we obtain dynamical equations in Z identical to those in Theorem 6.3 except that Eq. ( [25](#formula_88)) and Eq. ( [26](#)) need to be modified. We state the general result below. Theorem 6.4. Consider a 2-hidden-layer MLP in µP and any training routine with learning rate 1. Suppose φ is pseudo-Lipschitz. [35](#b34) As n → ∞, for every input ξ, f t (ξ) a.s.

--→ ft (ξ) where ft (ξ) is defined the same way as in Theorem 6.3 except that Eq. (25) should be replaced with

$Z ht(ξ) def = ẐW xt(ξ) + ŻW xt(ξ) - t-1 s=0 χs Z d hs E Z xs Z xt(ξ) { ẐW xt(ξ) } ξ,t centered, jointly Gaussian with Cov( ẐW xt(ξ) , ẐW xs(ζ) ) = E Z xt(ξ) Z xs(ζ)$and Eq. (26) should be replaced with

$Z dxt def = ẐW d ht + ŻW d ht - t-1 s=0 χs Z xs E Z d hs Z d ht { ẐW d ht } t centered, jointly Gaussian with Cov( ẐW d ht , ẐW d hs ) = E Z d ht Z d hs .$Like in Theorem 6.3, by definition, { ẐW xt(ξ) } ξ,t , { ẐW d ht } t , Z U0 , and Z nV0 are mutually independent sets of random variables.

Here, ŻW xt(ξ) def = t-1 r=0 θ r Z d hr where θ r is calculated like so: Z xt(ξ) by definition is constructed as

$Z xt(ξ) = Φ( ẐW d h0 , . . . , ẐW d ht-1 , Z U0 )$for some function[foot_27](#foot_27) Φ : R t+1 → R. Then

$θ r def = E ∂Φ( ẐW d h0 , . . . , ẐW d ht-1 , Z U0 )/∂ ẐW d hr .$Likewise, ŻW d ht def = t-1 r=0 θ r Z xr where θ r is calculated as follows: Z d ht by definition is constructed as

$Z d ht = Ψ( ẐW x0 , . . . , ẐW xt-1 , Z V0 )$for some function 36 Ψ : R t+1 → R. Then

$θ r def = E ∂Ψ( ẐW x0 , . . . , ẐW xt-1 , Z V0 )/∂ ẐW xr .$For example, generalizing Eq. ( [29](#formula_100)), for any input ξ, we have

$Z x1(ξ) = Φ(Z W d h0 , Z U0 ), where Φ(z, u) def = φ(ξu -χ0 ξ 0 ξφ (ξ 0 u)z).$Then

$θ 0 = E ∂ z Φ(Z W d h0 , Z U0 ) = -χ 0 ξ 0 ξ E φ (Z h1(ξ) )φ (Z h0$), which specializes to c in Eq. (30). Altogether,

$ŻW x1(ξ) = -χ 0 ξ 0 ξZ d h0 E φ (Z h1(ξ) )φ (Z h0 ).$Note that ẐW xt here does not equal Z W xt in Eq. ( [25](#formula_88)) in general, because the covariance Cov( ẐW xt , ẐW xs ) = E Z xt Z xs is affected by the presence of ŻW xr for all r ≤ max(s, t).

## MLP of Arbitrary Depth

The µP limit of deeper MLPs can be derived along similar logic; see Appendices H.3 to H.5 for a rigorous treatment within the Tensor Programs framework, which also covers all stable abcparametrizations.

What happens in other feature learning parametrizations If we are in the feature learning regime, then any W l that is not maximally updated (Definition 5.2) will be effectively fixed (to its initialized value) in the infinite-width limit (i.e. no learning occurs).

## Summary of Main Intuitions for Deriving the µP Limit

Law of Large Numbers Any vector z has roughly iid coordinates given by Z z . For any two vectors

$z, z ∈ R n , 1 n n α=1 z α z α → E Z z Z z . 1.$This is all we needed to derive the 1-hidden-layer dynamics of Section 6.1, since all the matrices there are size-n vectors. 2. In Sections 6.2 and 6.3, this is also used in calculating the limit of ∆W t x t . Central Limit If the underlying computation graph never involves the transpose W of a n × n Gaussian matrix W in a matrix multiplication, then W z is roughly iid Gaussian with coordinate

$Z W z d = N (0, E(Z z ) 2 ) (if W αβ ∼ N (0, 1/n)) 1.$This along with the last intuition are all we used to derive the 2-hidden-layer decoupled dynamics of Section 6.2, where W is the middle layer weight matrix. (W , W ) Correlation If W is involved, then W z has coordinates distributed like random variable ẐW z + ŻW z where ẐW z is the Gaussian obtained by pretending W is independent from W , and ŻW z results from the correlation between W and W . ŻW z is purely a linear combination of Z z for previously defined vectors z such that z depends on W z . 1. All three intuitions above are needed to derive the 2-hidden-layer dynamics of normal SGD (Section 6.3), where W is used in backpropagation. 2. The calculation of ŻW x is quite intricate, which is why we first discussed decoupled SGD in Section 6.

2, which doesn't need ŻW x calculation, before discussing normal SGD in Section 6.3. 𝑎. 𝑠. 𝑥 𝑊 MatMul 𝑍 𝑊𝑥 = ሶ 𝑍 𝑊𝑥 + መ 𝑍 𝑊𝑥 𝑊 iid 𝒩 0, 𝜎 𝑊 2 /𝑛 entries 𝒲 = Setup 𝑣 1 𝑣 2 𝑣 3 𝑣 𝑗 𝑍 𝒱 𝑍 𝑣 1 𝑍 𝑣 2 𝑍 𝑣 3 𝑍 𝑣 𝑗 ( ) = 𝒱 = 𝒩 0, 𝜎 𝑊 2 𝔼 𝑍 𝑥 2 Correction due to (𝑊, 𝑥) correlation 𝑥 1 𝑥 2 𝑥 3 𝑥 𝑘 𝜙( ( ( ( ( ( ) ) ) ) ) ) 𝜙 𝜙 𝜙 𝜙 𝜙 Nonlin 𝑍 𝑥 1 𝑍 𝑥 2 𝑍 𝑥 3 𝑍 𝑥 𝑘 ( ) 𝜙 𝑍 𝜙(𝑥 1 ,…,𝑥 𝑘 ) = ; ; ; ; ; ; ; ሞ 𝜃 1 ሞ 𝜃 2 ሞ 𝜃 ℓ 𝜃 1 𝜃 2 𝜃 ℓ Moment ሞ 𝜗 = 𝑥 1 𝑥 2 𝑥 3 𝑥 𝑘 𝜙( ( ( ( ( ( ) ) ) ) ) ) 𝜙 𝜙 𝜙 𝜙 𝜙 𝑍 𝑥 1 𝑍 𝑥 2 𝑍 𝑥 3 𝑍 𝑥 𝑘 ( ) 𝜙 ; ; ; ; ; ; ; ሞ 𝜃 1 ሞ 𝜃 2 ሞ 𝜃 ℓ 𝜃 1 𝜃 2 𝜃 ℓ 𝔼 Average 1 𝑛 1 𝑛 𝜗 𝒞 = ሞ 𝜗 as 𝑛 → ∞ Master Theorem 𝑛 → ∞ 𝑛 → ∞ Figure 3: Graphical overview of the Tensor Programs framework. For the Master Theorem, we illustrate Theorem 7.4(2) since Theorem 7.4(1) is a corollary of Theorem 7.4(2) for a larger program.

## Tensor Programs Framework

While the previous section demonstrates the intuition of how to derive the µP limit, it also lays bare 1) the increasing complexity of a manual derivation as the training goes on, as well as 2) the mounting uncertainty for whether the intuition still holds after many steps of SGD. This is a perfect call for the Tensor Programs framework, which automates (and makes rigorous) the limit derivation for any "computation graph" -including the computation graph underlying SGD. Here we review this framework (developed in Yang [49, 50, 51, 52]) in the context of µP limit. Fig. [3](#) graphically overviews the content of this section.

As seen abundantly in Section 6, the computation underlying SGD can be expressed purely via three instructions: matrix multiplication (by a Gaussian matrix, e.g. W 0 x 0 ), coordinatewise nonlinearities (e.g. φ), and taking coordinatewise average (e.g. 1 n n α=1 (nV 1 ) α x 1α ). In deriving the µP SGD limit, we focused mostly on keeping track of R n vectors (e.g. xt or dh t ), but importantly we also computed scalars f t and χ t by (what amounts to) taking coordinatewise average (e.g.

$f 1 = 1 n n α=1 (nV 1 ) α x 1α ).$We implicitly compute scalars as well inside ∆W t x t . This motivates the following notion of a program, which can be thought of as a low-level symbolic representation of a computation graph common in deep learning (e.g. underlying Tensorflow and Pytorch). Definition 7.1. A Tensor Program[foot_28](#foot_28) is a sequence of R n -vectors and R-scalars inductively generated via one of the following ways from an initial set C of random scalars, V of random R n vectors, and a set W of random R n×n matrices (which will be sampled with iid Gaussian entries in Setup 7.2)

$MatMul Given W ∈ R n×n and x ∈ R n , we can generate W x ∈ R n or W x ∈ R n$Nonlin Given φ : R k × R l → R, previous scalars θ 1 , . . . , θ l ∈ R and vectors x 1 , . . . , x k ∈ R n , we can generate a new vector φ(x 1 , . . . , x k ; θ 1 , . . . , θ l ) ∈ R n where φ(-; θ 1 , . . . , θ l ) applies coordinatewise to each "α-slice" (x 1 α , . . . , x k α ). Moment Given same setup as above, we can also generate a new scalar

$1 n n α=1 φ(x 1 α , . . . , x k α ; θ 1 , . . . , θ l ) ∈ R.$Explanation of Definition 7.1 The vectors mentioned in Definition 7.1 are exemplified by h t , x t , dh t , dx t in Section 6. The scalars mentioned are exemplified by f t , χ t as well as e.g. x s x t /n inside the calculating of h t (Eq. ( [24](#formula_83))). The θ i s in Nonlin and Moment rules may appear cryptic at first. These scalars are not needed in the first forward and backward passes. But in the second forward pass, for example for the 1-hidden-layer MLP (Section 6.1),

$x 1 = φ(h 1 ) = φ(ξ 1 U 0 -χ 0 ξ 1 ξ 0 nV 0 φ (h 0 ))$depends on the scalar χ 0 , ξ 0 , ξ 1 , and can be written in the form of Nonlin as φ(U 0 , nV 0 , h 0 ; χ 0 ) for some φ appropriately defined.

The initial set of scalars C is the training sequence {ξ t , y t } t for all three examples of Section 6. In our 2-hidden-layer MLP examples, the initial set of matrices W is {W } (Section 6.3) or {W, W } (Section 6.2), i.e. the random R n×n Gaussian matrices. On the other hand, in the 1-hidden-layer MLP example (Section 6.1), W is empty. The initial set of vectors V in all three examples are V = {U 0 , nV 0 }. [3839](#) Notice how the vectors of these V are sampled with iid standard Gaussian coordinates. We formalize a more general setup for arbitrary Tensor Programs: Setup 7.2. 1) For each initial W ∈ W, we sample iid W αβ ∼ N (0, σ 2 W /n) for some variance σ 2 W associated to W , independent of other W ∈ W; 2) for some multivariate Gaussian --→ θ for some deterministic θ ∈ R.

$Z V = Z h : h ∈ V ∈ R V ,$In all of our examples, we took σ 2 W = 1 for simplicity, but Setup 7.2 allows for other initializations (e.g. a typical initialization for relu networks is σ 2 W = 2); additionally, Z h , h ∈ V, are all standard Gaussians, independent from one another, since U 0 , nV 0 are sampled this way; and our initial scalars {ξ t , y t } t are fixed with n, so they are their own limits. [40](#b39) What Does a Tensor Program Vector Look Like? Recall that we represented the coordinate distribution of each vector h with a random variable Z h in Section 6 and kept track of how different Zs are correlated with each other. We also calculated scalar limits like f t → ft , χ t → χt . These calculations led to a set of formulas for the µP limit (e.g. Theorems 6.1, 6.3 and 6.4). We can also construct such Z h and θ for vectors h and scalars θ in any Tensor Program. They intuitively capture the coordinate distribution of vector h and the deterministic limit of θ. The following definition formally defines Z h and θ, but the connection between Z h (resp. θ) and the coordinates of h (resp. θ) is not made rigorously until Theorem 7.4 later. The ZMatMul rule below perhaps asks for some discussion, and we shall do so after the definition. Definition 7.3 (Z h and θ). Given a Tensor Program, we recursively define Z h for each vector h and θ for each scalar θ as follows.

ZInit If h ∈ V, then Z h is defined as in Setup 7.2. We also set Ẑh def = Z h and Żh def = 0.

ZNonlin + Given φ : R k × R l → R, previous scalars θ 1 , . . . , θ l ∈ R and vectors x 1 , . . . , x k ∈ R n , we have Z φ(x 1 ,...,x k ;θ1,...,θ l ) def = φ(Z x 1 , . . . , Z x k ; θ1 , . . . , θl ).

ZMoment Given same setup as above and scalar θ = 1 n n α=1 φ(x 1 α , . . . , x k α ; θ 1 , . . . , θ l ), then

$θ def = E φ(Z x 1 , . . . , Z x k ; θ1 , . . . , θl ).$Here θ1 , . . . , θl are deterministic, so the expectation is taken over Z x 1 , . . . , Z x k .

ZMatMul Z W x def = ẐW x + ŻW x for every matrix W (with N (0, σ 2 W /n) entries) and vector x, where

ZHat ẐW x is a Gaussian variable with zero mean. Let V W denote the set of all vectors in the program of the form W y for some y. Then { ẐW y : W y ∈ V W } is defined to be jointly Gaussian with zero mean and covariance

$Cov ẐW x , ẐW y def = σ 2 W E Z x Z y , for any W x, W y ∈ V W . Furthermore, { ẐW y : W y ∈ V W } is mutually independent from { Ẑv : v ∈ V ∪ W =W V W }, where W ranges over W ∪ {A : A ∈ W}. ZDot We can always unwind Z x = Φ(• • • ), for some arguments (• • • ) = ({ ẐW y i } k i=1 , { Ẑz i } j i=1 ; { θi } l i=1 ), z i ∈ V W (where V W is defined in ZHat), and deterministic function Φ : R k+j+l → R. Define ∂Z x /∂ ẐW y i def = ∂ i Φ(• • • ). Then we set ŻW x def = σ 2 W k i=1 Z y i E ∂Z x ∂ ẐW y i ,(31)$There is some nuance in this definition, so see Remark F.1 and F.2.

Explanation of Definition 7.3 Nonlin and Moment should appear only natural. However, we pause to digest the meaning of ZMatMul by relating back to our examples in Section 6. First notice that ŻW x = 0 if W is not used in the program, so that Z W x = ẐW x . This is the case in Section 6.2, where W is used in backprop instead of W . There (in Eq. ( [25](#formula_88))),

$Z W xt is Gaussian with covariance Cov(Z W xt , Z W xs ) = E Z xt Z$xs for any s, consistent with ZHat. In Section 6.3, however, ŻW x = 0 in general. The ZDot rule is a direct generalization of the calculation of Ż in Theorem 6.4. ŻW xt and ŻW d ht of Section 6.3 for general t will all be nonzero but have no easy expression. Here we seek to convey the complexity of computing them; this is optional reading for the first time reader. To calculate ŻW xt ( ŻW d ht is similar), we need to express Z xt as a function of purely ẐW d hs , s < t, and Z U0 = ẐU0 . Then we symbolically differentiate Z xt by ẐW d hs and take expectation to obtain the coefficient of Z d hs in ŻW xt . For t = 1 as in the examples in Section 6.3, this task is easy because ẐW d h0 = Ẑdx0 = Z dx0 . But in general, the calculation can balloon quickly. Indeed, note Z xt = φ(Z ht ) and

$Z ht = ξ t Z Ut = ξ t Z U0 -ξ t t-1 s=0 χs ξ s Z dhs = ξ t Z U0 -ξ t t-$1 s=0 χs ξ s φ (Z hs )Z dxs . However, each Z dxs is a linear combination of Z W d hs = ẐW d hs + ŻW d hs and Z xr , r < s (coming from ∆W t d hs ). Each of ŻW d hs and Z xr then needs to be recursively expanded in terms of Ẑ before we can calculate the symbolic partial derivative ∂Z xt /∂ ẐW d hs . Algorithm 1 Compute the infinite-width limit of an NN in any abc-parametrization and any task 1: Write the computation graph underlying training and inference in a Tensor Program (akin to writing low level PyTorch or Tensorflow code). 2: Calculate Z h for each vector h and θ for each scalar θ in the program, according to Definition 7.3. 3: The logits f t (ξ) of the neural network at any time t should be written as a collection of scalars, so ft (ξ) is calculated in the previous step. For t being inference time, ft (ξ) is the output of the infinite-width network after training. Master Theorem Finally, we relate the symbolic nature of a Tensor Program given in Definition 7.3 to the analytic limit of its computation, in the following Master Theorem. Pseudo-Lipschitz functions are, roughly speaking, functions whose (weak) derivatives are polynomially bounded. We state the theorem assuming mild regularity conditions (Assumption F.4) that roughly says most nonlinearities in the program should be pseudo-Lipschitz. Theorem 7.4 (Tensor Program Master Theorem, c.f. Theorem E.15 of [52]). Fix a Tensor Program initialized accordingly to Setup 7.2. Adopt Assumption F.4. Then 1. For any fixed k and any pseudo-Lipschitz ψ : R k

$→ R, as n → ∞, 1 n n α=1 ψ(h 1 α , . . . , h k α ) a.s. --→ E ψ(Z h 1 , . . . , Z h k ),(32)$for any vectors h 1 , . . . , h k in the program, where Z h i are as defined in Definition 7.3.

2. Any scalar θ in the program tends to θ almost surely, where θ is as defined in Definition 7.3.

Intuitively, Theorem 7.4(1) says that each "coordinate slice" (h 1 α , . . . , h k α ) can be thought of as an iid copy of (Z h 1 , . . . , Z h k ). [41](#b40) This intuition is consistent with our heuristic derivation in Section 6, and Theorem 7.4 underlies the proof of Theorems 6.1, 6.3 and 6.4. Theorem 7.4(2) allows us to directly obtain the function learned at the end of training: For example, for a 1-hidden-layer MLP, it shows that the network's output on any input ξ at time t converges to ft (ξ) given in Theorem 6.1.

Algorithm 1 summarizes how to compute the infinite-width limit of any network in any abcparametrization and for any task, using the Tensor Programs framework laid out in this section. It generalizes the manual derivations of Section 6. We carry out Algorithm 1 for MLPs in all of our experiments.

Architectural and algorithmic universality Given that Tensor Programs can express the first forward and backward computation of practically any architecture [49, 51], it should perhaps come as no surprise that they can also express practically any training and inference procedure -or just any computation -involving any such architecture. This includes both feature learning and kernel limits. We leverage this flexibility to derive and compute the µP and kernel limits for metalearning and Word2Vec; see Section 9.

Extensions We focused on programs whose vectors all have the same dimension n here. But it's easy to generalize to the case where vectors have different dimensions, which corresponds to e.g. when a network's widths are non-uniform. See [52].

## Computational Considerations

While the TP framework is very general, computing the feature learning limits analytically is inherently computationally intensive aside from special cases like the linear 1-hidden-layer MLP (Corollary 6.2). Here we explain why, so as to motivate our experimental choices below.

No closed-form formula for evaluating the expectations (e.g. in Eq. ( [32](#formula_122))) involving general nonlinearities except in special cases For example, for a 1-hidden-layer MLP (Section 6.1), after 1 step of SGD, the logit is of the form E(Z 1 + bφ(Z 2 ))φ(Z 3 + cZ 1 φ (Z 2 )) where Z i s denote different (correlated) Gaussians (Eq. ( [17](#formula_60))). While one can still evaluate this via Monte-Carlo, the error will compound quickly with training time. On the other hand, because of the nesting of φ inside φ, there is no closed-form formula for this expectation in general.

Notable Exception: If the nonlinearity φ is polynomial, then the expectation is a polynomial moment of a multivariate Gaussian and can be evaluated analytically, e.g. using Isserlis' theorem from the covariance matrix.

Even with nonlinear polynomial φ, there is exponential computational bottleneck As training time t increases, due to the nesting of φ and φ in the preactivations, the integrand of the expectation, e.g. E Z xt Z nVt , will turn out to be a polynomial in Ω(1) Gaussian variables with degree Ω(2 t ). The covariance matrix of the Gaussian variables will in general be nontrivial, so evaluating the expectation, e.g. using Isserlis' theorem, requires super-exponential time. This is because we would need to expand the polynomial integrand into monomials, and there would be Ω(2 t ) monomials, each of which require Ω(2 t ) time to evaluate using Isserlis' theorem.

n × n Gaussian matrices Both points above apply to 1-hidden-layer MLPs. Additional difficulties with deeper networks is caused by the n × n initial Gaussian matrix W l 0 , 2 ≤ l ≤ L, in the middle of the network. 1) In general, due to the nonlinearities, x l-1 t would be linearly independent from x l-1 s for all s < t. Therefore, in calculating

$W l t x l-1 t = W l 0 x l-1 t + ∆W l t x l-1 t , we create a new Gaussian variable ẐW l 0 x l-1 t$linearly independent from all previous ẐW l 0 x l-1 s , s < t. This then requires us to compute and store the covariance between them. Thus, t steps of SGD costs Ω(t 2 ) space and time (not mentioning that the computation of each covariance entry can require exponential time, as discussed above). 2) In addition, due to the interaction between W l t in the forward pass and W l t in the backward pass, there is nonzero Ż, as demonstrated in Eq. ( [30](#formula_101)). This Ż is generally a linear combination of Ω(t) terms, and the coefficients of this combination require evaluation of some expectations that typically run into the exponential bottleneck discussed above.

Summary From easiest to hardest in terms of µP limit's computational cost, we have 1) 1-hiddenlayer linear networks; 2) L-hidden-layer linear MLP, L ≥ 2; 3) nonlinear MLP with polynomial activations; 4) nonlinear MLP with nonpolynomial activations. Nevertheless, 1-hidden-layer linear networks are more than sufficient to demonstrate feature learning in Word2Vec and few-shot learning with MAML, as we show below.

## Experiments

In light of the computational difficulties discussed above, we divide our experiments into two groups: 1) Verifying our theory; 2) Scaling up to realistic datasets to demonstrate feature learning. The experiments in group 1 focus on stress-testing our theory in many scenarios to show that it describes empirical phenomena accurately. They will run into the discussed computational difficulties (Section 8), so we cannot train the infinite-width µP networks for very long, but nevertheless long enough to verify the theory. Those in group 2 focus on real datasets (metalearning and Word2Vec) where feature learning is critical, and demonstrate that the GP and NTK limits are inadequate for those tasks. Necessarily, we adopt simpler neural architectures for this purpose so we can scale up.

## Verifying the Theory

In Fig. [4](#fig_4), we analytically computed the µP limits derived in Section 6 for quadratic and linear activations, and verified them against finite width networks.

## Few-Shot Learning on Omniglot via First Order MAML

In few-shot learning, the model is given only a small number of labeled examples before asking to make predictions on unseen data. Therefore, this tests whether a model contains a good prior that can adapt quickly to the small amount of data at hand. We analytically compute the infinite-width µP limit for the three kinds of networks (depth 1, depth 2 decoupled, depth 2) described in Section 6, with either quadratic φ(x) = x 2 or linear φ(x) = x activation. The training set is random ξ t ∈ {±1}, y t ∈ {±1}, so that the deviation of finite width from infinite width losses are accentuated. We compare against finite width µP networks with width 1024 or 4096. For each width, we randomly initialize with 100 different seeds and aggregate the loss curves. The mean across these seeds is plotted as solid curves, and the standard deviation represented by the shade. As discussed in Section 8, nonlinear activation functions and higher depth face computational difficulties exponential with training time. Thus here we only train for a few steps. We observe that the quadratic network converges slower to the limit with width. This is expected since the tail of Z xt is fatter for a quadratic activation than a linear activation. 

## MAML

## Repeat

In practice, we batch the tasks, just like batches in SGD, so that we accumulate all the gradients from

Step 5 and update θ only at the end of the batch.

During meta-test time, we are tested on random unseen few-shot tasks, where each task T provides a training set D and a test set D as during meta-training. We adapt to D as in Step 3 above (or more generally we can take multiple gradient steps to adapt better) to obtain adapted parameters θ . Finally, we calculate the accuracy of θ on the test set D. We average this accuracy over many tasks T , which we report as the meta-test accuracy.

First Order vs Second Order MAML Notice in Step 5, we take the gradient of L D (f θ ) with respect to the adapted parameters θ . In Second Order MAML, we would instead take the gradient against the unadapted parameters θ, which would involve the Hessian ∇ θ ∇ θ L D (f θ ). Second Order MAML generally achieves performance slightly better than First Order MAML, but at the cost of significantly slower updates [37]. In order to scale up, we will focus on First Order MAML, hereafter referred to as just MAML. Omniglot Omniglot is a standard few-shot learning benchmark. It consists of 20 instances of 1623 characters from 50 different alphabets, each handwritten by a different person. We test our models on 1-shot 5-way classification: We draw 5 random characters, along with 1 training instance and 1 test instance for each character. After the model adapts to the training instances, it's asked to predict the character of the test instances (choosing among the 5 characters).

Models Our main model is the µP limit of a 1-hidden-layer linear MLP. We compare against: 1) finite width versions of the same; [42](#b41) 2) the NNGP and NTK limits of the same; 3) the NNGP and NTK limits of a 1-hidden-layer relu MLP. Note 2) is equivalent to a 0-hidden-layer perceptron, because the NNGP and NTK there are both linear kernels. In addition, the infinite-width SP limit of a 1-hidden-layer network is the same as the NNGP limit. Both 2) and 3) are equivalent to linear models with fixed (not learned) features, so MAML's adaptation only applies to the linear weights. On the other hand, the µP limit and the finite µP networks will learn new representations of the data over time that can quickly adapt to new tasks. [43](#b42) Hyperparameters We use (task) batch size 32 and adaptation step size 0.4 ( in Step 3). We also clip the gradient in Step 5 if the gradient has norm ≥ 0.5. [44](#b43) For each model, we tune its weight initializaton variances and the meta learning rate (η in Step 5). During meta-test time, we take 20 gradient steps during adaptation (i.e. we loop Step 3 above 20 times to obtain θ ). See Appendix D.1 for more details.  [2](#tab_4), where curves indicate means and shades indicate standard deviations. There are three key takeaways:

1) The feature learning µP limit significantly outperforms the kernel limits.

2) The benefit of feature learning dominates the benefit of having nonlinearities. 3) As width increases, the finite µP networks approach the performance of the µP limit from below.

## Word2Vec

Word2Vec [32, 33] is an early example of large-scale pretraining and transfer learning in natural language processing, where one learns a feature vector h(ξ) for every word ξ based on the principle of distributional semantics. For simplicity, we focus on a specific scheme of Word2Vec using context as a bag-of-word (CBOW), negative example sampling, and Sigmoid loss function.

## Word2Vec Pretraining

Consider training on a corpus with vocabulary V. At each time step, we sample a sentence for the corpus and choose a word i ∈ V. This word's context J ⊆ V is a window of words around it in the sentence, thought of as a bag of words. Let ξ i ∈ R |V| be the one-hot vector corresponding to word i. We pass the averaged context ξ J def = 1 |J| n j∈J ξ j through a 1-hidden-layer MLP with hidden size n and identity activation:

$f (ξ J ) = V h(ξ J ) ∈ R |V| , h(ξ J ) = U ξ J ∈ R n ,(33)$where

$V ∈ R |V|×n , U ∈ R n×|V| factor as V = n -av v, U = n -au u with initialization v α ∼ N (0, n -2bv ), u α ∼ N (0, n -2bu$), where {a v , b v , a u , b u } specify the parametrization of the network.

After each forward pass, we sample a target word τ from V: with probability p, we take τ = i; with probability 1 -p, we sample τ uniformly from V \ {i}. Following [32, 33], we take p = 1/21 ≈ 4.76%. The loss is then calculated with the Sigmoid function σ(•) :

$L(f (ξ J ), ξ τ ) = log(1 -σ(f (ξ J ) ξ τ )) τ = i log σ(f (ξ J ) ξ τ ) τ = i(34)$Then v and u are updated via SGD as usual (causing V and U to update). Conventionally, h(ξ) ∈ R n is taken as the Word2Vec embedding for a word ξ after many iterations of forward-backward updates.

Word Analogy Evaluation We evaluate the word embeddings h(ξ) with the word analogy task. This task asks the question of the kind: What to a 'queen' is as a 'man' to a 'woman'? (answer is 'king'). The Word2Vec model answers this question by computing

$argmax i h(ξ i ) (h(ξ 'man' ) -h(ξ 'woman' ) + h(ξ 'queen' ))(35)$where i ranges over V \ {'man', 'woman', 'queen'}. If the argmax here is i = 'king', then the model answers correctly; otherwise, it's incorrect. The accuracy score is the percentage of such questions answered correctly.

Dataset We train the models on text8,[foot_36](#foot_36) a clean dataset consisting of the first 100 million characters of a 2006 Wikipedia dump. The dataset has been featured in the original Word2Vec codebase and the Hutter Prize. text8 contains the first 100 million characters of fil9, a larger dataset obtained by filtering the first 1 billion characters in the aforementioned Wikipedia dump. We space-separate the datasets into tokens and keep ones that appear no less than 5 times in the entire dataset for text8 and 10 times for fil9. The resulting datasets have 71,291 and 142,276 unique vocabulary items.

Models Our main model is the µP limit of Eq. (33). We compare against the baselines of 1) finitewidth versions of the same, and 2) the NTK and GP limits of Eq. (33). As shown in Corollary 3.9, the features of the NTK limit are fixed at initialization as n → ∞ (and so are those of the GP limit, by definition), so its answer to Eq. ( [35](#formula_127)) is uniformly selected from the whole vocabulary. [46](#b45) Its accuracy is thus 1 |V|-3 . Since |V| is 71,291 for text8 and 142,276 for fil9, this number is practically 0. We compute the µP limit according to Algorithm 1, but we relate more implementation details in Appendix D.2. Findings We show our results in Table [3](#tab_5) and Figure to the right. As expected, the infinite-width and finitewidth µP networks significantly outperform the NTK limit. In addition, we observe the finite width µP networks converge to the performance of the µP limit from below, as width increases. 

## Conclusion

In this paper, we presented a framework, based on the notion of abc-parametrizations and Tensor Programs technique, that unifies the Neural Tangent Kernel (NTK) and Mean Field limits of large width neural networks (NNs). In the Dynamical Dichotomy theorem, we classified the abc-parametrizations into feature learning and kernel regimes. We identified the lack of feature learning as a fatal weakness of NTK as a model for real NN. In fact, we showed the standard parametrization suffers from the same problem. As a solution, we proposed the Maximal Update Parametrization (µP) and derived its infinite-width limit, which admits feature learning. Through experiments on Word2Vec and few-shot learning, we demonstrated that µP is a good model for feature learning behavior in neural networks.

More generally, this paper showcased the power of the Tensor Programs technique: Any computation expressable in a Tensor Program has a "infinite-width" limit we can derive. Because of the universality of Tensor Programs for expressing deep learning computation [49, 51], this technique systematically solves the mathematical problem of taking infinite-width limits which has been dealt with haphazardly in prior literature. Its immense flexibility means that the theory of reinforcement learning, selfsupervised learning, deep generative models, etc with overparametrized neural networks in the feature learning regime are now ripe for the picking.

## A A Short Origin Story of the Tensor Programs Paper Series

The Tensor Programs framework was initially proposed in [50] in February 2019, and was mainly applied to extend the NNGP and NTK limits to arbitrary architectures (and to make rigorous the signal propagation literature [9, 18, 20-22, 40-42, 44, 53-56]). While NNGP and NTK amount to taking limits of neural networks at initialization, it was soon, in April 2019, realized that Tensor Programs could 1) also trivially take limits of the entire training procedure of neural networks (which is the main theoretical idea of this paper), and 2) calculate the feature learning limit. However, at that point, it also became clear that [50] was not written accessibly, and its formulation of Tensor Programs was cumbersome to use. A question had to be asked: Should the feature learning paper be written immediately on such an unwieldy foundation, or should significant effort be devoted to fixing this foundation first? Eventually, a decision was made in favor of the latter. The Tensor Programs series was created as way to re-organize and re-present the Tensor Programs machinery in a user-friendly way to the machine learning audience (the first 3 papers [49, 51, 52] of the series), before extracting payoffs from this foundation (starting from this paper).

## B Further Discussions on the Shallow NTK and MF Examples

How does the Function Change? If the NTK limit does not allow features to evolve, then how does learning occur? To answer this question, note

$∆f t (ξ) = V 0 ∆x t (ξ) + ∆V t x 0 (ξ) + ∆V t ∆x t (ξ).$In short, then, the evolution of f t (ξ) in the NTK limit is predominantly due to V 0 ∆x t (ξ) and ∆V t x 0 (ξ) only, while in the MF limit, ∆V t ∆x t (ξ) also contributes nontrivially.

Example:

$For t = 1, ∆f 1 (ξ) = V 0 ∆x 1 (ξ) + n -2av x 0 x 0 (ξ) + n -2av x 0 ∆x 1 (ξ). In NTP, a v = 1/2, so the term n -2av x 0 x 0 (ξ) = Θ(1) for generic ξ, ξ 0 . On the other hand, n -2av x 0 ∆x 1 (ξ) = O(1/ √ n) because ∆x 1 (ξ) = O(1/ √ n) as noted above. Likewise, V 0 ∆x 1 (ξ) ≈ V 0 [φ (h 0 (ξ)) ∆h 1 (ξ)] = V 0 [φ (h 0 (ξ)) ∆h 1 (ξ)] = C n α=1 V 0α φ (h 0 (ξ) α )V 0α φ (h 0α ) = C n α=1 (V 0α ) 2 φ (h 0 (ξ) α )φ (h 0α ),$where C = χ 0 ξ 0 ξ = Θ(1). Now (V 0α ) 2 = Θ(1/n) and is almost surely positive. On the other hand, φ (h 0 (ξ) α )φ (h 0α ) = Θ(1) and should have a nonzero expectation over random initialization (for example, if φ is relu then this is obvious). Therefore, the sum above should amount to V 0 ∆x 1 (ξ) ≈ Θ(1). In summary, in the NTK limit, ∆f 1 (ξ) = Θ(1) due to the interactions between V 0 and ∆x 1 (ξ) and between ∆V 1 and x 0 (ξ), but there is only vanishing interaction between ∆V 1 and ∆x 1 (ξ).

The case for general t, again, can be derived easily using Tensor Programs.

## C abc-Parametrization for General Neural Architectures

We can straightforwardly generalize abc-parametrizations to an arbitrary neural architecture. Each parameter tensor W would get its own a W and b W , such that W = n -a W w and w is the actual trainable parameter with initialization w αβ ∼ N (0, n -2b W ). The learning rate is still ηn -c for some fixed η. General Neural Architectures More generally, µP can be defined easily for any neural architecture whose forward pass can be written down as a Tensor Program (e.g. ResNet or Transformer; see [49] for explicit programs). The learning rate is always independent of width, i.e. c = 0. For any parameter tensor W , b W is always 1/2, and a W can be defined as follows: If W is not an output weight matrix, then a W should be set to -1 + 1 2 p W , where p W = lim n→∞ log n #(W ) is a) 0 if both sides of W are fixed w.r.t. n; b) 1 if W is a vector (e.g. bias) or with one side being fixed dimensional (e.g. W 1 ); and c) 2 if W is a matrix with both sides scaling like n (e.g. weights in the middle of an MLP). If W is an output weight matrix (and thus the output dimension is fixed w.r.t. n), then a W should be 1 2 . If W is an output bias, then a W should be 0.

Optimality Properties One can formalize, in this general context, the notion of stability and the notions of a parameter tensor being updated maximally and (a set of readout weights) being initialized maximally. Then one can show that µP is the unique stable abc-parametrization such that all of its parameter tensors are updated maximally and all of its readout weights are initialized maximally.

## D Experimental Details

The main models in our experiments are all 1-hidden-layer linear MLPs with input dimension d and output dimension d o . In our experiments, we will consider more advanced forms, but, as warmup, a basic version of such a network is given by

$f (ξ) = V h(ξ), h(ξ) = U ξ,(36)$for

$U ∈ R n×d , V ∈ R do×n parametrized like U = √ nu, V = 1$√ n v and with initialization u αβ , v αβ ∼ N (0, 1/n). In this case, Corollary 6.2 generalizes to Theorem D.1. Consider a 1-hidden-layer linear MLP in µP (Eq. ( [36](#formula_130))) and any training routine with learning rate η. As n → ∞, for every input ξ ∈ R d , f t (ξ) ∈ R do converges almost surely to ft (ξ) defined as follows:

$ft (ξ) = (A t C t + B t D t )ξ ∈ R do , χt = L ( ft , y t ) ∈ R do , (A t+1 , B t+1 ) = (A t , B t ) -ηχ t ⊗ (C t ξ t , D t ξ t ), (C t+1 , D t+1 ) = (C t , D t ) -η(A t χt , B t χt ) ⊗ ξ t ,$where ⊗ denotes outer product (u ⊗ v = uv ), with initial condition

$A 0 = I do ∈ R do×do , D 0 = I d ∈ R d×d , B 0 = 0 ∈ R do×d , C 0 = 0 ∈ R d×do .$While we will not use this theorem, we intend it to give an idea of the mathematical process underneath our implementations, which we discuss now.

## D.1 Few-Shot Learning on Omniglot via MAML D.1.1 Linear 1-Hidden-Layer µP Network

We consider a linear 1-hidden-layer MLP with bias, input dimension d, output dimension d o , given by

$f (ξ) = V h(ξ) ∈ R do , h(ξ) = U ξ + B ∈ R n , where ξ ∈ R d . Following µP, we factor U = √ nu ∈ R n×d , V = 1 √ n v ∈ R do×n , B = α √ nβ ∈ R n , where u, v, β are the trainable parameters. We initialize u αβ ∼ N (0, σ 2 u /n), v αβ ∼ N (0, σ 2 v /n), β = 0 ∈ R n . We can cancel the factors of √ n and rewrite f (ξ) = vh(ξ) ∈ R do , h(ξ) = uξ + b ∈ R n ,$where b = αβ. We will also consider gradient clipping with threshold g and weight decay with coefficient γ. So in summary, the hyperparameters are σ u , σ v (init. std.), α (bias multiplier), η (LR), g (grad. clip), γ (weight decay).

As in Corollary 6.2, it's easy to see that each column of u t at any time t is always a linear combination of the columns of u 0 and the rows of v 0 such that the coefficients of these linear combinations converge deterministically in the n → ∞ limit; likewise for b t and the rows of v t . To track the evolution of f , it suffices to track these coefficients. Therefore, for implementation, we reparametrize as follows:

Coefficient matrix and vector Let µ 1 , . . . , µ d , ν 1 , . . . , ν do ∈ R n be standard Gaussian vectors such that the columns of u 0 will be initialized as σ u µ 1 / √ n, . . . , σ u µ d / √ n and the rows of V 0 will be initialized as

$σ v ν 1 / √ n, . . . , σ v ν do / √ n. Write µ = (µ 1 , . . . , µ d ) ∈ R n×d , ν = (ν 1 , . . . , ν do ) ∈ R n×do . Define coefficient matrices u ∈ R d×(d+do) , v ∈ R do×(d+do) , such that at any time, (u, v ) ∈ R n×(d+do) is 1 √ n (µ, ν)(u, v ) in the infinite-width limit. We initialize u v ← σ u I 0 0 σ v I ,$i.e. a "diagonal" initialization. Likewise, define coefficient vector b ∈ R d+do , initialized at 0, such that, at any time, b is approximately distributed as

$1 √ n (µ, ν)b.$To track the evolution of the infinite-width network, we will track the evolution of u, v, b.

In general, we use bold to denote the coefficients (in µ, ν) of a tensor (e.g. b for coefficients of b). We also use capital letters to denote the batched version (e.g. H for batched version of h). Algorithms 2 and 3 below summarize the SGD training of the finite-and the infinite-width networks. Note that aside from initialization and the hidden size (n vs d + d o ), the algorithms are essentially identical.

Algorithm 2 SGD Training of Finite-Width Linear µP 1-Hidden-Layer Network Input: Hyperparameters n, σ u , σ v , α, η, g, γ.

1: Initialize

$u αβ ∼ N (0, σ 2 u /n) 2: Initialize v αβ ∼ N (0, σ 2 v /n) 3: Initialize b ← 0 4: for each batch of inputs Ξ ∈ R B×d and la- bels Y ∈ R B×do do 5:$// Forward Pass 6:

$H ← Ξu + b ∈ R B×n 7: f (Ξ) ← Hv ∈ R B×do 8:$// Backward Pass

$9: χ ← L (f (Ξ), Y ) ∈ R B×do 10: du ← -v χ Ξ ∈ R n×d 11: dv ← -χ H ∈ R do×n 12: db ← -α 2 1 χv ∈ R n 13:$// Gradient Clipping 14:

$G ← du 2 F + dv 2 F + db α2 15:$ρ ← min(1, g/G)

16: du ← ρdu 17: dv ← ρdv 18: db ← ρdb 19: // Gradient Step w/ Weight Decay 20:

$u += ηdu -ηγu ∈ R d×n 21: v += ηdv -ηγv ∈ R do×n 22: b += ηdb -ηγb ∈ R n 23: end for Algorithm 3 SGD Training of Infinite-Width Lin- ear µP 1-Hidden-Layer Network Input: Hyperparameters σ u , σ v , α, η, g, γ. 1: Initialize u ← (σ u I, 0) 2: Initialize v ← (0, σ v I) 3: Initialize b ← 0 4: for each batch of inputs Ξ ∈ R B×d and la- bels Y ∈ R B×do do 5:$// Forward Pass 6:

$H ← Ξu + b ∈ R B×(d+do) 7: f (Ξ) ← Hv ∈ R B×do 8:$// Backward Pass

$9: χ ← L (f (Ξ), Y ) ∈ R B×do 10: du ← -v χ Ξ ∈ R (d+do)×d 11: dv ← -χ H ∈ R do×(d+do)$12:

db ← -α 2 1 χv ∈ R d+do 13:

// Gradient Clipping 14:

$G ← du 2 F + dv 2 F + db α2 15:$ρ ← min(1, g/G)

16: du ← ρdu 17: dv ← ρdv 18: db ← ρdb 19: // Gradient Step w/ Weight Decay 20:

$u += ηdu -ηγu ∈ R (d+do)×d 21: v += ηdv -ηγv ∈ R do×(d+do)$22:

b += ηdb -ηγb ∈ R d+do 23: end for During inference, we just run the Forward Pass section with Ξ substituted with test data.

The algorithms for MAML can then be obtained by a straightforward modification of these algorithms. (Note that in MAML, we do not clip gradients during adaptation, but rather clip the gradient against the validation loss of task; we also disable weight decay by setting the coefficient γ to 0).

## Hyperparameter Sweep

We sweep σ u , σ v , η and α with the following grid for finite width and µP networks. for each task in batch do for each input/label pair (ξ i , y i ) ∈ D do 8:

$χ i ← L (f Q (ξ i ), y i ) 9:$end for 10:

for each input/label pair (ξ i , y i ) ∈ D do 11:

Q.push((ξ i , -χ i ))

12:

end for

13: // Calculate Test Set Gradient 14: Sample test set D 15:

for each input/label pair ( ξi , ŷi ) ∈ D do

$16: χi ← L (f Q ( ξi ), ŷi ) 17:$end for 18:

for each input/label pair (ξ i , y i ) ∈ D do

$19: Q.pop((ξ i , -χ i )) 20:$end for 21:

$// Gradient Clip 22: G ← ( ξi,ŷi)∈ D ( ξj ,ŷj )∈ D χi χj K( ξi, ξj ) 23: ρ ← min(1, g/G) 24:$// Gradient Update 25:

for each input/label pair ( ξi , ŷi ) ∈ D do 26:

Q.push(( ξi , -ρη χi ))

27:

end for 28:

end for 29: end while

$• σ v : [2 -5 , 2 -4 , 2 -3 , 2 -2 , 2 -1 ],$• η : [0.025, 0.05, 0.1, 0.2, 0.4],

• α : [0.25, 0.5, 1,

We are interested in 1-shot, 5-way learning with Omniglot. This means that each task provides 5 training samples, each corresponding to one of the 5 labels of the task. Each hyperparameter combination above is used to train for 100 epochs over 3 random seeds, where each epoch consists of 100 batches of 32 tasks. We average the validation accuracy across the last 10 epochs and document the best hyperparameters in Table [4](#tab_9), along with the test accuracy from a 15-seed rerun [47](#b46) for better benchmarking. For NTK and GP, we additionally tune the initialization σ b for biases, which is set to 0 for both finite and µP networks for simplicity.

## D.1.2 NNGP and NTK for Relu Networks

Consider a kernel K, which in our case will be the NNGP or NTK of a 1-hidden-layer relu network. WLOG, it is induced by an embedding Φ such that K(ξ, ζ) = Φ(ξ), Φ(ζ) where , is the inner product in the embedding space; we do not care about the details of Φ or , as eventually our algorithm only depends on K.

In our setting, we will train a linear layer W on top of Φ via MAML, f (ξ) def = W, Φ(ξ) . One can see easily that W is always a linear combination of Φ(ζ) for various ζ from the training set we've seen so far. Thus, to track W , it suffices to keep an array Q of pairs (ζ, q) such that W = (ζ,q)∈Q qΦ(ζ) [47](#b46) After excluding outliers at least one standard deviation away from the mean. 

$f Q (ξ) = (ζ,q ζ )∈Q q ζ K(ζ, ξ).$In our case, the number of possible inputs is too large to instantiate a value q for every ζ, so we gradually grow a dynamic array Q, which we model as a stack. Then MAML can be implemented as in Algorithm 4.

Hyperparameter Sweep We sweep σ u , σ v , σ b and η with the following grid for GP and NTK.

• σ u : [0.25, 0.

• σ v : [0.25, 0.

• σ b : [0.25, 0.

• η : [0.05, 0.1, 0.2, 0.4, 0.8] Each hyperparameter combination above is used to train for 5 epochs (the first epoch is almost always the best) over 3 random seeds, where each epoch consists of 100 batches of 32 tasks. We take the validation accuracy among all epochs and document the best hyperparameters in Table [4](#tab_9), along with the test accuracy from a 15-seed rerun.

## D.2 Word2Vec Experimental Details

## D.2.1 µP Limit

We shall derive the training algorithm for µP Word2Vec. First, we introduce the notation for word embeddings. We denote Φ i def = h(ξ i ). If ξ i is a one-hot vector with the i th element set to 1, Φ i is essentially the i th column of the weight matrix U . We also define the following short-hands for the context embedding:

$Φ J def = E j∈J Φ j = h(ξ J ).$Similarly, V ξ τ describes a row in V ; we can define Φ τ def = ĥ(ξ τ ) def = V ξ τ and rewrite the loss function.

$L(f (ξ J ), ξ τ ) = log(1 -σ(Φ J Φ τ )) τ = i log σ(Φ J Φ τ ) τ = i.(37)$Consequently, the backward pass becomes:

$∆Φ j = 1 |J| ∆Φ J = η |J| ∂L ∂Φ J = η |J| Φ τ (1 -σ(Φ J Φ τ )) τ = i -η |J| Φ τ σ(Φ J Φ τ ) τ = i.(38)$Following µP, we initialize U αβ ∼ N (0, σ u n -1 ) and V αβ ∼ N (0, σ v n -1 ), where n is the width of the finite network. (Here the explicit multipliers of √ n in U and 1/ √ n in V cancel out because the network is linear). The tunable hyperparameters are the initialization std σ u and σ v , learning rate η and weight decay ratio γ. Rather than tuning the hyperparameters extensively for each width, we pick some reasonable values and use them for all of our experiments. Specifically, we have σ u = σ v = 1, η = 0.05 and γ = 0.001.

Again, using Corollary 6.2, we can train the µP limit in the coefficient space of u ∈ R |V|×2|V| , v ∈ R |V|×2|V| , with the same "diagonal" initialization:

$u v ← σ u I 0 0 σ v I ,$We can adopt the embedding notation and represent a row of u with the embedding coefficient vector Φ • and a column of v with Φ •. This is computationally equivalent to training with a hidden size of 2|V| and with embeddings initialized as rows (or columns) of one-hot vectors. The full algorithm is described in Algorithm 2 and Algorithm 3; in this case, we remove biases and use weight decay with coefficient γ = 0.001. After training, rows of the weight matrix u (resp. coefficient matrix u), i.e. Φ • (resp. Φ • ), are taken as the word vectors.

## D.2.2 NTK Limit

In the NTK parametrization, V and U in Eq. ( [33](#formula_124)) factor as V = 1

√ n v and U = u, and the learning rate is Θ(1). Each column U •i of U is equal to h(ξ i ). At any fixed time t, it is easy to see via Tensor Programs that

$h t (ξ i ) = h 0 (ξ i ) + j∈V O(1/ √ n)v j + O coord (1/n)$where v j denotes the jth row of v at initialization, and where O coord (1/n) means a vector that is O(1/n) coordinatewise. Recall that U = u and v are initialized with iid standard Gaussian entries.

Because ξ i is one-hot, this in particular implies h 0 (ξ i ) has standard Gaussian entries, and h 0 (ξ i ) is independent from h 0 (ξ j ) for i = j. Then for any i = j,

$1 √ n h t (ξ i ) h t (ξ j ) - 1 √ n h 0 (ξ i ) h 0 (ξ j ) a.s. --→ 0, 1 √ n h 0 (ξ i ) h 0 (ξ j ) d -→ N (0, 1)$by Law of Large Numbers (or more formally, Theorem 7.4) and Central Limit Theorem. In other words, 1

√ n h 0 (ξ i ) h 0 (ξ j ) is distributed completely randomly, with no regard to the semantic similarities of i and j. Likewise, the inner product in Eq. ( [35](#formula_127)) is random, and the argmax is a uniform sample. [48](#b47) Therefore, in the NTK limit, Word2Vec gives random answers and achieves an accuracy of 1 |V|-3 .

## E More Detailed Comparison with Deep Mean Field Limits

The key idea of previous works [5, 15, 34, 35, 46] proposing multilayer mean field limits of MLPs is to initialize each

$n × n matrix W like W αβ ← F (u α , v β )/n for some function F and u α ∼ Z u , v β ∼ Z v sampled iid for each α, β ∈ [n],$where Z u and Z v are some fixed (wrt n) random variables. If x is an activation with approximately iid coordinates distributed like random variable Z x , then W x looks like

$(W x) α ≈ E Z v ,Z x F (u α , Z v )Z$x by Law of Large Numbers (LLN), roughly iid across α. This logic will in fact hold throughout training. For well-chosen (F, Z u , Z v ), this does not get stuck at initialization but this form of initialization is very unnatural. In Nguyen & Pham (2020), this is adapted to iid initialization straightforwardly. For example, this includes W αβ ← N (0, 1)/n. If x is as above, then (W x) α → 0 by LLN because W is sampled independently from x at init and has 0 mean. This means that preactivations every layer will vanish coordinatewise to 0, from which it's easy to see that the gradients vanish where there are more than 2 hidden layers. Hence we say that the function gets stuck at initialization. Contrast this 1/n scaling with the more typical 1/ √ n scaling, i.e. W αβ ← N (0, 1)/ √ n, which is what we deal with here. On a technical level, their limit calculation purely goes through LLN, whereas we need to wrestle with Central Limit effects (from the 1/ √ n scaling) as well.

## F Nuances of the Master Theorem

Remark F.1 (Partial derivative). The partial derivative in ZDot should be interpreted as follows. By a simple inductive argument, Z x for every vector x in the program is defined uniquely as a deterministic function ϕ( Ẑx 1 , . . . , Ẑx k ) of some x 1 , . . . , x k in V or introduced by MatMul (notationally, we are suppressing the possible dependence on limit scalars θ1 , . . . , θl ). For instance, if in a program we have A ∈ W, v ∈ V, y = Av, x = A y, then Z x = Ẑx + Ẑv , so ϕ is given by ϕ(a, b) = a + b. Then

$∂Z x /∂ Ẑx i def = ∂ i ϕ( Ẑx 1 , . . . , Ẑx k$), and ∂Z x /∂ Ẑz def = 0 for any z ∈ {x 1 , . . . , x k }. Note this definition depends on the precise way the program is written, not just on the underlying mathematics. For example, if y, z ∈ V and x = φ(W (y + z)), then

$Z x = φ( ẐW (y+z) ) so that ∂Z x /∂ ẐW y = ∂Z x /∂ ẐW z = 0. If instead, we have x = φ(W y+W z), then Z x = φ( ẐW y + ẐW z ) so that ∂Z x /∂ ẐW (x+y) = 0. However, in both cases, ŻW x = (Z y + Z z ) E φ ( ẐW (y+z) ).$Remark F.2 (Partial derivative expectation). The quantity E ∂Z x ∂ ẐW y is well defined if Z x is differentiable in ẐW y . However, even if this is not the case, e.g. if x = θ(W y) where θ is the Heavyside step function, we can still define this expectation by leveraging Stein's lemma:

$In ZDot, suppose {W y i } k i=1 are all elements of V W introduced before x. Define the matrix C ∈ R k×k by C ij def = E Z y i Z y j and define the vector b ∈ R k by b i def = E ẐW y i Z x . If a = C + b (where C + denotes the pseudoinverse of C), then in ZDot we may set σ 2 W E ∂Z x ∂ ẐW y i = a i .(39)$This definition agrees with the partial derivative expectation by Stein's lemma when the latter is well defined. Theorem 7.4 holds with this broader definition of partial derivative expectation.

Pseudo-Lipschitz functions are, roughly speaking, functions whose weak derivatives are polynomially bounded.

$Definition F.3. A function f : R k → R is called pseudo-Lipschitz of degree d if |f (x) -f (y)| ≤ C x -y (1 + k i=1 |x i | d + |y i | d$) for some C. We say f is pseudo-Lipschitz if it is so for any degree.

Here are some basic properties of pseudo-Lipschitz functions:

• The norm • in Definition F.3 can be any norm equivalent to the 2 norm, e.g. p , p ≥ 1, norms. Similarly, k i=1 |x i | d + |y i | d can be replaced by x d p + y d p , for any p ≥ 1. • A pseudo-Lipschitz function is polynomially bounded. • A composition of pseudo-Lipschitz functions of degrees d 1 and d 2 is pseudo-Lipschitz of degree d 1 + d 2 . • A pseudo-Lipschitz function is Lipschitz on any compact set. We adopt the following assumption for the Master Theorem Theorem 7.4. Assumption F.4. Suppose 1. If a function φ(; -) : R 0+l → R with only parameter arguments is used in Moment, then φ is continuous in those arguments. 2. Any other function φ(-; -) : R k+l → R with parameters (where k > 0) used in Nonlin or Moment is pseudo-Lipschitz in all of its arguments (both inputs and parameters). Statement 1 in Assumption F.4 essentially says that if we have scalars θ 1 , . . . , θ l in the program, then we can produce a new scalar by applying a continuous function (a weaker restriction than a pseudo-Lipschitz function) to them. Indeed, if θ 1 , . . . , θ l converge almost surely, then this new scalar does too. In our setting, statement 1 is used to allow any loss function whose derivative is continuous.

Other versions of the Master Theorem can be found in [52], for example, versions where the we do not assume any smoothness condition at all on the nonlinearities beyond that they be polynomially bounded, in exchange for assuming what's called a rank stability condition. This rank stability should be generically true, but checking it rigorously is subtle, so we are content with the pseudo-Lipschitz condition in this paper.

## G A Rough Sketch of the Geometry of abc-Parametrizations

By the results of Section 3.2, the stable abc-parametrizations form a polyhedron defined by the inequalities of Theorem 3.3. We call the polyhedron obtained by quotienting Eq. ( [5](#formula_8)) the stable polyhedron. In this section, we remark on some geometric properties of this polyhedron.

First, observe that the stable polyhedron is unbounded (thus, we say polyhedron instead of polytope). Indeed, given any stable parametrization, for any l, we can set a l ← a l + θ, b l ← b l -θ for any θ ≥ 0 to obtain another stable parametrization. This corresponds decreasing the layer l learning rate, so that as θ → ∞, W l is not trained.

Second, by Theorem 3.4, the nontrivial parametrizations reside in two facets of the stable polyhedron. These facets are unbounded for the same reason as above.

Next, we show that NTP (as well as µP) is a vertex on the intersection of these two facets, and NTP and µP are connected by an edge. Definition G.1. Consider a stable abc-parametrization of the MLP in Eq. ( [1](#formula_0)). We say the body of the MLP is uniformly updated if, for some training routine, time t ≥ 1, and input ξ, ∆W l t x l t (ξ) = Θ(n -r ) for all l simultaneously, where r is as defined in Definition 3.2.

In the results of this section below, we assume Assumption H.22. Proposition G.2. In a stable abc-parametrization, the MLP body is uniformly updated iff r l = r for all l ∈ [L], where r l is as defined in Proposition 5.3. Theorem G.3. In NTP, the MLP body is updated uniformly and W L+1 is both initialized and updated maximally. Furthermore, at initialization, f 0 converges in distribution [49](#b48) to a Gaussian Process with nonzero kernel. NTP is the unique (modulo Eq. ( [5](#formula_8))) stable abc-parametrization with both of these properties. Theorem G.4. For any r ∈ [0, 1/2], there is a unique (modulo Eq. ( [5](#formula_8))) stable abc-parametrization with 1) that value of r and the property that 2) the MLP body is updated uniformly and W L+1 is both initialized and updated maximally. We call this parametrization the Uniform Parametrization with r-value r, denoted UP r . Its abc values are

$a l = - 1 2 I(l = 1) + r ∀l ∈ [L], a L+1 = 1/2; b l = 1/2 -r; c = 0.$In particular, UP 0 is µP and UP 1/2 is NTP. For r > 1/2, such a uniform parametrization is not stable because W 0 would need to be Θ(n r-1 ), which would cause the initial GP to blow up. Thus, geometrically, UP r , r ∈ [0, 1/2], form an edge of the stable polyhedron.

We can define the uniform stable polyhedron to be the subset of the stable polyhedron corresponding to parametrizations which update the MLP body uniformly. This is isomorphic to the stable polyhedron when L = 1. Since stable abc-parametrizations with L = 1 has only 3 degrees of freedom, say a 1 , a 2 , b 2 while we fix c = 0 (via Eq. ( [5](#formula_8))) and b 1 = -a 1 , we can visualize the corresponding stable polyhedron in 3D. However, the nontrivial parametrizations only reside in the boundary of this polyhedron. Because of its unbounded nature, we can project its boundary in 2D and visualize it. This is done in Fig. [5](#fig_11).

## H Proofs of Main Results

## H.1 Rigorous Statements of Main Results

Applicable Nonlinearities For technical reasons, in our main results we restrict our attention to the canonical examples of nonlinearities: tanh and relu -or rather, a smooth version of relu called gelu [24] common in transformer models [8]. More precisely,

$𝑎 1 = - 1 2 𝑎 1 = 0 𝑎 2 + 𝑏 2 = 1/2 𝑏 2 = 0 -𝑎 1 = 𝑎 2 = 𝑏 2 𝑓 0 is a nonzero GP 𝜇P NTP 𝑎 2 + 𝑏 2 + 2𝑎 1 = 0 𝑎 2 + 𝑎 1 = 0 𝑎 2 =$1/2 𝑎 2 + 𝑏 2 + 𝑎 1 = 1/2 𝑊 𝐿+1 updated maximally 𝑊 𝐿+1 init. maximally 𝑟 = 0 Feature Learning 𝑟 > 0 Kernel Regime Nontrivial feature learning Δ𝑊 𝐿+1 dominates 𝑊 0 𝐿+1 𝑊 0 𝐿+1 dominates Δ𝑊 𝐿+1 Boundary of Uniform Stable Polyhedron Trivial Nontrivial Legend Body NTK limit . We obtain the caricature in Fig. [2](#) by taking the nontrivial subspace of the graph here and quotienting the two facets by their respective points at infinity. Explanation of some captions: GP limit means the training dynamics amounts to training only the last layer in the infinite-width limit, starting from a nonzero initial GP.

Body NTK limit means NTK dynamics except the last layer does not contribute to the NT kernel.

Definition H.1. Define σ-gelu to be the function

$x → 1 2 xerf(σ -1 x) + σ e -σ -2 x 2 2 √ π + x 2 .$σ-gelu is a smooth approximation of relu and is the integral of 1 2 (erf(σ -1 x) + 1) that is 0 at -∞. The large σ is, the smoother σ-gelu is. As σ → 0, σ-gelu converges to relu. We believe our results will hold for generic nonlinearities, but making this precise is outside our scope here. (See Remark H.15 for some discussion).

## Notations and Terminologies

Definition H.2 (Big-O Notation). Given a sequence of scalar random variables c = {c n ∈ R} ∞ n=1 , we write c = Θ(n -a ) if there exist constants A, B such that An -a ≤ |c| ≤ Bn -a for sufficiently large n, almost surely [50](#b49) . Given a sequence of random vectors x = {x n ∈ R n } ∞ n=1 , we say x has coordinates of size Θ(n -a ) and write x = Θ(n -a ) to mean the scalar random variable sequence { x n 2 /n} n is Θ(n -a ). Similarly for the notations O(n -a ), Ω(n -a ). We use the notations Θ ξ (n -a ), O ξ (n -a ), Ω ξ (n -a ) if the hidden constants A, B are allowed to depend on some object ξ. For brevity, we will often abuse notation and say c itself is a random variable or x itself is a random vector.

Most often, the vector x will have "approximately iid" coordinates, so the notation x = Θ(n -a ) can be interpreted intuitively to say x has coordinates of "standard deviation" Θ(n -a ), which justifies the name. Definition H.3. An abc-parametrization is a joint parametrization of an MLP and the learning rate specified by the numbers {a l , b l } l ∪ {c} as in Eq. ( [1](#formula_0)). Below we will often say abc-parametrization of an MLP for short, even though the parametrization affects the learning rate as well. A training routine is a combination of learning rate ηn -c , training sequence {(ξ t , y t )} t≥0 , and a loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ).

## Main Results

We will mainly focus on stable parametrizations, defined below, which intuitively means 1) the preactivations {h l } l and activations {x l } l have Θ(1) coordinates at initialization, and 2) their coordinates and the logit f (ξ) all stay O(1) (i.e. bounded independent of n) throughout the course of SGD. [51](#b50) Otherwise, they tend to ∞ with n, eventually going out of floating point range. [50](#b49) Here almost surely means for almost every instantiation of c 1 , c 2 , . . ., i.e. it is with regard to the product probability space generated by all of {c n } ∞ n=1 . In this paper, this probability space will be generated by random initializations of a neural network at every width n. Very importantly, note the order of the qualifiers: we are saying for almost every instantiation of c 1 , c 2 , . . ., for large enough n, An -a ≤ |c| ≤ Bn -a .

51 but they may depend on training time and η; in particular, it's possible that they diverge with time Indeed, this is an acute and real problem common in modern deep learning, where float16 is necessary to train large models. Definition H.4 (Stability). We say an abc-parametrization of an L-hidden layer MLP is stable if 1. For every nonzero input ξ ∈ X ,

$h l 0 (ξ), x l 0 (ξ) = Θ ξ (1), ∀l ∈ [L], and E f 0 (ξ) 2 = O ξ (1),(40)$where the expectation is taken over the random initialization.

2. For any training routine, any time t ≥ 0, l ∈ [L], ξ ∈ X , we have

$∆h l t (ξ), ∆x l t (ξ) = O * (1), ∀l ∈ [L]$, and f t (ξ) = O * (1), where the hidden constant inside O can depend on the training routine, t, ξ, and the initial function values f 0 (X ). [52](#b51) Recall from the main text, Definition H.5. For any abc-parametrization, we write r for the quantity

$r def = min(a L+1 + b L+1 , 2a L+1 + c) + c -1 + L min l=1 [2a l + I(l = 1)] .$For example, in NTP, r = 1/2, while in µP, r = 0. Intuitively, r is the exponent such that ∆x L t (ξ) = Θ ξ (n -r ). Thus, to avoid activation blowup, we want r ≥ 0; to perform feature learning, we want r = 0. Theorem H.6 (Stability Characterization). Suppose φ is tanh or σ-gelu for sufficiently small σ. An abc-parametrization is stable iff all of the following are true (with intuitions in parentheses):

1. ((pre)activations at initialization are Θ(1) and logits are O(1))

$a 1 + b 1 = 0; a l + b l = 1/2, ∀l ∈ [2, L]; a L+1 + b L+1 ≥ 1/2.(41) 2.$(features don't blowup, i.e. ∆x l t = O(1) for all l) r ≥ 0. (42) 3. (logits don't blow up during training, i.e. ∆W L+1 t x L t , W L+1 0

$∆x L t = O(1)) 2a L+1 + c ≥ 1; a L+1 + b L+1 + r ≥ 1.(43)$Here, r is as defined in Definition H.5.

In Eq. ( [43](#formula_175)), ∆W L+1 t turns out to be Θ(n -(2a L+1 +c) ) and is correlated with x L t = Θ(1) such that their product behaves according to Law of Large Numbers; the first inequality says this should not blow up. Similarly, W L+1 0 = Θ(n -(a L+1 +b L+1 ) ) and it turns out ∆x L t = Θ(n -r ) and they will interact via Law of Large Numbers, so the second inequality says their product shouldn't blow up.

Our main results concern nontrivial parametrizations: Definition H.7 (Nontriviality). We say an abc-parametrization of an L-hidden layer MLP is trivial if for every training routine, f t (ξ) -f 0 (ξ) a.s.

--→ 0 for any time t ≥ 1 and input ξ ∈ X (i.e. the function does not evolve in the infinite-width limit). We say the parametrization is nontrivial otherwise.

Theorem H.8 (Nontriviality Characterization). Suppose φ is tanh or σ-gelu for sufficiently small σ. A stable abc-parametrization is nontrivial iff a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1. Definition H.9 (Feature Learning). We say an abc-parametrization of an L-hidden layer MLP admits feature learning in the lth layer if there exists some training routine such that

$∆x l t (ξ) = Ω * (1)(44)$for some t ≥ 0, ξ ∈ X , where the hidden constant inside Ω can depend on the training routine, t, ξ, and the initial function values f 0 (X ). We say the parametrization admits feature learning if it does so in any layer.

We say the parametrization fixes the lth layer features if for all training routine, ∆x l t (ξ) 2 /n a.s.

--→ 0 for all t ≥ 0, ξ ∈ X . We say the parametrization fixes all features if it does so in every layer.

We make similar definitions as above replacing feature with prefeature and x l with h l .

Note that the probabilistic nature of Ω * (1) means that no feature learning does not imply fixing all features (because ∆x l t (ξ) can just fluctuate wildly between 0 and infinity), but we will see that in the context of nontrivial stable abc-parametrizations, this is true. Remark H.10. We note that this is a rather weak notion of "feature learning", as we only require that the embedding x L t (ξ) changes from its initialization for some scenario, rather than, say for generic scenarios; nor do we speak at all about the "quality" of feature learning, e.g. how it helps downstream tasks. But our proofs (see Appendix H.7) will show that "some scenario" in fact implies much more general scenarios. In addition, we argue that such formal weakness is more than compensated by our experiments, which show that infinite-width limits of feature learning (in the sense defined here) abc-parametrized MLPs outperform finite MLPs and their NTK limits on tasks (namely, Word2Vec and few-shot learning) where feature learning, in the colloquial notion of the phrase, is crucial.

A somewhat stronger notion of feature learning is that the feature kernel evolves. This is, for example, essential for linear transfer learning such as in self-supervised learning of image data. Definition H.11 (Feature Kernel Evolution). We say an abc-parametrization of an L-hidden layer MLP evolves the lth layer feature kernel if there exists some training routine such that

$x l t (ξ) x l t (ζ)/n -x l 0 (ξ) x l 0 (ζ)/n = Ω *(1$) for some t ≥ 0, ξ, ζ ∈ X , where the hidden constant inside Ω can depend on the training routine, t, ξ, ζ, and the initial function values f 0 (X ). We say the parametrization evolves feature kernels if it does so in any layer.

We say the parametrization fixes the lth layer feature kernel if for all training routine,

$x l t (ξ) x l t (ζ)/n -x l 0 (ξ) x l 0 (ζ)/n a.s.$--→ 0, as n → ∞, for all t ≥ 0, ξ, ζ ∈ X . We say the parametrization fixes all feature kernels if it does so in every layer.

We make similar definitions as above replacing feature with prefeature and x l with h l .

Intuitively, for a stable parametrization, feature kernel evolution should imply feature learning (one can see the contrapositive easily). In fact, we shall see below they are equivalent notions.

On the other hand, from the NTK example, we know certain limits can be described entirely through kernel gradient descent with some kernel. Appropriately, we make the following definition. Definition H.12 (Kernel Regime). We say an abc-parametrization of an L-hidden layer MLP is in kernel regime if there exists a positive semidefinite kernel K : X 2 → R such that for every training routine, the MLP function evolves under kernel gradient descent, i.e. there exist random variables ft (ξ) for each time t ≥ 0 and input ξ ∈ X such that, as n → ∞,[foot_41](#foot_41)

${f t (ξ)} t≤T,ξ∈X d -→ { ft (ξ)} t≤T,ξ∈X , ∀T ≥ 1,$Observe that, in kernel regime, ft (ξ) is deterministic conditioned on f0 (ξ), as evident inductively from Eq. (45). For example, in the NTK limit, { f0 (ξ) : ξ ∈ X } is a nontrivial Gaussian Process (GP), but the function evolution conditioned on this GP is deterministic.

All of the concepts defined above are related to each other by the following theorem. If there is feature learning or feature kernel evolution or prefeature learning or prefeature kernel evolution in layer l, then there is feature learning and feature kernel evolution and prefeature learning and prefeature kernel evolution in layers l, . . . , L.

4. If r = 0, then for all ξ ∈ X , f 0 (ξ) a.s.

--→ 0 and f t (ξ) a.s.

--→ ft (ξ) for some deterministic ft (ξ). However, the converse is not true.

5. If r > 0, a L+1 + b L+1 + r > 1 and 2a L+1 + c = 1, then we have the Neural Network-Gaussian Process limit.

In particular, Statement 4 implies that feature learning, at least in our context, is incompatible with Bayesian, distributional perspectives of neural network limits, such as the NNGP limit.

The characterization above then trivially implies the following dichotomy. Corollary H.14 (Dynamical Dichotomy). For φ being tanh or σ-gelu for sufficiently small σ, a nontrivial stable parametrization of an L-hidden layer MLP either admits feature learning or is in kernel regime, but not both.

Remark H.15 (The Role of the φ Assumption). The dependence on φ being tanh or σ-gelu for sufficiently small σ is only needed to explicitly construct a training routine that leads to feature learning for r = 0. We expect this should be true for generic φ, but we leave this for future work. We expand more on how the φ assumption is used below.

To calculate the infinite width limit of any abc-parametrization rigorously, we only need the nonlinearity to have a polynomially bounded 2nd derivative (or more generally pseudo-Lipschitz, so as to apply the Master Theorem). The specific choice of tanh or gelu is needed to prove the part of the Dynamical Dichotomy that says a limit cannot be simultaneously in kernel regime and in feature learning regime (which, e.g. is not true for linear activation). To do so, we use Properties H.44 and H.47 of tanh and gelu, expanded below. This is really for a more convenient proof, but we believe a more general approach should work for general nonlinearities. Our argument is as follows (this is also overviewed in the start of Appendix H.7): If r = 0, we show that a sufficiently small nonzero learning rate (scaled with width in the corresponding parametrization) in 1 SGD step 1) induces a change in the features but 2) the resulting change in the NN output is not linear in the loss derivative χ. 1) means it's feature learning, and 2) means it's not in kernel regime. This argument involves showing certain derivatives of certain expectations with respect to learning rate is positive. In the case of tanh and gelu, this is checked explicitly using Properties H.44 and H.47.

## a

$1 + b 1 = 0; a l + b l = 1/2, ∀l ∈ [2, L]; a L+1 + b L+1 ≥ 1/2.$Proof. Fix an input ξ = 0. Here, because we focus on initialization, we will suppress the time 0 subscript and ξ dependence of h l , x l to mean t = 0, applied to ξ.

Obviously, h 1 = W 1 ξ is a Gaussian vector with N (0, n -(a1+b1) ξ 2 ) coordinates, so h 1 = Θ ξ (1) iff a 1 + b 1 = 0. Assume a 1 + b 1 = 0. By Law of Large Numbers, 1 n x 1 2 a.s.

--→ E φ(Z h 1 ) 2 where Z h 1 = N (0, ξ 2 ). Since φ is not almost everywhere zero and ξ = 0, this expectation is nonzero so that x 1 = Θ ξ (1).

We construct the following Tensor Program: the lone initial vector is h 1 , the initial matrices are W l , 2 ≤ l ≤ L, and initial scalars θ l def = n 1/2-(a l +b l ) . We sample h 1 α ∼ N (0, ξ 2 ) and W l αβ ∼ N (0, 1/n). Mathematically, we will represent W l = θ l W l . The program is then given by

$x l = φ(h l ), ∀l ∈ [L], ĥl = W l x l-1 , h l = θ l ĥl , ∀l ∈ [2, L],$where we used Nonlin, MatMul, and Nonlin (with parameter θ l ).

Suppose a l + b l = 1/2 (i.e. θ l = 1) for all 2 ≤ l ≤ L. Then,

$Z h l = Z ĥl = N (0, E φ(Z h l-1 ) 2 )$for each l ≤ L. Because φ is not everywhere zero, this inductively implies E(Z h l ) 2 > 0 (and so also E(Z x l ) 2 > 0) for all l ≤ L. By the Master Theorem, 1 n h l 2 a.s.

--→ E(Z h l ) 2 and 1 n x l 2 a.s.

--→ E(Z x l ) 2 so this implies h l , x l = Θ ξ (1) for all l ≤ L as desired.

Conversely, suppose m is the smallest l ≥ 2 such that a l + b l = 1/2. Then by the above reasoning, ĥm = Θ ξ (1) so h m = Θ ξ (n 1/2-(a l +b l ) ) is either blowing up to ∞ or shrinking to 0 with n. This

$shows that h l , x l = Θ ξ (1) for all l ≤ L iff a 1 + b 1 = 0 and a l + b l = 1/2 for all 2 ≤ l ≤ L. Finally, if a 1 + b 1 = 0 and a l + b l = 1/2 for all 2 ≤ l ≤ L, then we see E f 0 (ξ) 2 = (n 1/2-(a L+1 +b L+1 ) ) 2 E Z x L 2 /n. For large n, this is Θ ξ ((n 1/2-(a L+1 +b L+1 ) ) 2 ) and is O ξ (1) iff a L+1 + b L+1 ≥ 1/2.$Definition H.20. We say a parametrization is initialization-stable if it satisfies Eq. ( [40](#formula_171)) (or equivalently, Eq. ( [41](#formula_174))).

## H.3 Program Setup

In the next section, we construct the Tensor Program that encodes the training of an L-hidden layer MLP under an abc-parametrization. Here we first describe the initial matrices, vectors, and scalars of the program, along with necessary notations.

We first remark on a simplification we will make to streamline the proof.

$The Size of W L+1 0 vs ∆W L+1 t By construction, W L+1 0 = Θ(n -(a L+1 +b L+1 ) ). If x L t (ξ) = Θ(1) as in a stable parametrization, then ∆W L+1 t = Θ(n -(2a L+1 +c) ). Therefore, if a L+1 + b L+1 ≤ 2a L+1 + c, then W L+1 0 is at least as large as ∆W L+1 t$, so that W L+1 t will stay the same order (in terms of n) for all t. If the reverse inequality is true, then W L+1 0 is smaller than W L+1 t for t ≥ 1. This in particular implies that the gradients at time 0 is smaller than gradients at subsequent times. For example, we can take a L+1 + b L+1 → ∞ while fixing 2a L+1 + c, in which case W L+1 0 = 0 and the weight gradients at initialization are all 0 except for that of W L+1 . One can thus think of this as a "lag" in the training dynamics for 1 step.

Assumption H.21. For clarity of the proof, we will assume a L+1 + b L+1 ≤ 2a L+1 + c, i.e. W L+1 t stays the same order for all t. The case of a L+1 + b L+1 > 2a L+1 + c, corresponding to a 1-step "lag" as explained above, can be dealt with similarly. We will remark whenever this requires some subtlety.

For the construction of the program and the application of the Master Theorem, we will also assume the following for the rest of this paper.

Assumption H.22. φ is pseudo-Lipschitz and not almost everywhere zero. Initial Matrices, Vectors, Scalars We will assume the parametrization is initialization-stable. For ease of presentation, we also assume the input dimension d = 1.

1. Initial matrices: W 2 0 , . . . , W L 0 , sampled like (W l 0 ) αβ ∼ N (0, 1/n). 2. Initial vectors: input layer matrix W 1 0 ∈ R n×1 and normalized output layer matrix W L+1

$0 def = W L+1 0 n a L+1 +b L+1 ∈ R 1×n , sampled like (W 1 0 ) α , ( W L+10$) α ∼ N (0, 1). 3. Initial scalars: We define the following scalars (where we explain the intuition in parenthesis).

The reader can skip this part on a first read but come back when referred to. (a) (n times the scale of coordinates of

$∆W l t ) For l ≥ 2, define θ W l def = n -(a L+1 +b L+1 +c-1+2a l ) (b) (scale of coordinates of ∆W 1 t and ∆h 1 t ) Define θ 1 = θ W 1 def = n -(a L+1 +b L+1 +c+2a1) (c) (scale of coordinates of ∆W L+1 t ) θ L+1 = θ W L+1 def = n -2a L+1 -c (d) (scale of ∆h l t and ∆x l t ) For l ∈ [L], define θ h l = θ x l = θ l def = max m≤l θ W m = max(θ W l , θ l-1 )(46)$= n -(a L+1 +b L+1 +c-1+min l m=1 (2am+I(m=1)))

Note that θ L = n -r with r defined in Definition H.5. (e) (scale of

$W L+1 t ) θ f def = n -(a L+1 +b L+1 ) (f) (convenience scalars) θ x l-1 /h l = θ x l-1 /θ h l θ W l /h l = θ W l /θ h l θ W l x l-1 /h l = θ W l θ x l-1 /θ h l θ L+1/f = θ L+1 /θ f θ L+1 = nθ L+1 = n 1-2a L+1 -c θ Lf = nθ L θ f = n 1-(r+a L+1 +b L+1 )$(g) Depending on the the value of a L+1 + b L+1 , we will also construct the values of f at initialization as initial scalars. See Appendix H.4.1 for an explanation.

By our assumption that a L+1 + b L+1 ≤ 2a L+1 + c, the pseudostability inequalities of Theorem H.6 imply all of these θs either converge to 0 or stay constant at 1. This means that, assuming appropriate regularity conditions on the nonlinearities and rank stability, we can apply the Master Theorem (if θ blows up to ∞ then we can't do that).

Notations We use := to more clearly denote assignment happening in the program, as opposed to mathematical equality. To clearly demonstrate the application of Nonlin, we will also freely introduce function symbols Ψ to put things into Nonlin form.

Preview of Names for Vectors In the program, for each z ∈ {x l , h l } l , we will construct vectors δz t (ξ) to mathematically represent θ -1 z (z t (ξ) -z t-1 (ξ)) (intuition: change in z scaled to have Θ(1) coordinates). Similarly, for w ∈ {W L+1 , W 1 }, we will construct δw t to mathematically represent θ -1 w (w t -w t-1 ) (intuition: change in w scaled to have Θ(1) coordinates). Then, mathematically, z t (ξ) = z t-1 (ξ) + θ z δz t (ξ), w t = w t-1 + θ w δw t .

We will also construct dz to mathematically represent θ -1 f ∇ z f (intuition: gradient ∇ z f scaled to have Θ(1) coordinates). For weight changes, we have the following identity

$W l t -W l t-1 = -ηn -c χ t-1 n -2a l θ f dh l t-1 x l-1 t-1 = -ηχ t-1 θ W l 1 n h l t-1 x l-1 t-1 , ∀l ∈ [2, L], (47$$) and for l = 1, W l t -W l t-1 = -ηn -c χ t-1 n -2a l θ f dh l t-1 ξ t-1 = -ηχ t-1 θ W l h l t-1 ξ t-1 .(48)$
## H.4 Program Construction

Here we construct the Tensor Program encoding the SGD of an MLP. We separately describe the first forward and backward passes followed by the later forward and backward passes.

## H.4.1 First Forward Pass

For every ξ ∈ X , we compute h 1 0 (ξ) := W 1 0 ξ ∈ R n via Nonlin (as Ψ(W 1 0 ; ξ), where Ψ is multiplication by ξ), and we construct the following vectors via Nonlin and MatMul

$x l 0 (ξ) := φ(h l 0 (ξ)) ∈ R n , h l+1 0 (ξ) := W l+1 0 x l 0 (ξ) ∈ R n , for l = 1, . . . , L -1,(49)$Function Output The first output is f 0 (ξ) = W L+1 0

x L 0 (ξ), but we will define f 0 (ξ) in the program slightly differently.

## Case when a

$L+1 + b L+1 > 1/2 Then f 0 (ξ) a.s.$--→ 0 for all ξ ∈ X . In the program, we will construct f 0 (ξ) as an initial scalar mathematically defined by W L+1 0 x L 0 (ξ). 5960

Case when a L+1 + b L+1 = 1/2 If a L+1 + b L+1 = 1/2, then f 0 (ξ) converges to a nontrival Gaussian via CLT [49], so we will condition on f 0 (ξ) for all ξ ∈ X . Given values g(ξ) ∈ R for all ξ ∈ X , let E be the event that f 0

$(ξ) = 1 √ n W L+1 0 x L 0 (ξ) equals g(ξ) for all ξ ∈ X . The distribution of W L+1 0 conditioned on E is given by W L+1 0 d = E √ nX + g + Π W L+1 0 where W L+1 0 is an iid copy of W L+1 0 , g ∈ R X is the vector of {g(ξ) : ξ ∈ X }, X ∈ R X ×n has x L$0 (ξ) as rows, and Π is the orthogonal projection into the orthogonal complement of the space spanned by {x L 0 (ξ) : ξ ∈ X }. Here X + denotes the pseudo-inverse of X. By standard formulas for pseudo-inverse and orthogonal projection, we can write X + = --→ 0 because W L+1 0 is independent from X, and Σ a.s.

--→ Σ for some PSD matrix Σ. At this point in the program, all scalars we used (like ξ) are constant with n and can be absorbed into nonlinearities. By the rank stability property of any program without scalars [52], the rank of Σ is fixed for large enough n, almost surely, so Σ + a.s.

--→ Σ+ by the continuity of pseudo-inverse on fixed rank matrices.

We will now replace W L+1 0 in the program with

$W L+1 E def = X Σ + g √ n + W L+1 0 -X Σ + γ$constructed using Nonlin, where Σ + g √ n and (Σ + γ) are finite dimensional and formally considered (collections of) scalars involved as coefficients for linear combination of rows of X. Since Σ + g √ n , Σ + γ a.s.

--→ 0, we have Z W L+1 E = Z W L+1 0 . Intuitively, this means that, even after conditioning on f 0 = g, the conditional distribution of W L+1 0 is practically the same as the original distribution. We can then proceed exactly as in the case when a L+1 + b L+1 > 1/2, with W L+1 E taking the role of W L+1 0 . The program then encodes the evolution of f conditioned on f 0 (ξ) = g(ξ), ∀ξ ∈ X . 61 59 It is completely OK to define an initial scalar using randomness from other parts of the program, as long as this scalar converges almost surely to a deterministic limit [60](#) We cannot define it using a Moment instruction because, intuitively, the mechanism of this convergence is through CLT, not Law of Large Numbers.

61 Formally, we can also have {g(ξ) : ξ ∈ X } as initial scalars, but since they are fixed with n, they can be absorbed into the Nonlin that defines W L+1 E .

Assumption H.23. For the above reason, we will assume a L+1 + b L+1 > 1/2, and remark whenever the case a L+1 + b L+1 = 1/2 involves subtleties.

## H.4.2 First Backward Pass

Next, we write the backward pass dx L 0 (ξ) := W L+1 0 dh l 0 (ξ) := dx l 0 (ξ) φ (h l 0 (ξ)) dx l-1 0 (ξ) := W l 0 dh l 0 (ξ)

where, recall, dz mathematically equals θ -1 f ∇ z f . For ξ = ξ 0 and its label y 0 , we define the first loss derivative as χ 0 := L (f 0 (ξ 0 ), y 0 ) a.s.

--→ χ0 (ξ) = L (0, y 0 ) where the convergence is because L is continuous by assumption.

We also define δW L+1

1 := -ηχ 0 x L 0 (ξ 0 ) to represent the (normalized) change in W L+1 due to the first gradient step.

## H.4.3 tth Forward Pass, t ≥ 1

Overview We iteratively define δz t (ξ) to mathematically represent θ -1 z (z t (ξ) -z t-1 (ξ)), for z ∈ {x l , h l } l . Then we eventually set z t (ξ) := z 0 (ξ) + θ z δz 1 (ξ) + • • • + θ z δz t (ξ).

Likewise, we will define δW L+1 t so that

$W L+1 t = θ f W L+1 0 + θ L+1 (δW L+1 1 + • • • + δW L+1 t$). In the program, we will not directly use W L+1 t but instead use

$W L+1 t := W L+1 0 + θ L+1/f (δW L+1 1 + • • • + δW L+1 t )(50)$where θ L+1/f = θ L+1 /θ f . Mathematically,

$W L+1 t = θ -1 f W L+1 t .$Recall we shorthand z t = z t (ξ t ) for all z ∈ {x l , h l , dx l , dh l } l ∪ {f, χ}.

The Construction of (Pre)Activations We start with h = h 1 : By Eq. ( [48](#formula_190)), we have δh t (ξ) := -ηχ t-1 ξ t-1 ξdh t-1 = Ψ(dh t-1 ; ξ t-1 ξ, ηχ t-1 ).

(Notationally, recall we freely introduce function symbols Ψ to clarify the way we apply Nonlin). For higher layers, if h = h l , x = x l-1 , and W = W l , then h = W x. By Eq. ( [47](#formula_189)), we have, mathematically,

$θ h δh t (ξ) = θ x W t-$1 δx t (ξ) + (W t -W t-1 )x t (ξ) = θ x W 0 δx t (ξ) + t-1 s=1 (W s -W s-1 )δx t (ξ) + (W t -W t-1 )x t (ξ) = θ x W 0 δx t (ξ) -ηθ W t-1 s=1 χ s-1 x s-1 δx t (ξ) n dh s-1 -ηχ t-1 θ W x t-1 x t (ξ) n dh t-1

Recall θ x/h = θ -1 h θ x , θ W/h = θ -1 h θ W , θ W x/h = θ -1 h θ W θ x . With c s denoting

x s δxt(ξ) n

, we construct δh t (ξ) := θ x/h W 0 δx t (ξ) -ηθ W x/h t-1 s=1 χ s-1 c s-1 dh s-1 -ηχ t-1 θ W/h c t-1 dh t-1 = Ψ(W 0 δx t (ξ), dh 0 , . . . , dh t-1 ; η, θ x/h , θ W x/h , θ W/h , {c s , χ s } t-1 s=0 )

2. For z ∈ {x l , h l } l , we have Z zt(ξ) = Z z0(ξ) + θz Z δz1(ξ) + • • • + θz Z δzt(ξ) (52)

3. For l ∈ [L], x = x l , h = h l , we have Z δxt(ξ) = Ψ(Z ht-1(ξ) , Z δht(ξ) ; θh ) where Ψ is as in Eq. ( [51](#)). If θh = 0 (e.g. if r > 0), then Z δxt(ξ) = φ (Z ht-1(ξ) )Z δht(ξ) .

Otherwise, θh = 1, and Z δxt(ξ) = φ(Z ht(ξ) ) -φ(Z ht-1(ξ) ).

4. For h = h 1 , we have Z δht(ξ) = -ηχ t-1 ξ t-1 ξZ dht-1 .

5. For l ≥ 2, h = h l , x = x l-1 , W = W l , we have

$Z δht(ξ) = θx/h Z W0δxt(ξ) -η θW x/h t-2 s=0 χs Z dhs E Z xs Z xt(ξ) -ηχ t-1 θW/h Z dht-1 E Z xt-1 Z xt(ξ)(55)$where at least one of θx/h and θW/h equals 1. As usual, here we have the ZHat-ZDot decomposition of Z W0δxt(ξ) .

$Z W0δxt(ξ) = ẐW0δxt(ξ) + ŻW0δxt(ξ) = ẐW0δxt(ξ) + t-1 s=0 Z dhs E ∂Z δxt(ξ)$∂ ẐW 0 dhs .

## For last layer weight

$Z δW L+1 t = -ηχ t-1 Z x L t-1(56)$and

$Z W L+1 t = Z W L+1 0 + θL+1/f (Z δW L+1 1 + • • • + Z δW L+1 t )(57)$7. The output deltas have limits

$δ ft (ξ) = θ L+1 E Z δW L+1 t Z x L t (ξ) + θ Lf E Z W L+1 t-1 Z δx L t (ξ)(58)$and ft (ξ) = δ f1 (ξ) + • • • + δ ft (ξ).

8. For gradients:

$Z dx L t (ξ) = Z W L+1 t Z dh l t (ξ) = Z dx l t (ξ) φ (Z h l t (ξ) ) Z dx l-1 t (ξ) = Z W l 0 dh l t (ξ) -η θW l t-1 s=0 χs Z x l-1 s E Z dh l s Z dh l t (ξ)$9. Loss derivative χt = L ( ft , y 0 ).

The following fact follows from the results of [51] (or can be verified by straightforward calculation) and will be useful for us. Proposition H.24. Żdx l 0 (ξ) = 0 and Z dx l 0 (ξ) = Ẑdx l 0 (ξ) for any ξ ∈ X .

If Eq. ( [61](#)) is true for l = L, then

$E Z W L+1 0 Z δx L t (ξ) = -ηχ t-1 E Z W L+1 0 Z dh L 0 (ξt-1) φ (Z h L 0 (ξ) ) L-1 m= -1 θm+1 Σ m,L-1 (ξ t-1 , ξ)$where the contributions from ẐW L 0 • in Z δx L t (ξ) vanish as they are independent from Z W L+1 0 . Since Z dh L 0 (ξ) = Z W L+1 0 φ (Z h L 0 (ξ) ), we continue

$E Z W L+1 0 Z δx L t (ξ) = -ηχ t-1 E Z W L+1 0 2$φ (Z h L 0 (ξt-1) )φ (Z h L 0 (ξ) )

L-1 m= -1 θm+1 Σ m,L-1 (ξ t-1 , ξ)

$= -ηχ t-1 L-1 m= -1 θm+1 Σ mL (ξ t-1 , ξ).$Similarly, by Eq. ( [56](#formula_203)),

$E Z δW L+1 t Z x L t (ξ) = -ηχ t-1 E Z x L t-1 (ξt-1) Z x L t (ξ)$= -ηχ t-1 E Z x L 0 (ξt-1) Z x L 0 (ξ) = -ηχ t-1 Σ LL (ξ t-1 , ξ).

Altogether, these prove the desired claim. --→ 0 and δ ft (ξ) = -ηχ t-1 Σ LL (ξ t-1 , ξ), i.e. we have the Neural Network-Gaussian Process (NNGP) limit.

Conventionally, the NNGP limit is associated with only training the last layer and nothing else. This result says that the same limit can be achieved if we train the body of the network slightly, so that ∆x L t does not interact with W L+1 0 enough (embodied in the inequality a L+1 + b L+1 + r > 1) to cause changes in f t .

Proof. The premise implies θ L+1 = 1 and θ Lf = 0, and the rest follows from Theorem H.32. Remark H.36. We have assumed for simplicity of the proof that a L+1 + b L+1 ≤ 2a L+1 + c. If this is not the case, then we can easily see Corollary H.35 applies anyway.

## H.7 r = 0 Implies Feature Learning

In this section, we assume r = 0 and show any such pseudostable parametrization 1) admits (pre)feature learning and (pre)feature kernel evolution, and 2) is not in kernel regime (Theorem H.51). The overarching logic goes like this.

1. The Master Theorem shows that the specific entry 1 n x L 1 (ξ 0 ) 2 of the feature kernel converges to E(Z x L 1 (ξ0) ) 2 . If the learning rate η = 0, then x L 1 (ξ 0 ) = x L 0 and E(Z x L 1 (ξ0) ) 2 = E(Z x L 0 ) 2 . We hope to say that as η increases, E(Z x L 1 (ξ0) ) 2 moves away Theorem H.41. For all l < ,∂ 2 η λ l (0) = ∂ 2 η γ l (0) = 0, and for all l ≥ ,

$∂ 2 η λ l (0) = Cκ l 2 + 1 2 κ l 1 ∂ 2 η λ l-1 (0) ∂ 2 η γ l (0) = Cκ l 3 + 1 2$γ l 02 (0)∂ 2 η λ l-1 (0) + γ l 11 (0)∂ 2 η γ l-1 (0), where C = 2(β l (0)) 2 E( Zl 0 ) 2 > 0.

Proof. We start with the ∂ 2 η λ l (0) recurrence. For l ≥ , ∂ 2 η λ l is a sum of 3 terms, representing 1) 2 derivatives in the integrand, 2) 2 derivatives in the Gaussian variance, and 3) 1 derivative each. When evaluated at η = 0, only the first two terms survive because ∂ η λ l-1 (0) = 0 by Theorem H.40:

$∂ 2 η λ l (0) = E ∂ 2 η φ 2 (Z l 1 )| η=0 + 1 2 E(φ 2 ) (Z l 0 )∂ 2 η λ l-1 (0). Now E ∂ 2 η φ 2 (Z l 1 ) = 2∂ η (E φ(Z l 1 )φ (Z l 1 )(β l Zl 0 φ (Z l 0 ) + η Zl 0 φ (Z l 0 )∂ η β l )) = 2 E(φ 2 ) (Z l 1 )(β l Zl 0 φ (Z l 0 ) + η Zl 0 φ (Z l 0 )∂ η β l ) 2 + • • •$where other terms appear in this sum but they vanish at η = 0 because Zl 0 appears unpaired in the expectation. Thus, E ∂ 2 η φ 2 (Z l 1 )| η=0 = 2(β l (0)) 2 E( Zl 0 ) 2 E(φ 2 ) (Z l 0 )φ (Z l 0 ) 2 . Plugging this back in, we get the recurrence on ∂ 2 η λ l (0). The ∂ 2 η γ l (0) recurrence is derived similarly.

The following result will be useful for showing ∂ 3 η f1 (ξ 0 ) = 0. Theorem H.42. Define Proof. Similar to the proof of Theorem H.41.

The following result will be useful for showing prefeature kernel evolution. Theorem H.43. For all l ≥ ,

$∂ 2 η E(Z l 1 ) 2 | η=0 = 2C + γ l 11 (0)∂ 2 η λ l-1 (0),$where C = 2(β l (0)) 2 E( Zl 0 ) 2 > 0.

Proof. Similar to the proof of Theorem H.41.

## H.7.2 Applications to σ-Gelu

The following proposition regarding σ-gelu is easy to verify. Proposition H.44. Let φ be σ-gelu. For any centered Gaussian Z ∈ R with nonzero variance, E(φ 2 ) (Z), E(φ 2 ) (Z)φ (Z) 2 , E φ(Z)φ (Z)φ (Z) 2 , E φ(Z)φ (Z), E φ (Z) 2 > 0, and they converge to 0 as σ → 0. Also, E φ (Z)φ (Z) [3](#b2) , E φ (Z)φ (Z) < 0, and they converge to -∞ as σ → 0.

In particular, this means κ l 1 , κ l 2 , γ l 22 > 0, κ l 3 , γ l 02 (0), κl 3 , γ l 13 < 0. Theorem H.48. Consider a pseudostable parametrization with r = 0. If φ is tanh, then for all l ≥ , ∂ 2 η γ l (0) < 0, ∂ 2 η λ l (0) > 0.

Proof. Similar to the proof of Theorem H.45, except that here κ l 3 , γ l 02 (0) < 0, making ∂ 2 η γ l (0) < 0. . if there is feature learning or feature kernel evolution or prefeature learning or prefeature kernel evolution in layer l, then there is feature learning and feature kernel evolution and prefeature learning and prefeature kernel evolution in layers l, . . . , L.

Proof. The parametrization cannot be in kernel regime since ∂ 3 η f1 (ξ 0 ) = 0 by Theorem H.49 or Theorem H. 46. By Theorem H.45 or Theorem H.48, ∂ 2 η λ l (0) > 0 for all l ≥ , so the feature kernel evolves in layer , . . . , L, for some normalized learning rate η > 0. This implies feature learning in layer , . . . , L, since Z x L 1 (ξ0) -Z x L 0 = 0 in this case. This then implies Z h L 1 (ξ0) -Z h L 0 = 0, so we have prefeature learning in layer , . . . , L. Prefeature kernel evolution in layer , . . . , L is implied by Theorem H.43. Finally, the last statement follows clearly from our logic above.

![Take 𝑤𝑖𝑑𝑡ℎ → ∞ Limit via Tensor Programs Prior works This work Key Theoretical Idea: Tensor Programs In Section 7 and Appendix H.4, we describe the Tensor Programs technique for deriving (rigorously) the infinite-width training dynamics of any abc-parametrization. The main insight of this approach is:]()

![we sample the initial set of vectors V like {h α : h ∈ V} ∼ Z V iid for each α ∈ [n]. 3) For each initial scalar θ ∈ C, we require θ a.s.]()

![Figure4: Empirical Simulation Agrees with Theory. We analytically compute the infinite-width µP limit for the three kinds of networks (depth 1, depth 2 decoupled, depth 2) described in Section 6, with either quadratic φ(x) = x 2 or linear φ(x) = x activation. The training set is random ξ t ∈ {±1}, y t ∈ {±1}, so that the deviation of finite width from infinite width losses are accentuated. We compare against finite width µP networks with width 1024 or 4096. For each width, we randomly initialize with 100 different seeds and aggregate the loss curves. The mean across these seeds is plotted as solid curves, and the standard deviation represented by the shade. As discussed in Section 8, nonlinear activation functions and higher depth face computational difficulties exponential with training time. Thus here we only train for a few steps. We observe that the quadratic network converges slower to the limit with width. This is expected since the tail of Z xt is fatter for a quadratic activation than a linear activation.]()

![In Model Agnostic Meta-Learning (MAML), the model performs few-shot learning by one or more SGD steps on the given training data; this is called adaptation. In a pretraining (also called meta-training) phase, MAML learns a good initialization of the model parameters for this adaptation. The training objective is to minimize the loss on a random task's test set after the model has adapted to its training set. More precisely, the basic First Order MAML at training time goes as follows: With f θ denoting the model with parameters θ, and with step sizes , η, we do 1. At each time point, sample a few-shot task T 2. From T , sample a training set D 3. Adapt θ ← θ -∇ θ L D (f θ ), where L D (f θ ) is the loss of f θ over D 4. Sample a test set D from T 5. Update θ ← θ -η∇ θ L D (f θ ), where L D (f θ ) is the loss of f θ over D]()

![Our results are summarized in the Figure to the right and Table]()

![Maximal Update Parametrization MLP with Biases Suppose in Eq. (1), for each l ∈ [L], we have h l (ξ) = W l x l-1 (ξ) + b l instead, for bias b l ∈ R n . Then in µP, the bias b l should have a b l = -1/2 and b b l = 1/2. We can also have bias b L+1 in the logits f (ξ) = W L+1 x L (ξ) + b L+1 . Then we set a b L+1 = b b L+1 = 0.]()

![σ u : [0.5, 1, 2, 4, 8], Algorithm 4 MAML Training of Kernel Model with Kernel K Input: Kernel K, adaptation step size , meta learning rate η, batch size B, gradient clip g 1: Initialize Q = {} 2: while True do]()

![Figure 5: 2D Projection of the Boundary of the Uniform Stable Polyhedron (Equivalently, the Boundary of the Stable Polyhedron for L = 1).Here, we label each facet and edge of the graph with orange text to indicate the corresponding defining algebraic condition in the L = 1 case (as part of the stable polyhedron, assuming c = 0 and b 1 = -a 1 ), and with black text to indicate the verbal interpretation valid for all L (as part of the uniform stable polyhedron). We obtain the caricature in Fig.2by taking the nontrivial subspace of the graph here and quotienting the two facets by their respective points at infinity. Explanation of some captions: GP limit means the training dynamics amounts to training only the last layer in the infinite-width limit, starting from a nonzero initial GP. Body NTK limit means NTK dynamics except the last layer does not contribute to the NT kernel.]()

![convergence in distribution, and ft+1 (ξ) = ft (ξ) -ηK(ξ, ξ t )L ( ft (ξ t ), y t ), ∀t ≥ 0.]()

![Classification of abc-Parametrizations). Suppose φ is tanh or σ-gelu for sufficiently small σ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then 1. The following are equivalent to r = 0 (a) feature learning (b) feature learning in the Lth layer (c) feature kernels evolution (d) feature kernel evolution in the Lth layer (e) prefeature learning (f) prefeature learning in the Lth layer (g) prefeature kernels evolution (h) prefeature kernel evolution in the Lth layer 2. The following are equivalent to r > 0 (a) kernel regime (b) fixes all features (c) fixes features in the Lth layer (d) fixes all feature kernels (e) fixes feature kernel in the Lth layer (f) fixes all prefeatures (g) fixes prefeatures in the Lth layer (h) fixes all prefeature kernels (i) fixes prefeature kernel in the Lth layer 3.]()

![X (XX /n) + , Π = I -1 n X (XX /n) + X. Let Σ def = XX /n and γ def = (X W L+1 0 /n). Then Π W L+1 0 X Σ + γ,and√ nX + g = 1 √ n X Σ + g.By the Master Theorem, γ a.s.]()

![A pseudostable parametrization with r > 0 is nontrivial iffa L+1 + b L+1 + r = 1 or 2a L+1 + c = 1. Proof. The kernel Σ in Theorem H.32 is nonzero iff θ L+1 or θ Lf is 1, which is equivalent to saying a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1. Corollary H.34. An initialization-stable parametrization with r > 0 but a L+1 + b L+1 + r < 1 or 2a L+1 + c < 1 is not stable. Proof. If a L+1 + b L+1 + r < 1 or 2a L+1 + c < 1, then θ L+1 → ∞ or θ Lf → ∞.Clearly, from the definition, Σ mL (ξ, ξ) > 0 for any ξ = 0 and m ∈ [0, L]. All of our reasoning leading up to Theorem H.32 applied at t = 1 holds, so Theorem H.32 (along with the Master Theorem) implies |δf t (ξ)| a.s. --→ ∞. Corollary H.35. If a L+1 + b L+1 + r > 1 and 2a L+1 + c = 1, then for all ξ ∈ X , ft (ξ) a.s.]()

![E φ (Z l 0 )φ (Z l 0 ) E φ (Z l 0 )φ (Z l 0 )Then for all l ≥ ,λ l-1 (0) + γ l 22 ∂ 2 η γ l-1 (0),where C = 2(β l (0)) 2 E( Zl 0 ) 2 > 0.]()

![Consider a pseudostable parametrization with r = 0. Suppose a L+1 + b L+1 + r = 1or 2a L+1 + c = 1. If φ is tanh, then ∂ 3 η f1 (ξ 0 ) = 0.Proof. Similar to the proof of Theorem H.46, except in the expression∂ 3 η f1 (ξ 0 ) = -θ L+1 ∂ 2 η γ L (0) + θ Lf ∂ 2 η (γ L 11 γ L-1 )(0) , ∂ 2 η γ L (0) and ∂ 2 η (γ L 11 γ L-1 )(0) are both negative.The former is because of Theorem H.48. The latter is because ∂ 2 η γ L-1 (0) ≤ 0 for the same reason, and ∂ 2 η γ L 11 (0) < 0 since κl 3 , γ l 13 < 0, γ l 22 > 0 by Proposition H.47.H.7.4 Main ResultsProposition H.50. Suppose φ is tanh or σ-gelu for sufficiently small σ. A pseudostable parametrization with r = 0 is nontrivial iffa L+1 + b L+1 = 1 or 2a L+1 + c = 1. Proof. If a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1, then Theorem H.46 and Theorem H.49 show that the parametrization is nontrivial. Otherwise, it is trivial by Proposition H.25.Theorem H.51. Suppose φ is tanh or σ-gelu for sufficiently small σ. For any nontrivial pseudostable parametrization with r = 0, the following are true of the parametrization: 1. not in kernel regime 2. feature learning 3. feature learning in the Lth layer 4. feature kernels evolution 5. feature kernel evolution in the Lth layer 6. prefeature learning 7. prefeature learning in the Lth layer 8. prefeature kernels evolution 9. prefeature kernel evolution in the Lth layer 10]()

![Omniglot Meta-Test Accuracies after Pretraining with First Order MAML.]()

![Test Accuracies on Word Analogy after Pretraining with CBOW Word2Vec. number = log 2 width]()

![Best hyperparameters for the MAML experiment.Let f Q be the function with W given by Q. Then]()

Short for Model Agnostic Meta-Learning

Observe that by changing a l , b l while holding a l + b l fixed, we effectively give layer l its own learning rate.

One can further include a set of constants in front of n -a l and n -b l , for example powers of input dimension d, but we shall keep it simple here as we are only concerned with scaling behavior with n.

This is also known as the "fanin" or "Lecun" initialization; "Kaiming" initialization is the same up to multiplicative constants. The default in Tensorflow[[1]](#b0) uses Glorot initialization, where the variance of an entry scales like 1/(f anin + f anout). This causes the first layer preactivation to converge to 0 as n → ∞, and thus yields pathological behavior in the limit.

We stress this is in the n → ∞ limit, so does not contradict the feature learning seen in finite-width SP NN.

e.g. by extending the example programs of[[49,](#b48)[51]](#b50), which express only the first forward and backward passes, into the entire training computation.

In fact, empirically we observe such Gaussian random initialization to be crucial to performance compared to the mean-field-style initialization in this literature.

Actually, it is more similar to the Gaussian matrix in asymmetric message passing[[6]](#b5) in that care must be taken to keep track of correlation between W and W .

We won't expand further here, but it can be derived straightforwardly from the Master Theorem (Theorem 7.4).

Contrast this with a common semantics ofv = O(n a ) as v = O(n a ).

χ0 = L (f0, y0) = Θ(1) because f0 has variance Θ(1).

here the limit should be construed as almost sure limits; seeTheorem 7.4.   

but they may depend on training time and η; in particular, it's possible that they diverge with time.

In particular, it's possible for the function f to stay fixed with time, but for the features to change.

For simplicity, we only consider batch size 1; it's straightforward to generalize to larger batch sizes.

For the sake of streamlining the main text presentation, we defined feature learning and feature kernel evolution slightly differently than in Definition H.9, but ultimately they are equivalent as a result of our theorems.

It may seem that Neural Tangent Hierarchy[[25]](#b24), which allow some kind of higher order dynamics in the function space, violates our observation. But their infinite-width limit is identical to NTK in the constant time t = O(1) regime, which is what Remark 3.12 (and this paper) concerns. Moreover, here we are talking about functional dynamics that doesn't depend on n (because we are already at the n → ∞ limit) whereas their functional dynamics does.

linear and nonlinear; see Theorem H.17.

Recall that training routine means a package of learning rate ηn -c , training sequence {(ξt, yt)} t≥0 , and a loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ).

It is indeed possible to perform feature learning in a trivial parametrization, e.g. b l = 1/2 ∀l, a1 = -1/2, a2 = 100 + 1/2, c = -100 in a 2-hidden-layer MLP.

e.g. take aL+1 = 100 + 1/2, bL+1 = -100 + 1/2, then ∆W L+1 is negligible.

Again, more generally, we can insert constants in this parametrization, like U = √ n √ d u, but we omit them here for simplicity.

All convergence in this section will be almost sure, but to focus on the intuition here and less on the formalities, we do not explicitly write this down.

This roughly means that φ has a polynomially bounded weak derivative; see Definition F.3.

[[11,](#b10)[30,](#b29)[43,](#b42)[45]](#b44) present the equations in terms of the PDF of Z random variables. Formally, the PDF limit can be obtained by taking the continous-time limit of Eqs. (18) and (19) and then applying Focker-Planck. Note our derivation, when formalized using the Tensor Programs framework below, does not require smoothness and support assumptions on the initialization of U, V in those works: The initialization distribution here can be replaced with any image of Gaussians under pseudo-Lipschitz functions, which includes nonsmooth and singular distributions.

This roughly means that φ has a polynomially bounded weak derivative; see Definition F.3.

This roughly means that φ has a polynomially bounded weak derivative; see Definition F.3.

that may depend on various scalars such as χs, E Z xs Z x s (ξ) , and E Z d hs Z d hs

What we refer to as Tensor Program is the same as NETSOR + in Yang[[52]](#b51); we will not talk about other languages (like NETSOR ) so this should not cause any confusion

Here we write nV0 instead of V0 because we want all vectors to have Θ(1) coordinates; see Setup 7.2.

In Section 6 we assumed input dimension is 1. In general, each column of U0 would be a separate initial vector. Likewise, if the output dimension is greater than 1, then each row of V0 would be a separate initial vector.

Since {ξt, yt}t are fixed with n, we can WLOG absorb them into any nonlinearities in Nonlin that they are involved in, and set C = ∅. But, in kernel regime or nonmaximal feature learning parametrization, we usually have initial scalars, such as n -2a L+1 -c , that tend to 0 with n; see Appendix H.4.

This implies an explicit convergence in distribution (see[[52]](#b51)), but this convergence in distribution is strictly weaker than the formulation in Theorem 7.4, which is in general much more useful.

Because we will tune initialization variances, our results also represent finite-width SP networks.

Note that the transfer learning comment in Section 3.1 does not apply directly to the few-shot setting here, because the readout weights of the network carry over from the pretraining phase. Nevertheless, we will see a large performance gap between the kernel limits (2,3) and the µP limit.

One can write down gradient clipping easily in a Tensor Program, so the its infinite-width limit can be computed straightforwardly via Theorem 7.4; see Appendix D.

http://mattmahoney.net/dc/textdata.html

There is some nuance here because h(ξ) h( ξ) is actually Θ( √ n) instead of Θ(n) because ξ, ξ are one-hot, but the conclusion is the same; see Appendix D.2.

Here the randomness comes from initialization: the argmax is different for different random initializations, but it is fixed throughout training in the large width limit.

as is conventional in the machine learning literature, the convergence in distribution we mean here is really over finite dimensional marginals, i.e. (f0(ξ1), . . . , f0(ξ k )) d -→ ( f0(ξ1), . . . , f0(ξk)) where f0 is the limit GP.

For e.g. the NTK limit, f0 is a GP, so that we should expect the bounds on ∆h l t (ξ), ∆x l t (ξ) to depend on f0.

Here because we want to avoid topological issues arising for convergence in distribution of infinite sequences, we only require convergence in distribution jointly in all ξ ∈ X and time t below some cutoff T for every finite T .

Again, if aL+1 + bL+1 = 1/2, remember we are conditioning on f0(ξ), ξ ∈ X .

