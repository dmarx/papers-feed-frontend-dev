- **Neural Tangent Kernel (NTK)**: Describes the behavior of deep neural networks under gradient descent as width approaches infinity; however, does not support feature learning in standard and NTK parametrizations.

- **Feature Learning**: Essential for tasks like pretraining and transfer learning (e.g., BERT); standard and NTK parametrizations fail to learn features in the infinite-width limit.

- **Maximal Update Parametrization (µP)**: Proposed parametrization that allows maximal feature learning; defined by parameters \( a_1 = -1/2 \), \( a_{L+1} = 1/2 \), \( a_l = 0 \) for \( 2 \leq l \leq L \), \( b_l = 1/2 \) for all \( l \), and \( c = 0 \).

- **Dynamical Dichotomy Theorem**: States that any nontrivial stable abc-parametrization yields either feature learning limits or kernel limits, but not both.

- **Tensor Programs Technique**: A method to derive infinite-width limits of neural networks; involves expressing neural computations as Tensor Programs and applying the Master Theorem for coordinate distributions.

- **Empirical Results**: µP limits outperform NTK baselines and finite-width networks on tasks like Word2Vec and few-shot learning on Omniglot via MAML.

- **Parameter Initialization**: For abc-parametrizations, weights are initialized as \( w_l \sim N(0, n^{-2b_l}) \); crucial for ensuring stability and feature learning.

- **Learning Rate Dynamics**: The learning rate \( \eta n^{-c} \) must be carefully chosen; too small leads to trivial dynamics, while too large causes blowup.

- **Feature Learning Limit**: A nontrivial limit where the embedding \( x_L(\xi) \) evolves meaningfully; contrasted with kernel limit where dynamics are described by kernel gradient descent.

- **Comparison with Mean Field Limits**: The µP limit is equivalent to the mean field limit for 1-hidden-layer MLPs, differing primarily in initialization schemes.

- **Code Repository**: Implementation and experiments available at [github.com/edwardjhu/TP4](https://github.com/edwardjhu/TP4).

- **Key Figures**: Figure 1 illustrates PCA of Word2Vec embeddings, showing the separation of cities and states in embedding space as width increases in the feature learning regime.