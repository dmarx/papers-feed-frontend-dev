- Decision to focus on infinite-width neural networks
- Choice of standard and NTK parametrizations as baseline models
- Modification of standard parametrization for feature learning
- Adoption of Tensor Programs technique for deriving limits
- Selection of Word2Vec and Omniglot as experimental tasks
- Definition and introduction of abc-parametrizations
- Establishment of the Dynamical Dichotomy theorem
- Proposal of the Maximal Update Parametrization (µP)
- Verification of max learning rate predictions
- Use of empirical results to validate theoretical claims
- Comparison of µP limits with NTK and finite-width networks
- Decision to focus on multilayer perceptrons for pedagogical clarity
- Consideration of discrete-time vs continuous-time gradient descent
- Analysis of initialization schemes and their effects on learning
- Exploration of the implications of Gaussian random initialization
- Classification of neural network parametrizations based on feature learning capabilities
- Documentation of the architectural universality of the Tensor Programs technique
- Decision to limit the scope of experiments to specific tasks and architectures
- Consideration of numerical stability in parametrization choices
- Assessment of the impact of learning rate on training dynamics