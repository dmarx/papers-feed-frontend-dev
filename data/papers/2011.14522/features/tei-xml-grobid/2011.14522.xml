<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Learning in Infinite-Width Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-07-15">15 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
							<email>gregyang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research AI</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
							<email>edwardhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research AI</orgName>
								<orgName type="institution" key="instit2">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Learning in Infinite-Width Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-15">15 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">ADA99647BF40F9394A474E741BEBD5AF</idno>
					<idno type="arXiv">arXiv:2011.14522v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the Tensor Programs technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTK type state city</head><p>Width 64 Width (Feature Learning)</p><p>* Work done partly during the Microsoft AI Residency Program</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure" target="#fig_8">1</ref>: PCA of Word2Vec embeddings of top US cities and states, for NTK, width-64, and width-∞ feature learning networks (Definition 5.1). NTK embeddings are essentially random, while cities and states get naturally separated in embedding space as width increases in the feature learning regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The study of infinite-width limits of neural networks, in particular the Neural Tangent Kernel (NTK), has recently solved many longstanding open problems on the optimization and generalization of overparametrized neural networks [26]. However, in the NTK limit, (last layer) features learned during pretraining are essentially the same as those from random initialization (Corollary 3.9 and Theorem H.13); this is verified empirically in Word2Vec in Fig. <ref type="figure" target="#fig_8">1</ref>. As feature learning (e.g. Imagenet and BERT) lies at the core of deep learning's far-ranging impact so far [7, 13, 23], this insight amounts to a fatal weakness of the NTK theory as a model of neural networks in practice.</p><p>We seek to capture feature learning in overparametrized networks by considering other parametrizations and their infinite-width limits. By slightly modifying the standard parametrization (SP), in fact, we can enable feature learning that is maximal in a sense to be explained shortly. We describe how to compute this limit exactly (and rigorously) via the Tensor Programs technique developed in [49-52]. Feature Learning Infinite-Width Networks on Real Tasks We explicitly calculate this limit for the tasks of Word2Vec [32, 33] and few-shot learning on Omniglot via MAML [16], <ref type="bibr" target="#b1">2</ref> two standard tasks relying crucially on feature learning. In Word2Vec, an important early instance of large-scale language pretraining, we must learn, in an unsupervised manner, word embeddings so that similar words have close embeddings. Then we test the learned embeddings on the word analogy task, which asks questions of the kind "what to a queen is as a man to a woman?" In few-shot learning, the model is asked to make predictions given only a handful (e.g. 5) of labeled examples. Metalearning/MAML makes this possible by having the model learn good representations of typical examples that can adapt quickly, via a small number of SGD steps, to new few-shot learning tasks. On both tasks, we find our feature learning infinite-width networks outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width performance as width increases.</p><p>Figure <ref type="figure">right</ref> shows this for one of our Word2Vec results. See Section 9 for our other experiments.</p><p>abc-Parametrizations This paper studies a natural class of parametrizations, which we call the abc-Parametrization and describe here. Consider an L-hidden-layer perceptron: For weight matrices W 1 ∈ R n×d and W<ref type="foot" target="#foot_0">foot_0</ref> , . . . , W L ∈ R n×n , and nonlinearity φ : R → R, such a neural network on input ξ ∈ R d is given by h 1 (ξ) = W 1 ξ ∈ R n , and</p><formula xml:id="formula_0">x l (ξ) = φ(h l (ξ)) ∈ R n , h l+1 (ξ) = W l+1 x l (ξ) ∈ R n , for l = 1, . . . , L -1,<label>(1)</label></formula><p>and the network output (also called the logit(s)) is f (ξ) = W L+1 x L (ξ) for W L+1 ∈ R 1×n . An abc-parametrization is specified by a set of numbers {a l , b l } l ∪ {c} such that (a) We parametrize each weight as W l = n -a l w l for actual trainable parameter w l (b) We initialize each w l αβ ∼ N (0, n -2b l ), and (c) The SGD learning rate is ηn -c for some width-independent η. <ref type="bibr">3 4</ref> Examples: The NTK parametrization (NTP) [26] has a 1 = 0 and a l = 1/2 for l ≥ 2; b l = 0 for all l; c = 0. When depth L = 1, the Mean Field parametrization (MFP) [11, 30, 43, 45] has a 1 = 0, a 2 = 1; b l = 0 for all l; c = -1. The standard parametrization (SP) available as the default setting in PyTorch [39]  <ref type="bibr" target="#b4">5</ref> has a l = 0 for all l; b 1 = 0 and b l = 1/2 for l ≥ 2; c = 0. However, we shall see that c is too small (learning rate too large) in SP. We can define abc-parametrization and generalize our results to arbitrary neural architectures (Appendix C), but we shall focus on MLPs in the main text.</p><p>Dynamical Dichotomy For any abc-parametrization, if c is too small (i.e. learning rate too large), SGD can lead to blowup of preactivation and/or logits; we say this parametrization is unstable. In practice this translates to numerical issues. If c is too large (i.e. learning rate too small), then the function computed by the network does not change in finite time; we say this parametrization is trivial. We prove what we call the Dynamical Dichotomy theorem (Corollary 3.9):</p><p>Any nontrivial stable abc-parametrization yields a (discrete-time) infinite-width limit. This limit either 1) allows the embedding x L (ξ) to evolve nontrivially (Definition 3.5) or 2) is described by kernel gradient descent in function space (Definition 3.7), but not both.</p><p>We call the former kind a feature learning limit and the latter a kernel limit. For 1-hidden-layer MLPs, the former is exemplified by MFP, and the latter, NTP. This dichotomy implies that certain functional dynamics, such as higher order generalizations of the NTK dynamics, are not valid infinite-width limits (see Remark 3.12). In addition, the neural network function f (defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>)) in any feature learning limit must be identically 0 at initialization (see Corollary 3.10). <ref type="bibr" target="#b5">6</ref>  Verifying Max Learning Rate for P and SP Standard Param. Does Not Learn Features We show that the SP (resp. NTP) can only allow O(1/width) (resp. O(1)) learning rate (i.e. c = 1, resp. c = 0), so as to avoid blowup, and yield kernel limits (Section 4). Instead, we propose a parametrization that has Θ(1) max learning rate and admits feature learning maximally: it allows every parameter to be updated maximally (in terms of scaling with width) without leading to blowup (Section 5). We thus call it the Maximal Update Parametrization (abbreviated MUP or µP). It is given by a 1 = -1/2, a L+1 = 1/2, and a l = 0 for all 2 ≤ l ≤ L; b l = 1/2 for all l; c = 0. In a 1-hidden-layer MLP, this specializes to MFP, up to symmetry (see Eq. ( <ref type="formula" target="#formula_8">5</ref>)). The "feature learning limits" mentioned above in our main experiments are µP limits. Figure to the right: We empirically verify our max learning rate predictions on relu MLP with 2 hidden layers, trained with square loss on CIFAR10. We plot learning rate vs accuracy in each subplot. Each curve represents MLP with a specific width. The right edge of each curve indicates the max learning rate. The diagonal subplots scale the x-axes (log learning rate) in the correct width-scaling for the corresponding parametrizations. We see, indeed, max learning rate for SP scales like 1/width but is constant in µP. When width is large, every activation vector has roughly iid coordinates, at any time during training. Using Tensor Programs, we can recursively calculate such coordinate distributions, and consequently understand how the neural network function evolves.</p><p>The Tensor Programs technique was developed in a series of papers [49-52] that proved the architectural universality of the Neural Network-Gaussian Process (NNGP) Correspondence and the Neural Tangent Kernel (NTK) limits and showed how to compute the corresponding infinite-width kernels. In the Figure above, the NNGP kernel can be thought of as the "limit" of the first forward pass of a randomly initialized model; the NTK can be similarly thought of as the "limit" of its first backward pass. The mechanics of calculating such limits is 1) to write down the relevant neural network computation (e.g. the first forward pass in the NNGP case) as a principled composition of matrix multiplication and coordinatewise nonlinearities, called a Tensor Program, and 2) to recursively calculate the distribution of coordinates of each vector via what's called the Master Theorem. In this paper, we follow the exact same recipe, where in 1) we just write down the entire SGD training instead of only the first step. More generally,</p><p>To derive the infinite-width limit of any neural computation (e.g. SGD training), 1) express it as a Tensor Program, and 2) mechanically apply the Master Theorem.</p><p>For example, we easily recover the (discrete-time) 1-hidden-layer mean field limit (Theorem 6.1). It readily applies to practically any neural architecture (e.g. ResNet and Transformers) <ref type="bibr" target="#b6">7</ref> as well as many common variants of SGD; however, in this paper, for pedagogical clarity, we only focus on multilayer perceptrons. The generality of our approach allows us to easily adapt to settings outside the traditional (CIFAR10-style) supervised classification, such as the Word2Vec and few-shot learning tasks in this paper, or reinforcement learning and image generation outside of our scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions</head><p>1. Formulate a natural space of NN parametrizations (abc-parametrizations).</p><p>2. Prove Dynamical Dichotomy: Any nontrivial stable abc-parametrization yields either feature learning or kernel limits, but not both.</p><p>3. Show both NTK and standard parametrizations yield kernel limits and propose the Maximal Update Parametrization (µP) , which admits maximal feature learning in a suitable sense.</p><p>4. Use Tensor Programs to derive the infinite-width limit of µP and, more generally, the limit of any abc-parametrization. We verify our theory using extensive experiments.</p><p>5. Show the µP limit outperforms both NNGP/NTK baselines and finite networks on 1) Word2Vec and 2) Omniglot few-shot learning, trained via first-order MAML.</p><p>Tensor Programs Series While this work is self-contained, it is positioned as the 4th paper in the series, following Yang [49, 51, 52]. We do not extend the Tensor Programs machinery further here, but instead extract the first major payoff of the foundation laid in the earlier works. In fact, this paper is the original motivation for this series; for a short history, see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Comparison with Mean Field Limits For 1-hidden-layer MLP, the mean field limit [11, 30, 43,  45] is equivalent to the µP limit modulo the symmetry of Eq. ( <ref type="formula" target="#formula_8">5</ref>) (see Section 3.1). Several works also proposed different versions of mean field frameworks for deeper MLPs [5, 15, 34, 35, 46]. However, they did not consider the typical Gaussian N (0, 1/n) random initialization (or the appropriately rescaled version in their respective parametrizations) <ref type="bibr" target="#b7">8</ref> , which has a Central-Limit effect as opposed to a Law-of-Large-Numbers effect. For example, [5, 35] can cover the case of N (0, 1/n 2 ), instead of N (0, 1/n), initialization, which in fact causes the function to be stuck at initialization. See Appendix E for more explanations. Of these works, the mean field limit of [15] has the form most similar to what we derive here. There, as we do here, the coordinate distribution of each (pre)activation vector is tracked recursively. The main difference is, while [15] has an atypical initialization involving 2 regression, we consider the usual Gaussian N (0, 1/n) scheme. Such a (size n×n) Gaussian matrix in the middle of the network has a distinctly different effect, more similar to that of a Gaussian matrix in the usual NNGP/NTK calculation, <ref type="bibr" target="#b8">9</ref> than the "mean field" matrices considered in [15] and previous works [5, 34, 35, 46], which has an "integral kernel" effect that is the straightforward generalization of matrices to function spaces. Nevertheless, discrete time versions of the 1-hidden-layer mean field limit and of many of the multilayer limits (such as [15, 35]) can be derived directly by writing the corresponding initialization and training inside a Tensor Program and applying the Master Theorem (Theorem 7.4).</p><p>Discrete-vs Continuous-Time Gradient Descent At a high level, there are two natural limits of neural networks training dynamics: large-width and continuous-time. Most prior works on infinite-width limits of neural networks also took the continuous-time limit simultaneously, e.g. [11, 26, 30, 43, 45]. In contrast, here we only take the large width limit, so that gradient descent stays discrete-time. Then the results of these prior works can be recovered by taking another continuoustime limit. From a practical perspective, the continuous-time limit is often unnatural, e.g. 1) because the step size is usually as large as possible to speed up training, 2) because of the task (such as reinforcement learning), or 3) because of the importance of hyperparameters like batch size that are hidden away in such limits. On the theory side, taking the continuous-time limit can create issues with 1) well-posedness and 2) existence and uniqueness of the resulting ODE/PDE. While they can sometimes be proved to hold, they are artifacts of the continuous-time limit, as the corresponding questions for the discrete time evolution are trivial, and thus not relevant to the behavior of real networks.</p><p>Technical Assumptions Earlier works on neural tangent or mean field limits (e.g. [11, 15, 26,  30, 35, 43, 45]) assume various forms of regularity conditions, such as 1) 0th, 1st, and/or 2nd order smoothness on the nonlinearity or other related functions, and 2) the support boundedness, subgaussianity, and/or PDF smoothness of initialization distributions. These are often either unnatural or difficult to check. In our work, the only assumption needed to rigorously obtain the infinite-width limit is that the nonlinearity φ has a polynomially bounded weak 2nd derivative and that the loss function has a continuous derivative w.r.t. the prediction (Assumption H.22). In particular, when we specialize to the 1-hidden-layer case and derive the discrete time version of the mean field limit, we cover the standard Gaussian initialization; in fact, we can allow any heavy-tailed initialization that can be written as the image of a Gaussian under a pseudo-Lipschitz function, which include nonsmooth PDFs and singular distributions. <ref type="bibr" target="#b9">10</ref> This generosity of technical assumptions is due to that of the Tensor Programs Master Theorems proven in [49, 51, 52].</p><p>Training Time Many prior works (e.g. [4, 25, 30]) derived explicit time dependence of the convergence to infinite-width limit, so that a larger width can allow the network to stay close to the limit for longer. In this paper, our results only concern training time independent of width, since our primary objective is to investigate the limit itself and its feature learning capabilities. Moreover, recent evidence suggests that, given a fixed computational budget, it's always better to train a larger model for a shorter amount of time [29], which validates the practical relevance of our limit mode. Nevertheless, it is possible to prove a quantitative version of the Tensor Programs Master Theorem, by which one can straightforwardly allow training time to increase with width.</p><p>Classification of Parametrizations [10] pointed out that the weights move very little in the NTK limit, so that linearization approximately holds around the initial parameters, in contrast to the mean field limit (for 1-hidden-layer networks) where the weights move substantially. For this reason, they called the former "lazy training" and the latter "active training," which are classified nonrigorously by a multiplicative scaling factor of the logit (similar to n -a L+1 in this paper). While these terms are not formally defined, they intuitively correspond to the kernel and feature learning regimes in our paper. From a different perspective, [31] observed that the NTK and mean field limit can be thought of as short and long time-scale regimes of the mean field evolution equations. Neither of the above works attempted to formally classify natural parametrizations of neural networks. In contrast, [48] studied a toy class of neural networks in the context of implicit regularization due to the scale α of initialization (which is closely related to logit multiplier of [10] noted above). They identified the α → ∞ limit (of the scale α, not of width) with the "kernel regime" and the α → 0 limit with what they call the "rich regime". They showed that the former is implicitly minimizing an 2 risk while the latter, an 1 risk. They claim width allows the toy model to enter the kernel regime more naturally, but as we see in this work, both kernel and feature learning regimes are admissible in the large width limit of a standard MLP. Closer to our approach, [19] studied what amounts to a 2-dimensional subspace of the space of stable abc-parametrizations for L = 1. They proposed a notion of stability which is similar to the combination of stability and nontriviality in this paper. They characterized when the Neural Tangent Kernel, suitably generalized to any parametrization and playing a role similar to the feature kernel in this paper, evolves over time. However, to simplify the proofs, they assumed that the gradients for the different weight matrices are estimated using different inputs, a very unnatural condition. In contrast, here our results are for the usual SGD algorithm applied to MLPs of arbitrary depth. In all of the above works and most of existing literature, not much attention is paid to the feature learning capabilities of neural networks in the right parametrization, as opposed to our focus here. A notable exception is [12], which showed that the mean field limit, but not the NTK limit, can learn low dimension linear structure of the input distribution resulting in ambient-dimension-independent generalization bounds.</p><p>Other Related Works [27] proposed a toy model to study how large learning rate can induce a neural network to move out of the kernel regime in Ω(log(width)) time. Since our dichotomy result only concerns training for O(1) time (which, as we argue above, is more practically relevant), there is no contradiction. [47] also noted that standard parametrization leads to unstable training dynamics. They then injected constants in the NTK parametrization, such as α/ √ n instead of 1/ √ n and tuned α in the resulting kernel. [2, 3] also observed the lack of feature learning in NNGP and NTK limits but, in contrast to taking the exact limit of SGD training as we do here, they proposed a deep kernel process as a way of loosely mimicking feature learning in finite-width networks. [17]  empirically observed that wider networks achieve better downstream performance with linear transfer learning, even though on the original pretraining task there can be little difference. We fix the input dimension d in this work, but one can also consider varying d with width n, e.g. [36, 38]. [28] proved a complexity separation between NTK and finite-width networks by showing the latter approximates a sort of infinite-width feature learning network. In the literature surrounding NTK, often there are subtle differences in parametrization leading to subtle differences in conclusion (e.g. [4, 14, 57]). Our abc framework encapsulates all such parametrizations, and can easily tell when two ostensibly different parametrizations (e.g. [14, 57]) are actually equivalent or when they are really different (e.g. [4, 14]) via Eq. ( <ref type="formula" target="#formula_8">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Learning vs Kernel Behavior</head><p>In this section, we give a characterization of training procedures that induce feature learning vs kernel behavior; we will elaborate on what we mean by these two kinds of behavior below. We first motivate this discussion by reviewing the well-known tangent kernel and mean field limits of a shallow neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivating Examples: Neural Tangent Kernel and Mean Field Limits</head><p>For simplicity, define a shallow network f (ξ) with input/output dimension 1 by</p><formula xml:id="formula_1">f (ξ) = V x(ξ) ∈ R, x(ξ) = φ(h(ξ)) ∈ R n , h(ξ) = U ξ ∈ R n . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>As a specialization of Eq. ( <ref type="formula" target="#formula_0">1</ref>), we parametrize weights</p><formula xml:id="formula_3">V = n -av v ∈ R 1×n and U = n -au u ∈ R n×1 ,</formula><p>where the width n should be thought of as tending to ∞, and v, u should be thought of as the actual trainable parameters. We will sample</p><formula xml:id="formula_4">v α ∼ N (0, n -2bv ), u α ∼ N (0, n -2bu ) for α ∈ [n].</formula><p>The learning rate is ηn -c for some η independent of n.</p><p>For example, in the Neural Tangent Parametrization (abbreviated NTP) [26],</p><formula xml:id="formula_5">a u = b v = b u = 0, a v = 1/2, c = 0. The Mean Field Parametrization (abbreviated MFP) corresponds to a v = 1, a u = b u = b v = 0, c = -1;</formula><p>however, as will be explained shortly, we will use the equivalent formulation</p><formula xml:id="formula_6">a u = -1/2, a v = b u = b v = 1/2,</formula><p>c = 0 in this section so c = 0 for both NTP and MFP. We remark that the GP limit, i.e. training only the last layer of a infinite-wide, randomly initialized network, is a special case of the NTK limit where the first layer is not trained. Everything we discuss below about the NTK limit specializes to the GP limit appropriately.</p><p>Given an input ξ, the gradient of f can be calculated as</p><formula xml:id="formula_7">dx(ξ) = V, dh(ξ) = dx(ξ) φ (h(ξ)), dv(ξ) = n -av x(ξ), du(ξ) = n -au dh(ξ)ξ</formula><p>where d • (ξ) is shorthand for ∇ • f (ξ) (however, note that later in Section 6, d • (ξ) will stand for n∇ • f (ξ)). For loss function L : R × R → R, the loss gradient on a pair (ξ, y) is then given by L (f (ξ), y)[dv(ξ), du(ξ)] (where L denotes derivative in first argument).</p><p>Note that one can keep the function f invariant while changing the magnitude of the gradient dv by changing a v , b v , holding a v + b v constant; likewise for du. Thus, the trajectory of f stays fixed if, for any θ ∈ R, we set )). With θ = -1/2, this explains why the two formulations of MFP above are equivalent. Then, for both NTP and MFP, we will consider the dynamics of f trained under stochastic gradient descent with learning rate η = 1 and batch size 1, where the network is fed the pair (ξ t , y t ) at time t, starting with t = 0. This simplicity is intended to intuitively illustrate our points below, but we shall state formal results regarding more common settings in Section 3.2.</p><formula xml:id="formula_8">a u ← a u + θ, a v ← a v + θ, b u ← b u -θ, b v ← b v -θ, c ← c -2θ (also see Eq. (<label>5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation and Setup</head><p>Below, when we say a (random <ref type="bibr" target="#b10">11</ref> we mean v 2 /n = O(n a ) with high probability for large n. Intuitively, this means that each coordinate has a typical fluctuation of O(n a ). Likewise if O(n a ) is replaced with Θ(n a ) or Ω(n a ). See Definition H.2 for a formal definition. Let f t , h t , x t , U t , V t , dx t , dh t , dv t , du t denote the corresponding objects at time t, with t = 0 corresponding to random initialization. We also abuse notation and write x t = x t (ξ t ), i.e. applying the function x t specifically to tth input ξ t ; similarly for f t , h t , dx t , dh t , dv t , du t . These symbols will never appear by themselves to denote the corresponding function, so this should cause no confusion. Then SGD effectively updates U and V by</p><formula xml:id="formula_9">) vector v ∈ R n has coordinate size O(n a ) (written v = O(n a )),</formula><formula xml:id="formula_10">U t+1 = U t -χ t n -au du t , V t+1 = V t -χ t n -av dv t . where χ t def = L (f t , y t ). Finally, let ∆• t def = • t -• 0 ,</formula><p>for all • ∈ {f, h, x, U, V, dx, dh, dv, du}. For example, after 1 SGD update, we have, for any ξ ∈ R,</p><formula xml:id="formula_11">∆h 1 (ξ) = h 1 (ξ) -h 0 (ξ) = -n -au χ 0 ξdu 0 = -n -2au χ 0 ξ 0 ξdh 0 = -n -2au χ 0 ξ 0 ξdx 0 φ (h 0 ) (3) ∆f 1 (ξ) = V 0 ∆x 1 (ξ) + ∆V 1 x 1 (ξ) = V 0 ∆x 1 (ξ) -n -av dv 0 x 1 (ξ) = V 0 ∆x 1 (ξ) -n -2av x 0 x 1 (ξ)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Key Observations</head><p>Let's list a few characteristics of the NTK and MF limits in the context of the shallow network in Eq. ( <ref type="formula" target="#formula_1">2</ref>), and then discuss them in the general setting of deep MLP. We will keep our discussion intuitive to carry across the key ideas.</p><p>Feature Evolution For a generic ξ ∈ R, its embedding vector x 0 (ξ) has coordinates of Θ(1) size in both NTP and MFP. However, for any t ≥ 1 independent of n, ∆x t (ξ) generically has coordinate size</p><formula xml:id="formula_12">Θ(1/ √ n) in NTP but Θ(1) in MFP.</formula><p>Example for t = 1: By Eq. ( <ref type="formula">3</ref>), we have</p><formula xml:id="formula_13">∆h 1 (ξ) = n -2au χ 0 ξ 0 ξdx 0 φ (h 0 ).</formula><p>Plug in a u = 0 for NTP. Observe that ξ 0 , ξ, χ 0 = Θ(1), <ref type="bibr" target="#b11">12</ref> so</p><formula xml:id="formula_14">∆h 1 (ξ) = Θ(1) • dx 0 φ (h 0 ). (in NTP) In addition, φ (h 0 ) = Θ(1) because h 0 = Θ(1), so ∆h 1 (ξ) = Θ(1) • dx 0 Θ(1). (in NTP) Finally, dx 0 = V 0 = Θ(1/ √ n) in NTP. Altogether, this implies ∆h 1 (ξ) = Θ(1/ √ n) =⇒ ∆x 1 (ξ) ≈ φ (h 0 (ξ)) ∆h 1 (ξ) = Θ(1/ √ n) → 0, as n → ∞. (in NTP)</formula><p>On the other hand, in MFP, the only thing different is a u = -1/2 and dx 0 = Θ(1/n), which implies</p><formula xml:id="formula_15">∆h 1 (ξ) = Θ(n) • Θ(1/n) Θ(1) = Θ(1) =⇒ ∆x 1 (ξ) = Θ(1). (in MFP)</formula><p>Feature Kernel Evolution Therefore the feature kernel F t (ξ, ζ) def = x t (ξ) x t (ζ)/n does not change in the NTK limit but it does in the MF limit, i.e. for any fixed t ≥ 1, <ref type="bibr" target="#b12">13</ref> lim</p><formula xml:id="formula_16">n→∞ F t (ξ, ζ) = lim n→∞ F 0 (ξ, ζ), in NTP, but lim n→∞ F t (ξ, ζ) = lim n→∞ F 0 (ξ, ζ), in MFP, in general.</formula><p>Indeed, regardless of parametrization, we have</p><formula xml:id="formula_17">F t (ξ, ζ) = 1 n x 0 (ξ) x 0 (ζ) + ∆x t (ξ) x 0 (ζ) + x 0 (ξ) ∆x t (ζ) + ∆x t (ξ) ∆x t (ζ) .</formula><p>In NTP, because ∆x t (ξ) = Θ(1/ √ n) as noted above,</p><formula xml:id="formula_18">1 n ∆x t (ξ) x 0 (ζ) = 1 n n α=1 ∆x t (ξ) α x 0 (ζ) α = 1 n n α=1 O(n -1/2 ) = O(n -1/2 ),</formula><p>and likewise the other terms involving ∆x t will vanish as n → ∞. But in MFP, ∆x t (ξ) = Θ(1) will in general be correlated with</p><formula xml:id="formula_19">x 0 (ζ) such that 1 n ∆x t (ξ) x 0 (ζ) = 1 n n α=1 Θ(1) = Θ(1)</formula><p>. It may seem somewhat puzzling how the NTK limit induces change in f without feature or feature kernel evolution. We give some intuition in Appendix B.</p><p>Pretraining and Transfer Learning The simple fact above about the feature kernel K implies that the NTK limit is unable to perform linear transfer learning. By linear transfer learning, we mean the popular style of transfer learning where one discards the pretrained linear classifier layer and train a new one on top of the features (e.g. x in our example), which are fixed. Indeed, this is a linear problem and thus only depends on the kernel of the features. If this kernel is the same as the kernel at initialization, then the pretraining phase has had no effect on the outcome of this "transfer" learning.</p><p>In fact, a more sophisticated reasoning shows pretraining in the NTK limit is no better than random initialization for transfer learning even if finetuning is performed to the whole network, not just the classifier layer. This remains true if we replace the linear classifier layer by a new deep neural network. See Remark H.16 and Theorem H.17. The Word2Vec experiment we do in this paper is a linear transfer task.</p><p>In some other settings, such as some settings of metalearning, like the few-shot learning task in this paper, the last layer of the pretrained network is not discarded. This is called adaptation. Then the NTK limit does not automatically trivialize transfer learning. However, as will be seen in our experiments, the NTK limit still vastly underperforms the feature learning limit, which is exemplified by the MF limit here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Gradient Descent in Function Space</head><formula xml:id="formula_20">In NTP, as n → ∞, ∇ U,V f 0 (ξ), ∇ U,V f 0 (ζ)</formula><p>converges to some deterministic value K(ξ, ζ) such that K forms a kernel (the NTK). Then, in this limit, if the learning rate is η, the function f evolves according to kernel gradient descent f t+1 (ξ) = f t (ξ) -ηK(ξ, ξ t )χ t . However, this shouldn't be the case for the MF limit. For example, if φ is identity, then intuitively f t+1 (ξ) -f t (ξ) should be quadratic in η, not linear, because two layers are updated at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">abc-Parametrizations and Dynamical Dichotomy</head><p>In this section, we broaden our scope to the abc-parametrizations of deeper MLPs, defined by Eq. ( <ref type="formula" target="#formula_0">1</ref>), and their infinite-width limits. In Table <ref type="table">1</ref>, we summarize the {a l , b l } l ∪ {c} values of various abc-parametrizations in the literature. Assumption 3.1. Our main results in this section (and this section only) will assume φ is either tanh or a smooth version of relu called σ-gelu (see Definition H.1), for sufficiently small σ &gt; 0 (which means σ-gelu approximates relu arbitrarily well).</p><p>Note this assumption is only needed for the classification of abc-parametrizations. For deriving the infinite-width limits, the much weaker Assumption H.22 suffices. We believe our results here will hold for generic nonlinearities, but making this precise is outside our scope. (See Remark H.15 for an overview on how Assumption 3.1 is used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetries of abc-Parametrizations</head><p>As above, we can scale the parameter gradients ∇ w l f arbitrarily while keeping f fixed, if we vary a l , b l while fixing a l + b l : ∇ w l f is scaled by n -θ if a l ← a l + θ, b l ← b l -θ. In other words, changing a l , b l this way effectively gives w l a per-layer learning rate. If we apply this gradient with learning rate ηn -c , then the change in W l is scaled by ηn -c-2θ . Consequently, if c ← c -2θ, then W l is not affected by the change in a l , b l . In summary, ∀θ ∈ R : f t (ξ) stays fixed for all t and ξ if we set</p><formula xml:id="formula_21">a l ← a l + θ, b l ← b l -θ, c ← c -2θ. (5)</formula><p>Stable abc-Parametrizations We will only consider abc-parametrizations such that, as n → ∞, 1) the preactivations {h l } l and activations {x l } l have Θ(1) coordinates at initialization, and 2) their coordinates and the logit f (ξ) all stay O(1) throughout the course of SGD. <ref type="bibr" target="#b13">14</ref> Otherwise, they tend to ∞ with n, eventually going out of floating point range. Indeed, this is an acute and real problem common in modern deep learning, where float16 is necessary to train large models. We call any such parametrization stable (see Definition H.4 for a formal definition). Thus unstable parametrizations are of no practical interest.</p><p>It turns out stable abc-parametrizations can be characterized by a set of inequalities on {a l , b l } l ∪ {c} (so that the stable ones form a polyhedron). To present these inequalities succinctly, it's useful to define Table <ref type="table">1</ref>: We summarize the abc values of SP (standard), NTP (Neural Tangent), MFP (Mean Field, for 1-hidden-layer nets), µP (Maximal Update, ours). We show the minimal value of c such that the parametrization is stable (Definition H.4). We also list the quantities r, 2a L+1 + c, a L+1 + b L+1 + r involved in stability, feature learning, and kernel regime properties of the parametrizations. Here we only focus on scaling with n and ignore dependence on input dimension. Recall the MLP definition:</p><formula xml:id="formula_22">h 1 = W 1 ξ ∈ R n , x l = φ(h l ) ∈ R n , h l+1 = W l+1 x l ∈ R n , f (ξ) = W L+1 x L Definition SP (w/ LR 1 n ) NTP MFP (L = 1) µP (ours) a l W l = n -a l w l 0 0 l = 1 1 /2 l ≥ 2 0 l = 1 1 l = 2    -1 /2 l = 1 0 2 ≤ l ≤ L 1 /2 l = L + 1 b l w l αβ ∼ N (0, n -2b l ) 0 l = 1 1 /2 l ≥ 2 0 0 1 /2 c LR = ηn -c 1 0 -1 0 r Definition 3.2 1 /2 1 /2 0 0 2a L+1 + c 1 1 1 1 a L+1 + b L+1 + r 1 1 1 1 Nontrivial? Stable?</formula><p>Feature Learning? Kernel Regime? Definition 3.2. For any abc-parametrization, we write r for the quantity</p><formula xml:id="formula_23">r def = min(a L+1 + b L+1 , 2a L+1 + c) + c -1 + L min l=1 [2a l + I(l = 1)] .</formula><p>For example, in NTP, r = 1/2, while in MFP (when L = 1), r = 0. Intuitively, r is the exponent such that ∆x L t (ξ) = Θ(n -r ). Thus, to avoid activation blowup, we want r ≥ 0; to perform feature learning, we want r = 0. Theorem 3.3 (Stability Characterization, c.f. Theorem H.6). An abc-parametrization is stable iff all of the following are true (with intuitions in parentheses):</p><p>1. ((pre)activations x l 0 , h l 0 at initialization are Θ(1) and logits f 0 are O(1))</p><formula xml:id="formula_24">a 1 + b 1 = 0; a l + b l = 1/2, ∀l ∈ [2, L]; a L+1 + b L+1 ≥ 1/2.<label>(6)</label></formula><p>2. (features don't blowup, i.e. ∆x l t = O(1) for all l) r ≥ 0.</p><p>3. (logits don't blow up during training, i.e.</p><formula xml:id="formula_26">∆W L+1 t x L t , W L+1 0 ∆x L t = O(1)) 2a L+1 + c ≥ 1; a L+1 + b L+1 + r ≥ 1.<label>(8)</label></formula><p>Nontrivial abc-Parametrizations Among stable abc-parametrizations, there are also those where f does not change throughout training in the infinite-width limit. We say such parametrizations are trivial. Our dichotomy result will only apply to nontrivial stable abc-parametrizations. <ref type="bibr" target="#b14">15</ref> Nontrivial abc-parametrizations can also be described by a disjunction of equations on {a l , b l } l ∪ {c} (geometrically, they correspond to the union of two faces on the polyhedron of stable abcparametrizations).</p><p>Theorem 3.4. A stable abc-parametrization is nontrivial iff</p><formula xml:id="formula_27">a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1.</formula><p>Feature Learning Below, for brevity, we say training routine to mean the package of learning rate ηn -c , training sequence {(ξ t , y t )} t≥0 , <ref type="bibr" target="#b15">16</ref> and a loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ). As above, we use • t to denote the object • after t steps of SGD. Definition 3.5 (c.f. Definitions H.9 and H.11). We say an abc-parametrization admits feature learning (resp. evolves the feature kernel) if, as n → ∞, ∆x L t (ξ) has Ω(1) coordinates (resp.</p><formula xml:id="formula_28">1 n (x L t (ξ) x L t (ζ) -x L 0 (ξ) x L 0 (ζ)) = Ω(1)</formula><p>) for some training routine, time t ≥ 1, and input ξ (resp. ξ, ζ). <ref type="bibr" target="#b16">17</ref> MFP, in the 1-hidden-layer case, is an example of feature learning parametrization.</p><p>Intuitively, feature kernel evolution implies feature learning, but a priori it seems possible that the latter can occur without the former (akin to some kind of rotation of features). If so, then, e.g. in terms of linear transfer learning, the pretraining ultimately had no benefit. But, in fact, Theorem 3.6. A nontrivial stable abc-parametrization admits feature learning iff it evolves the feature kernel iff r = 0.</p><p>Kernel Regime While feature learning here is defined by looking at the embedding of an input ξ, we can also look at the dynamics of the function represented by the neural network. Definition 3.7 (c.f. Definition H.12). We say an abc-parametrization is in kernel regime if there exists a positive semidefinite kernel K such that, for any training routine, time t ≥ 0, and input ξ, in the n → ∞ limit,</p><formula xml:id="formula_29">f t+1 (ξ) = f t (ξ) -ηK(ξ, ξ t )L (f t (ξ t ), y t ), ∀t ≥ 0.<label>(9)</label></formula><p>In other words, SGD reduces to kernel gradient descent in the large n limit. Theorem 3.8. A nontrivial stable abc-parametrization is in kernel regime iff r &gt; 0.</p><p>NTP is a typical example of this, where r = 1/2 and K is given by the NTK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Regime</head><p>Neural Tangent</p><formula xml:id="formula_30">Standard 𝐿𝑅 = Θ(1/𝑤𝑖𝑑𝑡ℎ) Maximal Update (ours)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unstable</head><p>or Trivial</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Space of abc-Parametrizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Field when depth=1</head><p>Standard 𝐿𝑅 = Θ(1) Figure <ref type="figure">2</ref>: A Caricature of abc-Parametrizations. The nontrivial stable parametrizations form a high dimensional polyhedron. Those on a part of its boundary admit feature learning, while all others are in kernel regime. µP is a vertex in the former, while NTP, latter. See Fig. <ref type="figure" target="#fig_11">5</ref> for a more geometrically accurate depiction.</p><p>Dynamical Dichotomy Since a stable abc-parametrization has either r = 0 or r &gt; 0 by Eq. ( <ref type="formula" target="#formula_25">7</ref>): Corollary 3.9. A nontrivial stable abc-parametrization either admits feature learning or is in kernel regime, but not both.</p><p>Note that kernel regime (Definition 3.7) is not defined as lack of feature learning, so Corollary 3.9 is not a trivial statement. In addition, Assumption 3.1 is necessary. For example, if φ is linear, then this dichotomy doesn't hold, as a 1-hidden-layer linear network where only the first layer is trained would both admit feature learning and is in kernel regime.</p><p>An interesting consequence of Dynamical Dichotomy is Corollary 3.10. Any nontrivial stable feature learning abcparametrization must have lim n→∞ f 0 (ξ) = 0 for all ξ, where the limit is almost sure. Theorems 3.6 and 3.8 and Corollary 3.10 are consequences of the more general classification theorem Theorem H.13, which in addition shows: 1) feature learning in layer l would imply the same for layers l, . . . , L; 2) in any feature learning parametrization, f t in the large n limit becomes deterministic, and thus is incompatible with any Bayesian perspective (in contrast to the NNGP limit).</p><p>Dynamical Dichotomy in the shallow perceptron case is illustrated by the NTK and MF limits, as presented in Section 3.1, which shows the NTK limit exemplifies Theorem 3.8 while the MF limit exemplifies Theorem 3.6. We present a simplified picture of abc-parametrizations in Fig. <ref type="figure">2</ref>, but see Fig. <ref type="figure" target="#fig_11">5</ref> for a more geometrically accurate depiction.</p><p>The paragraph above Appendix H.2 gives a quick outline of the proof of Dynamical Dichotomy, and the beginning of each succeeding section outlines the logic of that section. Remark 3.11 (Function Space Picture). A kernel regime limit resides solely in the function space picture, i.e. the evolution of f at any time being solely determined by the function values {lim f t (ζ)} ζ themselves (as opposed to the internal activations of f as well) along with η, L, and (ξ t , y t ). Intuitively, this cannot be true for the feature learning limit, and therefore, at least informally, Dynamical Dichotomy is also a dichotomy over the sufficiency of the function space picture for determining the training evolution: We can construct two settings where {lim f t (ζ)} ζ , η, L, and (ξ t , y t ) are the same but f t+1 are different. 1) The first setting is at t = 0, where lim f t (ζ) = 0 for all input ζ by Corollary 3.10. Here a typical SGD will change f . 2) In the second setting, suppose φ is relu. Design a sequence of inputs such that training the MLP on them with very large learning rate will make all relu neurons saturated in the 0 region. Then f is everywhere 0, and an SGD step will not change that. Remark 3.12 (Not All Dynamics are Infinite-Width Limits). Accordingly, a nonlinear function space dynamics cannot be a valid infinite-width limit of some abc-parametrization. By nonlinear, we mean f t+1 (ξ) -f t (ξ) is nonlinear in L (f t (ξ t ), y t ). For example, any natural higher-order generalization of Eq. ( <ref type="formula" target="#formula_29">9</ref>) (perhaps derived from a Taylor expansion at initialization) is not a valid limit. <ref type="bibr" target="#b17">18</ref> Pretraining and Transfer Learning As in the shallow examples, Corollary 3.9 says that any kernel regime parametrization (including NTP) trivializes pretraining and transfer learning <ref type="bibr" target="#b18">19</ref> in the infinite-width limit.</p><p>By calculating r for the standard parametrization (SP), we can easily see that it cannot admit feature learning in the sense here without becoming unstable. However, in the next section, we will manually analyze the training dynamics in an SP MLP to give an intuition why this is the case. In turn, we then propose a simple modification of SP, the Maximal Update Parametrization (MUP or µP), which does admit feature learning and, in fact, does so maximally in a suitable sense. In the pedagogical spirit, we will focus on the key insights and stress the right heuristics without dwelling on formal aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Standard Parametrization</head><p>In this section, we give intuition for why gradient descent of neural network in standard parametrization (SP) will lead to logits blowup after 1 step, if the learning rate is ω(1/n), where n is the width. In addition, we will see why, with learning rate O(1/n), SP is in kernel regime. We first consider the simplest example and then state the general result at the end of the section.</p><p>To demonstrate the general principle in deep networks, it is necessary to consider the behavior of an n × n matrix in the middle of the network. Thus, the simplest case is a 2-hidden-layer linear MLP, i.e. Eq. ( <ref type="formula" target="#formula_0">1</ref>) with L = 2 and φ = id. The standard parametrization is given by</p><formula xml:id="formula_31">a l = 0 ∀l, b 1 = 0, b l = 1/2 ∀l ≥ 2.</formula><p>(SP)</p><p>We consider 1 step of SGD with learning rate n -c on a single data pair (ξ, y). Then we can without ambiguity suppress explicit dependence on ξ and write</p><formula xml:id="formula_32">f = V h, h = W h, h = U ξ,<label>(10)</label></formula><p>where U αβ ∼ N (0, 1) and W αβ , V αβ ∼ N (0, 1/n) are the trainable parameters (simplifying the notation in Section 3). As in Section 3, we use • t to denote the quantity • after t step of SGD.</p><p>Because we only focus on the 1st step of SGD, we lighten notation and write • = • 0 .</p><p>Initialization Since U, W, V are independently sampled, a standard Central Limit argument would show that h, h, f all have roughly iid Gaussian coordinates of variance Θ(1).</p><p>First Gradient Now let's consider the gradients of f on the data pair (ξ, y), which are given by</p><formula xml:id="formula_33">d h = V , dh = W d h, dV = h, dW = d h h = V h , dU = dh ξ .<label>(11)</label></formula><p>For simplicity, suppose we only update W by learning rate n -c (and leave U, V unchanged); our conclusion will not change in the general case where we train all layers. Then with χ denoting the loss derivative L (f, y), we can write</p><formula xml:id="formula_34">W 1 = W -n -c χ dW.</formula><p>We shall show now that c ≥ 1 or else f 1 blows up with the width n after this SGD step.</p><p>After First SGD Step At t = 1, h 1 = h since we did not update U , but</p><formula xml:id="formula_35">h1 = W 1 h = h -n -c χ dW h = h -n -c χ • V h h<label>(12)</label></formula><formula xml:id="formula_36">f 1 = V h1 = f -n -c χ V V h h.<label>(13)</label></formula><p>Now, as noted above, h has iid Θ(1) coordinates, so</p><formula xml:id="formula_37">h h = Θ(n) ∈ R. Similarly, V ∈ R 1×n has Gaussian coordinates of variance Θ(1/n), so V V = Θ(1) ∈ R.</formula><p>Finally, for typical loss function L like MSE or cross entropy, χ = L (f, y) is of order Θ(1) because f fluctuates on the order Θ(1). Altogether,</p><formula xml:id="formula_38">f 1 = f -Θ(n 1-c ). Therefore, for f 1 to remain O(1), we must have c ≥ 1, i.e. the learning rate is O(1/n).</formula><p>Kernel Regime and Lack of Feature Learning Consequently, the network cannot learn features in the large width limit if we would like the logits to not blow up. Indeed, this version of SGD where only W is updated can be seen to correspond to the limit where</p><formula xml:id="formula_39">a 1 = θ, b 1 = -θ, a 2 = 0, b 2 = 1/2, a 3 = θ, b 3 = -θ + 1/2, θ → ∞.</formula><p>With c = 1 as derived above, the parametrization is stable and nontrivial, as can be checked from Theorems 3.3 and 3.4. Then we get r = 1/2 &gt; 0, so by Corollary 3.9, this parametrization is in kernel regime and does not admit feature learning. We can also see this directly from Eq. ( <ref type="formula" target="#formula_35">12</ref>): from our calculations above,</p><formula xml:id="formula_40">h1 -h = O(n 1-c ) V = O(1) V</formula><p>whose coordinates have size O(n -1/2 ) since V 's coordinates do, so there's no feature learning (at least in the first step). Finally, from Eq. ( <ref type="formula" target="#formula_36">13</ref>), because</p><formula xml:id="formula_41">V V → 1 and n -c h h = n -1 h h → ξ 2 , we get 20 f 1 -f → -χK(ξ, ξ) def = -χ ξ 2 , i.</formula><p>e. f evolves by kernel gradient descent with the linear kernel. Our derivations here only illustrate the first SGD step, but we can get the same conclusion from all steps of SGD similarly.</p><p>We summarize the general case below, which follows trivially from Theorem 3.3 and Corollary 3.9. Theorem 4.1. An L-hidden-layer MLP in standard parametrization (see Eq. (SP) and Table <ref type="table">1</ref>) can only allow SGD learning rate of order O(1/n) if we require lim n→∞ E f t (ξ) 2 &lt; ∞ for all training routine, time t, and input ξ. In this case, it is in kernel regime and does not admit feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Maximal Update Parametrization</head><p>As shown in the last section, the standard parametrization does not admit a feature learning infinitewidth limit without blowing up logits. Here we propose simple modifications of the standard parametrization to make this possible while maintaining stability: 1) To enable feature learning, it suffices to divide the logits by √ n and use Θ(1) learning rate, i.e. set a L+1 = 1/2, c = 0 on top of Eq. (SP); 2) to allow every layer to perform feature learning, we should furthermore set a 1 = -1/2, b 1 = 1/2. We will see that this essentially means we update each weight matrix as much as possible without blowing up the logits or activations, so we call this the Maximal Update Parametrization (abbreviated MUP or µP). <ref type="bibr" target="#b19">20</ref> Formally, these are almost sure convergences, but we suppress these details to emphasize on intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dividing Logits by √ n</head><p>For example, in the 2-hidden-layer linear MLP example above, the network would compute</p><formula xml:id="formula_42">f (ξ) = 1 √ n v h(ξ), h(ξ) = W h(ξ), h(ξ) = U ξ,<label>(14)</label></formula><p>where U αβ ∼ N (0, 1) and W αβ , v αβ ∼ N (0, 1/n) are the trainable parameters. Compared to SP (Eq. ( <ref type="formula" target="#formula_32">10</ref>)), h(ξ), h(ξ) stays the same; only the logit f (ξ) is scaled down. Again, to simplify notation, we abbreviate • = • 0 and suppress explicit dependence on ξ. This has two consequences Logits at Initialization Converge to 0 since f has variance Θ(1/n) (compare to the GP limit of MLP in SP at initialization).</p><p>Θ(1) Learning Rate and Feature Learning Even though f → 0, the loss derivative χ = L (f, y) stays Θ(1) if y = 0. When we redo the calculation in Eq. ( <ref type="formula" target="#formula_35">12</ref>), we see</p><formula xml:id="formula_43">h1 = h -n -c-1/2 χ v h h = h -Θ(n -c+1/2 )v<label>(15)</label></formula><formula xml:id="formula_44">f 1 = f -n -c-1 χ vv h h = f -Θ(n -c ).</formula><p>Because v has coordinates of size Θ(n -1/2 ), we see that h and f both change by Θ(1) coordinatewise if c = 0 (i.e. learning rate is Θ( <ref type="formula" target="#formula_0">1</ref>)). This directly illustrates feature learning after just 1 step of SGD.</p><p>For general MLPs, we can also check a L+1 = 1/2, c = 0 on top of Eq. (SP) implies r = 0 and thus admits feature learning by Theorem 3.6.</p><p>Kernel Behavior or Lack Thereof The example we have here, where we only train the middle layer in a linear MLP, actually is in kernel regime. This does not violate Corollary 3.9, however, which assumes Assumption 3.1. If, for example, we have tanh nonlinearity, then it is easy to see the µP SGD dynamics does not have a kernel limit: If so, then f 1 -f is linear in the learning rate η. But note h1 -h is Θ(1) as n → ∞ and linear in η, as can be derived similarly to Eq. ( <ref type="formula" target="#formula_43">15</ref>). Because tanh is bounded, this cannot happen. Contrast this with SP or NTP, where h1 -h is Θ(1/ √ n) and thus "resides in the linear regime of tanh", allowing perfect scaling with η.</p><p>In addition, even in an linear MLP, if we train the middle layer and the last layer, then the dynamics intuitively will become quadratic in the weights, so will not have a kernel limit. Contrast this with SP or NTP, which suppress these higher order interactions because the learning rate is small, and a first order Taylor expansion heuristic holds.</p><p>How is this different from standard parametrization with learning rate 1/ √ n? As shown above, the logit f blows up like Θ( √ n) after 1 step of SGD with learning rate Θ(1/ √ n) in the standard parametrization, but remains Θ(1) in our parametrization here. The reason these two parametrizations seem similar is because in the 1st step, the weights receive the same updates modulo the loss derivative χ = L (f, y). Consequently,</p><formula xml:id="formula_45">x L 1 -x L and h L 1 -h L are Θ(1) coordinatewise in both cases. However, this update makes x L 1 correlated with W L+1 1 , so that W L+1 1 x L 1 (and f 1 ) scales like Θ(n 1-a L+1 -b L+1 ) due to Law of Large Numbers. Thus only in our parametrization here (a L+1 = b L+1 = 1/2) is it Θ(1), while in standard parametrization (a L+1 = 0, b L+1 = 1/2) it blows up like Θ( √ n).</formula><p>Contrast this with the behavior at initialization, where W L+1 and x L are independent and zero-mean, so W L+1 x L scales like Θ(n 1/2-a L+1 -b L+1 ) by Central Limit Theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">First Layer Parametrization</head><p>While this now enables feature learning, the first layer preactivation h effectively stays fixed throughout training even if we were to train U . For example, if we update U in the linear MLP example Eq. ( <ref type="formula" target="#formula_42">14</ref>), then by Eq. ( <ref type="formula" target="#formula_33">11</ref>),</p><formula xml:id="formula_46">U 1 = U -n -c χ dU = U -n -c χ dhξ h 1 = U 1 ξ = h -n -c χ dhξ ξ = h -Θ(n -c )dh since ξ ξ, χ = Θ(1). Now dh = W d h = W 1 √ n v has roughly iid Gaussian coordinates, each of size Θ(1/n), since 1</formula><p>√ n v has coordinates of the same size. Therefore, even with c = 0, h changes by at most O(1/n) coordinatewise, which is dominated by its value at initialization. This O(1/n) change also induces a O(1/n) change in f , which would be dominated by the Θ(1) change due to W 's evolution, as seen in Eq. (15). We therefore propose to set a 1 = -1/2, b 1 = 1/2 on top of Section 5.1's parametrization. This implies the forward pass of f remains the same but U 's gradient is scaled up by n, so that h now changes by Θ(1) coordinatewise. In summary, we define Definition 5.1. The Maximal Update Parametrization (abbreviated MUP, or µP), in the context of an L-hidden-layer MLP (Eq. ( <ref type="formula" target="#formula_0">1</ref>)), is given by</p><formula xml:id="formula_47">c = 0, b l = 1/2 ∀l, a l =    -1/2 l = 1 0 2 ≤ l ≤ L 1/2 l = L + 1.</formula><p>Notice that µP for a 1-hidden-layer perceptron is equivalent to the mean field parametrization by Eq. ( <ref type="formula" target="#formula_8">5</ref>). We also describe µP for any architecture in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">What is µP Maximal In?</head><p>For technical reasons, we adopt Assumption 3.1 again for the formal results of this section.</p><p>In an abc-parametrization, the change in weight W = W l t for any l ≥ 2 due to learning rate n -c is δW def = -n -c • n -2a dh x where we abbreviated x = x l-1 t , h = h l t , a = a l . (We will use δ to denote 1-step change, but ∆ to denote lifetime change). In the next forward pass, δW contributes δW x = -n 1-c-2a (x x/n)dh, where x is the new activation due to change in previous layers' weights. In general, x and x are strongly correlated. Then x x/n → R for some R = 0 by Law of Large Numbers (as they both have Θ(1) coordinates in a stable parametrization). One can heuristically see that dh has the same size as the last layer weights, which is</p><formula xml:id="formula_48">Θ(n -(a L+1 +b L+1 ) + n -(2a L+1 +c) ) (where the first summand is from W L+1 0 and the other from ∆W L+1 t ). Thus, δW x is a vector with Θ(n -r l ) def = Θ((n -(a L+1 +b L+1 ) + n -(2a L+1 +c) )n 1-c-2a</formula><p>) coordinates. If r l &gt; 0, then δW x contributes vanishingly; if r l &lt; 0, then δW x blows up. For l = 1, we get similar insights after accounting for the finite dimensionality of ξ. Definition 5.2. For l ∈ [L], we say W l is updated maximally if ∆W l t x l-1 t (ξ) has Θ(1) coordinates for some training routine <ref type="bibr" target="#b20">21</ref> , time t ≥ 1, and input ξ. Proposition 5.3. In a stable abc-parametrization, for any l ∈ [L], W l is updated maximally iff</p><formula xml:id="formula_49">r l def = min(a L+1 + b L+1 , 2a L+1 + c) + c -1 + 2a l + I(l = 1) = 0.</formula><p>Note that r (Definition 3.2) is the minimum of r l over all l. In µP, we can calculate that r l = 0 for all l ∈ [L], so all W l , l ∈ [L], are updated maximally. Put another way, the final embedding x L (ξ) will have nonvanishing (nonlinear) contributions from ∆W l of all l. These contributions cause the logit f (ξ) to change via interactions with W L+1 0 and ∆W L+1 t . If both W L+1 0 and ∆W L+1 t are too small, then the logit is fixed to its initial value, so all of the feature learning would have been useless. <ref type="bibr" target="#b21">22</ref> It's also possible for one to contribute vanishingly but not the other. <ref type="bibr" target="#b22">23</ref> But both contribute in µP.</p><formula xml:id="formula_50">Definition 5.4. We say W L+1 is updated maximally (resp. initialized maximally) if ∆W L+1 t x L t (ξ) = Θ(1) (resp. W L+1 0 ∆x L t (ξ) = Θ(1)</formula><p>) for some training routine, time t ≥ 1, and input ξ.</p><p>Note Definition 5.4 is similar to Definition 5.2 except</p><formula xml:id="formula_51">∆W L+1 t x L t (ξ) ∈ R but ∆W l t x l-1 t (ξ) ∈ R n . Proposition 5.5. In a stable abc-parametrization, W L+1 is 1) updated maximally iff 2a L+1 + c = 1, and 2) initialized maximally iff a L+1 + b L+1 + r = 1.</formula><p>We remark that, by Theorem 3.4, a parametrization is nontrivial iff W L+1 is maximally updated or initialized. Using Propositions 5.3 and 5.5 and Theorem 3.3, we can now easily conclude Theorem 5.6. In µP, W l is updated maximally for every l ∈ [L + 1], and W L+1 is also initialized maximally. µP is the unique stable abc-parametrization with this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Deriving Feature Learning Infinite-Width Limit: Intuition and Examples</head><p>We propose the Tensor Programs technique for deriving the infinite-width limit of any abcparametrization. This ultimately just requires the researcher to mechanically apply a set of rules to the computation graph underlying SGD. However, while operationally simple, this procedure would seem "too magical" at first. In this section, through a series of examples, we seek to build intuition for what is being automated by this procedure. Then, in the next section, we formally describe the Tensor Programs framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup and Notation</head><p>For pedagogical simplicity, we only consider input dimension d = 1 and learning rate η = 1 here, but generalization to d &gt; 1, η = 1 is straightforward. We consider SGD with a singleton minibatch {(ξ t , y t )} at time t = 0, 1, 2, . . ., where ξ t is the network input and y t is the label. We write W l t for the matrix W l after t steps of such training. For any network input ξ ∈ R, we write x l t (ξ) (resp. h l t (ξ), f t (ξ)) for the activation x l (resp. preactivation h l , logits f ) of the network after t steps of SGD. We denote the scaled gradient n∇</p><formula xml:id="formula_52">x l t f t (ξ) (resp. n∇ h l t f t (ξ)) by dx l t (ξ) (resp. dh l t (ξ)).</formula><p>For brevity, we abuse notation and use x l t (without being applied to ξ) to also denote the vector x l t (ξ t ) (applied specifically to ξ t ); likewise for h l t , dh l t , dx l t , f t . We will not use x l t on its own to denote the function ξ → x l t (ξ) so this should not cause confusion. The loss function is denoted L and the loss derivative L (logit, target) is in the first argument. We write χ t def = L (f t , y t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">1-Hidden-Layer MLP</head><p>As mentioned above, for 1 hidden layer, the infinite-width µP limit is the same as the mean field limit of [11, 30, 43, 45]. Nevertheless, we present a slightly different derivation of this that is more consistent with the philosophy of Tensor Programs. Such a network on input ξ ∈ R is given by</p><formula xml:id="formula_53">f (ξ) = V x(ξ), x(ξ) = φ(h(ξ)), h(ξ) = U ξ,<label>(16)</label></formula><p>for</p><formula xml:id="formula_54">U ∈ R n×1 , V ∈ R 1×n parametrized like U = √ nu, V = 1</formula><p>√ n v and with initialization u αβ , v αβ ∼ N (0, 1/n). <ref type="bibr" target="#b23">24</ref> Then U 0 (the initial value of U ) has iid N (0, 1) coordinates. It will turn out to be convenient to represent each such coordinate distribution as a random variable Z U0 def = N (0, 1). Likewise, let Z nV0 def = N (0, 1), independent from Z U0 , represent the coordinate distribution of nV 0 (we do nV 0 instead of V 0 so that the Z random variable is always independent of n). We derive the µP limits of the first forward and backward passes manually before stating the general case. To lighten notation, we suppress the t = 0 subscript (e.g. U = U 0 , h = h 0 , f = f 0 , etc), as we will spend some time on the first SGD step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Forward Pass After randomly initialization, the preactivation</head><formula xml:id="formula_55">h = h(ξ) (where ξ = ξ 0 ∈ R is the first input) has iid coordinates, each a sample from Z h def = ξZ U ∈ R. Naturally, x = x(ξ) has iid coordinates as well, each a sample from Z x def = φ(Z h ). Finally, f = V x = 1 n n α=1 (nV ) α x α → f def = E Z nV Z</formula><p>x by Law of Large Numbers as n → ∞. <ref type="bibr" target="#b24">25</ref> In particular, f becomes deterministically 0 in this limit because V and U are independent. For a typical loss function L, the loss derivative</p><formula xml:id="formula_56">χ def = L (f, y) then also become deterministic, χ → χ def = L ( f , y). First Backward Pass Similarly, dx = nV (recall dx t def = n∇ xt f t ) has coordinates distributed like Z dx def = Z nV and dh = dx φ (h) has coordinates distributed like Z dh def = Z dx φ (Z h ) = Z nV φ (Z h ).</formula><p>Then SGD with learning rate 1 makes the following updates:</p><formula xml:id="formula_57">v 1 = v -χx/</formula><p>Since χ converges to a deterministic limit χ, the coordinates of these updates are roughly iid, corresponding to an update of Z random variables:</p><formula xml:id="formula_58">Z nV1 = Z nV -χZ x , Z U1 = Z U -χξZ dh .</formula><p>Second Forward Pass Thus V 1 and U 1 still have roughly iid coordinates after 1 SGD step. Then, in the second forward pass, h 1 has coordinates</p><formula xml:id="formula_59">Z h1 def = ξ 1 Z U1 = ξ 1 Z U -ξ 1 χξZ dh = ξ 1 Z U -ξ 1 χξZ nV φ (Z h ), x 1 has coordinates Z x1 def = φ(Z h1</formula><p>), and the output is</p><formula xml:id="formula_60">f 1 = 1 n n α=1 (nV 1 ) α x α → f1 def = E Z nV1 Z x1 = E(Z nV -χZ x )Z x1<label>(17)</label></formula><p>as n → ∞. Then χ</p><formula xml:id="formula_61">1 def = L (f 1 , y 1 ) → χ1 def = L ( f1 , y 1</formula><p>) becomes deterministic. The gradient vectors have roughly iid coordinates by a similar logic.</p><p>tth Iteration Repeating the above reasoning shows that at any time t (independent of n), we obtain Theorem 6.1. Consider a 1-hidden-layer MLP in µP (Eq. ( <ref type="formula" target="#formula_53">16</ref>)) and any training routine with learning rate 1. Suppose φ is pseudo-Lipschitz. <ref type="bibr" target="#b25">26</ref> As n → ∞, for every input ξ, f t (ξ) converges almost surely to ft (ξ) defined as follows:</p><formula xml:id="formula_62">f t (ξ) a.s. --→ ft (ξ) def = E Z nVt Z xt(ξ) , Z xt(ξ) def = φ(Z ht(ξ) ), Z ht(ξ) def = ξZ Ut ,<label>(18)</label></formula><formula xml:id="formula_63">χt def = L ( ft , y t ), Z nVt+1 def = Z nVt -χt Z xt , Z Ut+1 def = Z Ut -χt ξ t Z nVt φ (Z ht ),<label>(19)</label></formula><p>with, as initial conditions, Z U0 and Z nV0 being independent standard Gaussians, where in Eq. (19)  we abbreviated ft = ft (ξ t ),</p><formula xml:id="formula_64">x t = x t (ξ t ), h t = h t (ξ t ).</formula><p>As aforementioned, this is a discrete time, minibatched version of the mean field limit of [11, 30,  43, 45]. <ref type="bibr" target="#b26">27</ref> When φ is identity, it's easy to see that Z nVt and Z Ut are always (deterministic) linear combinations of Z nV0 and Z U0 , say Z nVt = A t Z nV0 + B t Z U0 and Z Ut = C t Z nV0 + D t Z U0 . Then the limit ft depends solely on A t , B t , C t , D t . By tracking their evolution, we get the following greatly simplified formula for an infinite-width µP linear network. Corollary 6.2. Consider a 1-hidden-layer linear MLP in µP (Eq. ( <ref type="formula" target="#formula_53">16</ref>)) and any training routine with learning rate 1. As n → ∞, for every input ξ, f t (ξ) converges almost surely to ft (ξ) defined as follows:</p><formula xml:id="formula_65">ft (ξ) = (A t C t + B t D t )ξ, χt = L ( ft , y t ), (A t+1 , B t+1 ) = (A t , B t ) -χt ξ t (C t , D t ), (C t+1 , D t+1 ) = (C t , D t ) -χt ξ t (A t , B t ), with initial condition A 0 = D 0 = 1, B 0 = C 0 = 0.</formula><p>This can be easily generalized to larger input and output dimenions (see Appendix D.1). In a gist, such an infinite-width µP linear network with input dimension d and output dimension d o is equivalent to a width-(d + d o ) linear network with the same input/output dimensions but an "diagonal", instead of random, initialization. Our Word2Vec and MAML experiments will crucially rely on this simplifying observation. We remark that, in contrast to our approach, such an observation would be obscured by the PDE perspective of prior works [11, 30, 43, 45].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">2-Hidden-Layer MLP: SGD with Partially Decoupled Backpropagation</head><p>A 2-hidden-layer MLP is given by f</p><formula xml:id="formula_66">(ξ) = V x(ξ), x(ξ) = φ( h(ξ)), h(ξ) = W x(ξ), x(ξ) = φ(h(ξ)), h(ξ) = U ξ, for U ∈ R n×1 , W ∈ R n×n , V ∈ R 1×n parametrized like U = √ nu, V = 1 √ n v and with initial- ization u αβ , W αβ , v αβ ∼ N (0, 1/n).</formula><p>The presence of the n × n Gaussian matrix W ("∞ × ∞" as opposed to "∞× finite" like U or "finite ×∞" like V ) is new and has two major effects on the infinite-width training dynamics: 1) A Central Limit effect from the random Gaussian nature of W and 2) a correlation effect between W and its transpose W . We isolate the first effect here by analyzing a slightly different version of backpropagation (which has a different limit than normal backpropagation), and then discuss the second effect in the next section. We abuse notation and abbreviate W = W 0 .</p><p>Partially Decoupled Backpropagation In this section, we analyze a version of SGD where the backpropagation weights are partially decoupled from the forward propagation weights. Here, we think of ∆W t as the trainable weights, initialized at 0, and think of the Gaussian W as untrainable "constants". The forward pass proceeds normally <ref type="bibr" target="#b27">28</ref> with W t = W + ∆W t . But we sample and fix an iid copy W of W before training, and in the backward pass compute</p><formula xml:id="formula_67">dx t = ( W + ∆W t )d ht instead of dx t = (W + ∆W t )d ht = W t d ht .<label>(20)</label></formula><p>In particular, at initialization, we would have dx 0 = W d h0 instead of dx 0 = W d h0 . Everything else stays the same in the backward pass <ref type="bibr" target="#b28">29</ref> . Finally, each weight is still updated by SGD via the usual outer products: with</p><formula xml:id="formula_68">χ t def = L (f t , y t ), v t+1 = v t -χ t x t / √ n, ∆w t+1 = ∆w t -χ t d ht x t /n, u t+1 = u t -χ t ξ t dh t / √ n. (21) Since V = v/ √ n, W = w, U = √</formula><p>nu per µP, this causes the following changes in W s:</p><formula xml:id="formula_69">V t+1 = V t -χ t x t /n, ∆W t+1 = ∆W t -χ t d ht x t /n, U t+1 = U t -χ t ξ t dh t<label>(22)</label></formula><p>Note here we update ∆w and ∆W instead of w and W .</p><p>Why This Decoupled SGD? The reasons we talk about this version of SGD is that it isolates the effect of having a Gaussian n × n matrix W in the backward pass, and we can derive its infinite-width limit relatively easily using Central Limit heuristics. In the normal version of SGD, W would equal W , and its correlation with W creates additional terms in the infinite-width dynamics, that are better explained on their own. Again, we walk through the first few forward and backward passes to gain some intuition for the infinite-width limit, before stating the general case.</p><p>First Forward Pass is similar to that in Section 6.1 and follows the usual calculations involved in deriving the NNGP <ref type="bibr" target="#b29">30</ref> .</p><p>First Backward Pass is similar to that in Section 6.1 and to calculations involved in deriving Neural Tangent Kernel, except swapping W with W (which at this point has no visible effect, because of the Gradient Independence Phenomenon [51]; but the effect will become clear in the second forward pass) <ref type="bibr" target="#b30">31</ref> . We end up with ∆W 1 = -χ 0 d h0 x 0 , as usual. <ref type="bibr" target="#b27">28</ref> </p><formula xml:id="formula_70">i.e. ft = Vt xt, xt = φ( ht), ht = (W + ∆Wt)xt, xt = φ(ht), ht = U ξt. 29 i.e. dxt = nV t , d ht = φ ( ht) dxt, dht = φ (ht) dxt 30 1) h0 is iid Gaussian with coordinates drawn from Z h 0 = ξ0Z U 0 ; 2) x0 has coordinates Z x 0 = φ(Z h 0 );</formula><p>3) h0 = W x0 has roughly iid coordinates drawn from a zero-mean Gaussian Z h0 by a Central Limit heuristic, where Z h0 is correlated with Z h0 (ξ) for any ξ (including</p><formula xml:id="formula_71">ξ = ξ0) with covariance Cov(Z h0 , Z h0 (ξ) ) = limn→∞ 1 n x 0 x0(ξ) = E Z x 0 Z x 0 (ξ) ; 4) x0 has coordinates Z x0 = φ(Z h0 ); 5) f0 = 1 n n α=1 (nV0)α x0α → f0 def = E Z nV 0 Z x0 by a Law of Large Number heuristic. 31 1) dx0 = nV 0 so Z dx 0 = Z nV 0 ; 2) Z d h0 = φ (Z h0 ) Z dx 0 ; 3) Z dx 0 = Z W d h0 is Gaussian with covariance Cov(Z dx 0 , Z dx 0 (ξ) ) = limn→∞ 1 n d h 0 d h0(ξ) = E Z d h0 Z d h0 (ξ)</formula><p>for any input ξ; 4) Z dh 0 = φ (Z h 0 ) Z dx 0 . Since f converges to a deterministic number f0, we also generically have L (f, y0) → χ0 def = L ( f0, y0). Finally, the weights are updated like Eq. (22).</p><p>Second Forward Pass As usual, we have</p><formula xml:id="formula_72">Z h1 = ξ 1 Z U1 = ξ 1 Z U0 -χ0 ξ 1 ξ 0 Z dh0 and Z x1 = φ(Z h1</formula><p>), reflecting the coordinate distributions of h 1 and x 1 <ref type="bibr" target="#b31">32</ref> . Next,</p><formula xml:id="formula_73">h1 = W x 1 + ∆W 1 x 1 = W x 1 -χ 0 d h0 x 0 x 1 n .<label>(23)</label></formula><p>On one hand, 1)</p><formula xml:id="formula_74">x 0 x1 n → E Z x1 Z x0</formula><p>by a Law of Large Numbers heuristic. On the other hand, 2) by a Central Limit heuristic, W x 1 should roughly have Gaussian coordinates Z W x1 correlated with</p><formula xml:id="formula_75">Z h0 = Z W x0 with Cov(Z W x1 , Z W x0 ) = lim x 0 x1 n = E Z x1 Z x0</formula><p>. However, very importantly, this Central Limit heuristic is correct only because we used W in backprop instead of W ; otherwise, h 1 has a strong correlation with W through dh 0 = φ (h 0 ) (W d h0 ), and thus so does x 1 , so that W x 1 no longer has Gaussian coordinates. This is the "second major effect" referred to in the beginning of this section. See Section 6.3 for how to handle this correlation.</p><p>In any case, in our scenario here,</p><formula xml:id="formula_76">Z h1 def = Z W x1 -cZ d h0 , where c = χ0 E Z x1 Z x0</formula><p>, is a linear combination of a Gaussian variable and the gradient d h0 's coordinate random variable. Finally, Z x1 = φ(Z h1 ) and the logit is</p><formula xml:id="formula_77">f 1 = 1 n n α=1 (nV 1 ) α x1α → f1 def = E Z nV1 Z x1 = E Z nV0 Z x1 -χ0 E Z x0 Z x1 .</formula><p>Second Backward Pass Everything proceeds just like in the 1-hidden-layer case <ref type="bibr" target="#b32">33</ref> except for the computation of</p><formula xml:id="formula_78">dx 1 = W d h1 -∆W 1 d h1 = W d h1 -χ 0 x 0 d h 0 d h1 n .</formula><p>Like in the computation of h1 in Eq. ( <ref type="formula" target="#formula_73">23</ref>),</p><formula xml:id="formula_79">d h 0 d h1 n → E Z d h0 Z d h1</formula><p>and W d h1 is roughly Gaussian (and correlated with W d h0 in the natural way). But again, for this Gaussian intuition to be correct, it is crucial that we use W here instead of W , or else dx 1 (and thus d h1 ) is strongly correlated with</p><formula xml:id="formula_80">W (through x0 = φ(W x 0 ) inside n∆V 1 = -χ 0 x 0 ).</formula><p>In any case, we have</p><formula xml:id="formula_81">Z dx1 = Z W d h1 -cZ x0 , where c = χ0 E Z d h0 Z d h1 ,</formula><p>is a sum of Gaussian Z W d h1 and a multiple of Z x0 . Then weights are updated according to Eq. (22).</p><p>tth Iteration For general t, we always have (true in normal SGD as well)</p><formula xml:id="formula_82">∆W t = - 1 n t-1 s=0 χ s d hs x s</formula><p>so that in the forward pass</p><formula xml:id="formula_83">ht = W x t + ∆W t x t = W x t - t-1 s=0 χ s d hs x s x t n<label>(24)</label></formula><formula xml:id="formula_84">Z ht def = Z W xt - t-1 s=0 χs Z d hs E Z xs Z xt .</formula><p>Here</p><formula xml:id="formula_85">Z W xt is Gaussian with covariance Cov(Z W xt , Z W xs ) = E Z xt Z xs for any s.</formula><p>This means that Z ht and Z hs are correlated through Z W xt , Z W xs (but also through Z d hr , r ≤ min(t, s)). Likewise, in the backward pass,</p><formula xml:id="formula_86">dx t = W d ht -∆W d ht = W d ht - t-1 s=0 χ s x s d h s d ht n Z dxt def = Z W d ht - t-1 s=0 χs Z xs E Z d hs Z d ht</formula><p>32 Recall they abbreviate h1(ξ1) and x1(ξ1)</p><formula xml:id="formula_87">33 dx1 = nV 1 , d h1 = dx1 φ ( h1), dh1 = dx1 φ (h1)</formula><p>Here, Z W d ht is Gaussian with covariance Cov(Z W d ht , Z W d hs ) = E Z d ht Z d hs for any s. Thus, Z dxt and Z dxs are correlated through Z W d ht , Z W d hs (but also through Z xr , r ≤ min(t, s)). Again, the Gaussianity of Z W xt and Z W d ht depend crucially on the fact that we use W instead of W in backpropagation.</p><p>Other parts of the forward and backward propagations are similar to before. Our reasoning can be formalized via Tensor Programs to prove the following Theorem 6.3. Consider a 2-hidden-layer MLP in µP with partially decoupled backpropagation as in Eq. ( <ref type="formula" target="#formula_67">20</ref>) and any training routine with learning rate 1. Suppose φ is pseudo-Lipschitz. <ref type="bibr" target="#b33">34</ref> As n → ∞, for every input ξ, f t (ξ) a.s.</p><p>--→ ft (ξ), where ft (ξ) is defined as follows:</p><formula xml:id="formula_88">(forward pass) ft (ξ) def = E Z nVt Z xt(ξ) , Z xt(ξ) def = φ(Z ht(ξ) ), Z xt(ξ) def = φ(Z ht(ξ) ), Z ht(ξ) def = ξZ Ut Z ht(ξ) def = Z W xt(ξ) - t-1 s=0 χs Z d hs E Z xs Z xt(ξ) (<label>25</label></formula><formula xml:id="formula_89">)</formula><formula xml:id="formula_90">{Z W xt(ξ) } ξ,t centered, jointly Gaussian with Cov(Z W xt(ξ) , Z W xs(ζ) ) = E Z xt(ξ) Z xs(ζ)</formula><p>(backward pass)</p><formula xml:id="formula_91">χ t def = L ( ft , y t ), Z dxt def = Z nVt , Z d ht def = φ (Z ht )Z dxt Z dht def = φ (Z ht )Z dxt Z dxt def = Z W d ht - t-1 s=0 χs Z xs E Z d hs Z d ht (26) {Z W d ht } t centered, jointly Gaussian with Cov(Z W d ht , Z W d hs ) = E Z d ht Z d hs (U, V updates) Z nVt+1 def = Z nVt -χt Z xt Z Ut+1 def = Z Ut -χt ξ t Z dht</formula><p>with Z U0 and Z nV0 being independent standard Gaussians as initial conditions, and by definition,</p><formula xml:id="formula_92">{Z W xt(ξ) } ξ,t , {Z W d ht } t , Z U0 ,</formula><p>and Z nV0 are mutually independent sets of random variables. Here, if h t appears without argument, it means h t (ξ t ); likewise for ht , x t , xt , dh t , d ht , dx t , dx t , ft . 6.3 2-Hidden-Layer MLP: Normal SGD Finally, we dicuss normal SGD for 2-hidden-layer MLP, i.e. in backprop we compute</p><formula xml:id="formula_93">dx t = W t d ht = (W + ∆W )d ht .</formula><p>The first forward and backward passes are essentially the same as in the last section. However, as mentioned there, in the second forward pass, W x 1 (a part of h1 = W x 1 + ∆W 1 x 1 ) will no longer be approximately Gaussian because of the correlation between x 1 and W . Let's first get some intuition for why this is before stating the infinite-width limit formally.</p><p>Warmup: φ = id First, as warmup, suppose φ = id. In this case, W x 1 will actually still be Gaussian, but its variance will be different than what's predicted in the previous section. To lighten notation, we write x = x 1 in this section. Then unwinding the definition of x, we have</p><formula xml:id="formula_94">x = h + aW z where we abbreviated h = ξ 1 U 0 , z = d h0 , a = -χ 0 ξ 0 ξ 1 . Then W x has coordinates (W x) α = (W h) α + a(W W z) α .</formula><p>As derived in the first forward pass in Section 6.2, (W h) α is approximately Gaussian (particularly because W, U 0 are independent). This is true for (W W z) α as well here because we assumed φ = id, but not true generally. Indeed,</p><formula xml:id="formula_95">(W W z) α = β,γ W αβ W γβ z γ = z α β (W αβ ) 2 + β γ =α W αβ W γβ z γ .</formula><p>We will soon see the derivations of Section 6.2 correspond to ignoring the first term: In the second term, there are n summands of the form γ =α W αβ W γβ z γ that are approximately iid with variance ≈ z 2 /n 2 . Thus, the second term itself, by a Central Limit heuristic, should converge to N (0, lim n→∞ z 2 /n). On the other hand, the first term z α β (W αβ ) 2 → z α by Law of Large Numbers. Tying it all together, (W x) α is a linear combination of two Gaussian terms (W h) α and β γ =α W αβ W γβ z γ , as well as as z α (which is Gaussian in the case of φ = id, but not generally).</p><p>Note that, if we did (W W z) α instead of (W W z) α , as in the last section, then the same analysis would show the first term is z α β W αβ W βα → 0, while the second term converge in distribution to the same Gaussian. Thus, the effect of decoupling in Section 6.2 is killing the copy of z in (W x) α .</p><p>We can summarize our derivation here in terms of Z:</p><formula xml:id="formula_96">For φ = id: Z W x def = Z W h + aZ W W z = Z W h + a( ẐW W z + Z z ),<label>(27)</label></formula><p>where</p><formula xml:id="formula_97">ẐW W z def = N (0, E(Z z ) 2 ).</formula><p>Note the Central Limit heuristic in the derivation of ẐW W z also shows ẐW W z is jointly Gaussian with Z W h with Cov( ẐW W z , Z W h ) = E Z W z Z h . So, to put Eq. ( <ref type="formula" target="#formula_96">27</ref>) in a form more suggestive of the general case, we will write</p><formula xml:id="formula_98">Z W x = ẐW x + aZ z , where ẐW x = Z W h + a ẐW W z d = N (0, E(Z x ) 2 ). (<label>28</label></formula><formula xml:id="formula_99">)</formula><p>General φ Unwinding the definition of x, we have</p><formula xml:id="formula_100">x = φ(h + aW z φ (h 0 )).<label>(29)</label></formula><p>By Taylor-expanding φ, we can apply a similar (though more tedious) argument as above to derive</p><formula xml:id="formula_101">Z W x = ẐW x + cZ z<label>(30)</label></formula><p>where c = a E φ (Z h1 )φ (Z h0 ) and ẐW x d = N (0, E(Z x ) 2 ). In the case of φ = id, c reduces to a as above, recovering Eq. (28). For general φ, we can immediately see that Z W x is not Gaussian because Z z = Z dx0 φ (Z h0 ) is not. In the Tensor Programs framework formalized in Section 7, cZ z is denoted ŻW x .</p><p>Similarly, coordinates distribution of dx 1 = W 1 d h1 will also change in the backward pass.</p><p>General t For general t, we obtain dynamical equations in Z identical to those in Theorem 6.3 except that Eq. ( <ref type="formula" target="#formula_88">25</ref>) and Eq. ( <ref type="formula">26</ref>) need to be modified. We state the general result below. Theorem 6.4. Consider a 2-hidden-layer MLP in µP and any training routine with learning rate 1. Suppose φ is pseudo-Lipschitz. <ref type="bibr" target="#b34">35</ref> As n → ∞, for every input ξ, f t (ξ) a.s.</p><p>--→ ft (ξ) where ft (ξ) is defined the same way as in Theorem 6.3 except that Eq. (25) should be replaced with</p><formula xml:id="formula_102">Z ht(ξ) def = ẐW xt(ξ) + ŻW xt(ξ) - t-1 s=0 χs Z d hs E Z xs Z xt(ξ) { ẐW xt(ξ) } ξ,t centered, jointly Gaussian with Cov( ẐW xt(ξ) , ẐW xs(ζ) ) = E Z xt(ξ) Z xs(ζ)</formula><p>and Eq. (26) should be replaced with</p><formula xml:id="formula_103">Z dxt def = ẐW d ht + ŻW d ht - t-1 s=0 χs Z xs E Z d hs Z d ht { ẐW d ht } t centered, jointly Gaussian with Cov( ẐW d ht , ẐW d hs ) = E Z d ht Z d hs .</formula><p>Like in Theorem 6.3, by definition, { ẐW xt(ξ) } ξ,t , { ẐW d ht } t , Z U0 , and Z nV0 are mutually independent sets of random variables.</p><p>Here, ŻW xt(ξ) def = t-1 r=0 θ r Z d hr where θ r is calculated like so: Z xt(ξ) by definition is constructed as</p><formula xml:id="formula_104">Z xt(ξ) = Φ( ẐW d h0 , . . . , ẐW d ht-1 , Z U0 )</formula><p>for some function<ref type="foot" target="#foot_27">foot_27</ref> Φ : R t+1 → R. Then</p><formula xml:id="formula_105">θ r def = E ∂Φ( ẐW d h0 , . . . , ẐW d ht-1 , Z U0 )/∂ ẐW d hr .</formula><p>Likewise, ŻW d ht def = t-1 r=0 θ r Z xr where θ r is calculated as follows: Z d ht by definition is constructed as</p><formula xml:id="formula_106">Z d ht = Ψ( ẐW x0 , . . . , ẐW xt-1 , Z V0 )</formula><p>for some function 36 Ψ : R t+1 → R. Then</p><formula xml:id="formula_107">θ r def = E ∂Ψ( ẐW x0 , . . . , ẐW xt-1 , Z V0 )/∂ ẐW xr .</formula><p>For example, generalizing Eq. ( <ref type="formula" target="#formula_100">29</ref>), for any input ξ, we have</p><formula xml:id="formula_108">Z x1(ξ) = Φ(Z W d h0 , Z U0 ), where Φ(z, u) def = φ(ξu -χ0 ξ 0 ξφ (ξ 0 u)z).</formula><p>Then</p><formula xml:id="formula_109">θ 0 = E ∂ z Φ(Z W d h0 , Z U0 ) = -χ 0 ξ 0 ξ E φ (Z h1(ξ) )φ (Z h0</formula><p>), which specializes to c in Eq. (30). Altogether,</p><formula xml:id="formula_110">ŻW x1(ξ) = -χ 0 ξ 0 ξZ d h0 E φ (Z h1(ξ) )φ (Z h0 ).</formula><p>Note that ẐW xt here does not equal Z W xt in Eq. ( <ref type="formula" target="#formula_88">25</ref>) in general, because the covariance Cov( ẐW xt , ẐW xs ) = E Z xt Z xs is affected by the presence of ŻW xr for all r ≤ max(s, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">MLP of Arbitrary Depth</head><p>The µP limit of deeper MLPs can be derived along similar logic; see Appendices H.3 to H.5 for a rigorous treatment within the Tensor Programs framework, which also covers all stable abcparametrizations.</p><p>What happens in other feature learning parametrizations If we are in the feature learning regime, then any W l that is not maximally updated (Definition 5.2) will be effectively fixed (to its initialized value) in the infinite-width limit (i.e. no learning occurs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Summary of Main Intuitions for Deriving the µP Limit</head><p>Law of Large Numbers Any vector z has roughly iid coordinates given by Z z . For any two vectors</p><formula xml:id="formula_111">z, z ∈ R n , 1 n n α=1 z α z α → E Z z Z z . 1.</formula><p>This is all we needed to derive the 1-hidden-layer dynamics of Section 6.1, since all the matrices there are size-n vectors. 2. In Sections 6.2 and 6.3, this is also used in calculating the limit of ∆W t x t . Central Limit If the underlying computation graph never involves the transpose W of a n × n Gaussian matrix W in a matrix multiplication, then W z is roughly iid Gaussian with coordinate</p><formula xml:id="formula_112">Z W z d = N (0, E(Z z ) 2 ) (if W αβ ∼ N (0, 1/n)) 1.</formula><p>This along with the last intuition are all we used to derive the 2-hidden-layer decoupled dynamics of Section 6.2, where W is the middle layer weight matrix. (W , W ) Correlation If W is involved, then W z has coordinates distributed like random variable ẐW z + ŻW z where ẐW z is the Gaussian obtained by pretending W is independent from W , and ŻW z results from the correlation between W and W . ŻW z is purely a linear combination of Z z for previously defined vectors z such that z depends on W z . 1. All three intuitions above are needed to derive the 2-hidden-layer dynamics of normal SGD (Section 6.3), where W is used in backpropagation. 2. The calculation of ŻW x is quite intricate, which is why we first discussed decoupled SGD in Section 6.</p><p>2, which doesn't need ŻW x calculation, before discussing normal SGD in Section 6.3. 𝑎. 𝑠. 𝑥 𝑊 MatMul 𝑍 𝑊𝑥 = ሶ 𝑍 𝑊𝑥 + መ 𝑍 𝑊𝑥 𝑊 iid 𝒩 0, 𝜎 𝑊 2 /𝑛 entries 𝒲 = Setup 𝑣 1 𝑣 2 𝑣 3 𝑣 𝑗 𝑍 𝒱 𝑍 𝑣 1 𝑍 𝑣 2 𝑍 𝑣 3 𝑍 𝑣 𝑗 ( ) = 𝒱 = 𝒩 0, 𝜎 𝑊 2 𝔼 𝑍 𝑥 2 Correction due to (𝑊, 𝑥) correlation 𝑥 1 𝑥 2 𝑥 3 𝑥 𝑘 𝜙( ( ( ( ( ( ) ) ) ) ) ) 𝜙 𝜙 𝜙 𝜙 𝜙 Nonlin 𝑍 𝑥 1 𝑍 𝑥 2 𝑍 𝑥 3 𝑍 𝑥 𝑘 ( ) 𝜙 𝑍 𝜙(𝑥 1 ,…,𝑥 𝑘 ) = ; ; ; ; ; ; ; ሞ 𝜃 1 ሞ 𝜃 2 ሞ 𝜃 ℓ 𝜃 1 𝜃 2 𝜃 ℓ Moment ሞ 𝜗 = 𝑥 1 𝑥 2 𝑥 3 𝑥 𝑘 𝜙( ( ( ( ( ( ) ) ) ) ) ) 𝜙 𝜙 𝜙 𝜙 𝜙 𝑍 𝑥 1 𝑍 𝑥 2 𝑍 𝑥 3 𝑍 𝑥 𝑘 ( ) 𝜙 ; ; ; ; ; ; ; ሞ 𝜃 1 ሞ 𝜃 2 ሞ 𝜃 ℓ 𝜃 1 𝜃 2 𝜃 ℓ 𝔼 Average 1 𝑛 1 𝑛 𝜗 𝒞 = ሞ 𝜗 as 𝑛 → ∞ Master Theorem 𝑛 → ∞ 𝑛 → ∞ Figure 3: Graphical overview of the Tensor Programs framework. For the Master Theorem, we illustrate Theorem 7.4(2) since Theorem 7.4(1) is a corollary of Theorem 7.4(2) for a larger program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Tensor Programs Framework</head><p>While the previous section demonstrates the intuition of how to derive the µP limit, it also lays bare 1) the increasing complexity of a manual derivation as the training goes on, as well as 2) the mounting uncertainty for whether the intuition still holds after many steps of SGD. This is a perfect call for the Tensor Programs framework, which automates (and makes rigorous) the limit derivation for any "computation graph" -including the computation graph underlying SGD. Here we review this framework (developed in Yang [49, 50, 51, 52]) in the context of µP limit. Fig. <ref type="figure">3</ref> graphically overviews the content of this section.</p><p>As seen abundantly in Section 6, the computation underlying SGD can be expressed purely via three instructions: matrix multiplication (by a Gaussian matrix, e.g. W 0 x 0 ), coordinatewise nonlinearities (e.g. φ), and taking coordinatewise average (e.g. 1 n n α=1 (nV 1 ) α x 1α ). In deriving the µP SGD limit, we focused mostly on keeping track of R n vectors (e.g. xt or dh t ), but importantly we also computed scalars f t and χ t by (what amounts to) taking coordinatewise average (e.g.</p><formula xml:id="formula_113">f 1 = 1 n n α=1 (nV 1 ) α x 1α ).</formula><p>We implicitly compute scalars as well inside ∆W t x t . This motivates the following notion of a program, which can be thought of as a low-level symbolic representation of a computation graph common in deep learning (e.g. underlying Tensorflow and Pytorch). Definition 7.1. A Tensor Program<ref type="foot" target="#foot_28">foot_28</ref> is a sequence of R n -vectors and R-scalars inductively generated via one of the following ways from an initial set C of random scalars, V of random R n vectors, and a set W of random R n×n matrices (which will be sampled with iid Gaussian entries in Setup 7.2)</p><formula xml:id="formula_114">MatMul Given W ∈ R n×n and x ∈ R n , we can generate W x ∈ R n or W x ∈ R n</formula><p>Nonlin Given φ : R k × R l → R, previous scalars θ 1 , . . . , θ l ∈ R and vectors x 1 , . . . , x k ∈ R n , we can generate a new vector φ(x 1 , . . . , x k ; θ 1 , . . . , θ l ) ∈ R n where φ(-; θ 1 , . . . , θ l ) applies coordinatewise to each "α-slice" (x 1 α , . . . , x k α ). Moment Given same setup as above, we can also generate a new scalar</p><formula xml:id="formula_115">1 n n α=1 φ(x 1 α , . . . , x k α ; θ 1 , . . . , θ l ) ∈ R.</formula><p>Explanation of Definition 7.1 The vectors mentioned in Definition 7.1 are exemplified by h t , x t , dh t , dx t in Section 6. The scalars mentioned are exemplified by f t , χ t as well as e.g. x s x t /n inside the calculating of h t (Eq. ( <ref type="formula" target="#formula_83">24</ref>)). The θ i s in Nonlin and Moment rules may appear cryptic at first. These scalars are not needed in the first forward and backward passes. But in the second forward pass, for example for the 1-hidden-layer MLP (Section 6.1),</p><formula xml:id="formula_116">x 1 = φ(h 1 ) = φ(ξ 1 U 0 -χ 0 ξ 1 ξ 0 nV 0 φ (h 0 ))</formula><p>depends on the scalar χ 0 , ξ 0 , ξ 1 , and can be written in the form of Nonlin as φ(U 0 , nV 0 , h 0 ; χ 0 ) for some φ appropriately defined.</p><p>The initial set of scalars C is the training sequence {ξ t , y t } t for all three examples of Section 6. In our 2-hidden-layer MLP examples, the initial set of matrices W is {W } (Section 6.3) or {W, W } (Section 6.2), i.e. the random R n×n Gaussian matrices. On the other hand, in the 1-hidden-layer MLP example (Section 6.1), W is empty. The initial set of vectors V in all three examples are V = {U 0 , nV 0 }. <ref type="bibr">3839</ref> Notice how the vectors of these V are sampled with iid standard Gaussian coordinates. We formalize a more general setup for arbitrary Tensor Programs: Setup 7.2. 1) For each initial W ∈ W, we sample iid W αβ ∼ N (0, σ 2 W /n) for some variance σ 2 W associated to W , independent of other W ∈ W; 2) for some multivariate Gaussian --→ θ for some deterministic θ ∈ R.</p><formula xml:id="formula_117">Z V = Z h : h ∈ V ∈ R V ,</formula><p>In all of our examples, we took σ 2 W = 1 for simplicity, but Setup 7.2 allows for other initializations (e.g. a typical initialization for relu networks is σ 2 W = 2); additionally, Z h , h ∈ V, are all standard Gaussians, independent from one another, since U 0 , nV 0 are sampled this way; and our initial scalars {ξ t , y t } t are fixed with n, so they are their own limits. <ref type="bibr" target="#b39">40</ref> What Does a Tensor Program Vector Look Like? Recall that we represented the coordinate distribution of each vector h with a random variable Z h in Section 6 and kept track of how different Zs are correlated with each other. We also calculated scalar limits like f t → ft , χ t → χt . These calculations led to a set of formulas for the µP limit (e.g. Theorems 6.1, 6.3 and 6.4). We can also construct such Z h and θ for vectors h and scalars θ in any Tensor Program. They intuitively capture the coordinate distribution of vector h and the deterministic limit of θ. The following definition formally defines Z h and θ, but the connection between Z h (resp. θ) and the coordinates of h (resp. θ) is not made rigorously until Theorem 7.4 later. The ZMatMul rule below perhaps asks for some discussion, and we shall do so after the definition. Definition 7.3 (Z h and θ). Given a Tensor Program, we recursively define Z h for each vector h and θ for each scalar θ as follows.</p><p>ZInit If h ∈ V, then Z h is defined as in Setup 7.2. We also set Ẑh def = Z h and Żh def = 0.</p><p>ZNonlin + Given φ : R k × R l → R, previous scalars θ 1 , . . . , θ l ∈ R and vectors x 1 , . . . , x k ∈ R n , we have Z φ(x 1 ,...,x k ;θ1,...,θ l ) def = φ(Z x 1 , . . . , Z x k ; θ1 , . . . , θl ).</p><p>ZMoment Given same setup as above and scalar θ = 1 n n α=1 φ(x 1 α , . . . , x k α ; θ 1 , . . . , θ l ), then</p><formula xml:id="formula_118">θ def = E φ(Z x 1 , . . . , Z x k ; θ1 , . . . , θl ).</formula><p>Here θ1 , . . . , θl are deterministic, so the expectation is taken over Z x 1 , . . . , Z x k .</p><p>ZMatMul Z W x def = ẐW x + ŻW x for every matrix W (with N (0, σ 2 W /n) entries) and vector x, where</p><p>ZHat ẐW x is a Gaussian variable with zero mean. Let V W denote the set of all vectors in the program of the form W y for some y. Then { ẐW y : W y ∈ V W } is defined to be jointly Gaussian with zero mean and covariance</p><formula xml:id="formula_119">Cov ẐW x , ẐW y def = σ 2 W E Z x Z y , for any W x, W y ∈ V W . Furthermore, { ẐW y : W y ∈ V W } is mutually independent from { Ẑv : v ∈ V ∪ W =W V W }, where W ranges over W ∪ {A : A ∈ W}. ZDot We can always unwind Z x = Φ(• • • ), for some arguments (• • • ) = ({ ẐW y i } k i=1 , { Ẑz i } j i=1 ; { θi } l i=1 ), z i ∈ V W (where V W is defined in ZHat), and deterministic function Φ : R k+j+l → R. Define ∂Z x /∂ ẐW y i def = ∂ i Φ(• • • ). Then we set ŻW x def = σ 2 W k i=1 Z y i E ∂Z x ∂ ẐW y i ,<label>(31)</label></formula><p>There is some nuance in this definition, so see Remark F.1 and F.2.</p><p>Explanation of Definition 7.3 Nonlin and Moment should appear only natural. However, we pause to digest the meaning of ZMatMul by relating back to our examples in Section 6. First notice that ŻW x = 0 if W is not used in the program, so that Z W x = ẐW x . This is the case in Section 6.2, where W is used in backprop instead of W . There (in Eq. ( <ref type="formula" target="#formula_88">25</ref>)),</p><formula xml:id="formula_120">Z W xt is Gaussian with covariance Cov(Z W xt , Z W xs ) = E Z xt Z</formula><p>xs for any s, consistent with ZHat. In Section 6.3, however, ŻW x = 0 in general. The ZDot rule is a direct generalization of the calculation of Ż in Theorem 6.4. ŻW xt and ŻW d ht of Section 6.3 for general t will all be nonzero but have no easy expression. Here we seek to convey the complexity of computing them; this is optional reading for the first time reader. To calculate ŻW xt ( ŻW d ht is similar), we need to express Z xt as a function of purely ẐW d hs , s &lt; t, and Z U0 = ẐU0 . Then we symbolically differentiate Z xt by ẐW d hs and take expectation to obtain the coefficient of Z d hs in ŻW xt . For t = 1 as in the examples in Section 6.3, this task is easy because ẐW d h0 = Ẑdx0 = Z dx0 . But in general, the calculation can balloon quickly. Indeed, note Z xt = φ(Z ht ) and</p><formula xml:id="formula_121">Z ht = ξ t Z Ut = ξ t Z U0 -ξ t t-1 s=0 χs ξ s Z dhs = ξ t Z U0 -ξ t t-</formula><p>1 s=0 χs ξ s φ (Z hs )Z dxs . However, each Z dxs is a linear combination of Z W d hs = ẐW d hs + ŻW d hs and Z xr , r &lt; s (coming from ∆W t d hs ). Each of ŻW d hs and Z xr then needs to be recursively expanded in terms of Ẑ before we can calculate the symbolic partial derivative ∂Z xt /∂ ẐW d hs . Algorithm 1 Compute the infinite-width limit of an NN in any abc-parametrization and any task 1: Write the computation graph underlying training and inference in a Tensor Program (akin to writing low level PyTorch or Tensorflow code). 2: Calculate Z h for each vector h and θ for each scalar θ in the program, according to Definition 7.3. 3: The logits f t (ξ) of the neural network at any time t should be written as a collection of scalars, so ft (ξ) is calculated in the previous step. For t being inference time, ft (ξ) is the output of the infinite-width network after training. Master Theorem Finally, we relate the symbolic nature of a Tensor Program given in Definition 7.3 to the analytic limit of its computation, in the following Master Theorem. Pseudo-Lipschitz functions are, roughly speaking, functions whose (weak) derivatives are polynomially bounded. We state the theorem assuming mild regularity conditions (Assumption F.4) that roughly says most nonlinearities in the program should be pseudo-Lipschitz. Theorem 7.4 (Tensor Program Master Theorem, c.f. Theorem E.15 of [52]). Fix a Tensor Program initialized accordingly to Setup 7.2. Adopt Assumption F.4. Then 1. For any fixed k and any pseudo-Lipschitz ψ : R k</p><formula xml:id="formula_122">→ R, as n → ∞, 1 n n α=1 ψ(h 1 α , . . . , h k α ) a.s. --→ E ψ(Z h 1 , . . . , Z h k ),<label>(32)</label></formula><p>for any vectors h 1 , . . . , h k in the program, where Z h i are as defined in Definition 7.3.</p><p>2. Any scalar θ in the program tends to θ almost surely, where θ is as defined in Definition 7.3.</p><p>Intuitively, Theorem 7.4(1) says that each "coordinate slice" (h 1 α , . . . , h k α ) can be thought of as an iid copy of (Z h 1 , . . . , Z h k ). <ref type="bibr" target="#b40">41</ref> This intuition is consistent with our heuristic derivation in Section 6, and Theorem 7.4 underlies the proof of Theorems 6.1, 6.3 and 6.4. Theorem 7.4(2) allows us to directly obtain the function learned at the end of training: For example, for a 1-hidden-layer MLP, it shows that the network's output on any input ξ at time t converges to ft (ξ) given in Theorem 6.1.</p><p>Algorithm 1 summarizes how to compute the infinite-width limit of any network in any abcparametrization and for any task, using the Tensor Programs framework laid out in this section. It generalizes the manual derivations of Section 6. We carry out Algorithm 1 for MLPs in all of our experiments.</p><p>Architectural and algorithmic universality Given that Tensor Programs can express the first forward and backward computation of practically any architecture [49, 51], it should perhaps come as no surprise that they can also express practically any training and inference procedure -or just any computation -involving any such architecture. This includes both feature learning and kernel limits. We leverage this flexibility to derive and compute the µP and kernel limits for metalearning and Word2Vec; see Section 9.</p><p>Extensions We focused on programs whose vectors all have the same dimension n here. But it's easy to generalize to the case where vectors have different dimensions, which corresponds to e.g. when a network's widths are non-uniform. See [52].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Computational Considerations</head><p>While the TP framework is very general, computing the feature learning limits analytically is inherently computationally intensive aside from special cases like the linear 1-hidden-layer MLP (Corollary 6.2). Here we explain why, so as to motivate our experimental choices below.</p><p>No closed-form formula for evaluating the expectations (e.g. in Eq. ( <ref type="formula" target="#formula_122">32</ref>)) involving general nonlinearities except in special cases For example, for a 1-hidden-layer MLP (Section 6.1), after 1 step of SGD, the logit is of the form E(Z 1 + bφ(Z 2 ))φ(Z 3 + cZ 1 φ (Z 2 )) where Z i s denote different (correlated) Gaussians (Eq. ( <ref type="formula" target="#formula_60">17</ref>)). While one can still evaluate this via Monte-Carlo, the error will compound quickly with training time. On the other hand, because of the nesting of φ inside φ, there is no closed-form formula for this expectation in general.</p><p>Notable Exception: If the nonlinearity φ is polynomial, then the expectation is a polynomial moment of a multivariate Gaussian and can be evaluated analytically, e.g. using Isserlis' theorem from the covariance matrix.</p><p>Even with nonlinear polynomial φ, there is exponential computational bottleneck As training time t increases, due to the nesting of φ and φ in the preactivations, the integrand of the expectation, e.g. E Z xt Z nVt , will turn out to be a polynomial in Ω(1) Gaussian variables with degree Ω(2 t ). The covariance matrix of the Gaussian variables will in general be nontrivial, so evaluating the expectation, e.g. using Isserlis' theorem, requires super-exponential time. This is because we would need to expand the polynomial integrand into monomials, and there would be Ω(2 t ) monomials, each of which require Ω(2 t ) time to evaluate using Isserlis' theorem.</p><p>n × n Gaussian matrices Both points above apply to 1-hidden-layer MLPs. Additional difficulties with deeper networks is caused by the n × n initial Gaussian matrix W l 0 , 2 ≤ l ≤ L, in the middle of the network. 1) In general, due to the nonlinearities, x l-1 t would be linearly independent from x l-1 s for all s &lt; t. Therefore, in calculating</p><formula xml:id="formula_123">W l t x l-1 t = W l 0 x l-1 t + ∆W l t x l-1 t , we create a new Gaussian variable ẐW l 0 x l-1 t</formula><p>linearly independent from all previous ẐW l 0 x l-1 s , s &lt; t. This then requires us to compute and store the covariance between them. Thus, t steps of SGD costs Ω(t 2 ) space and time (not mentioning that the computation of each covariance entry can require exponential time, as discussed above). 2) In addition, due to the interaction between W l t in the forward pass and W l t in the backward pass, there is nonzero Ż, as demonstrated in Eq. ( <ref type="formula" target="#formula_101">30</ref>). This Ż is generally a linear combination of Ω(t) terms, and the coefficients of this combination require evaluation of some expectations that typically run into the exponential bottleneck discussed above.</p><p>Summary From easiest to hardest in terms of µP limit's computational cost, we have 1) 1-hiddenlayer linear networks; 2) L-hidden-layer linear MLP, L ≥ 2; 3) nonlinear MLP with polynomial activations; 4) nonlinear MLP with nonpolynomial activations. Nevertheless, 1-hidden-layer linear networks are more than sufficient to demonstrate feature learning in Word2Vec and few-shot learning with MAML, as we show below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Experiments</head><p>In light of the computational difficulties discussed above, we divide our experiments into two groups: 1) Verifying our theory; 2) Scaling up to realistic datasets to demonstrate feature learning. The experiments in group 1 focus on stress-testing our theory in many scenarios to show that it describes empirical phenomena accurately. They will run into the discussed computational difficulties (Section 8), so we cannot train the infinite-width µP networks for very long, but nevertheless long enough to verify the theory. Those in group 2 focus on real datasets (metalearning and Word2Vec) where feature learning is critical, and demonstrate that the GP and NTK limits are inadequate for those tasks. Necessarily, we adopt simpler neural architectures for this purpose so we can scale up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Verifying the Theory</head><p>In Fig. <ref type="figure" target="#fig_4">4</ref>, we analytically computed the µP limits derived in Section 6 for quadratic and linear activations, and verified them against finite width networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Few-Shot Learning on Omniglot via First Order MAML</head><p>In few-shot learning, the model is given only a small number of labeled examples before asking to make predictions on unseen data. Therefore, this tests whether a model contains a good prior that can adapt quickly to the small amount of data at hand. We analytically compute the infinite-width µP limit for the three kinds of networks (depth 1, depth 2 decoupled, depth 2) described in Section 6, with either quadratic φ(x) = x 2 or linear φ(x) = x activation. The training set is random ξ t ∈ {±1}, y t ∈ {±1}, so that the deviation of finite width from infinite width losses are accentuated. We compare against finite width µP networks with width 1024 or 4096. For each width, we randomly initialize with 100 different seeds and aggregate the loss curves. The mean across these seeds is plotted as solid curves, and the standard deviation represented by the shade. As discussed in Section 8, nonlinear activation functions and higher depth face computational difficulties exponential with training time. Thus here we only train for a few steps. We observe that the quadratic network converges slower to the limit with width. This is expected since the tail of Z xt is fatter for a quadratic activation than a linear activation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAML</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Repeat</head><p>In practice, we batch the tasks, just like batches in SGD, so that we accumulate all the gradients from</p><p>Step 5 and update θ only at the end of the batch.</p><p>During meta-test time, we are tested on random unseen few-shot tasks, where each task T provides a training set D and a test set D as during meta-training. We adapt to D as in Step 3 above (or more generally we can take multiple gradient steps to adapt better) to obtain adapted parameters θ . Finally, we calculate the accuracy of θ on the test set D. We average this accuracy over many tasks T , which we report as the meta-test accuracy.</p><p>First Order vs Second Order MAML Notice in Step 5, we take the gradient of L D (f θ ) with respect to the adapted parameters θ . In Second Order MAML, we would instead take the gradient against the unadapted parameters θ, which would involve the Hessian ∇ θ ∇ θ L D (f θ ). Second Order MAML generally achieves performance slightly better than First Order MAML, but at the cost of significantly slower updates [37]. In order to scale up, we will focus on First Order MAML, hereafter referred to as just MAML. Omniglot Omniglot is a standard few-shot learning benchmark. It consists of 20 instances of 1623 characters from 50 different alphabets, each handwritten by a different person. We test our models on 1-shot 5-way classification: We draw 5 random characters, along with 1 training instance and 1 test instance for each character. After the model adapts to the training instances, it's asked to predict the character of the test instances (choosing among the 5 characters).</p><p>Models Our main model is the µP limit of a 1-hidden-layer linear MLP. We compare against: 1) finite width versions of the same; <ref type="bibr" target="#b41">42</ref> 2) the NNGP and NTK limits of the same; 3) the NNGP and NTK limits of a 1-hidden-layer relu MLP. Note 2) is equivalent to a 0-hidden-layer perceptron, because the NNGP and NTK there are both linear kernels. In addition, the infinite-width SP limit of a 1-hidden-layer network is the same as the NNGP limit. Both 2) and 3) are equivalent to linear models with fixed (not learned) features, so MAML's adaptation only applies to the linear weights. On the other hand, the µP limit and the finite µP networks will learn new representations of the data over time that can quickly adapt to new tasks. <ref type="bibr" target="#b42">43</ref> Hyperparameters We use (task) batch size 32 and adaptation step size 0.4 ( in Step 3). We also clip the gradient in Step 5 if the gradient has norm ≥ 0.5. <ref type="bibr" target="#b43">44</ref> For each model, we tune its weight initializaton variances and the meta learning rate (η in Step 5). During meta-test time, we take 20 gradient steps during adaptation (i.e. we loop Step 3 above 20 times to obtain θ ). See Appendix D.1 for more details.  <ref type="table" target="#tab_4">2</ref>, where curves indicate means and shades indicate standard deviations. There are three key takeaways:</p><p>1) The feature learning µP limit significantly outperforms the kernel limits.</p><p>2) The benefit of feature learning dominates the benefit of having nonlinearities. 3) As width increases, the finite µP networks approach the performance of the µP limit from below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Word2Vec</head><p>Word2Vec [32, 33] is an early example of large-scale pretraining and transfer learning in natural language processing, where one learns a feature vector h(ξ) for every word ξ based on the principle of distributional semantics. For simplicity, we focus on a specific scheme of Word2Vec using context as a bag-of-word (CBOW), negative example sampling, and Sigmoid loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word2Vec Pretraining</head><p>Consider training on a corpus with vocabulary V. At each time step, we sample a sentence for the corpus and choose a word i ∈ V. This word's context J ⊆ V is a window of words around it in the sentence, thought of as a bag of words. Let ξ i ∈ R |V| be the one-hot vector corresponding to word i. We pass the averaged context ξ J def = 1 |J| n j∈J ξ j through a 1-hidden-layer MLP with hidden size n and identity activation:</p><formula xml:id="formula_124">f (ξ J ) = V h(ξ J ) ∈ R |V| , h(ξ J ) = U ξ J ∈ R n ,<label>(33)</label></formula><p>where</p><formula xml:id="formula_125">V ∈ R |V|×n , U ∈ R n×|V| factor as V = n -av v, U = n -au u with initialization v α ∼ N (0, n -2bv ), u α ∼ N (0, n -2bu</formula><p>), where {a v , b v , a u , b u } specify the parametrization of the network.</p><p>After each forward pass, we sample a target word τ from V: with probability p, we take τ = i; with probability 1 -p, we sample τ uniformly from V \ {i}. Following [32, 33], we take p = 1/21 ≈ 4.76%. The loss is then calculated with the Sigmoid function σ(•) :</p><formula xml:id="formula_126">L(f (ξ J ), ξ τ ) = log(1 -σ(f (ξ J ) ξ τ )) τ = i log σ(f (ξ J ) ξ τ ) τ = i<label>(34)</label></formula><p>Then v and u are updated via SGD as usual (causing V and U to update). Conventionally, h(ξ) ∈ R n is taken as the Word2Vec embedding for a word ξ after many iterations of forward-backward updates.</p><p>Word Analogy Evaluation We evaluate the word embeddings h(ξ) with the word analogy task. This task asks the question of the kind: What to a 'queen' is as a 'man' to a 'woman'? (answer is 'king'). The Word2Vec model answers this question by computing</p><formula xml:id="formula_127">argmax i h(ξ i ) (h(ξ 'man' ) -h(ξ 'woman' ) + h(ξ 'queen' ))<label>(35)</label></formula><p>where i ranges over V \ {'man', 'woman', 'queen'}. If the argmax here is i = 'king', then the model answers correctly; otherwise, it's incorrect. The accuracy score is the percentage of such questions answered correctly.</p><p>Dataset We train the models on text8,<ref type="foot" target="#foot_36">foot_36</ref> a clean dataset consisting of the first 100 million characters of a 2006 Wikipedia dump. The dataset has been featured in the original Word2Vec codebase and the Hutter Prize. text8 contains the first 100 million characters of fil9, a larger dataset obtained by filtering the first 1 billion characters in the aforementioned Wikipedia dump. We space-separate the datasets into tokens and keep ones that appear no less than 5 times in the entire dataset for text8 and 10 times for fil9. The resulting datasets have 71,291 and 142,276 unique vocabulary items.</p><p>Models Our main model is the µP limit of Eq. (33). We compare against the baselines of 1) finitewidth versions of the same, and 2) the NTK and GP limits of Eq. (33). As shown in Corollary 3.9, the features of the NTK limit are fixed at initialization as n → ∞ (and so are those of the GP limit, by definition), so its answer to Eq. ( <ref type="formula" target="#formula_127">35</ref>) is uniformly selected from the whole vocabulary. <ref type="bibr" target="#b45">46</ref> Its accuracy is thus 1 |V|-3 . Since |V| is 71,291 for text8 and 142,276 for fil9, this number is practically 0. We compute the µP limit according to Algorithm 1, but we relate more implementation details in Appendix D.2. Findings We show our results in Table <ref type="table" target="#tab_5">3</ref> and Figure to the right. As expected, the infinite-width and finitewidth µP networks significantly outperform the NTK limit. In addition, we observe the finite width µP networks converge to the performance of the µP limit from below, as width increases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>In this paper, we presented a framework, based on the notion of abc-parametrizations and Tensor Programs technique, that unifies the Neural Tangent Kernel (NTK) and Mean Field limits of large width neural networks (NNs). In the Dynamical Dichotomy theorem, we classified the abc-parametrizations into feature learning and kernel regimes. We identified the lack of feature learning as a fatal weakness of NTK as a model for real NN. In fact, we showed the standard parametrization suffers from the same problem. As a solution, we proposed the Maximal Update Parametrization (µP) and derived its infinite-width limit, which admits feature learning. Through experiments on Word2Vec and few-shot learning, we demonstrated that µP is a good model for feature learning behavior in neural networks.</p><p>More generally, this paper showcased the power of the Tensor Programs technique: Any computation expressable in a Tensor Program has a "infinite-width" limit we can derive. Because of the universality of Tensor Programs for expressing deep learning computation [49, 51], this technique systematically solves the mathematical problem of taking infinite-width limits which has been dealt with haphazardly in prior literature. Its immense flexibility means that the theory of reinforcement learning, selfsupervised learning, deep generative models, etc with overparametrized neural networks in the feature learning regime are now ripe for the picking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A A Short Origin Story of the Tensor Programs Paper Series</head><p>The Tensor Programs framework was initially proposed in [50] in February 2019, and was mainly applied to extend the NNGP and NTK limits to arbitrary architectures (and to make rigorous the signal propagation literature [9, 18, 20-22, 40-42, 44, 53-56]). While NNGP and NTK amount to taking limits of neural networks at initialization, it was soon, in April 2019, realized that Tensor Programs could 1) also trivially take limits of the entire training procedure of neural networks (which is the main theoretical idea of this paper), and 2) calculate the feature learning limit. However, at that point, it also became clear that [50] was not written accessibly, and its formulation of Tensor Programs was cumbersome to use. A question had to be asked: Should the feature learning paper be written immediately on such an unwieldy foundation, or should significant effort be devoted to fixing this foundation first? Eventually, a decision was made in favor of the latter. The Tensor Programs series was created as way to re-organize and re-present the Tensor Programs machinery in a user-friendly way to the machine learning audience (the first 3 papers [49, 51, 52] of the series), before extracting payoffs from this foundation (starting from this paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Discussions on the Shallow NTK and MF Examples</head><p>How does the Function Change? If the NTK limit does not allow features to evolve, then how does learning occur? To answer this question, note</p><formula xml:id="formula_128">∆f t (ξ) = V 0 ∆x t (ξ) + ∆V t x 0 (ξ) + ∆V t ∆x t (ξ).</formula><p>In short, then, the evolution of f t (ξ) in the NTK limit is predominantly due to V 0 ∆x t (ξ) and ∆V t x 0 (ξ) only, while in the MF limit, ∆V t ∆x t (ξ) also contributes nontrivially.</p><p>Example:</p><formula xml:id="formula_129">For t = 1, ∆f 1 (ξ) = V 0 ∆x 1 (ξ) + n -2av x 0 x 0 (ξ) + n -2av x 0 ∆x 1 (ξ). In NTP, a v = 1/2, so the term n -2av x 0 x 0 (ξ) = Θ(1) for generic ξ, ξ 0 . On the other hand, n -2av x 0 ∆x 1 (ξ) = O(1/ √ n) because ∆x 1 (ξ) = O(1/ √ n) as noted above. Likewise, V 0 ∆x 1 (ξ) ≈ V 0 [φ (h 0 (ξ)) ∆h 1 (ξ)] = V 0 [φ (h 0 (ξ)) ∆h 1 (ξ)] = C n α=1 V 0α φ (h 0 (ξ) α )V 0α φ (h 0α ) = C n α=1 (V 0α ) 2 φ (h 0 (ξ) α )φ (h 0α ),</formula><p>where C = χ 0 ξ 0 ξ = Θ(1). Now (V 0α ) 2 = Θ(1/n) and is almost surely positive. On the other hand, φ (h 0 (ξ) α )φ (h 0α ) = Θ(1) and should have a nonzero expectation over random initialization (for example, if φ is relu then this is obvious). Therefore, the sum above should amount to V 0 ∆x 1 (ξ) ≈ Θ(1). In summary, in the NTK limit, ∆f 1 (ξ) = Θ(1) due to the interactions between V 0 and ∆x 1 (ξ) and between ∆V 1 and x 0 (ξ), but there is only vanishing interaction between ∆V 1 and ∆x 1 (ξ).</p><p>The case for general t, again, can be derived easily using Tensor Programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C abc-Parametrization for General Neural Architectures</head><p>We can straightforwardly generalize abc-parametrizations to an arbitrary neural architecture. Each parameter tensor W would get its own a W and b W , such that W = n -a W w and w is the actual trainable parameter with initialization w αβ ∼ N (0, n -2b W ). The learning rate is still ηn -c for some fixed η. General Neural Architectures More generally, µP can be defined easily for any neural architecture whose forward pass can be written down as a Tensor Program (e.g. ResNet or Transformer; see [49] for explicit programs). The learning rate is always independent of width, i.e. c = 0. For any parameter tensor W , b W is always 1/2, and a W can be defined as follows: If W is not an output weight matrix, then a W should be set to -1 + 1 2 p W , where p W = lim n→∞ log n #(W ) is a) 0 if both sides of W are fixed w.r.t. n; b) 1 if W is a vector (e.g. bias) or with one side being fixed dimensional (e.g. W 1 ); and c) 2 if W is a matrix with both sides scaling like n (e.g. weights in the middle of an MLP). If W is an output weight matrix (and thus the output dimension is fixed w.r.t. n), then a W should be 1 2 . If W is an output bias, then a W should be 0.</p><p>Optimality Properties One can formalize, in this general context, the notion of stability and the notions of a parameter tensor being updated maximally and (a set of readout weights) being initialized maximally. Then one can show that µP is the unique stable abc-parametrization such that all of its parameter tensors are updated maximally and all of its readout weights are initialized maximally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head><p>The main models in our experiments are all 1-hidden-layer linear MLPs with input dimension d and output dimension d o . In our experiments, we will consider more advanced forms, but, as warmup, a basic version of such a network is given by</p><formula xml:id="formula_130">f (ξ) = V h(ξ), h(ξ) = U ξ,<label>(36)</label></formula><p>for</p><formula xml:id="formula_131">U ∈ R n×d , V ∈ R do×n parametrized like U = √ nu, V = 1</formula><p>√ n v and with initialization u αβ , v αβ ∼ N (0, 1/n). In this case, Corollary 6.2 generalizes to Theorem D.1. Consider a 1-hidden-layer linear MLP in µP (Eq. ( <ref type="formula" target="#formula_130">36</ref>)) and any training routine with learning rate η. As n → ∞, for every input ξ ∈ R d , f t (ξ) ∈ R do converges almost surely to ft (ξ) defined as follows:</p><formula xml:id="formula_132">ft (ξ) = (A t C t + B t D t )ξ ∈ R do , χt = L ( ft , y t ) ∈ R do , (A t+1 , B t+1 ) = (A t , B t ) -ηχ t ⊗ (C t ξ t , D t ξ t ), (C t+1 , D t+1 ) = (C t , D t ) -η(A t χt , B t χt ) ⊗ ξ t ,</formula><p>where ⊗ denotes outer product (u ⊗ v = uv ), with initial condition</p><formula xml:id="formula_133">A 0 = I do ∈ R do×do , D 0 = I d ∈ R d×d , B 0 = 0 ∈ R do×d , C 0 = 0 ∈ R d×do .</formula><p>While we will not use this theorem, we intend it to give an idea of the mathematical process underneath our implementations, which we discuss now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Few-Shot Learning on Omniglot via MAML D.1.1 Linear 1-Hidden-Layer µP Network</head><p>We consider a linear 1-hidden-layer MLP with bias, input dimension d, output dimension d o , given by</p><formula xml:id="formula_134">f (ξ) = V h(ξ) ∈ R do , h(ξ) = U ξ + B ∈ R n , where ξ ∈ R d . Following µP, we factor U = √ nu ∈ R n×d , V = 1 √ n v ∈ R do×n , B = α √ nβ ∈ R n , where u, v, β are the trainable parameters. We initialize u αβ ∼ N (0, σ 2 u /n), v αβ ∼ N (0, σ 2 v /n), β = 0 ∈ R n . We can cancel the factors of √ n and rewrite f (ξ) = vh(ξ) ∈ R do , h(ξ) = uξ + b ∈ R n ,</formula><p>where b = αβ. We will also consider gradient clipping with threshold g and weight decay with coefficient γ. So in summary, the hyperparameters are σ u , σ v (init. std.), α (bias multiplier), η (LR), g (grad. clip), γ (weight decay).</p><p>As in Corollary 6.2, it's easy to see that each column of u t at any time t is always a linear combination of the columns of u 0 and the rows of v 0 such that the coefficients of these linear combinations converge deterministically in the n → ∞ limit; likewise for b t and the rows of v t . To track the evolution of f , it suffices to track these coefficients. Therefore, for implementation, we reparametrize as follows:</p><p>Coefficient matrix and vector Let µ 1 , . . . , µ d , ν 1 , . . . , ν do ∈ R n be standard Gaussian vectors such that the columns of u 0 will be initialized as σ u µ 1 / √ n, . . . , σ u µ d / √ n and the rows of V 0 will be initialized as</p><formula xml:id="formula_135">σ v ν 1 / √ n, . . . , σ v ν do / √ n. Write µ = (µ 1 , . . . , µ d ) ∈ R n×d , ν = (ν 1 , . . . , ν do ) ∈ R n×do . Define coefficient matrices u ∈ R d×(d+do) , v ∈ R do×(d+do) , such that at any time, (u, v ) ∈ R n×(d+do) is 1 √ n (µ, ν)(u, v ) in the infinite-width limit. We initialize u v ← σ u I 0 0 σ v I ,</formula><p>i.e. a "diagonal" initialization. Likewise, define coefficient vector b ∈ R d+do , initialized at 0, such that, at any time, b is approximately distributed as</p><formula xml:id="formula_136">1 √ n (µ, ν)b.</formula><p>To track the evolution of the infinite-width network, we will track the evolution of u, v, b.</p><p>In general, we use bold to denote the coefficients (in µ, ν) of a tensor (e.g. b for coefficients of b). We also use capital letters to denote the batched version (e.g. H for batched version of h). Algorithms 2 and 3 below summarize the SGD training of the finite-and the infinite-width networks. Note that aside from initialization and the hidden size (n vs d + d o ), the algorithms are essentially identical.</p><p>Algorithm 2 SGD Training of Finite-Width Linear µP 1-Hidden-Layer Network Input: Hyperparameters n, σ u , σ v , α, η, g, γ.</p><p>1: Initialize</p><formula xml:id="formula_137">u αβ ∼ N (0, σ 2 u /n) 2: Initialize v αβ ∼ N (0, σ 2 v /n) 3: Initialize b ← 0 4: for each batch of inputs Ξ ∈ R B×d and la- bels Y ∈ R B×do do 5:</formula><p>// Forward Pass 6:</p><formula xml:id="formula_138">H ← Ξu + b ∈ R B×n 7: f (Ξ) ← Hv ∈ R B×do 8:</formula><p>// Backward Pass</p><formula xml:id="formula_139">9: χ ← L (f (Ξ), Y ) ∈ R B×do 10: du ← -v χ Ξ ∈ R n×d 11: dv ← -χ H ∈ R do×n 12: db ← -α 2 1 χv ∈ R n 13:</formula><p>// Gradient Clipping 14:</p><formula xml:id="formula_140">G ← du 2 F + dv 2 F + db α<label>2 15:</label></formula><p>ρ ← min(1, g/G)</p><p>16: du ← ρdu 17: dv ← ρdv 18: db ← ρdb 19: // Gradient Step w/ Weight Decay 20:</p><formula xml:id="formula_141">u += ηdu -ηγu ∈ R d×n 21: v += ηdv -ηγv ∈ R do×n 22: b += ηdb -ηγb ∈ R n 23: end for Algorithm 3 SGD Training of Infinite-Width Lin- ear µP 1-Hidden-Layer Network Input: Hyperparameters σ u , σ v , α, η, g, γ. 1: Initialize u ← (σ u I, 0) 2: Initialize v ← (0, σ v I) 3: Initialize b ← 0 4: for each batch of inputs Ξ ∈ R B×d and la- bels Y ∈ R B×do do 5:</formula><p>// Forward Pass 6:</p><formula xml:id="formula_142">H ← Ξu + b ∈ R B×(d+do) 7: f (Ξ) ← Hv ∈ R B×do 8:</formula><p>// Backward Pass</p><formula xml:id="formula_143">9: χ ← L (f (Ξ), Y ) ∈ R B×do 10: du ← -v χ Ξ ∈ R (d+do)×d 11: dv ← -χ H ∈ R do×(d+do)</formula><p>12:</p><p>db ← -α 2 1 χv ∈ R d+do 13:</p><p>// Gradient Clipping 14:</p><formula xml:id="formula_144">G ← du 2 F + dv 2 F + db α<label>2 15:</label></formula><p>ρ ← min(1, g/G)</p><p>16: du ← ρdu 17: dv ← ρdv 18: db ← ρdb 19: // Gradient Step w/ Weight Decay 20:</p><formula xml:id="formula_145">u += ηdu -ηγu ∈ R (d+do)×d 21: v += ηdv -ηγv ∈ R do×(d+do)</formula><p>22:</p><p>b += ηdb -ηγb ∈ R d+do 23: end for During inference, we just run the Forward Pass section with Ξ substituted with test data.</p><p>The algorithms for MAML can then be obtained by a straightforward modification of these algorithms. (Note that in MAML, we do not clip gradients during adaptation, but rather clip the gradient against the validation loss of task; we also disable weight decay by setting the coefficient γ to 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Sweep</head><p>We sweep σ u , σ v , η and α with the following grid for finite width and µP networks. for each task in batch do for each input/label pair (ξ i , y i ) ∈ D do 8:</p><formula xml:id="formula_146">χ i ← L (f Q (ξ i ), y i ) 9:</formula><p>end for 10:</p><p>for each input/label pair (ξ i , y i ) ∈ D do 11:</p><p>Q.push((ξ i , -χ i ))</p><p>12:</p><p>end for</p><p>13: // Calculate Test Set Gradient 14: Sample test set D 15:</p><p>for each input/label pair ( ξi , ŷi ) ∈ D do</p><formula xml:id="formula_147">16: χi ← L (f Q ( ξi ), ŷi ) 17:</formula><p>end for 18:</p><p>for each input/label pair (ξ i , y i ) ∈ D do</p><formula xml:id="formula_148">19: Q.pop((ξ i , -χ i )) 20:</formula><p>end for 21:</p><formula xml:id="formula_149">// Gradient Clip 22: G ← ( ξi,ŷi)∈ D ( ξj ,ŷj )∈ D χi χj K( ξi<label>, ξj ) 23: ρ ← min(1, g/G) 24:</label></formula><p>// Gradient Update 25:</p><p>for each input/label pair ( ξi , ŷi ) ∈ D do 26:</p><p>Q.push(( ξi , -ρη χi ))</p><p>27:</p><p>end for 28:</p><p>end for 29: end while</p><formula xml:id="formula_150">• σ v : [2 -5 , 2 -4 , 2 -3 , 2 -2 , 2 -1 ],</formula><p>• η : [0.025, 0.05, 0.1, 0.2, 0.4],</p><p>• α : [0.25, 0.5, 1,</p><p>We are interested in 1-shot, 5-way learning with Omniglot. This means that each task provides 5 training samples, each corresponding to one of the 5 labels of the task. Each hyperparameter combination above is used to train for 100 epochs over 3 random seeds, where each epoch consists of 100 batches of 32 tasks. We average the validation accuracy across the last 10 epochs and document the best hyperparameters in Table <ref type="table" target="#tab_9">4</ref>, along with the test accuracy from a 15-seed rerun <ref type="bibr" target="#b46">47</ref> for better benchmarking. For NTK and GP, we additionally tune the initialization σ b for biases, which is set to 0 for both finite and µP networks for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 NNGP and NTK for Relu Networks</head><p>Consider a kernel K, which in our case will be the NNGP or NTK of a 1-hidden-layer relu network. WLOG, it is induced by an embedding Φ such that K(ξ, ζ) = Φ(ξ), Φ(ζ) where , is the inner product in the embedding space; we do not care about the details of Φ or , as eventually our algorithm only depends on K.</p><p>In our setting, we will train a linear layer W on top of Φ via MAML, f (ξ) def = W, Φ(ξ) . One can see easily that W is always a linear combination of Φ(ζ) for various ζ from the training set we've seen so far. Thus, to track W , it suffices to keep an array Q of pairs (ζ, q) such that W = (ζ,q)∈Q qΦ(ζ) <ref type="bibr" target="#b46">47</ref> After excluding outliers at least one standard deviation away from the mean. </p><formula xml:id="formula_152">f Q (ξ) = (ζ,q ζ )∈Q q ζ K(ζ, ξ).</formula><p>In our case, the number of possible inputs is too large to instantiate a value q for every ζ, so we gradually grow a dynamic array Q, which we model as a stack. Then MAML can be implemented as in Algorithm 4.</p><p>Hyperparameter Sweep We sweep σ u , σ v , σ b and η with the following grid for GP and NTK.</p><p>• σ u : [0.25, 0.</p><p>• σ v : [0.25, 0.</p><p>• σ b : [0.25, 0.</p><p>• η : [0.05, 0.1, 0.2, 0.4, 0.8] Each hyperparameter combination above is used to train for 5 epochs (the first epoch is almost always the best) over 3 random seeds, where each epoch consists of 100 batches of 32 tasks. We take the validation accuracy among all epochs and document the best hyperparameters in Table <ref type="table" target="#tab_9">4</ref>, along with the test accuracy from a 15-seed rerun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Word2Vec Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 µP Limit</head><p>We shall derive the training algorithm for µP Word2Vec. First, we introduce the notation for word embeddings. We denote Φ i def = h(ξ i ). If ξ i is a one-hot vector with the i th element set to 1, Φ i is essentially the i th column of the weight matrix U . We also define the following short-hands for the context embedding:</p><formula xml:id="formula_156">Φ J def = E j∈J Φ j = h(ξ J ).</formula><p>Similarly, V ξ τ describes a row in V ; we can define Φ τ def = ĥ(ξ τ ) def = V ξ τ and rewrite the loss function.</p><formula xml:id="formula_157">L(f (ξ J ), ξ τ ) = log(1 -σ(Φ J Φ τ )) τ = i log σ(Φ J Φ τ ) τ = i.<label>(37)</label></formula><p>Consequently, the backward pass becomes:</p><formula xml:id="formula_158">∆Φ j = 1 |J| ∆Φ J = η |J| ∂L ∂Φ J = η |J| Φ τ (1 -σ(Φ J Φ τ )) τ = i -η |J| Φ τ σ(Φ J Φ τ ) τ = i.<label>(38)</label></formula><p>Following µP, we initialize U αβ ∼ N (0, σ u n -1 ) and V αβ ∼ N (0, σ v n -1 ), where n is the width of the finite network. (Here the explicit multipliers of √ n in U and 1/ √ n in V cancel out because the network is linear). The tunable hyperparameters are the initialization std σ u and σ v , learning rate η and weight decay ratio γ. Rather than tuning the hyperparameters extensively for each width, we pick some reasonable values and use them for all of our experiments. Specifically, we have σ u = σ v = 1, η = 0.05 and γ = 0.001.</p><p>Again, using Corollary 6.2, we can train the µP limit in the coefficient space of u ∈ R |V|×2|V| , v ∈ R |V|×2|V| , with the same "diagonal" initialization:</p><formula xml:id="formula_159">u v ← σ u I 0 0 σ v I ,</formula><p>We can adopt the embedding notation and represent a row of u with the embedding coefficient vector Φ • and a column of v with Φ •. This is computationally equivalent to training with a hidden size of 2|V| and with embeddings initialized as rows (or columns) of one-hot vectors. The full algorithm is described in Algorithm 2 and Algorithm 3; in this case, we remove biases and use weight decay with coefficient γ = 0.001. After training, rows of the weight matrix u (resp. coefficient matrix u), i.e. Φ • (resp. Φ • ), are taken as the word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 NTK Limit</head><p>In the NTK parametrization, V and U in Eq. ( <ref type="formula" target="#formula_124">33</ref>) factor as V = 1</p><p>√ n v and U = u, and the learning rate is Θ(1). Each column U •i of U is equal to h(ξ i ). At any fixed time t, it is easy to see via Tensor Programs that</p><formula xml:id="formula_160">h t (ξ i ) = h 0 (ξ i ) + j∈V O(1/ √ n)v j + O coord (1/n)</formula><p>where v j denotes the jth row of v at initialization, and where O coord (1/n) means a vector that is O(1/n) coordinatewise. Recall that U = u and v are initialized with iid standard Gaussian entries.</p><p>Because ξ i is one-hot, this in particular implies h 0 (ξ i ) has standard Gaussian entries, and h 0 (ξ i ) is independent from h 0 (ξ j ) for i = j. Then for any i = j,</p><formula xml:id="formula_161">1 √ n h t (ξ i ) h t (ξ j ) - 1 √ n h 0 (ξ i ) h 0 (ξ j ) a.s. --→ 0, 1 √ n h 0 (ξ i ) h 0 (ξ j ) d -→ N (0, 1)</formula><p>by Law of Large Numbers (or more formally, Theorem 7.4) and Central Limit Theorem. In other words, 1</p><p>√ n h 0 (ξ i ) h 0 (ξ j ) is distributed completely randomly, with no regard to the semantic similarities of i and j. Likewise, the inner product in Eq. ( <ref type="formula" target="#formula_127">35</ref>) is random, and the argmax is a uniform sample. <ref type="bibr" target="#b47">48</ref> Therefore, in the NTK limit, Word2Vec gives random answers and achieves an accuracy of 1 |V|-3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Detailed Comparison with Deep Mean Field Limits</head><p>The key idea of previous works [5, 15, 34, 35, 46] proposing multilayer mean field limits of MLPs is to initialize each</p><formula xml:id="formula_162">n × n matrix W like W αβ ← F (u α , v β )/n for some function F and u α ∼ Z u , v β ∼ Z v sampled iid for each α, β ∈ [n],</formula><p>where Z u and Z v are some fixed (wrt n) random variables. If x is an activation with approximately iid coordinates distributed like random variable Z x , then W x looks like</p><formula xml:id="formula_163">(W x) α ≈ E Z v ,Z x F (u α , Z v )Z</formula><p>x by Law of Large Numbers (LLN), roughly iid across α. This logic will in fact hold throughout training. For well-chosen (F, Z u , Z v ), this does not get stuck at initialization but this form of initialization is very unnatural. In Nguyen &amp; Pham (2020), this is adapted to iid initialization straightforwardly. For example, this includes W αβ ← N (0, 1)/n. If x is as above, then (W x) α → 0 by LLN because W is sampled independently from x at init and has 0 mean. This means that preactivations every layer will vanish coordinatewise to 0, from which it's easy to see that the gradients vanish where there are more than 2 hidden layers. Hence we say that the function gets stuck at initialization. Contrast this 1/n scaling with the more typical 1/ √ n scaling, i.e. W αβ ← N (0, 1)/ √ n, which is what we deal with here. On a technical level, their limit calculation purely goes through LLN, whereas we need to wrestle with Central Limit effects (from the 1/ √ n scaling) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Nuances of the Master Theorem</head><p>Remark F.1 (Partial derivative). The partial derivative in ZDot should be interpreted as follows. By a simple inductive argument, Z x for every vector x in the program is defined uniquely as a deterministic function ϕ( Ẑx 1 , . . . , Ẑx k ) of some x 1 , . . . , x k in V or introduced by MatMul (notationally, we are suppressing the possible dependence on limit scalars θ1 , . . . , θl ). For instance, if in a program we have A ∈ W, v ∈ V, y = Av, x = A y, then Z x = Ẑx + Ẑv , so ϕ is given by ϕ(a, b) = a + b. Then</p><formula xml:id="formula_164">∂Z x /∂ Ẑx i def = ∂ i ϕ( Ẑx 1 , . . . , Ẑx k</formula><p>), and ∂Z x /∂ Ẑz def = 0 for any z ∈ {x 1 , . . . , x k }. Note this definition depends on the precise way the program is written, not just on the underlying mathematics. For example, if y, z ∈ V and x = φ(W (y + z)), then</p><formula xml:id="formula_165">Z x = φ( ẐW (y+z) ) so that ∂Z x /∂ ẐW y = ∂Z x /∂ ẐW z = 0. If instead, we have x = φ(W y+W z), then Z x = φ( ẐW y + ẐW z ) so that ∂Z x /∂ ẐW (x+y) = 0. However, in both cases, ŻW x = (Z y + Z z ) E φ ( ẐW (y+z) ).</formula><p>Remark F.2 (Partial derivative expectation). The quantity E ∂Z x ∂ ẐW y is well defined if Z x is differentiable in ẐW y . However, even if this is not the case, e.g. if x = θ(W y) where θ is the Heavyside step function, we can still define this expectation by leveraging Stein's lemma:</p><formula xml:id="formula_166">In ZDot, suppose {W y i } k i=1 are all elements of V W introduced before x. Define the matrix C ∈ R k×k by C ij def = E Z y i Z y j and define the vector b ∈ R k by b i def = E ẐW y i Z x . If a = C + b (where C + denotes the pseudoinverse of C), then in ZDot we may set σ 2 W E ∂Z x ∂ ẐW y i = a i .<label>(39)</label></formula><p>This definition agrees with the partial derivative expectation by Stein's lemma when the latter is well defined. Theorem 7.4 holds with this broader definition of partial derivative expectation.</p><p>Pseudo-Lipschitz functions are, roughly speaking, functions whose weak derivatives are polynomially bounded.</p><formula xml:id="formula_167">Definition F.3. A function f : R k → R is called pseudo-Lipschitz of degree d if |f (x) -f (y)| ≤ C x -y (1 + k i=1 |x i | d + |y i | d</formula><p>) for some C. We say f is pseudo-Lipschitz if it is so for any degree.</p><p>Here are some basic properties of pseudo-Lipschitz functions:</p><p>• The norm • in Definition F.3 can be any norm equivalent to the 2 norm, e.g. p , p ≥ 1, norms. Similarly, k i=1 |x i | d + |y i | d can be replaced by x d p + y d p , for any p ≥ 1. • A pseudo-Lipschitz function is polynomially bounded. • A composition of pseudo-Lipschitz functions of degrees d 1 and d 2 is pseudo-Lipschitz of degree d 1 + d 2 . • A pseudo-Lipschitz function is Lipschitz on any compact set. We adopt the following assumption for the Master Theorem Theorem 7.4. Assumption F.4. Suppose 1. If a function φ(; -) : R 0+l → R with only parameter arguments is used in Moment, then φ is continuous in those arguments. 2. Any other function φ(-; -) : R k+l → R with parameters (where k &gt; 0) used in Nonlin or Moment is pseudo-Lipschitz in all of its arguments (both inputs and parameters). Statement 1 in Assumption F.4 essentially says that if we have scalars θ 1 , . . . , θ l in the program, then we can produce a new scalar by applying a continuous function (a weaker restriction than a pseudo-Lipschitz function) to them. Indeed, if θ 1 , . . . , θ l converge almost surely, then this new scalar does too. In our setting, statement 1 is used to allow any loss function whose derivative is continuous.</p><p>Other versions of the Master Theorem can be found in [52], for example, versions where the we do not assume any smoothness condition at all on the nonlinearities beyond that they be polynomially bounded, in exchange for assuming what's called a rank stability condition. This rank stability should be generically true, but checking it rigorously is subtle, so we are content with the pseudo-Lipschitz condition in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G A Rough Sketch of the Geometry of abc-Parametrizations</head><p>By the results of Section 3.2, the stable abc-parametrizations form a polyhedron defined by the inequalities of Theorem 3.3. We call the polyhedron obtained by quotienting Eq. ( <ref type="formula" target="#formula_8">5</ref>) the stable polyhedron. In this section, we remark on some geometric properties of this polyhedron.</p><p>First, observe that the stable polyhedron is unbounded (thus, we say polyhedron instead of polytope). Indeed, given any stable parametrization, for any l, we can set a l ← a l + θ, b l ← b l -θ for any θ ≥ 0 to obtain another stable parametrization. This corresponds decreasing the layer l learning rate, so that as θ → ∞, W l is not trained.</p><p>Second, by Theorem 3.4, the nontrivial parametrizations reside in two facets of the stable polyhedron. These facets are unbounded for the same reason as above.</p><p>Next, we show that NTP (as well as µP) is a vertex on the intersection of these two facets, and NTP and µP are connected by an edge. Definition G.1. Consider a stable abc-parametrization of the MLP in Eq. ( <ref type="formula" target="#formula_0">1</ref>). We say the body of the MLP is uniformly updated if, for some training routine, time t ≥ 1, and input ξ, ∆W l t x l t (ξ) = Θ(n -r ) for all l simultaneously, where r is as defined in Definition 3.2.</p><p>In the results of this section below, we assume Assumption H.22. Proposition G.2. In a stable abc-parametrization, the MLP body is uniformly updated iff r l = r for all l ∈ [L], where r l is as defined in Proposition 5.3. Theorem G.3. In NTP, the MLP body is updated uniformly and W L+1 is both initialized and updated maximally. Furthermore, at initialization, f 0 converges in distribution <ref type="bibr" target="#b48">49</ref> to a Gaussian Process with nonzero kernel. NTP is the unique (modulo Eq. ( <ref type="formula" target="#formula_8">5</ref>)) stable abc-parametrization with both of these properties. Theorem G.4. For any r ∈ [0, 1/2], there is a unique (modulo Eq. ( <ref type="formula" target="#formula_8">5</ref>)) stable abc-parametrization with 1) that value of r and the property that 2) the MLP body is updated uniformly and W L+1 is both initialized and updated maximally. We call this parametrization the Uniform Parametrization with r-value r, denoted UP r . Its abc values are</p><formula xml:id="formula_168">a l = - 1 2 I(l = 1) + r ∀l ∈ [L], a L+1 = 1/2; b l = 1/2 -r; c = 0.</formula><p>In particular, UP 0 is µP and UP 1/2 is NTP. For r &gt; 1/2, such a uniform parametrization is not stable because W 0 would need to be Θ(n r-1 ), which would cause the initial GP to blow up. Thus, geometrically, UP r , r ∈ [0, 1/2], form an edge of the stable polyhedron.</p><p>We can define the uniform stable polyhedron to be the subset of the stable polyhedron corresponding to parametrizations which update the MLP body uniformly. This is isomorphic to the stable polyhedron when L = 1. Since stable abc-parametrizations with L = 1 has only 3 degrees of freedom, say a 1 , a 2 , b 2 while we fix c = 0 (via Eq. ( <ref type="formula" target="#formula_8">5</ref>)) and b 1 = -a 1 , we can visualize the corresponding stable polyhedron in 3D. However, the nontrivial parametrizations only reside in the boundary of this polyhedron. Because of its unbounded nature, we can project its boundary in 2D and visualize it. This is done in Fig. <ref type="figure" target="#fig_11">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Proofs of Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Rigorous Statements of Main Results</head><p>Applicable Nonlinearities For technical reasons, in our main results we restrict our attention to the canonical examples of nonlinearities: tanh and relu -or rather, a smooth version of relu called gelu [24] common in transformer models [8]. More precisely,</p><formula xml:id="formula_169">𝑎 1 = - 1 2 𝑎 1 = 0 𝑎 2 + 𝑏 2 = 1/2 𝑏 2 = 0 -𝑎 1 = 𝑎 2 = 𝑏 2 𝑓 0 is a nonzero GP 𝜇P NTP 𝑎 2 + 𝑏 2 + 2𝑎 1 = 0 𝑎 2 + 𝑎 1 = 0 𝑎 2 =</formula><p>1/2 𝑎 2 + 𝑏 2 + 𝑎 1 = 1/2 𝑊 𝐿+1 updated maximally 𝑊 𝐿+1 init. maximally 𝑟 = 0 Feature Learning 𝑟 &gt; 0 Kernel Regime Nontrivial feature learning Δ𝑊 𝐿+1 dominates 𝑊 0 𝐿+1 𝑊 0 𝐿+1 dominates Δ𝑊 𝐿+1 Boundary of Uniform Stable Polyhedron Trivial Nontrivial Legend Body NTK limit . We obtain the caricature in Fig. <ref type="figure">2</ref> by taking the nontrivial subspace of the graph here and quotienting the two facets by their respective points at infinity. Explanation of some captions: GP limit means the training dynamics amounts to training only the last layer in the infinite-width limit, starting from a nonzero initial GP.</p><p>Body NTK limit means NTK dynamics except the last layer does not contribute to the NT kernel.</p><p>Definition H.1. Define σ-gelu to be the function</p><formula xml:id="formula_170">x → 1 2 xerf(σ -1 x) + σ e -σ -2 x 2 2 √ π + x 2 .</formula><p>σ-gelu is a smooth approximation of relu and is the integral of 1 2 (erf(σ -1 x) + 1) that is 0 at -∞. The large σ is, the smoother σ-gelu is. As σ → 0, σ-gelu converges to relu. We believe our results will hold for generic nonlinearities, but making this precise is outside our scope here. (See Remark H.15 for some discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations and Terminologies</head><p>Definition H.2 (Big-O Notation). Given a sequence of scalar random variables c = {c n ∈ R} ∞ n=1 , we write c = Θ(n -a ) if there exist constants A, B such that An -a ≤ |c| ≤ Bn -a for sufficiently large n, almost surely <ref type="bibr" target="#b49">50</ref> . Given a sequence of random vectors x = {x n ∈ R n } ∞ n=1 , we say x has coordinates of size Θ(n -a ) and write x = Θ(n -a ) to mean the scalar random variable sequence { x n 2 /n} n is Θ(n -a ). Similarly for the notations O(n -a ), Ω(n -a ). We use the notations Θ ξ (n -a ), O ξ (n -a ), Ω ξ (n -a ) if the hidden constants A, B are allowed to depend on some object ξ. For brevity, we will often abuse notation and say c itself is a random variable or x itself is a random vector.</p><p>Most often, the vector x will have "approximately iid" coordinates, so the notation x = Θ(n -a ) can be interpreted intuitively to say x has coordinates of "standard deviation" Θ(n -a ), which justifies the name. Definition H.3. An abc-parametrization is a joint parametrization of an MLP and the learning rate specified by the numbers {a l , b l } l ∪ {c} as in Eq. ( <ref type="formula" target="#formula_0">1</ref>). Below we will often say abc-parametrization of an MLP for short, even though the parametrization affects the learning rate as well. A training routine is a combination of learning rate ηn -c , training sequence {(ξ t , y t )} t≥0 , and a loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>We will mainly focus on stable parametrizations, defined below, which intuitively means 1) the preactivations {h l } l and activations {x l } l have Θ(1) coordinates at initialization, and 2) their coordinates and the logit f (ξ) all stay O(1) (i.e. bounded independent of n) throughout the course of SGD. <ref type="bibr" target="#b50">51</ref> Otherwise, they tend to ∞ with n, eventually going out of floating point range. <ref type="bibr" target="#b49">50</ref> Here almost surely means for almost every instantiation of c 1 , c 2 , . . ., i.e. it is with regard to the product probability space generated by all of {c n } ∞ n=1 . In this paper, this probability space will be generated by random initializations of a neural network at every width n. Very importantly, note the order of the qualifiers: we are saying for almost every instantiation of c 1 , c 2 , . . ., for large enough n, An -a ≤ |c| ≤ Bn -a .</p><p>51 but they may depend on training time and η; in particular, it's possible that they diverge with time Indeed, this is an acute and real problem common in modern deep learning, where float16 is necessary to train large models. Definition H.4 (Stability). We say an abc-parametrization of an L-hidden layer MLP is stable if 1. For every nonzero input ξ ∈ X ,</p><formula xml:id="formula_171">h l 0 (ξ), x l 0 (ξ) = Θ ξ (1), ∀l ∈ [L], and E f 0 (ξ) 2 = O ξ (1),<label>(40)</label></formula><p>where the expectation is taken over the random initialization.</p><p>2. For any training routine, any time t ≥ 0, l ∈ [L], ξ ∈ X , we have</p><formula xml:id="formula_172">∆h l t (ξ), ∆x l t (ξ) = O * (1), ∀l ∈ [L]</formula><p>, and f t (ξ) = O * (1), where the hidden constant inside O can depend on the training routine, t, ξ, and the initial function values f 0 (X ). <ref type="bibr" target="#b51">52</ref> Recall from the main text, Definition H.5. For any abc-parametrization, we write r for the quantity</p><formula xml:id="formula_173">r def = min(a L+1 + b L+1 , 2a L+1 + c) + c -1 + L min l=1 [2a l + I(l = 1)] .</formula><p>For example, in NTP, r = 1/2, while in µP, r = 0. Intuitively, r is the exponent such that ∆x L t (ξ) = Θ ξ (n -r ). Thus, to avoid activation blowup, we want r ≥ 0; to perform feature learning, we want r = 0. Theorem H.6 (Stability Characterization). Suppose φ is tanh or σ-gelu for sufficiently small σ. An abc-parametrization is stable iff all of the following are true (with intuitions in parentheses):</p><p>1. ((pre)activations at initialization are Θ(1) and logits are O(1))</p><formula xml:id="formula_174">a 1 + b 1 = 0; a l + b l = 1/2, ∀l ∈ [2, L]; a L+1 + b L+1 ≥ 1/2.<label>(41) 2.</label></formula><p>(features don't blowup, i.e. ∆x l t = O(1) for all l) r ≥ 0. (42) 3. (logits don't blow up during training, i.e. ∆W L+1 t x L t , W L+1 0</p><formula xml:id="formula_175">∆x L t = O(1)) 2a L+1 + c ≥ 1; a L+1 + b L+1 + r ≥ 1.<label>(43)</label></formula><p>Here, r is as defined in Definition H.5.</p><p>In Eq. ( <ref type="formula" target="#formula_175">43</ref>), ∆W L+1 t turns out to be Θ(n -(2a L+1 +c) ) and is correlated with x L t = Θ(1) such that their product behaves according to Law of Large Numbers; the first inequality says this should not blow up. Similarly, W L+1 0 = Θ(n -(a L+1 +b L+1 ) ) and it turns out ∆x L t = Θ(n -r ) and they will interact via Law of Large Numbers, so the second inequality says their product shouldn't blow up.</p><p>Our main results concern nontrivial parametrizations: Definition H.7 (Nontriviality). We say an abc-parametrization of an L-hidden layer MLP is trivial if for every training routine, f t (ξ) -f 0 (ξ) a.s.</p><p>--→ 0 for any time t ≥ 1 and input ξ ∈ X (i.e. the function does not evolve in the infinite-width limit). We say the parametrization is nontrivial otherwise.</p><p>Theorem H.8 (Nontriviality Characterization). Suppose φ is tanh or σ-gelu for sufficiently small σ. A stable abc-parametrization is nontrivial iff a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1. Definition H.9 (Feature Learning). We say an abc-parametrization of an L-hidden layer MLP admits feature learning in the lth layer if there exists some training routine such that</p><formula xml:id="formula_176">∆x l t (ξ) = Ω * (1)<label>(44)</label></formula><p>for some t ≥ 0, ξ ∈ X , where the hidden constant inside Ω can depend on the training routine, t, ξ, and the initial function values f 0 (X ). We say the parametrization admits feature learning if it does so in any layer.</p><p>We say the parametrization fixes the lth layer features if for all training routine, ∆x l t (ξ) 2 /n a.s.</p><p>--→ 0 for all t ≥ 0, ξ ∈ X . We say the parametrization fixes all features if it does so in every layer.</p><p>We make similar definitions as above replacing feature with prefeature and x l with h l .</p><p>Note that the probabilistic nature of Ω * (1) means that no feature learning does not imply fixing all features (because ∆x l t (ξ) can just fluctuate wildly between 0 and infinity), but we will see that in the context of nontrivial stable abc-parametrizations, this is true. Remark H.10. We note that this is a rather weak notion of "feature learning", as we only require that the embedding x L t (ξ) changes from its initialization for some scenario, rather than, say for generic scenarios; nor do we speak at all about the "quality" of feature learning, e.g. how it helps downstream tasks. But our proofs (see Appendix H.7) will show that "some scenario" in fact implies much more general scenarios. In addition, we argue that such formal weakness is more than compensated by our experiments, which show that infinite-width limits of feature learning (in the sense defined here) abc-parametrized MLPs outperform finite MLPs and their NTK limits on tasks (namely, Word2Vec and few-shot learning) where feature learning, in the colloquial notion of the phrase, is crucial.</p><p>A somewhat stronger notion of feature learning is that the feature kernel evolves. This is, for example, essential for linear transfer learning such as in self-supervised learning of image data. Definition H.11 (Feature Kernel Evolution). We say an abc-parametrization of an L-hidden layer MLP evolves the lth layer feature kernel if there exists some training routine such that</p><formula xml:id="formula_177">x l t (ξ) x l t (ζ)/n -x l 0 (ξ) x l 0 (ζ)/n = Ω *<label>(1</label></formula><p>) for some t ≥ 0, ξ, ζ ∈ X , where the hidden constant inside Ω can depend on the training routine, t, ξ, ζ, and the initial function values f 0 (X ). We say the parametrization evolves feature kernels if it does so in any layer.</p><p>We say the parametrization fixes the lth layer feature kernel if for all training routine,</p><formula xml:id="formula_178">x l t (ξ) x l t (ζ)/n -x l 0 (ξ) x l 0 (ζ)/n a.s.</formula><p>--→ 0, as n → ∞, for all t ≥ 0, ξ, ζ ∈ X . We say the parametrization fixes all feature kernels if it does so in every layer.</p><p>We make similar definitions as above replacing feature with prefeature and x l with h l .</p><p>Intuitively, for a stable parametrization, feature kernel evolution should imply feature learning (one can see the contrapositive easily). In fact, we shall see below they are equivalent notions.</p><p>On the other hand, from the NTK example, we know certain limits can be described entirely through kernel gradient descent with some kernel. Appropriately, we make the following definition. Definition H.12 (Kernel Regime). We say an abc-parametrization of an L-hidden layer MLP is in kernel regime if there exists a positive semidefinite kernel K : X 2 → R such that for every training routine, the MLP function evolves under kernel gradient descent, i.e. there exist random variables ft (ξ) for each time t ≥ 0 and input ξ ∈ X such that, as n → ∞,<ref type="foot" target="#foot_41">foot_41</ref>  </p><formula xml:id="formula_179">{f t (ξ)} t≤T,ξ∈X d -→ { ft (ξ)} t≤T,ξ∈X , ∀T ≥ 1,</formula><p>Observe that, in kernel regime, ft (ξ) is deterministic conditioned on f0 (ξ), as evident inductively from Eq. (45). For example, in the NTK limit, { f0 (ξ) : ξ ∈ X } is a nontrivial Gaussian Process (GP), but the function evolution conditioned on this GP is deterministic.</p><p>All of the concepts defined above are related to each other by the following theorem. If there is feature learning or feature kernel evolution or prefeature learning or prefeature kernel evolution in layer l, then there is feature learning and feature kernel evolution and prefeature learning and prefeature kernel evolution in layers l, . . . , L.</p><p>4. If r = 0, then for all ξ ∈ X , f 0 (ξ) a.s.</p><p>--→ 0 and f t (ξ) a.s.</p><p>--→ ft (ξ) for some deterministic ft (ξ). However, the converse is not true.</p><p>5. If r &gt; 0, a L+1 + b L+1 + r &gt; 1 and 2a L+1 + c = 1, then we have the Neural Network-Gaussian Process limit.</p><p>In particular, Statement 4 implies that feature learning, at least in our context, is incompatible with Bayesian, distributional perspectives of neural network limits, such as the NNGP limit.</p><p>The characterization above then trivially implies the following dichotomy. Corollary H.14 (Dynamical Dichotomy). For φ being tanh or σ-gelu for sufficiently small σ, a nontrivial stable parametrization of an L-hidden layer MLP either admits feature learning or is in kernel regime, but not both.</p><p>Remark H.15 (The Role of the φ Assumption). The dependence on φ being tanh or σ-gelu for sufficiently small σ is only needed to explicitly construct a training routine that leads to feature learning for r = 0. We expect this should be true for generic φ, but we leave this for future work. We expand more on how the φ assumption is used below.</p><p>To calculate the infinite width limit of any abc-parametrization rigorously, we only need the nonlinearity to have a polynomially bounded 2nd derivative (or more generally pseudo-Lipschitz, so as to apply the Master Theorem). The specific choice of tanh or gelu is needed to prove the part of the Dynamical Dichotomy that says a limit cannot be simultaneously in kernel regime and in feature learning regime (which, e.g. is not true for linear activation). To do so, we use Properties H.44 and H.47 of tanh and gelu, expanded below. This is really for a more convenient proof, but we believe a more general approach should work for general nonlinearities. Our argument is as follows (this is also overviewed in the start of Appendix H.7): If r = 0, we show that a sufficiently small nonzero learning rate (scaled with width in the corresponding parametrization) in 1 SGD step 1) induces a change in the features but 2) the resulting change in the NN output is not linear in the loss derivative χ. 1) means it's feature learning, and 2) means it's not in kernel regime. This argument involves showing certain derivatives of certain expectations with respect to learning rate is positive. In the case of tanh and gelu, this is checked explicitly using Properties H.44 and H.47.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">a</head><formula xml:id="formula_181">1 + b 1 = 0; a l + b l = 1/2, ∀l ∈ [2, L]; a L+1 + b L+1 ≥ 1/2.</formula><p>Proof. Fix an input ξ = 0. Here, because we focus on initialization, we will suppress the time 0 subscript and ξ dependence of h l , x l to mean t = 0, applied to ξ.</p><p>Obviously, h 1 = W 1 ξ is a Gaussian vector with N (0, n -(a1+b1) ξ 2 ) coordinates, so h 1 = Θ ξ (1) iff a 1 + b 1 = 0. Assume a 1 + b 1 = 0. By Law of Large Numbers, 1 n x 1 2 a.s.</p><p>--→ E φ(Z h 1 ) 2 where Z h 1 = N (0, ξ 2 ). Since φ is not almost everywhere zero and ξ = 0, this expectation is nonzero so that x 1 = Θ ξ (1).</p><p>We construct the following Tensor Program: the lone initial vector is h 1 , the initial matrices are W l , 2 ≤ l ≤ L, and initial scalars θ l def = n 1/2-(a l +b l ) . We sample h 1 α ∼ N (0, ξ 2 ) and W l αβ ∼ N (0, 1/n). Mathematically, we will represent W l = θ l W l . The program is then given by</p><formula xml:id="formula_182">x l = φ(h l ), ∀l ∈ [L], ĥl = W l x l-1 , h l = θ l ĥl , ∀l ∈ [2, L],</formula><p>where we used Nonlin, MatMul, and Nonlin (with parameter θ l ).</p><p>Suppose a l + b l = 1/2 (i.e. θ l = 1) for all 2 ≤ l ≤ L. Then,</p><formula xml:id="formula_183">Z h l = Z ĥl = N (0, E φ(Z h l-1 ) 2 )</formula><p>for each l ≤ L. Because φ is not everywhere zero, this inductively implies E(Z h l ) 2 &gt; 0 (and so also E(Z x l ) 2 &gt; 0) for all l ≤ L. By the Master Theorem, 1 n h l 2 a.s.</p><p>--→ E(Z h l ) 2 and 1 n x l 2 a.s.</p><p>--→ E(Z x l ) 2 so this implies h l , x l = Θ ξ (1) for all l ≤ L as desired.</p><p>Conversely, suppose m is the smallest l ≥ 2 such that a l + b l = 1/2. Then by the above reasoning, ĥm = Θ ξ (1) so h m = Θ ξ (n 1/2-(a l +b l ) ) is either blowing up to ∞ or shrinking to 0 with n. This</p><formula xml:id="formula_184">shows that h l , x l = Θ ξ (1) for all l ≤ L iff a 1 + b 1 = 0 and a l + b l = 1/2 for all 2 ≤ l ≤ L. Finally, if a 1 + b 1 = 0 and a l + b l = 1/2 for all 2 ≤ l ≤ L, then we see E f 0 (ξ) 2 = (n 1/2-(a L+1 +b L+1 ) ) 2 E Z x L 2 /n. For large n, this is Θ ξ ((n 1/2-(a L+1 +b L+1 ) ) 2 ) and is O ξ (1) iff a L+1 + b L+1 ≥ 1/2.</formula><p>Definition H.20. We say a parametrization is initialization-stable if it satisfies Eq. ( <ref type="formula" target="#formula_171">40</ref>) (or equivalently, Eq. ( <ref type="formula" target="#formula_174">41</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 Program Setup</head><p>In the next section, we construct the Tensor Program that encodes the training of an L-hidden layer MLP under an abc-parametrization. Here we first describe the initial matrices, vectors, and scalars of the program, along with necessary notations.</p><p>We first remark on a simplification we will make to streamline the proof.</p><formula xml:id="formula_185">The Size of W L+1 0 vs ∆W L+1 t By construction, W L+1 0 = Θ(n -(a L+1 +b L+1 ) ). If x L t (ξ) = Θ(1) as in a stable parametrization, then ∆W L+1 t = Θ(n -(2a L+1 +c) ). Therefore, if a L+1 + b L+1 ≤ 2a L+1 + c, then W L+1 0 is at least as large as ∆W L+1 t</formula><p>, so that W L+1 t will stay the same order (in terms of n) for all t. If the reverse inequality is true, then W L+1 0 is smaller than W L+1 t for t ≥ 1. This in particular implies that the gradients at time 0 is smaller than gradients at subsequent times. For example, we can take a L+1 + b L+1 → ∞ while fixing 2a L+1 + c, in which case W L+1 0 = 0 and the weight gradients at initialization are all 0 except for that of W L+1 . One can thus think of this as a "lag" in the training dynamics for 1 step.</p><p>Assumption H.21. For clarity of the proof, we will assume a L+1 + b L+1 ≤ 2a L+1 + c, i.e. W L+1 t stays the same order for all t. The case of a L+1 + b L+1 &gt; 2a L+1 + c, corresponding to a 1-step "lag" as explained above, can be dealt with similarly. We will remark whenever this requires some subtlety.</p><p>For the construction of the program and the application of the Master Theorem, we will also assume the following for the rest of this paper.</p><p>Assumption H.22. φ is pseudo-Lipschitz and not almost everywhere zero. Initial Matrices, Vectors, Scalars We will assume the parametrization is initialization-stable. For ease of presentation, we also assume the input dimension d = 1.</p><p>1. Initial matrices: W 2 0 , . . . , W L 0 , sampled like (W l 0 ) αβ ∼ N (0, 1/n). 2. Initial vectors: input layer matrix W 1 0 ∈ R n×1 and normalized output layer matrix W L+1</p><formula xml:id="formula_186">0 def = W L+1 0 n a L+1 +b L+1 ∈ R 1×n , sampled like (W 1 0 ) α , ( W L+1<label>0</label></formula><p>) α ∼ N (0, 1). 3. Initial scalars: We define the following scalars (where we explain the intuition in parenthesis).</p><p>The reader can skip this part on a first read but come back when referred to. (a) (n times the scale of coordinates of</p><formula xml:id="formula_187">∆W l t ) For l ≥ 2, define θ W l def = n -(a L+1 +b L+1 +c-1+2a l ) (b) (scale of coordinates of ∆W 1 t and ∆h 1 t ) Define θ 1 = θ W 1 def = n -(a L+1 +b L+1 +c+2a1) (c) (scale of coordinates of ∆W L+1 t ) θ L+1 = θ W L+1 def = n -2a L+1 -c (d) (scale of ∆h l t and ∆x l t ) For l ∈ [L], define θ h l = θ x l = θ l def = max m≤l θ W m = max(θ W l , θ l-1 )<label>(46)</label></formula><p>= n -(a L+1 +b L+1 +c-1+min l m=1 (2am+I(m=1)))</p><p>Note that θ L = n -r with r defined in Definition H.5. (e) (scale of</p><formula xml:id="formula_188">W L+1 t ) θ f def = n -(a L+1 +b L+1 ) (f) (convenience scalars) θ x l-1 /h l = θ x l-1 /θ h l θ W l /h l = θ W l /θ h l θ W l x l-1 /h l = θ W l θ x l-1 /θ h l θ L+1/f = θ L+1 /θ f θ L+1 = nθ L+1 = n 1-2a L+1 -c θ Lf = nθ L θ f = n 1-(r+a L+1 +b L+1 )</formula><p>(g) Depending on the the value of a L+1 + b L+1 , we will also construct the values of f at initialization as initial scalars. See Appendix H.4.1 for an explanation.</p><p>By our assumption that a L+1 + b L+1 ≤ 2a L+1 + c, the pseudostability inequalities of Theorem H.6 imply all of these θs either converge to 0 or stay constant at 1. This means that, assuming appropriate regularity conditions on the nonlinearities and rank stability, we can apply the Master Theorem (if θ blows up to ∞ then we can't do that).</p><p>Notations We use := to more clearly denote assignment happening in the program, as opposed to mathematical equality. To clearly demonstrate the application of Nonlin, we will also freely introduce function symbols Ψ to put things into Nonlin form.</p><p>Preview of Names for Vectors In the program, for each z ∈ {x l , h l } l , we will construct vectors δz t (ξ) to mathematically represent θ -1 z (z t (ξ) -z t-1 (ξ)) (intuition: change in z scaled to have Θ(1) coordinates). Similarly, for w ∈ {W L+1 , W 1 }, we will construct δw t to mathematically represent θ -1 w (w t -w t-1 ) (intuition: change in w scaled to have Θ(1) coordinates). Then, mathematically, z t (ξ) = z t-1 (ξ) + θ z δz t (ξ), w t = w t-1 + θ w δw t .</p><p>We will also construct dz to mathematically represent θ -1 f ∇ z f (intuition: gradient ∇ z f scaled to have Θ(1) coordinates). For weight changes, we have the following identity</p><formula xml:id="formula_189">W l t -W l t-1 = -ηn -c χ t-1 n -2a l θ f dh l t-1 x l-1 t-1 = -ηχ t-1 θ W l 1 n h l t-1 x l-1 t-1 , ∀l ∈ [2, L], (<label>47</label></formula><formula xml:id="formula_190">) and for l = 1, W l t -W l t-1 = -ηn -c χ t-1 n -2a l θ f dh l t-1 ξ t-1 = -ηχ t-1 θ W l h l t-1 ξ t-1 .<label>(48)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4 Program Construction</head><p>Here we construct the Tensor Program encoding the SGD of an MLP. We separately describe the first forward and backward passes followed by the later forward and backward passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4.1 First Forward Pass</head><p>For every ξ ∈ X , we compute h 1 0 (ξ) := W 1 0 ξ ∈ R n via Nonlin (as Ψ(W 1 0 ; ξ), where Ψ is multiplication by ξ), and we construct the following vectors via Nonlin and MatMul</p><formula xml:id="formula_191">x l 0 (ξ) := φ(h l 0 (ξ)) ∈ R n , h l+1 0 (ξ) := W l+1 0 x l 0 (ξ) ∈ R n , for l = 1, . . . , L -1,<label>(49)</label></formula><p>Function Output The first output is f 0 (ξ) = W L+1 0</p><p>x L 0 (ξ), but we will define f 0 (ξ) in the program slightly differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case when a</head><formula xml:id="formula_192">L+1 + b L+1 &gt; 1/2 Then f 0 (ξ) a.s.</formula><p>--→ 0 for all ξ ∈ X . In the program, we will construct f 0 (ξ) as an initial scalar mathematically defined by W L+1 0 x L 0 (ξ). 5960</p><p>Case when a L+1 + b L+1 = 1/2 If a L+1 + b L+1 = 1/2, then f 0 (ξ) converges to a nontrival Gaussian via CLT [49], so we will condition on f 0 (ξ) for all ξ ∈ X . Given values g(ξ) ∈ R for all ξ ∈ X , let E be the event that f 0</p><formula xml:id="formula_193">(ξ) = 1 √ n W L+1 0 x L 0 (ξ) equals g(ξ) for all ξ ∈ X . The distribution of W L+1 0 conditioned on E is given by W L+1 0 d = E √ nX + g + Π W L+1 0 where W L+1 0 is an iid copy of W L+1 0 , g ∈ R X is the vector of {g(ξ) : ξ ∈ X }, X ∈ R X ×n has x L</formula><p>0 (ξ) as rows, and Π is the orthogonal projection into the orthogonal complement of the space spanned by {x L 0 (ξ) : ξ ∈ X }. Here X + denotes the pseudo-inverse of X. By standard formulas for pseudo-inverse and orthogonal projection, we can write X + = --→ 0 because W L+1 0 is independent from X, and Σ a.s.</p><p>--→ Σ for some PSD matrix Σ. At this point in the program, all scalars we used (like ξ) are constant with n and can be absorbed into nonlinearities. By the rank stability property of any program without scalars [52], the rank of Σ is fixed for large enough n, almost surely, so Σ + a.s.</p><p>--→ Σ+ by the continuity of pseudo-inverse on fixed rank matrices.</p><p>We will now replace W L+1 0 in the program with</p><formula xml:id="formula_194">W L+1 E def = X Σ + g √ n + W L+1 0 -X Σ + γ</formula><p>constructed using Nonlin, where Σ + g √ n and (Σ + γ) are finite dimensional and formally considered (collections of) scalars involved as coefficients for linear combination of rows of X. Since Σ + g √ n , Σ + γ a.s.</p><p>--→ 0, we have Z W L+1 E = Z W L+1 0 . Intuitively, this means that, even after conditioning on f 0 = g, the conditional distribution of W L+1 0 is practically the same as the original distribution. We can then proceed exactly as in the case when a L+1 + b L+1 &gt; 1/2, with W L+1 E taking the role of W L+1 0 . The program then encodes the evolution of f conditioned on f 0 (ξ) = g(ξ), ∀ξ ∈ X . 61 59 It is completely OK to define an initial scalar using randomness from other parts of the program, as long as this scalar converges almost surely to a deterministic limit <ref type="bibr">60</ref> We cannot define it using a Moment instruction because, intuitively, the mechanism of this convergence is through CLT, not Law of Large Numbers.</p><p>61 Formally, we can also have {g(ξ) : ξ ∈ X } as initial scalars, but since they are fixed with n, they can be absorbed into the Nonlin that defines W L+1 E .</p><p>Assumption H.23. For the above reason, we will assume a L+1 + b L+1 &gt; 1/2, and remark whenever the case a L+1 + b L+1 = 1/2 involves subtleties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4.2 First Backward Pass</head><p>Next, we write the backward pass dx L 0 (ξ) := W L+1 0 dh l 0 (ξ) := dx l 0 (ξ) φ (h l 0 (ξ)) dx l-1 0 (ξ) := W l 0 dh l 0 (ξ)</p><p>where, recall, dz mathematically equals θ -1 f ∇ z f . For ξ = ξ 0 and its label y 0 , we define the first loss derivative as χ 0 := L (f 0 (ξ 0 ), y 0 ) a.s.</p><p>--→ χ0 (ξ) = L (0, y 0 ) where the convergence is because L is continuous by assumption.</p><p>We also define δW L+1</p><p>1 := -ηχ 0 x L 0 (ξ 0 ) to represent the (normalized) change in W L+1 due to the first gradient step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4.3 tth Forward Pass, t ≥ 1</head><p>Overview We iteratively define δz t (ξ) to mathematically represent θ -1 z (z t (ξ) -z t-1 (ξ)), for z ∈ {x l , h l } l . Then we eventually set z t (ξ) := z 0 (ξ) + θ z δz 1 (ξ) + • • • + θ z δz t (ξ).</p><p>Likewise, we will define δW L+1 t so that</p><formula xml:id="formula_195">W L+1 t = θ f W L+1 0 + θ L+1 (δW L+1 1 + • • • + δW L+1 t</formula><p>). In the program, we will not directly use W L+1 t but instead use</p><formula xml:id="formula_196">W L+1 t := W L+1 0 + θ L+1/f (δW L+1 1 + • • • + δW L+1 t )<label>(50)</label></formula><p>where θ L+1/f = θ L+1 /θ f . Mathematically,</p><formula xml:id="formula_197">W L+1 t = θ -1 f W L+1 t .</formula><p>Recall we shorthand z t = z t (ξ t ) for all z ∈ {x l , h l , dx l , dh l } l ∪ {f, χ}.</p><p>The Construction of (Pre)Activations We start with h = h 1 : By Eq. ( <ref type="formula" target="#formula_190">48</ref>), we have δh t (ξ) := -ηχ t-1 ξ t-1 ξdh t-1 = Ψ(dh t-1 ; ξ t-1 ξ, ηχ t-1 ).</p><p>(Notationally, recall we freely introduce function symbols Ψ to clarify the way we apply Nonlin). For higher layers, if h = h l , x = x l-1 , and W = W l , then h = W x. By Eq. ( <ref type="formula" target="#formula_189">47</ref>), we have, mathematically,</p><formula xml:id="formula_198">θ h δh t (ξ) = θ x W t-</formula><p>1 δx t (ξ) + (W t -W t-1 )x t (ξ) = θ x W 0 δx t (ξ) + t-1 s=1 (W s -W s-1 )δx t (ξ) + (W t -W t-1 )x t (ξ) = θ x W 0 δx t (ξ) -ηθ W t-1 s=1 χ s-1 x s-1 δx t (ξ) n dh s-1 -ηχ t-1 θ W x t-1 x t (ξ) n dh t-1</p><p>Recall θ x/h = θ -1 h θ x , θ W/h = θ -1 h θ W , θ W x/h = θ -1 h θ W θ x . With c s denoting</p><p>x s δxt(ξ) n</p><p>, we construct δh t (ξ) := θ x/h W 0 δx t (ξ) -ηθ W x/h t-1 s=1 χ s-1 c s-1 dh s-1 -ηχ t-1 θ W/h c t-1 dh t-1 = Ψ(W 0 δx t (ξ), dh 0 , . . . , dh t-1 ; η, θ x/h , θ W x/h , θ W/h , {c s , χ s } t-1 s=0 )</p><p>2. For z ∈ {x l , h l } l , we have Z zt(ξ) = Z z0(ξ) + θz Z δz1(ξ) + • • • + θz Z δzt(ξ) (52)</p><p>3. For l ∈ [L], x = x l , h = h l , we have Z δxt(ξ) = Ψ(Z ht-1(ξ) , Z δht(ξ) ; θh ) where Ψ is as in Eq. ( <ref type="formula">51</ref>). If θh = 0 (e.g. if r &gt; 0), then Z δxt(ξ) = φ (Z ht-1(ξ) )Z δht(ξ) .</p><p>Otherwise, θh = 1, and Z δxt(ξ) = φ(Z ht(ξ) ) -φ(Z ht-1(ξ) ).</p><p>4. For h = h 1 , we have Z δht(ξ) = -ηχ t-1 ξ t-1 ξZ dht-1 .</p><p>5. For l ≥ 2, h = h l , x = x l-1 , W = W l , we have</p><formula xml:id="formula_201">Z δht(ξ) = θx/h Z W0δxt(ξ) -η θW x/h t-2 s=0 χs Z dhs E Z xs Z xt(ξ) -ηχ t-1 θW/h Z dht-1 E Z xt-1 Z xt(ξ)<label>(55)</label></formula><p>where at least one of θx/h and θW/h equals 1. As usual, here we have the ZHat-ZDot decomposition of Z W0δxt(ξ) .</p><formula xml:id="formula_202">Z W0δxt(ξ) = ẐW0δxt(ξ) + ŻW0δxt(ξ) = ẐW0δxt(ξ) + t-1 s=0 Z dhs E ∂Z δxt(ξ)</formula><p>∂ ẐW 0 dhs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">For last layer weight</head><formula xml:id="formula_203">Z δW L+1 t = -ηχ t-1 Z x L t-1<label>(56)</label></formula><p>and</p><formula xml:id="formula_204">Z W L+1 t = Z W L+1 0 + θL+1/f (Z δW L+1 1 + • • • + Z δW L+1 t )<label>(57)</label></formula><p>7. The output deltas have limits</p><formula xml:id="formula_205">δ ft (ξ) = θ L+1 E Z δW L+1 t Z x L t (ξ) + θ Lf E Z W L+1 t-1 Z δx L t (ξ)<label>(58)</label></formula><p>and ft (ξ) = δ f1 (ξ) + • • • + δ ft (ξ).</p><p>8. For gradients:</p><formula xml:id="formula_206">Z dx L t (ξ) = Z W L+1 t Z dh l t (ξ) = Z dx l t (ξ) φ (Z h l t (ξ) ) Z dx l-1 t (ξ) = Z W l 0 dh l t (ξ) -η θW l t-1 s=0 χs Z x l-1 s E Z dh l s Z dh l t (ξ)</formula><p>9. Loss derivative χt = L ( ft , y 0 ).</p><p>The following fact follows from the results of [51] (or can be verified by straightforward calculation) and will be useful for us. Proposition H.24. Żdx l 0 (ξ) = 0 and Z dx l 0 (ξ) = Ẑdx l 0 (ξ) for any ξ ∈ X .</p><p>If Eq. ( <ref type="formula">61</ref>) is true for l = L, then</p><formula xml:id="formula_207">E Z W L+1 0 Z δx L t (ξ) = -ηχ t-1 E Z W L+1 0 Z dh L 0 (ξt-1) φ (Z h L 0 (ξ) ) L-1 m= -1 θm+1 Σ m,L-1 (ξ t-1 , ξ)</formula><p>where the contributions from ẐW L 0 • in Z δx L t (ξ) vanish as they are independent from Z W L+1 0 . Since Z dh L 0 (ξ) = Z W L+1 0 φ (Z h L 0 (ξ) ), we continue</p><formula xml:id="formula_208">E Z W L+1 0 Z δx L t (ξ) = -ηχ t-1 E Z W L+1 0 2</formula><p>φ (Z h L 0 (ξt-1) )φ (Z h L 0 (ξ) )</p><p>L-1 m= -1 θm+1 Σ m,L-1 (ξ t-1 , ξ)</p><formula xml:id="formula_209">= -ηχ t-1 L-1 m= -1 θm+1 Σ mL (ξ t-1 , ξ).</formula><p>Similarly, by Eq. ( <ref type="formula" target="#formula_203">56</ref>),</p><formula xml:id="formula_210">E Z δW L+1 t Z x L t (ξ) = -ηχ t-1 E Z x L t-1 (ξt-1) Z x L t (ξ)</formula><p>= -ηχ t-1 E Z x L 0 (ξt-1) Z x L 0 (ξ) = -ηχ t-1 Σ LL (ξ t-1 , ξ).</p><p>Altogether, these prove the desired claim. --→ 0 and δ ft (ξ) = -ηχ t-1 Σ LL (ξ t-1 , ξ), i.e. we have the Neural Network-Gaussian Process (NNGP) limit.</p><p>Conventionally, the NNGP limit is associated with only training the last layer and nothing else. This result says that the same limit can be achieved if we train the body of the network slightly, so that ∆x L t does not interact with W L+1 0 enough (embodied in the inequality a L+1 + b L+1 + r &gt; 1) to cause changes in f t .</p><p>Proof. The premise implies θ L+1 = 1 and θ Lf = 0, and the rest follows from Theorem H.32. Remark H.36. We have assumed for simplicity of the proof that a L+1 + b L+1 ≤ 2a L+1 + c. If this is not the case, then we can easily see Corollary H.35 applies anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.7 r = 0 Implies Feature Learning</head><p>In this section, we assume r = 0 and show any such pseudostable parametrization 1) admits (pre)feature learning and (pre)feature kernel evolution, and 2) is not in kernel regime (Theorem H.51). The overarching logic goes like this.</p><p>1. The Master Theorem shows that the specific entry 1 n x L 1 (ξ 0 ) 2 of the feature kernel converges to E(Z x L 1 (ξ0) ) 2 . If the learning rate η = 0, then x L 1 (ξ 0 ) = x L 0 and E(Z x L 1 (ξ0) ) 2 = E(Z x L 0 ) 2 . We hope to say that as η increases, E(Z x L 1 (ξ0) ) 2 moves away Theorem H.41. For all l &lt; ,∂ 2 η λ l (0) = ∂ 2 η γ l (0) = 0, and for all l ≥ ,</p><formula xml:id="formula_211">∂ 2 η λ l (0) = Cκ l 2 + 1 2 κ l 1 ∂ 2 η λ l-1 (0) ∂ 2 η γ l (0) = Cκ l 3 + 1 2</formula><p>γ l 02 (0)∂ 2 η λ l-1 (0) + γ l 11 (0)∂ 2 η γ l-1 (0), where C = 2(β l (0)) 2 E( Zl 0 ) 2 &gt; 0.</p><p>Proof. We start with the ∂ 2 η λ l (0) recurrence. For l ≥ , ∂ 2 η λ l is a sum of 3 terms, representing 1) 2 derivatives in the integrand, 2) 2 derivatives in the Gaussian variance, and 3) 1 derivative each. When evaluated at η = 0, only the first two terms survive because ∂ η λ l-1 (0) = 0 by Theorem H.40:</p><formula xml:id="formula_212">∂ 2 η λ l (0) = E ∂ 2 η φ 2 (Z l 1 )| η=0 + 1 2 E(φ 2 ) (Z l 0 )∂ 2 η λ l-1 (0). Now E ∂ 2 η φ 2 (Z l 1 ) = 2∂ η (E φ(Z l 1 )φ (Z l 1 )(β l Zl 0 φ (Z l 0 ) + η Zl 0 φ (Z l 0 )∂ η β l )) = 2 E(φ 2 ) (Z l 1 )(β l Zl 0 φ (Z l 0 ) + η Zl 0 φ (Z l 0 )∂ η β l ) 2 + • • •</formula><p>where other terms appear in this sum but they vanish at η = 0 because Zl 0 appears unpaired in the expectation. Thus, E ∂ 2 η φ 2 (Z l 1 )| η=0 = 2(β l (0)) 2 E( Zl 0 ) 2 E(φ 2 ) (Z l 0 )φ (Z l 0 ) 2 . Plugging this back in, we get the recurrence on ∂ 2 η λ l (0). The ∂ 2 η γ l (0) recurrence is derived similarly.</p><p>The following result will be useful for showing ∂ 3 η f1 (ξ 0 ) = 0. Theorem H.42. Define Proof. Similar to the proof of Theorem H.41.</p><p>The following result will be useful for showing prefeature kernel evolution. Theorem H.43. For all l ≥ ,</p><formula xml:id="formula_213">∂ 2 η E(Z l 1 ) 2 | η=0 = 2C + γ l 11 (0)∂ 2 η λ l-1 (0),</formula><p>where C = 2(β l (0)) 2 E( Zl 0 ) 2 &gt; 0.</p><p>Proof. Similar to the proof of Theorem H.41.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.7.2 Applications to σ-Gelu</head><p>The following proposition regarding σ-gelu is easy to verify. Proposition H.44. Let φ be σ-gelu. For any centered Gaussian Z ∈ R with nonzero variance, E(φ 2 ) (Z), E(φ 2 ) (Z)φ (Z) 2 , E φ(Z)φ (Z)φ (Z) 2 , E φ(Z)φ (Z), E φ (Z) 2 &gt; 0, and they converge to 0 as σ → 0. Also, E φ (Z)φ (Z) <ref type="bibr" target="#b2">3</ref> , E φ (Z)φ (Z) &lt; 0, and they converge to -∞ as σ → 0.</p><p>In particular, this means κ l 1 , κ l 2 , γ l 22 &gt; 0, κ l 3 , γ l 02 (0), κl 3 , γ l 13 &lt; 0. Theorem H.48. Consider a pseudostable parametrization with r = 0. If φ is tanh, then for all l ≥ , ∂ 2 η γ l (0) &lt; 0, ∂ 2 η λ l (0) &gt; 0.</p><p>Proof. Similar to the proof of Theorem H.45, except that here κ l 3 , γ l 02 (0) &lt; 0, making ∂ 2 η γ l (0) &lt; 0. . if there is feature learning or feature kernel evolution or prefeature learning or prefeature kernel evolution in layer l, then there is feature learning and feature kernel evolution and prefeature learning and prefeature kernel evolution in layers l, . . . , L.</p><p>Proof. The parametrization cannot be in kernel regime since ∂ 3 η f1 (ξ 0 ) = 0 by Theorem H.49 or Theorem H. 46. By Theorem H.45 or Theorem H.48, ∂ 2 η λ l (0) &gt; 0 for all l ≥ , so the feature kernel evolves in layer , . . . , L, for some normalized learning rate η &gt; 0. This implies feature learning in layer , . . . , L, since Z x L 1 (ξ0) -Z x L 0 = 0 in this case. This then implies Z h L 1 (ξ0) -Z h L 0 = 0, so we have prefeature learning in layer , . . . , L. Prefeature kernel evolution in layer , . . . , L is implied by Theorem H.43. Finally, the last statement follows clearly from our logic above.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Take 𝑤𝑖𝑑𝑡ℎ → ∞ Limit via Tensor Programs Prior works This work Key Theoretical Idea: Tensor Programs In Section 7 and Appendix H.4, we describe the Tensor Programs technique for deriving (rigorously) the infinite-width training dynamics of any abc-parametrization. The main insight of this approach is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>we sample the initial set of vectors V like {h α : h ∈ V} ∼ Z V iid for each α ∈ [n]. 3) For each initial scalar θ ∈ C, we require θ a.s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Empirical Simulation Agrees with Theory. We analytically compute the infinite-width µP limit for the three kinds of networks (depth 1, depth 2 decoupled, depth 2) described in Section 6, with either quadratic φ(x) = x 2 or linear φ(x) = x activation. The training set is random ξ t ∈ {±1}, y t ∈ {±1}, so that the deviation of finite width from infinite width losses are accentuated. We compare against finite width µP networks with width 1024 or 4096. For each width, we randomly initialize with 100 different seeds and aggregate the loss curves. The mean across these seeds is plotted as solid curves, and the standard deviation represented by the shade. As discussed in Section 8, nonlinear activation functions and higher depth face computational difficulties exponential with training time. Thus here we only train for a few steps. We observe that the quadratic network converges slower to the limit with width. This is expected since the tail of Z xt is fatter for a quadratic activation than a linear activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>In Model Agnostic Meta-Learning (MAML), the model performs few-shot learning by one or more SGD steps on the given training data; this is called adaptation. In a pretraining (also called meta-training) phase, MAML learns a good initialization of the model parameters for this adaptation. The training objective is to minimize the loss on a random task's test set after the model has adapted to its training set. More precisely, the basic First Order MAML at training time goes as follows: With f θ denoting the model with parameters θ, and with step sizes , η, we do 1. At each time point, sample a few-shot task T 2. From T , sample a training set D 3. Adapt θ ← θ -∇ θ L D (f θ ), where L D (f θ ) is the loss of f θ over D 4. Sample a test set D from T 5. Update θ ← θ -η∇ θ L D (f θ ), where L D (f θ ) is the loss of f θ over D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Findings</head><figDesc>Our results are summarized in the Figure to the right and Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>C. 1</head><label>1</label><figDesc>Maximal Update Parametrization MLP with Biases Suppose in Eq. (1), for each l ∈ [L], we have h l (ξ) = W l x l-1 (ξ) + b l instead, for bias b l ∈ R n . Then in µP, the bias b l should have a b l = -1/2 and b b l = 1/2. We can also have bias b L+1 in the logits f (ξ) = W L+1 x L (ξ) + b L+1 . Then we set a b L+1 = b b L+1 = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>•</head><figDesc>σ u : [0.5, 1, 2, 4, 8], Algorithm 4 MAML Training of Kernel Model with Kernel K Input: Kernel K, adaptation step size , meta learning rate η, batch size B, gradient clip g 1: Initialize Q = {} 2: while True do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: 2D Projection of the Boundary of the Uniform Stable Polyhedron (Equivalently, the Boundary of the Stable Polyhedron for L = 1).Here, we label each facet and edge of the graph with orange text to indicate the corresponding defining algebraic condition in the L = 1 case (as part of the stable polyhedron, assuming c = 0 and b 1 = -a 1 ), and with black text to indicate the verbal interpretation valid for all L (as part of the uniform stable polyhedron). We obtain the caricature in Fig.2by taking the nontrivial subspace of the graph here and quotienting the two facets by their respective points at infinity. Explanation of some captions: GP limit means the training dynamics amounts to training only the last layer in the infinite-width limit, starting from a nonzero initial GP. Body NTK limit means NTK dynamics except the last layer does not contribute to the NT kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>convergence in distribution, and ft+1 (ξ) = ft (ξ) -ηK(ξ, ξ t )L ( ft (ξ t ), y t ), ∀t ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Theorem H. 13 (</head><label>13</label><figDesc>Classification of abc-Parametrizations). Suppose φ is tanh or σ-gelu for sufficiently small σ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then 1. The following are equivalent to r = 0 (a) feature learning (b) feature learning in the Lth layer (c) feature kernels evolution (d) feature kernel evolution in the Lth layer (e) prefeature learning (f) prefeature learning in the Lth layer (g) prefeature kernels evolution (h) prefeature kernel evolution in the Lth layer 2. The following are equivalent to r &gt; 0 (a) kernel regime (b) fixes all features (c) fixes features in the Lth layer (d) fixes all feature kernels (e) fixes feature kernel in the Lth layer (f) fixes all prefeatures (g) fixes prefeatures in the Lth layer (h) fixes all prefeature kernels (i) fixes prefeature kernel in the Lth layer 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 n= W L+1 0 -</head><label>10</label><figDesc>X (XX /n) + , Π = I -1 n X (XX /n) + X. Let Σ def = XX /n and γ def = (X W L+1 0 /n). Then Π W L+1 0 X Σ + γ,and√ nX + g = 1 √ n X Σ + g.By the Master Theorem, γ a.s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Corollary H. 33 .</head><label>33</label><figDesc>A pseudostable parametrization with r &gt; 0 is nontrivial iffa L+1 + b L+1 + r = 1 or 2a L+1 + c = 1. Proof. The kernel Σ in Theorem H.32 is nonzero iff θ L+1 or θ Lf is 1, which is equivalent to saying a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1. Corollary H.34. An initialization-stable parametrization with r &gt; 0 but a L+1 + b L+1 + r &lt; 1 or 2a L+1 + c &lt; 1 is not stable. Proof. If a L+1 + b L+1 + r &lt; 1 or 2a L+1 + c &lt; 1, then θ L+1 → ∞ or θ Lf → ∞.Clearly, from the definition, Σ mL (ξ, ξ) &gt; 0 for any ξ = 0 and m ∈ [0, L]. All of our reasoning leading up to Theorem H.32 applied at t = 1 holds, so Theorem H.32 (along with the Master Theorem) implies |δf t (ξ)| a.s. --→ ∞. Corollary H.35. If a L+1 + b L+1 + r &gt; 1 and 2a L+1 + c = 1, then for all ξ ∈ X , ft (ξ) a.s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>κl 3 def= 3 , γ l 13 def=, γ l 22 def= E φ (Z l 0 ) 2 . 2 η</head><label>33132222</label><figDesc>E φ (Z l 0 )φ (Z l 0 ) E φ (Z l 0 )φ (Z l 0 )Then for all l ≥ ,λ l-1 (0) + γ l 22 ∂ 2 η γ l-1 (0),where C = 2(β l (0)) 2 E( Zl 0 ) 2 &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Theorem H. 49 .</head><label>49</label><figDesc>Consider a pseudostable parametrization with r = 0. Suppose a L+1 + b L+1 + r = 1or 2a L+1 + c = 1. If φ is tanh, then ∂ 3 η f1 (ξ 0 ) = 0.Proof. Similar to the proof of Theorem H.46, except in the expression∂ 3 η f1 (ξ 0 ) = -θ L+1 ∂ 2 η γ L (0) + θ Lf ∂ 2 η (γ L 11 γ L-1 )(0) , ∂ 2 η γ L (0) and ∂ 2 η (γ L 11 γ L-1 )(0) are both negative.The former is because of Theorem H.48. The latter is because ∂ 2 η γ L-1 (0) ≤ 0 for the same reason, and ∂ 2 η γ L 11 (0) &lt; 0 since κl 3 , γ l 13 &lt; 0, γ l 22 &gt; 0 by Proposition H.47.H.7.4 Main ResultsProposition H.50. Suppose φ is tanh or σ-gelu for sufficiently small σ. A pseudostable parametrization with r = 0 is nontrivial iffa L+1 + b L+1 = 1 or 2a L+1 + c = 1. Proof. If a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1, then Theorem H.46 and Theorem H.49 show that the parametrization is nontrivial. Otherwise, it is trivial by Proposition H.25.Theorem H.51. Suppose φ is tanh or σ-gelu for sufficiently small σ. For any nontrivial pseudostable parametrization with r = 0, the following are true of the parametrization: 1. not in kernel regime 2. feature learning 3. feature learning in the Lth layer 4. feature kernels evolution 5. feature kernel evolution in the Lth layer 6. prefeature learning 7. prefeature learning in the Lth layer 8. prefeature kernels evolution 9. prefeature kernel evolution in the Lth layer 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Omniglot Meta-Test Accuracies after Pretraining with First Order MAML.</figDesc><table><row><cell cols="2">φ = relu</cell><cell></cell><cell></cell><cell cols="5">φ = identity ; number = log 2 width</cell><cell></cell><cell></cell></row><row><cell>GP</cell><cell>NTK</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell><cell>µP</cell><cell>GP/NTK</cell></row><row><cell cols="10">47.60 47.82 55.34 64.54 66.21 66.31 66.43 66.36 66.41 66.42</cell><cell>41.68</cell></row><row><cell cols="10">±.02 ±.04 ±1.24 ±0.70 ±.15 ±.16 ±.23 ±.22 ±.18 ±.19</cell><cell>±.09</cell></row><row><cell cols="11">Few-Shot Learning Terminologies An N -way classification task asks the model to predict a class</cell></row><row><cell cols="11">from N possiblities. A K-shot classification task provides K input/output pairs per class, for a total</cell></row><row><cell cols="5">of N K training points for N -way classification.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test Accuracies on Word Analogy after Pretraining with CBOW Word2Vec. number = log 2 width</figDesc><table><row><cell>Dataset</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>µP</cell><cell>GP/NTK</cell></row><row><cell cols="5">text8 33.35 41.58 42.56 43.31</cell><cell>0.0</cell></row><row><cell>fil9</cell><cell cols="4">44.39 54.24 55.69 56.45</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Best hyperparameters for the MAML experiment.Let f Q be the function with W given by Q. Then</figDesc><table><row><cell>log 2 Width/Limit</cell><cell>σ u</cell><cell>σ v</cell><cell>σ b</cell><cell>η</cell><cell>α</cell><cell cols="2">Val. Acc. (%) Test Acc. (%)</cell></row><row><cell>1</cell><cell>0.5</cell><cell>0.5</cell><cell>-</cell><cell>0.05</cell><cell>2</cell><cell cols="2">46.72 ± 4.30 55.34 ± 1.24</cell></row><row><cell>3</cell><cell>0.5</cell><cell>0.25</cell><cell>-</cell><cell>0.1</cell><cell>1</cell><cell>65.30 ± .27</cell><cell>64.54 ± .70</cell></row><row><cell>5</cell><cell>1</cell><cell>0.125</cell><cell>-</cell><cell cols="2">0.4 0.5</cell><cell>68.74 ± .18</cell><cell>66.21 ± .15</cell></row><row><cell>7</cell><cell>1</cell><cell>0.125</cell><cell>-</cell><cell>0.1</cell><cell>1</cell><cell>69.03 ± .04</cell><cell>66.31 ± .16</cell></row><row><cell>9</cell><cell>1</cell><cell>0.03125</cell><cell>-</cell><cell>0.1</cell><cell>1</cell><cell>69.32 ± .07</cell><cell>66.43 ± .23</cell></row><row><cell>11</cell><cell>1</cell><cell>0.03125</cell><cell>-</cell><cell>0.1</cell><cell>1</cell><cell>69.27 ± .11</cell><cell>66.36 ± .22</cell></row><row><cell>13</cell><cell>1</cell><cell>0.03125</cell><cell>-</cell><cell>0.1</cell><cell>1</cell><cell>69.27 ± .14</cell><cell>66.41 ± .18</cell></row><row><cell>µP</cell><cell>1</cell><cell>0.03125</cell><cell>-</cell><cell>0.1</cell><cell>1</cell><cell>69.26 ± .13</cell><cell>66.42 ± .19</cell></row><row><cell>NTK</cell><cell>0.25</cell><cell>1</cell><cell cols="2">1 0.05</cell><cell>1</cell><cell>47.47 ± .13</cell><cell>47.82 ± .04</cell></row><row><cell>GP</cell><cell>1</cell><cell>0.25</cell><cell cols="2">1 0.05</cell><cell>1</cell><cell>38.92 ± .15</cell><cell>47.60 ± .02</cell></row><row><cell>at all times.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Short for Model Agnostic Meta-Learning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Observe that by changing a l , b l while holding a l + b l fixed, we effectively give layer l its own learning rate.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>One can further include a set of constants in front of n -a l and n -b l , for example powers of input dimension d, but we shall keep it simple here as we are only concerned with scaling behavior with n.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>This is also known as the "fanin" or "Lecun" initialization; "Kaiming" initialization is the same up to multiplicative constants. The default in Tensorflow<ref type="bibr" target="#b0">[1]</ref> uses Glorot initialization, where the variance of an entry scales like 1/(f anin + f anout). This causes the first layer preactivation to converge to 0 as n → ∞, and thus yields pathological behavior in the limit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We stress this is in the n → ∞ limit, so does not contradict the feature learning seen in finite-width SP NN.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>e.g. by extending the example programs of<ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51]</ref>, which express only the first forward and backward passes, into the entire training computation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>In fact, empirically we observe such Gaussian random initialization to be crucial to performance compared to the mean-field-style initialization in this literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Actually, it is more similar to the Gaussian matrix in asymmetric message passing<ref type="bibr" target="#b5">[6]</ref> in that care must be taken to keep track of correlation between W and W .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>We won't expand further here, but it can be derived straightforwardly from the Master Theorem (Theorem 7.4).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>Contrast this with a common semantics ofv = O(n a ) as v = O(n a ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>χ0 = L (f0, y0) = Θ(1) because f0 has variance Θ(1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>here the limit should be construed as almost sure limits; seeTheorem 7.4.   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12"><p>but they may depend on training time and η; in particular, it's possible that they diverge with time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>In particular, it's possible for the function f to stay fixed with time, but for the features to change.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14"><p>For simplicity, we only consider batch size 1; it's straightforward to generalize to larger batch sizes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_15"><p>For the sake of streamlining the main text presentation, we defined feature learning and feature kernel evolution slightly differently than in Definition H.9, but ultimately they are equivalent as a result of our theorems.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_16"><p>It may seem that Neural Tangent Hierarchy<ref type="bibr" target="#b24">[25]</ref>, which allow some kind of higher order dynamics in the function space, violates our observation. But their infinite-width limit is identical to NTK in the constant time t = O(1) regime, which is what Remark 3.12 (and this paper) concerns. Moreover, here we are talking about functional dynamics that doesn't depend on n (because we are already at the n → ∞ limit) whereas their functional dynamics does.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_17"><p>linear and nonlinear; see Theorem H.17.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_18"><p>Recall that training routine means a package of learning rate ηn -c , training sequence {(ξt, yt)} t≥0 , and a loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_19"><p>It is indeed possible to perform feature learning in a trivial parametrization, e.g. b l = 1/2 ∀l, a1 = -1/2, a2 = 100 + 1/2, c = -100 in a 2-hidden-layer MLP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_20"><p>e.g. take aL+1 = 100 + 1/2, bL+1 = -100 + 1/2, then ∆W L+1 is negligible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_21"><p>Again, more generally, we can insert constants in this parametrization, like U = √ n √ d u, but we omit them here for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_22"><p>All convergence in this section will be almost sure, but to focus on the intuition here and less on the formalities, we do not explicitly write this down.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_23"><p>This roughly means that φ has a polynomially bounded weak derivative; see Definition F.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_24"><p><ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> present the equations in terms of the PDF of Z random variables. Formally, the PDF limit can be obtained by taking the continous-time limit of Eqs. (18) and (19) and then applying Focker-Planck. Note our derivation, when formalized using the Tensor Programs framework below, does not require smoothness and support assumptions on the initialization of U, V in those works: The initialization distribution here can be replaced with any image of Gaussians under pseudo-Lipschitz functions, which includes nonsmooth and singular distributions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="34" xml:id="foot_25"><p>This roughly means that φ has a polynomially bounded weak derivative; see Definition F.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35" xml:id="foot_26"><p>This roughly means that φ has a polynomially bounded weak derivative; see Definition F.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="36" xml:id="foot_27"><p>that may depend on various scalars such as χs, E Z xs Z x s (ξ) , and E Z d hs Z d hs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="37" xml:id="foot_28"><p>What we refer to as Tensor Program is the same as NETSOR + in Yang<ref type="bibr" target="#b51">[52]</ref>; we will not talk about other languages (like NETSOR ) so this should not cause any confusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="38" xml:id="foot_29"><p>Here we write nV0 instead of V0 because we want all vectors to have Θ(1) coordinates; see Setup 7.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="39" xml:id="foot_30"><p>In Section 6 we assumed input dimension is 1. In general, each column of U0 would be a separate initial vector. Likewise, if the output dimension is greater than 1, then each row of V0 would be a separate initial vector.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="40" xml:id="foot_31"><p>Since {ξt, yt}t are fixed with n, we can WLOG absorb them into any nonlinearities in Nonlin that they are involved in, and set C = ∅. But, in kernel regime or nonmaximal feature learning parametrization, we usually have initial scalars, such as n -2a L+1 -c , that tend to 0 with n; see Appendix H.4.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="41" xml:id="foot_32"><p>This implies an explicit convergence in distribution (see<ref type="bibr" target="#b51">[52]</ref>), but this convergence in distribution is strictly weaker than the formulation in Theorem 7.4, which is in general much more useful.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="42" xml:id="foot_33"><p>Because we will tune initialization variances, our results also represent finite-width SP networks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="43" xml:id="foot_34"><p>Note that the transfer learning comment in Section 3.1 does not apply directly to the few-shot setting here, because the readout weights of the network carry over from the pretraining phase. Nevertheless, we will see a large performance gap between the kernel limits (2,3) and the µP limit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="44" xml:id="foot_35"><p>One can write down gradient clipping easily in a Tensor Program, so the its infinite-width limit can be computed straightforwardly via Theorem 7.4; see Appendix D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="45" xml:id="foot_36"><p>http://mattmahoney.net/dc/textdata.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="46" xml:id="foot_37"><p>There is some nuance here because h(ξ) h( ξ) is actually Θ( √ n) instead of Θ(n) because ξ, ξ are one-hot, but the conclusion is the same; see Appendix D.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="48" xml:id="foot_38"><p>Here the randomness comes from initialization: the argmax is different for different random initializations, but it is fixed throughout training in the large width limit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="49" xml:id="foot_39"><p>as is conventional in the machine learning literature, the convergence in distribution we mean here is really over finite dimensional marginals, i.e. (f0(ξ1), . . . , f0(ξ k )) d -→ ( f0(ξ1), . . . , f0(ξk)) where f0 is the limit GP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="52" xml:id="foot_40"><p>For e.g. the NTK limit, f0 is a GP, so that we should expect the bounds on ∆h l t (ξ), ∆x l t (ξ) to depend on f0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="53" xml:id="foot_41"><p>Here because we want to avoid topological issues arising for convergence in distribution of infinite sequences, we only require convergence in distribution jointly in all ξ ∈ X and time t below some cutoff T for every finite T .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="64" xml:id="foot_42"><p>Again, if aL+1 + bL+1 = 1/2, remember we are conditioning on f0(ξ), ξ ∈ X .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>In alphabetical order, we thank <rs type="person">Sina Alemohammad</rs>, <rs type="person">Zeyuan Allen-Zhu</rs>, <rs type="person">Francis Bach</rs>, <rs type="person">Yasaman Bahri</rs>, <rs type="person">Lenaic Chizat</rs>, <rs type="person">Jeremy Cohen</rs>, <rs type="person">Yarin Gal</rs>, <rs type="person">Quanquan Gu</rs>, <rs type="person">Bobby He</rs>, <rs type="person">Di He</rs>, <rs type="person">Jiaoyang Huang</rs>, <rs type="person">Arthur Jacot</rs>, <rs type="person">Jaehoon Lee</rs>, <rs type="person">Jason Lee</rs>, <rs type="person">Zhiyuan Li</rs>, <rs type="person">Etai Littwin</rs>, <rs type="person">Yiping Lu</rs>, <rs type="person">Song Mei</rs>, <rs type="person">Roman Novak</rs>, <rs type="person">Vinay Rao</rs>, <rs type="person">Michael Santacroce</rs>, <rs type="person">Sam Schoenholz</rs>, <rs type="person">Lisa Schut</rs>, <rs type="person">Jascha Sohl-Dickstein</rs>, <rs type="person">Alessandro Sordoni</rs>, <rs type="person">Denny Wu</rs>, <rs type="person">Huishuai Zhang</rs>, and <rs type="person">Pengchuan Zhang</rs> for discusson and feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Remark H. 16. The equivalence between kernel regime and fixed feature kernel implies that linear transfer learning is trivialized in any kernel regime limit. This is where the classifier layer of the pretrained network is discarded and a new one (potentially outputting to a new output space) is trained on top of the body of the pretrained network. But we can in fact say more: any nonlinear transfer learning, where we replace the classifier layer with a neural network instead of a linear layer, is trivialized as well. In addition, linear or nonlinear transfer learning has no effect even if we finetune the entire network, instead of just the new classification network. The intuitive reason for this is that, as discussed in Appendix B, the effect of ∆x L (ξ) on the output of the MLP is solely through the interaction with W L+1 0 . If W L+1 , W L+2 , . . . , are sampled anew, then this effect vanishes. We formalize this below.</p><p>Theorem H.17 (Kernel Regime Limit Trivializes Transfer Learning). Suppose f is an L-hidden-layer MLP <ref type="bibr" target="#b53">54</ref> in a stable kernel regime parametrization. Let A and B be two training routines. <ref type="bibr" target="#b54">55</ref> For any T, t ≥ 0, <ref type="bibr" target="#b55">56</ref> we define a network <ref type="bibr" target="#b56">57</ref> g T ;t as follows. Train f on A for T steps to obtain f T . Then discard W L+1 in f T and extend the body of f T into an M -hidden-layer MLP g, where M ≥ L. <ref type="bibr">58</ref> Parametrize and initialize the new weights of g according to any stable abc-parametrization that extends the parametrization of f . Train g on B for t steps to obtain g T ;t . Then 1. (Finetuning the whole network) As n → ∞, for any ξ ∈ X and T, t ≥ 0, g T ;t (ξ) -g 0;t (ξ) a.s.</p><p>--→ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">(Training only the classifier)</head><p>The above is true even if we define g T ;t by only training the new weights W L+1 , . . . , W M in g.</p><p>The Organization for the Proof of Our Main Results Above Definition H. 18. Below, we will abbreviate abc-parametrization of an L-layer MLP to just parametrization. We will call parametrizations satisfying the conditions of Theorem H.6 pseudostable while we try to prove Theorem H.6 (which, in this terminology, says stability and pseudostability are equivalent).</p><p>We first characterize stability at initialization and prove Eq. ( <ref type="formula">40</ref>) holds iff Eq. ( <ref type="formula">41</ref>) (Appendix H.2). Then, we describe the Tensor Program encoding the SGD of an MLP, assuming its parametrization is pseudostable. The Master Theorem then naturally lets us calculate its infinite-width limit. We then divide into the case of r &gt; 0 and r = 0. In the former case, we show the infinite-width limit is described by kernel gradient descent as in Eq. ( <ref type="formula">45</ref>). In the latter case, we construct a training routine where feature learning occurs and where the limit is not given by kernel gradient descent for any kernel. Finally, in Appendix H.8, we combine all of our analyses to prove the main results in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Stability at Initialization</head><p>In this section, we characterize stability at initialization, which will form a foundation for our later results.</p><p>Theorem H.19. Assume φ is not zero almost everywhere. For any parametrization, Eq. (40) holds iff Eq. (41) holds, i.e. the following are equivalent 1. For every nonzero input ξ ∈ X ,</p><p>, where the expectation is taken over the random initialization. <ref type="bibr" target="#b53">54</ref> the "pretrained network" <ref type="bibr" target="#b54">55</ref> the "pretraining dataset" and the "finetuning dataset" <ref type="bibr" target="#b55">56</ref> the "pretraining time" and "finetuning time" <ref type="bibr" target="#b56">57</ref> the "finetuned network" <ref type="bibr">58</ref> If M = L, then this is linear transfer learning where we replace just the last layer of f ; otherwise, it's nonlinear transfer learning.</p><p>If x = x l , h = h l , then x = φ(h), and (using θ x = θ h (Eq. ( <ref type="formula">46</ref>))),</p><p>where Ψ is precisely the difference quotient for the function φ. 62</p><p>The Function Outputs We do not construct f t (ξ) directly, but rather through scalars</p><p>, but we shall write it slightly differently in the program:</p><p>t-1 is constructed in Eq. ( <ref type="formula">50</ref>).</p><p>H.4.4 tth Backward Pass, t ≥ 1</p><p>In the last layer, we construct</p><p>For each l = L, . . . , 1 for dh l and l = L, . . . , 2 for dx l-1 , we also calculate</p><p>where c s = dh l s dh l t (ξ) n</p><p>. For ξ = ξ t and its label y t , we define 63</p><p>Finally, we compute the (normalized) change in W L+1 after this SGD update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>δW L+1</head><p>t+1 := -ηχ t x L t (ξ t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.5 The Infinite-Width Limit</head><p>In this section, we describe the Z random variables (Definition 7.3) corresponding to the vectors of the program constructed above. According to the Master Theorem, each such vector z will have roughly iid coordinates distributed like Z z in the large n limit.</p><p>Let θ• denote the limit of any θ • in Appendix H.3. If pseudostability holds, then θ• is either 0 or 1, as one can easily verify. We can construct the Z random variables for each vector in the program, as follows.</p><p>1. For the first forward and backward passes, we have,</p><p>62 The pseudo-Lipschitzness of φ assumed in Assumption H.22 implies that Ψ here is pseudo-Lipschitz, so that we can ultimately apply our Master Theorem. <ref type="bibr">63</ref> Here we use Moment with the function φ(; ft(ξt)) = L (ft(ξt), yt) with no input and one parameter (we absorb yt into φ since it does not change with n). The continuity of L in its first argument satisfies Assumption F.4(1), so the Master Theorem can apply.</p><p>If the parametrization is pseudostable, then all the θ • converge to 0 or 1 so Setup 7.2 is satisfied. Therefore, the Master Theorem applies and says that, for any collection of vectors v 1 , . . . , v k such that Z v i is defined above, we have</p><p>for any pseudo-Lipschitz ψ. In addition, <ref type="bibr">64</ref> δf t (ξ) a.s.</p><p>--→ δ ft (ξ), f t (ξ) a.s.</p><p>--→ ft (ξ), χ t a.s.</p><p>--→ χt , ∀ξ ∈ X , t ≥ 1.</p><p>We now describe some immediate consequences of this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.5.1 Some Immediate Results</head><p>Proposition H.25. A pseudostable parametrization is trivial if</p><p>Proof. In this case, θ L+1 , θ Lf , θ L,L+1 → 0, and δ ft (ξ) = 0 for all t and ξ ∈ X by Eq. (58).</p><p>Proposition H.26. A pseudostable parametrization is stable.</p><p>Proof. For a pseudostable parametrization, all of θs converge to 1 or 0, and all of the Z δh l t (ξ) , Z δx l t (ξ)</p><p>have well defined (finite) limits, which implies</p><p>Proposition H.27. Consider a pseudostable parametrization. If r &gt; 0, then it fixes all (pre)features and all (pre)feature kernels. In addition, ∆W L+1 t ∆x L t (ξ)</p><p>a.s.</p><p>--→ 0.</p><p>--→ 0 by Eq. ( <ref type="formula">52</ref>) and the Master Theorem, i.e. all features are fixed. Similarly, for any pair ξ, ξ ∈ X , z t (ξ) z t ( ξ)/n -z 0 (ξ) z 0 ( ξ)/n a.s.</p><p>--→ 0, so all feature kernels are fixed. Finally, r &gt; 0 implies θ L,L+1 → 0, which means</p><p>--→ 0 by the Master Theorem.</p><p>Proposition H.28. An initialization-stable parametrization with r &lt; 0 is not stable.</p><p>Proof. If r &lt; 0, then there is some</p><p>Therefore, under these conditions, and with a training sequence that has χ0 = 0, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.6 r &gt; 0 Implies Kernel Regime</head><p>In this section, we analyze the case when r &gt; 0. Our main result is deriving the corresponding infinite-width kernel gradient descent dynamics (Theorem H.32). Nothing here depends on φ being tanh or σ-gelu.</p><p>Preliminary Derivations If r &gt; 0, then θl = θW l = 0 for all l ∈ [L], so that we have</p><p>Then for l ≥ + 1 and shorthand h = h l , x = x l-1 , W = W l , we have θx/h = 1, θW x/h = 0 and, by Eq. ( <ref type="formula">55</ref>),</p><p>where θW/h can be either 0 or 1. For l = , because</p><p>) so θx/h = θW x/h = 0 and θW/h = 1, we also have</p><p>Finally, for all l ∈ [L], we have, by Eq. ( <ref type="formula">53</ref>),</p><p>Definition H.29. For 1 ≤ m ≤ l and ξ, ζ ∈ X , define</p><p>We also define</p><p>For example,</p><p>and so on.</p><p>Notation For brevity, below we will shorthand ϑ m = θ W m /h m . We write Z x ≡ Z y mod ẐW • if Z x -Z y is a linear combination of ẐW u for various vectors u. Lemma H.30. For any input ξ, any l ≥ , at any time t,</p><p>Proof. We proceed by induction.</p><p>Base Case l = : this is given by Eq. (60).</p><p>Induction: Assume Eq. ( <ref type="formula">61</ref>) holds for l -1, and we shall prove it for l.</p><p>To alleviate notation, we write</p><p>i.e. we use • to denote time t -1 in contrast to • for time t, and we suppress layer index. In contrast, we will write h l 0 , h l t , and ξ for their usual meanings. First, note that Z δx(ξ) = φ (Z h(ξ) )Z δh(ξ) by Eq. (53). Because Z h(ξ) = Z h0(ξ) , and, by induction hypothesis, Z δh(ξ) is a scalar multiple of Z dh0( ξ) = Z dx0( ξ) φ (Z h0( ξ) ), Z δx(ξ) is symbolically solely a function of Z h0(ξ) , Z h0( ξ) , Z dx0( ξ) ,all of which are equal to their Ẑ versions (with the last due to Proposition H.24). Among these, only Z dx0( ξ) = Z W dh l 0 ( ξ) is constructed from matrix multiplication with W 0 . Thus,</p><p>By induction hypothesis,</p><p>Therefore,</p><p>By definition of Σ ml , this equals</p><p>Plugging this back into Eq. ( <ref type="formula">62</ref>), we get</p><p>Finally, by Eq. ( <ref type="formula">59</ref>),</p><p>Together with Eq. ( <ref type="formula">63</ref>), this completes the induction.</p><p>Lemma H.31. Assume pseudostability, r &gt; 0, and</p><p>Therefore θ Lf = 0. Theorem H.32. Consider a pseudostable parametrization. At any time t, for any input ξ ∈ X , we have δ ft (ξ) = -ηχ t-1 Σ(ξ t-1 , ξ), where the kernel Σ is defined for any ξ, ζ ∈ X by</p><p>Observe that in the NTK parametrization, = 1, and θ L+1 = θ Lf = θm+1 = 1 for all m, so Σ = L m=0 Σ mL is precisely the NTK (for MLP without biases).</p><p>Proof. By Eqs. ( <ref type="formula">57</ref>) and (58),</p><p>Now by Lemma H.31, either θL+1/f = 0 or θ Lf = 0. In both cases, (Z δW L+1 1</p><p>contributes 0 to δ ft (ξ). So we can replace Z W L+1 t-1 with Z W L+1 0 above, and write</p><p>.</p><p>from E(Z x L 0 ) 2 , which would imply feature kernel evolution in layer L. To do so, we compute ∂ 2 η E(Z x L 1 (ξ0) ) 2 evaluated at η = 0 and show it is nonzero (it turns out ∂ η vanishes, so the next best thing is ∂ 2 η ). This then also implies feature learning in layer L. Analogous results for prefeatures and for other layers can be derived similarly.</p><p>2. If the parametrization is in the kernel regime with kernel K, the first step of SGD in the large width limit would look like f1 (ξ) -f0 (ξ) = -ηχ 0 K(ξ, ξ 0 ); in particular, f1 (ξ) -f0 (ξ) is linear in η. To show that a pseudostable parametrization with r = 0 is not in the kernel regime, we will show</p><p>To calculate these η derivatives, we will derive recurrence relations involving quantities defined below (see Lemma H.38 and Theorem H.41).</p><p>Setup and Notation First, write</p><p>Note that Zl 0 is a centered Gaussian independent from Ẑl 0 = Z l 0 . Then we define</p><p>2 where the dependence on η is from Z l 1 . Naturally, since φ and φ are not almost everywhere zero, we have γ l (0), λ l (0), γ l 11 (0) &gt; 0. Note at η = 0, we have</p><p>is jointly Gaussian with mean zero and covariance</p><p>WLOG, for simplicity of notation, we assume we choose a training routine such that χ0 = 1. We assume ξ 0 = 0.</p><p>Since r = 0, WLOG we can suppose for some ∈ [L], we have</p><p>Lemma H.37. With the setup above, we have</p><p>, where β l is defined recursively by</p><p>Additionally, β l (0) &lt; 0 for all l ≥ .</p><p>Proof. Straightforward calculation using Moment and Zdot. Here, -γ l-1 (η) comes from ∆W l 1 x 1 1 (ξ 0 ) and β l-1 (η)γ l-1 11 (η) comes from Żh l 1 (ξ0) . Since γ l (0), γ l-1 11 (0) &gt; 0 for all l, the recurrence on β l implies that β l (0) &lt; 0 for all l ≥ . H.7.1 Deriving Recurrence Relations on ∂ η λ l , ∂ η γ l , ∂ 2 η λ l , ∂ 2 η γ l Below, we derive the recurrence relations required for our main result. They depend on the following constants.</p><p>Lemma H.38. With the setup above, we have, for all l ∈ [L],</p><p>Proof. We first derive the recurrence on ∂ η λ l . By Lemma H.39 below, we have</p><p>Note the second item in the sum evaluated at η = 0 is exactly the RHS of Eq. ( <ref type="formula">65</ref>). For the first item, since</p><p>we compute</p><p>At η = 0, Z l 1 = Z l 0 , so that Zl 0 is independent from everything else in the expectation. This implies the expectation is 0 and yields the recurrence for ∂ η λ l (0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20</head><p>. With Γ l-1 as in Eq. ( <ref type="formula">64</ref>), we have</p><p>By same reasoning as in Eq. ( <ref type="formula">65</ref>), the first term of this sum is zero when evaluated at η = 0. Since</p><p>Lemma H.39. Consider a twice continuously differentiable f and Gaussian vector Z ∼ N (0, Σ) such that f and Σ both depend on a parameter η. Then</p><p>where ∇ 2 denotes Hessian wrt z, and , denotes trace inner product of matrices.</p><p>Proof. Let p(z) denote the PDF of Z. We have</p><p>The second integral can be rewritten using integration-by-parts as E ∇ 2 f (z), ∂ η Σ . (e.g. see Lemma F.18 of [56])</p><p>We then easily have Theorem H. 40.</p><p>Proof. For l &lt; , we obviously have ∂ η γ l (η) = ∂ η λ l (0) = 0 for all η. Then this follows from Lemma H.38 and a simple induction.</p><p>Unfortunately, this means that the first η derivative doesn't give us what we need. So we try the second derivative, which will turn out to work.</p><p>This particularly implies that κ l 1 , κ l 2 , κ l 3 , γ l 02 (0), γ l 22 &gt; 0 and converges to 0 with small σ, but κl 3 , γ l 13 &lt; 0 and diverges to -∞ with small σ. Theorem H.45. Consider a pseudostable parametrization with r = 0. If φ is σ-gelu, then for all l ≥ , ∂ 2 η γ l (0), ∂ 2 η λ l (0) &gt; 0 and they converge to 0 as σ → 0.</p><p>Proof. We always have (β l (0)) 2 , E( Zl 0 ) 2 &gt; 0. By Proposition H.44, κ l 1 , κ l 2 &gt; 0 as well. Thus, by Theorem H.41, ∂ 2 η λ l (0) &gt; 0 for all l ≥ . By Proposition H.44, κ l 3 , γ l 02 (0) &gt; 0, so by Theorem H.41, ∂ 2 η γ l (0) &gt; 0 for all l ≥ as well. As σ → 0,</p><p>, where at least one of θ</p><p>Lf and θ L+1 is 1 because</p><p>We have</p><p>where we used Stein's Lemma for the last equality. Thus</p><p>Below we will show that for small σ, ∂ 2 η γ L (0) is small and positive and ∂ 2 η (γ L 11 γ L-1 )(0) is large and negative, so ∂ 3 η f1 (ξ 0 ) cannot be 0 no matter the values of θ L+1 and θ Lf . Claim: For sufficiently small σ, ∂ 2 η γ L 11 (0) &lt; 0. It converges to -∞ as σ → 0. Proof: By Theorem H.42, 45. Also, by Proposition H.44, κl 3 , γ l 13 &lt; 0, γ l 22 &gt; 0, and as σ → 0, κl 3 , γ l 13 → -∞, γ l 22 → 0 (as well as ∂ 2 η γ L-1 (0), ∂ 2 η λ l (0) → 0 by Theorem H.45). One can see that C converges to a positive constant as σ → 0 as well. Therefore, for small enough σ, ∂ 2 η γ l 11 (0) &lt; 0, and Finishing the main proof: Therefore, if θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.7.3 Applications to Tanh</head><p>The following property of tanh is easy to verify. Proposition H.47. Let φ = tanh. For any centered Gaussian Z ∈ R with nonzero variance,</p><p>Corollary H.52. Suppose φ is tanh or σ-gelu for sufficiently small σ. Consider any initializationstable parametrization with r = 0. If a L+1 + b L+1 &lt; 1 or 2a L+1 + c &lt; 1, then the parametrization is not stable.</p><p>Proof. First suppose a L+1 + b L+1 &lt; 1 and 2a</p><p>As in the proof of Theorem H.46, there is some η = 0 such that E Z W L+1 0 Z δx L 1 (ξ0) = R for some R = 0. Therefore, by the Master Theorem,</p><p>x L 1 (ξ 0 ), which by similar reasoning is O(1). So f 1 (ξ 0 ) diverges and the parametrization is not stable. Now suppose a L+1 + b L+1 ≥ 1 and 2a L+1 + c &lt; 1. This violates our simplifying assumption that a L+1 + b L+1 ≤ 2a L+1 + c, but it's easy to see that</p><p>, so f 1 (ξ 0 ) diverges. Therefore, the parametrization is not stable.  x L 1 (ξ 0 ), as in Theorem H.46; and in the case of tanh, both have the same sign, as in Theorem H. 49. In either case, f 1 (ξ 0 ) diverges, so the parametrization is not stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.8 Putting Everything Together</head><p>Finally, in this section we tie all of our insights above to prove our main theorems. Theorem H.53. Suppose φ is tanh or σ-gelu for sufficiently small σ. A parametrization is stable iff it is pseudostable. Proof. The "if" direction is given by Proposition H.26. We now show that when any (in)equality of pseudostability is violated, the parametrization is not stable.</p><p>First, if Eq. ( <ref type="formula">41</ref>) is not satisfied, then Theorem H. 19 shows lack of stability.</p><p>Second, if Eq. ( <ref type="formula">41</ref>) is satisfied but r &lt; 0, then Proposition H.28 shows lack of stability.</p><p>Finally, if Eq. ( <ref type="formula">41</ref>) is satisfied and r ≥ 0 but a L+1 +b L+1 &lt; 1 or 2a L+1 +c &lt; 1, then Corollary H.52 or Corollary H.34 shows lack of stability.</p><p>Given this result, we will now just say "stable" instead of "pseudostable" from here on. Theorem H.8 (Nontriviality Characterization). Suppose φ is tanh or σ-gelu for sufficiently small σ. A stable abc-parametrization is nontrivial iff a L+1 + b L+1 + r = 1 or 2a L+1 + c = 1.</p><p>Proof. The case of r = 0 and the case of r &gt; 0 are resp. given by Proposition H.50 and Corollary H.33.</p><p>Theorem H.13 (Classification of abc-Parametrizations). Suppose φ is tanh or σ-gelu for sufficiently small σ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then 1. The following are equivalent to r = 0 (a) feature learning (b) feature learning in the Lth layer (c) feature kernels evolution (d) feature kernel evolution in the Lth layer (e) prefeature learning (f) prefeature learning in the Lth layer (g) prefeature kernels evolution (h) prefeature kernel evolution in the Lth layer 2. The following are equivalent to r &gt; 0 (a) kernel regime (b) fixes all features (c) fixes features in the Lth layer (d) fixes all feature kernels (e) fixes feature kernel in the Lth layer (f) fixes all prefeatures (g) fixes prefeatures in the Lth layer (h) fixes all prefeature kernels (i) fixes prefeature kernel in the Lth layer 3. If there is feature learning or feature kernel evolution or prefeature learning or prefeature kernel evolution in layer l, then there is feature learning and feature kernel evolution and prefeature learning and prefeature kernel evolution in layers l, . . . , L.</p><p>4. If r = 0, then for all ξ ∈ X , f 0 (ξ) a.s.</p><p>--→ 0 and f t (ξ)</p><p>a.s.</p><p>--→ ft (ξ) for some deterministic ft (ξ). However, the converse is not true.</p><p>5. If r &gt; 0, a L+1 + b L+1 + r &gt; 1 and 2a L+1 + c = 1, then we have the Neural Network-Gaussian Process limit.</p><p>Proof. A nontrivial stable parametrization has either r = 0 or r &gt; 0. By Theorem H.51, Proposition H.27, and Theorem H.32, r = 0 implies all of the statements in (1) and r &gt; 0 implies all of the statements in (2). Consequently, if feature learning happens, then clearly r cannot be positive, so r must be 0. Likewise, all of the statements in (1) imply r = 0. Symmetrically, all of the statements in (2) about fixing features imply r &gt; 0. Finally, if the parametrization is in kernel regime, then by Theorem H.51(1), r cannot be 0, so r &gt; 0. This proves (1) and ( <ref type="formula">2</ref>).</p><p>If the premise of (3) holds, then by the above, r = 0, so the conclusion follows from Theorem H.51. This proves (3).</p><p>If r = 0, then nontriviality means a L+1 + b L+1 ≥ 1. This implies f 0 (ξ) a.s.</p><p>--→ 0 for all ξ ∈ X (more precisely, f 0 (ξ) has standard deviation Θ(n 1/2-(a L+1 +b L+1 ) ) → 0 by Central Limit Theorem). The program describes the unconditional SGD trajectory of f (as opposed to the case when a L+1 +b L+1 = 1/2), so f t (ξ) a.s.</p><p>--→ ft (ξ) does not depend on f 0 . The converse is not true, for example because of Corollary H.35. This prove (4).</p><p>(5) follows from Corollary H.35 (which actually allows much more general φ).</p><p>Proofs of Theorems 6.1, 6.3 and 6.4 For any finite subset X of the input space R d (where d = 1 here), we can write out the SGD computation as a Tensor Program like in Appendix H.4. Then the Master Theorem implies the convergence of f t (ξ) a.s.</p><p>--→ ft (ξ) for every ξ ∈ X . Let X 1 ⊆ • • • ⊆ X k ⊆ • • • be an infinite chain of finite subsets of R d such that k X k is a dense subset of R d . Then the convergence of f t (ξ) a.s.</p><p>--→ ft (ξ) holds for every ξ ∈ k X k (because we have almost sure convergence). Finally, we apply a continuity argument to get this convergence for all of R d : Because φ and thus φ are pseudo-Lipschitz, they are locally Lipschitz (i.e. Lipschitz on any compact set). In addition, the operator norms of W L are almost surely bounded from standard matrix operator bounds. Thus one can see that the Tensor Program is locally Lipschitz in ξ. Consequently, ft (ξ) is continuous in ξ. This allows to pass from k X k to R d .</p><p>Proofs of Propositions 5.3, 5.5 and G.2 and Theorems G.3 and G.4 follow by dividing into cases of r &gt; 0 and r = 0 and easy modification of the reasoning in Appendices H.6 and H.7.</p><p>Proof of Theorem H.17 follows from straightforward calculations. The basic outline of the calculations is: 1) During pretraining, f 's change is purely due to a) the interaction betwen ∆W l , l ≤ L, and W L+1 0 , and b) the interaction between x L and ∆W L+1 . 2) When W L+1 is re-initialized in g, these interactions are killed. The pretrained ∆W l , l ≤ L, will cause x M to differ by Θ(1/ √ n) coordinatewise compared to if ∆W l , l ≤ L, are all reset to 0, but this difference is uncorrelated with the last layer weights W M +1 of g, so their interaction is subleading in n, i.e. in the infinite-width limit, g T ;t (ξ) -g 0;t (ξ) a.s.</p><p>--→ 0, whether all of g or just the new weights are trained during fintetuning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Why bigger is not always better: on finite and infinite neural networks</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Aitchison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08013</idno>
		<ptr target="http://arxiv.org/abs/1910.08013" />
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Aitchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">W</forename><surname>Ober</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01590</idno>
		<ptr target="http://arxiv.org/abs/2010.01590" />
		<title level="m">Deep kernel processes</title>
		<imprint>
			<date type="published" when="2020-10">October 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Convergence Theory for Deep Learning via Over-Parameterization</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03962</idno>
		<ptr target="http://arxiv.org/abs/1811.03962" />
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A mean-field limit for certain deep neural networks</title>
		<author>
			<persName><forename type="first">Dyego</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">I</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yukimura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00193</idno>
		<ptr target="http://arxiv.org/abs/1906.00193" />
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The dynamics of message passing on dense graphs, with applications to compressed sensing</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Bayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2010.2094817</idno>
		<ptr target="http://arxiv.org/abs/1001.3448" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<idno type="ISSN">0018-9448</idno>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1557" to="9654" />
			<date type="published" when="2011-02">February 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<ptr target="http://arxiv.org/abs/2005.14165" />
		<title level="m">Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<ptr target="http://arxiv.org/abs/2005.14165" />
		<title level="m">Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/chen18i.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Note on Lazy Training in Supervised Differentiable Programming</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the Global Convergence of Gradient Descent for Overparameterized Models using Optimal Transport</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09545</idno>
		<ptr target="http://arxiv.org/abs/1805.09545" />
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04486</idno>
		<ptr target="http://arxiv.org/abs/2002.04486" />
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv: 1810.04805 version: 2</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradient Descent Provably Optimizes Over-parameterized Neural Networks</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<ptr target="http://arxiv.org/abs/1810.02054" />
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Cong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01452</idno>
		<idno>arXiv: 2007.01452</idno>
		<ptr target="http://arxiv.org/abs/2007.01452" />
		<title level="m">Modeling from Features: a Meanfield Framework for Over-parameterized Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<ptr target="http://arxiv.org/abs/1703.03400" />
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Dar</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.11572v1" />
		<title level="m">Wider Networks Learn Better Features</title>
		<imprint>
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Dar</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08987</idno>
		<ptr target="http://arxiv.org/abs/1901.08987" />
		<title level="m">Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs</title>
		<imprint>
			<date type="published" when="2019-01">January 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">A</forename><surname>Golikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06574</idno>
		<ptr target="http://arxiv.org/abs/2006.06574" />
		<title level="m">Dynamically Stable Infinite-Width Limits of Neural Classifiers</title>
		<imprint>
			<date type="published" when="2020-10">October 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.03744" />
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How to Start Training: The Effect of Initialization and Architecture</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01719</idno>
		<ptr target="http://arxiv.org/abs/1803.01719" />
		<imprint>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the Selection of Initialization and Activation Function for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Soufiane</forename><surname>Hayou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Rousseau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08266</idno>
		<ptr target="http://arxiv.org/abs/1805.08266" />
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<ptr target="http://arxiv.org/abs/1606.08415" />
		<title level="m">Gaussian Error Linear Units (GELUs)</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jiaoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horng-Tzer</forename><surname>Yau</surname></persName>
		</author>
		<title level="m">Dynamics of deep neural networks and neural tangent hierarchy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07572</idno>
		<ptr target="http://arxiv.org/abs/1806.07572" />
		<title level="m">Neural Tangent Kernel: Convergence and Generalization in Neural Networks</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The large learning rate phase of deep learning: the catapult mechanism</title>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02218</idno>
		<ptr target="http://arxiv.org/abs/2003.02218" />
		<imprint>
			<date type="published" when="2020-03">March 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning over-parametrized two-layer neural networks beyond ntk</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR, 09-12</idno>
		<ptr target="http://proceedings.mlr.press/v125/li20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Thirty Third Conference on Learning Theory</title>
		<editor>
			<persName><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shivani</forename><surname>Agarwal</surname></persName>
		</editor>
		<meeting>Thirty Third Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="2613" to="2682" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11794</idno>
		<ptr target="http://arxiv.org/abs/2002.11794" />
		<title level="m">Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A mean field view of the landscape of two-layer neural networks</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phan-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1806579115</idno>
		<ptr target="https://www.pnas.org/content/115/33/E7665" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2018-08">August 2018</date>
		</imprint>
	</monogr>
	<note>33): E7665-E7671</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06015</idno>
		<ptr target="http://arxiv.org/abs/1902.06015" />
		<title level="m">Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit</title>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<ptr target="http://arxiv.org/abs/1310.4546" />
		<title level="m">Distributed Representations of Words and Phrases and their Compositionality</title>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Phan-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02880</idno>
		<ptr target="http://arxiv.org/abs/1902.02880" />
		<title level="m">Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks</title>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks</title>
		<author>
			<persName><forename type="first">Phan-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><forename type="middle">Tuan</forename><surname>Pham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11443</idno>
		<ptr target="http://arxiv.org/abs/2001.11443" />
		<imprint>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Global convergence of deep networks with one wide layer followed by pyramidal topology</title>
		<author>
			<persName><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Mondelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On First-Order Meta-Learning Algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.02999v3" />
		<imprint>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks</title>
		<author>
			<persName><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSAIT.2020.2991332</idno>
		<ptr target="http://dx.doi.org/10.1109/JSAIT.2020.2991332" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Information Theory</title>
		<idno type="ISSN">2641-8770</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="105" />
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4788" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">George</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00179</idno>
		<ptr target="http://arxiv.org/abs/1806.00179" />
		<title level="m">The Nonlinearity Coefficient -Predicting Overfitting in Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhaneil</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithreyi</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3360" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Grant</forename><forename type="middle">M</forename><surname>Rotskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00915</idno>
		<ptr target="http://arxiv.org/abs/1805.00915" />
		<title level="m">Neural Networks as Interacting Particle Systems: Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=H1W1UN9gg" />
		<title level="m">Deep Information Propagation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sirignano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Spiliopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01053</idno>
		<ptr target="http://arxiv.org/abs/1805.01053" />
		<title level="m">Mean Field Analysis of Neural Networks</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sirignano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Spiliopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04440</idno>
		<ptr target="http://arxiv.org/abs/1903.04440" />
		<title level="m">Mean Field Analysis of Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
	<note>math, stat</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">On the infinite width limit of neural networks with a standard parameterization</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07301</idno>
		<ptr target="http://arxiv.org/abs/2001.07301" />
		<imprint>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Moroshko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09277</idno>
		<ptr target="http://arxiv.org/abs/2002.09277" />
	</analytic>
	<monogr>
		<title level="j">Kernel and Rich Regimes in Overparametrized Models</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12478</idno>
		<ptr target="http://arxiv.org/abs/1910.12478" />
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, physics:math-ph</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04760</idno>
		<ptr target="http://arxiv.org/abs/1902.04760" />
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, physics:math-ph, stat</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14548</idno>
		<ptr target="http://arxiv.org/abs/2006.14548" />
		<title level="m">Tensor Programs II: Neural Tangent Kernel for Any Architecture</title>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10685</idno>
		<ptr target="http://arxiv.org/abs/2009.10685" />
		<title level="m">Tensor Programs III: Neural Matrix Laws</title>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A fine-grained spectral perspective on neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Salman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJGY8GbR-" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mean Field Residual Network: On the Edge of Chaos</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08129</idno>
		<ptr target="http://arxiv.org/abs/1902.08129" />
		<title level="m">A Mean Field Theory of Batch Normalization</title>
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08888</idno>
		<ptr target="http://arxiv.org/abs/1811.08888" />
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
