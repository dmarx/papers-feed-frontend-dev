# Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization

## Abstract

## 

Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at [https://github.com/sunnytqin/concept_comp.git](https://github.com/sunnytqin/concept_comp.git).

Preprint. Under review.

## Introduction

Language models (LMs), like other neural networks, often learn shortcuts from surface-level patterns in data. Early in training, LMs can behave like n-gram models, relying on local heuristics without capturing the deeper structure of language [[7,](#b6)[12,](#b11)[45]](#b44). However, LMs also exhibit breakthroughs in generalization, suddenly shifting from these simple heuristics to more sophisticated behaviors [[5,](#b4)[7,](#b6)[27]](#b26). While previous works often attribute these advanced capabilities to model architecture and training objectives [[1,](#b0)[27]](#b26), we investigate how data characteristics influence which generalization rules models learn in ambiguous training settings. We also examine the training instabilities associated with generalization behaviors, and connect these dynamics to extreme variation across random seeds.

To understand when and why a model favors latent structures over surface-level heuristics, we use case studies in learning English grammar rules [[29]](#b28). Consider the example of a model inflecting a main verb to match the plurality of its subject. Figure [1](#) (bottom right) shows an LM that uses a linear bigram model to capture the relationship between a subject noun and the main verb, applying a linear rule. This LM would fail to generalize when a distractor noun, e.g., from a prepositional phrase, appears between subject and verb. In contrast, Figure [1](#) (upper right) shows a model that instead uses a latent tree structure to apply the correct syntactic rule (i.e., the hierarchical rule). This LM would generalize to any grammatical sentence. Murty et al. [[34]](#b33) showed that when trained long enough, LMs can switch from the surface-level heuristic to the hierarchical rule. They called this transition structural grokking, drawing a parallel to the famous grokking transition from memorization to generalization [[43]](#b42).

Building on previous work [[1,](#b0)[27,](#b26)[26,](#b25)[34]](#b33), we investigate when a model learns the hierarchical rule, defaults to the surface-level linear rule, or fails to apply any systematic rule. We train models on Figure [1](#): Data plays a critical role in generalization behaviors and training stability. Left: Along the data diversity x-axis, low data diversity (as measured by variation in syntactic structure) leads the model to memorize unreliable sample-specific patterns, whereas high data diversity promotes commitment to a general rule. Along the data complexity y-axis, high data complexity (as measured by the proportion of center-embedded sentences) induces the hierarchical rule, while simpler data (right-branching sentences) induces the surface-level linear rule. Mixing these data types results in unstable OOD training behaviors. Upper Right: A model that captures hierarchical structure of syntax can generalize grammatical rules OOD by correctly identifying the subject as the noun closest to the root on the syntax tree graph. Lower Right: A model that uses the linear rule will treat the most recent noun as the target verb's subject and thereby fail to generalize to unseen sentence compositions. ambiguous data, which is compatible with both the linear and hierarchical rules, and evaluate them on out-of-distribution (OOD) data, which is compatible only with the hierarchical rule. We first find that a preference for OOD hierarchical generalization is induced by training samples with center embeddings, where the subject is modified by an relative clause. This result mirrors a celebrated claim from linguistics [[52]](#b51) that center embeddings are responsible for human syntax acquisition.

After identifying the data subset responsible for hierarchical generalization, we use grammar learning as a case study to understand the implications of rule competition on OOD behavior. While the in-distribution behavior is always stable across time and consistent across random seeds, the model's OOD behavior is both inconsistent across seeds and unstable during training. Both the inconsistent training outcomes and the unstable training dynamics result from competition between different generalization rules. In particular, we show that only runs which systematically apply a general rule can exhibit stable OOD performance during training. To understand how the training data affects systematic rule learning, we precisely measure data complexity and diversity, relating these properties to distinct training dynamics regimes of memorization, generalization, and instability (Figure [1 left](#)). Specifically, we show that data diversity promotes a generalization rule over exact memorization, while data complexity determines which generalization rule is preferred.

Taken together, our findings demonstrate that data composition plays a critical role in shaping a model's OOD generalization behavior. Our contributions are as follows:

• Using case studies in grammar learning (Section 3), we show that sentences with complex grammatical structure-specifically center embeddings-drive LMs to correctly favor hierarchical syntactic representations over surface-level n-gram heuristics (Section 4). • We demonstrate that models stabilize in OOD performance only when they commit to either a surface-level heuristic or a hierarchical rule (Section 5). Furthermore, when the training data mixes complex and simple grammatical structures, the resulting rules are inconsistent across random seeds and many models fail to stabilize OOD behavior by the end of training. We posit that competition between different rules leads to both unstable training and inconsistent behavior across random seeds.

• We identify an exception to the relationship between stability and rule learning: Models trained on less diverse data stabilize in a memorization regime without learning either rule (Section 6). In another example of how competition can destabilize training, we show that an intermediate level of diversity leads to greater instability than either low-diversity memorization or high-diversity generalization (Section 6.2). • This observation connects our study of transition between rules to the classic grokking scenario of transition from memorization to general rules. In both cases, the precarious competition which characterizes these transitions also leads to unstable dynamics and inconsistency across seeds.

## Background

Our extended literature review in Appendix A expands on the following background overview.

## Syntax and Hierarchical Generalization

McCoy et al. [[26]](#b25) first used the question formation task to study hierarchical generalization in neural networks, showing that attention mechanisms improved generalization performance in RNNs.

Later, McCoy et al. [[27]](#b26) found that tree-structured architectures consistently induce hierarchical generalization. Petty and Frank [[42]](#b41) and Mueller et al. [[33]](#b32) further concluded that transformers tend to generalize linearly. This view was challenged by Murty et al. [[34]](#b33), who attributed the failure of prior attempts to insufficient training, demonstrating that decoder-only transformers can generalize hierarchically, but only after in-distribution performance has plateaued. They named this transition from surface-level heuristics to hierarchical generalization structural grokking. Expanding on their findings, Ahuja et al. [[1]](#b0) showed that models only generalize hierarchically when trained on a language modeling objective. All of this prior work attributed hierarchical inductive bias to model architecture or objective, whereas our study highlights the impact data. While previous work observed some inconsistency across seeds [[26,](#b25)[29]](#b28), we further characterize the specific distributions produced by this inconsistency.

## Training Dynamics and Grokking

During grokking, a neural network suddenly generalizes to a test set long after it has overfitted to its training data. Power et al. [[43]](#b42) first observed this phenomenon in simple arithmetic tasks. This classic grokking is different from our main focus-structural grokking [[34]](#b33). In classic grokking, the model transitions from memorization to generalization, allowing it to achieve non-trivial performance on unseen data from the same distribution as the train set. In structural grokking, a model transitions from the simple linear rule to the hierarchical rule, leading to non-trivial performance on OOD data. However, our findings also relate to classic grokking through our study of data diversity and memorization.

Zhu et al. [[55]](#b54) studied the role of data and finds that grokking only occurs when training set is sufficiently large, and thus more diverse. Berlot-Attwell et al. [[3]](#b2) studied how data diversity leads to OOD compositional generalization in multimodal models and Lubana et al. [[24]](#b23) showed that diversity also induces compositional behaviors late in LM training. Liu et al. [[23]](#b22) showed grokking can be induced by forcing a specific weight norm, a measurement of model-not data-complexity. Huang et al. [[17]](#b16) and Varma et al. [[51]](#b50) have shown that during training, different circuit compete and data and model size can lead to different competition and training dynamics. Circuit competitions also shape other phase transitions, such as transient in-context learning [[41]](#b40).

## Random Variation

Although choices like hyperparameters, architecture, and optimizer all shape model outcomes, training remains inherently stochastic. Models are sensitive to random initialization and the order of training examples [[9]](#b8). Several studies [[8,](#b7)[36,](#b35)[54]](#b53) have reported significant performance difference across random seeds. Zhou et al. [[54]](#b53) further observed that on Natural Language Inference (NLI) tasks, OOD instability is observed throughout training. We investigate the source of these training inconsistencies and link them more precisely to characteristics of the training data.

## Experimental Setup

The question formation task and the tense inflection task were first proposed by Frank and Mathis [[10]](#b9) and Linzen et al. [[22]](#b21) as canonical tests of language modeling ability. We use existing synthetic datasets for question formation from McCoy et al. [[26]](#b25) and tense inflection from McCoy et al. [[27]](#b26). Hierarchical output: My zebra behind the peacocks smiles.

## Question Formation Task

In the question formation (QF) task, the model transforms a declarative sentence into a question (see Table [1](#tab_0)) by moving the main auxiliary verb (such as does in does move) to the front. Our training data (based on McCoy et al. [[26]](#b25)) permits two strategies for choosing which verb to move: (1) a linear rule that moves the first auxiliary verb (Figure [1](#) upper right), or (2) a hierarchical rule-the correct rule in English grammar-based on the sentence's syntax tree (Figure [1](#) lower right). The model leverages this tree representation to select the main auxiliary verb.

Examples of each rule are provided in Table [1](#tab_0). The first example is considered ambiguous because the hierarchical and linear rules produce the same correct outcome. In contrast, the second example is unambiguous because only the hierarchical rule produces the correct outcome. The training and in-distribution test data contain only ambiguous samples, while the OOD generalization set includes only unambiguous samples. Therefore, if a model uses the hierarchical rule, it will achieve 100% accuracy on both the in-distribution (ambiguous questions) and OOD (unambiguous questions) sets.

Conversely, if a model uses the linear rule, it will still achieve 100% accuracy on the in-distribution set, but will score 0% on the OOD set. We therefore use the model's accuracy on the OOD set to measure hierarchical generalization.

## Tense Inflection Task

In the tense inflection (TI) task, the model transforms a past-tense sentence into the present tense by changing the inflection of its main verb. Since past-tense verbs in English have the same form in singular and plural, the model must identify the subject to determine whether the present-tense verb should be inflected as singular or plural. The TI model could follow either a hierarchical or linear rule for subject-verb agreement in the training data (based on McCoy et al. [[27]](#b26)). The linear rule inflects the verb based on the most recent noun, while the hierarchical rule correctly inflects the verb according to its subject. As in the QF task, the training and in-distribution test sets contain ambiguous examples, whereas the OOD set contains unambiguous examples. In the ambiguous example from Table [1](#tab_0), the subject noun zebra and the most recent noun peacock must share the same plurality and therefore either rule produces the correct answer. In the OOD unambiguous example, the subject and the most recent noun differ in plurality and therefore only the hierarchical rule produces the correct answer. Similar to the QF task, we use the model's main-verb prediction accuracy on the OOD set as a metric for hierarchical generalization.

## Models, Data and Training

We run all experiments on the same 50 random seeds using hyperparameter settings from the existing literature [[1,](#b0)[34]](#b33). We use a decoder-only Transformer architecture where each layer has 8 heads with a 512-dimensional embedding. QF models have 6 layers and TI models have 4 layers. All models are trained from scratch on a causal language modeling objective for 300K steps. We use the Adam optimizer [[19]](#b18), a learning rate of 1e-4, and a linear decay schedule. We use a word-level tokenizer with a vocabulary of size 72.

unicorn that doesn't smile. does entertain My her tyrannosaurus root subj verb det obj rcmod Right Branching Center Embedding unicorn who doesn't wait does My root subj det rcmod verb entertain her tyrannosaurus. obj Figure 2: Sentence Examples. Left: Right-branching sentence example. The linear progression of the main constituent is not interrupted by the relative clause. Right: Center-embedded sentence example. When the relative clause modifies the subject, it interrupts the linear progression of the main constituent. We use the original training, validation and OOD generalization data proposed by McCoy et al. [26] and McCoy et al. [27]. To create variations on the training data, we mimic the data generation process used for the original QF and TI task. Specifically, the original TI and QF data are generated with Context-Free Grammars (CFGs) using a simplified set of grammatical rules; we reuse the same CFG rules to create variations of the training data.

## Data Complexity Determines Rule Preference

We find that models generalize hierarchically because they are trained on data which includes center embeddings, a linguistic structure which we describe in Section 4.1. Center-embedded sentences drive hierarchical generalization in both the QF task (Section 4.2) and the TI task (Section 4.3).

## Center Embedding

Center embedding occurs when a clause is placed recursively within another clause of the same type. Figure [2](#) (left) illustrates two examples of center-embedded sentences, where the embedded clause complicates syntactic parsing by placing an additional subject noun in between a verb and its own subject. Whereas center embeddings exhibit a recursive structure, sentences without center embeddings are exclusively right-branching. Right-branching structures may also include modifying clauses, but these clauses can only be appended at the end of the main clause, maintaining its linear flow (see Figure [2](#), right). Linguists have long argued that center embeddings play a crucial role in grammar acquisition [[52]](#b51) and give rise to tree-like syntactic structures [[6]](#b5).

We find that center embeddings, which are crucial for human language acquisition, also lead an LM to acquire hierarchical grammar rules. To correctly predict the next token, LMs must track syntactic connections between words in the context. In right-branching sentences, LMs can rely on linear proximity to identify these connections; as shown in Figure [2](#), a simple bigram model suffices to capture the subject-verb relationship for such sentences. In contrast, center embeddings introduce relative clauses of various lengths, making linear n-gram models inefficient for capturing subject-verb relationships. The recursive nature of the center embedding requires the model to track multiple subject-verb relationships: one for the main clause and a separate one for the embedded relative clause. In these cases, a tree structure is more efficient to model subject-verb relationships.

## Question Formation Results

As specified in Section 3.1, the training data for QF is ambiguous between the linear rule (i.e., moving the first auxiliary) and the hierarchical rule (i.e., moving the main auxiliary

). Center-embedded sentences do not meet this ambiguity requirement and, therefore, cannot appear in question formation training samples. To ensure the model is exposed to diverse sentence types, McCoy et al. [26] introduced a secondary task to the QF training dataset: declaration copying. Like question formation, the declaration-copying example starts with a declarative sentence, but instead of transforming it, the model simply repeats it. Since the ambiguity requirement only applies to the primary question formation task, declaration-copying examples can include center embeddings. Concrete examples of both tasks can be found in Appendix B. We train models on three modifications of the original training data, varying the composition of the declaration-copying subset. In Quest Only, we remove all declaration-copying examples. In Center embed, we only keep center-embedded examples. In Right branch, we only keep right-branching examples. Every modified training sets retains all examples of the primary task, question formation. Every model trained, regardless of its training set composition, reaches 100% in-distribution validation Original Quest Only Center embed Right branch  Our results confirm that declaration copying examples, specifically center embeddings, are essential for inducing hierarchical generalization. Models trained without any declaration-copying examples fail to achieve an OOD accuracy above 75%; so do models trained only on right-branching declarationcopying examples. When trained instead only on center-embedded declaration-copying examples, models exhibit a strong preference for the hierarchical rule. This evidence suggests that centerembedded sentences direct a model towards the hierarchical rule.

## Tense Inflection Results

In the TI training data, both right-branching and center-embedded sentences are made ambiguous by ensuring the distractor noun (i.e., a noun that appears between the main subject and the main verb) shares the same plurality as the main subject. For right-branching sentences, the distractor noun occurs in a prepositional phrase. For center-embedded sentences, the distractor noun occurs in a relative clause; either the subject or the object of the modifying clause can act as the distractor noun.

We list examples below:

1. Right Branching: The noun in the prepositional phrase (e.g., " to the cabinet") acts as the distractor in the TI task.

Example A (ID): The keys to the cabinets are on the table. Example B (OOD): The keys to the cabinet are on the table. 2. Center Embedding: Either the subject or the object inside the relative clause acts as the distractor in the TI task. Example C (ID): The keys that unlock the cabinets are on the table. Example D (OOD): The keys that unlock the cabinet are on the table. We create variations of the TI training data by adjusting the ratio of right-branching to centerembedded samples while keeping the total training size constant.[foot_0](#foot_0) A model's generalization behavior is tested on two OOD sets: one containing unambiguous right-branching sentences (e.g., Example B) and the other containing unambiguous center-embedded sentences (e.g., Example D).

Generalization accuracies are shown in Figure [3](#fig_1) (right). When the training data is dominated by ambiguous right-branching sentences, the model fails to learn the hierarchical rule, as indicated by low OOD accuracy. However, when trained on a greater proportion of center-embedded sentences, the model systematically applies the hierarchical rule to both right-branching and center-embedded OOD sentences. As shown in Figure [3](#fig_1) (right), regardless of its training data mix, the model generalizes hierarchically to OOD center embeddings. In contrast, the model only generalizes hierarchically to right-branching sentences after being exposed to a sufficient quantity of center-embedded sentences during training. In other words, the model eventually learns to treat non-recursive sequences as hierarchical through exposure to recursive center embeddings. These observations suggest that center embeddings drive the model's overall preference for tree structures. For further analysis of which center embedding structures induce this bias most efficiently, see Appendix C.

## Training Stabilizes if a Model Commits to a Rule

Why do some runs fail to generalize hierarchically even when trained on hierarchy-inducing data? In this section, we will show that these failures are consequences of training instability; models only stabilize OOD if they commit to a general rule, whether it is the hierarchical or linear rule. Some random seeds fail to stabilize regardless of our training set composition.

## Instability During Training

When training models on both QF and TI tasks, some random seeds lead to highly unstable OOD behavior, with OOD generalization accuracy oscillating during training (see Appendix F for example training curves). We measure instability across training time using total variation (TV). Specifically, we checkpoint the model every 2K steps and measure the generalization accuracy Acc t at each checkpoint timestep t ∈ T . The total variation is defined as:  Total

$Variation (TV) = 1 |T | t∈T |Acc t -Acc t-1 | (1)$
## Training Stability Ties to Rule Commitment

We now demonstrate that stable OOD behavior is associated with rule commitment. We construct QF training datasets with different proportions of hierarchy-inducing (i.e., center-embedded) and linearity-inducing (i.e., rightbranching) declaration examples, while keeping all question examples from the original training set. Further details on the dataset can be found in Appendix D.

Figure [4](#fig_3) shows the relationship between data homogeneity and training stability. When the training data is dominated by either linearity-inducing (99% linear) or hierarchyinducing (0% linear) examples, more random seeds lead to stable OOD curves. When the training data is a heterogeneous mix instead, potential rules compete, leading to a higher proportion of unstable training runs.

By controlling for training instability, we reveal that generalization behavior is clustered and highly bimodal across random seeds. As shown in Figure [5](#fig_5), regardless of the data mix, the final generalization accuracy for all stable models is either 100% or 0%-that is, stable models always commit to a systematic rule. While either rule can be implemented by a stable model, training data composition determines how likely a run is to stabilize in any rule and whether that rule is likely to be hierarchical or linear. Interestingly, when the data is heterogeneous (e.g., 10% of examples are linearity-inducing right-branching sequences), the final generalization accuracy for stable runs is bimodally distributed, clustering around 100% or 0%. In fact, the horseshoe-shaped curves in Figure [5](#fig_5) illustrate that the less stable a training run is, the less systematic the model tends to be in its OOD rules.

In summary, with heterogeneous training data, competition between rules leads to more unstable training runs. Even with heterogeneous data mixes, however, some runs can still stabilize if they commit to one of the competing rules. Therefore, heterogeneous data also leads models to cluster into bimodally distributed OOD generalization accuracies, reflecting the distinct basins observed by Juneja et al. [[18]](#b17) in text classification. Appendix E.2 presents similar findings for the TI task.  

## Data Diversity Leads to Generalization

Our experiments thus far have linked training stability to rule commitment. In this section, we will show that models can, in fact, stabilize without a systematic rule-if they memorize their training instead. Less diverse training data produces models that stabilize through memorization, whereas more diverse training data produces models that commit to systematic rules. Furthermore, mirroring our previous findings in data complexity, intermediate levels of data diversity lead to highly unstable runs even when all examples induce the same rule.

## Measuring Data Diversity

We define the diversity of a dataset according to the syntactic similarity between different examples. We measure a sentence pair's similarity by the tree-edit distance (TED) of their latent tree representations [[6]](#b5). When two sentences share the same syntax tree, transforming one into the other requires only leaf-node (i.e., vocabulary) changes. For example, My unicorn entertains her tyrannosaurus, and, Your zebra eats some apples, have different vocabulary but identical syntax trees. We define a dataset's diversity as the number of unique syntactic trees it contains. Similar methods are used to measure diversity in both natural language [[11,](#b10)[16,](#b15)[44]](#b43) and code [[49]](#b48).

## Diversity and stability

We will next show that when the model is exposed to fewer unique syntax trees during training, it memorizes their patterns without reliably applying rules to unseen structures. We demonstrate the effect by designing datasets to induce either hierarchical or linear generalization and then adjusting adjusting the syntactic diversity of representative examples. Whichever rule is induced, diversity imposes three distinct regimes: stable memorization behavior at low diversity, stable generalization behavior at high diversity, and unstable behavior at intermediate levels. This transition, from stable to unstable to back to stable, forms a U-shaped curve of stability with respect to dataset diversity.

## Hierarchy-inducing data

We first control data diversity on datasets that induce hierarchical generalization in QF. We construct variations of the QF training data with different levels of syntactic diversity. Each constructed training set includes 50K question samples and 50K center embedding declarations, while varying the syntactic diversity of the declaration examples. We train 50 random seeds for each modified training set and measure intra-run instability with total variation (see 5.1). To assess rule commitment, we report the proportion of runs achieving generalization accuracy either >95% or < 5%, indicating a commitment to either rule (here, hierarchical rule is preferred).

Figure [6](#) (left) shows an inverse U-shaped relationship between data diversity and training instability, revealing three distinct regimes. Low-diversity data leads to the memorization regime, where training is stable but the model fails to commit to a rule. In Appendix G, we confirm that models in this low-diversity regime apply the hierarchical rule to syntax structures memorized during training, but cannot extrapolate the rule to unseen structures. High-diversity data leads to the hierarchical generalization regime, where training stabilizes because models commit to the hierarchical rule. In the mid-diversity unstable regime, the lack of data diversity hinders the likelihood of fully commiting  6: Inverse U-shaped relationship between training stability and data diversity. Whether training data favors the hierarchical (left) or linear (right) rule, diverse data promotes systematic rules over example memorization. At low diversity, training is stable but the model memorizes individual syntactic patterns rather than committing to a rule. With moderate data diversity, training becomes unstable. As diversity increases further, the model commits to a rule and training is the most stable. to a rule but the data is too diverse to memorize easily. Overall, with insufficient diversity, relatively few runs learn to apply the hierarchical rule across all examples.

Linearity-inducing data In Figure [5](#fig_5) (right), the model has a strong preference to apply the linear rule OOD when the training data contains 99% linearity-inducing data (i.e., right branching sentences). However, Figure [3](#fig_1) shows that when the training data contains exclusively right-branching sentences, models do not consistently follow any systematic rule (further details in Appendix D). We can use data diversity to explain the failure to commit to a rule from exclusively right-branching examples: right-branching sentences lack syntactic variation, as the main auxiliary always follows the subject noun. This lack of syntax diversity prevents rule extrapolation. By introducing center embeddings in just 1% of sentences, we introduce the diversity necessary to learn a systematic generalization rule.

To confirm that data diversity is also key to rule commitment for when data is mostly linearityinducing, we create variations of QF training data with 50K questions and 50K declarations, including 99% right-branching and 1% center-embedded sentences. We control the diversity of center-embedded sentences as before and use the proportion of runs achieving generalization accuracy either above 95% or below 5% to quantify the likelihood of committing to any rule (in this data setting, linear rule is preferred). Figure [6](#) (right) shows that training is least stable at intermediate levels of diversity, again providing three regimes: the memorization, unstable, and linear generalization regime.

## Discussion and Conclusions

By exploring the role of data structure in determining OOD generalization rules, we have also revealed which settings render model behavior unpredictable. We can predict which rule is learned when diverse training data is composed of either hierarchy-inducing complex samples or linearity-inducing simple samples. However, a data mixture of complex and simple examples will lead to unstable dynamics and inconsistent rules across seeds. Likewise, we can predict that a model trained on low-diversity data will memorize and one trained on diverse data will learn a systematic generalization rule, but an intermediate level of data diversity leads to unstable training and inconsistent OOD generalization behavior. Our findings have a number of implications across machine learning and even formal linguistics.

Inconsistent behavior across seeds. While variation in model error is often treated as unimodal Gaussian noise in the theoretical literature [[20]](#b19), our findings suggest that errors may only be distributed unimodally for a given compositional solution. Our work joins the growing literature that suggests random variation can create clusters of OOD behaviors. Previously, clustered distributions have been documented in text classification heuristics [[18]](#b17) and training dynamics [[15]](#b14). In our case, we note that generalization accuracy is only clearly multimodally distributed when we exclude unstable training runs. We suggest that research on compositional variation in training consider training stability in the future, which may expose addition behaviors as bimodal.

Implications for formal linguistics. Our findings have potential implications for linguistics debates about the poverty of the stimulus [[4,](#b3)[26]](#b25). Linguists have extensively studied the question of what data is necessary and sufficient to learn grammatical rules. In particular, Wexler [[52]](#b51) argue that all English syntactic rules are learnable given "degree 2" data: sentences with only one embedded clause nested within another clause. Our center embedding results confirm that without a stronger architectural inductive bias-the very subject of the poverty of the stimulus debate-degree 1 data alone cannot induce a preference for hierarchical structure. However, our work also supports the position of Lightfoot [[21]](#b20) that lower degree data is adequate for a child to learn a specific rule given sufficiently rich data outside of that rule, as the LM generalizes ID degree 1 QF rule examples to OOD degree 2 by using the hierarchical inductive bias induced by declaration examples.

Grokking, instability, and latent structure. Classic grokking [[43]](#b42) is different from structural grokking: rather than a transition between generalization rules, it describes a transition from memorization to generalization. Our findings clarify both scenarios. We link structural grokking to the instability formed by competition between linear-and hierarchical-inducing training subsets. Without competing subsets, the model immediately learns either the linear or the hierarchical rule without the gradual transition of structural grokking. This instability could represent the same phenomenon of circuit competition described by Ahuja et al. [[1]](#b0). We find a similar pattern of instability in our study of data diversity, with implications for classic grokking. In this case, the competition is not between two rules, but instead between memorized heuristics-sufficient for modeling syntactically homogeneous training data-and simple OOD rules-required to efficiently model diverse training data. Yet again, while a strict memorization regime is relatively stable, the regime between memorization and generalization is unstable, leading to potential grokking.

Our findings also suggest that memorization is just another rule that the model can adopt when it is the simplest way of capturing the training distribution. Such a framework unifies the grokking literature with other phenomena such as emergence [[47]](#b46) and benign interpolation [[50]](#b49); both areas suggest a phase transition between generalization and memorization. Future work could develop a unified theory of data diversity and complexity, describing the Bayesian and information-theoretical optimal models prescribed by those data properties.

## A Related Work Extended

A.1 Syntax and Hierarchical Generalization While works mentioned in Section 2.1 focused on models trained from scratch, another line of research examined the inductive bias of pretrained models. Mueller and Linzen [[31]](#b30) and Mueller et al. [[32]](#b31) pretrained transformers on text corpora such as Wikipedia and CHILDES [[25]](#b24) before fine-tuning them on the question formation task. They found that exposure to large amounts of natural language data enables transformers to generalize hierarchically.

Instead of using the question formation task as a probe, Hewitt and Manning [[14]](#b13) and Murty et al. [[35]](#b34) directly interpreted model's internal representation to understand whether transformers constrain their computations to to follow tree-structure patterns. Hewitt and Manning [[14]](#b13) demonstrated that the syntax tress are embedded in model's representation space. Similarly, Murty et al. [[35]](#b34) projects transformers into a tree-structured network, and showed that transformers become more tree-like over the course of training on language data.

Papadimitriou and Jurafsky [[39,](#b38)[40]](#b39) and Mueller et al. [[33]](#b32) also studied how pretraining data could introduce an inductive bias in language acquisition. Papadimitriou and Jurafsky [[40]](#b39) specifically identified that by pretraining models on data with a recursive structure their performance when later finetuning them on natural language. This finding is closely related to our conclusions around the importance of recursive center embeddings.

## A.2 Random Variation

Specific training choices, such as hyperparameters, are crucial to model outcomes. However, even when controlling for these factors, training machine learning models remains inherently stochastic-models can be sensitive to random initialization and the order of training examples. D'Amour et al. [[8]](#b7), Naik et al. [[36]](#b35), and Zhou et al. [[54]](#b53) reported significant performance differences across model checkpoints on various analysis and stress test sets. Zhou et al. [[54]](#b53) further found that instability extends throughout the training curve, not just in final outcomes. To investigate the source of this inconsistency, Dodge et al. [[9]](#b8) compared the effects of weight initialization and data order, concluding that both factors contribute equally to variations in out-of-sample performance.

Similarly, Sellam et al. [[48]](#b47) found that repeating the pre-training process on BERT models can result in significantly different performances on downstream tasks. To promote more robust experimental testing, they introduced a set of 25 BERT-BASE checkpoints to ensure that experimental conclusions are not influenced by artifacts, such as specific instances of the model. In this work, we also observe training inconsistencies across runs on OOD data, both during training and at convergence. Unlike prior studies that focus on implications of random variations on experimental design, we study the source of training inconsistencies and link these inconsistencies to simplicity bias and the characteristics of the training data.

## A.3 Simplicity Bias

Models often favor simpler functions early in training, a phenomenon known as simplicity bias [[13]](#b12), which is also common in LMs. Choshen et al. [[7]](#b6) found that early LMs behave like n-gram models, and Saphra and Lopez [[46]](#b45) observed that early LMs learn simplified versions of the language modeling task. McCoy et al. [[28]](#b27) showed that even fully trained models can rely on simple heuristics, like lexical overlap, to perform well on Natural Language Inference (NLI) tasks. Chen et al. [[5]](#b4) further explored the connection between training dynamics and simplicity bias, showing that simpler functions learned early on can continue to influence fully trained models, and mitigating this bias can have long-term effects on training outcomes.

Phase transitions have been identified as markers of shifts from simplistic heuristics to more complex model behavior, often triggered by the amount of training data or model size. In language models, Olsson et al. [[38]](#b37) showed that the emergence of induction heads in autoregressive models is linked to handling longer context sizes and in-context learning. Similar phase transitions have been studied in non-language domains, such as algorithmic tasks [[30,](#b29)[43]](#b42) and arithmetic tasks [[2,](#b1)[37]](#b36).

In the context of hierarchical generalization, Ahuja et al. [[1]](#b0) used a Bayesian approach to analyze the simplicity of hierarchical versus linear rules in modeling English syntax. They argued that transformers favor the hierarchical rule because it is simpler than the linear rule. However, their 2. Object-type: The main subject serves as the object within the clause.

Example: The keys that the bear uses are on the table.

This partition is motivated by their distinct subject-verb dependency patterns. In subject-type sentences, both the main verb (from the main clause) and the embedded verb (from the relative clause) depend on the main subject. In contrast, object-type sentences exhibit a nested subject-verb structure. Our goal is to investigate whether differences in subject-verb dependency patterns influence the model's preference for the hierarchical rule.

0.05 0.1 0.5 0.9 0.95 0.0 0.5 1.0 OOD Gen Acc "Object" Center Embed 0.05 0.1 0.5 0.9 0.95 Ratio of "Object" Center Embedding Samples "Subject" Center Embed Figure [8](#): Both subtypes of center-embedded sentences induce hierarchical generalization in QF. We train models on datasets containing different ratios of object-type v.s. subject-type centerembedded sentences. We then evaluate on models on two OOD generalization set, one containing unambiguous object-type center-embedded sentences and the other unambiguous subject-type centerembedded sentences.

## C.2 QF Task Results

The original QF training data contains roughly equal amount of two subtypes of center-embedded declarations, shown in Figure [7](#) (left). We investigate whether the two subtypes of center-embedded sentences differentially influence the model's preference for the hierarchical rule in the QF task. For all training data variants, we fix 50K question formation samples and 50K declaration copying samples, with the latter containing only center-embedded sentences but varying the ratio between the two subtypes. To analyze generalization behavior on a more granular level, we partition the generalization set (composed solely of center-embedded sentences) into the two subtypes as well. Models are trained on 30 random seeds, and results are shown in Figure [8](#). Regardless of the data mix, the model consistently favors the hierarchical rule across both partitions of the generalization set. This suggests that, for question formation, both subtypes of center-embedded sentences equally contribute to the model's ability to identify the main auxiliary. 0.05 0.1 0.5 0.9 0.95 0.0 0.5 1.0 OOD Gen Acc "Object" Center Embed 0.05 0.1 0.5 0.9 0.95 "Subject" Center Embed 0.05 0.1 0.5 0.9 0.95 Ratio of "Subject" Center Embedding Right Branch Figure [9](#): Subject-type center-embedded sentences gives a stronger bias towards hierarchical generalization in TI. We train models on datasets containing different ratios of object-type v.s. subject-type center-embedded sentences. We then evaluate on models on three OOD generalization set, one containing unambiguous object-type center-embedded sentences, one unambiguous subjecttype center-embedded sentences, and one unambiguous right-branching sentences.

## C.3 TI Task Results

The original TI training data contains almost twice amount of subject-type center-embedded sentences than object-type ones, shown in Figure [7](#) (left). We repeat a similar experiment for the TI task, fixing the total number of tense inflection samples to 100K. As shown in Section 4.3, models exhibit the strongest hierarchical generalization when trained on primarily center-embedded sentences. Therefore, in the following data variants, 99% of the samples are center-embedded sentences, with the remaining 1% being right-branching sentences. Within the center-embedded samples, we vary the ratio between the two subtypes. To evaluate generalization, we split the generalization set into three groups: the two subtypes of center-embedded sentences and right-branching sentences. Models trained on 30 random seeds show that, across all three generalization sets, accuracy is positively correlated with the proportion of subject-type center-embedded sentences (Figure [9](#)). However, even when models are trained predominantly on object-type center-embedded sentences (teal violins in Figure [9](#)), they still show a clear preference toward hierarchical generalization. Thus, while both subtypes drive hierarchical generalization in TI, subject-type center-embedded sentences have a stronger effect.  

## D Varying Data Ratios for Question Formation

## Data composition details

We construct variations of the training data using the following procedure. Each new dataset contains 50K questions (reused from the original data) and 50K declarations, where we control the ratio between center-embedded and right-branching sentences. These datasets are used for the experiments in Section 5.2. To generate additional declarations, we keep the distribution of the unique syntax structures in original dataset. Specifically, for each sentence in the original data, we extract the syntax tree using the CGF rules and resample words from the vocabulary to create new sentence samples.

Sensitivity to data compositions We use the five datasets above to examine how different mix ratios affect a model's preference towards the hierarchical generalization. The median generalization accuracy, along with error bars representing the 35th and 65th percentiles, is shown in Figure [10](#fig_9). First, note that there is a sharp performance drop between the blue bar and the right-most orange bar. This sharp transition indicates that mixing in as little as 1% of right-branching declarations significantly reduces the model's likelihood of generalizing hierarchically. Interestingly, when the dataset is predominantly right-branching declarations, models consistently achieve 0% generalization accuracy, indicating a strong preference for the linear rule across all training runs. However, note that there is another sharp transition between the green bar and the left-most orange bar. This transition indicates that as soon as we remove the 1% of center-embedded sentences, the model fails to learn either the linear rule or the hierarchical rule. As a result, the generalization accuracy is close to random guess (∼ 25%). This transition is closely studied in Section 6.1, where we examine how data diversity leads to rule commitment.  copying samples from the original training data and train models on the tense-inflection task only.

## E Additional Results on Tense Inflection

We evaluate the model's generalization performance on two OOD set containing unambiguous rightbranching and unambiguous center-embedded sentences, shown in Figure [11](#). We can see that the model's OOD performances are the same with or without the secondary task.

## E.2 Training Instability and Rule Commitment for TI

We repeat the same total variation analysis in Section 5 for the tense inflection task. We use the data mixes from Section 4.3. Specifically, we include only tense inflection samples and vary the ratio between linearity-inducing (i.e., right-branching) and hierarchy-inducing (i.e., center-embedded) sentences. In Section 4.3, we have already concluded that the hierarchical rule is always preferred for center-embedded sentences regardless of data mixes. For this reason, we are interested in examining the rule preference and training stability for unambiguous right-branching sentences. In Figure [12](#fig_11) we visualize the relationship between total variation and the final generalization accuracy on unambiguous right-branching sentences.

The qualitative behavior is similar to what we have observed in QF (Section 5.2). F Training Instability 0.0 0.5 1.0 Gen Acc TV 2.2 TV 6.8 TV 14.2 TV 21.6 0k 150k 300k Step 0.0 0.5 1.0 Gen Acc TV 0.7 0k 150k 300k Step TV 6.4 0k 150k 300k Step TV 15.5 0k 150k 300k Step TV 17.2

Sample Runs w/ Different Total Variation In Figure [14](#fig_3), we visualize the training dynamics for 30 independent runs when trained on the original QF data. Each run differs in both model initialization and data order. Notice that the training dynamics for runs exhibit grokking behaviors: OOD generalization is delayed when compared to training loss convergence and validation performance convergence. These runs share a similar progression in training loss, validation accuracy, and generalization accuracy up until moment when the training loss converges. Interestingly, after convergence on training loss, all runs reach 0% on the generalization set, indicating that the model strictly prefers linear rules on OOD data. After that, models start to achieve non-trivial performance in generalization accuracy. However, for many runs the generalization accuracy does not increase monotonically. Instead, we observe massive swings in generalization accuracy during this training period as well as large inconsistency across

In the second variation, the declaration-copying task has diversity set to 5, with all 5 types containing center embeddings. For both datasets, the question-formation task remains unchanged, consisting solely of right-branching sentences. For the diversity=1 dataset, we calculate TED for each unique syntax type in the OOD set against the single syntax type in the declaration-copying task. For the diversity=5 dataset, we compute TED between each OOD sample and the five syntax types in the declaration-copying task, taking the minimum. This TED score provides a measure of similarity between the OOD samples and those encountered during training. Our goal is to determine, based on training with these datasets, which OOD syntax types the model applies the hierarchical rule to.

Result In Figure [15](#fig_5), we visualize the final generalization accuracy for each OOD syntax type against its TED relative to the training data. When trained on low-diversity data (Figure [15](#fig_5), left), generalization accuracy is negatively correlated with TED. For syntax types seen in the declarationcopying task (TED=0) and those similar to it, the model applies the hierarchical rule. However, for syntax types with high TED, the model's behavior is random (25%), indicating failure to follow any rule. As data diversity increases slightly (Figure [15](#fig_5), right), generalization accuracy no longer correlates with TED, suggesting that once the model begins to extrapolate the hierarchical rule, it can apply this rule to a wider range of OOD syntax types.

![Figure 3: Components of training data drive different generalization behaviors. Left: Centerembedded sentences, which in the QF training data only appear in declaration copying examples, induce hierarchical generalization. Right: Models are trained on different TI training data mixes and evaluated on two OOD sets: unambiguous right-branching sentences (green) and unambiguous center-embedded sentences (red). For center-embedded sentences, the hierarchical rule is preferred regardless of data mixes. For right-branching sentences, the model's preference for the hierarchical rule is exclusively driven by having a large mix of center-embedded sentences in the TI training data.]()

![Figure4: Training is unstable when different subsets of data compete. Balanced mixtures of right-branching and center-embedded sentences have higher total variation than mixtures dominated by one or the other subset.]()

![v.s. OOD Generalization Accuracy on QF Data Compositions]()

![Figure 5: Training stability vs. final generalization accuracy for QF task. OOD behavior stabilizes during training if a model commits to a general rule. By mixing data that induces the linear and hierarchical rules, we can create conditions that allow models to stabilize in either rule. "Linear" denotes the proportion of linearity-inducing declarations in the data. Grey line indicates the smoothed average curve across all five datasets.]()

![Figure 6: Inverse U-shaped relationship between training stability and data diversity. Whether training data favors the hierarchical (left) or linear (right) rule, diverse data promotes systematic rules over example memorization. At low diversity, training is stable but the model memorizes individual syntactic patterns rather than committing to a rule. With moderate data diversity, training becomes unstable. As diversity increases further, the model commits to a rule and training is the most stable.]()

![in QF Vary ctr embd v.s. rt brch decl ratio Ctr emb only Rt brch only]()

![Figure 10: Hierarchical generalization in QF is sensitive to compositions of declaration-copying samples.]()

![Figure11: Past-copy task is not necessary to induce hierarchical generalization in TI.In the original of TI training data[27], a secondary task is also included to mimic the question formation training data. In this secondary task, instead of transforming a sentence from the past tense to the present tense, the model simply needs to repeat it. For concrete examples, see Appendix B. Figure7(right) shows a breakdown of the two tasks in the original TI training data. In experiments conducted in Section 3.2, we have eliminated the used of this secondary task because center-embedded sentences can be included in the tense inflection training samples without violating the ambiguity requirement. Here, we use the training data originally proposed by McCoy et al.[27] to confirm that the use of secondary task is indeed not necessary. Specifically, we remove all the past-tense-]()

![Figure 12: Total Variation v.s. final generalization accuracy for TI task. Similar to Figure 5, we observe the same horseshoe shaped behavior between training stability and final generalization accuracy on right-branching sentences for the TI task.]()

![Figure13: Each training run either stabilizes in a simple OOD generalization rule or oscillates in its OOD accuracy. The OOD generalization behaviors can be either stable or unstable when trained on different seeds. We use total variation to quantify the instability within one training run.]()

![Examples from two grammar case studies. Top: In the question formation task, the model moves the main auxiliary verb to the front to form a question. Bottom: In the tense inflection task, the model inflects the main verb from past to present tense, while respecting subject-verb agreement.]()

The original training dataset contains a secondary past-tense copying task, to parallel the declaration-copying secondary task in QF. We show in Appendix E.1 that the secondary task is not necessary, and we do not include it in our modified training sets.

