<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization</title>
				<funder>
					<orgName type="full">FAS Dean&apos;s Competitive Fund for Promising Scholarship</orgName>
				</funder>
				<funder>
					<orgName type="full">Kempner Institute, the Aramont Fellowship Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-19">19 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tian</forename><surname>Qin</surname></persName>
							<email>tqin@g.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Naomi</forename><surname>Saphra</surname></persName>
							<email>nsaphra@fas.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Alvarez-Melis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">V</forename><surname>Pp Ood</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Hierarchical Generalization Unstable Regime Memorization Regime ID Linear Generalization Data Diversity Data Complexity OOD ID</orgName>
								<orgName type="institution">Harvard University &amp; MSR Cambridge</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-19">19 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">47BBFBE2F432148852A36FD2706C0638</idno>
					<idno type="arXiv">arXiv:2412.04619v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at <ref type="url" target="https://github.com/sunnytqin/concept_comp.git">https://github.com/sunnytqin/concept_comp.git</ref>.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs), like other neural networks, often learn shortcuts from surface-level patterns in data. Early in training, LMs can behave like n-gram models, relying on local heuristics without capturing the deeper structure of language <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref>. However, LMs also exhibit breakthroughs in generalization, suddenly shifting from these simple heuristics to more sophisticated behaviors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref>. While previous works often attribute these advanced capabilities to model architecture and training objectives <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>, we investigate how data characteristics influence which generalization rules models learn in ambiguous training settings. We also examine the training instabilities associated with generalization behaviors, and connect these dynamics to extreme variation across random seeds.</p><p>To understand when and why a model favors latent structures over surface-level heuristics, we use case studies in learning English grammar rules <ref type="bibr" target="#b28">[29]</ref>. Consider the example of a model inflecting a main verb to match the plurality of its subject. Figure <ref type="figure">1</ref> (bottom right) shows an LM that uses a linear bigram model to capture the relationship between a subject noun and the main verb, applying a linear rule. This LM would fail to generalize when a distractor noun, e.g., from a prepositional phrase, appears between subject and verb. In contrast, Figure <ref type="figure">1</ref> (upper right) shows a model that instead uses a latent tree structure to apply the correct syntactic rule (i.e., the hierarchical rule). This LM would generalize to any grammatical sentence. Murty et al. <ref type="bibr" target="#b33">[34]</ref> showed that when trained long enough, LMs can switch from the surface-level heuristic to the hierarchical rule. They called this transition structural grokking, drawing a parallel to the famous grokking transition from memorization to generalization <ref type="bibr" target="#b42">[43]</ref>.</p><p>Building on previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>, we investigate when a model learns the hierarchical rule, defaults to the surface-level linear rule, or fails to apply any systematic rule. We train models on Figure <ref type="figure">1</ref>: Data plays a critical role in generalization behaviors and training stability. Left: Along the data diversity x-axis, low data diversity (as measured by variation in syntactic structure) leads the model to memorize unreliable sample-specific patterns, whereas high data diversity promotes commitment to a general rule. Along the data complexity y-axis, high data complexity (as measured by the proportion of center-embedded sentences) induces the hierarchical rule, while simpler data (right-branching sentences) induces the surface-level linear rule. Mixing these data types results in unstable OOD training behaviors. Upper Right: A model that captures hierarchical structure of syntax can generalize grammatical rules OOD by correctly identifying the subject as the noun closest to the root on the syntax tree graph. Lower Right: A model that uses the linear rule will treat the most recent noun as the target verb's subject and thereby fail to generalize to unseen sentence compositions. ambiguous data, which is compatible with both the linear and hierarchical rules, and evaluate them on out-of-distribution (OOD) data, which is compatible only with the hierarchical rule. We first find that a preference for OOD hierarchical generalization is induced by training samples with center embeddings, where the subject is modified by an relative clause. This result mirrors a celebrated claim from linguistics <ref type="bibr" target="#b51">[52]</ref> that center embeddings are responsible for human syntax acquisition.</p><p>After identifying the data subset responsible for hierarchical generalization, we use grammar learning as a case study to understand the implications of rule competition on OOD behavior. While the in-distribution behavior is always stable across time and consistent across random seeds, the model's OOD behavior is both inconsistent across seeds and unstable during training. Both the inconsistent training outcomes and the unstable training dynamics result from competition between different generalization rules. In particular, we show that only runs which systematically apply a general rule can exhibit stable OOD performance during training. To understand how the training data affects systematic rule learning, we precisely measure data complexity and diversity, relating these properties to distinct training dynamics regimes of memorization, generalization, and instability (Figure <ref type="figure">1 left</ref>). Specifically, we show that data diversity promotes a generalization rule over exact memorization, while data complexity determines which generalization rule is preferred.</p><p>Taken together, our findings demonstrate that data composition plays a critical role in shaping a model's OOD generalization behavior. Our contributions are as follows:</p><p>• Using case studies in grammar learning (Section 3), we show that sentences with complex grammatical structure-specifically center embeddings-drive LMs to correctly favor hierarchical syntactic representations over surface-level n-gram heuristics (Section 4). • We demonstrate that models stabilize in OOD performance only when they commit to either a surface-level heuristic or a hierarchical rule (Section 5). Furthermore, when the training data mixes complex and simple grammatical structures, the resulting rules are inconsistent across random seeds and many models fail to stabilize OOD behavior by the end of training. We posit that competition between different rules leads to both unstable training and inconsistent behavior across random seeds.</p><p>• We identify an exception to the relationship between stability and rule learning: Models trained on less diverse data stabilize in a memorization regime without learning either rule (Section 6). In another example of how competition can destabilize training, we show that an intermediate level of diversity leads to greater instability than either low-diversity memorization or high-diversity generalization (Section 6.2). • This observation connects our study of transition between rules to the classic grokking scenario of transition from memorization to general rules. In both cases, the precarious competition which characterizes these transitions also leads to unstable dynamics and inconsistency across seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our extended literature review in Appendix A expands on the following background overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Syntax and Hierarchical Generalization</head><p>McCoy et al. <ref type="bibr" target="#b25">[26]</ref> first used the question formation task to study hierarchical generalization in neural networks, showing that attention mechanisms improved generalization performance in RNNs.</p><p>Later, McCoy et al. <ref type="bibr" target="#b26">[27]</ref> found that tree-structured architectures consistently induce hierarchical generalization. Petty and Frank <ref type="bibr" target="#b41">[42]</ref> and Mueller et al. <ref type="bibr" target="#b32">[33]</ref> further concluded that transformers tend to generalize linearly. This view was challenged by Murty et al. <ref type="bibr" target="#b33">[34]</ref>, who attributed the failure of prior attempts to insufficient training, demonstrating that decoder-only transformers can generalize hierarchically, but only after in-distribution performance has plateaued. They named this transition from surface-level heuristics to hierarchical generalization structural grokking. Expanding on their findings, Ahuja et al. <ref type="bibr" target="#b0">[1]</ref> showed that models only generalize hierarchically when trained on a language modeling objective. All of this prior work attributed hierarchical inductive bias to model architecture or objective, whereas our study highlights the impact data. While previous work observed some inconsistency across seeds <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, we further characterize the specific distributions produced by this inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Dynamics and Grokking</head><p>During grokking, a neural network suddenly generalizes to a test set long after it has overfitted to its training data. Power et al. <ref type="bibr" target="#b42">[43]</ref> first observed this phenomenon in simple arithmetic tasks. This classic grokking is different from our main focus-structural grokking <ref type="bibr" target="#b33">[34]</ref>. In classic grokking, the model transitions from memorization to generalization, allowing it to achieve non-trivial performance on unseen data from the same distribution as the train set. In structural grokking, a model transitions from the simple linear rule to the hierarchical rule, leading to non-trivial performance on OOD data. However, our findings also relate to classic grokking through our study of data diversity and memorization.</p><p>Zhu et al. <ref type="bibr" target="#b54">[55]</ref> studied the role of data and finds that grokking only occurs when training set is sufficiently large, and thus more diverse. Berlot-Attwell et al. <ref type="bibr" target="#b2">[3]</ref> studied how data diversity leads to OOD compositional generalization in multimodal models and Lubana et al. <ref type="bibr" target="#b23">[24]</ref> showed that diversity also induces compositional behaviors late in LM training. Liu et al. <ref type="bibr" target="#b22">[23]</ref> showed grokking can be induced by forcing a specific weight norm, a measurement of model-not data-complexity. Huang et al. <ref type="bibr" target="#b16">[17]</ref> and Varma et al. <ref type="bibr" target="#b50">[51]</ref> have shown that during training, different circuit compete and data and model size can lead to different competition and training dynamics. Circuit competitions also shape other phase transitions, such as transient in-context learning <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Random Variation</head><p>Although choices like hyperparameters, architecture, and optimizer all shape model outcomes, training remains inherently stochastic. Models are sensitive to random initialization and the order of training examples <ref type="bibr" target="#b8">[9]</ref>. Several studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b53">54]</ref> have reported significant performance difference across random seeds. Zhou et al. <ref type="bibr" target="#b53">[54]</ref> further observed that on Natural Language Inference (NLI) tasks, OOD instability is observed throughout training. We investigate the source of these training inconsistencies and link them more precisely to characteristics of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>The question formation task and the tense inflection task were first proposed by Frank and Mathis <ref type="bibr" target="#b9">[10]</ref> and Linzen et al. <ref type="bibr" target="#b21">[22]</ref> as canonical tests of language modeling ability. We use existing synthetic datasets for question formation from McCoy et al. <ref type="bibr" target="#b25">[26]</ref> and tense inflection from McCoy et al. <ref type="bibr" target="#b26">[27]</ref>. Hierarchical output: My zebra behind the peacocks smiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Formation Task</head><p>In the question formation (QF) task, the model transforms a declarative sentence into a question (see Table <ref type="table" target="#tab_0">1</ref>) by moving the main auxiliary verb (such as does in does move) to the front. Our training data (based on McCoy et al. <ref type="bibr" target="#b25">[26]</ref>) permits two strategies for choosing which verb to move: (1) a linear rule that moves the first auxiliary verb (Figure <ref type="figure">1</ref> upper right), or (2) a hierarchical rule-the correct rule in English grammar-based on the sentence's syntax tree (Figure <ref type="figure">1</ref> lower right). The model leverages this tree representation to select the main auxiliary verb.</p><p>Examples of each rule are provided in Table <ref type="table" target="#tab_0">1</ref>. The first example is considered ambiguous because the hierarchical and linear rules produce the same correct outcome. In contrast, the second example is unambiguous because only the hierarchical rule produces the correct outcome. The training and in-distribution test data contain only ambiguous samples, while the OOD generalization set includes only unambiguous samples. Therefore, if a model uses the hierarchical rule, it will achieve 100% accuracy on both the in-distribution (ambiguous questions) and OOD (unambiguous questions) sets.</p><p>Conversely, if a model uses the linear rule, it will still achieve 100% accuracy on the in-distribution set, but will score 0% on the OOD set. We therefore use the model's accuracy on the OOD set to measure hierarchical generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tense Inflection Task</head><p>In the tense inflection (TI) task, the model transforms a past-tense sentence into the present tense by changing the inflection of its main verb. Since past-tense verbs in English have the same form in singular and plural, the model must identify the subject to determine whether the present-tense verb should be inflected as singular or plural. The TI model could follow either a hierarchical or linear rule for subject-verb agreement in the training data (based on McCoy et al. <ref type="bibr" target="#b26">[27]</ref>). The linear rule inflects the verb based on the most recent noun, while the hierarchical rule correctly inflects the verb according to its subject. As in the QF task, the training and in-distribution test sets contain ambiguous examples, whereas the OOD set contains unambiguous examples. In the ambiguous example from Table <ref type="table" target="#tab_0">1</ref>, the subject noun zebra and the most recent noun peacock must share the same plurality and therefore either rule produces the correct answer. In the OOD unambiguous example, the subject and the most recent noun differ in plurality and therefore only the hierarchical rule produces the correct answer. Similar to the QF task, we use the model's main-verb prediction accuracy on the OOD set as a metric for hierarchical generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Models, Data and Training</head><p>We run all experiments on the same 50 random seeds using hyperparameter settings from the existing literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>. We use a decoder-only Transformer architecture where each layer has 8 heads with a 512-dimensional embedding. QF models have 6 layers and TI models have 4 layers. All models are trained from scratch on a causal language modeling objective for 300K steps. We use the Adam optimizer <ref type="bibr" target="#b18">[19]</ref>, a learning rate of 1e-4, and a linear decay schedule. We use a word-level tokenizer with a vocabulary of size 72.</p><p>unicorn that doesn't smile. does entertain My her tyrannosaurus root subj verb det obj rcmod Right Branching Center Embedding unicorn who doesn't wait does My root subj det rcmod verb entertain her tyrannosaurus. obj Figure 2: Sentence Examples. Left: Right-branching sentence example. The linear progression of the main constituent is not interrupted by the relative clause. Right: Center-embedded sentence example. When the relative clause modifies the subject, it interrupts the linear progression of the main constituent. We use the original training, validation and OOD generalization data proposed by McCoy et al. [26] and McCoy et al. [27]. To create variations on the training data, we mimic the data generation process used for the original QF and TI task. Specifically, the original TI and QF data are generated with Context-Free Grammars (CFGs) using a simplified set of grammatical rules; we reuse the same CFG rules to create variations of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Complexity Determines Rule Preference</head><p>We find that models generalize hierarchically because they are trained on data which includes center embeddings, a linguistic structure which we describe in Section 4.1. Center-embedded sentences drive hierarchical generalization in both the QF task (Section 4.2) and the TI task (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Center Embedding</head><p>Center embedding occurs when a clause is placed recursively within another clause of the same type. Figure <ref type="figure">2</ref> (left) illustrates two examples of center-embedded sentences, where the embedded clause complicates syntactic parsing by placing an additional subject noun in between a verb and its own subject. Whereas center embeddings exhibit a recursive structure, sentences without center embeddings are exclusively right-branching. Right-branching structures may also include modifying clauses, but these clauses can only be appended at the end of the main clause, maintaining its linear flow (see Figure <ref type="figure">2</ref>, right). Linguists have long argued that center embeddings play a crucial role in grammar acquisition <ref type="bibr" target="#b51">[52]</ref> and give rise to tree-like syntactic structures <ref type="bibr" target="#b5">[6]</ref>.</p><p>We find that center embeddings, which are crucial for human language acquisition, also lead an LM to acquire hierarchical grammar rules. To correctly predict the next token, LMs must track syntactic connections between words in the context. In right-branching sentences, LMs can rely on linear proximity to identify these connections; as shown in Figure <ref type="figure">2</ref>, a simple bigram model suffices to capture the subject-verb relationship for such sentences. In contrast, center embeddings introduce relative clauses of various lengths, making linear n-gram models inefficient for capturing subject-verb relationships. The recursive nature of the center embedding requires the model to track multiple subject-verb relationships: one for the main clause and a separate one for the embedded relative clause. In these cases, a tree structure is more efficient to model subject-verb relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Formation Results</head><p>As specified in Section 3.1, the training data for QF is ambiguous between the linear rule (i.e., moving the first auxiliary) and the hierarchical rule (i.e., moving the main auxiliary</p><p>). Center-embedded sentences do not meet this ambiguity requirement and, therefore, cannot appear in question formation training samples. To ensure the model is exposed to diverse sentence types, McCoy et al. [26] introduced a secondary task to the QF training dataset: declaration copying. Like question formation, the declaration-copying example starts with a declarative sentence, but instead of transforming it, the model simply repeats it. Since the ambiguity requirement only applies to the primary question formation task, declaration-copying examples can include center embeddings. Concrete examples of both tasks can be found in Appendix B. We train models on three modifications of the original training data, varying the composition of the declaration-copying subset. In Quest Only, we remove all declaration-copying examples. In Center embed, we only keep center-embedded examples. In Right branch, we only keep right-branching examples. Every modified training sets retains all examples of the primary task, question formation. Every model trained, regardless of its training set composition, reaches 100% in-distribution validation Original Quest Only Center embed Right branch  Our results confirm that declaration copying examples, specifically center embeddings, are essential for inducing hierarchical generalization. Models trained without any declaration-copying examples fail to achieve an OOD accuracy above 75%; so do models trained only on right-branching declarationcopying examples. When trained instead only on center-embedded declaration-copying examples, models exhibit a strong preference for the hierarchical rule. This evidence suggests that centerembedded sentences direct a model towards the hierarchical rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tense Inflection Results</head><p>In the TI training data, both right-branching and center-embedded sentences are made ambiguous by ensuring the distractor noun (i.e., a noun that appears between the main subject and the main verb) shares the same plurality as the main subject. For right-branching sentences, the distractor noun occurs in a prepositional phrase. For center-embedded sentences, the distractor noun occurs in a relative clause; either the subject or the object of the modifying clause can act as the distractor noun.</p><p>We list examples below:</p><p>1. Right Branching: The noun in the prepositional phrase (e.g., " to the cabinet") acts as the distractor in the TI task.</p><p>Example A (ID): The keys to the cabinets are on the table. Example B (OOD): The keys to the cabinet are on the table. 2. Center Embedding: Either the subject or the object inside the relative clause acts as the distractor in the TI task. Example C (ID): The keys that unlock the cabinets are on the table. Example D (OOD): The keys that unlock the cabinet are on the table. We create variations of the TI training data by adjusting the ratio of right-branching to centerembedded samples while keeping the total training size constant.<ref type="foot" target="#foot_0">foot_0</ref> A model's generalization behavior is tested on two OOD sets: one containing unambiguous right-branching sentences (e.g., Example B) and the other containing unambiguous center-embedded sentences (e.g., Example D).</p><p>Generalization accuracies are shown in Figure <ref type="figure" target="#fig_1">3</ref> (right). When the training data is dominated by ambiguous right-branching sentences, the model fails to learn the hierarchical rule, as indicated by low OOD accuracy. However, when trained on a greater proportion of center-embedded sentences, the model systematically applies the hierarchical rule to both right-branching and center-embedded OOD sentences. As shown in Figure <ref type="figure" target="#fig_1">3</ref> (right), regardless of its training data mix, the model generalizes hierarchically to OOD center embeddings. In contrast, the model only generalizes hierarchically to right-branching sentences after being exposed to a sufficient quantity of center-embedded sentences during training. In other words, the model eventually learns to treat non-recursive sequences as hierarchical through exposure to recursive center embeddings. These observations suggest that center embeddings drive the model's overall preference for tree structures. For further analysis of which center embedding structures induce this bias most efficiently, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training Stabilizes if a Model Commits to a Rule</head><p>Why do some runs fail to generalize hierarchically even when trained on hierarchy-inducing data? In this section, we will show that these failures are consequences of training instability; models only stabilize OOD if they commit to a general rule, whether it is the hierarchical or linear rule. Some random seeds fail to stabilize regardless of our training set composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Instability During Training</head><p>When training models on both QF and TI tasks, some random seeds lead to highly unstable OOD behavior, with OOD generalization accuracy oscillating during training (see Appendix F for example training curves). We measure instability across training time using total variation (TV). Specifically, we checkpoint the model every 2K steps and measure the generalization accuracy Acc t at each checkpoint timestep t ∈ T . The total variation is defined as:  Total</p><formula xml:id="formula_0">Variation (TV) = 1 |T | t∈T |Acc t -Acc t-1 | (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Stability Ties to Rule Commitment</head><p>We now demonstrate that stable OOD behavior is associated with rule commitment. We construct QF training datasets with different proportions of hierarchy-inducing (i.e., center-embedded) and linearity-inducing (i.e., rightbranching) declaration examples, while keeping all question examples from the original training set. Further details on the dataset can be found in Appendix D.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the relationship between data homogeneity and training stability. When the training data is dominated by either linearity-inducing (99% linear) or hierarchyinducing (0% linear) examples, more random seeds lead to stable OOD curves. When the training data is a heterogeneous mix instead, potential rules compete, leading to a higher proportion of unstable training runs.</p><p>By controlling for training instability, we reveal that generalization behavior is clustered and highly bimodal across random seeds. As shown in Figure <ref type="figure" target="#fig_5">5</ref>, regardless of the data mix, the final generalization accuracy for all stable models is either 100% or 0%-that is, stable models always commit to a systematic rule. While either rule can be implemented by a stable model, training data composition determines how likely a run is to stabilize in any rule and whether that rule is likely to be hierarchical or linear. Interestingly, when the data is heterogeneous (e.g., 10% of examples are linearity-inducing right-branching sequences), the final generalization accuracy for stable runs is bimodally distributed, clustering around 100% or 0%. In fact, the horseshoe-shaped curves in Figure <ref type="figure" target="#fig_5">5</ref> illustrate that the less stable a training run is, the less systematic the model tends to be in its OOD rules.</p><p>In summary, with heterogeneous training data, competition between rules leads to more unstable training runs. Even with heterogeneous data mixes, however, some runs can still stabilize if they commit to one of the competing rules. Therefore, heterogeneous data also leads models to cluster into bimodally distributed OOD generalization accuracies, reflecting the distinct basins observed by Juneja et al. <ref type="bibr" target="#b17">[18]</ref> in text classification. Appendix E.2 presents similar findings for the TI task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data Diversity Leads to Generalization</head><p>Our experiments thus far have linked training stability to rule commitment. In this section, we will show that models can, in fact, stabilize without a systematic rule-if they memorize their training instead. Less diverse training data produces models that stabilize through memorization, whereas more diverse training data produces models that commit to systematic rules. Furthermore, mirroring our previous findings in data complexity, intermediate levels of data diversity lead to highly unstable runs even when all examples induce the same rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Measuring Data Diversity</head><p>We define the diversity of a dataset according to the syntactic similarity between different examples. We measure a sentence pair's similarity by the tree-edit distance (TED) of their latent tree representations <ref type="bibr" target="#b5">[6]</ref>. When two sentences share the same syntax tree, transforming one into the other requires only leaf-node (i.e., vocabulary) changes. For example, My unicorn entertains her tyrannosaurus, and, Your zebra eats some apples, have different vocabulary but identical syntax trees. We define a dataset's diversity as the number of unique syntactic trees it contains. Similar methods are used to measure diversity in both natural language <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b43">44]</ref> and code <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Diversity and stability</head><p>We will next show that when the model is exposed to fewer unique syntax trees during training, it memorizes their patterns without reliably applying rules to unseen structures. We demonstrate the effect by designing datasets to induce either hierarchical or linear generalization and then adjusting adjusting the syntactic diversity of representative examples. Whichever rule is induced, diversity imposes three distinct regimes: stable memorization behavior at low diversity, stable generalization behavior at high diversity, and unstable behavior at intermediate levels. This transition, from stable to unstable to back to stable, forms a U-shaped curve of stability with respect to dataset diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy-inducing data</head><p>We first control data diversity on datasets that induce hierarchical generalization in QF. We construct variations of the QF training data with different levels of syntactic diversity. Each constructed training set includes 50K question samples and 50K center embedding declarations, while varying the syntactic diversity of the declaration examples. We train 50 random seeds for each modified training set and measure intra-run instability with total variation (see 5.1). To assess rule commitment, we report the proportion of runs achieving generalization accuracy either &gt;95% or &lt; 5%, indicating a commitment to either rule (here, hierarchical rule is preferred).</p><p>Figure <ref type="figure">6</ref> (left) shows an inverse U-shaped relationship between data diversity and training instability, revealing three distinct regimes. Low-diversity data leads to the memorization regime, where training is stable but the model fails to commit to a rule. In Appendix G, we confirm that models in this low-diversity regime apply the hierarchical rule to syntax structures memorized during training, but cannot extrapolate the rule to unseen structures. High-diversity data leads to the hierarchical generalization regime, where training stabilizes because models commit to the hierarchical rule. In the mid-diversity unstable regime, the lack of data diversity hinders the likelihood of fully commiting  6: Inverse U-shaped relationship between training stability and data diversity. Whether training data favors the hierarchical (left) or linear (right) rule, diverse data promotes systematic rules over example memorization. At low diversity, training is stable but the model memorizes individual syntactic patterns rather than committing to a rule. With moderate data diversity, training becomes unstable. As diversity increases further, the model commits to a rule and training is the most stable. to a rule but the data is too diverse to memorize easily. Overall, with insufficient diversity, relatively few runs learn to apply the hierarchical rule across all examples.</p><p>Linearity-inducing data In Figure <ref type="figure" target="#fig_5">5</ref> (right), the model has a strong preference to apply the linear rule OOD when the training data contains 99% linearity-inducing data (i.e., right branching sentences). However, Figure <ref type="figure" target="#fig_1">3</ref> shows that when the training data contains exclusively right-branching sentences, models do not consistently follow any systematic rule (further details in Appendix D). We can use data diversity to explain the failure to commit to a rule from exclusively right-branching examples: right-branching sentences lack syntactic variation, as the main auxiliary always follows the subject noun. This lack of syntax diversity prevents rule extrapolation. By introducing center embeddings in just 1% of sentences, we introduce the diversity necessary to learn a systematic generalization rule.</p><p>To confirm that data diversity is also key to rule commitment for when data is mostly linearityinducing, we create variations of QF training data with 50K questions and 50K declarations, including 99% right-branching and 1% center-embedded sentences. We control the diversity of center-embedded sentences as before and use the proportion of runs achieving generalization accuracy either above 95% or below 5% to quantify the likelihood of committing to any rule (in this data setting, linear rule is preferred). Figure <ref type="figure">6</ref> (right) shows that training is least stable at intermediate levels of diversity, again providing three regimes: the memorization, unstable, and linear generalization regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusions</head><p>By exploring the role of data structure in determining OOD generalization rules, we have also revealed which settings render model behavior unpredictable. We can predict which rule is learned when diverse training data is composed of either hierarchy-inducing complex samples or linearity-inducing simple samples. However, a data mixture of complex and simple examples will lead to unstable dynamics and inconsistent rules across seeds. Likewise, we can predict that a model trained on low-diversity data will memorize and one trained on diverse data will learn a systematic generalization rule, but an intermediate level of data diversity leads to unstable training and inconsistent OOD generalization behavior. Our findings have a number of implications across machine learning and even formal linguistics.</p><p>Inconsistent behavior across seeds. While variation in model error is often treated as unimodal Gaussian noise in the theoretical literature <ref type="bibr" target="#b19">[20]</ref>, our findings suggest that errors may only be distributed unimodally for a given compositional solution. Our work joins the growing literature that suggests random variation can create clusters of OOD behaviors. Previously, clustered distributions have been documented in text classification heuristics <ref type="bibr" target="#b17">[18]</ref> and training dynamics <ref type="bibr" target="#b14">[15]</ref>. In our case, we note that generalization accuracy is only clearly multimodally distributed when we exclude unstable training runs. We suggest that research on compositional variation in training consider training stability in the future, which may expose addition behaviors as bimodal.</p><p>Implications for formal linguistics. Our findings have potential implications for linguistics debates about the poverty of the stimulus <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. Linguists have extensively studied the question of what data is necessary and sufficient to learn grammatical rules. In particular, Wexler <ref type="bibr" target="#b51">[52]</ref> argue that all English syntactic rules are learnable given "degree 2" data: sentences with only one embedded clause nested within another clause. Our center embedding results confirm that without a stronger architectural inductive bias-the very subject of the poverty of the stimulus debate-degree 1 data alone cannot induce a preference for hierarchical structure. However, our work also supports the position of Lightfoot <ref type="bibr" target="#b20">[21]</ref> that lower degree data is adequate for a child to learn a specific rule given sufficiently rich data outside of that rule, as the LM generalizes ID degree 1 QF rule examples to OOD degree 2 by using the hierarchical inductive bias induced by declaration examples.</p><p>Grokking, instability, and latent structure. Classic grokking <ref type="bibr" target="#b42">[43]</ref> is different from structural grokking: rather than a transition between generalization rules, it describes a transition from memorization to generalization. Our findings clarify both scenarios. We link structural grokking to the instability formed by competition between linear-and hierarchical-inducing training subsets. Without competing subsets, the model immediately learns either the linear or the hierarchical rule without the gradual transition of structural grokking. This instability could represent the same phenomenon of circuit competition described by Ahuja et al. <ref type="bibr" target="#b0">[1]</ref>. We find a similar pattern of instability in our study of data diversity, with implications for classic grokking. In this case, the competition is not between two rules, but instead between memorized heuristics-sufficient for modeling syntactically homogeneous training data-and simple OOD rules-required to efficiently model diverse training data. Yet again, while a strict memorization regime is relatively stable, the regime between memorization and generalization is unstable, leading to potential grokking.</p><p>Our findings also suggest that memorization is just another rule that the model can adopt when it is the simplest way of capturing the training distribution. Such a framework unifies the grokking literature with other phenomena such as emergence <ref type="bibr" target="#b46">[47]</ref> and benign interpolation <ref type="bibr" target="#b49">[50]</ref>; both areas suggest a phase transition between generalization and memorization. Future work could develop a unified theory of data diversity and complexity, describing the Bayesian and information-theoretical optimal models prescribed by those data properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work Extended</head><p>A.1 Syntax and Hierarchical Generalization While works mentioned in Section 2.1 focused on models trained from scratch, another line of research examined the inductive bias of pretrained models. Mueller and Linzen <ref type="bibr" target="#b30">[31]</ref> and Mueller et al. <ref type="bibr" target="#b31">[32]</ref> pretrained transformers on text corpora such as Wikipedia and CHILDES <ref type="bibr" target="#b24">[25]</ref> before fine-tuning them on the question formation task. They found that exposure to large amounts of natural language data enables transformers to generalize hierarchically.</p><p>Instead of using the question formation task as a probe, Hewitt and Manning <ref type="bibr" target="#b13">[14]</ref> and Murty et al. <ref type="bibr" target="#b34">[35]</ref> directly interpreted model's internal representation to understand whether transformers constrain their computations to to follow tree-structure patterns. Hewitt and Manning <ref type="bibr" target="#b13">[14]</ref> demonstrated that the syntax tress are embedded in model's representation space. Similarly, Murty et al. <ref type="bibr" target="#b34">[35]</ref> projects transformers into a tree-structured network, and showed that transformers become more tree-like over the course of training on language data.</p><p>Papadimitriou and Jurafsky <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> and Mueller et al. <ref type="bibr" target="#b32">[33]</ref> also studied how pretraining data could introduce an inductive bias in language acquisition. Papadimitriou and Jurafsky <ref type="bibr" target="#b39">[40]</ref> specifically identified that by pretraining models on data with a recursive structure their performance when later finetuning them on natural language. This finding is closely related to our conclusions around the importance of recursive center embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Random Variation</head><p>Specific training choices, such as hyperparameters, are crucial to model outcomes. However, even when controlling for these factors, training machine learning models remains inherently stochastic-models can be sensitive to random initialization and the order of training examples. D'Amour et al. <ref type="bibr" target="#b7">[8]</ref>, Naik et al. <ref type="bibr" target="#b35">[36]</ref>, and Zhou et al. <ref type="bibr" target="#b53">[54]</ref> reported significant performance differences across model checkpoints on various analysis and stress test sets. Zhou et al. <ref type="bibr" target="#b53">[54]</ref> further found that instability extends throughout the training curve, not just in final outcomes. To investigate the source of this inconsistency, Dodge et al. <ref type="bibr" target="#b8">[9]</ref> compared the effects of weight initialization and data order, concluding that both factors contribute equally to variations in out-of-sample performance.</p><p>Similarly, Sellam et al. <ref type="bibr" target="#b47">[48]</ref> found that repeating the pre-training process on BERT models can result in significantly different performances on downstream tasks. To promote more robust experimental testing, they introduced a set of 25 BERT-BASE checkpoints to ensure that experimental conclusions are not influenced by artifacts, such as specific instances of the model. In this work, we also observe training inconsistencies across runs on OOD data, both during training and at convergence. Unlike prior studies that focus on implications of random variations on experimental design, we study the source of training inconsistencies and link these inconsistencies to simplicity bias and the characteristics of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Simplicity Bias</head><p>Models often favor simpler functions early in training, a phenomenon known as simplicity bias <ref type="bibr" target="#b12">[13]</ref>, which is also common in LMs. Choshen et al. <ref type="bibr" target="#b6">[7]</ref> found that early LMs behave like n-gram models, and Saphra and Lopez <ref type="bibr" target="#b45">[46]</ref> observed that early LMs learn simplified versions of the language modeling task. McCoy et al. <ref type="bibr" target="#b27">[28]</ref> showed that even fully trained models can rely on simple heuristics, like lexical overlap, to perform well on Natural Language Inference (NLI) tasks. Chen et al. <ref type="bibr" target="#b4">[5]</ref> further explored the connection between training dynamics and simplicity bias, showing that simpler functions learned early on can continue to influence fully trained models, and mitigating this bias can have long-term effects on training outcomes.</p><p>Phase transitions have been identified as markers of shifts from simplistic heuristics to more complex model behavior, often triggered by the amount of training data or model size. In language models, Olsson et al. <ref type="bibr" target="#b37">[38]</ref> showed that the emergence of induction heads in autoregressive models is linked to handling longer context sizes and in-context learning. Similar phase transitions have been studied in non-language domains, such as algorithmic tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref> and arithmetic tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>In the context of hierarchical generalization, Ahuja et al. <ref type="bibr" target="#b0">[1]</ref> used a Bayesian approach to analyze the simplicity of hierarchical versus linear rules in modeling English syntax. They argued that transformers favor the hierarchical rule because it is simpler than the linear rule. However, their 2. Object-type: The main subject serves as the object within the clause.</p><p>Example: The keys that the bear uses are on the table.</p><p>This partition is motivated by their distinct subject-verb dependency patterns. In subject-type sentences, both the main verb (from the main clause) and the embedded verb (from the relative clause) depend on the main subject. In contrast, object-type sentences exhibit a nested subject-verb structure. Our goal is to investigate whether differences in subject-verb dependency patterns influence the model's preference for the hierarchical rule.</p><p>0.05 0.1 0.5 0.9 0.95 0.0 0.5 1.0 OOD Gen Acc "Object" Center Embed 0.05 0.1 0.5 0.9 0.95 Ratio of "Object" Center Embedding Samples "Subject" Center Embed Figure <ref type="figure">8</ref>: Both subtypes of center-embedded sentences induce hierarchical generalization in QF. We train models on datasets containing different ratios of object-type v.s. subject-type centerembedded sentences. We then evaluate on models on two OOD generalization set, one containing unambiguous object-type center-embedded sentences and the other unambiguous subject-type centerembedded sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 QF Task Results</head><p>The original QF training data contains roughly equal amount of two subtypes of center-embedded declarations, shown in Figure <ref type="figure">7</ref> (left). We investigate whether the two subtypes of center-embedded sentences differentially influence the model's preference for the hierarchical rule in the QF task. For all training data variants, we fix 50K question formation samples and 50K declaration copying samples, with the latter containing only center-embedded sentences but varying the ratio between the two subtypes. To analyze generalization behavior on a more granular level, we partition the generalization set (composed solely of center-embedded sentences) into the two subtypes as well. Models are trained on 30 random seeds, and results are shown in Figure <ref type="figure">8</ref>. Regardless of the data mix, the model consistently favors the hierarchical rule across both partitions of the generalization set. This suggests that, for question formation, both subtypes of center-embedded sentences equally contribute to the model's ability to identify the main auxiliary. 0.05 0.1 0.5 0.9 0.95 0.0 0.5 1.0 OOD Gen Acc "Object" Center Embed 0.05 0.1 0.5 0.9 0.95 "Subject" Center Embed 0.05 0.1 0.5 0.9 0.95 Ratio of "Subject" Center Embedding Right Branch Figure <ref type="figure">9</ref>: Subject-type center-embedded sentences gives a stronger bias towards hierarchical generalization in TI. We train models on datasets containing different ratios of object-type v.s. subject-type center-embedded sentences. We then evaluate on models on three OOD generalization set, one containing unambiguous object-type center-embedded sentences, one unambiguous subjecttype center-embedded sentences, and one unambiguous right-branching sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 TI Task Results</head><p>The original TI training data contains almost twice amount of subject-type center-embedded sentences than object-type ones, shown in Figure <ref type="figure">7</ref> (left). We repeat a similar experiment for the TI task, fixing the total number of tense inflection samples to 100K. As shown in Section 4.3, models exhibit the strongest hierarchical generalization when trained on primarily center-embedded sentences. Therefore, in the following data variants, 99% of the samples are center-embedded sentences, with the remaining 1% being right-branching sentences. Within the center-embedded samples, we vary the ratio between the two subtypes. To evaluate generalization, we split the generalization set into three groups: the two subtypes of center-embedded sentences and right-branching sentences. Models trained on 30 random seeds show that, across all three generalization sets, accuracy is positively correlated with the proportion of subject-type center-embedded sentences (Figure <ref type="figure">9</ref>). However, even when models are trained predominantly on object-type center-embedded sentences (teal violins in Figure <ref type="figure">9</ref>), they still show a clear preference toward hierarchical generalization. Thus, while both subtypes drive hierarchical generalization in TI, subject-type center-embedded sentences have a stronger effect.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Varying Data Ratios for Question Formation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data composition details</head><p>We construct variations of the training data using the following procedure. Each new dataset contains 50K questions (reused from the original data) and 50K declarations, where we control the ratio between center-embedded and right-branching sentences. These datasets are used for the experiments in Section 5.2. To generate additional declarations, we keep the distribution of the unique syntax structures in original dataset. Specifically, for each sentence in the original data, we extract the syntax tree using the CGF rules and resample words from the vocabulary to create new sentence samples.</p><p>Sensitivity to data compositions We use the five datasets above to examine how different mix ratios affect a model's preference towards the hierarchical generalization. The median generalization accuracy, along with error bars representing the 35th and 65th percentiles, is shown in Figure <ref type="figure" target="#fig_9">10</ref>. First, note that there is a sharp performance drop between the blue bar and the right-most orange bar. This sharp transition indicates that mixing in as little as 1% of right-branching declarations significantly reduces the model's likelihood of generalizing hierarchically. Interestingly, when the dataset is predominantly right-branching declarations, models consistently achieve 0% generalization accuracy, indicating a strong preference for the linear rule across all training runs. However, note that there is another sharp transition between the green bar and the left-most orange bar. This transition indicates that as soon as we remove the 1% of center-embedded sentences, the model fails to learn either the linear rule or the hierarchical rule. As a result, the generalization accuracy is close to random guess (∼ 25%). This transition is closely studied in Section 6.1, where we examine how data diversity leads to rule commitment.  copying samples from the original training data and train models on the tense-inflection task only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results on Tense Inflection</head><p>We evaluate the model's generalization performance on two OOD set containing unambiguous rightbranching and unambiguous center-embedded sentences, shown in Figure <ref type="figure">11</ref>. We can see that the model's OOD performances are the same with or without the secondary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Training Instability and Rule Commitment for TI</head><p>We repeat the same total variation analysis in Section 5 for the tense inflection task. We use the data mixes from Section 4.3. Specifically, we include only tense inflection samples and vary the ratio between linearity-inducing (i.e., right-branching) and hierarchy-inducing (i.e., center-embedded) sentences. In Section 4.3, we have already concluded that the hierarchical rule is always preferred for center-embedded sentences regardless of data mixes. For this reason, we are interested in examining the rule preference and training stability for unambiguous right-branching sentences. In Figure <ref type="figure" target="#fig_11">12</ref> we visualize the relationship between total variation and the final generalization accuracy on unambiguous right-branching sentences.</p><p>The qualitative behavior is similar to what we have observed in QF (Section 5.2). F Training Instability 0.0 0.5 1.0 Gen Acc TV 2.2 TV 6.8 TV 14.2 TV 21.6 0k 150k 300k Step 0.0 0.5 1.0 Gen Acc TV 0.7 0k 150k 300k Step TV 6.4 0k 150k 300k Step TV 15.5 0k 150k 300k Step TV 17.2</p><p>Sample Runs w/ Different Total Variation In Figure <ref type="figure" target="#fig_3">14</ref>, we visualize the training dynamics for 30 independent runs when trained on the original QF data. Each run differs in both model initialization and data order. Notice that the training dynamics for runs exhibit grokking behaviors: OOD generalization is delayed when compared to training loss convergence and validation performance convergence. These runs share a similar progression in training loss, validation accuracy, and generalization accuracy up until moment when the training loss converges. Interestingly, after convergence on training loss, all runs reach 0% on the generalization set, indicating that the model strictly prefers linear rules on OOD data. After that, models start to achieve non-trivial performance in generalization accuracy. However, for many runs the generalization accuracy does not increase monotonically. Instead, we observe massive swings in generalization accuracy during this training period as well as large inconsistency across</p><p>In the second variation, the declaration-copying task has diversity set to 5, with all 5 types containing center embeddings. For both datasets, the question-formation task remains unchanged, consisting solely of right-branching sentences. For the diversity=1 dataset, we calculate TED for each unique syntax type in the OOD set against the single syntax type in the declaration-copying task. For the diversity=5 dataset, we compute TED between each OOD sample and the five syntax types in the declaration-copying task, taking the minimum. This TED score provides a measure of similarity between the OOD samples and those encountered during training. Our goal is to determine, based on training with these datasets, which OOD syntax types the model applies the hierarchical rule to.</p><p>Result In Figure <ref type="figure" target="#fig_5">15</ref>, we visualize the final generalization accuracy for each OOD syntax type against its TED relative to the training data. When trained on low-diversity data (Figure <ref type="figure" target="#fig_5">15</ref>, left), generalization accuracy is negatively correlated with TED. For syntax types seen in the declarationcopying task (TED=0) and those similar to it, the model applies the hierarchical rule. However, for syntax types with high TED, the model's behavior is random (25%), indicating failure to follow any rule. As data diversity increases slightly (Figure <ref type="figure" target="#fig_5">15</ref>, right), generalization accuracy no longer correlates with TED, suggesting that once the model begins to extrapolate the hierarchical rule, it can apply this rule to a wider range of OOD syntax types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Components of training data drive different generalization behaviors. Left: Centerembedded sentences, which in the QF training data only appear in declaration copying examples, induce hierarchical generalization. Right: Models are trained on different TI training data mixes and evaluated on two OOD sets: unambiguous right-branching sentences (green) and unambiguous center-embedded sentences (red). For center-embedded sentences, the hierarchical rule is preferred regardless of data mixes. For right-branching sentences, the model's preference for the hierarchical rule is exclusively driven by having a large mix of center-embedded sentences in the TI training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Training is unstable when different subsets of data compete. Balanced mixtures of right-branching and center-embedded sentences have higher total variation than mixtures dominated by one or the other subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>v.s. OOD Generalization Accuracy on QF Data Compositions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training stability vs. final generalization accuracy for QF task. OOD behavior stabilizes during training if a model commits to a general rule. By mixing data that induces the linear and hierarchical rules, we can create conditions that allow models to stabilize in either rule. "Linear" denotes the proportion of linearity-inducing declarations in the data. Grey line indicates the smoothed average curve across all five datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><figDesc>Figure 6: Inverse U-shaped relationship between training stability and data diversity. Whether training data favors the hierarchical (left) or linear (right) rule, diverse data promotes systematic rules over example memorization. At low diversity, training is stable but the model memorizes individual syntactic patterns rather than committing to a rule. With moderate data diversity, training becomes unstable. As diversity increases further, the model commits to a rule and training is the most stable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>in QF Vary ctr embd v.s. rt brch decl ratio Ctr emb only Rt brch only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Hierarchical generalization in QF is sensitive to compositions of declaration-copying samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>E. 1 AFigure 11 :</head><label>111</label><figDesc>Figure11: Past-copy task is not necessary to induce hierarchical generalization in TI.In the original of TI training data<ref type="bibr" target="#b26">[27]</ref>, a secondary task is also included to mimic the question formation training data. In this secondary task, instead of transforming a sentence from the past tense to the present tense, the model simply needs to repeat it. For concrete examples, see Appendix B. Figure7(right) shows a breakdown of the two tasks in the original TI training data. In experiments conducted in Section 3.2, we have eliminated the used of this secondary task because center-embedded sentences can be included in the tense inflection training samples without violating the ambiguity requirement. Here, we use the training data originally proposed by McCoy et al.<ref type="bibr" target="#b26">[27]</ref> to confirm that the use of secondary task is indeed not necessary. Specifically, we remove all the past-tense-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Total Variation v.s. final generalization accuracy for TI task. Similar to Figure 5, we observe the same horseshoe shaped behavior between training stability and final generalization accuracy on right-branching sentences for the TI task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Each training run either stabilizes in a simple OOD generalization rule or oscillates in its OOD accuracy. The OOD generalization behaviors can be either stable or unstable when trained on different seeds. We use total variation to quantify the instability within one training run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples from two grammar case studies. Top: In the question formation task, the model moves the main auxiliary verb to the front to form a question. Bottom: In the tense inflection task, the model inflects the main verb from past to present tense, while respecting subject-verb agreement.</figDesc><table><row><cell>Dataset</cell><cell>Task Type</cell><cell>Examples</cell></row><row><cell>Question Formation</cell><cell>Quest (Ambiguous)</cell><cell>Input: My unicorn does move the dogs that do wait. Output: Does my unicorn move the dogs that do wait?</cell></row><row><cell></cell><cell>Quest</cell><cell>Input: My unicorn who doesn't sing does move. Linear Output: Doesn't my unicorn who sing does move?</cell></row><row><cell></cell><cell>(Unambiguous)</cell><cell>Hierarchical Output: Does my unicorn who doesn't sing move?</cell></row><row><cell></cell><cell>Present</cell><cell>Input: My zebra behind the peacock smiled.</cell></row><row><cell></cell><cell>(Ambiguous)</cell><cell>Output: My zebra behind the peacock smiles.</cell></row><row><cell>Tense Inflection</cell><cell>Present</cell><cell>Input: My zebra behind the peacocks smiled.</cell></row><row><cell></cell><cell></cell><cell>Linear output: My zebra behind the peacocks smile.</cell></row><row><cell></cell><cell>(Unambiguous)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The original training dataset contains a secondary past-tense copying task, to parallel the declaration-copying secondary task in QF. We show in Appendix E.1 that the secondary task is not necessary, and we do not include it in our modified training sets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Tian Qin</rs> and <rs type="person">David Alvarez-Melis</rs> were partially supported by the <rs type="funder">Kempner Institute, the Aramont Fellowship Fund</rs>, and the <rs type="funder">FAS Dean's Competitive Fund for Promising Scholarship</rs>. We are very grateful to <rs type="person">David Chiang</rs>, <rs type="person">Isabel Papadimitriou</rs>, <rs type="person">Ekdeep Singh</rs>, <rs type="person">John Rawski</rs>, <rs type="person">Will Merrill</rs> for their valuable feedback and discussions that helped improve this work.</p></div>
			</div>
			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The keys are on the table.</p></div>
			</div>


			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Data Samples B.1 Question Formation</head><p>We use the term declarations to refer to the declaration copying task and questions refer to the question formation task. Here are two examples randomly taken from the training data:</p><p>• Declaration Example: our zebra doesn't applaud the unicorn . decl our zebra doesn't applaud the unicorn . • Question Example: some unicorns do move . quest do some unicorns move ?</p><p>Both tasks begin with an input declarative sentence, followed by a task indicator token (decl or quest), and end with the output. During training, the entire sequence is used in the causal language modeling objective. The ID validation set and the OOD generalization set only contain question formation samples. In Figure <ref type="figure">7</ref> (left), we show a breakdown of two task types in QF training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Tense Inflection</head><p>• Past Example: our peacocks above our walruses amused your zebras . PAST our peacocks above our walruses amused your zebras . • Present Example: your unicorns that our xylophones comforted swam . PRESENT your unicorns that our xylophones comfort swim .</p><p>The tense inflection task is indicate by the PRESENT token. The secondary task only requires repeating the given sentence, which is always in the past tense, and the copying task is marked by the PAST token. In Appendix E.1, show that the past-tense copying task is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further Partitions on Center-Embedded Sentences C.1 Two Subtypes of Center-Embedded Sentences</head><p>In Section 4, we showed that center-embedded sentences drive hierarchical generalization in both the QF and TI tasks. Here, we further partition center-embedded sentences based on the syntactic role of the main subject (i.e., the subject of the main clause) within the modifying clause. Specifically, we classify them into two types:</p><p>1. Subject-type: The main subject serves as the subject within the clause.</p><p>Example: The keys that unlock the cabinet are on the table.  different seeds. Overall, training is always stable for ID data while the performance for OOD data is inconsistent across seeds. We visualize runs with different of total variation values in Figure <ref type="figure">13</ref>.  Measuring data similarity Building on the diversity measure from Section 6.1, we now use Tree-Edit Distance (TED) as a measure of sentence similarity. As before, we first construct syntax trees using CFG rules, then calculate TED using the Zhang-Shasha Tree-Edit Distance algorithm <ref type="bibr" target="#b52">[53]</ref>. We define TED=0 for sentences that share the same syntax structure but differ only in vocabulary. This similarity measure allows us to quantify, for each sample in the OOD generalization set, the closest matching sentence type in the training data. In the memorization regime, where the model encounters only a few syntax types, we suspect it cannot extrapolate rules to syntactically distinct OOD sentences. In contrast, with a more diverse syntax exposure, rule extrapolation may enable the model to apply rules even to OOD sentence types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Data Diversity and Memorization Patterns</head><p>Experiment To verify our intuition about memorization and generalization, we train models on two variations of the QF data. In the first variation, the declaration-copying task has data diversity set to 1, meaning only one syntax type appears, and we specifically choose one with center embedding.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning syntax without planting trees: Understanding when and why transformers generalize hierarchically</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16367[cs.CL</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hidden progress in deep learning: SGD learns parities near the computational limit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08799[cs.LG]</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attribute diversity determines the systematicity gap in VQA</title>
		<author>
			<persName><forename type="first">I</forename><surname>Berlot-Attwell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.08695[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poverty of the stimulus revisited</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Berwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pietroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yankama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1207" to="1242" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07311[cs.CL</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aspects of the theory of syntax</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>MIT Press</publisher>
			<pubPlace>London, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The grammar-learning trajectories of neural language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8281" to="8297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Underspecification Presents Challenges for Credibility in Modern Machine Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="61" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305[cs.CL</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformational networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Models of Human Language Acquisition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dependency distance approach to the syntactic complexity variation in the connected speech of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Humanit. Soc. Sci. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What shapes feature representations? Exploring datasets, architectures, and training</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12433[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Structural Probe for Finding Syntax in Word Representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Latent state models of training dynamics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09543[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ParaAMR: A large-scale syntactically diverse paraphrase dataset by AMR back-translation</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unified view of grokking, double descent and emergent abilities: A perspective from circuits competition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15175[cs.LG]</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Linear connectivity reveals generalization strategies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Juneja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12411[cs.LG]</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980[cs.LG]</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01474</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The child&apos;s trigger experience: Degree-0 learnability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lightfoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="334" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntaxsensitive dependencies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Omnigrok: Grokking beyond algorithmic data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01117[cs.LG]</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A percolation model of emergence: Analyzing Transformers trained on a formal language</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12578[cs.LG]</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The childes project: Tools for analyzing talk</title>
		<author>
			<persName><forename type="first">B</forename><surname>Macwhinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The database. 3</title>
		<meeting><address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09091[cs.CL</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="125" to="140" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01007[cs.CL</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<ptr target="https://rtmccoy.com/rnn_hierarchical_biases.html" />
		<imprint>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A tale of two circuits: Grokking as competition of sparse and dense subnetworks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsilivis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11873[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How to plant trees in language models: Data and architectural effects on the emergence of syntactic inductive biases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19905[cs.CL</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In-context learning generalizes, but not always robustly: The case of syntax</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4761" to="4779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coloring the blank slate: Pre-training imparts a hierarchical inductive bias to sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grokking of hierarchical structure in vanilla transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Characterizing intrinsic compositionality in transformers with tree projections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01288[cs.CL</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stress Test Evaluation for Natural Language Inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2340" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Progress measures for grokking via mechanistic interpretability</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05217[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">In-context learning and induction heads</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895[cs.LG]</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning music helps you read: Using transfer to study linguistic structure in language models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Injecting structural hints: Using language models to study inductive biases in language learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13060[cs.CL</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Competition dynamics shape algorithmic phases of in-Context Learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.01003[cs.LG]</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Transformers generalize linearly</title>
		<author>
			<persName><forename type="first">J</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12036[cs.CL</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Grokking: Generalization beyond overfitting on small algorithmic datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02177[cs.LG]</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crowdsourcing Syntactically Diverse Paraphrases with Diversity-Aware Prompts and Workflows</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramírez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Information Systems Engineering: 34th International Conference, CAiSE 2022</title>
		<meeting><address><addrLine>Leuven, Belgium; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2022">June 6-10, 2022. 2022</date>
			<biblScope unit="page" from="253" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding learning dynamics of language models with SVCCA</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00225[cs.CL</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding learning dynamics of language models with</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3257" to="3267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Are emergent abilities of large language models a mirage?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.15004</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In: arXiv [cs.AI cs.AI</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The MultiBERTs: BERT Reproductions for Robustness Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Revisiting code similarity evaluation with Abstract Syntax Tree Edit Distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.08817[cs.CL</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Benign interpolation of noise in deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Theunissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">S. Afr. Comput. J</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Explaining grokking through circuit efficiency</title>
		<author>
			<persName><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02390[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Formal principles of language acquisition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wexler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simple fast algorithms for the editing distance between trees and related problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1245" to="1262" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Critical data size of language models from a grokking perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10463[cs.CL</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
