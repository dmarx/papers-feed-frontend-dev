{
  "arxivId": "2412.04619",
  "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
  "authors": "Tian Qin, Naomi Saphra, David Alvarez-Melis",
  "abstract": "Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.",
  "url": "https://arxiv.org/abs/2412.04619",
  "issue_number": 164,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/164",
  "created_at": "2025-01-05T08:23:53.656301",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 60,
  "last_read": "2025-01-05T08:23:53.657116",
  "last_visited": "2024-12-22T17:55:18.862Z",
  "main_tex_file": null,
  "published_date": "2024-12-05T21:12:37Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL"
  ]
}