<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets</title>
				<funder ref="#_Ra87kU4">
					<orgName type="full">São Paulo Research Foundation FAPESP</orgName>
				</funder>
				<funder ref="#_ZSr5zEf">
					<orgName type="full">University Blockchain Research Initiative</orgName>
				</funder>
				<funder>
					<orgName type="full">LOEWE program of the State of Hesse (Germany)</orgName>
				</funder>
				<funder>
					<orgName type="full">Finnish Center for Artificial Intelligence FCAI)</orgName>
				</funder>
				<funder ref="#_BgukPTq">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_uTD2Nuj">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_vRd7pS8">
					<orgName type="full">Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro FAPERJ</orgName>
				</funder>
				<funder ref="#_qxjfwaK">
					<orgName type="full">Conselho Nacional de Desenvolvimento Científico e Tecnológico CNPq</orgName>
				</funder>
				<funder>
					<orgName type="full">Silicon Valley Community Foundation</orgName>
				</funder>
				<funder ref="#_39wWgwQ">
					<orgName type="full">Academy of Finland</orgName>
				</funder>
				<funder ref="#_FCKqhJ7">
					<orgName type="full">UKRI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-04">November 4, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tiago</forename><surname>Da Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Getulio Vargas Foundation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eliezer</forename><surname>Silva</surname></persName>
							<email>eliezer.silva@fgv.br</email>
						</author>
						<author>
							<persName><forename type="first">António</forename><surname>Góis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Getulio Vargas Foundation</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Mila -Quebec AI Institute</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dominik</forename><surname>Heider</surname></persName>
							<email>dominik.heider@uni-muenster.de</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Münster</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
							<email>samuel.kaski@aalto.fi</email>
							<affiliation key="aff3">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><surname>Mesquita</surname></persName>
							<email>diego.mesquita@fgv.br</email>
							<affiliation key="aff0">
								<orgName type="department">Getulio Vargas Foundation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adèle</forename><surname>Ribeiro</surname></persName>
							<email>adele.ribeiro@uni-muenster.de</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Münster</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-04">November 4, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">BD9ACEB3EC8D30B4D6EBDFB9C7FFBFDD</idno>
					<idno type="arXiv">arXiv:2309.12032v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Drawing conclusions about cause-and-effect relationships presents a fundamental challenge in various scientific fields and significantly impacts decision-making across diverse domains <ref type="bibr" target="#b43">Pearl [2000]</ref>. The importance of having structural knowledge, often encoded as a causal diagram, for conducting causal inferences is widely recognized, a concept made prominent by <ref type="bibr" target="#b9">Cartwright [1989]</ref>'s dictum: "no causes in, no causes out". When there is no objective knowledge to fully specify a causal diagram, causal discovery (CD) tools are instrumental in partially uncovering causal relationships among variables from, for example, observational data. Formally, let V = {V 1 , V 2 , . . . , V n } be a set of n observed variables and D be a dataset containing |D| = m samples for each V i ∈ V. A CD algorithm takes D as input and typically returns a single graph G = (V, E) with well-defined causal semantics, in which each node in V represents a variable V i ∈ V and each edge in E encodes the possible underlying (causal/confounding) mechanisms compatible with D. We first train an AGFN to fit a data-informed belief over AGs. Then, we iteratively refine it by 1) questioning (Q) experts on the relation between a highly informative pair of variables and 2) updating the belief given the potentially noisy answers (A). The histograms on top of the edges show marginals over edge types (green denotes ground truth). Notably, our belief increasingly concentrates on the true AG, 1 → 2 ↔ 3. This work focuses on recovering the structure of the underlying causal diagram when unobserved confounders may be at play. We propose to address this task by not only leveraging observational data but also by accounting for potentially noisy pieces of expert knowledge, otherwise unavailable as data. Throughout this work, we consider ancestral graphs (AGs) as surrogates for causal diagrams. AGs are particularly convenient since they encode latent confounding without explicitly invoking unobserved variables. Moreover, AGs capture all conditional independencies and ancestral relations among observed variables V, as entailed by a causal diagram <ref type="bibr" target="#b46">[Richardson and Spirtes, 2002]</ref>.</p><p>In the realm of CD from solely observational data, algorithms aim to construct a compact representation of the joint observational distribution P (V), which implies a factorization as a product of conditional probabilities. Notably, multiple models may entail the same conditional independencies; in such cases, they are denoted as Markov-equivalent. As a result, these algorithms can only reconstruct the class of Markov-equivalent models (AGs), denoted as the Markov Equivalence Class (MEC) and typically represented by a Partial Ancestral Graph (PAG). Importantly, CD beyond the MEC by leveraging domain knowledge presents a critical challenge. Notably, there is no proper characterization of an equivalence class that accounts for knowledge stemming from both humans and data <ref type="bibr">[Wang et al., 2022]</ref>.</p><p>There is a variety of algorithms for CD from observational data, primarily categorized into constraintand score-based methods. The former uses (in)dependence constraints derived via conditional independence tests to directly construct a PAG representing the MEC. The latter uses a goodness-of-fit score to navigate the space of AGs, selecting an optimum as a representative for the MEC.</p><p>Nonetheless, methods within both paradigms suffer from unreliability when data is scarce. Specifically, for the majority of the CD algorithms, formal assurances that the inferred MEC accurately represents the true causal model heavily rely on the so-called faithfulness assumption, which posits that all conditional independencies satisfied by P (V) are entailed by the true causal model <ref type="bibr" target="#b65">[Zhang and Spirtes, 2016]</ref>. However, this presents a critical challenge in real-world scenarios, as violations of the faithfulness assumption become more prominent when relying on P (V) estimated from limited data <ref type="bibr" target="#b54">Uhler et al. [2012]</ref>, <ref type="bibr" target="#b0">Andersen [2013]</ref>, <ref type="bibr">Marx et al. [2021]</ref>. For constraint-based methods, hypothesis tests may lack the statistical power to detect conditional independencies accurately. These errors may propagate and trigger a chain reaction of erroneous orientations <ref type="bibr" target="#b64">Zhang and Spirtes [2008]</ref>, <ref type="bibr">Zhalama et al. [2017b]</ref>, <ref type="bibr">Ng et al. [2021]</ref>. For scorebased methods, although score functions directly measure goodness-of-fit on observational data, small sample sizes can significantly skew the estimates for the population parameters. Consequently, structures deemed score-optimal may not necessarily align with the ground-truth MEC Ogarrio et al. <ref type="bibr">[2016]</ref>. A major concern is that the overwhelming majority of CD algorithms produce a single representation of the MEC as output, without quantifying the uncertainty that arises during the learning process <ref type="bibr" target="#b11">Claassen and Heskes [2012]</ref>, <ref type="bibr" target="#b24">Jabbari et al. [2017]</ref>. This poses a significant challenge for experts, as it hinders their ability to validate the algorithm's outcome or gain insights into potential venues for improving inference quality.</p><p>To alleviate the lack of uncertainty quantification in CD, we propose sampling AGs from a distribution defined using a score function, which places best-scoring AGs around the mode by design. This effectively provides end-users with samples that reflect the epistemic uncertainty inherent in CD, thus allowing their propagation through downstream causal inferences. In particular, we sample from our belief using Generative Flow Networks [GFlowNets; <ref type="bibr">Bengio et al., 2021a,b]</ref>, which are generative models known for sampling diverse modes while avoiding the mixing time problem of MCMC methods, and not requiring handcrafted proposals nor accept-reject steps <ref type="bibr">[Bengio et al., 2021a]</ref>.</p><p>Acknowledging the low-data regime as CD's primary challenge, we also propose actively integrating human feedback in the inferential process. This involves modeling user knowledge on the existence and nature (confounding/ancestral) of the relations and using it to weigh our beliefs over AGs. During our interactions with experts, we probe them about the relation of the pair of variables that optimizes a utility/acquisition function, for which we propose the negative expected cross-entropy between our prior and updated beliefs. Unlike prior strategies, our acquisition avoids the need to estimate the normalizing constant and predictive distribution of our updated belief, as needed for information gain and mutual information, respectively. Notably, we use importance sampling <ref type="bibr" target="#b34">[Marshall, 1954</ref><ref type="bibr" target="#b20">, Geweke, 1989]</ref> to update our initial belief with the human feedback, which avoids retraining GFlowNets after each human interaction.</p><p>While incorporating expert knowledge into CD has been a long-standing goal <ref type="bibr">[Meek, 1995a</ref><ref type="bibr" target="#b10">, Chen et al., 2016</ref><ref type="bibr" target="#b28">, Li and Beek, 2018</ref><ref type="bibr">, Wang et al., 2022]</ref>, existing works either rely on strong assumptions (e.g., causal sufficiency) or assume the knowledge is noiseless, aligned with the ground truth <ref type="bibr" target="#b1">[Andrews, 2020</ref><ref type="bibr">, Wang et al., 2022]</ref>. Importantly, our work introduces the first iterative CD framework for AGs involving a human in the loop and accommodating potentially noisy feedback, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To validate our approach, we conduct experiments using the BIC score, for linear Gaussian causal models. Specifically, we assess: i) our ability to sample from score-based beliefs over AGs, ii) how our samples compare to samples from bootstrapped versions of state-of-the-art (SOTA) methods, and iii) the efficacy of our active knowledge elicitation framework using simulated humans. We observe that our method, Ancestral GFlowNet (AGFN), i) accurately samples from our beliefs over AGs; ii) consistently includes AGs with low structural error among its top-scored samples; and iii) is able to greatly improve performance metrics (i.e., SHD and BIC) when incorporating human in the loop.</p><p>In summary, the contributions of our work are:</p><p>1. We leverage GFlowNets to introduce AGFN, the first CD algorithm for scenarios under latent confounding that employs fully-probabilistic inference on AGs;</p><p>2. We show AGFN accurately learns distributions over AGs, effectively capturing epistemic uncertainty.</p><p>3. We propose an experimental design to query potentially noisy expert insights on relationships among pairs of variables that lead to optimal uncertainty reduction.</p><p>4. We show how to incorporate expert feedback into AGFN without retraining them from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This section introduces the relevant notation and concepts. We use uppercase letters V to represent a random variable or node in a graph, and boldface uppercase letters V to represent matrices or sets of random variables or nodes. Ancestral graphs. Under the assumption of no selection bias, an ancestral graph (AG) G over V is a directed graph comprising directed (→) and bidirected (↔) edges <ref type="bibr" target="#b46">Richardson and Spirtes [2002]</ref>, <ref type="bibr" target="#b61">Zhang [2007]</ref>. In any directed graph, if a sequence of directed edges, V i → • • • → V j , connects two nodes V i and V j , we refer to this sequence as a directed path. In this case, we also say that V i is an ancestor of V j and denote this relation as V i ∈ An(V j ). By definition, any AG G must further satisfy the following:</p><formula xml:id="formula_0">G 1 = {X 1 → X 2 } G 5 = {X 1 → X 2 , X 2 → X 3 } G 2 = {X 1 ↔ X 2 } G 0 = {} G 6 = {X 1 ↔ X 2 , X 1 → X 3 } □ G 3 = {X 1 → X 3 } G 4 = {X 2 → X 3 } G 7 = {X 1 ↔ X 2 , X 2 → X 3 } R(G5) R(G6) R(G7)</formula><p>Figure <ref type="figure">2</ref>: Illustration of the generative process of AGs {G 5 , G 6 , G 7 } using GFlowNets. Starting with an empty graph G 0 , we add edges between variables {X 1 , X 2 , X 3 } according to the action-policy π F . Solid edges trace trajectories leading to sampled graphs. Dashed lines represent non-realized transitions to terminal state □.</p><p>1. there is no directed cycle, i.e., if</p><formula xml:id="formula_1">V i → V j is in G, then V j ̸ ∈ An(V i ); and</formula><p>2. there is no almost directed cycle, i.e., if</p><formula xml:id="formula_2">V i ↔ V j is in G, then V j ̸ ∈ An(V i ) and V i ̸ ∈ An(V j ).</formula><p>As a probabilistic model, the nodes in an AG represent random variables, directed edges represent ancestral (causal) relationships, and bidirected edges represent associations solely due to latent confounding. For a complete characterization of AGs, refer to <ref type="bibr" target="#b46">Richardson and Spirtes [2002]</ref>. Data generating model. We assume that the data-generating model corresponds to a linear Gaussian structural causal model (SCM) <ref type="bibr" target="#b43">[Pearl, 2000]</ref> defined by a 4-tuple M = ⟨V, U, F, P (U)⟩, in which V = {V 1 , V 2 , . . . , V n } is a set of n observed random variables and U = {U 1 , U 2 , . . . , U n } is the set of unobserved random variables. Further, let P a i ⊆ V \ {V i } be the set of observed causes (parents) of V i , and U i be the set of unobserved causes of V i . Then, each structural equation f i ∈ F is defined as:</p><formula xml:id="formula_3">V i = j:Vj ∈P ai β ij V j + U i<label>(1)</label></formula><p>with P (U) being a multivariate Gaussian distribution with zero mean and a not necessarily identity covariance matrix Ω = (ω ij ) 1≤i,j≤n -the error terms {U i } are not necessarily mutually independent, implying that the system can be semi-Markovian (i.e., latent confounding may be present). Consider a lower triangular matrix of structure coefficients B = (β ij ) 1≤i,j≤n such that (I -B) is invertible, and β ij ̸ = 0 only if V j ∈ P a i . Then, the set of structural equations is given in matrix form by</p><formula xml:id="formula_4">V = BV + U =⇒ V = (I -B) -1 U.</formula><p>(2)</p><p>The class of all linear Gaussian SCMs parametrized as</p><formula xml:id="formula_5">N M = {N (0, Σ)|Σ = (I -B) -1 Ω(I -B) -⊤ } (3)</formula><p>is represented by an AG in which, for every i ̸ = j, there is a directed edge <ref type="bibr" target="#b46">[Richardson and Spirtes, 2002]</ref>.</p><formula xml:id="formula_6">V j → V i if β ij ̸ = 0 and a bidirected edge V j ↔ V i if ω ij ̸ = 0</formula><p>GFlowNets. Generative Flow Networks [GFlowNet; <ref type="bibr">Bengio et al., 2021a,b]</ref> are generative models designed to sample from a finite domain X proportionally to some reward function R : X → R + , which may be parametrized using neural networks. In this work, we define R as a strictly decreasing transformation of the BIC (more details in section 3). GFlowNets also assume there is a compositional nature to the elements x ∈ X , meaning that they can be built by iteratively acting to modify a base object (i.e., an initial state). For instance, graphs can be built by adding edges to a node skeleton <ref type="bibr">[Deleu et al., 2022]</ref> or molecules by adding atoms to an initial structure <ref type="bibr">[Bengio et al., 2021a]</ref>.</p><p>The generative process follows a trajectory of states s ∈ S guided by a transition probability π F : S 2 → [0, 1]. In turn, π F is proportional to a foward flow function F θ : S 2 → R + , which is parameterized by a neural network θ. Let Pa(s ′ ) (Ch(s ′ )) be the set of all states which can transition into (directly reached from) s ′ . Then, π F is defined as</p><formula xml:id="formula_7">π F (s ′ |s) = F θ (s → s ′ ) s ′ ∈Ch(s) F θ (s → s ′ ) . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>The support X of R is contained within S. There are also two special states in S: an initial state s 0 and a terminal state s f . We start with the initial state s 0 and transform it to a new valid state s with probability π F (s|s 0 ; θ). We keep iterating this procedure until reaching s f . States s valid as final samples (s ∈ X ) are known as terminating states and have a positive probability for the transition s → s f . Figure <ref type="figure">2</ref> illustrates this process with X being the space of AGs. Crucially, the same parameterization θ is used for all transition probabilities π F (•|s; θ) given any departing state s, allowing for generalization to states never visited during training.</p><p>As the GFlowNet framework requires that no sequence of actions leads to a loop, we represent the space of possible action sequences by a pointed Directed Acyclic Graph (DAG) <ref type="bibr">[Bengio et al., 2021b]</ref>. The generation of any sample x ∈ X follows a trajectory τ = (s 0 , s 1 , . . . , s T = x, s f ) ∈ S T +2 for a T ≥ 0. Different trajectories may lead to the same sample x. To ensure we sample proportionally to R, we search for a GFlowNet that satisfies the flow-matching condition, i.e., ∀s ′ ∈ S:</p><formula xml:id="formula_9">s∈Pa(s ′ ) F θ (s → s ′ ) = R(s ′ ) + s ′′ ∈Ch(s ′ ) F θ (s ′ → s ′′ ).</formula><p>(5)</p><p>Equation ( <ref type="formula">5</ref>) implies the flow that enters s ′ equals the flow leaving s ′ , except for some flow R(s ′ ) leaking from s ′ into s f . We let R(s) = 0 for s / ∈ X . Eventually, it may be that all states s are valid candidates, i.e., S = X ∪ {s f }. If so, each of eq. ( <ref type="formula">5</ref>)'s solutions satisfies a detailed-balance condition,</p><formula xml:id="formula_10">R(s)F θ (s → s ′ )F θ (s ′ → s f ) F θ (s → s f ) = R(s ′ )F B,θ (s ′ → s),<label>(6)</label></formula><p>for a parametrized backward flow F B,θ : S 2 → R + <ref type="bibr">Deleu et al. [2022]</ref>. In practice, we enforce eq. ( <ref type="formula" target="#formula_10">6</ref>) by minimizing</p><formula xml:id="formula_11">L(θ) = E s→s ′ log R(s ′ )π F B (s|s ′ ; θ)π F (s f |s; θ) R(s)π F (s ′ |s; θ)π F (s f |s ′ ; θ) 2 . (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>3 Ancestral GFlowNets</p><p>We propose AGFN, a GFlowNet-based method for sampling AGs using a score function. Specifically, AGFN encompasses a GFlowNet with the following characteristics:</p><p>1. Each trajectory state is a valid AG G t .</p><p>2. A terminating state's reward R(G T ) is a score-based potential suitable for CD of AGs.</p><p>3. A well-trained AGFN samples AGs with frequencies proportional to their rewards and with the best-scoring AG being, by design, the mode.</p><p>The generation of a trajectory τ = {{}, G 1 , G 2 , . . . , G T } begins with a totally disconnected graph with nodes V, iteratively adding edges of types {←, →, ↔} between pairs of variables. The following paragraphs describe AGFN. For further details, please refer to the Appendix. Action constraints. To ensure AGFN only samples AGs, we mask out actions that would lead to paths forming cycles or almost cycles. To achieve this, we verify whether the resulting graph respects <ref type="bibr">Bhattacharya et al. [2021]</ref>'s algebraic characterization of the space of AGs. More specifically, any AG G is characterized by an adjacency matrix A d ∈ R n×n for its directed edges and another adjacency matrix A b ∈ R n×n for its bidirected edges, adhering to:</p><formula xml:id="formula_13">trace(exp{A d }) -n + 1 T (exp{A d } ⊙ A b )1 = 0, (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>in which 1 is a n-dimensional unit vector and ⊙ denotes the Hadamard (elementwise) product of matrices. Score-based belief. We propose using a strictly decreasing transformation of a score function U as the reward R for AGFN. More precisely, we define R as</p><formula xml:id="formula_15">R(G) = exp µ -U (G) σ (9)</formula><p>for given constants µ ∈ R and σ ∈ R + that ensure numerical stability <ref type="bibr">[Zhang et al., 2023]</ref>. In practice, we sample S AGs {G (s) } S s=1 from an untrained AGFN, and set µ = 1 /S s U (G (s) ) and</p><formula xml:id="formula_16">σ = 1 /S s (U (G (s) ) -µ) 2 .</formula><p>Score for linear Gaussian models. Since we focus on linear Gaussian models, we choose the extended Bayesian Information Criterion <ref type="bibr" target="#b19">Foygel and Drton [2010]</ref> as our score function. Specifically, for any AG G = (V, E):</p><formula xml:id="formula_17">U (G) = -2l N ( B, Ω) + |E| log n + 2|E| log |V|,<label>(10)</label></formula><p>in which ( B, Ω) is the MLE estimate of model parameters (see eq. ( <ref type="formula">3</ref>)) obtained using the residual iterative conditional fitting algorithm <ref type="bibr" target="#b18">Drton et al. [2009]</ref>.</p><p>Forward flow. We use a Graph Isomorphism Network (GIN) <ref type="bibr" target="#b56">Xu et al. [2019]</ref> Φ to compute a ddimensional representation for each node in the AG G t at the t-th step of the generative process and use sum pooling to get an embedding for G t . Then, considering A t as the space of feasible actions at G t (i.e., those leading to an AG), we use an MLP ϕ : R d → R |At| with a softmax activation at its last layer to map G t 's embedding to a distribution over A t . More precisely, given</p><formula xml:id="formula_18">H (t) = Φ(G t ) ∈ R |V|×d , we compute p = ϕ v∈V H (t) v (<label>11</label></formula><formula xml:id="formula_19">)</formula><p>as the probability distribution over the feasible actions at G t . Backward flow. Backward actions correspond to removing edges. Following <ref type="bibr" target="#b48">Shen et al. [2023]</ref>, we parametrize the backward flow F B with an MLP and alternate between updating π F and π F B , using gradient-based optimization.</p><p>4 Human-in-the-Loop Causal Discovery</p><p>Concluding the training, we propose leveraging the AGFN-generated samples to design questions for the expert that optimize the reduction of entropy in the distribution p θ (G) over the space of AGs. Then, we use the human feedback to update p θ (G), and iteratively repeat the process. The following paragraphs describe i) how we model human feedback, ii) how we update our belief over AGs given human responses, and iii) our experimental design strategy for expert inquiry.</p><p>Modeling human feedback.</p><p>We assume humans are capable of answering questions regarding the ancestral relationship between pairs of random variables. In this case, we model their prior knowledge on a relation r = {U, V } between nodes U, V ∈ V as a categorical distribution over a random variable denoted ω r . Fix an arbitrary total order &lt; in V. By definition, ω r = 1 if there is no edge between U and</p><formula xml:id="formula_20">V ; ω r = 2 if U is ancestor of V &lt; U ; ω r = 3 if V is ancestor of U &gt; V ;</formula><p>and ω r = 4 if there is a bidirected edge between U and V . Since the human has access to our AGFN before being probed for the first time, we set ρ r,k = p θ (ω r = k) as the prior probability of ω r = k. Moreover, we consider that the expert's feedback f r ∈ {1, 2, 3, 4} on the relation r is a noisy realization of the true, unobserved value of the relation feature ω r under the expert's model. Putting all elements together results in a two-level Bayesian hierarchical scheme for categorical data:</p><formula xml:id="formula_21">ω r ∼ Cat(ρ r ), (<label>12</label></formula><formula xml:id="formula_22">) f r |ω r ∼ Cat δ ωr • π + (1 -δ ωr ) • 1 -π 3 ,<label>(13)</label></formula><p>in which ρ r = (ρ r,1 , ρ r,2 , ρ r,3 , ρ r,4 ) represents our prior beliefs about the relations' features, π ∈ [0, 1] reflects the reliability of the expert's feedback, and δ k is the k-th canonical basis of R 4 . Conveniently, the posterior distribution of the relation feature ω r given the feedback f r is a categorical distribution parametrized by with</p><formula xml:id="formula_23">ρ r η r ⊙ π • δ fr + 1 -π 3 • (1 -δ fr ) ,<label>(14)</label></formula><formula xml:id="formula_24">η r = ρ r,fr • π + 1-π 3 • (1 -ρ r,fr ).</formula><p>Updating beliefs.</p><p>We update our AGFN by weighing it by our posterior over the expert's knowledge, described in the previous paragraph, similarly to a product-of-experts approach <ref type="bibr" target="#b22">Hinton [2002]</ref>. For this, let f K = (f r k ) 1≤k≤K be the sequence of K feedbacks issued by the expert and define our novel belief distribution q(G; f K ) over the space of AGs as q(G;</p><formula xml:id="formula_25">f K ) ∝ p θ (G) 1≤k≤K p(ω r k |f r k ). (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>Importantly, we use p θ as proposal distribution in importance (re-)sampling Gordon et al. <ref type="bibr">[1993]</ref> to approximately sample from q(G; f K ) -or to approximate the expected value of a test function. More precisely, we estimate the value of a function h over the space of AGs as:</p><formula xml:id="formula_27">E q [h(G)] ≈ T t=1 c -1 q(G (t) ; f K ) p θ (G (t) ) h(G (t) ) (16) with (G (t) ) T t=1 ∼ p θ (G) and c = T t=1 q(G (t) ;f K ) /p θ (G (t) ).</formula><p>Active knowledge elicitation.</p><p>To make the most out of possibly costly human interactions, we query the human about the relation that maximally reduces the expected cross-entropy between our belief over AGs before and after human feedback. More precisely, we define an acquisition function a k : V 2 → R for the k &gt; 1-th inquiry as:</p><formula xml:id="formula_28">a k (r) = -E fr∼p(•|f K ) H (q(G; f K , f r ), q(G; f K )) (17)</formula><p>in which p(f r |f K ) is the posterior predictive distribution according to the user model, q 0 ∝ R and H(•, •) is the cross-entropy. Then, we maximize this acquisition to select which relation rk we will probe the expert, i.e.: rk = arg max</p><formula xml:id="formula_29">r∈( V 2 ) a k (r). (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>As aforementioned, we use importance sampling with q 0 as a proposal to estimate the acquisition function a k . This allows us to leverage AGFN samples and effectively avoid the need for retraining them. It is worth mentioning that because H(p, p ′ ) ≥ H(p, p) for any two distributions p and p ′ of the same support, our strategy is equivalent to minimizing an upper bound on the entropy of q k . Also, different from acquisitions based on information gain or mutual information, approximating Equation (17) via Monte Carlo does not require exhaustive integration over the space of AGs to yield asymptotically unbiased estimates -see Appendix.</p><p>chain4 IV collfork SHD BIC SHD BIC SHD BIC FCI ⋆ 3.03±1.13 5481.33±2.69 3.75±0.64 5426.18±1.74 6.26±1.20 5433.80±6.94 GFCI ⋆ 2.24±0.64 5479.77±1.75 4.21±0.96 5427.09±2.85 5.23±1.08 5431.67±7.91 DCD ⋆ 3.38±1.30 5482.97±5.16 5.22±1.23 5429.51±4.37 6.02±1.22 5436.84±9.41 N-ADMG 6.14±1.49 5520.01±75.34 8.50±1.44 5583.17±79.47 7.16±1.50 5491.86±84.47 AGFN (ours) 6.04±2.12 5494.67±37.08 8.72±2.04 5456.16±52.25 6.58±2.34 5478.01±40.36 Table <ref type="table">1</ref>: Average SHD and BIC. The ⋆ denotes methods yielding point estimates. We use Bootstrap to report the mean and average standard deviation for these. For N-ADMG and AGFN, we estimate the quantities using 100k samples. Figure <ref type="figure">4</ref>: Ancestral graphs representing the data generating models for the three considered datasets in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments have three objectives. First, we validate that AGFN can accurately learn the target distribution over the space of AGs. Second, we show that AGFN performs competitively with alternative methods on three data sets. Third, we attest that our experimental design for incorporating the expert's feedback efficiently reduces the uncertainty over AGFN's distribution. We provide further experimental details in the Appendix. Code is available in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Distributional Assessment of AGFN</head><p>Data. Since violations of faithfulness are more likely in dense graphs <ref type="bibr" target="#b54">Uhler et al. [2012]</ref>, we create 20 5-node random graphs <ref type="bibr" target="#b54">Uhler et al. [2012]</ref> from a directed configuration model <ref type="bibr" target="#b38">[Newman, 2010]</ref> whose in-and out-degrees are uniformly sampled from {0, 1, 2, 3, 4}. We draw 500 independent samples from a structure-compatible linear Gaussian SCM with random parameters for each graph.</p><p>Setup. We train AGFN for each random graph using their respective samples. Then, we collect AGFN samples and use them to compute empirical distributions over the (i) edge features (i.e., p θ (U → V ), p θ (U ← V ), p θ (U ↔ V ) and p θ (U ̸ -V ) for each pair (U, V )), (ii) BIC, and (iii) structural Hamming distance to the true causal diagram (SHD).</p><p>Results. Figure <ref type="figure" target="#fig_1">3</ref> shows that the AGFN adequately approximates the theoretical distribution induced by the reward in Equation ( <ref type="formula">9</ref>). Furthermore, AGFNs induce distributions over BIC and SHD values that closely resemble those induced by p(G) ∝ R(G). We also note an important improvement over the prior art on probabilistic CD (N-ADMG): we found that over 60% of its samples were non-ancestral and that this method was of little use for making inferences over AGs. Meanwhile, AGFN does not sample non-ancestral graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with SOTA CD algorithms</head><p>Data. We generate 10 datasets with 500 independent samples from the randomly parametrized linear Gaussian SCMs corresponding to the canonical causal diagram <ref type="bibr" target="#b46">Richardson and Spirtes [2002]</ref> in each AG depicted in Figure <ref type="figure">4</ref>. Unshielded colliders and discriminating paths are fundamental patterns in the detection of invariances by CD algorithms under latent confounding <ref type="bibr" target="#b50">Spirtes and Richardson [1997]</ref>, <ref type="bibr">Zhang [2008b]</ref>. Thus, we consider the following 4-node causal diagrams with increasingly difficult configurations: (i) chain4, a chain without latent confounders; (ii) collfork, a graph with triplets involving colliders and non-colliders under latent confounding, and (iii) IV, a structure with a discriminating path for Z: Baselines. We compare AGFN with four notable CD methods: FCI <ref type="bibr" target="#b51">[Spirtes et al., 2001</ref><ref type="bibr">, Zhang, 2008b]</ref>, <ref type="bibr">GFCI [Ogarrio et al., 2016]</ref>, DCD <ref type="bibr">[Bhattacharya et al., 2021]</ref>, and N-ADMG <ref type="bibr" target="#b2">Ashman et al. [2023]</ref>. The baselines span four broad classes of CD methods. FCI is a seminal constraint-based CD algorithm that learns a PAG consistent with conditional independencies entailed by statistical tests. GFCI is a hybrid CD algorithm that learns a PAG by first obtaining an approximate structure using FGS <ref type="bibr" target="#b44">[Ramsey, 2015]</ref> (a BIC-score-based search algorithm for causally sufficient scenarios) and then by applying FCI to identify possible confounding and remove some edges added by FGS. DCD casts CD as continuous optimization with differentiable algebraic constraints defining the space of AGs and uses gradient-based algorithms to solve it. N-ADMG computes a variational approximation of the joint posterior distribution over the space of bow-free causal diagrams <ref type="bibr" target="#b40">[Nowzohour et al., 2017]</ref> associated with non-linear SCMs with additive noise. While N-ADMG focuses on a more restricted setting compared to AGs, it offers some uncertainty quantification in the variational pohttps<ref type="url" target="://www.overleaf.com/project/650454a084b798332af29ebesterior">://www.overleaf.com/project/650454a084b798332af29ebesterior</ref>, making it more closely comparable to our approach. We rigorously follow the experimental guidelines in the original works.</p><formula xml:id="formula_31">W → X ← Z → Y .</formula><p>Experimental setup We train AGFN on each dataset and use it to sample 100k graphs. We also apply FCI, GFCI, and DCD to 100 bootstrapped resamplings of each dataset to emulate confidence distributions induced by these algorithms. To compare the algorithms' outputs, we compute the sample mean and standard deviation of the BIC and SHD at the PAG level. Specifically, we compute the SHD between the ground-truth PAG and each estimated PAG obtained by each method. If the output is a PAG member (as for DCD, N-ADMG, and AGFN) we use FCI to transform the output using these graphs as oracles for conditional independencies. Furthermore, we directly compute the BIC for the outputs, as all PAG members are asymptotically score-equivalent.</p><p>Results. Table <ref type="table">1</ref> compares AGFN against baseline CD algorithms. Notably, our method consistently outperforms the only probabilistic baseline in the literature (N-ADMG) in terms of both SHD and BIC. As expected, however, the average BIC and SHD induced by AGFN are larger than those induced by the bootstrapped versions of the non-probabilistic algorithms, and the variances are greater; this is due to the inherent sampling diversity of our method and the resulting generation of possibly implausible samples. Indeed, Table <ref type="table" target="#tab_1">2</ref> shows that the three most rewarding samples from AGFN are as good as (and sometimes better than) the other CD algorithms. Results for N-ADMG comprise the three most frequent samples from the variational distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Simulating humans in the loop</head><p>Data. We follow the procedure from Section 5.1 to generate graphs with 4, 6, 8 and 10 nodes. We draw 500 samples from a compatible linear Gaussian SCM and use them to train an AGFN. Then, we follow our active elicitation strategy from Section 4 to probe simulated humans, adhering to the generative model described in the same section, with π = 0.9.</p><p>Setup. Since we are the first to propose an optimal design for expert knowledge elicitation, there are no baselines to compare AGFN against. That being said, we aim to determine whether the inclusion of expert feedback enhances the concentration of the learned distribution around the true AG, and evaluate the effectiveness of our elicitation strategy. To do so, we measure SHD to the true AG and BIC as a function of the number of expert interactions.</p><p>Results. Figure <ref type="figure" target="#fig_3">5</ref> shows that incorporating expert feedback substantially decreases the expected SHD and BIC under our belief over AGs. On the one hand, the remarkable decrease in expected SHD shows that our belief becomes increasingly focused on the true AG as we iteratively request the expert's feedback, regardless of the querying strategy. On the other hand, the second row shows that our querying strategy results in a substantial decrease in the BIC, demonstrating a faster reduction than random queries. This validates the notion that some edges are more informative than others, and we should prioritize them when probing the expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>CD under latent confounding. Following the seminal works by <ref type="bibr" target="#b51">Spirtes et al. [2001]</ref> and <ref type="bibr">Zhang [2008b]</ref> introducing the complete FCI, a variety of works have emerged. Among them are algorithms designed for sparse scenarios, including RFCI <ref type="bibr" target="#b13">[Colombo et al., 2012]</ref> and others <ref type="bibr" target="#b49">[Silva, 2013</ref><ref type="bibr" target="#b12">, Claassen et al., 2013]</ref>. Notably, <ref type="bibr" target="#b49">Silva [2013]</ref>'s framework uses a Bayesian approach to CD of Gaussian causal diagrams based on sparse covariance matrices. Nonetheless, it requires sampling one edge at a time and relies on numerical heuristics that might effectively alter the posterior we are sampling from. <ref type="bibr" target="#b13">Colombo et al. [2012]</ref> introduced the conservative FCI to handle conflicts arising from statistical errors in scenarios with limited data, even though it yields less informative results. Subsequent efforts to improve reliability led to the emergence of constraint-based CD algorithms based in Boolean satisfiability <ref type="bibr" target="#b23">[Hyttinen et al., 2014</ref><ref type="bibr" target="#b32">, Magliacane et al., 2016]</ref>, although they are known to scale poorly on <ref type="bibr">|V| [Lu et al., 2021]</ref>. In another paradigm, score-based search algorithms rank MAGs according to goodness-of-fit measures, commonly using BIC for linear Gaussian SCMs <ref type="bibr" target="#b52">[Triantafillou and Tsamardinos, 2016</ref><ref type="bibr">, Zhalama et al., 2017a</ref><ref type="bibr">, Rantanen et al., 2021]</ref>.</p><p>There are also hybrid approaches that combine constraint-based strategies to reduce the search space, such as GFCI <ref type="bibr" target="#b41">[Ogarrio et al., 2016]</ref>, M3HC <ref type="bibr" target="#b53">[Tsirlis et al., 2018]</ref>, BCCD <ref type="bibr" target="#b11">[Claassen and Heskes, 2012]</ref>,</p><p>and GSPo <ref type="bibr" target="#b6">[Bernstein et al., 2020]</ref>. Continuous optimization approaches have recently emerged as a novel approach to score-based CD, such as DCD <ref type="bibr">[Bhattacharya et al., 2021]</ref> and N-ADMG <ref type="bibr" target="#b2">Ashman et al. [2023]</ref>.</p><p>CD with expert knowledge. Previous works on CD have explored various forms of background knowledge. This includes knowledge on edge existence/non-existence <ref type="bibr">Meek [1995b]</ref>, ancestral constraints <ref type="bibr" target="#b10">Chen et al. [2016]</ref>, variable grouping <ref type="bibr" target="#b42">Parviainen and Kaski [2017]</ref>, partial order <ref type="bibr" target="#b1">Andrews [2020]</ref> and typing of variables <ref type="bibr">[Brouillard et al., 2022]</ref>. Incorporating expert knowledge is pivotal to reducing the search space and the size of the learned equivalence class. However, due to significant challenges, up to date, there are only a few works trying to integrate human knowledge into CD within the context of latent confounding <ref type="bibr" target="#b1">[Andrews, 2020</ref><ref type="bibr">, Wang et al., 2022]</ref>. These works operate under the assumption of perfect expert feedback. In contrast, our contribution is novel in that it confronts the challenges of real-world situations where expert input might be inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We presented AGFN, the first probabilistic CD method that accounts for latent confounding and incorporates potentially noisy human feedback in the loop. AGFN samples AGs according to a score function, quantifying the uncertainty in the learning process. Furthermore, it can leverage human feedback in an optimal design strategy, efficiently reducing our uncertainty on the true data-generating model. This work is focused on linear Gaussian models, using BIC as our score. However, the implementation of AGFNs is not restricted by this choice. In principle, we could replace the BIC with alternative score functions that are more appropriate for different types of variables, e.g. for discrete data <ref type="bibr" target="#b17">[Drton and Richardson, 2008]</ref>. It is also important to highlight that our framework does not require retraining the AGFN after we see human feedback. Moreover, AGFN is a GPU-powered algorithm, and while we used only one GPU in our experiments, it is possible to greatly accelerate AGFN by using cluster architectures with multiple GPUs.</p><p>By offering uncertainty-quantified CD together with a recipe for including humans in the loop, we expect AGFNs will significantly enhance the accuracy and reliability of CD, especially in real-world domains. Moreover, AGFNs bring a novel perspective to developing more comprehensive tools for downstream causal tasks <ref type="bibr" target="#b3">Bareinboim and Pearl [2016]</ref>, as the resulting distribution encodes knowledge from data and human feedback while accounting for epistemic uncertainty. For example, methods for causal reasoning that currently rely on a single AG <ref type="bibr">Zhang [2008a]</ref>, <ref type="bibr">Jaber et al. [2022]</ref> could exploit this distribution to incorporate a richer understanding of uncertainty and knowledge, thereby enhancing their robustness and reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Related Works</head><p>Generative Flow Networks [GFlowNets; <ref type="bibr">Bengio et al., 2021a,b]</ref> are generative models that sample discrete composite objects from an unnormalized reward function. They have been successfully used to sample various structures such as protein sequences <ref type="bibr">[Jain et al., 2022]</ref> and schedules <ref type="bibr">[Zhang et al., 2023]</ref>. They have also been used to train energy-based models <ref type="bibr" target="#b60">[Zhang et al., 2022]</ref>. In the field of structure learning, they have been applied to Bayesian networks -more specifically to sample a posterior over DAGs in linear Gaussian networks, although without accounting for unobserved confounding <ref type="bibr">[Deleu et al., 2022]</ref>. Recently, <ref type="bibr" target="#b16">Deleu et al. [2023]</ref> proposed an extension to jointly infer the structure and parameters, also grounded in the assumption of causal sufficiency. It is worth highlighting that training GFlowNets in these scenarios presents optimization challenges, resulting in the utilization of a variety of loss functions <ref type="bibr" target="#b48">[Shen et al., 2023]</ref>. Moreover, <ref type="bibr" target="#b27">Lahlou et al. [2023]</ref> proposed an extension of GFlowNets to continuous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Cross-entropy acquisition</head><p>The expected mutual information and the information gain are the most widely used informationtheoretic measures to actively interact with a human and choose the most informative data points to be labeled <ref type="bibr" target="#b47">Ryan et al. [2015]</ref>. However, we instead use the negative expected cross-entropy between the current and updated beliefs as the acquisition function of our experimental design (see eq. ( <ref type="formula">17</ref>)). As we show next, the approximation of both the mutual information and the information gain is intrinsically dependent upon the estimation of the log-partition of the updated beliefs over the space of ancestral graphs. Doing so is computationally intensive, and we would either need to use a Monte Carlo estimator of the integrals or use some posterior approximation -in both cases, leading to asymptotically biased estimates of the acquisition. In contrast, we can easily leverage AGFN samples to compute asymptotically unbiased estimates of our acquisition function. The next paragraphs provide further details.</p><p>Mutual information. The mutual information between two random variables X and Y with joint distribution p(X, Y ) and marginal distributions p(X) and p(Y ) is</p><formula xml:id="formula_32">I(X, Y ) = D KL [p(X, Y )||p(X) ⊗ p(Y )],<label>(19)</label></formula><p>in which D KL is the Kullback-Leibler divergence. In this context, an alternative approach to our experimental design for active knowledge elicitation would consist in iteratively maximizing the expected mutual information between the observed samples, G, and the elicited feedback, f K , to select the relation about which the expert would provide feedback. More specifically, we could choose</p><formula xml:id="formula_33">r K+1 = arg max r∈( V 2 ) E fr∼p(•|f K ) [I(G, f r )],<label>(20)</label></formula><p>in which</p><formula xml:id="formula_34">I(G, f r ) = D KL [q(G, f r |f K )||q(G|f K ) ⊗ p(f r |f K )],<label>(21)</label></formula><p>at each interaction with the expert. Nonetheless, note that</p><formula xml:id="formula_35">q(G, f r |f K ) = q(G|f K+1 )p(f r |f K ) = c K+1 (f r )p θ (G)   1≤k≤K+1 p(ω r k |f r k )   • p(f r |f K ), with f r K+1 = f r and c K+1 (f r ) =   G p θ (G)   1≤k≤K+1 p(ω r k |f r k )     -1<label>(22)</label></formula><p>as the partition function of our updated beliefs. Note also that Equation ( <ref type="formula" target="#formula_34">21</ref>) entails computing the entropy of q(G, f r |f K ). Thus, the selection criterion in eq. ( <ref type="formula" target="#formula_33">20</ref>) requires an accurate estimate of log c K+1 (f r ) -which is well-known for being a difficult problem <ref type="bibr" target="#b31">Ma et al. [2013]</ref> -and the Monte Carlo estimator for the log-partition function is asymptotically biased.</p><p>Information gain. The expected information gain of an elicitation is defined as the expected KL divergence between our updated and current beliefs over ancestral graphs. This approach is widely employed in Bayesian experimental design <ref type="bibr" target="#b47">Ryan et al. [2015]</ref>. In our framework, the information gain resulting from a feedback f r is</p><formula xml:id="formula_36">IG K (f r ) = D KL [q(G|f K ∪ f r )||q(G|f K )],<label>(23)</label></formula><p>which yields the criterion</p><formula xml:id="formula_37">r K+1 = arg max r∈( V 2 ) E fr∼p(•|f K ) [IG K (f r )] .<label>(24)</label></formula><p>Nonetheless, eq. ( <ref type="formula" target="#formula_37">24</ref>) suffers from the same problems of eq. ( <ref type="formula" target="#formula_33">20</ref>): it requires approximating the logarithm of the partition function c K+1 (f r ) of a distribution over the combinatorially large space of ancestral graphs, which is notably very challenging to estimate. Indeed, as</p><formula xml:id="formula_38">D KL [q(G|f K+1 )||q(G|f K )] = E G∼q(•|f K+1 ) log q(G|f K+1 ) q(G|f K ) = E G∼q(•|f K+1 ) [log p(f r |ω r ) + log c K+1 (f r ) -log c K ] ,</formula><p>with f r K+1 = f r , c K as the partition function of q(•|f K ) -that does not depend upon f r -, and c K+1 (f r ) defined in eq. ( <ref type="formula" target="#formula_35">22</ref>), the estimation of the information gain is inherently dependent upon the estimation of the log-partition function.</p><p>Cross-entropy. The cross-entropy between our updated and current beliefs is an intuitively plausible and practically useful strategy to interact with an expert efficiently. In fact, since</p><formula xml:id="formula_39">H[q(•|f K+1 ), q(•|f K )] = E G∼q(•|f K+1 ) [-log q(G|f K )] = E G∼q(•|f K+1 )   -log p θ (G) - 1≤k≤K log p(ω r k |f r k ) -l K   ,</formula><p>in which l K is the log-partition function of the distribution q(•|f K ). Further, the cross-entropy depends exclusively upon i) the logarithm of the samples' rewards, log p θ (G), which is readily computed within AGFN's generative process, and ii) the posterior distribution over the relations' features ω r given the expert's feedbacks f r , which is available in closed form. Hence, the previously mentioned expectation is unbiasedly and consistently estimated by our importance sampling scheme. Furthermore, our empirical findings in fig. <ref type="figure" target="#fig_3">5</ref> suggest that the cross-entropy yields good results and consistently outperforms a uniformly random strategy with respect to the BIC score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental details</head><p>We lay out the experimental and implementational details of our empirical analysis in the next subsections. In Appendix C.1, we describe the specific configurations of the CD algorithms that we compared with our method in table 2. Then, we consider in appendix C.2 some practical guidelines and architectural specifications that enable us to train and make inferences with AGFN efficiently. Finally, we contemplate in appendix C.3 the algorithmic details for simulating the expert's feedback according to our model for active knowledge elicitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Baselines</head><p>FCI. For the results in table 1, we first estimated a PAG using the stable version of FCI, which produces a fully order-independent final skeleton <ref type="bibr" target="#b14">[Colombo et al., 2014]</ref>. To identify conditional independencies, we used Fisher's Z partial correlation test with a significance level of α = 0.05. The BIC score associated with the PAG estimated by the FCI was computed as the BIC of a randomly selected maximal AG  <ref type="formula">25</ref>)) enables us to increase the proportion of high-scoring graphs (i.e., with a low BIC-score) with the drawback of reducing the AGFN's sampling diversity.</p><p>(MAG) within the equivalence class characterized by such PAG. The maximality of an AG depends on the absence of inducing paths between non-adjacent variables, which are paths where every node along it (except the endpoints) is a collider and every collider is an ancestor of an endpoint <ref type="bibr">[Rantanen et al., 2021]</ref>. This ensures that in the MAG every non-adjacent pair of nodes is m-separated by some set of other variables. Importantly, Markov equivalent MAGs exhibit asymptotic equivalence in terms of BIC scores <ref type="bibr" target="#b46">[Richardson and Spirtes, 2002]</ref>. As a result, the choice of a random MAG does not disrupt the validity of our results.</p><p>GFCI. Similarly, we applied GFCI with an initial search algorithm (FGS) based on the BIC score and the subsequent application of the FCI with conditional independencies identified by the Fisher's Z partial correlation test with a significance level α = 0.05. This was performed for all datasets listed in table 1. Also similar to the procedure adopted with the FCI, the BIC score associated with the estimated PAG was computed as the BIC of a randomly selected MAG within the equivalence class characterized by such PAG.</p><p>DCD. We adhered to the instructions provided in the official repository<ref type="foot" target="#foot_0">foot_0</ref> to apply the DCD method on the datasets in table 1. The SHD was obtained between the ground-truth PAG and the PAG corresponding to the estimated ADMG (i.e., the one obtained via FCI by using the d-separations entailed by the estimated ADMG as an oracle for conditional independencies). On the other hand, the BIC was computed for the estimated ADMG directly.</p><p>N-ADMG. To estimate the parameters of the variational distribution defined by N-ADMG, we executed the code provided at the official repository<ref type="foot" target="#foot_1">foot_1</ref> For fairness, we used the same hyperparameters and architectures reported in their original work <ref type="bibr" target="#b29">Li et al. [2023]</ref>; in particular, we trained the models for 30k epochs. After this, we sampled 100k graphs from the learned distribution. It is worth mentioning that the constraints of bow-free ADMG are guaranteed in the N-ADMG samples only in an asymptotic sense. Thus, we manually removed any cyclic graphs from the learned distribution. Then, we proceeded exactly as with DCD to estimate both the average SHD and the average BIC under the variational distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Implementational details for AGFN</head><p>Masking. To ensure AGFN only samples ancestral graphs, we keep track of a binary mask m t that indicates which actions lead to a valid state at the iteration t of the generative process; this mask defines the support of the policy evaluated at the corresponding state. In more detail, let y t be the last layer embedding (prior to a softmax) at iteration t of the neural network used to parametrize the forward flow of AGFN. The probability distribution over the space of feasible actions is then  As expected, the improvements entailed by the expert's feedback become increasingly effective as we increase the expert's reliability from 0.1 to 0.9. Results reflect the outcome of 30 scenarios simulated accordingly to algorithm 1 with a random canonical diagram G ⋆ with 5 nodes. We used our active knowledge elicitation scheme to select the query at each iteration.</p><formula xml:id="formula_40">p t = Softmax (y t ⊙ m t + ϵ • (1 -m t ))</formula><p>for a large and negative constant ϵ. We empirically verified that ϵ = -10 5 is sufficient to avoid the sampling of non-ancestral graphs.</p><p>Exploratory policy. During training, we must use an exploratory policy that (i) enables the exploration of yet unvisited states within the pointed DAG and (ii) exploits highly valuable and already visited states. To alleviate this phenomenon, we also draw trajectories from a uniform policy, which is a widespread practice in the literature <ref type="bibr">[Bengio et al., 2021a</ref><ref type="bibr">, Deleu et al., 2022</ref><ref type="bibr" target="#b48">, Shen et al., 2023]</ref>. More precisely, let Ch(G t ) be the set of states (i.e., ancestral graphs) directly reachable from G t and α ∈ [0, 1].</p><p>At each iteration t of the generative process, we sample an action (either an edge to be appended to the graph or a signal to stop the process)</p><formula xml:id="formula_41">a t ∼ (1 -α) • U(Ch(G t )) + α • π F (•|G t )</formula><p>and modify G t accordingly. The parameter α quantifies the mean proportion of on-policy actions and represents a trade-off between choosing actions that lead to highly valuable states (α = 1) and actions that lead to unvisited states (α = 0). We fix α = 1 2 throughout the experiments. During inference, we set α = 1 to sample actions exclusively from the GFlowNet's learned policy.</p><p>Detection of invalid states. We use the algebraic condition in eq. ( <ref type="formula" target="#formula_13">8</ref>) to check whether a graph G is ancestral. At each iteration of the generative process, we draw an action from the current exploratory policy and test the ancestrality of the updated graph; if it is not ancestral, we revert the sampled action and mask it. Importantly, this protocol guarantees that all graphs sampled from AGFN are ancestral.</p><p>Batch sampling. We exploit batch sampling to fully leverage the power of GPU-based computing in AGFN. As both the maximum-log-likelihood-based reward and the validation of the states are parallelizable operations, we are able to distribute them across multiple processing units and efficiently draw samples from the learned distribution. Crucially, this end-to-end parallelization substantially improves the computational feasibility of our algorithm and is a notable feature generally unavailable in prior works <ref type="bibr">Zhang [2008b]</ref>, <ref type="bibr" target="#b41">Ogarrio et al. [2016]</ref>, <ref type="bibr">Rantanen et al. [2021]</ref>. We use a batch size of 256 for all the experiments -independently of the graph size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of feedbacks</head><p>Figure <ref type="figure">9</ref>: Human-aided AGFN significantly outperforms alternative CD algorithms. Updating AGFN's distribution according to the feedback of an oracle substantially improves AGFN's capacity to correctly identify the true ancestral graph; indeed, a single feedback is sufficient to yield results better than (or indistinguishable from) alternative CD algorithms. We select the sampled AG with the highest posterior reward as a point estimate of AGFN and use the same datasets listed in table 2. The plots summarize the results of 30 HITL simulations using π = 0.9 and an oracle as an expert (see algorithm 1).</p><p>as the reward upon which the GFlowNet is trained; if T → 0, the distribution p T ∝ R T converges to a point mass in R(G)'s mode and, if T → ∞, p T converges to an uniform distribution. This approach resembles the simulated tempering scheme commonly exploited in Monte Carlo methods <ref type="bibr" target="#b33">Marinari and Parisi [1992]</ref> and was previously considered in the context of GFlowNets by <ref type="bibr">Zhang et al. [2023]</ref>. Figure <ref type="figure" target="#fig_4">6</ref> shows that progressively cold distributions (i.e., with T → 0) lead to progressively concentrated and decreasingly diverse samples. Notably, the use of cold distributions may be adequate if we are highly confident in our score and are mostly interested in high-scoring samples [e.g., as in <ref type="bibr">Rantanen et al., 2021]</ref>.</p><p>Sensitivity analysis for different noise levels. Figure <ref type="figure" target="#fig_6">7</ref> displays the effect of the feedback of an increasingly reliable expert over the expectations of both the SHD and the BIC. Notably, the usefulness of these feedbacks increases as the feedback noise decreases. This is expected as, for example, a completely unreliable expert consistently rules out only one of four possibilities for the features of each relation;</p><p>then, there remains a great ambiguity, albeit not as much as there was prior to their feedback, about the true nature of the elicited causal relation. Moreover, this experiment highlights the potential to adjust the reliability parameter π to incorporate knowledge into AGFN's learned distribution regarding the non-existence of a particular relation, rather than its existence. More specifically, assume that the expert is certain that there is no directed edge from the variable U to the variable V in the underlying ancestral graph; for instance, a doctor may be certain that cancer (U ) is not an ancestor (cause) of smoking (V ), but may be uncertain about the definite relation between U and V (i.e., smoking may or may not cause cancer). To incorporate such knowledge into our model, one approach is to set a necessarily small reliability parameter π (possibly, π = 0) along with the improbable relation U → V . This feedback will then be modeled as a relation unlikely to exist in the true ancestral graph. We emphasize that our model for the expert's responses is straightforwardly extensible to accommodate multiple feedbacks about the same causal relation under different reliability levels.</p><p>Ablation studies. Figure <ref type="figure" target="#fig_7">8</ref> shows the increase in the training efficiency due to our architectural designs for parametrizing both the forward and backward flows of AGFN. Noticeably, the use of a two-layer graph isomorphism network [GIN; <ref type="bibr" target="#b56">Xu et al., 2019]</ref> with a 256-dimensional embedding for the forward flow entailed a decrease of more than 10x in the number of epochs required for successfully training AGFN; this highlights the effectiveness of an inductively biased architectural design for the parametrization of GFlowNet's flows. Correlatively, the use of a parametrized backward flow significantly enhances the training efficiency of AGFN and emphasizes the inadequacy of a uniformly distributed backward policy pointed out in a previous work <ref type="bibr" target="#b48">Shen et al. [2023]</ref>.</p><p>Human-aided AGFN versus alternative CD methods. Figure <ref type="figure">9</ref> exposes the significant enhancement of AGFN's point estimates entailed by our HITL framework for CD. This underlines the usefulness of the elicited knowledge, which is simply incorporated into our model through a re-weighting of the reward function, enabling the identification of the true ancestral graph. In contrast, most alternative CD algorithms cannot be as easily adapted to include various forms of expert knowledge -and such incorporation, when it is possible, usually precedes any inferential process <ref type="bibr" target="#b1">Andrews [2020]</ref> or assumes the knowledge is perfect <ref type="bibr">Wang et al. [2022]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Human-in-the-loop probabilistic CD. We first train an AGFN to fit a data-informed belief over AGs. Then, we iteratively refine it by 1) questioning (Q) experts on the relation between a highly informative pair of variables and 2) updating the belief given the potentially noisy answers (A). The histograms on top of the edges show marginals over edge types (green denotes ground truth). Notably, our belief increasingly concentrates on the true AG, 1 → 2 ↔ 3.</figDesc><graphic coords="2,342.64,217.25,71.65,52.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sampling quality. The reward-induced marginal distribution over graph features is adequately approximated by the marginal distribution learned by the GFlowNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: CD with simulated human feedback. The top/bottom row shows the mean SHD/BIC of AGFN samples as a function of human interactions. Probing the expert about the edge that minimizes the mean cross-entropy leads to a faster decrease in BIC compared to a random strategy. The SHD decreases similarly in both cases. Results reflect the outcomes of 30 simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Tempered rewards. Training AGFN to sample from increasingly cold distributions (eq. (25)) enables us to increase the proportion of high-scoring graphs (i.e., with a low BIC-score) with the drawback of reducing the AGFN's sampling diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Sensitivity of our active knowledge elicitation framework to the reliability of the expert. Each column represents either the expected SHD (top) or expected BIC (bottom) as a function of the degree of confidence π ∈ [0, 1] in the expert as a function of the number of feedbacks. As expected, the improvements entailed by the expert's feedback become increasingly effective as we increase the expert's reliability from 0.1 to 0.9. Results reflect the outcome of 30 scenarios simulated accordingly to algorithm 1 with a random canonical diagram G ⋆ with 5 nodes. We used our active knowledge elicitation scheme to select the query at each iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Architectural design of AGFN. Top: The inductively biased parametrization of AGFN's forward flow -based upon a GNN -enables the substantial reduction of the number of epochs required for training. Bottom: The use of a parametrized backward policy similarly enhances the training efficiency compared to a uniform policy. For both experiments, we considered L(θ) &lt; 0.1 as the early stopping criterion to interrupt AGFN's training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SHD for point estimates. The mean SHD of the top-3 AGFN draws is comparable to or better than baselines.</figDesc><table><row><cell></cell><cell>chain4</cell><cell>IV</cell><cell>collfork</cell></row><row><cell>FCI</cell><cell cols="3">2.07±2.00 3.83±2.90 5.43±1.87</cell></row><row><cell>GFCI</cell><cell cols="3">1.50±1.63 3.63±3.16 5.53±2.11</cell></row><row><cell>DCD</cell><cell cols="3">2.27±1.46 4.80±2.17 5.60±2.13</cell></row><row><cell cols="4">N-ADMG (top 3) 4.38±0.81 6.08±1.77 6.87±0.93</cell></row><row><cell>AGFN (top 3)</cell><cell cols="3">2.00±1.55 3.50±3.29 4.90±2.70</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available online at https://gitlab.com/rbhatta8/dcd.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Available online at https://github.com/microsoft/causica/releases/tag/v0.0.0.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Diego Mesquita</rs> acknowledges the support by the <rs type="funder">Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro FAPERJ</rs> (<rs type="grantNumber">SEI-260003/000709/2023</rs>), the <rs type="funder">São Paulo Research Foundation FAPESP</rs> (<rs type="grantNumber">2023/00815-6</rs>), the <rs type="funder">Conselho Nacional de Desenvolvimento Científico e Tecnológico CNPq</rs> (<rs type="grantNumber">404336/2023-0</rs>), and the <rs type="funder">Silicon Valley Community Foundation</rs> through the <rs type="funder">University Blockchain Research Initiative</rs> (Grant #<rs type="grantNumber">2022-199610</rs>). António Góis acknowledges the support by <rs type="institution">Samsung Electronics Co., Ldt</rs>. <rs type="person">Adèle Ribeiro</rs> and <rs type="person">Dominik Heider</rs> were supported by the <rs type="funder">LOEWE program of the State of Hesse (Germany)</rs> in the Diffusible Signals research cluster and by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> [<rs type="projectName">031L0267A] (Deep Insight</rs>). <rs type="person">Samuel Kaski</rs> was supported by the <rs type="funder">Academy of Finland</rs> (<rs type="programName">Flagship programme</rs>: <rs type="funder">Finnish Center for Artificial Intelligence FCAI)</rs>, <rs type="funder">EU</rs> <rs type="programName">Horizon 2020 (European Network of AI Excellence Centres ELISE</rs>, grant agreement <rs type="grantNumber">951847</rs>), <rs type="funder">UKRI</rs> <rs type="grantName">Turing AI World-Leading Researcher Fellowship</rs> (<rs type="grantNumber">EP/W002973/1</rs>). We also acknowledge the computational resources provided by the <rs type="programName">Aalto Science-IT Project from Computer Science IT</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vRd7pS8">
					<idno type="grant-number">SEI-260003/000709/2023</idno>
				</org>
				<org type="funding" xml:id="_Ra87kU4">
					<idno type="grant-number">2023/00815-6</idno>
				</org>
				<org type="funding" xml:id="_qxjfwaK">
					<idno type="grant-number">404336/2023-0</idno>
				</org>
				<org type="funding" xml:id="_ZSr5zEf">
					<idno type="grant-number">2022-199610</idno>
				</org>
				<org type="funded-project" xml:id="_uTD2Nuj">
					<orgName type="project" subtype="full">031L0267A] (Deep Insight</orgName>
				</org>
				<org type="funding" xml:id="_39wWgwQ">
					<orgName type="program" subtype="full">Flagship programme</orgName>
				</org>
				<org type="funding" xml:id="_BgukPTq">
					<idno type="grant-number">951847</idno>
					<orgName type="program" subtype="full">Horizon 2020 (European Network of AI Excellence Centres ELISE</orgName>
				</org>
				<org type="funding" xml:id="_FCKqhJ7">
					<idno type="grant-number">EP/W002973/1</idno>
					<orgName type="grant-name">Turing AI World-Leading Researcher Fellowship</orgName>
					<orgName type="program" subtype="full">Aalto Science-IT Project from Computer Science IT</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Simulating humans in the loop Require: {G t } 1≤t≤T samples from AGFN, G * = (V, E) true ancestral graph, π reliability of the expert's feedback</p><p>Training hyperparameters. For AGFN's forward flow, we use a Graph Isomorphism Network (GIN, <ref type="bibr" target="#b56">Xu et al. [2019]</ref>) with 2 layers to compute embeddings of dimension 256. Then, we project these embeddings to a probability distribution using a three-layer MLP having leaky RELUs with a negative slope of -0.01 as activation functions. Correspondingly, we use an equally configured three-layer MLP to parametrize AGFN's flow. For training, we use the Adam method for the stochastic optimization problem defined by the minimization of the loss in eq. ( <ref type="formula">7</ref>). Moreover, we trained the neural networks for 3000 epochs for the human-in-the-loop simulations (in which we considered graphs having up to 10 nodes) and for 500 epochs for both the assessment of the distributional quality of AGFN and the comparison of AGFN with alternative CD approaches.</p><p>Computational settings. We trained the AGFNs for the experiments in fig. <ref type="figure">3</ref> and table 1 and fig. <ref type="figure">5</ref> for 500 epochs in computers equipped with NVIDIA's V100 GPUs. All the experiments were executed in a cluster of NVIDIA's V100 GPUs and the algorithms were implemented using the machine learning framework PyTorch. To estimate the PAG corresponding to AGFN's samples and compute the SHDs reported in table 1, we used the FCI's implementation of the pcalg package in R considering the dseparations entailed by these samples as a criterion for conditional dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Human in the loop</head><p>Algorithmic details. We describe in algorithm 1 our procedure for simulating interactions with an expert. Initially, we estimate the marginal probabilities p(ω r = k) of a relation r displaying the feature k ∈ {1, 2, 3, 4} under AGFN's learned distribution. This is our prior distribution. In algorithm 1, we denote {1, 2, 3, 4} by <ref type="bibr">[4]</ref>. Then, we iteratively select the relation that maximizes our acquisition function; the simulated human thus returns a feedback that equals the selected relation's true feature with probability π or is otherwise uniformly distributed among the incorrect alternatives. Importantly, this iterative mechanism can be interrupted at any iteration and the collected feedbacks can be used to compute the importance weights necessary for estimating expectations of functionals under our updated beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional experiments</head><p>Trade-off between diversity and optimality in AGFN. We may use tempered rewards to increase the frequency of high-scoring samples and thereby reduce the diversity of AGFN's distribution. More precisely, we choose a temperature T and consider <ref type="bibr">25)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When to expect violations of causal faithfulness and why it matters</title>
		<author>
			<persName><forename type="first">Holly</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="672" to="683" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the completeness of causal discovery in the presence of latent confounding with tiered background knowledge</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal reasoning in the presence of latent confounders via neural ADMG learning</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrin</forename><surname>Hilmkil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=dcN0CaXQhT" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causal inference and the data-fusion problem</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1510507113</idno>
		<ptr target="https://www.pnas.org/doi/abs/10.1073/pnas.1510507113" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7345" to="7352" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Korablyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salem</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GFlowNet Foundations. arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ordering-based causal structure learning in the presence of latent variables</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under unmeasured confounding</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Typing assumptions improve identification in causal discovery</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causal Learning and Reasoning (CLeaR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Nature&apos;s Capacities and their Measurement</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Cartwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning bayesian networks with ancestral constraints</title>
		<author>
			<persName><forename type="first">Eunice Yuh-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A bayesian approach to constraint based causal inference</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning sparse causal models is not np-hard</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning highdimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian structure learning with generative flow networks</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">António</forename><surname>Góis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chinenye Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Rankawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Joint bayesian inference of graphical structure and parameters with a single generative flow network</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mizu</forename><surname>Nishikawa-Toomey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jithendaraa</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19366</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Binary models for marginal independence</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas S Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computing maximum likelihood estimates in recursive linear models with correlated errors</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extended bayesian information criteria for gaussian graphical models</title>
		<author>
			<persName><forename type="first">Rina</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing (NeurIPS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian inference in econometric models using Monte Carlo integration</title>
		<author>
			<persName><forename type="first">John</forename><surname>Geweke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Novel approach to nonlinear/non-gaussian bayesian state estimation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Neil J Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian Fm</forename><surname>Salmond</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings F (Radar and Signal Processing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery: Conflict resolution with answer set programming</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discovery of causal models that contain latent variables through bayesian scoring of independence constraints</title>
		<author>
			<persName><forename type="first">Fattaneh</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">D</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10535</biblScope>
			<biblScope unit="page" from="142" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Causal Identification under Markov equivalence: Calculus, Algorithm, and Completeness</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adele</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biological sequence design with GFlowNets</title>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernandez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarrid</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Bonaventure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanakya</forename><surname>Dossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ajit Ekbote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payel</forename><surname>Simine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A theory of continuous generative flow networks</title>
		<author>
			<persName><forename type="first">Salem</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hernández-Garcıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léna</forename><surname>Néhale Ezzine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18269" to="18300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian network structure learning with side constraints</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Beek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic Graphical Models (PGM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gflownets with human feedback</title>
		<author>
			<persName><forename type="first">Yinchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tiny Papers @ (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving causal discovery by optimal bayesian network learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating the partition function of graphical models using langevin importance sampling</title>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Sixteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ancestral causal inference</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simulated tempering: A new monte carlo scheme</title>
		<author>
			<persName><forename type="first">E</forename><surname>Marinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europhysics Letters (EPL)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="451" to="458" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The use of multi-stage sampling schemes in Monte Carlo computations</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Marshall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>Rand Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A weaker faithfulness assumption based on triple interactions</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causal inference and causal explanation with background knowledge</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (UAI)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Strong completeness and faithfulness in bayesian networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Networks: an introduction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reliable causal discovery with improved exact search and weaker assumptions</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distributional equivalence and structure learning for bow-free acyclic path diagrams</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Nowzohour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A hybrid causal search algorithm for latent variable models</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ogarrio</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic Graphical Models</title>
		<imprint>
			<publisher>PGM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning structures of bayesian networks for variable groups</title>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Parviainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reason</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="110" to="127" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Scaling up greedy causal search for continuous variables</title>
		<author>
			<persName><forename type="first">Ramsey</forename><surname>Joseph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Maximal ancestral graph structure learning via exact search</title>
		<author>
			<persName><forename type="first">Kari</forename><surname>Rantanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (UAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ancestral graph markov models</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review of modern computational algorithms for bayesian optimal design</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">G</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">C</forename><surname>Drovandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Mcgree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">N</forename><surname>Pettitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="154" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Biancalani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07170</idno>
		<title level="m">Towards understanding and improving gflownet training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A MCMC approach for learning the structure of gaussian acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Models for Data Analysis, Studies in Classification, Data Analysis, and Knowledge Organization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A polynomial time algorithm for determining dag equivalence in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<idno>PMLR on 30</idno>
		<ptr target="https://proceedings.mlr.press/r1/spirtes97b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Madigan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</editor>
		<meeting>the Sixth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="1997-01">Jan 1997. March 2021</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
	<note>R1 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Score-based vs constraint-based causal learning in the presence of confounders</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causation: Foundation to Application Workshop (CFA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="59" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On scoring Maximal Ancestral Graphs with the Max-Min Hill Climbing algorithm</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Tsirlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincenzo</forename><surname>Lagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<idno type="ISSN">0888-613X</idno>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Geometry of the faithfulness assumption in causal inference</title>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garvesh</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Buhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sound and complete causal identification with latent variables given local background knowledge</title>
		<author>
			<persName><forename type="first">Tian-Zuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sat-based causal discovery under weaker assumptions</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhalama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (UAI)</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Weakening faithfulness: some heuristic causal discovery algorithms</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhalama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Mayer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41060-016-0033-y</idno>
		<ptr target="https://doi.org/10.1007/s41060-016-0033-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<idno type="ISSN">2364-4168</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust scheduling with GFlownets</title>
		<author>
			<persName><forename type="first">Corrado</forename><surname>David W Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rainone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Peschl</surname></persName>
		</author>
		<author>
			<persName><surname>Bondesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generative flow networks for discrete probabilistic modeling</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26412" to="26428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A characterization of markov equivalence classes for directed acyclic graphs with latent variables</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (UAI)</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="450" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Causal reasoning with ancestral graphs</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Detection of unfaithfulness and robust causal inference</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<idno type="ISSN">1572-8641</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="271" />
			<date type="published" when="2008-06">Jun 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The three faces of faithfulness</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11229-015-0673-9</idno>
		<ptr target="https://doi.org/10.1007/s11229-015-0673-9" />
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<idno type="ISSN">1573-0964</idno>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1011" to="1027" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
