- Decision to use SpecAugment for data augmentation in ASR
- Choice of log mel spectrogram as the input representation
- Selection of augmentation techniques: time warping, frequency masking, time masking
- Policy for applying multiple masks and their overlap
- Decision to use Listen, Attend and Spell (LAS) networks for ASR tasks
- Choice of network architecture parameters (e.g., number of layers, cell sizes)
- Implementation of learning rate schedules and their impact on performance
- Decision to incorporate shallow fusion with language models
- Choice of language model architecture and parameters
- Selection of datasets for training and evaluation (LibriSpeech and Switchboard)
- Decision on hyperparameter tuning and fixed parameters during experiments
- Choice of evaluation metrics (e.g., Word Error Rate)
- Decision to run experiments on Google Cloud TPU chips
- Policy for label smoothing and its application during training
- Decision to report state-of-the-art results and comparison with previous work
- Choice of augmentation policies for different datasets and tasks