{
  "arxivId": "2407.01392",
  "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
  "authors": "Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, Vincent Sitzmann",
  "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a\ndiffusion model is trained to denoise a set of tokens with independent\nper-token noise levels. We apply Diffusion Forcing to sequence generative\nmodeling by training a causal next-token prediction model to generate one or\nseveral future tokens without fully diffusing past ones. Our approach is shown\nto combine the strengths of next-token prediction models, such as\nvariable-length generation, with the strengths of full-sequence diffusion\nmodels, such as the ability to guide sampling to desirable trajectories. Our\nmethod offers a range of additional capabilities, such as (1) rolling-out\nsequences of continuous tokens, such as video, with lengths past the training\nhorizon, where baselines diverge and (2) new sampling and guiding schemes that\nuniquely profit from Diffusion Forcing's variable-horizon and causal\narchitecture, and which lead to marked performance gains in decision-making and\nplanning tasks. In addition to its empirical success, our method is proven to\noptimize a variational lower bound on the likelihoods of all subsequences of\ntokens drawn from the true joint distribution. Project website:\nhttps://boyuan.space/diffusion-forcing",
  "url": "https://arxiv.org/abs/2407.01392",
  "issue_number": 295,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/295",
  "created_at": "2025-01-04T15:03:03.858592",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-26T22:05:31.374Z",
  "main_tex_file": null,
  "published_date": "2024-07-01T15:43:25Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CV",
    "cs.RO"
  ]
}