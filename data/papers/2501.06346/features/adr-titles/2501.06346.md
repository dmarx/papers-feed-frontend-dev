- Decision to use sparse autoencoders for feature disentanglement
- Choice of Llama-3-8B and Aya-23-8B as the models for training
- Selection of Universal Dependencies 2.1 as the data source
- Decision to focus on morphosyntactic concepts like grammatical number, gender, and tense
- Use of causal interventions to verify multilingual feature representations
- Choice of probing classifiers for evaluating morphosyntactic feature sharing
- Decision to train on a primarily English dataset while ensuring multilingual effectiveness
- Implementation of Gated SAE architecture for improved feature representation
- Decision to measure classifier performance decrease upon ablation of multilingual features
- Choice of logistic regression for training probing classifiers
- Decision to pool non-padding token activations for classifier input
- Use of attribution patching for causal analysis of model behavior
- Decision to quantify overlap of causally influential features across languages
- Choice to validate the role of features in model generations through experiments
- Decision to explore the implications of parameter efficiency in multilingual models
- Use of a specific layer (L = 16) for capturing high-level features in the model
- Decision to analyze the impact of training data distribution on model performance across languages
- Choice to investigate the generalizability of learned abstractions in language models
- Decision to assess the robustness of cross-lingual representations in low-resource languages
- Use of a specific evaluation metric for measuring the effectiveness of interventions on model behavior