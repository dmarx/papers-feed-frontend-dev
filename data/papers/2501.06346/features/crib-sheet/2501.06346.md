- **Research Objective**: Investigate how large language models (LLMs) share representations of morphosyntactic concepts (e.g., grammatical number, gender, tense) across diverse languages.
  
- **Key Models**: 
  - Llama-3-8B: Primarily trained on English data.
  - Aya-23-8B: Developed for 23 typologically diverse languages.

- **Methodology**:
  - **Sparse Autoencoders (SAEs)**: Used to identify interpretable feature directions in neural networks.
    - Decomposition formula: 
      \[
      x = b + \sum_{i} f_i(x) d_i + \epsilon(x)
      \]
      where \(x\) is the input activation, \(b\) is a bias term, \(f_i(x)\) are sparse coefficients, \(d_i\) are unit vectors, and \(\epsilon(x)\) is the approximation error.
  
- **Causal Interventions**: 
  - Ablation of multilingual features leads to near-chance performance in classifiers across languages, indicating the importance of shared representations.

- **Feature Sharing**: 
  - Investigated overlap of causally influential sparse features for morphosyntactic concepts across languages.
  - Consistent decrease in classifier performance upon ablation of multilingual features confirms shared encoding.

- **Probing Classifiers**: 
  - Trained to predict morphosyntactic concepts using residual stream activations from the model's middle layer (L = 16).
  - Logistic regression used for mapping activations to task labels.

- **Findings**:
  - LLMs can develop robust, cross-lingual abstractions of morphosyntactic concepts, even when trained predominantly on English data.
  - Shared representations suggest a more efficient learning mechanism compared to redundant language-specific representations.

- **Implications**: 
  - Supports the hypothesis that LLMs can generalize knowledge across languages, enhancing performance in low-resource languages through cross-lingual transfer.

- **Attribution Patching**: 
  - A method to estimate the causal influence of components in neural networks, using the indirect effect (IE) to quantify importance.
    - Approximation formula:
      \[
      \hat{IE}_{atp}(m; a; x_{clean}, x_{patch}) = \nabla_a m|_{a=a_{clean}} (a_{patch} - a_{clean})
      \]

- **Conclusion**: 
  - The internal representations of LLMs may not be language-specific but rather concept-based, facilitating multilingual processing and understanding.