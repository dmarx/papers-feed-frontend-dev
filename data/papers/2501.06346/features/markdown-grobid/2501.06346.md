# Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages

## Abstract

## 

Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphsyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded in feature directions shared across many languages. We use causal interventions to verify the multilingual nature of these representations; specifically, we show that ablating only multilingual features decreases classifier performance to near-chance across languages. We then use these features to precisely modify model behavior in a machine translation task; this demonstrates both the generality and selectivity of these feature's roles in the network. Our findings suggest that even models trained predominantly on English data can develop robust, cross-lingual abstractions of morphosyntactic concepts.

## Introduction

In the brains of human bilinguals, syntax processing may occur in similar regions for their first and second language, depending on factors like when the second language was learned [(Cargnelutti et al., 2019)](#b6), language proficiency [(Połczyńska and Bookheimer, 2021)](#b44), among many other factors [(Sulpizio et al., 2020;](#b54)[Costa and Sebastián-Gallés, 2014)](#b9). In multilingual language models (LMs; [Shannon, 1948)](#b51), how apt is the analogy of shared processing to human bilinguals? If we desire parameter-efficiency, we might want multilingual Figure [1](#): Using sparse autoencoders, we find that language models share representations of grammatical concepts across languages. By intervening on these multilingual representations, we can change the model behavior given inputs in different languages. For example, we can make the model predict plural verbs in different languages by activating the same plural feature.

representations of concepts such as grammatical number, rather than many language-specific representations of the same concept.

Past work has emphasized language-balanced pretraining corpora (e.g., [Conneau et al., 2020;](#b8)[Xue et al., 2021;](#)[Mohammadshahi et al., 2022)](#b38), such that an LM could be said to have many primary languages. However, many of the best-performing multilingual LMs are now primarily English models, trained on over 90% English text (e.g., [Dubey et al., 2024)](#). Why do these models perform so well in non-English languages? We hypothesize that these models learn generalizable abstractions that enable more efficient learning of new languages. In other words, being able to deeply characterize a smaller distribution could allow models to acquire more robust abstractions that may generalize more effectively to a wider distribution post-hoc; in contrast, more balanced corpora (wider distributions) could encourage the model to start by learning language-specific abstractions, which are po-tentially never merged into higher-level languageinvariant concepts. For small models, on a synthetic language pair (English and a token-level duplication of it), [Schäfer et al. (2024)](#) show that imbalanced datasets lead to higher sample efficiency and higher overall performance in both languages in the low-data regime. [1](#foot_0)In large language models, multilinguality has both practical and theoretical scientific value. High performance on natural language processing tasks across many languages increases the impact and inclusivity of language technologies, and in a more compute-efficient manner than would be possible by training a large series of monolingual models. Moreover, multilingual models are hypothesized to be able to outperform monolingual models in low-resource languages: past work has emphasized the importance of cross-lingual transfer, where knowledge in one language is shared with another language whose corpora did not contain the information of interest. More abstractly, the extent to which abstract concepts are shared across languages addresses a key question: what kinds of multi-lingual processing mechanisms are acquirable from exposure to large-scale text distributions? It would be more parameter-efficient to learn generalizable abstractions that apply across languages, as opposed to redundantly learning the same grammatical feature for each language separately. Moreover, the existence of generalizable abstractions informs debates on memorization versus generalization: cross-linguistic generalization suggests a more sophisticated and broad application of particular concepts.

Past work has largely investigated multilingual language models using behavioral/benchmarking analyses [(Gupta and Srikumar, 2021;](#b24)[Raganato et al., 2020, inter alia)](#) or neuron-level mechanistic analyses [(Stanczak et al., 2022)](#b53). [Stanczak et al.](#) observe that masked language models often shared neurons across languages for a particular concept, but causally verifying this is difficult, as counterfactual interventions to neurons often affect the processing of irrelevant concepts (see §2). To overcome this challenge, we make our units of causal analysis sparse autoencoder features, which have been shown to be more monosemantic, and therefore more human-interpretable [(Cunningham et al., 2024;](#b10)[Bricken et al., 2023;](#b5)[Rajamanoharan et al., 2024)](#b48). This enables more precise causal interven-tions [(Marks et al., 2024;](#)[Mueller et al., 2024)](#b39), and allows us to qualitatively verify that features of interest are truly sensitive to the hypothesized feature, rather than a spurious but close-enough feature.

In this work, we train a set of sparse autoencoders on the intermediate activations of [Llama-3-8B (Dubey et al., 2024)](#) and Aya-3-8B [(Aryabumi et al., 2024)](#b1) and locate massively multilingual features for various morphosyntactic concepts. We design experiments to quantify the degree to which these concepts are shared across languages, and validate the their role in the models generations. Our results reveal that language models share morphosyntactic concept representations across typologically diverse languages, and that the internal lingua franca of large language models may not be English words per se, but rather concepts.[foot_1](#foot_1)

## Background

Feature disentanglement using sparse autoencoders. The features underlying model computation are not guaranteed to be aligned to neuron bases; they may instead be represented in a distributed manner, such that there is a many-to-many relationship between neurons and concepts [(Hinton et al., 1986;](#b26)[Smolensky, 1986)](#b52). In practice, single neurons[foot_2](#foot_2) are often polysemantic; in other words, they activate on a range of seemingly unrelated concepts [(Bolukbasi et al., 2021;](#)[Elhage et al., 2022b)](#). For example, [Bricken et al. (2023)](#b5) observe that a single neuron in a small LM responds to a mixture of academic citations, English dialogue, HTTP requests, and Korean text. To address this, [Cunningham et al. (2024)](#b10); [Bricken et al. (2023)](#b5) propose sparse autoencoders (SAEs) as a scalable technique for unsupervised discovery of interpretable feature directions in neural networks. Given an input activation x ∈ R d model , the autoencoder computes a decomposition

$x = x + ϵ(x) = b + i f i (x)d i + ϵ(x) (1)$into an approximate reconstruction x as a linear combination of features d i . The features d i ∈ R d model are unit vectors, the feature activations

$f i (x) ∈ R are a sparse set of coefficients, b ∈ R d model$is a bias term, and ϵ(x) ∈ R d model is the approximation error. [Bricken et al. (2023)](#b5) train SAEs by minimizing an L2 reconstruction error and an L1 regularization term to promote sparsity. However, the L1 regularization term introduces biases that can harm the accuracy of the reconstruction, as the loss can be decreased by trading-off reconstruction for lower L1. Therefore, we use the Gated SAE architecture [(Rajamanoharan et al., 2024)](#b48), which separates the functionality of selecting the features to use and estimating the activation magnitude of those directions. We provide additional details about Gated SAEs and their training in App. D.

A key idea underlying SAE-based interpretability is that one can reinterpret the language model's (LM) internal computations in terms of the SAE features. By applying the decomposition in equation 1 to hidden states x in the LM and folding the SAEs into the forward pass,[foot_3](#foot_3) we can express model representations as a combination of SAE feature activations f i and reconstruction errors ϵ. This reframing enables attribution patching techniques [(Syed et al., 2024)](#b56) to identify which SAE features have causal influence on particular aspects of model behaviors.

Finding causally relevant features using attribution patching. Causal interpretability methods aim to locate components in neural networks (such as attention heads or neurons) responsible for particular behaviors [(Mueller et al., 2024)](#b39). This requires constructing a distribution D over pairs of inputs (x clean , x patch ), where x clean is a prompt on which the behaviour occurs, and x patch is a reference prompt on which the behaviour does not occur. Then, we can quantify the importance of components using a target metric L : R V → R, e.g., the difference in log-probabilities between a correct token continuation and minimally differing incorrect token continuation. In other words, we quantify the importance of a component on the model behaviour as the expected change in L when replacing the components's activation on the clean prompt with its value on the patch prompt. This is known as the indirect effect (IE; [Pearl, 2001)](#b42).

An exact but compute-intensive method for causal analysis is activation patching [(Meng et al., 2022)](#b37), where we observe the change in model behaviour when performing a counterfactual intervention to a component. However, this requires a sepa-rate forward pass for each component. Therefore, we use attribution patching (AtP; [Kramar et al., 2024)](#b31) to estimate ÎE, a linear approximation of the IE computed as a first-order Taylor expansion:

$ÎE atp (m; a; x clean , x patch ) = ∇ a m| a=a clean a patch -a clean (2)$Here, m is the target metric, usually defined as the logit difference between a correct and minimally differing incorrect token completion given context x clean , a clean is the activation of component a given x clean , and x patch is a minimally differing context that changes the correct answer. Thus, the intuition is that we can approximate the causal contribution of a to m by taking the slope of m with respect to a (where this is approximated as the gradient of m at a[foot_4](#foot_4) ) and multiplying this by the change in activations in a.

AtP requires two forward passes and one backward pass to compute an estimate score for all components in parallel. In practice, we employ a more expensive but more accurate approximation based on integrated gradients [(Sundararajan et al., 2017)](#b55). For details, please see App. C. Probing classifiers. Linear probes have frequently been used to locate representations of morphosyntactic features (e.g., [Hewitt and Manning, 2019;](#b25)[Giulianelli et al., 2018;](#b21)[Chi et al., 2020)](#b7). Given labeled classification data, we may train logistic regressions to map from the activations of frozen intermediate layers of pre-trained models [(Belinkov, 2022)](#b37) to the task labels. We similarly train classifiers to predict morphosyntactic concepts. We then quantify the extent of crosslingual feature sharing for morphosyntactic concepts, and later evaluate the selectivity of targeted morphosyntactic concept interventions.

## Multilingual Features

In this section, we investigate the extent to which morphosyntactic concepts are shared (or redundantly encoded) across languages. We first measure the overlap between the most causally influential sparse features for a given concept across languages. Then, we investigate whether ablating the multilingual features leads to a consistent decrease in classifier performance across languages. 

## Experimental Setup

Models. We consider Llama-3-8B [(Dubey et al., 2024)](#) and Aya-23-8B [(Aryabumi et al., 2024)](#b1). While Llama 3 was trained primarily on English data, Aya was explicitly developed to support 23 typologically diverse and resource-diverse languages.

Data. We select 23 languages from Universal Dependencies 2.1 (UD; Nivre et al., 2017), a multilingual treebank containing dependency-parsed sentences. This corresponds to the 23 languages that Aya-23 was trained on (see Appendix B). Each word in each sentence in UD is annotated with its part of speech and morphosyntactic features, as defined in the UniMorph schema [(Kirov et al., 2018)](#b30).

Training probing classifiers. We train a probing classifier for each combination of morphosyntactic concept (e.g., gender) and language (e.g., English). We filter for morphosyntactic concepts that are annotated in at least six languages. During training, the inputs to the classifiers are residual stream activations from the end of the model's middle layer (L = 16), as this layer is expected to capture abstract, high-level and human-interpretable features [(Elhage et al., 2022a;](#)[Lad et al., 2024)](#b32). Following [Marks et al. (2024)](#), we pool non-padding token activations by summing them; then, we fit a logistic regression to the pooled representations.

Training sparse autoencoders. We train Gated SAEs [(Rajamanoharan et al., 2024)](#b48) for both Llama-3-8B and Aya-23-8B. We collect activations over 250 million tokens of The Pile [(Gao et al., 2020)](#b20) extracted from the residual stream at the same layer at which we train the probing classifiers. To verify the quality of the SAEs, we measure the proportion of loss recovered when replacing the model activa-tions with SAE reconstructions thereof. While the training dataset is primarily composed of English texts, we verify that it also reconstructs other language effectively using the mC4 dataset [(Xue et al., 2021;](#)[](#) see App. D).

## Results

Representations of morphosynthactic concepts are shared across languages. First, we measure the extent of overlap across languages for the top features encoding a morphosyntactic category. For this, we perform attribution patching ( §2), where the target metric is the logit of the probing classifiers we trained in §2. This identifies the set of causally relevant features that are most influential on the probe's logits. We observe that a small fraction of features dominate the causal effect, with a long tail of features that have a small effect. Therefore, for each language and concept, we select the top-32 most informative features per probe and compute the overlap across languages within a concept. We present the results in Figure [2](#fig_0).

We observe significant overlap of up to 50 % in the features identified for the same grammatical concepts across typologically diverse languages. For example, feature 22860 is among the most influential features across all 15 languages that inflect for grammatical gender. The overlap is greater for morphosyntactic concepts present in a larger number of languages, such as grammatical number. We also find that the extent of overlap is largely consistent across Llama-3-8B and Aya-23-8B, with only a small number of exceptions (e.g. negative polarity). Thus, despite the lack of one-to-one mapping of grammatical concepts across languages, there exist highly multilingual representations of abstract morphosyntactic concepts.  Effect of causal interventions on multilingual features are consistent across languages. How important are multilingual features compared to monolingual features? We evaluate the importance of multilingual features by measuring the impact of ablating the multilingual features on the performance of the probing classifiers. We define multilingual features as those in the top feature set across at least two languages. The results (Figure [4](#)) suggest that the classifiers crucially rely on multilingual features to predict the presence of morphosyntactic concepts. This suggests that large language models-including those trained primarily on English-learn to rely on shared representations to detect particular concepts, rather than relying on language-specific representations.

We also observe that while many features are shared across at least two languages, there is a small set of features shared across a large number of languages (App. E Therefore, we investigate ablating only massively multilingual features. Specifically, we order features by the number of languages for which they are causally relevant, and the aver-age strength of their causal effect. Then, we select the upper quartile of features in this ranking-i.e., the most multilingual features-and only ablate those. For Llama 3, the performance after ablating only massively multilingual features drops to 67% on average. Notably, when ablating all multilingual features, the performance drops to 61%a difference of only 6% percent, despite ablating four times as many features. This suggests that massively multilingual features explain most of the probes' behavior.

Feature overlap across concepts. We now investigate the extent of feature overlap across grammatical concepts. This provides insights into the specificity of the functional roles of features. Specifically, we analyze the overlap in the sets of multilingual features associated with different conceptvalue pairs (e.g., masculine gender and singular number). [6](#foot_5) For each pair, we measure the fraction of multilingual features shared, and compute the mean overlap across all pairs. We find that the mean overlap across concepts is 13.9% (± 10.7%). This relatively low (but significant) overlap is expected, as some linguistic concepts are jointly morphologically realized. For instance, singular number and masculine gender share 40.6% of their features; this is intuitive, given the frequent joint inflection of adjectives and pronouns for both number and gender. For example, in French and Hebrew, "il" and ‫"הוא"‬ are masculine singular pronouns, where changing either number or gender alters the form. Similarly, adjectives in these languages have distinct forms for combinations of grammatical gender and number.

Multilingual features are human-interpretable. One benefit of sparse features over neurons is their interpretability. This provides a qualitative way to verify that the features are truly relevant to the target concept, rather than related solely in some spurious correlational manner. Thus, we manually inspect a subset of the identified features, finding that many of them are intuitively meaningful and interpretable across different languages. We present the most multilingual features for two common concepts and a selection of their activation patterns across typologically diverse languages in Figure [3](#).

## Demonstrating Functional Selectivity: A Case Study in Machine Translation

Another benefit of sparse features is the level of fine-grained control they provide over language model behaviors [(Templeton et al., 2024)](#b57). Indeed, feature-based interventions tend to be more effective then neuron-based methods at precisely modifying how language models generalize without destroying task performance [(Marks et al., 2024)](#).

We leverage this property in a machine translation setting to (i) demonstrate the functional selectivity of the discovered morphosyntactic features (i.e., to show that intervening with them has few side effects on other concepts), and (ii) provide additional evidence that their causal role is highly general (i.e., that it is effective in this MT setting and not just on the probes). To evaluate (i) and (ii), we use feature steering [(Templeton et al., 2024)](#b57), where we clamp features of interest to counterfactual activations. These activations can be either significantly higher or lower than the values observed in practice. This approach is similar to prior work on understanding the functional role of individual neurons in image models [(Bau et al., 2019)](#b2).

As translation is a difficult task that does not always have a single clear answer, we restrict to a smaller subsample of concepts and languages such that we can manually verify the baseline quality of translations, and then the efficacy of the interventions. We restrict our analysis to tense (Past vs. Present), gender (Masc. vs. Fem.), and number (Sing. vs. Plural), as these concepts are shared across most languages we consider, and narrow to three languages: English, German, and French. We provide details on the prompt format and translation performance in Appendix F.

## Experimental Setup

Data. We design datasets consisting of minimal pairs of inputs that differ only with respect to the presence of a grammatical concept (see Table [3](#) in App. G). For example, we generate counterfactual pairs that elicit singular or plural verbs based on the grammatical number of the subject: a. The parents near the cars → were b. The parent near the cars → was This is an adaptation and translation of data from [Arora et al. (2024)](#b0). Then, we check whether the probability of the grammatical sequence is higher for each pair. In our experiments, we only consider sentences where Llama-3-8B is indeed capable of making grammatically correct predictions.

Feature intervention. In our setting, we follow [Templeton et al. (2024)](#b57) and intervene in the model by scaling a single SAE feature to a multiple of its maximum activation value observed in the dataset. [7](#foot_6) More formally, for a given grammatical concept, we select one feature from the set of multilingual features identified in §3 and scale it to a multiple of the observed activation value. We implement feature steering by decomposing the residual stream activations x into the sum of two components, the SAE reconstruction and the reconstruction error (see Equation [1](#)). Then, we replace the SAE term with a modified SAE reconstruction in which we set the activation value of a specific feature in the SAE to a counterfactual value, but leave the error term unchanged. We then run the forward pass on the network using this modified residual stream. We only apply the intervention to the last token position of each generation step to prevent degenerate outputs. Thus, for a given morphosyntactic concept, similar to [Templeton et al. (2024)](#b57), we explore different multiples and choose the one that works best empirically.

We consider activating features in contexts where they would not be active otherwise. We observe that intervening on multiple features at the same time is very sensitive to the combination of features and their activation value. Therefore, we follow [Templeton et al. (2024)](#b57) in intervening on only a single feature at a time. To select the feature, we manually inspect the features and select one feature per concept where the activation values are most interpretable. En → Fr Fr → En En → De De → En De → Fr Fr → De 0 0.2 0.4 0.6 0.8 1 Efficacy Gender Masc. Number Sing. Tense Past Figure [5](#): Efficacy in flipping the model behavior on our counterfactual dataset when translating between languages and intervening on a single multilingual feature per concept. For each concept, we translate a sentence from some source language (e.g. German) where some concept (e.g. present tense) to another language (e.g. English) and measure the number of times the model starts to predict some alternative concept value (e.g. past tense). In each of these settings, we intervene on a single feature and measure the success rate over 64 examples.

## English

German French 0 0.5 1 Selectivity Gender Masc. Number Sing. Tense Past Figure [6](#): Fraction of times an intervention on a grammatical concept exclusively impacts that concept, indicating the degree of cross-concept interference. We observe that interference is generally very rare.

## Quantitative Results

Efficacy. We define the efficacy of the intervention as the proportion of examples where the probe's label flips after the intervention. To measure this, we generate a translation with and without the intervention. Then, in new forward passes with no interventions, we give the model these translations, pool their representations, and measure whether the probe's (from §2) labels differ between the two translations. We find that intervening on a single feature is often sufficient to change the models prediction towards the indended concept (e.g., a masculine instead of feminine pronoun; see Figure [5](#)). However, we also observe that, for some concepts, intervening on a single feature is often insufficient.

Selectivity. We also evaluate the selectivity of the interventions. Specifically, we want interventions to only flip the labels on the concept that we intervene on, and not others. We operationalise this using the probing classifiers trained in §2. As before, we generate two translations: one with and without feature interventions to a given concept.

Then, we compute selectivity as the proportion of translation pairs for which none of the other concept probes changed their predictions. Our results (Figure [6](#)) suggest that the interventions are generally selective to the concept.

## Qualitative Results

The quantitative results provide insights into the generalisation of the identified features to a different distribution, and allow us to quantify the success rate of the interventions. However, the dataset is neither naturalistic nor diverse. Therefore, we perform the same interventions on the Flores-101 dataset, which provides aligned translations across 101 languages [(Goyal et al., 2021)](#b23). We present a selection of qualitative examples in Figure [7](#fig_3).

However, we acknowledge that the intervention was not successful in some cases, and sometimes led to degenerate generations. Concept-specific tuning over the number of features and activation values used was also required. These limitations highlight common problems with feature steering methods: activations often must be scaled to significantly larger values than would naturally be observed in practice [(Templeton et al., 2024)](#b57). This, combined with intervening on multiple features simultaeously, can result in unexpected or degenerate outputs [(Shabalin et al., 2023)](#b50).

## Related Work

Multilingual language modeling. Multilingual language models are trained on multiple languages simultaneously, typically with an explicitly designed balance between languages in the pretraining corpus. For example, XLM-R [(Conneau et al., 2020)](#b8), M2M [(Mohammadshahi et al., 2022)](#b38), Turkish Televizyon haberleri santralden yükselen beyaz dumanı gösteriyor.

## ↑ Past Tense

## English

The TV news is showing the white smoke rising from the chimney of the plant.

A television news program showed a cloud of white smoke from the reactor.

## English

He was greeted by Singapore's Deputy Prime Minister Wong Kan Seng and discussed trade and terrorism issues with the Singapore Prime Minister Lee Hsien Loong.

## ↑ Feminine

## German

Er wurde von Wong Kan Seng, dem stellvertret enden Premierminister Singapurs, begrüßt und sprach mit Lee Hsien Loong, dem Premierminister Singapurs, über Handels-und Terrorismusangelegenheiten.

Sie wurde von der stellvertretenden Premierministerin Wong Kan Seng in Empfang genommen und unterhielt sich mit der singapurischen Premierministerin Lee Hsien Loong über Handelsund Terrorismus-Themen.

## French

Peu de gens les considèrent comme des dinosaures étant donne qu'ils ont des ailes et peuvent voler.

## ↑ Singular

## Spanish

Pocos los consideran dinosaurios porque tienen alas y pueden volar.

Pocos lo consideran como un dinosaurio, ya que tiene alas y puede volar. mT5 [(Xue et al., 2021)](#), and Aya [(Aryabumi et al., 2024)](#b1) are trained with corpora balanced across many languages. In contrast, Llama 3 [(Dubey et al., 2024)](#) is trained on a much more English-centric distribution: over 90% of the corpus is English text. Generally, the motivation behind balanced corpora during multilingual pretraining is to encourage cross-lingual transfer. We find that this balance may not be necessary for multilingual representations when the corpora are sufficiently large.

Interpreting multilingual language models. A central question in multilingual language modeling is whether language models develop a universal concept representation, disentangled from specific languages. [Huang et al. (2023)](#b27) studied different forms of cross-lingual transfer using a variety of models and probing tasks. [Wendler et al. (2024)](#b58) found that languages models trained on unbalanced, English-dominated corpora operate in a concept space that lies closer to English than to other languages. [Ferrando (2024)](#b18) studies the specific mechanism of subject-verb agreement and finds that these mechanisms are consistent across languages. [Dumas et al. (2024)](#b13) observed that concepts (e.g. "car") are encoded disentangled from specific languages and can be transferred between them. Similarly, [Stanczak et al. (2022)](#b53) found that probes trained on different languages read the same concepts from the same neurons in smaller-scale masked language models such as mBERT [(Devlin et al., 2019)](#) and XLM-R [(Conneau et al., 2020)](#b8), while [Feng et al. (2024)](#b17) observed that their propositional probes generalise to Spanish. De Varda and Marelli (2023) extended this study by investigating the cross-lingual consistency of individual neurons responding to syntactic phenomena. Our work extends these findings to large-scale models not explicitly intended to be multilingual, and shows stronger causal evidence for the functional selectivity of these conceptual representations.

Language models trained primarily on English data perform surprisingly well in other languages. Prior work has emphasized the importance of balanced pretraining corpora, so as to not overfit to a single language [(Conneau et al., 2020;](#b8)[Mohammadshahi et al., 2022)](#b38); however, more recent studies suggest that the size of the pretraining corpusnot balance-may be more important for crosslingual generalization [(Jiang et al., 2023;](#)[Dubey et al., 2024;](#)[Schäfer et al., 2024)](#). Why is this effective? Our results provide evidence that largescale pretraining, even with imbalanced corpora, induces equally cross-lingualistcally generalizable grammatical abstracts as pretraining with balanced corpora. This is evidence in favor of a hypothesis discussed in [Wendler et al. (2024)](#b58): the internal lingua franca of large language models may not be English words per se, but rather concepts. That said, it is still reasonable to assume these concepts are likely biased toward how English handles them in models pre-trained on imbalanced corpora.

How generalizable should grammatical concept representations be? While it is more parameterefficient to share representations across languages, this may lead to biases if two languages assign different social or semantic connotations to-or simply distribute differently-the same grammatical concepts. Indeed, there is not a one-to-one relationship between concepts across languages: different languages may have differing numbers of values for the same concept (e.g., Finnish has many more grammatical cases than German), and may use the same categories in different ways (e.g., the same nouns often have different genders in different languages, even within language families). More language-specific features could lead to less bias, but would require more parameters to represent. It is not clear what the optimal point between these two ends of generalization are.

More practically, the extent to which concepts are shared across languages has implications for multilingual downstream NLP tasks. For example, it helps explain the phenomenon that preference tuning for toxicity mitigation in a single language generalizes to other languages [(Li et al., 2024)](#b33). Thus, we might reasonably expect various types of interventions and model editing approaches to generalize from a single language to other languages.

## Limitations

Non-linear features. Sparse autoencoders are a method for unsupervised search for features. However, they will only recover a feature if it is linearly encoded. Thus, if a concept is encoded in a nonlinear manner, then sparse features may struggle to recover the full range of that variable. Thus, sparse features are generally best-suited to encoding binary relations, whereas some features may be encoded in more complex arrangements such as circular shapes (cf. [Engels et al. 2024)](#b16).

Feature interventions. We use SAE features to intervene on the models internal state. Similar to [Templeton et al. (2024)](#b57), we find that this typically requires clamping the feature activations to values outside their observed range over the training dataset. However, clamping feature activations to extreme values often causes the model to produce nonsensical generations, e.g., repeating the same token indefinitely. Thus, while our results demonstrate that the identified features can be causally responsible for model behavior, optimally intervening on groups of features, is an open problem that future work should address.

Distinction between model understanding and generation. In our experiments, the linear probes measure whether it is possible to linearly separate grammatical concepts in the model's representations, whereas the machine translation experiments measure the effect of feature directions on model generation. Past work [(Meng et al., 2022;](#b37)[Orgad et al., 2024;](#)[Gottesman and Geva, 2024)](#b22) has observed a distinction between a model encoding a concept versus being able to use that concept during generation (i.e., a "knowing vs. saying" distinction). Anecdotally, we observed that some features with a high causal effect on the linear probe do not necessarily have a similar effect on the model's generation. We hypothesize that this is due to an intrinsic distinction between concept understanding/representation and concept generation.

Human biases in feature interpretation. One must be cautious in applying human concepts to language model representations. It is probable that language models deploy distinct concept spaces from humans; thus, human explanations of neurons or features are likely to be biased in ways that can result in suboptimal predictions of when the neuron or feature will activate (cf. [Huang et al., 2023)](#b27).

where f (x) is the vector of feature activations, and x is the reconstruction. This model is trained using the following loss term:

$L standard = E x∼D train ∥x -x∥ 2 + λ∥f (x)∥ 1 . (5)$for some hyperparameter λ > 0 controlling sparsity.

The L 1 penalty introduces biases that can harm the accuracy of the reconstruction, as the loss can be decreased by trading-off reconstruction for lower L1 [(Wright and Sharkey, 2024)](#b59). To address this, [Rajamanoharan et al. (2024)](#b48) introduced a architectural modification that separates (i) the selection of dictionary elements to use in a reconstruction, and (ii) estimating the coefficients of these elements. This results in the following gated architecture:

$π gate (x) := W gate (x -b d ) + b gate f (x) := I π gate (x) > 0 ⊙ ReLU(W mag (x -b d ) + b mag ) x( f (x)) = W d f (x) + b d$where I[• > 0] is the Heaviside step function and ⊙ denotes elementwise multiplication. Then, the loss function uses xfrozen , a frozen copy of the decoder:

$L gated := E x∼D train ∥x -x( f (x))∥ 2 2 + λ∥ReLU(π gate (x))∥ 1 (6) + ∥x -xfrozen (ReLU(π gate (x)))∥ 2 2 D.$2 Training Parameters Parameter Value Optimiser Adam Learning Rate 0.001 L1 Coefficient 0.505 Expansion Factor 8 Number of Token 250 Million Batch Size 512 Warmup Steps 1, 000

Table 1: Training parameters of our sparse autoencoders for both Llama-3-8B and Aya-23-8B.

## D.3 Performance Across Languages

We evaluate the performance of the sparse autoencoder trained on Llama-3-8B using the multilingual C4 (mC4) dataset [(Xue et al., 2021)](#), itself a multilingual extension of the C4 dataset [(Raffel et al., 2020)](#b46). Table [2](#tab_3) presents the loss recovered when replacing the residual stream activations with the sparse autoencoder reconstructions across a range of languages. For this purpose, we compute the loss recovered as

$L recovered = L reconstructed -L zero L original -L zero (7)$where L original is the cross entropy (CE) loss of the language model without intervention, L zero is the CE loss of the model when zero ablating the activations, and L reconstructed is the loss when replacing the activations with the sparse autoencoder reconstructions. This is a standard extrinsic metric to measure autoencoder quality [(Mueller et al., 2024)](#b39). However, it does not measure the intrinsic interpretability of the SAE features, which is not possible to measure in the absence of ground truth features [(Karvonen et al., 2024;](#b29)[Makelov et al., 2024)](#b34). 

## E Features Across Languages

We measured the number of languages for which the top-32 features for each concept were strongly influential. Across grammatical concepts, the number of languages that a feature is influential for roughly follows a power law distribution. Below, we present the distributions for a selection of common concepts (see [Figures 8,](#)[9,](#)[10)](#). We find that for most concepts, there are multiple features that are shared across nearly all languages. Note that the maximum number of languages varies across concepts, as not all languages inflect for a given concept nor are annotated for it in Universal Dependencies.     

## F Machine Translation

## F.1 Prompt Format

The following is an example of our 2-shot translation prompt:    

## F.2 Translation Performance

We evaluate the ability of Llama-3-8B and Aya-23-8B to translate languages in-context using the Flores-101 dataset [(Goyal et al., 2021)](#b23). Specifically, we measure the BLEU score [(Papineni et al., 2002)](#b41) using the SacreBLEU implementation [(Post, 2018)](#b45). We prompt models with 2 ground-truth exemplar translations, and then give it the test sentence in the source language (see Figure [14](#fig_13)). Results are presented in Figure [15](#fig_14) and 16. The governor's office said nineteen of the injured were police officers. \\ Das Gouverneursamt erklärte, dass von den Verletzten neun Polizisten waren.

He produced over 1,000 stamps for Sweden and 28 other countries. \\ Er produzierte über 1.000 Briefmarken für Schweden und 28 weitere Länder.

The truck driver, who is aged 64, was not injured in the crash. \\   

## G Counterfactual Dataset

Our counterfactual dataset considers three concepts (gender, number, tense) and three languages (English, French, German). The dataset format is inspired by CausalGym [(Arora et al., 2024)](#b0) and the templates are in part taken from that dataset. We present example counterfactuals in Table [3](#).

![Figure 2: Proportion of features shared across languages (intersection over union) among the top 32 features for each morphosyntactic concept. A significant fraction of the morphosyntactic concept representations are shared across languages in both Llama-3-8B and Aya-23-8B.]()

![Plural NounsThe teacher helped the students Die Frauen haben Taiteilij at ovat kokoontuneet Lelaki-lel aki itu juga diekspor ke luar negeri #10710 Past Tense When Mary was in the bed , she Als Sam im Park war , Kun John oli koul ussa parc , hän Toen James op het strand was , hij]()

![Figure3: Examples of the activation patterns of selected features that correspond to cross-lingual representations of grammatical concepts. For example, we locate features that indicate the presence of plural nouns across languages or features that indicate past tense.]()

![Figure 7: Comparison of translations with (highlighted in red) and without intervening on multilingual features encoding a morphosyntactic concept. The words indicating that the model flipped its behavior are highlighted in bold. The input texts are sampled from the Flores-101 dataset.]()

![Figure 8: Distribution of the number of languages across which a given feature associated with masculine gender is shared (Llama 3).]()

![Figure 9: Distribution of the number of languages across which a given feature associated with past tense is shared (Llama 3).]()

![Figure 10: Distribution of the number of languages across which a given feature associated with accusative case is shared (Llama 3).]()

![Figure 11: Distribution of the number of languages across which a given feature associated with masculine gender is shared (Aya-23).]()

![Figure 12: Distribution of the number of languages across which a given feature associated with past tense is shared (Aya-23).]()

![Figure13: Distribution of the number of languages across which a given feature associated with accusative case is shared (Aya-23).]()

![Figure 14: Example 2-shot translation prompt between English and German. Note that the double backslashes are literals, not newlines. We found this prompt format to work best empirically in 2-shot settings in initial experiments.]()

![Figure 15: SacreBLEU scores of Llama-3-8B.]()

![Figure 16: SacreBLEU scores of Aya-23-8B.]()

![Evaluation of the sparse autoencoder trained on Llama-3-8B across different subsets on the AllenAI C4 dataset. The table shows the fraction of loss recovered when replacing residual stream activations with sparse autoencoders reconstructions across nine languages.]()

For real languages, we expect that features will be shared across languages to a much greater extent.

Though we note that these concepts are likely to be biased toward English-like representations, as discussed in[Wendler et al. (2024)](#b58).

We use "neuron" to refer to a single dimension of any hidden representation vector in a model.

As described in detail inMarks et al. (2024).

The gradient can be viewed as a local estimate of how much the metric we backpropagate from would be affected by changing the activation of the component.

We use concept to refer to the variable-e.g., gender or number-and concept-value to refer to specific values that the variable can take-e.g., masculine gender or singular number.

For this purpose, we record the maximum activations of features across 20 M tokens.

