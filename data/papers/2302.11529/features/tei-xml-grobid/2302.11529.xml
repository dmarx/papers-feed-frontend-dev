<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modular Deep Learning</title>
				<funder ref="#_5RfsakN">
					<orgName type="full">Royal Society</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-27">27 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
							<email>jonaspfeiffer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<email>ruder@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edoardo</forename><forename type="middle">M</forename><surname>Ponti</surname></persName>
							<email>eponti@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modular Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-27">27 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">AED219C9B610C0DF9BF4430EA904438F</idno>
					<idno type="arXiv">arXiv:2302.11529v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning has recently become the dominant paradigm of machine learning. Pretrained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table of Contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>Transfer learning has recently become pervasive in machine learning technology, such as in natural language processing <ref type="bibr">(Ruder et al., 2019b;</ref><ref type="bibr" target="#b38">Brown et al., 2020)</ref>, computer vision <ref type="bibr" target="#b73">(Dosovitskiy et al., 2021)</ref>, and reinforcement learning <ref type="bibr">(Reed et al., 2022)</ref>, among other areas. In its most successful incarnation, transfer learning consists of pre-training a model on vast amounts of raw data in a self-supervised fashion and subsequently fine-tuning it for new tasks based on a small number of labelled examples. Despite its success, this paradigm for transfer learning suffers from a series of limitations in various settings. Firstly, in multi-task fine-tuning, the learning signals from different tasks may negatively interfere with each other <ref type="bibr" target="#b203">(McCloskey &amp; Cohen, 1989)</ref>. Similarly, in continuous learning, adapting to new examples can result in catastrophic forgetting of knowledge acquired from previous examples <ref type="bibr" target="#b294">(Sutton, 1986;</ref><ref type="bibr" target="#b91">French, 1999)</ref>. <ref type="foot" target="#foot_0">1</ref> Secondly, in settings where the training and evaluation distributions are not identical, these models fail in generalising systematically <ref type="bibr" target="#b161">(Lake &amp; Baroni, 2018;</ref><ref type="bibr" target="#b136">Hupkes et al., 2020)</ref>. This makes models brittle and inaccurate and hampers their deployment in real-world applications, where distribution shifts are common.</p><p>In contrast, many biological and artificial systems do not suffer from these weaknesses by virtue of their modularity <ref type="bibr" target="#b86">(Fodor, 1983;</ref><ref type="bibr" target="#b23">Ballard, 1986)</ref>, defined as the correspondence between strongly interconnected components of a system (i.e., modules) and the functions they perform <ref type="bibr" target="#b22">(Baldwin &amp; Clark, 2000;</ref><ref type="bibr" target="#b301">Ulrich, 1995)</ref>. In other words, each module is specialised for a unique purpose, for which it is reused consistently.</p><p>In animal brains, this favours evolvability, the ability to adapt quickly to new environments, and resilience to environment perturbations <ref type="bibr" target="#b317">(Wagner et al., 2005)</ref> because it makes rewiring connections easier than in monolithic, entangled networks <ref type="bibr" target="#b148">(Kashtan &amp; Alon, 2005)</ref>. Artificial systems, such as programming languages and computer hardware, are similarly designed in a modular fashion <ref type="bibr" target="#b35">(Booch et al., 2008;</ref><ref type="bibr" target="#b22">Baldwin &amp; Clark, 2000)</ref> because this modular design favours consistency, ease of adaptation, and interpretability.</p><p>To what extent, then, do 'vanilla' neural networks display the desirable property of being modular? In principle, given their fully connected nature, they could develop such a structure as a by-product of optimising a loss for a downstream task. Recent structural analyses based on hierarchical clustering of neurons revealed that vanilla neural networks can indeed learn such a modular pattern <ref type="bibr" target="#b324">(Watanabe, 2019;</ref><ref type="bibr" target="#b44">Casper et al., 2022;</ref><ref type="bibr" target="#b87">Foroutan et al., 2022)</ref>. Favourable conditions for the emergence of modularity include multi-task learning <ref type="bibr" target="#b70">(Dobs et al., 2022)</ref> and regularisation through dropout <ref type="bibr" target="#b163">(Lange et al., 2022)</ref>. In particular, from a structural perspective, populations of neurons may activate jointly in response to specific features of the input or the output classes<ref type="foot" target="#foot_1">foot_1</ref> , resulting in similar changes in model performance when ablated <ref type="bibr" target="#b206">(Meyes et al., 2020)</ref>. From a functional perspective, multi-task learning may lead to segregated, specialised sub-networks <ref type="bibr" target="#b338">(Yang et al., 2019;</ref><ref type="bibr" target="#b70">Dobs et al., 2022)</ref>. On the other hand, <ref type="bibr" target="#b62">Csordás et al. (2021)</ref> revealed that a given sub-network does not tend to be re-used for similar sub-tasks nor to be combined with others to express more complex functions. In fact, in many cases, the performance of a model on simple tasks requiring a certain skill and composite tasks requiring a combination thereof is entirely uncorrelated <ref type="bibr">(Li et al., 2022a)</ref>.</p><p>For this reason, previous work explored the idea of designing neural networks that are explicitly modular <ref type="bibr">(Jacobs et al., 1991a;</ref><ref type="bibr" target="#b256">Rosenbaum et al., 2018;</ref><ref type="bibr">Ponti, 2021;</ref><ref type="bibr">Mittal et al., 2022)</ref>. This has the goal of achieving not only functional specialisation <ref type="bibr">(Zhang et al., 2022b)</ref>, but also re-usability and composability. In particular, these methods involve identifying 1) modules in a neural network that can be updated locally and asynchronously, without affecting the rest of the parameters; 2) a routing function that chooses a subset of modules for each example or task; and 3) an aggregation function that aggregates the outputs of the active modules. Each of these three ingredients can be manually specified or learned. We provide several case studies of different configurations of these components in Figure <ref type="figure">1</ref>.</p><p>The main advantages of modular neural architectures are positive transfer, compositionality, and parameter efficiency. Firstly, modularity encourages positive transfer by encoding similar functions with the same module. At the same time, it prevents interference and forgetting by allocating distinct functions to different dedicated modules <ref type="bibr">(Jacobs et al., 1991b)</ref>. For instance, massively multilingual Transformer-based models in NLP are 1a) MAD-X <ref type="bibr">(Pfeiffer et al., 2020b)</ref> uses Adapter layers with fixed routing for zero-shot cross-lingual transfer. 1b) Polytropon <ref type="bibr">(Ponti et al., 2022)</ref> uses low-rank adapters (LoRA; <ref type="bibr" target="#b132">Hu et al., 2022)</ref> with hard learned routing for few-shot task adaptation. 1c) MoE Transformers <ref type="bibr" target="#b82">(Fedus et al., 2021;</ref><ref type="bibr">Clark et al., 2022, inter alia)</ref> use Multi-Layer Perceptrons with top-k soft routing, in order to scale to larger model sizes. The three representative models illustrated here are only a fraction of possible configurations from the 'configuration manifold' that can be created by varying the components surveyed in §3- §6.</p><p>known to suffer from a 'curse of multilinguality' <ref type="bibr" target="#b58">(Conneau et al., 2020)</ref> due to the conflicting information that the gradient from each language-specific loss carries <ref type="bibr">(Wang et al., 2021b)</ref>. A possible solution is augmenting these entangled, fully shared models with specialised modules responsible for individual languages <ref type="bibr">(Pfeiffer et al., 2020b;</ref><ref type="bibr">2022b)</ref>. More generally, as the range of tasks modelled jointly by a single model becomes increasingly diverse, modularity may be instrumental in the advent of general-purpose, multi-modal agents that encompass vision, language, and action <ref type="bibr">(Reed et al., 2022)</ref>.</p><p>Secondly, modules representing different skills (at the task level) or features (at the example level) can be composed together and updated locally, without affecting the rest of the network. These two properties are crucial in two main settings, which correspond to different aspects of systematic generalisation: one is the ability to re-compose, i.e. zero-shot transfer to tasks consisting of new combinations of learned skills, or examples consisting of new combinations of observed features <ref type="bibr" target="#b136">(Hupkes et al., 2020)</ref>. For instance, while modules for the Guaraní language and for dependency parsing can only be trained separately due to the lack of annotated data for dependency parsing in Guaraní, they can be composed to perform inference on this unobserved task-language combination <ref type="bibr">(Pfeiffer et al., 2020b)</ref>. Similarly, in hierarchical reinforcement learning, an agent can follow different sequences of modular policies known as options in tasks requiring the completion of similar sub-goals in different orders <ref type="bibr" target="#b295">(Sutton et al., 1999;</ref><ref type="bibr" target="#b245">Precup, 2000)</ref>. The other aspect of systematic generalisation is robustness. In fact, if modules are taken to correspond to independent and reusable physical mechanisms <ref type="bibr" target="#b273">(Schölkopf et al., 2012)</ref>, local shifts in their distributions require updating only the parameters accounting for the affected skills or features <ref type="bibr" target="#b102">(Goyal et al., 2021;</ref><ref type="bibr" target="#b274">Schölkopf et al., 2021)</ref>, while the rest of the model remains invariant to the change. In practice, the ability to perform local updates facilitates sample efficiency, as fewer examples are necessary to adapt models to new tasks <ref type="bibr" target="#b29">(Bengio et al., 2020;</ref><ref type="bibr">Ponti et al., 2022)</ref>.</p><p>Thirdly, an additional advantage of modular neural architectures is parameter and time efficiency. In this framework, fine-tuning a model on a specific task only requires storing a modular adapter rather than a separate copy of the entire (typically large) model. What is more, modules can be added or removed on-the-fly in an incremental manner, adjusting the model capacity according to the task complexity. This ability is known as conditional computation <ref type="bibr" target="#b27">(Bengio et al., 2015)</ref>. Finally, modularity enables language models to scale to larger numbers of parameters while retaining the same time complexity, by selecting only a small set of experts per example <ref type="bibr" target="#b277">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b82">Fedus et al., 2021)</ref>.</p><p>As the main contribution of this survey, we offer a unified view of modular deep learning, illustrating how many families of methods can be defined along four key dimensions: 1) how they implement modules, which constitute the minimum unit of computation; 2) how they select active modules through a routing function;</p><p>3) how module outputs are aggregated; and 4) how the modules are trained with the rest of the model.</p><p>For module implementation, we discuss sparse subnetworks <ref type="bibr" target="#b132">(Hu et al., 2022;</ref><ref type="bibr" target="#b11">Ansell et al., 2022)</ref>, adapter layers <ref type="bibr" target="#b251">(Rebuffi et al., 2018;</ref><ref type="bibr">Pfeiffer et al., 2020b)</ref>, and prefix tuning <ref type="bibr">(Li &amp; Liang, 2021)</ref>, among others. These methods have been proven as an effective way to adapt large pre-trained models, achieving better performance and sample efficiency than alternative strategies such as in-context learning <ref type="bibr">(Liu et al., 2022b)</ref>, which may be brittle <ref type="bibr" target="#b192">(Lu et al., 2022)</ref>. In fact, modules can also take the form of human-engineered prompts, where the model is provided with input-output examples <ref type="bibr" target="#b38">(Brown et al., 2020)</ref> or task instructions <ref type="bibr">(Wei et al., 2022a)</ref>. While many module implementations share the same underlying functional form <ref type="bibr" target="#b124">(He et al., 2021)</ref>, they offer different trade-offs between efficiency and performance.</p><p>We then discuss how routing functions control the flow of information to the modules: in fixed routing, module allocation is manually defined when expert knowledge is available <ref type="bibr" target="#b117">(Hampshire &amp; Waibel, 1992;</ref><ref type="bibr">Rajendran et al., 2017, inter alia)</ref>. In learned routing, a parameterised routing function is inferred during training. This, however, poses a series of challenges, such as training instability, module collapse, and overfitting <ref type="bibr" target="#b257">(Rosenbaum et al., 2019)</ref>. Orthogonally, we also distinguish between hard and soft routing. In hard routing, only a subset of modules is activated <ref type="bibr" target="#b256">(Rosenbaum et al., 2018;</ref><ref type="bibr">Ponti et al., 2022;</ref><ref type="bibr">Fernando et al., 2017, inter alia)</ref>. In soft routing, all modules are aggregated according to continuous scores <ref type="bibr">(Jacobs et al., 1991b;</ref><ref type="bibr" target="#b145">Jordan &amp; Jacobs, 1994)</ref>. While soft routing is amenable to vanilla gradient descent, it is highly inefficient. On the other hand, hard routing requires approximate inference but facilitates conditional computation and module specialisation.</p><p>When multiple modules are selected, several aggregation strategies are possible. For instance, these can be based on interpolating the parameters of active modules <ref type="bibr" target="#b11">(Ansell et al., 2022)</ref> or an attention mechanism over the module outputs <ref type="bibr">(Pfeiffer et al., 2021a)</ref>. Alternative methods include input prompt concatenation <ref type="bibr">(Vu et al., 2022b)</ref> and function composition <ref type="bibr">(Andreas et al., 2016b)</ref>.</p><p>Finally, modules can be trained jointly with the rest of the base model in multi-task learning <ref type="bibr" target="#b42">(Caruana, 1997;</ref><ref type="bibr" target="#b260">Ruder, 2017)</ref>, added sequentially in classic continual learning <ref type="bibr" target="#b264">(Rusu et al., 2016)</ref>, or integrated post-hoc into an already pre-trained and frozen model <ref type="bibr" target="#b250">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b129">Houlsby et al., 2019)</ref>. The last scenario is most common with current state-of-the-art models, which are trained as dense, fully shared models and may be 'modularised' after pre-training.</p><p>Crucially, this taxonomy reveals unexpected connections between several independent threads of research, including aggregation functions and mode connectivity <ref type="bibr" target="#b89">(Frankle et al., 2020)</ref>, routing and hypernetworks <ref type="bibr" target="#b115">(Ha et al., 2017)</ref>, among others.</p><p>We further illustrate a series of applications of modular networks in transfer learning across different areas such as natural language processing, computer vision, and speech processing. In addition, we show how modularity plays an important role in causal inference and discovery, programme simulation, and hierarchical reinforcement learning.</p><p>We hope that our overview will spark future research on modular deep learning in areas that may benefit from it such as community-driven efforts to develop and maintain machine learning technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modular Deep Learning</head><p>This survey focuses on modular deep learning: namely, on models composed of modules. These are autonomous computation functions that, depending on their architecture and purpose, are variously referred to as adapters <ref type="bibr" target="#b250">(Rebuffi et al., 2017;</ref><ref type="bibr">Pfeiffer et al., 2020a)</ref>, options <ref type="bibr" target="#b295">(Sutton et al., 1999;</ref><ref type="bibr" target="#b245">Precup, 2000)</ref>, or experts <ref type="bibr">(Jacobs et al., 1991a;</ref><ref type="bibr" target="#b145">Jordan &amp; Jacobs, 1994)</ref>. Crucially, these modules are distinguished from a routing function, which controls the information flow to the modules. Finally, an aggregation function aggregates their outputs.</p><p>Modules can be optionally combined with fully shared (thus, non-modular) parameters as part of the same neural architecture. In order to provide a unified view of the landscape of modular deep learning, we create a taxonomy of four dimensions of variation: computation, routing, aggregation, and training. These dimensions are mutually independent; hence, many methods can be interpreted as different combinations of these dimensions, listed in § 2.1. Concurrently, we provide a unified, consistent notation in § 2.2, which helps illuminate the relationship among such methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Taxonomy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Computation function:</head><p>How is each module implemented? ( § 3) A module may consist of any component of a neural architecture, such as multiple copies of a model <ref type="bibr">(Jacobs et al., 1991a)</ref> or one of its layers <ref type="bibr" target="#b82">(Fedus et al., 2021)</ref>. Alternatively, as it is common in transfer learning, modules can be combined with a function parameterised by fully shared pre-trained weights. In this case, we distinguish between modification of parameters (parameter composition), concatenation with input features (input composition), and function composition by stacking neural modules.</p><p>2) Routing function: How are active modules selected? ( § 4) Under fixed routing, we categorise approaches where the routing function is fixed. This assumes that the specialisation of each module, as well as the combination of modules required for each task, is known a priori. In learned routing, the parameters of the routing mechanism are learned during training. In this case, routing is soft if all modules are ranked through a continuous score, or hard if each module is given a binary score (active or inactive).</p><p>3) Aggregation function: How are the outputs of the active modules aggregated? <ref type="bibr">( § 5)</ref> We differentiate between methods that compose the outputs of the active modules deterministically (e.g., based on a weighted average) from those where the aggregation function is implemented as a learnable neural network that depends on the output of all modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Training setting:</head><p>How are the modules trained? ( § 6) Some methods, such as MoEs, train the modules (and possibly the routing function) jointly with the shared weights of a randomly initialised model. As an alternative, transfer learning approaches introduce modules post-hoc after pre-training weights and adapt them during fine-tuning. In continuous learning settings, instead, new modules may be introduced iteratively for every new task in a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Notation</head><p>More formally, let a neural network f θ : X → Y be decomposed into a graph of sub-functions. In the simplest case, this graph is a linear chain</p><formula xml:id="formula_0">f θ1 • f θ2 • • • • • f θ l</formula><p>, where • stands for function composition. These sub-functions refer to the model's l layers, each with unique indexed parameters θ i , i = 1, . . . , l. <ref type="foot" target="#foot_2">3</ref> In turn, these can be further decomposed recursively into a graph of their constituent sub-functions: for instance, a Transformer layer <ref type="bibr" target="#b307">(Vaswani et al., 2017)</ref> includes linear mappings for the query, key, value, and output, as well as a non-linear feed-forward network, and residual connections. We further denote the values of the parameters at initialisation as θ 0 , and the parameters after training are denoted as θ ⋆ .</p><p>Any i-th sub-function with input x can be modified by a module with parameters ϕ from the inventory</p><formula xml:id="formula_1">M i = f ϕ1 , . . . , f ϕ |M | in the following different ways: Notation Definition x ∈ X Input data y ∈ Y Output data h ∈ H Hidden representation t ∈ T Task index f : X ∪ H → Y ∪ H A computation function θ Shared parameters M = {ϕ 1 , . . . , ϕ |M | } Set of module parameters α ∈ A</formula><p>Vector of routing scores r : X ∪ H ∪ T → A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing function ρ</head><p>Routing parameters g Aggregation function γ</p><p>Aggregation parameters</p><p>Table <ref type="table">1</ref>: Notation and definition of important variables, functions, and operators.</p><p>1. parameter composition:</p><formula xml:id="formula_2">f ′ i (x) = f θi⊕ϕ (x)</formula><p>, where ⊕ stands for an operation that composes the original parameters with the module parameters, such as element-wise addition. An example is low-rank <ref type="bibr" target="#b132">(Hu et al., 2022)</ref> or sparse <ref type="bibr" target="#b11">(Ansell et al., 2022)</ref> adapters. <ref type="bibr">([ϕ, x]</ref>), where [•, •] stands for concatenation. An example is prefix tuning <ref type="bibr">Li &amp; Liang (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">input composition: f</head><formula xml:id="formula_3">′ i (x) = f θi</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">function composition: f</head><formula xml:id="formula_4">′ i (x) = f ϕ • f θi (x)</formula><p>, where the outputs of the first function is fed into the second function. An example are adapter layers <ref type="bibr" target="#b250">(Rebuffi et al., 2017)</ref>.</p><p>For each i-th sub-function, multiple modules from the inventory M i can be selected through a routing function r(•), which returns a score α j for each module f ϕj conditioned on the data itself, such as a language token or visual region x or the full input x, or metadata such as the task identity t ∈ T . Note that α can be fixed a priori through expert knowledge or learned through an appropriate parameterisation r ρ <ref type="bibr">(•)</ref>, where ρ refers to (learnable) parameters of the routing function. Often, the routing function takes special forms:</p><p>1. In hard routing, α ∈ {0, 1} |M | is a discrete binary vector. If these parameters are learned, inference usually relies on score function estimators, stochastic re-parameterisation, or evolutionary algorithms.</p><p>2. In soft routing, α ∈ [0, 1] |M | is a continuous probability distribution, such that j α j = 1.</p><p>3. Finally, α ∈ R |M | can be an unnormalised score vector. This is the case in linear hypernetworks <ref type="bibr" target="#b115">(Ha et al., 2017)</ref>, where α is usually interpreted as a task embedding and the row-wise stacked module parameters Φ = [ϕ 1 , . . . , ϕ |M | ] act as a parameter generator.</p><p>Finally, the output of each module is combined through an aggregation function g(•). <ref type="foot" target="#foot_3">4</ref> The aggregation function usually takes two possible forms. One consists of a deterministic operation based on the routing scores (e.g., weighted averaging of module parameters or outputs). The other consists of a learnable neural network, such as an attention mechanism between the modules' inputs and outputs <ref type="bibr">(Pfeiffer et al., 2021a)</ref>. When we put the computation function, routing function, and aggregation function together, we obtain the general recipe for a modular function, illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Computation Function</head><p>The computation function determines the design of a module. Various module architectures have been proposed such as MLP layers <ref type="bibr" target="#b256">(Rosenbaum et al., 2018;</ref><ref type="bibr" target="#b155">Kirsch et al., 2018;</ref><ref type="bibr" target="#b45">Chang et al., 2019)</ref>, independent RNNs <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref>, independent CNNs <ref type="bibr" target="#b226">(Parascandolo et al., 2018)</ref>, or special-purpose architectures <ref type="bibr">(Andreas et al., 2016b)</ref>. However, in transfer learning, modules are most often integrated into a base architecture whose parameters are fully shared. We identify three core methods to merge a single module with the corresponding sub-function: parameter composition, input composition, and function composition. While all three methods instantiate modules differently, we demonstrate how they can be seen in a unified view in § 3.5. We provide example illustrations of the three computation functions (in addition to a hypernetwork) as part of a Transformer architecture in Figure <ref type="figure" target="#fig_3">2</ref> and provide a high-level overview of their trade-offs in Table <ref type="table">3</ref>, which we further discuss in the respective sections. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter Composition</head><p>Parameter composition methods augment the function f W of a base model with weights W ∈ R o×i with module parameters Φ ∈ R o×i , where i is the input dimensionality, and o is the output dimensionality. In particular, the module inventory consists of a set of sparse or low-rank weights to ensure that the modules are parameter-efficient. Therefore, the resulting function is parameterised as f θ⊕ϕ i , where ⊕ stands for element-wise addition.</p><p>Sparse Subnetworks Sparsity is a common inductive bias based on the assumptions (i) that only a small number of parameters of an over-parameterised model are relevant for a particular task, and that (ii) similar tasks share similar sub-networks. This is the case, for instance, for language subnetworks in multilingual language models <ref type="bibr" target="#b284">(Stanczak et al., 2022;</ref><ref type="bibr" target="#b87">Foroutan et al., 2022)</ref>.</p><p>The most widespread method to induce sparsity is pruning. This can be interpreted as the application of a binary mask b ∈ {0, 1} |θ| that selectively keeps or removes each connection in a model with trained parameters θ ⋆ : f ′ = f θ ⋆ ⊙b where ⊙ is element-wise multiplication. The merger of θ and b results in a sparse subnetwork, but the corresponding model parameters usually remain dense for hardware and software reasons. 6 After training, the trained weights are sorted based on a criterion and a fraction (bottom-k) of the weights are set to zero. Examples of criteria include magnitude after convergence <ref type="bibr" target="#b119">(Han et al., 2017)</ref> and change of magnitude between initialisation and convergence <ref type="bibr" target="#b88">(Frankle &amp; Carbin, 2019)</ref>.</p><formula xml:id="formula_5">5</formula><p>The comparison is mainly meant as a high-level guideline. Individual methods may have different trade-offs and mitigate certain weaknesses indicated in the table.</p><p>6 In fact, sparse linear algebra operations on graphic processing units remain highly inefficient, if available at all. Examples include the sparse tensor classes in Pytorch: <ref type="url" target="https://pytorch.org/docs/stable/sparse.html">https://pytorch.org/docs/stable/sparse.html</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Reference Function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse subnetwork</head><p>Frankle &amp; Carbin ( <ref type="formula">2019</ref>)</p><formula xml:id="formula_6">f ′ = f θ ⋆ ⊙b Supermasks Wortsman et al. (2020) f ′ = f θ0⊙b Sparse fine-tuning Ansell et al. (2022) f ′ = f θ+b⊙ϕ Intrinsic dimension Li et al. (2018) f ′ = f θ+ϕM Low-rank adaptation Hu et al. (2022) f ′ i = f θi+vec(BiAi) Prompting Brown et al. (2020) f ′ 1 = f θ1 ([ϕ, x]) where ϕ = Emb(p) Retrieval augmentation Guu et al. (2020) f ′ 1 = f θ1 ([ϕ, x]) where ϕ = Emb([p, c]) Prompt tuning Lester et al. (2021) f ′ 1 = f θ1 ([ϕ, x]) Multi-layer prompt tuning Li &amp; Liang (2021) f ′ i = f θi ([ϕ i , x]) Parameter sharing Ruder (2017) f t ϕi = f s ϕi ∀i ∈ G Convolutional adapter Rebuffi et al. (2017) f ′ i = f ϕi (f θi (x)) where f ϕi (x) = F * x Transformer adapter Houlsby et al. (2019) f ′ i = f ϕi (f θi (x)) where f ϕi (x) = W d (σ(W u x)) Compacter Mahabadi et al. (2021a) f ′ i = f ϕi (f θi (x))</formula><p>where</p><formula xml:id="formula_7">f ϕi (x) = W d (σ(W u x)), W = n j=1 A j ⊗ B j Parallel adapter Rebuffi et al. (2018) f ′ i = f θi (x) + f ϕi (x) Rescaling Bilen &amp; Vedaldi (2017) f ′ i = f θi (x) ⊙ ϕ Hypernetwork Platanios et al. (2018) f ′ i = (αW )x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing function</head><p>Fixed routing <ref type="bibr" target="#b117">Hampshire &amp; Waibel (1992)</ref> </p><formula xml:id="formula_8">f ′ i = 1 |K| j f (ϕ j ) 1 j (K)</formula><p>Top-1 learned routing Rosenbaum et al. ( <ref type="formula">2018</ref>)</p><formula xml:id="formula_9">f ′ i = f (x; θ i , ϕ j ) where j = argmax[α] Top-k learned routing Goyal et al. (2021) f ′ i = cat j∈ top k [α] f (x; θ i , ϕ j ) Variable-size (threshold) Rahaman et al. (2021) f ′ i = cat j∈M s.t. αj &gt;t f (x; θ i , ϕ j ) Variable-size (soft partition) Ponti et al. (2022) f ′ i = 1 α j∈M s.t. αj =1 f (x; θ i , ϕ j ) Mixture of experts Jacobs et al. (1991b) f ′ i = j∈M α j f (x; θ i , ϕ j ) Weighted top-k routing Shazeer et al. (2017) f ′ i = j∈ top k [α] αj α f (x; θ i , ϕ j )</formula><p>Aggregation function Sparse weight addition Ansell et al. (2022) f ′ = f θ0+ϕl+ϕt Representation averaging Ma et al. (2018)</p><formula xml:id="formula_10">f ′ i = |Mi| j α j h j Input concatenation Vu et al. (2022b) f ′ i = f θ ([ϕ t , ϕ l , x]) Attention-based aggregation Pfeiffer et al. (2021a) f ′ i = Attn(h i-1 Q i , H i K i , H i V i ) Sequential aggregation Pfeiffer et al. (2020b) f ′ i = f ϕt (f ϕl (f θ0 (x)))</formula><p>Table 2: An overview of representative computation, routing, and aggregation functions. Each method is paired with a representative reference. In computation functions, skip connections are omitted for simplicity. Definitions: the model f , a model's sub-function f i , model parameters θ, module parameters ϕ, parameters at initialisation θ 0 , parameters after training θ ⋆ , binary mask b ∈ {0, 1} |θ| , random matrix M, group G, input x, a model's embedding layer Emb(•), text prompt p, retrieved context c, filter bank F , routing scores or task embedding α, routing function r, subset of modules K, module inventory M .  A small separate neural network generates modular parameters conditioned on metadata. We show its application to function composition but it is compatible with all computation functions.</p><p>As pruning generally leads to a loss in performance due to the change in network connections, the non-pruned weights are typically re-wound to their initialisation value and re-trained. In practice, rather than pruning all weights in a single run, iterative pruning is carried out over multiple stages <ref type="bibr" target="#b118">(Han et al., 2015;</ref><ref type="bibr" target="#b88">Frankle &amp; Carbin, 2019)</ref>. The models pruned in this fashion often retain-if not surpass -the performance of the original dense model. The existence of a subnetwork with this property in any given randomly initialised model is known as the Lottery Ticket Hypothesis (LTH; <ref type="bibr" target="#b88">Frankle &amp; Carbin, 2019;</ref><ref type="bibr" target="#b46">Chen et al., 2020)</ref>. These 'winning tickets' have also been shown to exist in RL and NLP <ref type="bibr" target="#b343">(Yu et al., 2020)</ref>, as well as in computer vision <ref type="bibr" target="#b89">(Frankle et al., 2020)</ref>. Subnetworks achieve above-random performance even when kept fixed at their random initialisation <ref type="bibr" target="#b354">(Zhou et al., 2019;</ref><ref type="bibr" target="#b333">Wortsman et al., 2020;</ref><ref type="bibr" target="#b349">Zhao et al., 2020)</ref>, so f ′ = f θ0⊙b . In this case, they are known as supermasks.</p><p>Winning tickets also occur in pre-trained models, such as language models <ref type="bibr" target="#b46">(Chen et al., 2020;</ref><ref type="bibr" target="#b244">Prasanna et al., 2020)</ref>. These often outperform tickets from randomly initialised models <ref type="bibr" target="#b244">(Prasanna et al., 2020)</ref> and are less sensitive to specific hyper-parameter choices <ref type="bibr">(Sun et al., 2020a)</ref>. Magnitude pruning, which relies on zeroth-order information (the absolute value of a weight), is sub-optimal in this setting as fine-tuned weights typically stay close to their pre-trained values. Thus, magnitude pruning selects a similar set of weights for pruning regardless of the downstream task. Pruning based on first-order (gradient-based) information better captures the task-specific relevance of each weight <ref type="bibr" target="#b214">(Molchanov et al., 2017)</ref>. For instance, movement pruning <ref type="bibr" target="#b266">(Sanh et al., 2020)</ref> learns the mask b jointly with the parameters θ. As the mask is a discrete binary variable, they rely on straight-through estimators <ref type="bibr" target="#b28">(Bengio et al., 2013)</ref>. Alternatively, b can be first learned as a real-valued mask and then binarised via a thresholding function <ref type="bibr">(Mallya et al., 2018)</ref>.</p><p>In addition to pruning, sparsification techniques can be employed for adaptation. In particular, a sparse module ϕ can be merged with pre-trained parameters θ. For instance, in Sparse Fine-Tuning (SFT; <ref type="bibr" target="#b11">Ansell et al., 2022)</ref> the LTH is re-purposed such that, instead of zeroing out weights with the lowest change in magnitude, they are simply frozen. Thus, only a subset of weights is fine-tuned. <ref type="foot" target="#foot_5">7</ref> The difference between these and the original pre-trained model results in a sparse module ϕ where</p><formula xml:id="formula_11">ϕ i = 0 if b i = 0,</formula><p>which can be Parameter Training Inference Performance Compositionality efficiency efficiency efficiency Parameter composition + -++ + + Input composition ++ ---+ Function composition -+ -++ + Table 3: Comparison of computation functions along different dimensions. See the end of § 3.1 (parameter composition), § 3.2 (input composition), and § 3.3 (function composition) for further explanation. Compositionality is discussed in § 5.</p><p>plugged in and out of the model as f ′ θ = f θ⊕ϕ . Diff pruning <ref type="bibr" target="#b109">(Guo et al., 2021)</ref> instead obtains a sparse adapter by fine-tuning a dense difference vector ϕ regularised to be sparse with a differentiable approximation to the L 0 -norm penalty. <ref type="bibr" target="#b292">Sung et al. (2021)</ref> induce a fixed sparse mask by selecting the top-k weights ranked according to (a diagonal approximation of) their Fisher information. This second-order information reveals the impact of the change of a parameter on the model predictions. Thus,</p><formula xml:id="formula_12">b j = 1 if j ∈ top-k 1 n n i=1 Ey∼f θ ⋆ (y|xi) (∇ θ log f θ ⋆ (y | x i )) 2 0 otherwise (1)</formula><p>Beyond the sparsification of individual weights, sparse model adaptation can also be structured. In this case, only a group of model sub-functions is fine-tuned, while the rest of the parameters remain frozen. The most common setting is for such a group to correspond to a subset of layers, e.g. the last one <ref type="bibr" target="#b72">(Donahue et al., 2014)</ref>. Groups can also relate to more fine-grained parts of the model. For instance, a group consisting of a model's bias parameters is a practical choice as this removes the need to store the model's intermediate activations <ref type="bibr" target="#b41">(Cai et al., 2020;</ref><ref type="bibr" target="#b26">Ben Zaken et al., 2022)</ref>. At the level of parameter tensors, some methods prune filters in CNNs <ref type="bibr" target="#b12">(Anwar et al., 2017;</ref><ref type="bibr" target="#b221">Newell et al., 2019)</ref> whereas others prune attention heads in pre-trained Transformers <ref type="bibr" target="#b311">(Voita et al., 2019;</ref><ref type="bibr" target="#b208">Michel et al., 2019)</ref>. In structured diff pruning, members of a group are encouraged to share the same mask value <ref type="bibr" target="#b109">(Guo et al., 2021)</ref>.</p><p>Low-Rank Modules Similar to sparsity, another efficient solution is for the module parameters ϕ i to lie in a low-dimensional subspace. <ref type="bibr" target="#b174">Li et al. (2018)</ref> show that models can be optimised in a low-dimensional, randomly oriented subspace rather than the full parameter space. In this setting, the module parameters ϕ ∈ R d are low-dimensional compared to the model parameters θ ∈ R D and d ≪ D. A random matrix M ∈ R d×D can be used to project from d to D: f ′ θ = f θ+ϕM . An efficient way to compute M is via the Fastfood transform <ref type="bibr" target="#b167">(Le et al., 2014)</ref>, which factorises M as random linear matrices. Specifically, M = HGΠHB consists of a Hadamard matrix H, a random diagonal matrix with independent standard normal entries G, a random diagonal matrix with equal probability ±1 entries B, and a random permutation matrix Π. <ref type="bibr" target="#b174">Li et al. (2018)</ref> refer to the minimum d that achieves within 90% of the full-parameter model performance as the intrinsic dimensionality of a given task. <ref type="bibr" target="#b0">Aghajanyan et al. (2021)</ref> investigate the intrinsic dimensionality of various NLP tasks with different pre-trained models. They observe that it decreases during pre-training and that larger models have lower values.</p><p>However, storing the random matrices results in a substantial memory overhead and is slow to train <ref type="bibr">(Mahabadi et al., 2021a)</ref>. If the weight matrix W ∈ R o×i is small enough, we can directly compose it into low-rank matrices W = λBA where A ∈ R k×i and B ∈ R o×k , where i is the input dimensionality, o is the output dimensionality, k is the rank of the matrix, and λ is a scaling hyper-parameter. To save space, the factorisation may be only applied to certain groups of parameters G. In LoRA <ref type="bibr" target="#b132">(Hu et al., 2022)</ref>, this group corresponds to the linear projections in the self-attention mechanisms of each Transformer layer:</p><formula xml:id="formula_13">f ′ j = f θj +vec(Bj Aj ) ∀f ′ j ∈ G.</formula><p>Overall, parameter composition methods (both sparse and low-rank) are very parameter-efficient and often require updating less than 0.5% of a model's parameters <ref type="bibr" target="#b109">(Guo et al., 2021)</ref>. At inference time, they keep the model size constant or even reduce it, if the resulting model is sparse. This is compelling as it enables a plug-in replacement of the original with the modular model without any changes to the underlying architecture. Sparse modules, however, increase the time complexity of optimisation as they typically require multiple iterations of re-training. Finally, state-of-the-art parameter composition methods, e.g., LoRA <ref type="bibr" target="#b132">(Hu et al., 2022)</ref> and SFT <ref type="bibr" target="#b11">(Ansell et al., 2022)</ref> achieve strong performance in zero-shot and few-shot transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input Composition</head><p>Input composition methods augment a function's input x by concatenating it with a parameter vector ϕ i :</p><formula xml:id="formula_14">f ′ i (x) = f θi ([ϕ i , x]).</formula><p>The most common strategy is to augment the input fed to the model's first layer f 1 .</p><p>Prompting In a prompting setup with auto-regressive language models <ref type="bibr" target="#b38">(Brown et al., 2020)</ref> or encoders <ref type="bibr">(Schick &amp; Schütze, 2021a;</ref><ref type="bibr">b)</ref>, the input prompt p consists of (optional) instructions and (optional) in-context examples that have been converted to natural language. From a different perspective, the task-specific text prompt, when encoded using the model's embedding layer Emb(•), corresponds to modular parameters ϕ that elicit the desired behaviour <ref type="bibr">(Gao et al., 2021b;</ref><ref type="bibr">Liu et al., 2023)</ref>: Emb(p) = ϕ. More elaborate prompts p such as rationales have led to improved few-shot reasoning performance <ref type="bibr">(Wei et al., 2022c;</ref><ref type="bibr" target="#b156">Kojima et al., 2022;</ref><ref type="bibr" target="#b280">Shi et al., 2023)</ref>. However, models are ostensibly sensitive to the formulation of the prompt as well as to the set and order of the (few-shot) examples <ref type="bibr" target="#b350">(Zhao et al., 2021;</ref><ref type="bibr" target="#b192">Lu et al., 2022;</ref><ref type="bibr" target="#b325">Webson &amp; Pavlick, 2022)</ref>.</p><p>Continuous Prompts Instead, a continuous prompt vector ϕ can be learned directly <ref type="bibr" target="#b169">(Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b352">Zhong et al., 2021;</ref><ref type="bibr" target="#b116">Hambardzumyan et al., 2021)</ref>. However, if ϕ is only concatenated with the first layer's input, the model has limited capacity to adapt to a specific task. As a result, such continuous (also called soft) prompts perform poorly at smaller model sizes and on some harder tasks <ref type="bibr">(Mahabadi et al., 2021a;</ref><ref type="bibr">Liu et al., 2022c)</ref>. To mitigate this, initialisation via multi-task learning has been proposed <ref type="bibr">(Vu et al., 2022c)</ref>. As an alternative, module vectors ϕ i can be learned for each layer of the model (Figure <ref type="figure" target="#fig_3">2b</ref>; <ref type="bibr">Li &amp; Liang, 2021;</ref><ref type="bibr">Liu et al., 2022c)</ref>. While this increases the number of parameters, it increases the modules' capacity to adapt to a given task. In practice, module parameters in the form of prefix vectors ϕ i = P i k , P i v ∈ R l×d are prepended to the keys and values of every multi-head attention layer. Attention is defined as</p><formula xml:id="formula_15">f i (x) = Attn(xW i q , CW i k , CW i v )</formula><p>where W q , W k , W v ∈ R d×d h are the projections that produce the queries, keys, and values, and C ∈ R m×d is a sequence of context vectors. Multi-layer prompt tuning thus takes the following form:</p><formula xml:id="formula_16">f ′ i (x) = Attn(xW i q , [P i k , CW i k ], [P i v , CW i v ]).<label>(2)</label></formula><p>Retrieval Augmentation Beyond individual prompts, the input can be augmented with additional context c from a retrieval model. Retrieved documents are appended to the input and are used for conditioning the language model <ref type="bibr" target="#b114">(Guu et al., 2020;</ref><ref type="bibr">Lewis et al., 2020a)</ref></p><formula xml:id="formula_17">: Emb([p, c]) = ϕ.</formula><p>In summary, input composition is exceptionally parameter-efficient as it only adds a very small number of parameters. However, these parameters extend a model's context window, which makes them less efficient during training and inference. Prompt tuning methods also require large models to achieve decent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Function Composition</head><p>While parameter composition deals with individual weights and input composition methods act only on a function's input, function composition methods augment the model with new task-specific sub-functions (see Figure <ref type="figure" target="#fig_3">2c</ref>):</p><formula xml:id="formula_18">f ′ i (x) = f ϕi • f θi (x) = f ϕi (f θi (x))</formula><p>, where • stands for function composition.</p><p>Parameter Sharing Models in multi-task learning traditionally consist of shared layers f θ stacked under task-specific modules f ϕ <ref type="bibr" target="#b260">(Ruder, 2017)</ref>. Conversely, given models for tasks t and s expressed as a composition of functions f t ϕ1 • . . . • f t ϕ l and f s ϕ1 • . . . • f s ϕ l , respectively, a multi-task architecture can also be obtained by tying sets of parameters between the models: f t ϕi = f s ϕi ∀i ∈ G where the group G contains the set of shared  layer indices.<ref type="foot" target="#foot_6">foot_6</ref> Many multi-task neural architectures can be characterised in terms of their definition of G, which determines which modules are task-specific and which ones are shared. This is the case, for instance, of 'shared trunk' approaches in computer vision <ref type="bibr" target="#b347">(Zhang et al., 2014;</ref><ref type="bibr" target="#b194">Ma et al., 2018)</ref> and approaches with supervision at different layers in NLP <ref type="bibr" target="#b282">(Søgaard &amp; Goldberg, 2016;</ref><ref type="bibr" target="#b265">Sanh et al., 2019;</ref><ref type="bibr" target="#b188">Liu et al., 2019)</ref>.</p><p>Some approaches learn finer-grained interactions between pairs of modules. <ref type="bibr" target="#b211">Misra et al. (2016)</ref> propose the cross-stitch unit, which linearly combines the inputs at every layer<ref type="foot" target="#foot_7">foot_7</ref> : ( x t , x s ) = W [x t , x s ] where</p><formula xml:id="formula_19">x t ij x s ij = α tt α ts α st α ss x t ij x s ij</formula><p>and α ∈ R. Sluice networks <ref type="bibr">(Ruder et al., 2019a)</ref> extend cross-stitch units to multiple modules per layer and additionally employ a soft selection of the skip connections from all layers at the output layer l:</p><formula xml:id="formula_20">x t⊤ =   β t 1 • • • β t l   ⊤ x t 1 ⊤ , . . . ,</formula><p>x t l ⊤ and β ∈ R. On the other hand, Gao et al. (2019) fuse features from multiple tasks through a 1x1 convolution. Bragman et al. (2019) employ variational inference to assign filters in a CNN to task-specific or shared roles. Rather than learning which modules should be shared among which tasks, which is a combinatorially large problem, Lu et al. (2017) and Vandenhende et al. (2020) start with a fully shared model and then dynamically widen it during training, by cloning function f θi into new modules f ϕi,1 , . . . , f ϕ i,k shared among a smaller subset of tasks, in top-down order across layers. More information on parameter-sharing strategies in multi-task learning can be found in relevant surveys <ref type="bibr" target="#b260">(Ruder, 2017;</ref><ref type="bibr" target="#b61">Crawshaw, 2020)</ref>.</p><p>Adapter Layers As an alternative to parameter sharing, a new task-specific learnable function f ϕi can be composed with an (often frozen) shared function f θi . As the main purpose of such modules is adapting a pre-trained model to new tasks, they are also simply known as 'adapter layers'. We provide examples of different adapter layers in Figure <ref type="figure" target="#fig_5">3</ref>.</p><p>The adapter's design and composition with the pre-trained model are often modality-specific. In computer vision, the adapter typically consists of a 1×1 convolution, i.e., f ϕi (x) = F * x where F is a bank of 1×1 filters and * is the convolution operation <ref type="bibr" target="#b250">(Rebuffi et al., 2017)</ref>. The module is then inserted between the convolutional blocks of a pre-trained model, such as a ResNet <ref type="bibr" target="#b123">(He et al., 2016)</ref>. In NLP, a bottleneck architecture has become popular which consists of a down-and up-projection, coupled with an intermediate activation function <ref type="figure">d</ref> x is the dimensionality of the input (typically the hidden dimension), and k is the bottleneck dimension. σ is commonly a non-linearity such as a ReLU unit (Figure <ref type="figure" target="#fig_5">3a</ref>; <ref type="bibr" target="#b129">Houlsby et al., 2019;</ref><ref type="bibr">Pfeiffer et al., 2020b)</ref>. In a Transformer model, adapters are placed both after the multi-head attention and the feed-forward layer <ref type="bibr" target="#b129">(Houlsby et al., 2019)</ref>, just after the multi-head attention <ref type="bibr" target="#b24">(Bapna &amp; Firat, 2019)</ref>, or just after the feed-forward layer <ref type="bibr">(Pfeiffer et al., 2020b)</ref>.</p><formula xml:id="formula_21">σ: f ϕi (x) = W d (σ(W u x)) where W d ∈ R dx×k and W U ∈ R k×dx ,</formula><p>Other variants for σ such as the identity function, standard multi-head attention, and multi-head attention with shared projection matrices have also been explored <ref type="bibr" target="#b285">(Stickland &amp; Murray, 2019)</ref>. <ref type="bibr">Mahabadi et al. (2021a)</ref> propose Compacter, a hyper-complex, low-rank adapter that reparameterises W in the adapter as:</p><formula xml:id="formula_22">W = n i=1 A i ⊗ B i where A i ∈ R n×n is shared across layers (n is a hyper-parameter), B i ∈ R k n × d</formula><p>n is parameterised as a low-rank matrix B i = s i t ⊤ i and ⊗ is the Kronecker product. Adapters can be routed sequentially or in parallel. Sequential adapters, are inserted between existing functions: <ref type="bibr">et al., 2017;</ref><ref type="bibr" target="#b129">Houlsby et al., 2019)</ref>. Parallel adapters are applied in parallel to a pretrained function: <ref type="bibr">(Figure 3b;</ref><ref type="bibr" target="#b251">Rebuffi et al., 2018;</ref><ref type="bibr" target="#b285">Stickland &amp; Murray, 2019;</ref><ref type="bibr">He et al., 2022a)</ref>. Moreover, adapters involve two residual connections: between the output of f θi and the output of f ϕi , which is further added to x and normalised. Adapters have been shown to lead to increased sample efficiency, flatter minima, and more robustness to hyper-parameter choices compared to standard model fine-tuning <ref type="bibr">(Mahabadi et al., 2021b;</ref><ref type="bibr" target="#b124">He et al., 2021;</ref><ref type="bibr" target="#b120">Han et al., 2021)</ref>.</p><formula xml:id="formula_23">f ′ i (x) = f ϕi (f θi (x)) (Rebuffi</formula><formula xml:id="formula_24">f ′ i (x) = x + f θi (x) + f ϕi (x)</formula><p>Function Augmentation Adapters and more complex module designs can also be used to augment a base model with information and behaviour that it otherwise would not be able to access. This can be through adapter layers pre-trained on specific domains <ref type="bibr">(Wang et al., 2021a)</ref> or other modalities <ref type="bibr">(Alayrac et al., 2022)</ref>. Modules can also be designed to attend over explicit key-value memory representations of entities and facts <ref type="bibr" target="#b309">(Verga et al., 2021)</ref> and general domain knowledge <ref type="bibr" target="#b51">(Cheng et al., 2023)</ref> to enable a model to perform certain types of operations such as arithmetic reasoning <ref type="bibr" target="#b300">(Trask et al., 2018;</ref><ref type="bibr" target="#b6">Andor et al., 2019)</ref>. More broadly, function composition enables the use of arbitrarily complex auxiliary modules. We highlight how function composition has been used to inject knowledge into models in §7.1.4. For an overview of how modules can be used to allow language models to use tools and to act, we direct the reader to <ref type="bibr">Mialon et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rescaling</head><p>The output representations can also be directly transformed via element-wise multiplication with a vector of learned parameters:</p><formula xml:id="formula_25">f ′ i (x) = f θi (x) ⊙ ϕ.</formula><p>Crucially, this is equivalent to stacking the original function f θi with a linear transformation W = Iϕ. Such task-specific rescaling is typically applied to batch normalisation parameters in computer vision <ref type="bibr">(Bilen &amp; Vedaldi, 2017)</ref> and to layer normalisation parameters in NLP <ref type="bibr" target="#b129">(Houlsby et al., 2019)</ref>.</p><p>The adapter (IA) 3 (Figure <ref type="figure" target="#fig_5">3c</ref>; <ref type="bibr">Liu et al., 2022b</ref>) multiplies learned vectors with the keys and values in self-attention blocks and the intermediate activations in position-wise feedforward networks in the Transformer. Rescaling activations favours dimensions that are important for a given task. Multiplication with a binary mask is a special case of rescaling that incorporates sparsity: <ref type="bibr" target="#b287">Strezoski et al. (2019)</ref> multiply a task-specific random binary mask b with a function's input x at every layer.</p><p>Overall, standard function composition methods such as adapter layers typically require more parameters as the new function depends on a model's input size and hidden size. While they do not require storing the gradients of the frozen parameters, they increase the number of operations at training and inference time. State-of-the-art function composition methods match or outperform standard fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hypernetworks</head><p>In the above-mentioned adapters, different modules ϕ 1 , . . . , ϕ |M | correspond to disjoint sets of parameters. However, the modules may benefit from sharing information. Rather than learning ϕ i directly, a (small) neural network W , known as a hypernetwork, can generate the module parameters instead, conditioned on an embedding α <ref type="bibr" target="#b115">(Ha et al., 2017;</ref><ref type="bibr" target="#b239">Platanios et al., 2018)</ref>. Thus, ϕ = W α. As a result, the modules are 'entangled', which violates the strong definition of modularity that postulates that modules are autonomous <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref>. In fact, in hypernetworks, computation and routing are inseparably intertwined. In fact, foreshadowing our discussion in § 4.2.4, the embedding α can also be interpreted as unnormalised, learned routing scores for each task. In turn, the parameter generator weight would correspond to a set of modules stacked column-wise:</p><formula xml:id="formula_26">W = [ϕ 1 , . . . , ϕ |M | ].</formula><p>Hypernetworks can also be conditioned on inputs x (Figure <ref type="figure" target="#fig_3">2d</ref>). For instance, in conditional batch normalisation <ref type="bibr" target="#b65">(de Vries et al., 2017)</ref>, rescaling parameters are generated based on a representation of the model input obtained via an LSTM. Feature-wise linear modulation (FiLM; <ref type="bibr" target="#b228">Perez et al., 2018)</ref> generates an element-wise affine transformation that is applied to image features, conditioned on the linguistic input of the model, for text-and-vision tasks. In self-modulation for Generative Adversarial Networks <ref type="bibr" target="#b48">(Chen et al., 2019)</ref>, the affine transformation is applied to hidden representations of the generator conditioned on the noise sample. <ref type="bibr" target="#b31">Bertinetto et al. (2016)</ref> conditions the parameter generator on individual examples, in order to perform one-shot learning.</p><p>Hypernetworks have been used to generate a diverse set of module parameters, including classifier heads <ref type="bibr">(Ponti et al., 2021)</ref>, continuous prompts <ref type="bibr">(He et al., 2022c)</ref>, and adapter layers <ref type="bibr" target="#b302">(Üstün et al., 2020;</ref><ref type="bibr" target="#b10">Ansell et al., 2021;</ref><ref type="bibr">Mahabadi et al., 2021b)</ref>, most commonly conditioned on task <ref type="bibr">(Mahabadi et al., 2021b)</ref> or language embeddings <ref type="bibr" target="#b302">(Üstün et al., 2020;</ref><ref type="bibr" target="#b25">Baziotis et al., 2022)</ref>. Such task or language embeddings α can themselves be learned directly from random initialisations or fixed as the typological features of a language <ref type="bibr" target="#b302">(Üstün et al., 2020;</ref><ref type="bibr" target="#b10">Ansell et al., 2021)</ref>. This is a strategy to integrate side (or metadata) information about the relationship among languages. Other examples of side information, such as the example label y, can be integrated into the hypernetwork input embedding via bi-linear interaction <ref type="bibr" target="#b48">(Chen et al., 2019)</ref>. Nevertheless, even the smallest possible module generator network is a linear projection W ∈ R d ϕ ×dα . To make the hypernetwork more parameter-efficient, it can be shared across layers by conditioning it on the module position in the neural architecture, in addition to the task index <ref type="bibr">(Mahabadi et al., 2021b)</ref>. In general, the hypernet can be conditioned on multiple (concatenated) embeddings: e.g., one corresponding to the task index and another to the language index. This allows the hypernetwork to generalise systematically to new task-language combinations at inference time. In particular, the hypernet can either generate a single module from all the embeddings <ref type="bibr">(Ponti et al., 2021)</ref> or separate modules <ref type="bibr" target="#b10">(Ansell et al., 2021;</ref><ref type="bibr" target="#b304">Üstün et al., 2022)</ref>. In turn, the embedding combination chosen for any example is a form of hard routing (cf. § 4.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Unifying Parameter, Input, and Function Composition</head><p>While the above methods may seem different, they all covertly share a similar functional form. <ref type="bibr">He et al. (2022a)</ref> cast LoRA <ref type="bibr" target="#b132">(Hu et al., 2022)</ref>, prefix tuning <ref type="bibr">(Li &amp; Liang, 2021)</ref>, and bottleneck adapters <ref type="bibr" target="#b129">(Houlsby et al., 2019)</ref>, representative methods of the three composition functions, into the same framework. We extend their framework to cover parameter composition, input composition, and function composition in general. Specifically, all modular computation functions can be reduced to function composition: the output of the function f θi of a model is added to a new term that depends on a learned function</p><formula xml:id="formula_27">f ϕ : f ′ i (x) = f θ (x) + f ϕi (x).</formula><p>For function composition methods, this form is the most natural. In the case of parallel adapters, for instance,</p><formula xml:id="formula_28">f ′ i (x) = f θi (x) + f ϕi (x) where f θi (x) may be a multi-head attention module f θi (x) = MHA(C, x) = [head 1 , . . . , head h ]W o , with head j = Attn(xW j q , CW j k , CW j v ), and f ϕi (x) = W d (σ(W u x)</formula><p>). In this setting, θ i and ϕ i are independent and must only agree regarding the dimensionality of their inputs and outputs.</p><p>For parameter composition methods, which modify the parameters directly, the dimensionality of the module parameters ϕ should match exactly the original parameters θ i . For instance, if we apply the module to a linear projection, then they should consist of weight matrices</p><formula xml:id="formula_29">θ i = W i ∈ R dx×k and ϕ i = V i ∈ R dx×k ,</formula><p>respectively. Because of linearity:</p><formula xml:id="formula_30">f ′ i (x) = f θi⊕ϕ (x) = f W +V (x) = (W + V )x = W x + V x = f θi (x) + f ϕi (x)</formula><p>For instance, in the case of LoRA <ref type="bibr" target="#b132">(Hu et al., 2022)</ref>, V = λB i A i . In the case of sparse adapters <ref type="bibr" target="#b11">(Ansell et al., 2022)</ref>, V is a sparse matrix.</p><p>For input composition methods, with the form</p><formula xml:id="formula_31">f ′ i (x) = f θi ([ϕ i , x]</formula><p>), the equivalence is derived as follows. Prefix tuning <ref type="bibr">(Li &amp; Liang, 2021)</ref> generalises other continuous prompt methods by concatenating prefix vectors ϕ i = P i k , P i v ∈ R l×d to the keys and values of self-attention. <ref type="bibr">He et al. (2022a)</ref> show that prefix tuning can be expressed in the following way:</p><formula xml:id="formula_32">f ′ i (x) = Attn(xW i q , [P i k , CW i k ], [P i v , CW i v ]) = (1 -λ(x))f θi (x) + λ(x) softmax(xW q P ⊤ k )P v</formula><p>where λ(x) is a scalar that represents the sum of normalised attention weights on the prefixes and f θi (x) is the attention module in a Transformer. If we set, f ϕi (x) = softmax(xW q P ⊤ k )P v , then we obtain a function composition (1 -λ(x))f θi (x) + λ(x)f ϕi (x) that incorporates a weighted addition. For function and parameter composition, in contrast, the sum is unweighted.</p><p>Overall, despite their conceptual differences, most modular approaches are similar in their functional form and can be expressed as function composition. In practice, the way different methods are realised, however, leads to different trade-offs, which we illustrate in Table <ref type="table">3</ref>. Recent empirical studies <ref type="bibr">(Mahabadi et al., 2021a;</ref><ref type="bibr">He et al., 2022a;</ref><ref type="bibr">Liu et al., 2022b)</ref> provide further evidence for the strengths and weaknesses of different methods. For instance, prompt tuning <ref type="bibr">(Vu et al., 2022a)</ref> underperforms other methods due to limited capacity while intrinsic dimensionality <ref type="bibr" target="#b0">(Aghajanyan et al., 2021)</ref> uses a very small number of parameters but leads to a large memory footprint and poor performance. Fine-tuning only biases <ref type="bibr" target="#b26">(Ben Zaken et al., 2022)</ref> has a small memory footprint but achieves lower performance. Finally, function composition methods such as adapter layers <ref type="bibr">(Pfeiffer et al., 2021a) and</ref><ref type="bibr">compacter layers (Mahabadi et al., 2021a)</ref>, achieve the best performance, but add more parameters. (IA) 3 <ref type="bibr">(Liu et al., 2022b)</ref> mitigates this by composing a lightweight linear diagonal weight. Modular deep learning architectures, however, have many other differences beyond their choice of computation function. In the following sections, we discuss the routing, aggregation, and training settings for the modules presented so far.</p><p>• Computation functions may consist of any neural module. Modules may modify the original parameters, be concatenated to the input, or composed with the original function.</p><p>• Parameter composition methods utilise sparsity or low-rank constraints. They are very parameter-efficient and efficient at inference time and show strong performance.</p><p>• Input composition methods concatenate a function's input with a parameter vector via prompting, continuous prompts, and retrieval augmentation. They are extremely parameterefficient but inefficient during training and inference and require large models.</p><p>• Function composition methods augment a model with arbitrary functions via parameter sharing, adapters, or rescaling. They require more parameters but often achieve the best performance.</p><p>• Rather than learning module parameters directly, hypernetworks can be used to generate module parameters, which enables sharing of information and conditioning on auxiliary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Routing Function</head><p>In the previous section, we described how to compose a sub-function f i with shared weights θ with a single module function with weights ϕ. However, in a modular neural architecture, multiple modules are available from an inventory M = f ϕ1 , . . . , f ϕ |M | . A decision-making process is required to determine which modules are active, conditioned on the model input or auxiliary metadata. This process is implemented through a routing function r(•) that assigns a score α i to each module from the inventory M . These scores determine</p><p>Published in Transactions on Machine Learning Research (11/2023) which subset of modules is active, i.e. contributes to the computation. We provide an overview of different routing methods in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>When metadata such as expert knowledge about sub-tasks (or skills) involved in a task is available, r(•) can be designed as a fixed function, that is, each routing decision can be made a priori (Figure <ref type="figure" target="#fig_6">4a</ref>). For instance, when using a language model to generate dialogue in Swahili, a task module for dialogue generation and a language module for Swahili can be selected. When no such prior information is available-for instance when modelling heterogeneous unlabelled data-routing of a given example needs to be learned (Figures <ref type="figure" target="#fig_6">4b-4c</ref>). In this case, the routing function can be conditioned on the current example x. <ref type="foot" target="#foot_9">10</ref>Unfortunately, learned routing is crucially under-constrained, as multiple possible ways of decomposing tasks into sub-tasks are reasonable <ref type="bibr">(Jacobs et al., 1991a)</ref>. In addition, it presents a series of unique challenges (see § 4.2.1). In an empirical study on synthetic data, <ref type="bibr">Mittal et al. (2022)</ref> found that learned routing is sub-optimal compared to fixed routing, as it tends to under-utilise modules and to specialise them to a lesser degree. This behaviour is exacerbated as the number of tasks in the data grows. In real-world applications, <ref type="bibr" target="#b216">Muqeeth et al. (2022)</ref> report similar results; however, Ponti et al. ( <ref type="formula">2022</ref>) find that learned routing may surpass expert module selection even in settings where tasks are procedurally constructed to require certain skills, such as instruction following in simulated environments.</p><p>Learning-to-route can roughly be split into hard routing and soft routing <ref type="bibr" target="#b257">(Rosenbaum et al., 2019)</ref>. Hard routing methods learn a binary selection of modules, similarly to the fixed routing scheme, where only a subset of modules is selected for each decision-making step (Figure <ref type="figure" target="#fig_6">4b</ref>). Inference for hard routing systems typically builds on score function estimators <ref type="bibr" target="#b329">(Williams, 1988;</ref><ref type="bibr" target="#b330">1992)</ref> or stochastic re-parameterisation <ref type="bibr" target="#b140">(Jang et al., 2017)</ref>. On the other hand, soft routing methods learn a probability distribution over modules (Figure <ref type="figure" target="#fig_6">4c</ref>; <ref type="bibr">Jacobs et al., 1991a)</ref>. While soft selection is more easily amenable to end-to-end learning via gradient descent, hard selection may lead to a sparse architectural design, owing to the fact that inactive modules are not part of the forward and backward computation graph. This reduces time complexity while augmenting the model capacity <ref type="bibr" target="#b28">(Bengio et al., 2013)</ref>.</p><p>While not the central focus of this paper, routing algorithms have recently garnered significant attention, due to their efficiency implications. There exists an intricate interplay between routing techniques and sparsity within modular models: When a modular architecture exhibits sparsity, signifying that only a select few modules are active during inference, a notable reduction in computational complexity during inference can be achieved <ref type="bibr" target="#b75">(Fedus et al., 2022)</ref>.<ref type="foot" target="#foot_10">foot_10</ref> Finally, <ref type="bibr">Shen et al. (2023a)</ref>; <ref type="bibr" target="#b141">Jang et al. (2023)</ref> showcase how sparse models, when combined with instruction tuning techniques, can yield substantial gains in performance and efficiency over dense models, potentially reshaping the landscape of large language model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fixed Routing</head><p>Making the routing decision a priori-i.e. when we utilise metadata (e.g. task identity t) to make the discrete routing decisions before training-is referred to as fixed routing (Figure <ref type="figure" target="#fig_6">4a</ref>). Here the routing function r(•) simplifies to a selection of a subset of modules K ⊆ M for the examples with certain metadata:</p><formula xml:id="formula_33">r(ϕ i ) = 1 if i ∈ K 0 otherwise (3)</formula><p>This function defines a binary matrix A ∈ {0, 1} |T |×|M | , where the number of rows corresponds to possible tasks and the number of columns corresponds to the size of the module inventory.</p><p>One simple example of fixed routing in multi-task learning is when all parameters, except the final classification layer, are shared among all tasks <ref type="bibr" target="#b260">(Ruder, 2017)</ref>. Independently from the task identity, the examples are passed through the same network until after the penultimate layer. The penultimate layer's representations are then routed to their respective final classification layer according to the task identity. This boils down to setting |K| = 1, with the additional constraint that tasks cannot share modules, which results in the allocation matrix being an identity matrix, A = I.</p><p>While not immediately apparent, methods that adapt pre-trained models towards individual tasks <ref type="bibr" target="#b250">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b310">2018;</ref><ref type="bibr" target="#b129">Houlsby et al., 2019;</ref><ref type="bibr" target="#b24">Bapna &amp; Firat, 2019;</ref><ref type="bibr">Li &amp; Liang, 2021;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr" target="#b132">Hu et al., 2022;</ref><ref type="bibr" target="#b11">Ansell et al., 2022;</ref><ref type="bibr" target="#b26">Ben Zaken et al., 2022</ref>, inter alia)-as discussed in § 3-deterministically route representations through the newly introduced module f ϕ . Given that the pre-trained weights are frozen and modules trained on different tasks can be added or removed, the components become modular even if they are developed asynchronously and independently of each other <ref type="bibr">(Pfeiffer et al., 2021a)</ref>. In a sense, community-based hubs of pre-trained adapters such as AdapterHub <ref type="bibr">(Pfeiffer et al., 2020a</ref>) can be considered as ever-evolving multi-task models, the development of whose components has been distributed throughout the community.<ref type="foot" target="#foot_11">foot_11</ref> Moreover, since newly introduced weights are encapsulated between frozen (shared) weights, adapted representations of intermediate layers are implicitly aligned as they are passed as input to the same frozen components. <ref type="bibr" target="#b117">Hampshire &amp; Waibel (1992)</ref> were possibly among the first to train independent experts for a series of sub-tasks known a priori. In this case, the (fixed-size) subset of experts K associated with each task t is assumed as given, resulting in the rows of A being k-way vectors. In cross-lingual transfer, any problem can be decomposed into a task and language variety. Fixed routing can select separate language and task components, and facilitate generalisation to new, unobserved combinations of tasks and languages at inference time <ref type="bibr">(Pfeiffer et al., 2020b;</ref><ref type="bibr">Ponti et al., 2021;</ref><ref type="bibr" target="#b304">Üstün et al., 2022)</ref>. In this case, |K| = 2. Similarly, in reinforcement learning, <ref type="bibr" target="#b127">Heess et al. (2016)</ref> and <ref type="bibr" target="#b66">Devin et al. (2017)</ref> design a modular policy that is composed of a robot-specific module and a task-specific module, which are instantiated as separate neural networks.</p><p>Composing these modules enables generalisation to unseen robot-task combinations.</p><p>Beyond task identity, routing can be performed based on other metadata such as language, domain, or modality information. <ref type="bibr">Pfeiffer et al. (2022b)</ref> add adapters for each language to a multilingual language model during pre-training on unlabelled text. <ref type="bibr" target="#b80">Fan et al. (2021)</ref> route deterministically for multilingual machine translation according to the language family: as a consequence, all languages in a family share the same expert. In a similar vein, <ref type="bibr" target="#b113">Gururangan et al. (2022)</ref> add domain-specific adapters to language models, deterministically routing based on the text source domain. This concept was further extended by <ref type="bibr">Li et al. (2022b)</ref>, who proposed the branch-train-merge method: copies of the same model are trained on different domains and then averaged. Finally, modality can also inform fixed routing, such as in vision-and-language models <ref type="bibr">(Pfeiffer et al., 2022a)</ref>. This allows for adapting the encoders of different modality streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learned Routing</head><p>When the routing function r(•) is not known in advance, it can be implemented as a learnable neural network with parameters ρ. In input, it receives the example representation x or metadata such as the task t. In output, it returns routing scores α. Usually, r ρ is a linear projection or a Multi-Layer Perceptron. While the former represents a less expressive family of functions, the latter may collapse into ignoring the input features. Note that learning the routing function also implies that the specialisation of each module is unknown. Thus, modules are not trained on different sets of examples; rather, they are all trained jointly with the routing function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Challenges of Learned Routing</head><p>Learned routing introduces a number of challenges, including training instability, module collapse <ref type="bibr" target="#b155">(Kirsch et al., 2018)</ref>, and overfitting. These were first systematically described by <ref type="bibr" target="#b257">Rosenbaum et al. (2019)</ref>, and we follow a similar taxonomy. In general, they identify two root causes for all these challenges: first, the need to balance between exploration and exploitation <ref type="bibr" target="#b294">(Sutton, 1986)</ref>. More specifically, routing must find the optimal trade-off between allocating information to the most suitable modules versus under-explored modules. Second, routing must share modules across examples or tasks in such a way as to reap the benefits of positive transfer while avoiding negative interference. We elaborate on the individual challenges below.</p><p>Training Instability emerges especially in the early phases of training; at this point, modules are randomly initialised and have no clear functional specialisation. Thus, the router cannot make any principled decision in selecting modules. On the other hand, modules do not start specialising until they are consistently routed to different subsets of tasks or examples.</p><p>Curriculum learning can mitigate this challenge to some extent <ref type="bibr" target="#b45">(Chang et al., 2019)</ref>, as simpler tasks require simpler sets of skills. However, this assumes that information about task complexity is available and that the data can be ordered accordingly. As an alternative, the router parameters can be trained with a different learning rate than the module parameters, either lower <ref type="bibr" target="#b256">(Rosenbaum et al., 2018)</ref> or higher <ref type="bibr">(Ponti et al., 2022)</ref>. These create two different dynamics: either the necessary skills for a task are determined after specialisation, or the relationship among tasks is figured out first and modules are updated accordingly.</p><p>Module Collapse describes scenarios where only a small number of modules (in the extreme case, one) from the available inventory are selected. This leaves the remaining modules untrained and negatively impacts their overall diversity. Often, this results from excessively favouring exploitation over exploration, which leads to sub-optimal results. To amend this, <ref type="bibr" target="#b1">Ahn et al. (2019)</ref> use ϵ-greedy routing for initial exploration of all modules and afterwards switch to learned routing. Other strategies to avoid module collapse include auxiliary losses for load balancing <ref type="bibr" target="#b277">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b82">Fedus et al., 2021)</ref> and intrinsic rewards that encourage diversity in module selection <ref type="bibr" target="#b43">(Cases et al., 2019)</ref>. The choice of information that conditions the router also plays an important role: metadata, e.g. text genre <ref type="bibr" target="#b43">(Cases et al., 2019)</ref> or task identity <ref type="bibr" target="#b158">(Kudugunta et al., 2021)</ref>, make routing more robust than individual examples. The diversity of training tasks also facilitates diversity in routing selections <ref type="bibr" target="#b45">(Chang et al., 2019;</ref><ref type="bibr" target="#b40">Caccia et al., 2022)</ref>. <ref type="bibr" target="#b76">Dua et al. (2022)</ref> warms up the sampling temperature over training, in order to over-sample domains with fewer examples in unbalanced distributions.</p><p>Overfitting to noise is a risk faced by deep modular networks due to their ability to model subsets of examples independently <ref type="bibr" target="#b257">(Rosenbaum et al., 2019)</ref>. For instance, routing at the token level was shown to lead to performance drops in out-of-domain generalisation for MoEs <ref type="bibr" target="#b13">(Artetxe et al., 2022)</ref>. For a similar reason, gains in pre-training do not always translate into gains in fine-tuning for MoEs <ref type="bibr" target="#b82">(Fedus et al., 2021)</ref>. Increased robustness can be achieved by routing conditioned on metadata if available <ref type="bibr" target="#b45">(Chang et al., 2019;</ref><ref type="bibr" target="#b43">Cases et al., 2019;</ref><ref type="bibr" target="#b158">Kudugunta et al., 2021)</ref>. In addition, strategies that favour the combinatorial behaviour of modules yield superior generalisation <ref type="bibr" target="#b45">(Chang et al., 2019;</ref><ref type="bibr">Ponti et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Hard Learned Routing</head><p>A model may learn how to select modules through hard routing. This implies that the choice of whether a module is active or excluded from the computation graph is binary. Discrete decisions are not amenable to be learned through vanilla gradient descent: since small perturbations of parameters do not affect the selection of modules, the gradient of the loss with respect to the routing parameters is zero. Thus, various methods, including reinforcement learning, evolutionary algorithms, and stochastic re-parameterisation, have been proposed for inference. These are discussed separately below.</p><p>On the other hand, hard routing is more efficient than soft routing in terms of time and space complexity. In addition, binary selection implies that parameter updates are localised to a subset of modules. This reflects the intuition that the shifts in distribution of the variables in an environment are similarly local <ref type="bibr" target="#b226">(Parascandolo et al., 2018;</ref><ref type="bibr" target="#b102">Goyal et al., 2021)</ref>. Since the inactive module parameters are not affected, they remain invariant with respect to the distribution shift. On top of this, this type of routing may result in variable-size sets of active modules. This allocates model capacity according to task complexity, which follows the principle of conditional computation <ref type="bibr" target="#b27">(Bengio et al., 2015)</ref>. In fact, it is fair to assume that the skills required for complex tasks are a superset of those of simpler tasks. For instance, dialogue modelling requires (among others) intent detection, slot filling, and conditional response generation.</p><p>Reinforcement Learning In Routing Networks <ref type="bibr" target="#b256">(Rosenbaum et al., 2018)</ref>, Modular Networks <ref type="bibr" target="#b155">(Kirsch et al., 2018)</ref>, and the Compositional Recursive Learner (CRL; Chang et al., 2019), a router network is trained through reinforcement learning. Specifically, Routing Networks rely on multi-agent RL (MARL), Modular Networks rely on the score function estimator (REINFORCE), whereas the CRL relies on Proximal Policy Optimisation (PPO). Commonly, this family of methods alternate between a score function estimator for the routing parameters ρ and SGD for module parameters {ϕ 1 , . . . , ϕ |M | }. For a vanilla score function estimator, where routing is conditioned on the input example and m ∈ M , the update takes the form:</p><formula xml:id="formula_34">∇ ρ E x,y p(y | x, θ, ϕ 1 , . . . , ϕ |M | , ρ) ≈ 1 n n i=0 [ p(y i | x i , θ, ϕ m ) ∇ ρ log p(m | x i )]<label>(4)</label></formula><p>Under this lens, routing becomes a policy π(m | x). If applied layer-wise, each hidden representation at a given layer 1 ≥ t ≤ l constitutes a state h t ∈ H. The routing policy determines the action, i.e. the selection of a module index m. In particular, this assumes that the inventory M is shared across layers. <ref type="foot" target="#foot_12">13</ref> In turn, applying the transformation of the corresponding module on the input is equivalent to a transition function π : H → H, which returns the next layer's hidden state h t+1 . The loss function at the top layer corresponds to a (delayed) negative reward, i.e. L(•) = -R. <ref type="foot" target="#foot_13">14</ref> Crucially, in this setting the transition functions are non-stationary, as the module parameters are amenable to change. Because modules are applied sequentially based on the policy, the number of steps of computation in the model can vary when a special halting action is available.</p><p>Evolutionary Algorithms Alternatively, routing can be learned via a genetic algorithm. In PathNet <ref type="bibr" target="#b84">(Fernando et al., 2017)</ref>, the loss function indicates the fitness of a configuration of active modules K ⊆ M . For each task, two configurations are selected at random and trained until a stopping criterion is met. The one incurring the lower loss on a validation set overwrites the other. This copy, in turn, receives a random mutation, and then the procedure is repeated. In µNet <ref type="bibr">(Gesmundo &amp; Dean, 2022a;</ref><ref type="bibr">b)</ref>, mutations involve cloning, insertion, and removal of layers. The fitness criteria include not only performance but also parameter efficiency. This approach has been extended to a multi-task setting where multiple agents update different modules asynchronously <ref type="bibr">(Gesmundo, 2022)</ref>. However, as is common for evolutionary algorithms, this search is brute-force and thus highly inefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Re-parametrisation</head><p>Hard routing can also be performed via a continuous relaxation of the discrete latent variable α determining the module allocation. Several stochastic re-parameterisations such as Gumbel-Softmax <ref type="bibr" target="#b140">(Jang et al., 2017)</ref> or the Concrete distribution <ref type="bibr" target="#b195">(Maddison et al., 2017)</ref> have been proposed for this purpose. Compared to the score function estimator, stochastic re-parameterisations are biased but have lower variance. Moreover, they are differentiable, which makes a hard router trainable in an end-to-end fashion. For instance, AdaShare <ref type="bibr">(Sun et al., 2020b)</ref> uses Gumbel-Sigmoid to learn a binary vector for each task that indicates whether a model layer should be included in the forward pass or skipped entirely. This may be interpreted as choosing between a parameterised module and an identity function at each layer.</p><p>Stochastic re-parameterisation also allows for selecting module subsets of varying sizes for each layer. In Neural Interpreters <ref type="bibr" target="#b247">(Rahaman et al., 2021)</ref>, this is based on a threshold. Each module is associated with a 'signature vector'. The dot product between this vector and the output of an unnormalised routing function ('type inference') conditioned on a token determines a score. If this surpasses a certain threshold, then the module is allowed to access the given token. As an alternative, variable-size module routing can be achieved by learning a soft clustering (a.k.a. soft partition) of modules <ref type="bibr">(Ponti et al., 2022;</ref><ref type="bibr" target="#b40">Caccia et al., 2022)</ref>. Thus, each entry α ij , which represents the routing of the j-th module to the i-th task, is constructed as follows:</p><formula xml:id="formula_35">α i,j = sigmoid log sigmoid( αi,j ) u (1 -sigmoid(α i,j )) (1 -u) 1/τ u ∼ Uniform(0, 1). (<label>5</label></formula><formula xml:id="formula_36">)</formula><p>where αij represents the unnormalised routing score. This latent variable also admits priors such as the Indian Buffet Process (Griffiths &amp; Ghahramani, 2011) to encourage both diversification and sharing of module subsets across tasks <ref type="bibr">(Ponti et al., 2022)</ref>. <ref type="bibr" target="#b40">Caccia et al. (2022)</ref> extend this framework to multi-head routing, where different modules can be allocated to contiguous subsets of dimensions of the layer's input and output. While this just requires as many copies of α as the number of subsets of dimension, it provides higher expressivity to the routing function.</p><p>Top-k Selection Finally, hard selection can rely on top-k selection from (possibly unnormalised) scores α over modules. In the case of Independent Causal Mechanisms <ref type="bibr" target="#b226">(Parascandolo et al., 2018)</ref>, α is given by a discriminator that scores the outputs of a generator, and k = 1. In the case of Recurrent Independent Mechanisms <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref>, the scores are derived from attention between modules and the input, and k &gt; 1. These methods are grounded on the assumption that the competition among modules to be activated facilitates their specialisation (see § 8.3 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Soft Learned Routing</head><p>Mixture of Experts To sidestep discrete selections of modules, several works propose soft routing methods, where all modules are selected and aggregated according to a weighted combination, i.e. a mixture of experts (MoE; <ref type="bibr">Jacobs et al., 1991b;</ref><ref type="bibr" target="#b145">Jordan &amp; Jacobs, 1994)</ref>. <ref type="foot" target="#foot_14">15</ref> Here, the router learns a probability distribution over the available modules, i.e. p(M ) = r ρ (•). Hence, routing and aggregation take place as:</p><formula xml:id="formula_37">f ′ i (x) = ϕj ∈M r(ϕ j ) f (x; θ i , ϕ j )<label>(6)</label></formula><p>In contrast to the discrete selection of hard routing methods, this setup is easily trained end-to-end via gradient descent. A number of works <ref type="bibr" target="#b78">(Eigen et al., 2014;</ref><ref type="bibr" target="#b205">Meyerson &amp; Miikkulainen, 2018;</ref><ref type="bibr" target="#b333">Wortsman et al., 2020</ref>, inter alia) train a continuous weighting (i.e. a mixture) of all modules; however, this limits the degree of modularity as parameter updates are not local; instead, they always affect all modules. Additionally, activating all modules for each example significantly increases the computational cost for each forward and backward pass through the network. To circumvent this, <ref type="bibr" target="#b277">Shazeer et al. (2017)</ref> and <ref type="bibr" target="#b168">Lepikhin et al. (2021)</ref> only route to the top-k of |M | modules, where 1 &lt; k &lt; |M |. The output representations of the k active modules are averaged according to the respective routing weights, whose sum is re-normalised to 1. Thus, top-k MoEs stand between hard routing, as only a subset of modules is active, and soft routing, as their average is weighted by the routing scores. In practice, a layer performs the following computation:</p><formula xml:id="formula_38">f ′ i (x) = ϕj ∈ top k [r(ϕ)] r(ϕ j ) k 1 r(ϕ) f (x; θ i , ϕ j )<label>(7)</label></formula><p>Fedus et al. ( <ref type="formula">2021</ref>) and <ref type="bibr" target="#b57">Clark et al. (2022)</ref> demonstrate that even top-1 routing can achieve competitive results for language modelling.</p><p>Token-Level Routing MoEs have recently undergone a revival as part of the efforts to scale Transformers.</p><p>In particular, MoE Transformers route to a subset of Feed-Forward Network (FFN) modules per layer instead of a single FFN. The focus of these works is on computationally efficient training of very large models. This is achieved by splitting the input tokens across different (hardware) accelerators. The MoE routing algorithm is therefore required to (ideally) uniformly distribute the tokens of all the examples in an input batch across all accelerators, i.e. to load balance computation across "experts". The dominating routing strategy is for each token to choose the top-k experts <ref type="bibr" target="#b277">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b168">Lepikhin et al., 2021;</ref><ref type="bibr" target="#b82">Fedus et al., 2021;</ref><ref type="bibr" target="#b57">Clark et al., 2022;</ref><ref type="bibr" target="#b337">Yang et al., 2021;</ref><ref type="bibr" target="#b76">Dua et al., 2022;</ref><ref type="bibr" target="#b121">Hazimeh et al., 2021;</ref><ref type="bibr" target="#b248">Rajbhandari et al., 2022;</ref><ref type="bibr" target="#b254">Riquelme et al., 2021;</ref><ref type="bibr" target="#b75">Du et al., 2022;</ref><ref type="bibr" target="#b75">Zoph et al., 2022)</ref>. Alternative approaches let each expert choose the top-k tokens <ref type="bibr" target="#b342">(You et al., 2022;</ref><ref type="bibr">Zhou et al., 2022b)</ref> or globally determine the best routing path <ref type="bibr" target="#b170">(Lewis et al., 2021)</ref>. <ref type="foot" target="#foot_15">16</ref>However, since routing is conditioned on the token level, and the load balancing restriction limits the system from routing an entire example to a single module, the system potentially has to relearn similar concepts in multiple modules. Hence, load balancing hinders the router from selecting the single best module for longer (e.g., repetitive) sequences. This is investigated further by <ref type="bibr" target="#b170">Lewis et al. (2021)</ref>, who find that sparse models route syntactically and semantically similar words (in contrast to sentences or phrases) to the same modules. This sheds light on the limited expressiveness of modules which are learned on the token-level. Since scaling is the main focus of these works, their goals are orthogonal to modular approaches centred on parameter efficiency, transfer-interference trade-offs, and combinatorial generalisation. Since task identity (or other metadata) is not always given, especially in continual learning, it can be inferred through an auxiliary model. <ref type="bibr" target="#b305">Van de Ven &amp; Tolias (2019)</ref> refer to this scenario as 'class-incremental learning'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example-Level Routing</head><p>For instance, the current task can be identified based on the lowest predictive uncertainty or an auxiliary task classifier <ref type="bibr" target="#b313">(von Oswald et al., 2020)</ref>. In these cases, routing can depend on the predicted task identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mitigating Module Collapse</head><p>To address the challenge of module collapse, which was previously discussed in §4.2.1, several strategies have been introduced to enhance the effectiveness of models in utilizing the available experts' capacity. One such approach, presented by <ref type="bibr">Shen et al. (2023b)</ref>, introduces a novel loss function centered on Mutual Information. This loss aims to maximize the mutual information between the input and the target module, effectively mitigating module collapse issues. Another innovative solution, put forward by <ref type="bibr" target="#b52">Chi et al. (2022)</ref>, involves the modification of the routing algorithm. This modification incorporates techniques like dimension reduction, L2 normalization, and adjustment of gating temperature, all designed to address the challenges associated with module collapse.</p><p>Puigcerver et al. (2023) employ a fully differentiable soft assignment mechanism by applying weighted combinations of representations to each module, allowing for enhanced model capacity without significantly increasing inference costs. Muqeeth et al. (2023) tackle module collapse by employing a weighted average-based merging approach of the module's parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Hypernetworks</head><p>In addition to hard and soft routing, hypernetworks <ref type="bibr" target="#b115">(Ha et al., 2017)</ref>, as introduced in § 3.4, can be considered a third kind of routing, with unnormalised routing scores. More formally, the parameters θ t ∈ R d for a task t can be generated by a linear function Φα t . The task embedding α t ∈ R |M | can be interpreted as the output of a task-level routing function with unnormalised scores over |M | modules. In turn, the generator Φ ∈ R d×|M | can be considered a matrix of module parameters stacked column-wise, where each module has d parameters. Thus, the generated parameters θ t is a linear combination of the columns of the linear generator. This is also reminiscent of tensor factorisation models where parameters are factorised into shared tensors and task-specific tensors <ref type="bibr" target="#b339">(Yang &amp; Hospedales, 2017)</ref>, which in hypernetworks correspond to the generator and the task embedding, respectively. However, hypernetworks learn both sets of parameters jointly rather than obtaining them from a factorisation of task-specific networks a posteriori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Level of Routing</head><p>Another aspect of designing a routing function is its level of granularity. Routing can select modules globally for the entire network, make different allocation decisions per layer, or even hierarchically select sub-routers. This last method is also referred to as 'dispatched routing' by <ref type="bibr" target="#b256">Rosenbaum et al. (2018)</ref>. A naive version of global routing (Figure <ref type="figure" target="#fig_8">5b</ref>) assumes that a single routing configuration is shared across layers. Allowing for different decisions per layer (Figure <ref type="figure" target="#fig_8">5a</ref>) is more challenging as the space of potential architectures grows exponentially as |M | l , where l is the number of layers or sub-functions of the network. In fact, to compute the posterior over parameters, one would need to marginalise over every possible configuration of A = [α 1 , . . . , α l ]. <ref type="bibr" target="#b155">Kirsch et al. (2018)</ref> resort to Expectation Maximisation to make it tractable. Instead, per-layer routing (Figure <ref type="figure" target="#fig_8">5c</ref>) assumes conditional independence among decisions, thus facilitating scaling. Crucially, routing scores are sometimes employed not only to select a subset of modules but also to aggregate their outputs. This second purpose is addressed in more depth in § 5.</p><p>Most methods assume that routing decisions occur in a sequence, whose length is bounded or unbounded. This is the case where the output of every layer is fed into the next. However, routing may also involve defining both the selection of modules and their order of composition (i.e., the model architecture). For instance, in Neural Module Networks (NMNs; <ref type="bibr">Andreas et al., 2016b;</ref><ref type="bibr" target="#b260">2017)</ref>, the routing function consists of a parser that takes in a query and produces a dependency tree. This is post-processed and transformed into a tree graph where nodes are modules and directed edges control the flow of the information, i.e. route the output(s) of a subset of modules as input to another module. In Modular Meta Learning, <ref type="bibr" target="#b4">Alet et al. (2018)</ref> alternate between sampling compositional graphs using simulated annealing <ref type="bibr" target="#b154">(Kirkpatrick et al., 1983)</ref> and performing a step of gradient descent on the network parameters for a set of meta-training tasks.</p><p>• The Routing Function is a critical component in modular neural networks, responsible for determining how information flows through modules.</p><p>• Routing can be categorized into Fixed Routing, Learned Routing, and Hypernetworks.</p><p>-Fixed Routing uses predetermined rules to direct information flow.</p><p>-Learned Routing employs neural networks to dynamically allocate modules based on input data or task information. * Challenges in Learned Routing include Training Instability, Module Collapse, and Overfitting, which require specialized strategies for mitigation. * Hard Learned Routing involves discrete module selection, often requiring reinforcement learning or stochastic re-parameterization for training. * Soft Learned Routing often use weighted combinations of modules, where predominantly top-k routing strategies are employed for computational efficiency.</p><p>-Hypernetworks offer a flexible approach by generating task-specific parameters with unnormalized routing scores.</p><p>• Routing decisions can occur at different levels of granularity, including global, per-layer, and hierarchical routing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Aggregation Function</head><p>While in the previous section on routing we have covered the topic of how to select different modules during training, we will now focus on how we can aggregate these functions in order to combine the respective information. It is important to emphasise that, for the majority of current approaches, routing and aggregation are inseparable; that is, the selection and aggregation of modules are performed simultaneously.<ref type="foot" target="#foot_16">foot_16</ref> On the other hand, the strategies for aggregating functions in this section are reminiscent of the taxonomy previously discussed for computation functions (see §3); while in the latter we looked into the composition of shared components with modules, in this section we provide insights into the composition of multiple modules. This is often required when modules are recombined for zero-shot transfer or task-level generalisation (for more details on these applications, see § 7).</p><p>In particular, for a subset of active modules K ⊆ M i the aggregation of modular components can (similarly) be realised on the parameter level f</p><formula xml:id="formula_39">′ i (x) = f ϕ1⊕•••⊕ϕ |K| (x), input level f ′ i (x) = f θi ([ϕ 1 , . . . , ϕ |K| , x]), as well as function level f ′ i (x) = f ϕ1 • ... • f ϕ |K| (x).</formula><p>In addition, we cover output or representation level aggregation</p><formula xml:id="formula_40">f ′ i (x) = f θi (x) ⊕ f ϕ1 (x) ⊕ • • • ⊕ f ϕ |K| (x).</formula><p>Crucially, this differs from parameter aggregation if f is non-linear.</p><p>We discuss these different strategies in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameter Aggregation</head><p>Mode Connectivity A natural strategy to aggregate information from multiple modules is interpolating their weights. However, given that neural architectures differ, and that hidden representations might not necessarily be equivalent (e.g. under invariance to invertible linear transformations) even if the model architectures are the same <ref type="bibr" target="#b157">(Kornblith et al., 2019)</ref>, naively aggregating module weights may have catastrophic consequences. However, recent work on linear mode connectivity <ref type="bibr" target="#b89">(Frankle et al., 2020)</ref> suggests that under certain conditions, it is in fact possible to interpolate between multiple models, which has positive ramifications for modular aggregation methods. To understand these conditions, we first provide a brief introduction to the constraints under which parameter aggregation is permissible.</p><p>The phenomenon where the minima found by two networks are connected by a path of non-increasing error, has been the subject of research for many years <ref type="bibr" target="#b90">(Freeman &amp; Bruna, 2017;</ref><ref type="bibr" target="#b74">Draxler et al., 2018;</ref><ref type="bibr" target="#b96">Garipov et al., 2018;</ref><ref type="bibr" target="#b218">Nagarajan &amp; Kolter, 2019)</ref>. However, most works demonstrate that mode paths are in fact not linear. While Nagarajan &amp; Kolter (2019) find linear paths between networks, their experimental setup requires initialising models with the same set of weights. <ref type="bibr" target="#b89">Frankle et al. (2020)</ref> and <ref type="bibr" target="#b222">Neyshabur et al. (2020)</ref> demonstrate that this linear mode connectivity phenomenon is closely linked to the Lottery Ticket Hypothesis <ref type="bibr" target="#b88">(Frankle &amp; Carbin, 2019)</ref>, which suggests that only a small subset of randomly initialised weights are the main drivers for the final performance of a model-the so-called winning tickets (see § 3.1). When interpolating between models trained on different tasks but initialised with the same set of weights, the models tend to stay in the same loss basin, indicated by the lack of a sudden increase in loss when interpolating the weights.</p><p>Consequently, it appears that the flatness of the basin of the loss landscape translates to better generalisation capabilities of a model. <ref type="bibr">Gueta et al. (2023)</ref> find that fine-tuned models reside in distinct regions in weight space, and models within the same region exhibit high performance. On the other hand, Ainsworth et al.</p><p>(2022) argue that the success of such interpolation is strongly connected to the inherent bias of the optimiser being used, and not the neural network architecture itself.</p><p>Weight Interpolation Building on the findings of interpolating the weights of models, <ref type="bibr" target="#b11">Ansell et al. (2022)</ref> propose Lottery Ticket Sparse Fine-Tuning (LT-SFT), described in § 3.1. In particular, they identify language, and task-specific sub-networks ϕ l and ϕ t . These can be aggregated by simply adding them to the base model parameters, i.e. θ ′ = θ 0 + ϕ l + ϕ t . Instead of identifying task adaptations on subsets of model parameters, <ref type="bibr" target="#b334">Ilharco et al. (2022)</ref> propose to edit entire models with further arithmetic operations. For example, for toxic language generation and language modelling tasks, by performing the arithmetic negation operation θ ′ = θ 0 + (ϕ general -ϕ toxic ), their new model f θ ′ (x) generates less toxic text. This idea was influenced by the word analogy task (i.e., 'word arithmetics') <ref type="bibr" target="#b209">(Mikolov et al., 2013)</ref>.<ref type="foot" target="#foot_17">foot_17</ref> </p><p>Rather than interpolating sparse adapters, <ref type="bibr" target="#b14">Asai et al. (2022)</ref> aggregate parameters of soft prompts learned via prefix tuning ( § 3.2). In order to generalise to new tasks, (frozen) modules from past tasks and a learnable module created for the new task are interpolated according to the weights of an attention mechanism between the modules and the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Merging</head><p>Mode connectivity has enabled the fusion of entire models without extensive retraining, yielding performance improvements across a range of applications <ref type="bibr" target="#b54">(Choshen et al., 2022;</ref><ref type="bibr" target="#b112">Gupta et al., 2020;</ref><ref type="bibr" target="#b336">Yadav et al., 2023;</ref><ref type="bibr" target="#b144">Jin et al., 2023)</ref>. These developments have made frameworks, such as Git-Theta <ref type="bibr" target="#b146">(Kandpal et al., 2023)</ref>, which facilitate collaborative model development through version control, reasonable. Soft Merging of Experts with Adaptive Routing (SMEAR) <ref type="bibr" target="#b217">(Muqeeth et al., 2023)</ref> introduces gradient-based training for sparsely activated models, offering specialization benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Representation Aggregation</head><p>Closely related to parameter aggregation, representation aggregation consists of interpolating the outputs of individual modules. Crucially, both operations are equivalent if the functions are linear:</p><formula xml:id="formula_41">(α i Φ i + α j Φ j )x = α i Φ i x + α j Φ j x.</formula><p>However, this does not hold true for non-linear functions, e.g. if the module is an adapter layer <ref type="bibr" target="#b129">(Houlsby et al., 2019)</ref> or a feed-forward component of a Transformer layer <ref type="bibr" target="#b82">(Fedus et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Representation Averaging</head><p>At the i-th sub-function of the model, where multiple modules ϕ ∈ M i exist, the representations are passed through the (active) modules, outputting |K i | (latent) representations h 1 , . . . , h |Ki| . One way of performing aggregation is to learn the weights α to interpolate over the hidden representations:</p><formula xml:id="formula_42">f ′ i (x) = |Ki| j α j h j (8)</formula><p>with α j being a module-specific scalar weighting.</p><p>This aggregation is equivalent to Equation ( <ref type="formula" target="#formula_37">6</ref>) when interpreting each weight α j ∈ [0, 1] as the output of a soft router, i.e. α j = r(ϕ j ). Consequently, all soft-learned routing approaches (e.g. MoE) that do not perform top-1 routing (see § 4.2.3) also determine how to aggregate the representations of different modules.</p><p>As an extension to the traditional MoE aggregation/routing function, <ref type="bibr" target="#b194">Ma et al. (2018)</ref> propose to learn one aggregation function per task t in a multi-task setup. <ref type="bibr" target="#b113">Gururangan et al. (2022)</ref>  </p><formula xml:id="formula_43">f ′ i (x) = d∈D p(d | x) f ϕ d (x) (9)</formula><p>This posterior is inferred through the Bayes rule. This does not require any auxiliary model, and only relies on the original d-conditioned language model. In fixed routing, module representations are often averaged without weighting <ref type="bibr">(Zhang et al., 2022a;</ref><ref type="bibr">Chronopoulou et al., 2022a)</ref>. Similarly, in hard routing methods, the representations of all active modules are averaged, such as in Polytropon <ref type="bibr">(Ponti et al., 2022)</ref>, or summed, as in PathNet <ref type="bibr" target="#b84">(Fernando et al., 2017)</ref>. <ref type="foot" target="#foot_18">19</ref>One disadvantage of simply learning gating parameters is that the weights do not depend on the hidden representations. Thus, they do not take into account their information content. This issue is tackled by attention-based aggregation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-Based Representation Aggregation</head><p>Instead of inferring the weighting before a module has performed its transformation on the latent representation, the aggregation decision can take place afterwards. This allows for identifying whether or not the information added by the respective module is ancillary to the target task. In AdapterFusion, <ref type="bibr">Pfeiffer et al. (2021a)</ref> propose an attention mechanism <ref type="bibr" target="#b19">(Bahdanau et al., 2015)</ref> between the stacked hidden representations H i produced by the modules and their input x:</p><formula xml:id="formula_44">f i (x) = Attn(xQ i , H i K i , H i V i )<label>(10)</label></formula><p>where Q, K, V ∈ R d×h are the projections that produce the queries, keys, and values, and x is the input representation to each of the modules (i.e., the output representation of the previous layer). H i ∈ R |M |×d is a matrix consisting of row-wise stacking of the output representations h 1 , . . . , h |Mi| of each module. In other words, the input of each module is interpreted as the query and the output of each module is interpreted as the value and key. The attention mechanism thus learns to attend over the module representations and weigh them according to their relevance for the current task.</p><p>Instead of aggregating module outputs into a single representation, Recurrent Independent Mechanisms <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref> concatenate the outputs of the top-k active modules. However, in between the application of recurrent computation functions, they exploit an attention mechanism over hidden representations to enable sparse communication among modules.</p><p>One major disadvantage of both weighted and attention-based representation averaging, is that-when used in combination with soft routing-they require a full forward pass through all modules, even if they contribute only marginally to the final aggregation. Thus, they incur significant increases in time and space complexity. While this can be mitigated by pruning (i.e., dropping) some modules during inference <ref type="bibr" target="#b258">(Rücklé et al., 2021)</ref>, latency still remains an issue for scalability. Thus, top-k hard routing offers a more efficient solution for both weighted averaging <ref type="bibr" target="#b277">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b168">Lepikhin et al., 2021)</ref> and attention-based aggregation <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Input Aggregation</head><p>Input aggregation lends itself naturally to adapters such as prompts or prefix tuning (Brown et al</p><p>., 2020; Lester et al., 2021; Li &amp; Liang, 2021, see § 3.2). In prompting, we have a set of instructions or few-shot examples ϕ 1 , . . . , ϕ |K| . Given that the nature of prompting is to prepend the prompts to the input, aggregating the respective modules boils down to concatenating all prompts. That is, providing the model with multiple instructions, or with multiple examples (i.e. few-shot in-context learning) is a version of module input aggregation f ′ 1</p><formula xml:id="formula_45">(x) = f θ1 ([ϕ 1 , . . . , ϕ |K| , x]</formula><p>). This concept also extends to prefix-tuning, where we can simply concatenate all prefixes at every layer:</p><formula xml:id="formula_46">f ′ i (x) = f θi ([ϕ 1 i , . . . , ϕ |K| i , x]</formula><p>). In the context of prompting, <ref type="bibr">Schick et al. (2021)</ref> leverage input aggregation by concatenating multiple textual descriptions of undesired behaviours of a language model to generate toxic text for model debiasing. In the context of prompt tuning, <ref type="bibr">Vu et al. (2022b)</ref> learn separate task and language soft prompts that are recombined for zero-shot cross-lingual transfer in summarization. Nayak et al. ( <ref type="formula">2022</ref>) compose soft prompts of attributes and objects in visual tasks to generalise to new classes. Soft prompts can also be aggregated with methods different from concatenation such as attention-based parameter interpolation <ref type="bibr" target="#b14">(Asai et al., 2022)</ref>. Furthermore, input aggregation methods have found significant utility in retrieval augmented generation <ref type="bibr">(Lewis et al., 2020a)</ref>, a technique where retrieval models are employed to retrieve external knowledge for addressing knowledge-intensive NLP tasks. <ref type="foot" target="#foot_19">20</ref> In RAG methods, retrieved documents are appended to the input, essentially aggregating external information with the model's input. These methods facilitate knowledge injection and editing <ref type="bibr" target="#b309">(Verga et al., 2021;</ref><ref type="bibr" target="#b51">Cheng et al., 2023)</ref>, allowing models to access and incorporate information from external sources, which can be crucial for tasks demanding domain-specific knowledge or real-time data updates. This aligns with the broader theme of knowledge enhancement within modular neural architectures, extending their capabilities to handle complex and dynamic information needs.<ref type="foot" target="#foot_20">foot_20</ref> Hypernetworks Similarly to soft prompts, hypernetworks may aggregate information from different embeddings by combining them in the input to the parameter generator. For instance, in <ref type="bibr">(Ponti et al., 2021)</ref> task and language embeddings are concatenated in the input when training a multilingual multi-task architecture where the encoder is fully shared and the hypernetwork generates the classifier head. By recombining embeddings appropriately, this method allows for inferring the parameters of unseen tasklanguage combinations. Combinations of embeddings have been used to generate adapters in multilingual <ref type="bibr">(Üstün et al., 2020)</ref> and multi-task settings <ref type="bibr">(Mahabadi et al., 2021b;</ref><ref type="bibr" target="#b238">Pilault et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Function Aggregation</head><p>Finally, aggregation can be achieved on the function level;</p><formula xml:id="formula_47">f ′ i (x) = f ϕ1 • f ϕ2 (x)</formula><p>. Different aggregation methods infer either a sequence or a (tree) structure that determines the order of the aggregation.</p><p>Sequential Aggregation By performing a forward pass through multiple modules, where the input to the next module is the output of the previous one, the respective hidden representations are sequentially transformed:</p><formula xml:id="formula_48">f ′ i (x) = f ϕ1 (f ϕ2 (. . . (f ϕ |M | (x)))</formula><p>). This form of information aggregation is often chosen in conjunction with fixed routing, as discussed in § 4.1, given that the routing order is determined by the role of each module (e.g. language and task adapters). <ref type="bibr">Pfeiffer et al. (2020b;</ref><ref type="bibr">2021b)</ref> propose a two-stage setup where language-specific components are disentangled from task-specific components, in order to perform zero-shot cross-lingual transfer. First, language (adapter) modules f ϕ ls and f ϕ l t are trained on monolingual unlabelled data for the source language s and the target language t, respectively. Then, in the second stage, the language component f ϕ ls is inserted but frozen, and a new (adapter) module is added for a task f ϕt and trained on annotated data for the source language: f ϕt (f ϕ ls (x)). Since this effectively disentangles language from task information, this also enables zero-shot inference on the target language t without annotated data. In particular, f ϕ ls is substituted with f ϕ l t , thereby hierarchically aggregating the information from the respective modular components: f ϕt (f ϕ l t (x)). <ref type="bibr">Similarly, Stickland et al. (2021)</ref> perform function composition of a language module f ϕ l and a domain module f ϕ d for multilingual multi-domain machine translation. For more examples, see § 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Aggregation</head><p>Alternatively, when global routers jointly determine the selection of modules and the model architecture, the order of function composition follows the structure of a tree. For instance, Neural Module Networks <ref type="bibr">(Andreas et al., 2016b</ref>) leverage a semantic parse to infer a graphical structure for module aggregation. While all leaf nodes find objects by identifying regions of an image through attention, intermediate nodes either transform or combine these representations (depending on the arity of the node). The root then predicts the label by describing or measuring the attended objects.</p><p>• Aggregation functions play a crucial role in combining information from multiple modules in modular neural architectures.</p><p>• Parameter aggregation strategies interpolate weights of multiple modules and are influenced by concepts like linear mode connectivity.</p><p>• Weighted representation averaging and attention-based aggregation are methods for combining the outputs of modules, with attention mechanisms allowing for dynamic weighting based on relevance.</p><p>• Input aggregation methods, such as prompting and prefix tuning, involve concatenating instructions or examples to the input, enabling modular control over tasks and domains.</p><p>• Hypernetworks can also perform input aggregation by combining different embeddings.</p><p>• Function aggregation occurs on the function level and can be sequential or hierarchical, with different methods determining the order of aggregation.</p><p>• Sequential aggregation is often used with fixed routing, while hierarchical aggregation is employed when global routers jointly determine module selection and model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training Setting</head><p>Finally, we explore the training settings for modular architectures. We can identify three main strategies in the literature: 1) all modules are jointly trained for multi-task learning; 2) modules are introduced at different stages during continual learning; and 3) in transfer learning, modules are added post-hoc after pre-training, often as a way to fine-tune the model in a parameter-efficient fashion. Importantly, these strategies are not necessarily mutually exclusive and can be realised in combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Joint Multitask Learning</head><p>In joint multi-task learning, there are two main settings. Firstly, task-specific parameterised components can be integrated into shared neural network architectures as a means to mitigate catastrophic forgetting or negative interference <ref type="bibr" target="#b203">(McCloskey &amp; Cohen, 1989;</ref><ref type="bibr" target="#b91">French, 1999)</ref> and as a way to scale the model size efficiently <ref type="bibr" target="#b158">(Kudugunta et al., 2021)</ref>. In these scenarios, modules are often optimised on individual tasks via fixed routing and specialise accordingly <ref type="bibr" target="#b117">(Hampshire &amp; Waibel, 1992;</ref><ref type="bibr" target="#b249">Rajendran et al., 2017</ref>, inter alia; see § 4.1 for more details). As an alternative, the architecture can be fully modular, sharing only the parameters for learned routing <ref type="bibr">(Jacobs et al., 1991b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b256">Rosenbaum et al., 2018;</ref><ref type="bibr" target="#b155">Kirsch et al., 2018;</ref><ref type="bibr">Chang et al., 2019, inter alia</ref>; see § 4.2.3 for more details).</p><p>Joint training can also be performed before post-hoc training: a shared base model can be pre-trained on multiple tasks as a warm-up before creating task-specific sparse subnetworks <ref type="bibr">(Sun et al., 2020a)</ref> or as a way to provide a useful initialisation for modular parameters <ref type="bibr">(Vu et al., 2022c)</ref>. <ref type="bibr" target="#b76">Dua et al. (2022)</ref> convert a dense language model pre-trained on text data into an MoE by decomposing the learned feed-forward layers. <ref type="bibr">Pfeiffer et al. (2022b)</ref> add language-specific layers during multilingual pre-training of a language model. This prepares the model to be extended to more languages post-hoc; when new languages become available, a new (randomly initialised) learnable layer can be added to the inventory of modules, whereas the shared parameters remain untouched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Continual Learning</head><p>In a similar vein to countering negative interference in multi-task learning, continual learning-that is, continuously integrating new data into the model-often aims at mitigating catastrophic forgetting (i.e., the knowledge learned at early stages of training should not get overwritten by updates to the model later on).</p><p>Similar to the multi-task learning approaches discussed in § 6.1, new layers can be continuously introduced within the network which are only updated on the new data, keeping the others untouched. In methods like Progressive Networks <ref type="bibr" target="#b264">(Rusu et al., 2016)</ref>, PathNet <ref type="bibr" target="#b84">(Fernando et al., 2017)</ref>, and PackNet <ref type="bibr">(Mallya &amp; Lazebnik, 2018)</ref> when the model is trained on a new task, the parameters of the previous tasks are frozen; however, for new tasks, new modules may be learned, which connect to the existing set of modules. Often, the decision of inserting new modules at a given stage is made dynamically based on outlier detection <ref type="bibr" target="#b223">(Ostapenko et al., 2021)</ref>. Progressive Networks <ref type="bibr" target="#b264">(Rusu et al., 2016)</ref>, on the other hand, scale the model capacity linearly with the number of tasks. <ref type="bibr" target="#b5">Aljundi et al. (2017)</ref> train separate experts for every task and route new examples based on the distribution of the reconstruction errors of task-specific auto-encoders.</p><p>Instead of adding new parameters to the model, other works in the continual learning landscape identify subnetworks for different tasks. For instance, some works identify subnetworks of the model, which have not been used by previous tasks. Consequently, updating these parts of the model will have little effect on the previously learned knowledge <ref type="bibr" target="#b142">(Javaloy &amp; Valera, 2022)</ref>. Similarly, 'supermasks' ( §3.1; <ref type="bibr" target="#b333">Wortsman et al., 2020)</ref>, which learn a binary mask over a randomly initialised model, enable the extension to a potentially vast number of tasks during continual learning. Supermasks of previous tasks can be also linearly combined as a way to generalise to new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Parameter-efficient Transfer Learning</head><p>Recently, transfer learning has become the dominant strategy for state-of-the-art results on most tasks. Auxiliary self-supervised objectives are utilised to pre-train models on a large amount of data. Subsequently, the model's weights are fine-tuned on the target tasks <ref type="bibr" target="#b130">(Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b67">Devlin et al., 2019)</ref>. Updating a small set of parameters of these large models has been demonstrated to perform equally well as full model fine-tuning, leading to the emergence of parameter-efficient fine-tuning strategies.</p><p>Most methods discussed in § 3 that are applied to large pre-trained models can be considered as post-hoc adaptation. Modularity can be achieved through parameter composition ( § 3.1) using sparse subnetworks <ref type="bibr" target="#b204">(Mehta, 2019;</ref><ref type="bibr" target="#b46">Chen et al., 2020;</ref><ref type="bibr" target="#b72">Donahue et al., 2014;</ref><ref type="bibr" target="#b41">Cai et al., 2020;</ref><ref type="bibr" target="#b26">Ben Zaken et al., 2022;</ref><ref type="bibr" target="#b109">Guo et al., 2021)</ref>, or low-rank adapters <ref type="bibr" target="#b174">(Li et al., 2018;</ref><ref type="bibr" target="#b132">Hu et al., 2022)</ref>, input composition ( § 3.2) by augmenting the function's input <ref type="bibr" target="#b38">(Brown et al., 2020;</ref><ref type="bibr">Li &amp; Liang, 2021)</ref>, and function composition ( § 3.3) through adapter layers <ref type="bibr" target="#b250">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b129">Houlsby et al., 2019)</ref> and rescaling <ref type="bibr">(Liu et al., 2022b)</ref>. Additionally, hypernetworks can be used to generate the parameters of any of the above-mentioned types of modules ( § 3.4).</p><p>Essentially, all of these methods are tightly connected as they share the same functional form ( § 3.5).</p><p>• There are three main training strategies: (1) Joint Multitask Learning, (2) Continual Learning, and (3) Parameter-efficient Transfer Learning.</p><p>• In Joint Multitask Learning, task-specific components are integrated into shared neural architectures, allowing modules to specialize via fixed or learned routing.</p><p>• Continual Learning methods aim to integrate new data while mitigating catastrophic forgetting, with options to introduce new modules dynamically or identify subnetworks for different tasks.</p><p>• Parameter-efficient Transfer Learning involves pre-training models on large datasets and finetuning on target tasks. Modular strategies can be applied post-hoc through various composition methods, including parameter composition, input composition, and function composition.</p><p>• These training strategies are not mutually exclusive and can be combined to achieve specific goals in modular neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Applications in Transfer Learning</head><p>Most applications of modular deep learning revolve around transfer learning. In particular, the two main purposes are: 1) parameter-efficient fine-tuning ( § 7.1), which achieves superior efficiency, prevents negative interference, and enables zero-shot transfer; and 2) zero/few-shot generalisation to new tasks <ref type="bibr">( § 7.2)</ref>. In what follows, we provide a quick overview of transfer learning applications of modular deep learning. For the in-depth discussions and illustrations of the key concepts, we will first focus on applications in NLP, and then draw direct analogies with other deep learning areas such as speech processing, computer vision, and multi-modal (representation) learning. In § 8, we will explore additional purposes of modular deep learning, including hierarchical reinforcement learning, programme simulation, and causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Parameter-Efficient Fine-tuning</head><p>Regardless of the application area, one of the principal uses of modules has been to boost parameter efficiency and decrease model storage requirements of fine-tuning, eschewing so-called full model fine-tuning which requires storing a separate copy of the full model per task <ref type="bibr" target="#b130">(Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b67">Devlin et al., 2019)</ref>, see §6.3. In the simplest formulation, all task-specific updates are pushed to the parameters of the lightweight modules, while the parameters of the large base model are kept frozen throughout task fine-tuning. The modules then store task-specific knowledge that can be composed with the 'general-purpose' knowledge of the base model to adapt it to the task at hand. In NLP, this led to a number of research papers that introduced diverse modular architectures, as surveyed in § 3 and § 6. A typical evaluation protocol is fine-tuning a type of module on the popular GLUE and SuperGLUE benchmarks <ref type="bibr" target="#b318">(Wang et al., 2019)</ref>, comparing against full model fine-tuning or alternative modular architectures. The results usually corroborate either of two main goals: (i) improving performance with the same parameter budget versus (ii) maintaining performance with a smaller parameter budget <ref type="bibr">(Mahabadi et al., 2021a;</ref><ref type="bibr">Zhou et al., 2023)</ref>. In addition, modular adaptation has further benefits: first, it prevents negative interference between tasks <ref type="bibr" target="#b24">(Bapna &amp; Firat, 2019)</ref>. Second, it allows for combining adapters to enable zero-shot transfer <ref type="bibr">(Pfeiffer et al., 2020b)</ref>. In light of the enormous size of state-of-the-art large language models (LLMs), parameter-efficient fine-tuning has emerged as the main way to update the pretrained models <ref type="bibr" target="#b132">(Hu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Machine Translation</head><p>In the seminal work of <ref type="bibr" target="#b24">Bapna &amp; Firat (2019)</ref>, bilingual (i.e., language-pair) adapters (see §3.3) were used to adapt a massively multilingual NMT model (spanning 103 languages) to a particular source-target translation direction. One benefit of such bilingual adapters is their ability to 'skew' the multilingual model to the language pair at hand without losing the benefits of massively multilingual training for low-resource languages.</p><p>Another positive effect of bilingual adapters concerns recovering the MT performance also for high-resource languages. High-resource languages might typically suffer from performance deterioration due to the particular interference phenomenon known as the 'curse of multilinguality' <ref type="bibr" target="#b58">(Conneau et al., 2020;</ref><ref type="bibr" target="#b322">Wang et al., 2020)</ref>: when (too) many languages compete for the fixed parameter budget of the model, the model's expressiveness and representation power deteriorates for all languages. The use of modules extends the parameter budget to recover the detrimental effects of multilingual inference through dedicated (i.e., modular) bilingual adaptation. Their work also demonstrates the superior performance of a multilingual model specialised towards a particular language pair over merely training a bilingual NMT model for the same pair from scratch.</p><p>However, fine-tuning bilingual adapters (or more generally, modules) for each translation direction assumes parallel data for all language pairs and requires n(n -1) modules to cater for all possible language pairs (one dedicated module in the encoder and another module in the decoder). Therefore, follow-up work <ref type="bibr" target="#b236">(Philip et al., 2020;</ref><ref type="bibr" target="#b303">Üstün et al., 2021)</ref> aimed to learn monolingual (i.e., language-specific) adapters. Again assuming standard encoder-decoder architectures for MT such as mBART <ref type="bibr" target="#b189">(Liu et al., 2020)</ref>, this design requires only 2n modules in total. Besides improving parameter efficiency, this also bypasses the critical dependency on parallel data for all language pairs and allows for learning from monolingual data. Crucially, this design also enables translation to or from languages without parallel data, in a fully unsupervised way, and even to/from languages unseen by the base pre-trained encoder-decoder model. Put simply, when translating from language l s to l t , only the encoder adapters for l s plus the decoder adapters for l t are activated: the model is able to translate from l s to l t without seeing a single parallel l s to l t sentence. This application in the field of NMT exemplifies the power of modular design: available components, which were previously learned locally and asynchronously, can be recombined in novel ways to generalise systematically to unseen applications (i.e., in this particular case, to unseen translation directions). This is one of the main goals of modular deep learning ( § 1).</p><p>The separation into dedicated language-specific modules mitigates interference and catastrophic forgetting; however, it also hinders any positive transfer between modules of similar languages. The positive transfer can be achieved through the use of hypernetworks (see §3.4): <ref type="bibr" target="#b25">Baziotis et al. (2022)</ref> learn to generate monolingual language-specific adapters for NMT. In fact, sharing the parameter generator takes advantage of language similarities <ref type="bibr" target="#b239">(Platanios et al., 2018)</ref>. As discussed in more detail later in §7.1.2, similar ideas of combining the modular design with hypernetworks have also been applied earlier and beyond NMT, e.g., for task fine-tuning with adapters in monolingual multi-task setups <ref type="bibr">(Mahabadi et al., 2021b)</ref> and for cross-lingual transfer in single-task <ref type="bibr" target="#b10">(Ansell et al., 2021)</ref> as well as in multi-task setups <ref type="bibr">(Ponti et al., 2021;</ref><ref type="bibr" target="#b304">Üstün et al., 2022)</ref>.</p><p>The curse of multilinguality and catastrophic interference in multilingual MT models have also been tackled through sparse sub-networks (see § 3.1). Lin et al. ( <ref type="formula">2021</ref>) extract sparse sub-networks for specific language pairs from a trained multilingual MT model via pruning. Subnetworks are then trained separately in order to specialise towards the particular translation direction. In fact, there exist dedicated small sub-networks (which can be obtained via standard masking) that store language pair-specific knowledge within the large network, where such knowledge should not interfere with other language pair-specific sub-networks <ref type="bibr" target="#b76">(Dua et al., 2022)</ref>. The same high-level idea has also been applied to domain adaptation of bilingual MT systems: e.g., <ref type="bibr" target="#b178">Liang et al. (2021)</ref> show that it is possible to learn domain-specific sub-networks when fine-tuning the MT system on new domains, where a single large network (i.e., the full neural MT system) comprises multiple disjoint domain-specific sub-networks specialised to particular domains.</p><p>Another approach that leverages modularity for an increased language-specific capacity in MT is mixtureof-experts. Each expert is typically dedicated to a particular language or translation direction <ref type="bibr" target="#b158">(Kudugunta et al., 2021;</ref><ref type="bibr">Costa-jussà et al., 2022)</ref>. To maintain feasible decoding time, the procedure works as follows:</p><p>(i) during training, mix the inputs from different translation directions in the same batch, in order to learn the routing network and encourage positive transfer among related tasks; (ii) at inference time, different translation directions are decoded separately, and only the corresponding subset for elevant experts is loaded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Cross-Lingual Transfer</head><p>NMT focuses on translation as a single task and modularity was exploited mainly to carve language-specific and/or domain-specific modules that can support multilingual and multi-domain systems, respectively. In more general cross-lingual transfer setups, the aim is to transfer large models <ref type="bibr" target="#b67">(Devlin et al., 2019;</ref><ref type="bibr" target="#b58">Conneau et al., 2020)</ref> fine-tuned for any task (e.g., sequence labelling tasks such as NER, text classification tasks such as NLI, sentiment analysis or intent detection for dialogue systems) on one or more source languages (where such task annotations exist) to one or more target languages <ref type="bibr" target="#b133">(Hu et al., 2020;</ref><ref type="bibr" target="#b263">Ruder et al., 2021)</ref>. Ideally, the transfer should be achieved without fine-tuning the full model <ref type="bibr" target="#b133">(Hu et al., 2020)</ref>, which results in catastrophic forgetting and negative interference, or requires the creation of separate model copies for each task.</p><p>The idea of training language modules thus largely follows what already outlined for MT in §7.1.1, with the addition of another set of dedicated modules that aim to capture task-related knowledge: task modules. Such language modules and task modules can then be combined to 1) favour zero-shot cross-lingual transfer for particular source-target directions <ref type="bibr">(Pfeiffer et al., 2020b;</ref><ref type="bibr" target="#b10">Ansell et al., 2021;</ref><ref type="bibr" target="#b269">2022;</ref><ref type="bibr" target="#b227">Parović et al., 2022)</ref>; 2) provide extra capacity to low-resource languages under-represented (or even not covered) in the large multilingual models such as mBERT or XLM-R <ref type="bibr">(Pfeiffer et al., 2021b;</ref><ref type="bibr">2022b;</ref><ref type="bibr" target="#b243">Ponti et al., 2020;</ref><ref type="bibr" target="#b79">Faisal &amp; Anastasopoulos, 2022)</ref>, independently from task knowledge; and 3) enable handling unseen language-task or even language-domain-task configurations <ref type="bibr">(Ponti et al., 2021;</ref><ref type="bibr">Stickland et al., 2021)</ref>.</p><p>As an example of zero-shot cross-lingual transfer, the original MAD-X framework <ref type="bibr">(Pfeiffer et al., 2020b</ref>, Figure <ref type="figure">1a</ref>) relies on bottleneck adapters to implement language and task modules: In particular: 1) Language modules are inserted into each layer of the original neural model and are fine-tuned on (unsupervised) data of the particular language (e.g., via Masked Language Modelling) while the weights of the original model are kept</p><p>Published in Transactions on Machine Learning Research (11/2023) fixed. 2) After obtaining language modules, task modules are stacked on top of the source language module(s) and are fine-tuned relying on the task objective and task-annotated data in the source language(s), while both the original model and language modules are kept fixed. 3) At inference, source language module(s) are replaced with the desired target language module while retaining the task module: this enables zero-shot task inference in the target language. Recent work has introduced a spectrum of variations and enhancements to this core idea. For instance, inspired by the bilingual 'translation direction' adapters for NMT systems ( §7.1.1), <ref type="bibr" target="#b227">Parović et al. (2022)</ref> learn bilingual adapters instead of single language adapters to boost transfer for a particular language pair. Faisal &amp; Anastasopoulos (2022) and <ref type="bibr">Chronopoulou et al. (2022b)</ref> learn language family adapters to reduce data sparsity for low-resource languages and capitalise on language similarity and cross-language sharing. Stickland et al. ( <ref type="formula">2021</ref>) decouple language and domain knowledge into dedicated modules (see also §7.1.3 later). Further, <ref type="bibr" target="#b11">Ansell et al. (2022)</ref> implement dedicated modules as sparse sub-networks, the so-called language and task masks, which can be composed with the base model via parameter composition.</p><p>Following the analogy between language-specific and bilingual adapters, instead of learning separate language and task subnetworks, Foroutan et al. ( <ref type="formula">2022</ref>) learn dedicated task-language sub-networks, demonstrating the variance in the extracted subnetworks across different task-language combinations. The use of such language sub-networks as language modules, even without dedicated task modules, improves cross-lingual transfer for dependency parsing when used within a meta-learning setup <ref type="bibr">(Choenni et al., 2022)</ref>. <ref type="bibr" target="#b180">Litschko et al. (2022)</ref> compare sparse sub-networks and bottleneck adapters for transferring ranking functions for information retrieval tasks across languages and find them both superior to full model fine-tuning.</p><p>Finally, a body of work again focuses on 'contextually generating' the modules via hypernetworks, aiming to increase efficiency and benefit from connections between different languages and tasks. A representative example is the Hyper-X framework <ref type="bibr">(Üstün et al., 2022)</ref> provided in Figure <ref type="figure" target="#fig_10">6</ref>, where the module parameter generation is conditioned on the (disentangled) task and language, and additionally on the index of the Transformer layer where the generated module is inserted. Each task and language are parameterised via separate embeddings, which enables adaptation to any tasklanguage combination, where these embeddings are low-dimensional vectors which are learned together with the parameters of the hypernetwork (see Figure <ref type="figure" target="#fig_10">6</ref> again). The framework thus leverages supervision and positive transfer from both multiple tasks and languages. Hyper-X can be seen as a more general variant of a series of precursors backed by the idea of contextual generation: Ponti et al. ( <ref type="formula">2021</ref>) condition the hypernetwork on both task and language embeddings but generates only the model's classifier head. Other methods generate modules but condition the hypernetwork only on tasks in a monolingual setup <ref type="bibr">(Mahabadi et al., 2021b)</ref> or only on languages in a cross-lingual transfer setup <ref type="bibr" target="#b302">(Üstün et al., 2020;</ref><ref type="bibr" target="#b10">Ansell et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Domain Adaptation</head><p>As already hinted at in §7.1.1 and §7.1.2, dealing with different domains adds another tier to the modular design: domain-specific knowledge might be captured within dedicated domain modules. <ref type="foot" target="#foot_21">22</ref> This can again be accomplished through similar modular architectures as with language and task adapters. For instance, it is possible to inject domain-specific knowledge into (bottleneck) adapters <ref type="bibr" target="#b346">(Zhang et al., 2021;</ref><ref type="bibr">Chronopoulou et al., 2022a)</ref> or to extract sparse domain-specific or task-specific sub-networks <ref type="bibr" target="#b298">(Thompson et al., 2018;</ref><ref type="bibr">Ke et al., 2021b)</ref> for multi-domain and multi-task learning. Mixture-of-experts also enable multi-domain joint learning as well as domain adaptation <ref type="bibr" target="#b110">(Guo et al., 2018;</ref><ref type="bibr" target="#b351">Zhong et al., 2022)</ref>. Similar strategies have also been used in multi-domain and cross-domain speech processing and computer vision applications (see §7.1.5 and §7.1.6 later).</p><p>In domain adaptation, it is common to combine both shared parameters and domain modules that are learned jointly <ref type="bibr" target="#b36">(Bousmalis et al., 2016)</ref>. Beyond this standard setting, many approaches employ additional regularisation terms. The most common are 1) a domain-adversarial loss on the shared parameters in order to encourage them to be domain-invariant <ref type="bibr" target="#b92">(Ganin et al., 2016;</ref><ref type="bibr" target="#b49">Chen &amp; Cardie, 2018)</ref>; 2) an orthogonality constraint on the domain modules to ensure that they capture different information <ref type="bibr" target="#b21">(Baktashmotlagh et al., 2013;</ref><ref type="bibr" target="#b153">Kim et al., 2017)</ref>; and 3) similarity constraints that bring representations of similar modules close together <ref type="bibr" target="#b36">(Bousmalis et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4">Knowledge Injection</head><p>Naturally, dedicated modules can also be assigned to inject and store external knowledge (e.g., from manually curated external knowledge bases), which can then interact with language, domain, or task knowledge. This idea has been explored with diverse external knowledge sources. For instance, <ref type="bibr" target="#b164">Lauscher et al. (2020)</ref> aimed at complementing the distributional knowledge of large language models with conceptual and commonsense knowledge from ConceptNet <ref type="bibr" target="#b283">(Speer et al., 2017)</ref>. The external knowledge was captured within dedicated bottleneck adapters: they were fine-tuned via language modelling on synthetically created sentences from random walks over the ConceptNet graph structures. <ref type="bibr" target="#b199">Majewska et al. (2021)</ref> stored verb-related knowledge from VerbNet <ref type="bibr" target="#b275">(Schuler, 2005)</ref>, a human-created verb classification repository, into bottleneck adapters, and demonstrated its usefulness in a range of tasks that require understanding of verb semantics. Along similar lines, <ref type="bibr">Wang et al. (2021a)</ref> offered a generalisation of these approaches where different knowledge sources (e.g., Wikipedia, WikiData) are mapped to different dedicated adapters, which can be aggregated according to the task at hand. The same idea has been explored by <ref type="bibr" target="#b191">Lu et al. (2021)</ref> in the biomedical domain, where the main knowledge sources were the UMLS Metathesaurus graph <ref type="bibr" target="#b34">(Bodenreider, 2004)</ref> and biomedical Wikipedia articles. <ref type="bibr" target="#b191">Lu et al. (2021)</ref> also introduce another component, the so-called knowledge controller, which can be seen as a standard attention-based function aggregator from §5.4. As an example of another relevant application, <ref type="bibr" target="#b165">Lauscher et al. (2021)</ref> learned bottleneck adapters without manually curated external data, with the focus on model debiasing: the debiasing adapters were fine-tuned via standard language modelling on a counterfactually augmented corpus.</p><p>Finally, the idea of modular knowledge injection is also directly linked to the retrieval-augmented language models in text-only settings <ref type="bibr">(Lewis et al., 2020b)</ref> as well as in multi-modal settings <ref type="bibr" target="#b340">(Yasunaga et al., 2023)</ref> where the standalone retrieval module, detached from the 'main' language model, is responsible to fetch knowledge from some external memory or a knowledge base, and that knowledge is then used to condition the language model. In this design, the retrieval step and capability is made explicit and decoupled from the language model generation capability: as such, one can work directly on a retrieval module without the need to change the other components of the entire model <ref type="bibr" target="#b344">(Yu et al., 2023)</ref>. The ability of standard language models to use external tools is also sparked by the modular design: different external tools specialised for performing particular functions (e.g., conducting Web search, performing mathematical operations) are stored as separate modules accessed from the main model via external API calls. For a comprehensive overview of such augmented language models, we refer the reader to the recent survey <ref type="bibr">Mialon et al. (2023)</ref>. The use of modular deep learning for speech processing applications closely matches the ideas already exposed for NLP tasks. The landscape of the possible modular designs is exactly the same, where the only crucial differences are (i) the choice of the underlying large model, and (ii) the corresponding objective functions used to inject the specialised knowledge into the modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.5">Speech Processing</head><formula xml:id="formula_49">Q K V f( ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CTC( ) . x12</head><p>For instance, the typical choice of the base model for automatic speech recognition (ASR) applications is one from the wav2vec family <ref type="bibr" target="#b18">(Baevski et al., 2020;</ref><ref type="bibr" target="#b16">Babu et al., 2022)</ref>, while the ASR-oriented objective function is the standard Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b103">(Graves et al., 2006)</ref>. The high-level modular structure remains the same, as illustrated in Figure <ref type="figure" target="#fig_11">7</ref> with an example from <ref type="bibr" target="#b297">Thomas et al. (2022)</ref>, which utilises standard bottleneck adapters.</p><p>While in theory a large variety of possible modular configurations from § 3- § 6 can be applied to diverse speech processing tasks, the majority of current work in the area has indeed focused on the use of bottleneck (sequentially placed) adapters for ASR in monolingual and multilingual contexts. Before that, the concept of modularity can be traced to the work of <ref type="bibr" target="#b296">Swietojanski et al. (2016)</ref>, where the model re-weights hidden units using small amounts of unsupervised data to better adapt to a particular speaker or an environment. More recently, bottleneck adapters have been used to perform ASR adaptation to atypical and accented speech <ref type="bibr" target="#b299">(Tomanek et al., 2021)</ref>, unseen speakers with limited adaptation data <ref type="bibr" target="#b319">(Wang &amp; Van hamme, 2022;</ref><ref type="bibr">Eeckt &amp; Van hamme, 2022;</ref><ref type="bibr" target="#b50">Chen et al., 2023)</ref>, new domains and manners of speaking (e.g., children's speech) <ref type="bibr">(Fan &amp; Alwan, 2022;</ref><ref type="bibr" target="#b357">Zhu et al., 2022)</ref>, or to perform further model customisation to specific speakers <ref type="bibr" target="#b32">(Biadsy et al., 2022;</ref><ref type="bibr" target="#b268">Sathyendra et al., 2022)</ref> and for multilingual learning <ref type="bibr" target="#b147">(Kannan et al., 2019;</ref><ref type="bibr" target="#b128">Hou et al., 2022)</ref>. A notable exception, not resorting to adapter layers, is the method of (Winata et al., 2020) which aims to learn low-rank modules ( § 3.1), akin to the idea of LoRA <ref type="bibr" target="#b132">(Hu et al., 2022)</ref>, for end-to-end ASR.</p><p>Multi-task (where the term 'task' in this context can e.g. refer to different languages, domains, speakers, or accents) ASR setups have also witnessed the usage of mixture-of-experts, closely following the basic ideas already discussed for NMT ( §7.1.1) where different languages are assigned their dedicated modules through fixed routing. For instance, in speech processing, MoEs have been applied to multilingual ASR and cross-lingual ASR transfer <ref type="bibr" target="#b20">(Bai et al., 2022;</ref><ref type="bibr" target="#b97">Gaur et al., 2021;</ref><ref type="bibr" target="#b160">Kumatani et al., 2021)</ref>, while <ref type="bibr" target="#b342">You et al. (2022)</ref> propose MoE for ASR with learned routing.</p><p>Beyond ASR, bottleneck adapters have also been used for speech translation <ref type="bibr" target="#b166">(Le et al., 2021)</ref>. Most recently, modular adapter-based approaches have been applied to text-to-speech methods (TTS) <ref type="bibr" target="#b131">(Hsieh et al., 2022;</ref><ref type="bibr" target="#b215">Morioka et al., 2022)</ref>, aiming to extend standard large multi-speaker TTS models such as FastPitch <ref type="bibr" target="#b162">(Lancucki, 2021)</ref> to new speakers without compromising the TTS quality for the seen speakers. From a high-level perspective, one can see a direct analogy of this goal to the objectives in the MT literature of extending multilingual MT systems to unseen languages without compromising seen languages (see §7.1.1 again).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.6">Computer Vision and Cross-Modal Learning</head><p>In computer vision, similar to NLP and speech processing ( §7.1.5), dedicated modules are again used to enable parameter-efficient fine-tuning across multiple tasks and domains <ref type="bibr" target="#b264">(Rusu et al., 2016;</ref><ref type="bibr" target="#b251">Rebuffi et al., 2018;</ref><ref type="bibr" target="#b30">Berriel et al., 2019;</ref><ref type="bibr">He et al., 2022b, among others)</ref>. The core difference, again, is the choice of the actual neural architecture for the underlying model as well as for the modules: e.g., residual adapters <ref type="bibr" target="#b250">(Rebuffi et al., 2017)</ref> consisted of simple 1 × 1 convolutions combined with the base ResNet neural model <ref type="bibr" target="#b123">(He et al., 2016)</ref> while other work learned task-specific convolutional filters <ref type="bibr" target="#b221">(Newell et al., 2019;</ref><ref type="bibr" target="#b37">Bragman et al., 2019)</ref>. More recent work aims to exploit modular architectures from NLP (e.g., sequential or parallel adapters, LoRA, prefix tuning) with pretrained Vision Transformer (ViT) architectures <ref type="bibr" target="#b73">(Dosovitskiy et al., 2021</ref>): e.g., <ref type="bibr">He et al. (2022b)</ref> run a comparative empirical analysis of various modular architectures for vision tasks, while <ref type="bibr" target="#b47">Chen et al. (2021)</ref> rely on sparse sub-networks.</p><p>Modular design lends itself naturally to cross-modal and multi-modal applications, where different modalities may be captured by modality-specific parameters and routing can also be modality-conditioned. For instance, in multilingual vision-and-language (V&amp;L) settings, it is possible to conduct inference in languages that lack labelled task examples. In fact, language knowledge is again disentangled from the task and modality knowledge, and the knowledge for different input modality streams can be captured in dedicated modules. This idea has been heavily explored in recent work in multi-modal multi-task scenarios, both in monolingual <ref type="bibr" target="#b293">(Sung et al., 2022)</ref> and multilingual contexts <ref type="bibr" target="#b39">(Bugliarello et al., 2022;</ref><ref type="bibr">Pfeiffer et al., 2022a)</ref>, for tasks such as image captioning <ref type="bibr">(Zhou et al., 2022a;</ref><ref type="bibr">Gao et al., 2021a</ref>), text-to-image generation <ref type="bibr" target="#b198">(Maharana et al., 2022)</ref>, visual question answering <ref type="bibr">(Liu et al., 2022a;</ref><ref type="bibr" target="#b293">Sung et al., 2022)</ref>, visual reasoning <ref type="bibr">(Liu et al., 2021a)</ref>, etc. For instance, Flamingo <ref type="bibr">(Alayrac et al., 2022)</ref> uses frozen pretrained vision and language models, and only trains adapter layers to handle sequences of arbitrarily interleaved visual and textual data. It is trained with a sequence modelling objective on Web-scale data <ref type="bibr" target="#b175">(Li et al., 2021)</ref> and displays impressive zero-shot and few-shot capabilities. <ref type="bibr">Pfeiffer et al. (2022a)</ref> use adapter modules to equip multilingual text-only models with the ability to also process the visual modality, as well as to equip monolingual multi-modal models to deal with input from multiple languages. <ref type="bibr">Papalampidi &amp; Lapata (2022)</ref> rely on hierarchical adapters (akin to hierarchical representation aggregation discussed in § 5) for the task of summarising long videos into textual descriptions. <ref type="bibr" target="#b224">Pan et al. (2022)</ref> demonstrate that modular design also helps in image-to-video transfer tasks: they use adapter modules to equip a large image-based model without temporal knowledge with the ability to reason about dynamic video content.</p><p>We note that in this survey, we aim to list some exemplary applications and draw parallels between different yet similar application areas such as NLP, speech processing, and computer vision. While we acknowledge that there exists a wealth of other work in these areas, we have no pretence of exhaustiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.7">Comparison and Design Principles</head><p>While a full-fledged comprehensive empirical study of the plethora of modular architectures across various application tasks and areas is still lacking, there exist initiatives such as the publicly available AdapterHub platform <ref type="bibr">(Pfeiffer et al., 2020a)</ref>: it provides (re)implementations of representative modular NLP architectures, within a unified framework tied to HuggingFace Transformers <ref type="bibr" target="#b332">(Wolf et al., 2020)</ref>. Among others, AdapterHub includes representatives of each computation method in § 3: LoRA <ref type="bibr">(Hu et al., 2022) (i.e., low-rank parameter composition)</ref>, prefix tuning of <ref type="bibr">Li &amp; Liang (2021)</ref> (input composition) and a number of bottleneck adapter configurations (function composition). The existence of AdapterHub delineates another crucial advantage of modularity: reusability of existing, already fine-tuned modules which can be (re)combined with the large neural models. In short, any practitioner can share or reuse a module specialised for a particular purpose (e.g., capturing specific task or language knowledge) with the community, facilitating community-wide sharing and thus avoiding time-and energy-costly repetitions of the same fine-tuning procedure. <ref type="foot" target="#foot_22">23</ref> As discussed in § 4, one can observe initiatives such as AdapterHub as continuously updating community-distributed multi-task models.</p><p>The discussion in this section also points to a more general principle: different end-goals even within the same end-application (e.g., NMT, cross-lingual transfer, domain adaptation) require rethinking the actual modular design, and the desired level and nature of modularity. For instance, if the goal in NMT (or cross-lingual transfer) is to boost performance for a particular translation or transfer direction, it might be useful to trade off some modularity for a better final performance by replacing language-specific monolingual modules with bilingual modules <ref type="bibr" target="#b24">(Bapna &amp; Firat, 2019;</ref><ref type="bibr" target="#b227">Parović et al., 2022)</ref>. On the other hand, if the goal is to enable zero-shot or few-shot translation or transfer, the design with monolingual modules might be a better choice. In another example, if the focus is on MT or transfer for a particular low-resource language, the model designer should enable positive transfer to that language by 'opening' the flow of information from a module storing knowledge on high-resource languages similar to the target language if such languages exist (e.g., from Spanish to Galician) <ref type="bibr">(Üstün et al., 2021)</ref>, or by learning modules for families or groups of similar languages <ref type="bibr">(Chronopoulou et al., 2022b;</ref><ref type="bibr" target="#b79">Faisal &amp; Anastasopoulos, 2022)</ref>. Analogously, related domains can also be grouped and hierarchically organised to enable positive transfer for domain adaptation <ref type="bibr">(Chronopoulou et al., 2022a)</ref>.</p><p>Other practical desiderata may also influence the selection of the actual modular design. If the final task performance is paramount, larger modules might be preferred, e.g., in order to offer enough extra capacity to store the wealth of language-specific information <ref type="bibr" target="#b11">(Ansell et al., 2022)</ref>. However, if model compactness is paramount, the criterion for choosing a specific design is instead the trade-off between efficiency (in terms of parameters and/or train and test time) and task performance; the optimisation of this trade-off has been the focus of recent research <ref type="bibr" target="#b258">(Rücklé et al., 2021;</ref><ref type="bibr">Mahabadi et al., 2021a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b290">Sun et al., 2022)</ref>. In another example, if time efficiency during inference is a crucial requirement (e.g., real-time ASR in dialogue systems, low latency for information search systems) parameter composition methods such as sparse subnetworks or low-rank composition methods may be preferred over function composition methods as the latter increase the number of computations required during the forward pass, (see Table <ref type="table">3</ref>). In yet another example, if storage requirements are a critical constraint, one cannot resort to huge mixture-of-expert models where billions of parameters must be stored <ref type="bibr" target="#b168">(Lepikhin et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Task Generalisation</head><p>The diverse applications of modular deep learning covered so far almost exclusively focus on learning modules associated with (arguably) well-formed and interpretable 'units of knowledge' such as languages, tasks, domains, dialects, accents, and speakers. However, modularity might also be achieved when such units are unknown. This relies on jointly learning arbitrarily sized inventories of so-called latent skills and a learned routing function ( § 4.2). Since such skills are learned end-to-end on a mixture of data from multiple tasks, they are often not straightforwardly interpretable. On the other hand, since arbitrary subsets of skills can be combined and each skill can be updated locally, these modular neural architectures are ideal for systematic generalisation to new tasks <ref type="bibr">(Zhang et al., 2022a;</ref><ref type="bibr">Ponti et al., 2022)</ref>.</p><p>In fact, another fundamental application in transfer learning is achieving zero-shot or few-shot generalisation to new tasks, where test examples are not i.i.d. with respect to training examples. The general experimental setup involves disjoint sets of training and test tasks. A model is pre-trained through multi-task learning on training tasks and then adapted to each new test task based on zero or few data points. Common examples of evaluation benchmarks for this setting include CrossFit <ref type="bibr" target="#b341">(Ye et al., 2021)</ref>, the T0 task suite <ref type="bibr" target="#b267">(Sanh et al., 2022)</ref>, or Natural Instructions <ref type="bibr" target="#b210">(Mishra et al., 2022)</ref>. While a common strategy to tackle this problem is instruction tuning <ref type="bibr" target="#b267">(Sanh et al., 2022;</ref><ref type="bibr">Wei et al., 2022a)</ref>, where models are fine-tuned prepending the instructions for each task, modular deep learning has emerged as a strong contender <ref type="bibr" target="#b4">(Alet et al., 2018;</ref><ref type="bibr" target="#b158">Kudugunta et al., 2021;</ref><ref type="bibr">Ponti et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Other Purposes of Modularity</head><p>In addition to scaling large models (for instance, through MoEs, as discussed in § 4.2.3) and facilitating transfer learning, which we covered in § 7, modularity serves multiple additional purposes. In particular, we devote this section to a cursory view of modularity for i) hierarchical reinforcement learning ( § 8.1); ii) neural programme simulation ( § 8.2); iii) neural causal inference ( § 8.3). While most of these applications predate the advent of neural networks, (modular) deep learning expands the scope and potential of these lines of research for a series of reasons. First, it holds promise to induce the relevant latent structures (such as options, programmes, or causal graphs, respectively) in an end-to-end fashion. Second, it marries these traditional problems with the ability to jointly model low-level perception, such as vision and language.  <ref type="bibr" target="#b9">(Andreas et al., 2017)</ref>. Two high-level policies Π corresponding to task instructions τ are illustrated. Each iteratively selects low-level policies π (options) corresponding to sub-tasks b from a shared inventory. These determine the choice of action given observations. In this case, options are implemented as predicate-argument pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Hierarchical Reinforcement Learning</head><p>The goal of reinforcement learning is to learn a policy, which predicts the next action of an agent based on past observation(s) from the environment, that maximises the return, i.e. the sum of future discounted rewards. However, many tasks span extremely dilated temporal horizons or provide only highly sparse or delayed rewards. In these cases, it becomes helpful to model intermediate abstractions between high-level goal specifications and low-level actions and observations <ref type="bibr" target="#b295">(Sutton et al., 1999;</ref><ref type="bibr" target="#b245">Precup, 2000)</ref>. This facilitates the planning abilities of the agent as well as their sample efficiency. In fact, the above-mentioned intermediate abstractions, known as options or skills, consist of sub-policies that are transferable across tasks.</p><p>More formally, each reinforcement learning task is a Markov Decision Process (MDP) consisting of states S and actions A, a transition function p : S × A × S → [0, 1] and a reward function r : S × A → R. We aim to learn a policy π : S × A → [0, 1]. We also define a value function as the expected (discounted) return from a given state s as</p><formula xml:id="formula_50">V π (s) = E π [ ∞ t=0 γ t r t+1 | s 0 = s],</formula><p>as well as a Q function from a state s and an action <ref type="bibr" target="#b295">Sutton et al. (1999)</ref> and <ref type="bibr" target="#b245">Precup (2000)</ref>, each option ω ∈ Ω is defined as a tuple (I ω , π ω , β ω ), where I ω ⊆ S is the initiation set, π ω : S × Ω → [0, 1] the option-specific policy, and β ω : S → [0, 1] is the termination function. For simplicity, many works assume that ∀s ∈ S, ∀ω ∈ Ω, s ∈ I ω : in other words, all options are available at every state. Augmenting a task with options transforms it into a Semi-MDP, with corresponding functions V Ω (ω) and Q Ω (s, ω).</p><formula xml:id="formula_51">a as Q π (s, a) = E π [ ∞ t=0 γ t r t+1 | s 0 = s, a 0 = a]. Following</formula><p>Learning options involves a series of challenges <ref type="bibr" target="#b143">(Jiang et al., 2019)</ref>. Firstly, it is not trivial to specialise sub-policies towards distinct behaviours. This shortcoming is common to many modular architectures with learned routing <ref type="bibr">(Mittal et al., 2022, see § 4.2)</ref>. Not only this, the problem of hard learned routing has often been cast in a reinforcement learning framework ( § 4.2.2). Secondly, one must define the space where the actions of the high-level policy, which are latent variables, lie. In practice, one could treat them as a discrete, unordered set. In this case, a module from an inventory is chosen for a certain amount of time steps. However, alternative methods operate in structured spaces such as language, which is more transferable and scalable due to its combinatorial nature. Thirdly, training multiple options dilates the training time and requires collecting an appropriate amount of experiences for each of them. Fourthly, if trained jointly, options change simultaneously with the master policy, which is a source of non-stationarity. As a consequence, previous experiences for the master policy become invalid if the options have been updated in the meantime. Again, this is reminiscent of the challenges of learned routing exposed in § 4.2.</p><p>The simplest solution to circumvent end-to-end joint learning of the master policy and options is to provide separate supervision to both <ref type="bibr" target="#b295">(Sutton et al., 1999;</ref><ref type="bibr" target="#b64">Dayan &amp; Hinton, 1992)</ref>. However, this may require extensive annotation, which is often not available. Thus, an alternative method is defining sub-goals, i.e. states an agent should reach as a stepping stone towards the high-level goal Dietterich <ref type="bibr">(2000)</ref>. Nevertheless, this similarly fails to scale due to the exponentially growing number of combinations of sub-goals some tasks may entail. Moreover, this does not eschew the need to train individual sub-policies for each sub-goal. A partial remedy is offered by hindsight learning, where an off-policy correction is introduced <ref type="bibr">(Nachum et al., 2018)</ref>. Specifically, the original target sub-goal of the current option is substituted with the one maximising the probability of the past sequence of low-level actions. Similarly, the master policy can be trained in hindsight through the currently predicted sequence of high-level sub-goals. Overall, relabelling past experiences significantly improves the model's sample efficiency.</p><p>A more radical solution to the challenge of scalability is jointly training both the master policy and options in an end-to-end fashion. To this end, <ref type="bibr" target="#b17">Bacon et al. (2017)</ref> put forth a new architecture, the option-critic, that discovers options from data, without supervision for the intermediate abstractions. This architecture is trained based on policy gradient theorems <ref type="bibr" target="#b17">Bacon et al. (2017)</ref> derive for options. Moreover, they augment the set of actions A available to each policy π ω with a special end-of-policy action eop instead of explicitly modelling β ω . Intuitively, formulating the execution as call-and-return, a master policy π Ω determines the active option ω, whose policy π ω is followed until the eop action is chosen. At this point, control returns to the master policy to choose the next option, and so on until termination. A downside of this method is that it is unstable and often diverges to degenerate solutions <ref type="bibr" target="#b143">(Jiang et al., 2019)</ref>. Thus, several inductive biases have been proposed to correct it. A popular method is leveraging intrinsic rewards: an auxiliary loss diversifies options by maximising the mutual information between each option and the next state conditioned on the current state <ref type="bibr" target="#b85">(Florensa et al., 2017;</ref><ref type="bibr" target="#b159">Kulkarni et al., 2016</ref>).</p><p>An orthogonal question revolves around the ideal space for the option variables. In fact, compared to a discrete, unordered inventory of (possibly hard-coded) options, language affords more flexibility <ref type="bibr" target="#b9">(Andreas et al., 2017;</ref><ref type="bibr" target="#b143">Jiang et al., 2019)</ref> as it solves many of the above-mentioned challenges. In fact, all sub-policies can be implemented through a single model conditioned on the linguistic label of the current option. This not only allows options to borrow statistical strength from each other but also makes options reusable in new tasks. Moreover, the nature of language (through its infinite use of finite means) is suitable to capture the extremely complex combination of sub-goals of many reinforcement learning tasks. Note that linguistic options can be interpreted as a generalisation of sub-goals, as every instruction implicitly corresponds to a subset of states <ref type="bibr" target="#b143">(Jiang et al., 2019)</ref>.</p><p>In practice, to learn linguistic options, <ref type="bibr" target="#b9">Andreas et al. (2017)</ref> assumes that 'sketches' of options are provided for supervision (see Figure <ref type="figure" target="#fig_12">8</ref>). To induce them, subsequent methods rely instead on synthetic experiences through relabelling <ref type="bibr" target="#b143">(Jiang et al., 2019)</ref>, or restricted vocabularies and syntax such as predicate-argument pairs <ref type="bibr" target="#b63">(Das et al., 2018)</ref>. Recently, the master policy has been frequently implemented as a large language model. Since these are pre-trained on text, they already contain world knowledge that can serve as a powerful inductive bias for grounded learning. For instance, <ref type="bibr" target="#b135">Huang et al. (2022)</ref> use frozen language models to generate options through prompting in a zero-shot fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Programme Simulation</head><p>Another distinct purpose of modular architectures is to model programmes, as a means to induce them from data or to simulate symbolic algorithms. The simplest (and least expressive) family of programmes are Finite State Automata (FSA). These receive a neural implementation in the Compositional Recursive Learner (CRL; <ref type="bibr" target="#b45">Chang et al., 2019)</ref>, similarly to Routing Networks <ref type="bibr" target="#b256">(Rosenbaum et al., 2018)</ref> and Modular Networks <ref type="bibr" target="#b155">(Kirsch et al., 2018)</ref>. In these neural architectures, a loose equivalence can be drawn as follows: transformations induced by modules are transition functions (arcs in the graph), input and output representations are the states (nodes in the graph), and the input is the starting state. A memoryless routing function selects the transition based on the current state. Thus, the programme graph is constructed dynamically. The final states are defined as those reached after the router selects a special end-of-computation action.  <ref type="bibr" target="#b105">(Graves et al., 2016)</ref>. A recurrent neural controller iteratively receives an input from the environment, writes to / reads from memory, and produces an output. Read and write operations are based on attention between generated keys and memory entries. A special mechanism keeps track of memory usage and temporal links between entries.</p><p>On the other hand, a programme graph can be constructed globally based on the task description before processing the data. In particular, Neural Module Networks (NMNs; <ref type="bibr">Andreas et al., 2016b;</ref><ref type="bibr">a)</ref> rely on an off-the-shelf semantic parser (and custom rules) to map a query in natural language into a tree graph. The nodes of this graph are learnable modules characterised by: 1) their types of input (raw image features and/or attention scores) and output (attention scores or labels); and 2) the particular instances of a type, indicated as an argument in the form of a natural language string. For instance, the module find[cat] takes an image and returns attention scores over the regions that contain cats. Compositionality is achieved by sharing weights across modules with the same type or instance. NMNs have been further extended to be amenable to end-to-end training without the aid of an external parser <ref type="bibr" target="#b134">(Hu et al., 2017)</ref>. In this case, the mapping from queries to programme graphs is learned by imitating expert demonstrations while the module parameters are learned based on the downstream loss of visual question answering.</p><p>In addition to the routing function and computation functions, a model can be extended with an external memory. In fact, these three mirror the fundamental components of a computer architecture: elementary operations, logical flow control, and a random-access memory that can be read and written to <ref type="bibr" target="#b312">(von Neumann, 1945;</ref><ref type="bibr" target="#b104">Graves et al., 2014)</ref>. While (appropriately wired) recurrent neural networks have been shown to be Turing-complete <ref type="bibr" target="#b281">(Siegelmann &amp; Sontag, 1995)</ref>, separating the three functions into distinct components provides an inductive bias to simulate the workflow or a computer programme. Neural Turing Machines (NTMs; <ref type="bibr" target="#b104">Graves et al., 2014)</ref> introduced a fully differentiable read-write memory matrix that interfaces with the main recurrent network through an attentional mechanism. In particular, this memory can be addressed both based on content (i.e., the match between its entries and the current input) and based on location, in order to store and retrieve temporally ordered information in contiguous entries. NTMs were further extended into the Differentiable Neural Computer (DNCs; <ref type="bibr">Graves et al., 2016, Figure 9)</ref>, which amended some of the limitations of NMTs, such as avoiding interference in the memory, freeing up previously written locations, and storing temporally ordered sequences in non-contiguous chunks. Another family of memory-augmented methods include the Neural Programmer Interpreter (NPI; <ref type="bibr" target="#b252">Reed &amp; de Freitas, 2016)</ref>. This model is trained with full supervision from execution traces or through reinforcement learning <ref type="bibr" target="#b237">(Pierrot et al., 2019)</ref>. In particular, a core recurrent network receives information from a programme module, as well as representations from the environment module. In its output, it produces the index for the next sub-programme and its arguments (as well as a special termination symbol). Finally, a recent thread of research focused on simulating the behaviour of symbolic algorithms with vanilla (non-modular) neural networks. An example is neural algorithmic reasoning <ref type="bibr" target="#b308">(Veličković &amp; Blundell, 2021)</ref>. First, a processor network is trained to emulate the output of a symbolic programme (e.g., Dijkstra's algorithm for shortest paths) that operates on abstract representations (e.g., weighted graphs). Second, encoder and decoder networks can be trained to operate on sensory real-world data while matching the input-output types expected by the processor network.</p><p>Among the main applications for programme simulations are settings where sub-problems are shared, such as multi-task or curriculum learning. By distilling the most common functionalities into modules, these can be reused to generalise compositionally to new sequences of sub-tasks. Another application is compositional reasoning, such as (visual) question answering <ref type="bibr">(Andreas et al., 2016b;</ref><ref type="bibr">a)</ref>. In general, external memory is useful for reasoning over complex data structures, such as graphs <ref type="bibr" target="#b104">(Graves et al., 2014;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b252">Reed &amp; de Freitas, 2016)</ref>. Finally, neural models can emulate symbolic algorithms to extend their capabilities to operate on sensory real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Causal Discovery and Inference</head><p>Modularity in the design of a model may be assumed to reflect the modularity in the (physical) mechanisms of the world. In fact, a crucial assumption in causal inference <ref type="bibr" target="#b273">(Schölkopf et al., 2012)</ref> is that such mechanisms underlying data generation are independent, as they do not influence each other, and reusable, as they may play a role in multiple distributions. Consequently, if one of the mechanisms, which defines a conditional distribution in the model graph, changes-possibly because of an intervention-the other modules remain invariant. If a machine learning model mirrors this modular structure, it is better suited to generalise in a sample-efficient way to new tasks: in fact, local distribution shifts require updating only the corresponding module parameters, which in turn results in faster adaptation <ref type="bibr" target="#b29">(Bengio et al., 2020;</ref><ref type="bibr">Mittal et al., 2022)</ref>.</p><p>The key challenge for this problem is how to specialise each module towards a specific mechanism based uniquely on observational data, especially when the number and nature of the mechanisms are unknown. Competition among the modules through top-k routing (see § 4.2.2) is a common feature of many proposed solutions. 24 Parascandolo et al. (2018) show how to invert causal independent mechanisms through a modular neural architecture, given data from the original distribution and an unlabelled mixture of their transformations (see Figure <ref type="figure" target="#fig_14">10</ref>). Their model consists of a mixture of experts and an adversarial discriminator, which enforces that the inverted transformation lies in the support of the original distribution. Another architecture relying on module competition and capable of modelling sequential data is Recurrent Independent Mechanisms (RIMs; <ref type="bibr" target="#b102">Goyal et al., 2021)</ref>. Here, the modules are recurrent networks with separate parameters, each representing a different transition dynamics. However, their states are not entirely independent, as active modules are allowed to communicate through attention. This reflects a second assumption, namely that the dependencies among variables are highly sparse <ref type="bibr">(Mittal et al., 2022)</ref>. Attention can also serve to direct the flow of bottom-up and top-down information <ref type="bibr" target="#b212">(Mittal et al., 2020)</ref>.</p><p>Another challenge of neural causal discovery is jointly inducing abstract latent variables (such as objects or entities) from low-level perception (e.g., pixels of an image) while simultaneously learning the causal graph underlying such variables, which determines how they interact <ref type="bibr">(Ke et al., 2021a)</ref>. The lacklustre abilities of vanilla neural models to understand the compositional properties of symbolic building blocks, i.e. their 'binding problem', arguably explains their current shortfalls in systematic generalisation <ref type="bibr" target="#b106">(Greff et al., 2020)</ref>.</p><p>Object-centric learning holds promise to mitigate these limitations. For instance, it can be facilitated by slot attention, which is a fully differentiable and iterative attention mechanism that interfaces between perceptual representations and slots, a set of unordered placeholder variables <ref type="bibr" target="#b190">(Locatello et al., 2020)</ref>. <ref type="bibr" target="#b68">(Didolkar et al., 2021)</ref> propose Neural Production Systems, where rule templates can be bound to specific entities present in the working memory, in order to update their representations. In particular, rules are MLP modules and the matching with entities (triggering updates) is parameterised by attention.</p><p>Crucially, observational data alone is often<ref type="foot" target="#foot_24">foot_24</ref> insufficient to learn structural causal models as they may not be identifiable <ref type="bibr">(Pearl, 2009)</ref>. Hence the necessity to augment observation with interventions and counterfactuals. These allow for answering questions about cause-effect relationships rather than mere correlations. In real-world scenarios, however, the nature and number of interventions are unknown <ref type="bibr">Ke et al. (2021a)</ref>. In this setting, there is no formal guarantee that causal discovery succeeds. Yet, <ref type="bibr" target="#b149">Ke et al. (2019)</ref> finds that DAG discovery on interventional data based on continuous optimisation recovers causal graphs reliably. In particular, modular architectures surpass both vanilla models and graph neural networks <ref type="bibr">(Ke et al., 2021a)</ref>. Recently, <ref type="bibr" target="#b98">Geffner et al. (2022)</ref> perform causal inference in a deep non-linear additive noise structural equation model, based on autoregressive flows. Variational inference is used to learn a posterior over causal graphs. The learned functions can be further used to estimate conditional average treatment effects based on simulations.</p><p>The main purpose of these deep modular methods is causal inference and discovery, which has applications in several branches of medicine and economics <ref type="bibr" target="#b98">(Geffner et al., 2022)</ref>. In addition, these methods are particularly relevant in grounded settings, where the distribution of the observations from the environment changes as the agent learns better policies <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref>. Moreover, causal discovery can be combined with model-based RL methods to learn a self-supervised model of the environment, i.e. its variables and their causal dependencies, from trajectories of observations, actions, and rewards. This allows for simulating the potential outcomes of a policy before execution and thus estimating better value functions, which dramatically improves sample efficiency in agents <ref type="bibr">(Ke et al., 2021a)</ref>. Another common application of this family of modular neural architectures is out-of-distribution generalisation: for instance, zero-shot transfer to images of different sizes or sequences of different lengths <ref type="bibr" target="#b102">(Goyal et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>• Modularity is defined as the functional specialisation of the components of a system.</p><p>• Specialised sub-networks may emerge in vanilla neural modules (from multitask training or regularisation), but they are seldom reused and recombined.</p><p>• Deep modular architectures rest on the separation between computation functions on the one hand and routing and aggregation functions on the other.</p><p>• Computation functions may consist of any neural module. Modules may modify the original parameters, be concatenated to the input, or composed with the original function. • All composition strategies are equivalent to summing the original output with a term depending on the new module. In practice, however, they offer different trade-offs between efficiency (in time and space, during training and inference) and performance.</p><p>• Routing controls the flow of information, i.e., module selection. In fixed routing, it is determined a priori based on expert knowledge. When this is not available, routing parameters are learned. • Learned routing is challenging because of training instability, module collapse, and overfitting. Thus, learned routing often underperforms fixed routing. • Routing can be conditioned on (parts of) the input or metadata such as task identity. Routing can take place at different levels, such as globally for the whole model or layer-wise.</p><p>• Soft routing assigns every module a continuous score and performs a weighted combination of their outputs. It is amenable to being learned via gradient descent but is highly inefficient. • Hard routing activates only a subset of modules via top-1, top-k, or variable-size selection. It is learned via reinforcement learning, evolutionary algorithms, or stochastic re-parameterisation. It corresponds to the principles of conditional computation and information bottleneck in cognition.</p><p>• Hypernetworks can be interpreted as combining unnormalised routing (task embedding) with modules (generator). They can in turn generate parameters of other modules.</p><p>• If routing selects multiple modules, these must be aggregated via a function.</p><p>• Module parameters or outputs can be interpolated for aggregation, according to scores from the routing function, an attention mechanism, or via simple averaging.</p><p>• Alternatively, aggregation may involve composing the module functions, either sequentially or based on a tree graph obtained from global routing.</p><p>• The applications include parameter-efficient fine-tuning in NLP, computer vision, and speech processing. These rely on the same types of modules and fixed routing. In addition to increased efficiency, this prevents negative interference and enables zero-shot transfer.</p><p>• Modularity also serves the purpose of generalising to new tasks systematically, by recombining modules and locally updating them.</p><p>• Modular deep learning transcends the confines of private research: it enables community-driven sharing, expanding, reusing, and updating of the modules.</p><p>• In hierarchical reinforcement learning, modular options serve as abstractions between task goals and low-level actions and observations. They facilitate planning in long-horizon and sparse-reward tasks and increase sample efficiency due to transferability.</p><p>• In programme induction, the components of deep models can mirror a computer architecture: modules are elementary operations and routing is logical flow control. These are often augmented by an external read-write memory. Modules can also simulate symbolic algorithms.</p><p>• In causal discovery and inference, modules may be taken to correspond to physical mechanisms that are independent and reusable.</p><p>• Modular deep learning empowers these traditional applications by learning abstractions (options, programmes, causal graphs) end-to-end from perceptual stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Future Work</head><p>While recently modularity has attracted increasing attention in research, there remain many interesting open research questions along the axes of modularity introduced in this survey. We provide an overview of some of these directions for future work.</p><p>Combination of Different Types of Computation Functions Existing computation functions (see § 3) are mostly associated with a single category: parameter composition, input composition, or function composition. There are a few exceptions such as compacter (Mahabadi et al., 2021a)-low-rank adapterswhich combine multiple types. In general, techniques from parameter composition that incorporate sparsity, a low-rank or structural constraint are agnostic of the form of the module. In practice, this should enable more efficient learning and aggregation.</p><p>Learned Module Structure Most modules used in current works share the same architecture, which is reused across different settings. Depending on the skill or knowledge that should be learned, a module may need to be structured differently and might require access to another component or other type of data. In the extreme, a model may require a special-purpose architecture to be able to perform a specific capability <ref type="bibr">(Andreas et al., 2016b)</ref>. As modules are more widely used, they may benefit from being learned in a more flexible manner, perhaps incorporating ideas from neural architecture search <ref type="bibr" target="#b220">(Negrinho et al., 2019)</ref> in a module-specific space of architecture primitives.</p><p>Standardising Modularity Evaluation Depending on the dimension studied, modular approaches may be evaluated based on a variety of factors including downstream performance, memory footprint, number of parameters, latency, and compositional generalisation. In order to make progress on modular models in general, evaluation should be standardised. Current evaluation is additionally mainly based on existing datasets that are re-purposed to enable modular evaluation such as by framing them in a zero-shot transfer setting. Future work on modularity evaluation should design forward-looking evaluation benchmarks that are designed to test the capabilities of the next generation of modular models such as assessing the composition of skills and acquisition of new types of reasoning abilities.</p><p>Nature of Modular Representations While modular representations have been aggregated and composed, it remains mostly unclear how the inductive bias of a computation function influences the modular representation that is learned. In addition, it remains unclear how computation functions differ on a representation level. Beyond the computation function, it is also unclear how the other dimensions of our taxonomy, i.e., the routing function, the aggregation function, and the training setting influence the nature of the modular representations.</p><p>Hierarchical Modularity Current approaches mostly do not differentiate between high-level and low-level skills and how they relate to each other. It might also be possible to designate particular parts of the model or dedicated modules to capture a set of specialised skills or options, and clearly distinguish between other (sets of) skills. At fine-tuning, even more specialised sub-modules could be learned focused only on the previously designated modules. One example might be learning fine-grained specialised subnetworks over larger subnetworks of the original model, offering gradual module specialisation.</p><p>Learned Routing for Pre-training Fixed routing (see § 4) is the most common strategy to disentangle knowledge into modular parts of the model. However, fixed routing limits the usability of the proposed methods as they cannot be used on data, which lacks the metadata needed for fixed routing; for instance, when training on heterogeneous data, metadata such as domain information often does not exist. While learned routing methods do not require this metadata to perform routing a priori, they suffer from training difficulties (as discussed in § 4.2). This opens up research directions that enable modular pre-training with learned routing, which would make modular models applicable to a broader set of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modular Instruction Tuning</head><p>The main way in which current LLMs are specialised to particular downstream settings is via instruction tuning <ref type="bibr">(Wei et al., 2022b)</ref>, i.e., fine-tuning on a collection of tasks described via instructions. These tasks are increasingly defined based on a set of skills and capabilities that a model should learn, which opens the room to developing modular instruction tuning methods that enable the acquisition, updating, and composition of specialised knowledge.</p><p>Benchmarking Routing Methods Existing studies mainly evaluate routing methods based on performance but do not take into account how different routing strategies influence modular representations. In order to make progress on better routing methods, benchmarks and metrics are necessary that compare routing mechanisms from a modularity perspective across different settings.</p><p>Structured and Sparse Aggregation Current aggregation methods (see § 5) combine the information from multiple modular components by applying arithmetic operations such as addition and subtraction across all parameters, which likely includes parameters that should not be modified. Structured or sparse aggregation methods could focus on aggregating information within salient subnetworks or parameter groups, which might make aggregation more efficient and improve out-of-distribution generalisation.</p><p>Learned Aggregation Methods Most aggregation methods are based on arithmetic operations. Depending on the nature of the modular information, it may be useful to (non-)linearly transform the representations. More complex domain-specific aggregation methods can be learned in conjunction with the modular representations to enable better generalisation to new settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging Modular Models</head><p>In recent work, merging models trained with different settings has led to improved performance <ref type="bibr">(Wortsman et al., 2022, inter alia)</ref>. Rather than requiring separate training runs of a model, a multi-task model can alternatively be trained with modular components that are designed to be merged at a later stage. This potentially allows for an architecture, which can be computationally efficiently trained while covering many modalities.</p><p>Extensible Multi-task Models Most approaches in multi-task learning have focused on training dense models, with a key limitation being that models cannot easily be extended to new settings. Focusing on training multi-task models with modular components ensures that the baseline models are much easier to adapt and extend to new settings. Given the trend of pre-training larger and larger models from scratch, modularising parts of such models and developing modular methods that can be shared across different architectures and model sizes may lead to more sustainable model development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.2 Input Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.3 Function Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.4 Hypernetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure 1: Case studies of modular deep learning; best viewed in colour. Green components illustrate different routing functions (see § 4); shade-of-purple components illustrate modular computation functions (see §3). 1a) MAD-X (Pfeiffer et al., 2020b) uses Adapter layers with fixed routing for zero-shot cross-lingual transfer. 1b) Polytropon (Ponti et al., 2022) uses low-rank adapters (LoRA; Hu et al., 2022) with hard learned routing for few-shot task adaptation. 1c) MoE Transformers<ref type="bibr" target="#b82">(Fedus et al., 2021;</ref> Clark et al., 2022, inter alia)   use Multi-Layer Perceptrons with top-k soft routing, in order to scale to larger model sizes. The three representative models illustrated here are only a fraction of possible configurations from the 'configuration manifold' that can be created by varying the components surveyed in §3- §6.</figDesc><graphic coords="4,95.83,81.86,69.49,215.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different modular designs for Transformer architectures; best viewed in colour. Task-specific modular components are illustrated in magenta and purple, respectively. (a) Parameter Composition ( § 3.1): A sparse sub-network in the linear layer as part of multi-head-attention. (b) Input Composition ( § 3.2): Prefix-tuning (Li &amp; Liang, 2021) extends the input by prepending embeddings to the key and value matrices in the Transformer layer. (c) Function Composition ( § 3.3): Task-specific bottleneck layers transforming the hidden representations are inserted in each layer (Houlsby et al., 2019). (d) Hypernetwork ( § 3.4):A small separate neural network generates modular parameters conditioned on metadata. We show its application to function composition but it is compatible with all computation functions.</figDesc><graphic coords="10,88.47,79.40,309.57,163.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different approaches of function composition. (a) Sequential Bottleneck Adapter:The first adapter architecture proposed for transformers which consists of two bottleneck layers placed after the multi-head attention (MHA) and feed-forward (FF) layers<ref type="bibr" target="#b129">(Houlsby et al., 2019)</ref>. (b) Parallel Bottleneck Adapter: Bottleneck layers processed in parallel to the MHA and FF layers of the pre-trained transformer components<ref type="bibr" target="#b251">(Rebuffi et al., 2018;</ref><ref type="bibr" target="#b285">Stickland &amp; Murray, 2019;</ref> He et al., 2022a). (c) (IA) 3 : Rescaling operations performed within the MHA and FF layers(Liu et al., 2022b).</figDesc><graphic coords="13,413.19,146.50,56.16,123.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Different routing methods. (a) Fixed Routing: Examples are passed to modules based on a pre-defined logic, known a priori. (b) Hard Learned Routing: Learned hard selection modules. (c) Soft Learned Routing: Soft selection and weighting of modules.</figDesc><graphic coords="18,119.46,94.01,115.83,105.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>Nevertheless, one could imagine obtaining the best of both worlds by hybridising sparse MoE Transformers models with deterministic or learned routing strategies from § 4.1 and § 4.2.2. Instead of routing each individual token separately, all tokens of a single example can be routed to the same experts.<ref type="bibr" target="#b158">Kudugunta et al. (2021)</ref> experiment with two versions of example-level routing for machine translation: In sentence-level routing, they average pool over the token embeddings, and condition the router on the resulting representation. In task-level routing, a task embedding is trained, based on which the router learns the distribution over modules. In a similar vein,<ref type="bibr" target="#b126">Gupta et al. (2022)</ref> and<ref type="bibr" target="#b335">Xi et al. (2022)</ref> implement task-level routing across modular experts to improve the amount of knowledge sharing during multi-task learning in NLP and computer vision, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Different routing levels. (a) Layer-wise Routing: The indices are chosen based on the input to the current layer. (b) Naive Global Routing: The same indices of modules are chosen for all the layers of the model. (c) Global Routing: The configuration (possibly different for each layer) is chosen globally.</figDesc><graphic coords="23,236.50,87.80,138.99,128.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>pre-train modular components for different textual domains d ∈ D. When utilising the pre-trained modules on unseen data, they weight the output representations h d of the respective domain modules ϕ d according to the posterior distribution over the input examples, i.e. α = p(D | x):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Hyper-X(Üstün et al., 2022): an example application of contextual module generation where a hypernetwork takes the concatenation of task, language and layer embeddings as input and generates a flat parameter vector. This is further reshaped into an adapter module within each Transformer layer. Learning independent layer embeddings and sharing a single hypernetwork across all layers<ref type="bibr" target="#b10">(Ansell et al., 2021</ref>) (i) enables information sharing across layers, and (ii) reduces trainable parameters of the hyper-network by a factor corresponding to the number of layers.</figDesc><graphic coords="32,383.22,168.51,140.40,192.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The structure of the wav2vec 2.0 model with taskspecific bottleneck adapters for parameter-efficient ASR fine-tuning from Thomas et al. (2022); f (•) denotes a convolutional encoder followed by 12 standard Transformer encoder blocks. For downstream ASR a linear classifier, CT C(•), is applied to the final encoder output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example of Hierarchical Reinforcement Learning ( § 8.1), Policy Sketches<ref type="bibr" target="#b9">(Andreas et al., 2017)</ref>. Two high-level policies Π corresponding to task instructions τ are illustrated. Each iteratively selects low-level policies π (options) corresponding to sub-tasks b from a shared inventory. These determine the choice of action given observations. In this case, options are implemented as predicate-argument pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example of Programme Simulation ( § 8.2): Differentiable Neural Computer<ref type="bibr" target="#b105">(Graves et al., 2016)</ref>. A recurrent neural controller iteratively receives an input from the environment, writes to / reads from memory, and produces an output. Read and write operations are based on attention between generated keys and memory entries. A special mechanism keeps track of memory usage and temporal links between entries.</figDesc><graphic coords="39,354.86,143.56,55.27,110.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: An example of Causal Inference ( § 8.3): Causal Independent Mechanisms (Parascandolo et al., 2018). A transformed example is routed to an expert which maps it to the original distribution. An adversarial discriminator attempts to distinguish between reconstructed and original examples.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>These phenomena have also been referred to as spatial and temporal 'crosstalk'(Jacobs et al., 1991b).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b163">Lange et al. (2022)</ref> found that clusters identified through downstream (output) information do not match with the clusters identified through upstream (input) information. They attribute this phenomenon to their different roles, namely disentanglement of the input structure and composition of the output structure, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We abuse notation by treating indexing over functions, f i , as identical to indexing over the parameters of a function, f θ i . In this survey, both are used interchangeably.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>To avoid clutter in terminology, throughout this work we use the term composition to refer to the merger of computation functions ( § 3), and the term aggregation to refer to different approaches of combining the outputs of different modules ( §</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>5).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>This is typically implemented by masking the gradient based on the binary mask b ⊙ ∇ θ L(f θ , D) where L is a loss function and D is a dataset<ref type="bibr" target="#b11">(Ansell et al., 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>In this view, there is no clear differentiation between model parameters θ and module parameters ϕ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>We omit the layer index n to simplify the presentation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>Published in Transactions on Machine LearningResearch (11/2023)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>Alternative non-parametric routing strategies include random routing<ref type="bibr" target="#b359">(Zuo et al., 2022;</ref> Wang et al., 2022)  or routing based on hash functions<ref type="bibr" target="#b255">(Roller et al., 2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>In §4.2.3 "Token-Level Routing" we provide a brief overview over recent works that focus on the efficiency aspect. However, it is important to note, that the focus of this paper is centred around modularity and not efficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>Alternatively, combining entire models stored in model repositories via distillation<ref type="bibr" target="#b152">(Khanuja et al., 2021)</ref> or averaging<ref type="bibr" target="#b202">(Matena &amp; Raffel, 2021)</ref> can also help avoid negative interference(Don-Yehiya et al., 2022); however, this is usually less efficient and subject to limitations such as those discussed later in § 5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>This encourages module re-usage at different layers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>Intrinsic rewards can be added, for instance favouring diversity in the module selection across time steps<ref type="bibr" target="#b256">(Rosenbaum et al., 2018)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14"><p>In the following sections we use the term "expert" and "module" interchangeably to reflect common practice in the body of research on MoEs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_15"><p>For more details on load balancing methods we refer to<ref type="bibr" target="#b75">Fedus et al. (2022)</ref>, Chapter 4.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_16"><p>Combining modules has the potential to significantly improve inference speed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_17"><p>vec('King') -vec('Man') + vec('Woman') ≈ vec('Queen'), with vec(•) denoting word embeddings of the respective words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_18"><p>Note that the latter strategy leads to high variance in the norms of hidden representations if the router can select variable-size subsets of modules.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_19"><p>Notably, RAG is also discussed in § 3.2 due to its dual capability of both input composition and aggregation, for instance when multiple documents are used in the retrieval process.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_20"><p>For further applications of RAG see § 7.1.4.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_21"><p>For instance, disentangling domain and language information yields benefits for NMT and cross-lingual transfer applications<ref type="bibr" target="#b310">(Vilar, 2018;</ref> Cooper Stickland et al., 2021;<ref type="bibr" target="#b235">Pham et al., 2021;</ref><ref type="bibr" target="#b269">Saunders, 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_22"><p>The (concept of) reusability enabled by the modular design also positively impacts energy consumption<ref type="bibr" target="#b288">(Strubell et al., 2019)</ref>, making an important leap towards Green(er) AI<ref type="bibr" target="#b276">(Schwartz et al., 2020)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_23"><p>In addition to causal inference, this strategy is also inspired by the global workspace theory<ref type="bibr" target="#b15">(Baars, 2005)</ref>. This theory postulates specialised modules compete to update a shared workspace, and the resulting communication bottleneck creates a crucial inductive bias in human cognition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_24"><p>Unless specific assumptions are made about the data generating process, such as linear but non-Gaussian data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Ivan Vulić is supported by a personal <rs type="funder">Royal Society</rs> <rs type="grantName">University Research Fellowship</rs> (no 221137; 2022-).</p><p>We are grateful to <rs type="person">Colin Raffel</rs> for his comments and suggestions, which have greatly improved the manuscript. We thank <rs type="person">Andrea Gesmundo</rs> for feedback on a draft of this paper. We are thankful to <rs type="person">Kyunghyun Cho</rs> and <rs type="person">Alessandro Sordoni</rs> for stimulating discussions. We thank the anonymous reviewers for helpful suggestions and feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5RfsakN">
					<orgName type="grant-name">University Research Fellowship</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://openreview.net/forum?id=z9EkXfvxta">https://openreview.net/forum?id=z9EkXfvxta</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.568</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.568" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7319" to="7328" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep elastic networks with model selection for multi-task learning</title>
		<author>
			<persName><forename type="first">Chanho</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00663</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00663" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="page" from="6528" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Git re-basin: Merging models modulo permutation symmetries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasa</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.04836</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2209.04836" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.14198</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.14198" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modular meta-learning</title>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Alet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Lozano-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaelbling</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v87/alet18a.html" />
	</analytic>
	<monogr>
		<title level="m">2nd Annual Conference on Robot Learning, CoRL</title>
		<meeting><address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">2018. October 2018. 2018</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="856" to="868" />
		</imprint>
	</monogr>
	<note>Proceedings PMLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expert gate: Lifelong learning with a network of experts</title>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.753</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.753" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="7120" to="7129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Giving BERT a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1609</idno>
		<ptr target="https://aclanthology.org/D19-1609" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="5947" to="5952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1181</idno>
		<ptr target="https://aclanthology.org/N16-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.12</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.12" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modular multitask reinforcement learning with policy sketches</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/andreas17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MAD-G: Multilingual adapter generation for efficient cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ansell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.410</idno>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.410" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="4762" to="4781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Composable sparse fine-tuning for cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ansell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.125</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1778" to="1796" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured pruning of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Sajid</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuyeon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3005348</idno>
		<ptr target="https://doi.org/10.1145/3005348" />
	</analytic>
	<monogr>
		<title level="j">ACM Journal on Emerging Technologies in Computing Systems (JETC)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient large scale language modeling with mixtures of experts</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giridharan</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.804" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="11699" to="11732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="6655" to="6672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global workspace theory of consciousness: toward a cognitive neuroscience of human experience</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><surname>Baars</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0079-6123(05)50004-9</idno>
		<ptr target="https://10.1016/S0079-6123(05)50004-9" />
	</analytic>
	<monogr>
		<title level="j">Progress in Brain Research</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">XLS-R: self-supervised cross-lingual speech representation learning at scale</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kritika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatharth</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2022-143</idno>
		<ptr target="https://doi.org/10.21437/Interspeech" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022</date>
			<biblScope unit="page" from="2022" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The option-critic architecture</title>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14858" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="1726" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/92" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parameterefficient conformers via sharing sparsely-gated experts for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaituo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2022-709</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2022-709" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tafazzoli</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><surname>Salzmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.100</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2013.100" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">December 1-8, 2013. 2013</date>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Design rules: The power of modularity</title>
		<author>
			<persName><forename type="first">Young</forename><surname>Carliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><forename type="middle">B</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/2366.001.0001</idno>
		<ptr target="https://doi.org/10.7551/mitpress/2366.001.0001" />
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><surname>Ballard</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X00021555</idno>
		<ptr target="https://psycnet.apa.org/doi/10.1017/S0140525X00021555" />
	</analytic>
	<monogr>
		<title level="m">Cortical connections and parallel processing: Structure and function</title>
		<imprint>
			<date type="published" when="1986">1986. 2023</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="67" to="90" />
		</imprint>
	</monogr>
	<note>Behavioral and Brain Sciences</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1165</idno>
		<ptr target="https://aclanthology.org/D19-1165" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="1538" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilingual machine translation with hyper-adapters</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.77" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="1170" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.1</idno>
		<ptr target="https://aclanthology.org/2022.acl-short.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Conditional computation in neural networks for faster models</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06297</idno>
		<ptr target="http://arxiv.org/abs/1511.06297" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>CoRR, abs/1308.3432</idno>
		<ptr target="http://arxiv.org/abs/1308.3432" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryxWIgBFPS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Budget-aware adapters for multi-domain learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><forename type="middle">Ferreira</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00047</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00047" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/839" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
	<note>ab46820b524afda05122893c2fe8e-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A scalable model specialization framework for training and inference using submodels and its application to speech model personalization</title>
		<author>
			<persName><forename type="first">Fadi</forename><surname>Biadsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2022-10613</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2022-10613" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>CoRR, abs/1701.07275</idno>
		<ptr target="http://arxiv.org/abs/1701.07275" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The unified medical language system (UMLS): integrating biomedical terminology</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC308795/" />
	</analytic>
	<monogr>
		<title level="m">D267-D270</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object-oriented analysis and design with applications</title>
		<author>
			<persName><forename type="first">Grady</forename><surname>Booch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Maksimchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Engle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobbi</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Conallen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelli</forename><forename type="middle">A</forename><surname>Houston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1402521.1413138</idno>
		<ptr target="https://doi.org/10.1145/1402521.1413138" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGSOFT Software Engineering Notes</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/45fbc" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Ulrike Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
	<note>d3e05ebd93369ce542e8f2322d-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryutaro</forename><surname>Bragman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">Jorge</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00147</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00147" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="page" from="1385" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020. 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">IGLUE: A benchmark for transfer learning across modalities, tasks, and languages</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/bugliarello22a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="2370" to="2392" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-head adapter routing for data-efficient fine-tuning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matheus</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2211.03831" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tinytl: Reduce memory, not parameters for efficient on-device learning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/81" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>f7acabd411274fcf65ce2070ed568a-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007379606734</idno>
		<ptr target="https://doi.org/10.1023/A:1007379606734" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recursive routing networks: Learning to compose modules for language understanding</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauri</forename><surname>Karttunen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1365</idno>
		<ptr target="https://aclanthology.org/N19-1365" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3631" to="3648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graphical clusterability and local specialization in deep neural networks</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Hod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Filan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Critch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=HreeeJvkue9" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop on PAIR 2 Struct</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatically composing representation transformations as a means for generalization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1ffQnRcKX" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis for pre-trained BERT networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/b" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>6af2c9703f203a2794be03d443af2e3-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chasing sparsity in vision transformers: An end-to-end exploration</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021, virtual, pp. 19974-19988, 2021</date>
		</imprint>
	</monogr>
	<note>a61f27ab2165df0e18cc9433bd7f27c5-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=Hkl5aoR5tm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multinomial adversarial networks for multi-domain text classification</title>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1111</idno>
		<ptr target="https://aclanthology.org/N18-1111" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1226" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring efficienttuning methods in self-supervised speech models</title>
		<author>
			<persName><forename type="first">Zih-Ching</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Lun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Shang-Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT54892.2023.10023274</idno>
		<ptr target="https://doi.org/10.1109/SLT54892.2023.10023274" />
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop, SLT 2022</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">January 9-12, 2023</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Decouple knowledge from paramters for plug-and-play language modeling</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.901</idno>
		<ptr target="https://aclanthology.org/2023.findings-acl.901" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="page" from="14288" to="14308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">On the representation collapse of sparse mixture of experts</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barun</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Data-efficient cross-lingual transfer with languagespecific subnetworks</title>
		<author>
			<persName><forename type="first">Rochelle</forename><surname>Choenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.00106</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.00106" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Fusing finetuned models for better pretraining</title>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Venezian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient hierarchical domain adaptation for pretrained language models</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.96</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.96" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="1336" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Language-family adapters for multilingual neural machine translation</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Stojanovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.15236</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2209.15236" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unified scaling laws for routed language models</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/clark22a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="4057" to="4086" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised crosslingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multilingual domain adaptation for NMT: Decoupling language and domain information with adapters</title>
		<author>
			<persName><forename type="first">Asa</forename><surname>Cooper Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilina</forename><surname>Nikoulina</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.64" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="578" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">No language left behind: Scaling human-centered machine translation</title>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Çelebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maha</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Kalbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Youngblood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bapi</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">Mejia</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prangthip</forename><surname>Hansanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semarley</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Sadagopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Spruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necip</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Fazil Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Mourachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safiyyah</forename><surname>Ropers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2207.04672</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2207.04672" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multi-task learning with deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Crawshaw</surname></persName>
		</author>
		<idno>abs/2009.09796</idno>
		<ptr target="https://arxiv.org/abs/2009.09796" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Are neural nets modular? inspecting functional modularity through differentiable weight masks</title>
		<author>
			<persName><forename type="first">Róbert</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=7uVcpu-gMD" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural modular control for embodied question answering</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v87/das18a.html" />
	</analytic>
	<monogr>
		<title level="m">2nd Annual Conference on Robot Learning</title>
		<meeting><address><addrLine>CoRL; Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. 29-31 October 2018. 2018</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
	<note>Proceedings PMLR</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Feudal reinforcement learning</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/714-feudal-reinforcement-learning" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5, [NIPS Conference</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992-12-03">November 30 -December 3, 1992. 1992</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
	<note>fab6e3aa34248ec1e34a4aeedecddc8-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning modular neural network policies for multi-task and multi-robot transfer</title>
		<author>
			<persName><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2017.7989250</idno>
		<ptr target="https://doi.org/10.1109/ICRA.2017.7989250" />
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="2169" to="2176" />
			<date type="published" when="2017-05-29">2017. 2017. May 29 -June 3, 2017. 2017</date>
			<publisher>IEEE</publisher>
			<pubPlace>Singapore, Singapore</pubPlace>
		</imprint>
	</monogr>
	<note>ICRA</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural production systems</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Didolkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Beaudoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="25673" to="25687" />
		</imprint>
	</monogr>
	<note>d785bf9067f8af9e078b93cf26de2b54-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hierarchical reinforcement learning with the MAXQ value function decomposition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Dietterich</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.639</idno>
		<ptr target="https://doi.org/10.1613/jair.639" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="227" to="303" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Brain-like functional specialization emerges spontaneously in deep neural networks</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Dobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander Je</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/35294241/" />
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Cold fusion: Collaborative descent for distributed multitask finetuning</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Shachar Don-Yehiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Venezian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><surname>Choshen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.01378</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2212.01378" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv prerint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/donahue14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014. 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Essentially no barriers in neural network energy landscape</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kambis</forename><surname>Veschgini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Salmhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/draxler18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><forename type="middle">P</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/du22c.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="5547" to="5569" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Tricks for training sparse translation models</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.244</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.244" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="3340" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Using adapters to overcome catastrophic forgetting in end-toend automatic speech recognition</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Vander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eeckt</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.16082</idno>
		<ptr target="https://arxiv.org/abs/2203.16082" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning factored representations in a deep mixture of experts</title>
		<author>
			<persName><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.4314" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Phylogeny-inspired adaptation of multilingual models to new languages</title>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.aacl-main.34" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-11">November 2022</date>
			<biblScope unit="page" from="434" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Beyond english-centric multilingual machine translation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-1307.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="107" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">DRAFT: A novel framework to reduce domain shifting in self-supervised learning and its application to children&apos;s ASR</title>
		<author>
			<persName><forename type="first">Ruchao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abeer</forename><surname>Alwan</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2022-11128</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2022-11128" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association</title>
		<editor>
			<persName><forename type="first">Hanseok</forename><surname>Ko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</editor>
		<meeting><address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/2101.03961</idno>
		<ptr target="https://arxiv.org/abs/2101.03961" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">A review of sparse expert models in deep learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.01667</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2209.01667" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>CoRR, abs/1701.08734</idno>
		<ptr target="http://arxiv.org/abs/1701.08734" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Stochastic neural networks for hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Florensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1oK8aoxe" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">The modularity of Mind</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<ptr target="https://mitpress.mit.edu/9780262560252/the-modularity-of-mind/" />
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Discovering language-neutral sub-networks in multilingual language models</title>
		<author>
			<persName><forename type="first">Negar</forename><surname>Foroutan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Banaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.513" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="7560" to="7575" />
		</imprint>
	</monogr>
	<note>United Arab Emirates</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl-b3RcF7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/frankle20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Topology and geometry of half-rectified network optimization</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bk0FWVcgx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1364-6613(99)01294-2</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1364661399012942" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v17/15-239.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">CLIP-Adapter: Better vision-language models with feature adapters</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno>CoRR, abs/2110.04544</idno>
		<ptr target="https://arxiv.org/abs/2110.04544" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">NDDR-CNN: layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00332</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_NDDR-CNN_Layerwise_Feature_Fusing_in_Multi-Task_CNNs_by_Neural_Discriminative_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16">2019. June 16-20, 2019. 2019</date>
			<biblScope unit="page" from="3205" to="3214" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Gordon</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="8803" to="8812" />
		</imprint>
	</monogr>
	<note>be3087e74e9100d4bc4c6268cdbe8456-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Mixture of informed experts for multilingual speech recognition</title>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Farris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manasa</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9414379</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9414379" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">June 6-11, 2021. 2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="6234" to="6238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Deep end-toend causal inference</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Geffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Antoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6DPVXzjnbDK" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Causality for Real-world Impact</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">A multi-agent framework for the asynchronous and collaborative extension of multitask ML systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.14745</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2209.14745" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">An evolutionary approach to dynamic introduction of tasks in largescale multitask learning systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.12755</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2205.12755" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">munet: Evolving pretrained deep neural networks into scalable autotuning multitask systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.10937</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2205.10937" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Recurrent independent mechanisms</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shagun</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=mLcmdlEUxy-" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143891</idno>
		<ptr target="https://doi.org/10.1145/1143844.1143891" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</editor>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">June 25-29, 2006. 2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>CoRR, abs/1410.5401</idno>
		<ptr target="http://arxiv.org/abs/1410.5401" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Puigdomènech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature20101</idno>
		<ptr target="https://doi.org/10.1038/nature20101" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">On the binding problem in artificial neural networks</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>CoRR, abs/2012.05208</idno>
		<ptr target="https://arxiv.org/abs/2012.05208" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">The indian buffet process: An introduction and review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="https://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Knowledge is a region in weight space for fine-tuned language models</title>
		<author>
			<persName><forename type="first">Almog</forename><surname>Gueta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Venezian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.04863</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.04863" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.378</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.378" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4884" to="4896" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation with mixture of experts</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1498</idno>
		<ptr target="https://aclanthology.org/D18-1498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Sparsely activated mixture-of-experts are robust multi-task learners</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishan</forename><surname>Subudhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.07689</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.07689" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Stochastic weight averaging in parallel: Large-batch training that generalizes well</title>
		<author>
			<persName><forename type="first">Vipul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><forename type="middle">Akle</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
		</author>
		<idno>CoRR, abs/2001.02312</idno>
		<ptr target="http://arxiv.org/abs/2001.02312" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">DEMix layers: Disentangling domains for modular language modeling</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.407</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.407" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="5557" to="5576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkpACe1lx" />
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.381</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.381" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">May. August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4921" to="4933" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Hampshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.142911</idno>
		<ptr target="https://doi.org/10.1109/34.142911" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="751" to="769" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/hash/ae" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
	<note>0eb3eed39d2bcef4622b2499a05fe6-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">DSD: dense-sparse-dense training for deep neural networks</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyoST_9xl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Robust transfer learning with pretrained language models through adapters</title>
		<author>
			<persName><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.108</idno>
		<ptr target="https://aclanthology.org/2021.acl-short.108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="854" to="861" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning</title>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Hazimeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheswaran</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/f5" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="29335" to="29347" />
		</imprint>
	</monogr>
	<note>ac21cd0ef1b88e9848571aeb53551a-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=0RDcd5Axok" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-0_38" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">On the effectiveness of adapter-based tuning for pretrained language model adaptation</title>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bosheng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.172</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.172" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2208" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Parameter-efficient finetuning for vision transformers</title>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.16329</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.16329" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Hyperprompt: Prompt-based taskconditioning of transformers</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Huaixiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><forename type="middle">Prakash</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Chi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/he22f.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="8678" to="8690" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Learning and transfer of modulated locomotor controllers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>CoRR, abs/1610.05182</idno>
		<ptr target="http://arxiv.org/abs/1610.05182" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Exploiting adapters for cross-lingual low-resource speech recognition</title>
		<author>
			<persName><forename type="first">Wenxin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takahiro</forename><surname>Shinozaki</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2021.3138674</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2021.3138674" />
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Transactions on Audio Speech Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="317" to="329" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/houlsby19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
		<ptr target="https://aclanthology.org/P18-1031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Adapter-based extension of multi-speaker textto-speech model for new speakers</title>
		<author>
			<persName><forename type="first">Cheng-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhankar</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.00585</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.00585" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nZeVKeeFYf9" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/hu20b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<title level="s">Virtual Event</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">18 July 2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.93</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.93" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2017</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/huang22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Compositionality decomposed: How do neural networks generalise</title>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verna</forename><surname>Dankers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathijs</forename><surname>Mul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.1.11674</idno>
		<ptr target="https://doi.org/10.1613/jair.1.11674" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="757" to="795" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Editing models with task arithmetic</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.04089</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2212.04089" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog1502_2</idno>
		<ptr target="https://doi.org/10.1207/s15516709cog1502_2" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="250" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1991.3.1.79</idno>
		<ptr target="https://doi.org/10.1162/neco.1991.3.1.79" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkE3y85ee" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Exploring the benefits of training expert language models over instruction tuning</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonghyeon</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Rotograd: Gradient homogenization in multitask learning</title>
		<author>
			<persName><forename type="first">Adrián</forename><surname>Javaloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=T8wHz4rnuGL" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. OpenReview.net, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Language as an abstraction for hierarchical deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/0" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="9414" to="9426" />
		</imprint>
	</monogr>
	<note>af787945872196b42c9f73ead2565c8-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Dataless knowledge fusion by merging weights of language models</title>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Preotiuc-Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1994.6.2.181</idno>
		<ptr target="https://doi.org/10.1162/neco.1994.6.2" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">181</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Git-theta: A git extension for collaborative development of machine learning models</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anisha</forename><surname>Mascarenhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monty</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Large-scale multilingual speech recognition with a streaming end-to-end model</title>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindrima</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungji</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2858</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2019-2858" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2019, 20th Annual Conference of the International Speech Communication Association</title>
		<meeting>Interspeech 2019, 20th Annual Conference of the International Speech Communication Association<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-19">15-19 September 2019. 2019</date>
			<biblScope unit="page" from="2130" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Spontaneous evolution of modularity and network motifs</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0503610102</idno>
		<ptr target="https://doi.org/10.1073/pnas.0503610102" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences (PNAS)</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="13773" to="13778" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Learning neural causal models from unknown interventions</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Rosemary Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1910.01075</idno>
		<ptr target="http://arxiv.org/abs/1910.01075" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Yoshua Bengio, and Chris Pal. Systematic evaluation of causal discovery in visual model based reinforcement learning</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Rosemary Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Didolkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mozer</surname></persName>
		</author>
		<ptr target="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/8" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</title>
		<editor>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</editor>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</meeting>
		<imprint>
			<date type="published" when="2021-12">December 2021</date>
		</imprint>
	</monogr>
	<note>f121ce07d74717e0b1f21d122e04521-Abstract-round2.html</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Adapting BERT for continual learning of a sequence of aspect sentiment classification tasks</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.378</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.378" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="4746" to="4755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">MergeDistill: Merging language models using pre-trained distillation</title>
		<author>
			<persName><forename type="first">Simran</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.254</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.254" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="page" from="2874" to="2887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Adversarial adaptation of synthetic or stale data</title>
		<author>
			<persName><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1119</idno>
		<ptr target="https://aclanthology.org/P17-1119" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1297" to="1307" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Daniel</forename><surname>Gelatt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.220.4598.671</idno>
		<ptr target="https://doi.org/10.1126/science.220.4598.671" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Modular networks: Learning to decompose neural computation</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/310" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
	<note>ce61c90f3a46e340ee8257bc70e93-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/kornblith19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Beyond distillation: Task-level mixture-of-experts for efficient inference</title>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.304" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3577" to="3599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/f442" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="3675" to="3683" />
		</imprint>
	</monogr>
	<note>d33fa06832082290ad8544a8da27-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition</title>
		<author>
			<persName><forename type="first">Ken'ichi</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Cruz</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devang</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<idno>CoRR, abs/2112.05820</idno>
		<ptr target="https://arxiv.org/abs/2112.05820" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/lake18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2879" to="2888" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Fastpitch: Parallel text-to-speech with pitch prediction</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Lancucki</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9413889</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9413889" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">June 6-11, 2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Clustering units in neural networks: upstream vs downstream information</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">D</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.11815</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.11815" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Rozanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.deelio-1.5</idno>
		<ptr target="https://aclanthology.org/2020.deelio-1.5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
	<note>The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Sustainable modular debiasing of language models</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Lueken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.411</idno>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.411" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="4782" to="4797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Lightweight adapter tuning for multilingual speech translation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.103</idno>
		<ptr target="https://aclanthology.org/2021.acl-short.103" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Fastfood: Approximate kernel expansions in loglinear time</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Johannes</forename><surname>Smola</surname></persName>
		</author>
		<idno>CoRR, abs/1408.3060</idno>
		<ptr target="http://arxiv.org/abs/1408.3060" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qrwe7XHTmYb" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.243" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">BASE layers: Simplifying training of large, sparse models</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/lewis21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6265" to="6274" />
		</imprint>
	</monogr>
	<note>Virtual Event PMLR</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledgeintensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>b493230205f780e1bc26945df7481e5-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Quantifying adaptability in pre-trained language models with 500 tasks</title>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.346</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.346" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="4696" to="4715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryup8-WCW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Hong</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/505259756244493872" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="9694" to="9705" />
		</imprint>
	</monogr>
	<note>b7709a8a01b536-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Branch-train-merge: Embarrassingly parallel training of expert language models</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.03306</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2208.03306" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.353" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Finding sparse structures for domain specific neural machine translation</title>
		<author>
			<persName><forename type="first">Jianze</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/17574" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
			<biblScope unit="page" from="13333" to="13342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Learning language specific sub-network for multilingual machine translation</title>
		<author>
			<persName><forename type="first">Zehui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.25</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.25" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Parameter-efficient neural reranking for cross-lingual and multilingual retrieval</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.coling-1.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
			<biblScope unit="page" from="1071" to="1082" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Delving deeper into cross-lingual visual question answering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>CoRR, abs/2202.07630</idno>
		<ptr target="https://arxiv.org/abs/2202.07630" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Visually grounded reasoning across languages and cultures</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><surname>Elliott</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="1/2021.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="10467" to="10485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<ptr target="https://aclanthology.org/2021.emnlp-main.818" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</title>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.05638</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2205.05638" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
		<ptr target="https://doi.org/10.1145/3560815" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.10385" />
		<title level="m">GPT understands, too. CoRR, abs/2103.10385, 2021b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.8</idno>
		<ptr target="https://aclanthology.org/2022.acl-short.8" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1441</idno>
		<ptr target="https://aclanthology.org/P19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
		<ptr target="https://aclanthology.org/2020.tacl-1.47" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/8511" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>df98c02ab60aea1b2356c013bc0f-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models</title>
		<author>
			<persName><forename type="first">Qiuhao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.325</idno>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.325" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="3855" to="3865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.556</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.556" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8086" to="8098" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogério</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feris</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.126</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.126" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="1131" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multitask learning with multi-gate mixture-of-experts</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220007</idno>
		<ptr target="https://doi.org/10.1145/3219819.3220007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">August 19-23, 2018. 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1jE5L5gl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Compacter: Efficient lowrank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/081" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="1022" to="1035" />
		</imprint>
	</monogr>
	<note>be9fdff07f3bc808f935906ef70c0-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.47</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.47" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="565" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Storydall-e: Adapting pretrained text-to-image transformers for story continuation</title>
		<author>
			<persName><forename type="first">Adyasha</forename><surname>Maharana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darryl</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19836-6_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19836-6_5" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022 -17th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022</date>
			<biblScope unit="volume">XXXVII</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Verb knowledge injection for multilingual event processing</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.541</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.541" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6952" to="6969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00810</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18">2018. June 18-22, 2018. 2018</date>
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_5</idno>
		<idno>978-3-030-01225-0_5</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 8-14, 2018. 2018</date>
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="72" to="88" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Merging models with fisher-weighted averaging</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>CoRR, abs/2111.09832</idno>
		<ptr target="https://arxiv.org/abs/2111.09832" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0079-7421(08)60536-8</idno>
		<ptr target="https://doi.org/10.1016/S0079-7421(08)60536-8" />
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Sparse transfer learning via winning lottery tickets</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mehta</surname></persName>
		</author>
		<idno>CoRR, abs/1905.07785</idno>
		<ptr target="http://arxiv.org/abs/1905.07785" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2018</title>
		<meeting>ICLR 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Under the hood of neural networks: Characterizing learned representations by functional neuron populations and network ablations</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Meyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Waubert De Puiseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Posada-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Meisen</surname></persName>
		</author>
		<idno>CoRR, abs/2004.01254</idno>
		<ptr target="https://arxiv.org/abs/2004.01254" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Augmented language models: A survey</title>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dessì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoforos</forename><surname>Nalmpantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Baptiste Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Scialom</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.07842</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.07842" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019. 2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html</date>
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/hash/9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>aa42b31882ec039965f3c4923ce901b-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.244</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.244" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3470" to="3487" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.433</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.433" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/mittal20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Is a modular architecture enough?</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.02713</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.02713" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJGCiw5gl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title level="m" type="main">Residual adapters for few-shot text-to-speech speaker adaptation</title>
		<author>
			<persName><forename type="first">Nobuyuki</forename><surname>Morioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.15868</idno>
		<idno>CoRR, abs/2210.15868</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2210.15868" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<monogr>
		<title level="m" type="main">Models with Conditional Computation Learn Suboptimal Solutions</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://colinraffel.com/publications/icbinb2022models.pdf" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/e6384711491713" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2023. 2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="3307" to="3317" />
		</imprint>
	</monogr>
	<note>Soft merging of experts with adaptive routing d29bc63fc5eeb5ba4f-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Uniform convergence may be unable to explain generalization in deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Vaishnavh Nagarajan</surname></persName>
		</author>
		<author>
			<persName><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019. 207235</date>
			<biblScope unit="page" from="11611" to="11622" />
		</imprint>
	</monogr>
	<note>Vancouver d63ceb1db43c60db7bbb-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Learning to compose soft prompts for compositional zero-shot learning</title>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Nihal V Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03574</idno>
		<ptr target="https://arxiv.org/pdf/2204.03574.pdf" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Towards modular and programmable architecture search</title>
		<author>
			<persName><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darshan</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Feature partitioning for efficient multi-task architectures</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.04339" />
	</analytic>
	<monogr>
		<title level="m">CoRR, abs/1908.04339</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">What is being transferred in transfer learning?</title>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/0607" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>f4c705595b911a4f3e7a127b44e0-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Continual learning via local module composition</title>
		<author>
			<persName><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30298" to="30312" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">ST-Adapter: Parameter-efficient image-to-video transfer learning for action recognition</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.13559</idno>
		<idno>CoRR, abs/2206.13559</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.13559" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<monogr>
		<title level="m" type="main">Hierarchical3d adapters for long video-to-text summarization</title>
		<author>
			<persName><forename type="first">Pinelopi</forename><surname>Papalampidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.04829</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2210.04829" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Learning independent causal mechanisms</title>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/parascandolo18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4033" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Marinela</forename><surname>Parović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.130</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.130" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16528" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">AdapterHub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.7</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.7" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.617</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="7654" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">AdapterFusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.39</idno>
		<ptr target="https://aclanthology.org/2021.eacl-main.39" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="487" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">UNKs everywhere: Adapting multilingual language models to new scripts</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.800</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.800" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="10186" to="10203" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">xGQA: Cross-lingual visual question answering</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Martin</forename><surname>Steitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.196</idno>
		<ptr target="https://aclanthology.org/2022.findings-acl" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="2497" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Lifting the curse of multilinguality by pre-training modular transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.255</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="3479" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Revisiting multi-domain machine translation</title>
		<author>
			<persName><forename type="first">Minhquang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00351</idno>
		<ptr target="https://aclanthology.org/2021.tacl-1.2" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Monolingual adapters for zero-shot neural machine translation</title>
		<author>
			<persName><forename type="first">Jerin</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.361</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.361" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4465" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Learning compositional neural programs with recursive tree search and planning</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pierrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Ligner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Laterre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karim</forename><surname>Beguir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/95" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="14646" to="14656" />
		</imprint>
	</monogr>
	<note>b431e51fc53692913da5263c214162-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters &amp; less data</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amine</forename><surname>Elhattami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=de11dbHzAMF" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Contextual parameter generation for universal neural machine translation</title>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Emmanouil Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1039</idno>
		<ptr target="https://aclanthology.org/D18-1039" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="425" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<monogr>
		<title level="m" type="main">Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><surname>Ponti</surname></persName>
		</author>
		<idno type="DOI">10.17863/CAM.66424</idno>
		<ptr target="https://doi.org/10.17863/CAM.66424" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Parameter space factorization for zero-shot learning across tasks and languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinela</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Parovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00374</idno>
		<ptr target="https://aclanthology.org/2021.tacl-1.25" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="410" to="428" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main">Combining modular skills in multitask learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
		<idno>CoRR, abs/2202.13914</idno>
		<ptr target="https://arxiv.org/abs/2202.13914" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">XCOPA: A multilingual dataset for causal commonsense reasoning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianchu</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.185</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.185" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="2362" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">When BERT Plays the Lottery, All Tickets Are Winning</title>
		<author>
			<persName><forename type="first">Sai</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.259</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="3208" to="3229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<monogr>
		<title level="m" type="main">Temporal Abstraction in Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<ptr target="https://scholarworks.umass.edu/dissertations/AAI9978540" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">From sparse to soft mixtures of experts</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Dynamic inference with neural interpreters</title>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Waleed</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/5" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="10985" to="10998" />
		</imprint>
	</monogr>
	<note>b4e9aa703d0bfa11041debaa2d1b633-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/rajbhandari22a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="18332" to="18346" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain</title>
		<author>
			<persName><forename type="first">Janarthanan</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><forename type="middle">S</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><surname>Ravindran</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy6iJDqlx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/e" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
	<note>b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00847</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Rebuffi_Efficient_Parametrization_of_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18">2018. June 18-22, 2018. 2018</date>
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Neural programmer-interpreters</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06279" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. May 2-4, 2016. 2016</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Bordbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.06175</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2205.06175" />
	</analytic>
	<monogr>
		<title level="j">A generalist agent</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/48237" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="8583" to="8595" />
		</imprint>
	</monogr>
	<note>d9f2dea8c74c2a72126cf63d933-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Hash layers for large sparse models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/92" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="17555" to="17566" />
		</imprint>
	</monogr>
	<note>bf5e6240737e0326ea59846a83e076-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Routing networks: Adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ry8dvM-R-" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<title level="m" type="main">Routing networks and the challenges of modular and compositional computation</title>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.12774" />
		<imprint>
			<date type="published" when="1904">1904.12774. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">On the efficiency of adapters in transformers</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilman</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><surname>Adapterdrop</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="1/2021.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="7930" to="7946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<ptr target="https://aclanthology.org/2021.emnlp-main.626" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno>CoRR, abs/1706.05098</idno>
		<ptr target="http://arxiv.org/abs/1706.05098" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Latent multi-task architecture learning</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/4410/4288" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4822" to="4829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-5004</idno>
		<ptr target="https://aclanthology.org/N19-5004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">XTREME-R: Towards more challenging and nuanced multilingual evaluation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.802</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.802" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="10215" to="10245" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<monogr>
		<title level="m" type="main">Progressive neural networks</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>CoRR, abs/1606.04671</idno>
		<ptr target="http://arxiv.org/abs/1606.04671" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">A hierarchical multi-task approach for learning embeddings from semantic tasks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016949</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016949" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019. 2019</date>
			<biblScope unit="page" from="6949" to="6956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/eae15" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>aabaa768ae4a5993a8a4f4fa6e4-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trishala</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2110.08207.pdf" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Contextual adapters for personalized speech recognition in neural transducers</title>
		<author>
			<persName><forename type="first">Thejaswi</forename><surname>Kanthashree Mysore Sathyendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng-Ju</forename><surname>Muniyappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinru</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><forename type="middle">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Strimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siegfried</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName><surname>Kunzmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP43922.2022.9746126</idno>
		<ptr target="https://doi.org/10.1109/ICASSP43922.2022.9746126" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore</title>
		<imprint>
			<date type="published" when="2022-05-27">23-27 May 2022. 2022</date>
			<biblScope unit="page" from="8537" to="8541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Domain adaptation and multi-domain adaptation for neural machine translation: A survey</title>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.1.13566</idno>
		<ptr target="https://doi.org/10.1613/jair.1.13566" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="351" to="424" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
		<ptr target="https://aclanthology.org/2021.eacl-main.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.185" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahana</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00434</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00434" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1408" to="1424" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">On causal and anticausal learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<ptr target="http://icml.cc/2012/papers/625.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning, ICML 2012</title>
		<meeting>the 29th International Conference on Machine Learning, ICML 2012<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-01">June 26 -July 1, 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2021.3058954</idno>
		<ptr target="https://doi.org/10.1109/JPROC.2021.3058954" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<monogr>
		<title level="m" type="main">VerbNet: A broad-coverage, comprehensive verb lexicon</title>
		<author>
			<persName><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<ptr target="https://repository.upenn.edu/dissertations/AAI3179808" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="DOI">10.1145/3381831</idno>
		<ptr target="https://doi.org/10.1145/3381831" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1ckMDqlg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b278">
	<monogr>
		<title level="m" type="main">Mixture-of-experts meets instruction tuning:a winning combination for large language models</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<monogr>
		<title level="m" type="main">Moduleformer: Modularity emerges from mixture-of-experts</title>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyou</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Language Models are Multilingual Chain-of-Thought Reasoners</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2210.03057" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2023</title>
		<meeting>ICLR 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcss.1995.1013</idno>
		<ptr target="https://doi.org/10.1006/jcss.1995.1013" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="150" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2038</idno>
		<ptr target="https://aclanthology.org/P16-2038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Same neurons, different languages: Probing morphosyntax in multilingual pre-trained models</title>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Stanczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Torroba Hennigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.114</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="1589" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning</title>
		<author>
			<persName><forename type="first">Asa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/stickland19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5986" to="5995" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Multilingual domain adaptation for NMT: decoupling language and domain information with adapters</title>
		<author>
			<persName><forename type="first">Asa</forename><surname>Cooper Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilina</forename><surname>Nikoulina ; Rajen Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Martins</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.64" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021</title>
		<editor>
			<persName><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</editor>
		<meeting>the Sixth Conference on Machine Translation, WMT@EMNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">November 10-11, 2021. 2021</date>
			<biblScope unit="page" from="578" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Many task learning with task routing</title>
		<author>
			<persName><forename type="first">Gjorgji</forename><surname>Strezoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanne</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Worring</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00146</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00146" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02">October 27 -November 2, 2019. 2019</date>
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1355</idno>
		<ptr target="https://aclanthology.org/P19-1355" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Learning sparse sharing architectures for multiple tasks</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/6424" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8936" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<monogr>
		<title level="m" type="main">BBTv2: pure black-box optimization can be comparable to gradient descent for few-shot learning</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengfu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.11200</idno>
		<idno>CoRR, abs/2205.11200</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2205.11200" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">AdaShare: Learning what to share for efficient deep multi-task learning</title>
		<author>
			<persName><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogério</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/634841" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>a6831464b64c072c8510c7f35c-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Training neural networks with fixed sparse masks</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/cb2653" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="24193" to="24205" />
		</imprint>
	</monogr>
	<note>f548f8709598e8b5156738cc51-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">VL-ADAPTER: parameter-efficient transfer learning for visionand-language tasks</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.00516</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.00516" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">June 18-24, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Two problems with back propagation and other steepest descent learning procedures for networks</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="https://cir.nii.ac.jp/crid/1572824499995923584" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual Conference of the Cognitive Science Society</title>
		<meeting>the Eighth Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="page" from="823" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(99)00052-1</idno>
		<ptr target="https://doi.org/10.1016/S0004-3702(99)00052-1" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Learning hidden unit contributions for unsupervised acoustic model adaptation</title>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2016.2560534</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2016.2560534" />
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1450" to="1463" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Efficient adapter transfer of self-supervised speech models for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Bethan</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salah</forename><surname>Karout</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP43922.2022.9746223</idno>
		<ptr target="https://doi.org/10.1109/ICASSP43922.2022.9746223" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-05-27">23-27 May 2022. 2022</date>
			<biblScope unit="page" from="7102" to="7106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">Freezing subnetworks to analyze domain adaptation in neural machine translation</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Gwinnup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Koehn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6313</idno>
		<ptr target="https://aclanthology.org/W18-6313" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Residual adapters for parameter-efficient ASR adaptation to atypical and accented speech</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicky</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Padfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kara</forename><surname>Vaillancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fadi</forename><surname>Biadsy</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.541" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="6751" to="6760" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Neural arithmetic logic units</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">The role of product architecture in the manufacturing firm</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Ulrich</surname></persName>
		</author>
		<idno type="DOI">10.1016/0048-7333(94)00775-3</idno>
		<ptr target="https://doi.org/10.1016/0048-7333(94)00775-3" />
	</analytic>
	<monogr>
		<title level="j">Research Policy</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="440" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">UDapter: Language adaptation for truly Universal Dependency parsing</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.180</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="2302" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Multilingual unsupervised neural machine translation with denoising adapters</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.533</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.533" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="6650" to="6662" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Hyper-X: A unified hypernetwork for multi-task multilingual transfer</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.541" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="7934" to="7949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName><surname>Tolias</surname></persName>
		</author>
		<idno>CoRR, abs/1904.07734</idno>
		<ptr target="http://arxiv.org/abs/1904.07734" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Branched multi-task networks: Deciding what layers to share</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<ptr target="https://www.bmvc2020-conference.com/assets/papers/0213.pdf" />
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2020">September 7-10, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural algorithmic reasoning. Patterns</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">100273</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Adaptable and interpretable neural MemoryOver symbolic knowledge</title>
		<author>
			<persName><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.288</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.288" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="3678" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">Learning hidden unit contribution for adapting neural machine translation models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2080</idno>
		<ptr target="https://aclanthology.org/N18-2080" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="500" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1580</idno>
		<ptr target="https://aclanthology.org/P19-1580" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<monogr>
		<title/>
		<author>
			<persName><surname>John Von Neumann</surname></persName>
		</author>
		<idno type="DOI">10.1109/85.238389</idno>
		<ptr target="https://doi.org/10.1109/85.238389" />
		<imprint>
			<date type="published" when="1945">1945</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>First draft of a report on the EDVAC</note>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Continual learning with hypernetworks</title>
		<author>
			<persName><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJgwNerKvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in zero-shot cross-lingual generation</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.630" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="9279" to="9300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in zero-shot cross-lingual generation</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.630" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="9279" to="9300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">SPoT: Better frozen model adaptation through soft prompt transfer</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.346</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.346" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5039" to="5059" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b317">
	<monogr>
		<title level="m" type="main">Modularity: Understanding the Development and Evolution of Complex Natural Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Günter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaele</forename><surname>Mezey</surname></persName>
		</author>
		<author>
			<persName><surname>Calabretta</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/4734.001.0001</idno>
		<ptr target="https://doi.org/10.7551/mitpress/4734.001.0001" />
		<editor>Werner Callebaut and Diego Rasskin-Gutman</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note>Natural selection and the origin of modules</note>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/4496" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
	<note>bf24afe7fab6f046bf4923da8de6-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Bottleneck low-rank transformers for low-resource spoken language understanding</title>
		<author>
			<persName><forename type="first">Pu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2022-10801</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2022-10801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association</title>
		<meeting>Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association<address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="1248" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Infusing Knowledge into Pre-Trained Models with Adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>K-Adapter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.121</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.121" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="page" from="1405" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">AdaMix: Mixture-of-adaptations for parameter-efficient model tuning</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.388" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="5744" to="5760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">On negative interference in multilingual models: Findings and a meta-learning treatment</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.359</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4438" to="4450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=F1vEjWK-lH_" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021b</note>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">Interpreting layered neural networks via hierarchical modular representation</title>
		<author>
			<persName><forename type="first">Chihiro</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-36802-9_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-36802-9_40" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -26th International Conference</title>
		<meeting><address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12-12">2019. December 12-15, 2019. 2019</date>
			<biblScope unit="volume">1143</biblScope>
			<biblScope unit="page" from="376" to="388" />
		</imprint>
	</monogr>
	<note>Proceedings ICONIP</note>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Do prompt-based models really understand the meaning of their prompts?</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.167</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.167" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="2300" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gEZrGCozdqR" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2022a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gEZrGCozdqR" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2022b</note>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<monogr>
		<title level="m" type="main">Toward a theory of reinforcement-learning connectionist systems</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno>NU-CCS-88-3</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Northeastern University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
		<ptr target="https://doi.org/10.1007/BF00992696" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Lightweight and efficient end-to-end speech recognition using low-rank transformer</title>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9053878</idno>
		<ptr target="https://doi.org/10.1109/ICASSP40776.2020.9053878" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">May 4-8, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6144" to="6148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">Supermasks in superposition</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><surname>Farhadi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/wortsman22a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="23965" to="23998" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">UFO: unified feature optimization</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lufei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19809-0_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19809-0_27" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022 -17th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><surname>Shai Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moustapha</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022</date>
			<biblScope unit="volume">13686</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVI</note>
</biblStruct>

<biblStruct xml:id="b336">
	<monogr>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<title level="m">Leshem Choshen, Colin Raffel, and Mohit Bansal. Resolving interference when merging models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<monogr>
		<title level="m" type="main">Exploring sparse expert models and beyond</title>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/2105.15082</idno>
		<ptr target="https://arxiv.org/abs/2105.15082" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Task representations in neural networks trained to perform many cognitive tasks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Guangyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhura</forename><forename type="middle">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jing</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-018-0310-2</idno>
		<ptr target="https://doi.org/10.1038/s41593-018-0310-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="306" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkhU2fcll" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b340">
	<analytic>
		<title level="a" type="main">Retrieval-augmented multimodal language modeling</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/yasunaga23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jonathan</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="39755" to="39769" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">CrossFit: A few-shot learning challenge for cross-task generalization in NLP</title>
		<author>
			<persName><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.572</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.572" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="7163" to="7189" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<analytic>
		<title level="a" type="main">Speechmoe2: Mixture-of-experts model with improved routing</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP43922.2022.9747065</idno>
		<ptr target="https://doi.org/10.1109/ICASSP43922.2022.9747065" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-05-27">23-27 May 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<analytic>
		<title level="a" type="main">Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1xnXRVFwH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main">Augmentation-adapted retriever improves generalization of language models as generic plug-in</title>
		<author>
			<persName><forename type="first">Zichun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.136</idno>
		<ptr target="https://aclanthology.org/2023.acl-long.136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2421" to="2436" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b345">
	<monogr>
		<title level="m" type="main">SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2203.03312</idno>
		<ptr target="https://arxiv.org/abs/2203.03312" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with adapter</title>
		<author>
			<persName><forename type="first">Rongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/2111.00667</idno>
		<ptr target="https://arxiv.org/abs/2111.00667" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-4_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10599-4_7" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b348">
	<monogr>
		<title level="m" type="main">Emergent modularity in pre-trained transformers</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=XHuQacT6sa6" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Masking as an efficient alternative to finetuning for pretrained language models</title>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.174</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="2226" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/zhao21c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b351">
	<monogr>
		<title level="m" type="main">Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.03885</idno>
		<idno>CoRR, abs/2210.03885</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2210.03885" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Factual probing is [MASK]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.398</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.398" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<monogr>
		<title level="m" type="main">AutoPEFT: automatic configuration search for parameter-efficient fine-tuning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.12132</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2301.12132" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<author>
			<persName><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/1113" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="3592" to="3602" />
		</imprint>
	</monogr>
	<note>d7a76ffceca1bb350bfe145467c6-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Conditional prompt learning for visionlanguage models</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01631</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.01631" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">June 18-24, 2022. 2022</date>
			<biblScope unit="page" from="16795" to="16804" />
		</imprint>
	</monogr>
	<note>CVPR 2022</note>
</biblStruct>

<biblStruct xml:id="b356">
	<monogr>
		<title level="m" type="main">Mixture-of-experts with expert choice routing</title>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<idno>CoRR, abs/2202.09368</idno>
		<ptr target="https://arxiv.org/abs/2202.09368" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Towards better meta-initialization with task augmentation for kindergarten-aged speech recognition</title>
		<author>
			<persName><forename type="first">Yunzheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abeer</forename><surname>Alwan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP43922.2022.9747599</idno>
		<ptr target="https://doi.org/10.1109/ICASSP43922.2022.9747599" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore</title>
		<imprint>
			<date type="published" when="2022-05-27">23-27 May 2022. 2022</date>
			<biblScope unit="page" from="8582" to="8586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<monogr>
		<title level="m" type="main">ST-MoE: designing stable and transferable sparse expert models</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.08906</idno>
		<ptr target="https://arxiv.org/abs/2202.08906" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Taming sparsely activated transformer with stochastic experts</title>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B72HXs80q4" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
