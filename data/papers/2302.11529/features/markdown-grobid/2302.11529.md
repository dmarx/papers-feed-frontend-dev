# Modular Deep Learning

## Abstract

## 

Transfer learning has recently become the dominant paradigm of machine learning. Pretrained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer.

## Table of Contents

## Introduction and Motivation

Transfer learning has recently become pervasive in machine learning technology, such as in natural language processing [(Ruder et al., 2019b;](#)[Brown et al., 2020)](#b38), computer vision [(Dosovitskiy et al., 2021)](#b73), and reinforcement learning [(Reed et al., 2022)](#), among other areas. In its most successful incarnation, transfer learning consists of pre-training a model on vast amounts of raw data in a self-supervised fashion and subsequently fine-tuning it for new tasks based on a small number of labelled examples. Despite its success, this paradigm for transfer learning suffers from a series of limitations in various settings. Firstly, in multi-task fine-tuning, the learning signals from different tasks may negatively interfere with each other [(McCloskey & Cohen, 1989)](#b203). Similarly, in continuous learning, adapting to new examples can result in catastrophic forgetting of knowledge acquired from previous examples [(Sutton, 1986;](#b294)[French, 1999)](#b91). [1](#foot_0) Secondly, in settings where the training and evaluation distributions are not identical, these models fail in generalising systematically [(Lake & Baroni, 2018;](#b161)[Hupkes et al., 2020)](#b136). This makes models brittle and inaccurate and hampers their deployment in real-world applications, where distribution shifts are common.

In contrast, many biological and artificial systems do not suffer from these weaknesses by virtue of their modularity [(Fodor, 1983;](#b86)[Ballard, 1986)](#b23), defined as the correspondence between strongly interconnected components of a system (i.e., modules) and the functions they perform [(Baldwin & Clark, 2000;](#b22)[Ulrich, 1995)](#b301). In other words, each module is specialised for a unique purpose, for which it is reused consistently.

In animal brains, this favours evolvability, the ability to adapt quickly to new environments, and resilience to environment perturbations [(Wagner et al., 2005)](#b317) because it makes rewiring connections easier than in monolithic, entangled networks [(Kashtan & Alon, 2005)](#b148). Artificial systems, such as programming languages and computer hardware, are similarly designed in a modular fashion [(Booch et al., 2008;](#b35)[Baldwin & Clark, 2000)](#b22) because this modular design favours consistency, ease of adaptation, and interpretability.

To what extent, then, do 'vanilla' neural networks display the desirable property of being modular? In principle, given their fully connected nature, they could develop such a structure as a by-product of optimising a loss for a downstream task. Recent structural analyses based on hierarchical clustering of neurons revealed that vanilla neural networks can indeed learn such a modular pattern [(Watanabe, 2019;](#b324)[Casper et al., 2022;](#b44)[Foroutan et al., 2022)](#b87). Favourable conditions for the emergence of modularity include multi-task learning [(Dobs et al., 2022)](#b70) and regularisation through dropout [(Lange et al., 2022)](#b163). In particular, from a structural perspective, populations of neurons may activate jointly in response to specific features of the input or the output classes[foot_1](#foot_1) , resulting in similar changes in model performance when ablated [(Meyes et al., 2020)](#b206). From a functional perspective, multi-task learning may lead to segregated, specialised sub-networks [(Yang et al., 2019;](#b338)[Dobs et al., 2022)](#b70). On the other hand, [Csordás et al. (2021)](#b62) revealed that a given sub-network does not tend to be re-used for similar sub-tasks nor to be combined with others to express more complex functions. In fact, in many cases, the performance of a model on simple tasks requiring a certain skill and composite tasks requiring a combination thereof is entirely uncorrelated [(Li et al., 2022a)](#).

For this reason, previous work explored the idea of designing neural networks that are explicitly modular [(Jacobs et al., 1991a;](#)[Rosenbaum et al., 2018;](#b256)[Ponti, 2021;](#)[Mittal et al., 2022)](#). This has the goal of achieving not only functional specialisation [(Zhang et al., 2022b)](#), but also re-usability and composability. In particular, these methods involve identifying 1) modules in a neural network that can be updated locally and asynchronously, without affecting the rest of the parameters; 2) a routing function that chooses a subset of modules for each example or task; and 3) an aggregation function that aggregates the outputs of the active modules. Each of these three ingredients can be manually specified or learned. We provide several case studies of different configurations of these components in Figure [1](#).

The main advantages of modular neural architectures are positive transfer, compositionality, and parameter efficiency. Firstly, modularity encourages positive transfer by encoding similar functions with the same module. At the same time, it prevents interference and forgetting by allocating distinct functions to different dedicated modules [(Jacobs et al., 1991b)](#). For instance, massively multilingual Transformer-based models in NLP are 1a) MAD-X [(Pfeiffer et al., 2020b)](#) uses Adapter layers with fixed routing for zero-shot cross-lingual transfer. 1b) Polytropon [(Ponti et al., 2022)](#) uses low-rank adapters (LoRA; [Hu et al., 2022)](#b132) with hard learned routing for few-shot task adaptation. 1c) MoE Transformers [(Fedus et al., 2021;](#b82)[Clark et al., 2022, inter alia)](#) use Multi-Layer Perceptrons with top-k soft routing, in order to scale to larger model sizes. The three representative models illustrated here are only a fraction of possible configurations from the 'configuration manifold' that can be created by varying the components surveyed in §3- §6.

known to suffer from a 'curse of multilinguality' [(Conneau et al., 2020)](#b58) due to the conflicting information that the gradient from each language-specific loss carries [(Wang et al., 2021b)](#). A possible solution is augmenting these entangled, fully shared models with specialised modules responsible for individual languages [(Pfeiffer et al., 2020b;](#)[2022b)](#). More generally, as the range of tasks modelled jointly by a single model becomes increasingly diverse, modularity may be instrumental in the advent of general-purpose, multi-modal agents that encompass vision, language, and action [(Reed et al., 2022)](#).

Secondly, modules representing different skills (at the task level) or features (at the example level) can be composed together and updated locally, without affecting the rest of the network. These two properties are crucial in two main settings, which correspond to different aspects of systematic generalisation: one is the ability to re-compose, i.e. zero-shot transfer to tasks consisting of new combinations of learned skills, or examples consisting of new combinations of observed features [(Hupkes et al., 2020)](#b136). For instance, while modules for the Guaraní language and for dependency parsing can only be trained separately due to the lack of annotated data for dependency parsing in Guaraní, they can be composed to perform inference on this unobserved task-language combination [(Pfeiffer et al., 2020b)](#). Similarly, in hierarchical reinforcement learning, an agent can follow different sequences of modular policies known as options in tasks requiring the completion of similar sub-goals in different orders [(Sutton et al., 1999;](#b295)[Precup, 2000)](#b245). The other aspect of systematic generalisation is robustness. In fact, if modules are taken to correspond to independent and reusable physical mechanisms [(Schölkopf et al., 2012)](#b273), local shifts in their distributions require updating only the parameters accounting for the affected skills or features [(Goyal et al., 2021;](#b102)[Schölkopf et al., 2021)](#b274), while the rest of the model remains invariant to the change. In practice, the ability to perform local updates facilitates sample efficiency, as fewer examples are necessary to adapt models to new tasks [(Bengio et al., 2020;](#b29)[Ponti et al., 2022)](#).

Thirdly, an additional advantage of modular neural architectures is parameter and time efficiency. In this framework, fine-tuning a model on a specific task only requires storing a modular adapter rather than a separate copy of the entire (typically large) model. What is more, modules can be added or removed on-the-fly in an incremental manner, adjusting the model capacity according to the task complexity. This ability is known as conditional computation [(Bengio et al., 2015)](#b27). Finally, modularity enables language models to scale to larger numbers of parameters while retaining the same time complexity, by selecting only a small set of experts per example [(Shazeer et al., 2017;](#b277)[Fedus et al., 2021)](#b82).

As the main contribution of this survey, we offer a unified view of modular deep learning, illustrating how many families of methods can be defined along four key dimensions: 1) how they implement modules, which constitute the minimum unit of computation; 2) how they select active modules through a routing function;

3) how module outputs are aggregated; and 4) how the modules are trained with the rest of the model.

For module implementation, we discuss sparse subnetworks [(Hu et al., 2022;](#b132)[Ansell et al., 2022)](#b11), adapter layers [(Rebuffi et al., 2018;](#b251)[Pfeiffer et al., 2020b)](#), and prefix tuning [(Li & Liang, 2021)](#), among others. These methods have been proven as an effective way to adapt large pre-trained models, achieving better performance and sample efficiency than alternative strategies such as in-context learning [(Liu et al., 2022b)](#), which may be brittle [(Lu et al., 2022)](#b192). In fact, modules can also take the form of human-engineered prompts, where the model is provided with input-output examples [(Brown et al., 2020)](#b38) or task instructions [(Wei et al., 2022a)](#). While many module implementations share the same underlying functional form [(He et al., 2021)](#b124), they offer different trade-offs between efficiency and performance.

We then discuss how routing functions control the flow of information to the modules: in fixed routing, module allocation is manually defined when expert knowledge is available [(Hampshire & Waibel, 1992;](#b117)[Rajendran et al., 2017, inter alia)](#). In learned routing, a parameterised routing function is inferred during training. This, however, poses a series of challenges, such as training instability, module collapse, and overfitting [(Rosenbaum et al., 2019)](#b257). Orthogonally, we also distinguish between hard and soft routing. In hard routing, only a subset of modules is activated [(Rosenbaum et al., 2018;](#b256)[Ponti et al., 2022;](#)[Fernando et al., 2017, inter alia)](#). In soft routing, all modules are aggregated according to continuous scores [(Jacobs et al., 1991b;](#)[Jordan & Jacobs, 1994)](#b145). While soft routing is amenable to vanilla gradient descent, it is highly inefficient. On the other hand, hard routing requires approximate inference but facilitates conditional computation and module specialisation.

When multiple modules are selected, several aggregation strategies are possible. For instance, these can be based on interpolating the parameters of active modules [(Ansell et al., 2022)](#b11) or an attention mechanism over the module outputs [(Pfeiffer et al., 2021a)](#). Alternative methods include input prompt concatenation [(Vu et al., 2022b)](#) and function composition [(Andreas et al., 2016b)](#).

Finally, modules can be trained jointly with the rest of the base model in multi-task learning [(Caruana, 1997;](#b42)[Ruder, 2017)](#b260), added sequentially in classic continual learning [(Rusu et al., 2016)](#b264), or integrated post-hoc into an already pre-trained and frozen model [(Rebuffi et al., 2017;](#b250)[Houlsby et al., 2019)](#b129). The last scenario is most common with current state-of-the-art models, which are trained as dense, fully shared models and may be 'modularised' after pre-training.

Crucially, this taxonomy reveals unexpected connections between several independent threads of research, including aggregation functions and mode connectivity [(Frankle et al., 2020)](#b89), routing and hypernetworks [(Ha et al., 2017)](#b115), among others.

We further illustrate a series of applications of modular networks in transfer learning across different areas such as natural language processing, computer vision, and speech processing. In addition, we show how modularity plays an important role in causal inference and discovery, programme simulation, and hierarchical reinforcement learning.

We hope that our overview will spark future research on modular deep learning in areas that may benefit from it such as community-driven efforts to develop and maintain machine learning technology.

## Modular Deep Learning

This survey focuses on modular deep learning: namely, on models composed of modules. These are autonomous computation functions that, depending on their architecture and purpose, are variously referred to as adapters [(Rebuffi et al., 2017;](#b250)[Pfeiffer et al., 2020a)](#), options [(Sutton et al., 1999;](#b295)[Precup, 2000)](#b245), or experts [(Jacobs et al., 1991a;](#)[Jordan & Jacobs, 1994)](#b145). Crucially, these modules are distinguished from a routing function, which controls the information flow to the modules. Finally, an aggregation function aggregates their outputs.

Modules can be optionally combined with fully shared (thus, non-modular) parameters as part of the same neural architecture. In order to provide a unified view of the landscape of modular deep learning, we create a taxonomy of four dimensions of variation: computation, routing, aggregation, and training. These dimensions are mutually independent; hence, many methods can be interpreted as different combinations of these dimensions, listed in § 2.1. Concurrently, we provide a unified, consistent notation in § 2.2, which helps illuminate the relationship among such methods.

## Taxonomy

## 1) Computation function:

How is each module implemented? ( § 3) A module may consist of any component of a neural architecture, such as multiple copies of a model [(Jacobs et al., 1991a)](#) or one of its layers [(Fedus et al., 2021)](#b82). Alternatively, as it is common in transfer learning, modules can be combined with a function parameterised by fully shared pre-trained weights. In this case, we distinguish between modification of parameters (parameter composition), concatenation with input features (input composition), and function composition by stacking neural modules.

2) Routing function: How are active modules selected? ( § 4) Under fixed routing, we categorise approaches where the routing function is fixed. This assumes that the specialisation of each module, as well as the combination of modules required for each task, is known a priori. In learned routing, the parameters of the routing mechanism are learned during training. In this case, routing is soft if all modules are ranked through a continuous score, or hard if each module is given a binary score (active or inactive).

3) Aggregation function: How are the outputs of the active modules aggregated? [( § 5)](#) We differentiate between methods that compose the outputs of the active modules deterministically (e.g., based on a weighted average) from those where the aggregation function is implemented as a learnable neural network that depends on the output of all modules.

## 4) Training setting:

How are the modules trained? ( § 6) Some methods, such as MoEs, train the modules (and possibly the routing function) jointly with the shared weights of a randomly initialised model. As an alternative, transfer learning approaches introduce modules post-hoc after pre-training weights and adapt them during fine-tuning. In continuous learning settings, instead, new modules may be introduced iteratively for every new task in a sequence.

## Notation

More formally, let a neural network f θ : X → Y be decomposed into a graph of sub-functions. In the simplest case, this graph is a linear chain

$f θ1 • f θ2 • • • • • f θ l$, where • stands for function composition. These sub-functions refer to the model's l layers, each with unique indexed parameters θ i , i = 1, . . . , l. [3](#foot_2) In turn, these can be further decomposed recursively into a graph of their constituent sub-functions: for instance, a Transformer layer [(Vaswani et al., 2017)](#b307) includes linear mappings for the query, key, value, and output, as well as a non-linear feed-forward network, and residual connections. We further denote the values of the parameters at initialisation as θ 0 , and the parameters after training are denoted as θ ⋆ .

Any i-th sub-function with input x can be modified by a module with parameters ϕ from the inventory

$M i = f ϕ1 , . . . , f ϕ |M | in the following different ways: Notation Definition x ∈ X Input data y ∈ Y Output data h ∈ H Hidden representation t ∈ T Task index f : X ∪ H → Y ∪ H A computation function θ Shared parameters M = {ϕ 1 , . . . , ϕ |M | } Set of module parameters α ∈ A$Vector of routing scores r : X ∪ H ∪ T → A

## Routing function ρ

Routing parameters g Aggregation function γ

Aggregation parameters

Table [1](#): Notation and definition of important variables, functions, and operators.

1. parameter composition:

$f ′ i (x) = f θi⊕ϕ (x)$, where ⊕ stands for an operation that composes the original parameters with the module parameters, such as element-wise addition. An example is low-rank [(Hu et al., 2022)](#b132) or sparse [(Ansell et al., 2022)](#b11) adapters. [([ϕ, x]](#)), where [•, •] stands for concatenation. An example is prefix tuning [Li & Liang (2021)](#).

## input composition: f

$′ i (x) = f θi$
## function composition: f

$′ i (x) = f ϕ • f θi (x)$, where the outputs of the first function is fed into the second function. An example are adapter layers [(Rebuffi et al., 2017)](#b250).

For each i-th sub-function, multiple modules from the inventory M i can be selected through a routing function r(•), which returns a score α j for each module f ϕj conditioned on the data itself, such as a language token or visual region x or the full input x, or metadata such as the task identity t ∈ T . Note that α can be fixed a priori through expert knowledge or learned through an appropriate parameterisation r ρ [(•)](#), where ρ refers to (learnable) parameters of the routing function. Often, the routing function takes special forms:

1. In hard routing, α ∈ {0, 1} |M | is a discrete binary vector. If these parameters are learned, inference usually relies on score function estimators, stochastic re-parameterisation, or evolutionary algorithms.

2. In soft routing, α ∈ [0, 1] |M | is a continuous probability distribution, such that j α j = 1.

3. Finally, α ∈ R |M | can be an unnormalised score vector. This is the case in linear hypernetworks [(Ha et al., 2017)](#b115), where α is usually interpreted as a task embedding and the row-wise stacked module parameters Φ = [ϕ 1 , . . . , ϕ |M | ] act as a parameter generator.

Finally, the output of each module is combined through an aggregation function g(•). [4](#foot_3) The aggregation function usually takes two possible forms. One consists of a deterministic operation based on the routing scores (e.g., weighted averaging of module parameters or outputs). The other consists of a learnable neural network, such as an attention mechanism between the modules' inputs and outputs [(Pfeiffer et al., 2021a)](#). When we put the computation function, routing function, and aggregation function together, we obtain the general recipe for a modular function, illustrated in Algorithm 1.

## Computation Function

The computation function determines the design of a module. Various module architectures have been proposed such as MLP layers [(Rosenbaum et al., 2018;](#b256)[Kirsch et al., 2018;](#b155)[Chang et al., 2019)](#b45), independent RNNs [(Goyal et al., 2021)](#b102), independent CNNs [(Parascandolo et al., 2018)](#b226), or special-purpose architectures [(Andreas et al., 2016b)](#). However, in transfer learning, modules are most often integrated into a base architecture whose parameters are fully shared. We identify three core methods to merge a single module with the corresponding sub-function: parameter composition, input composition, and function composition. While all three methods instantiate modules differently, we demonstrate how they can be seen in a unified view in § 3.5. We provide example illustrations of the three computation functions (in addition to a hypernetwork) as part of a Transformer architecture in Figure [2](#fig_3) and provide a high-level overview of their trade-offs in Table [3](#), which we further discuss in the respective sections. 5

## Parameter Composition

Parameter composition methods augment the function f W of a base model with weights W ∈ R o×i with module parameters Φ ∈ R o×i , where i is the input dimensionality, and o is the output dimensionality. In particular, the module inventory consists of a set of sparse or low-rank weights to ensure that the modules are parameter-efficient. Therefore, the resulting function is parameterised as f θ⊕ϕ i , where ⊕ stands for element-wise addition.

Sparse Subnetworks Sparsity is a common inductive bias based on the assumptions (i) that only a small number of parameters of an over-parameterised model are relevant for a particular task, and that (ii) similar tasks share similar sub-networks. This is the case, for instance, for language subnetworks in multilingual language models [(Stanczak et al., 2022;](#b284)[Foroutan et al., 2022)](#b87).

The most widespread method to induce sparsity is pruning. This can be interpreted as the application of a binary mask b ∈ {0, 1} |θ| that selectively keeps or removes each connection in a model with trained parameters θ ⋆ : f ′ = f θ ⋆ ⊙b where ⊙ is element-wise multiplication. The merger of θ and b results in a sparse subnetwork, but the corresponding model parameters usually remain dense for hardware and software reasons. 6 After training, the trained weights are sorted based on a criterion and a fraction (bottom-k) of the weights are set to zero. Examples of criteria include magnitude after convergence [(Han et al., 2017)](#b119) and change of magnitude between initialisation and convergence [(Frankle & Carbin, 2019)](#b88).

$5$The comparison is mainly meant as a high-level guideline. Individual methods may have different trade-offs and mitigate certain weaknesses indicated in the table.

6 In fact, sparse linear algebra operations on graphic processing units remain highly inefficient, if available at all. Examples include the sparse tensor classes in Pytorch: [https://pytorch.org/docs/stable/sparse.html](https://pytorch.org/docs/stable/sparse.html)

## Method

Reference Function

## Computation function

## Sparse subnetwork

Frankle & Carbin ( [2019](#))

$f ′ = f θ ⋆ ⊙b Supermasks Wortsman et al. (2020) f ′ = f θ0⊙b Sparse fine-tuning Ansell et al. (2022) f ′ = f θ+b⊙ϕ Intrinsic dimension Li et al. (2018) f ′ = f θ+ϕM Low-rank adaptation Hu et al. (2022) f ′ i = f θi+vec(BiAi) Prompting Brown et al. (2020) f ′ 1 = f θ1 ([ϕ, x]) where ϕ = Emb(p) Retrieval augmentation Guu et al. (2020) f ′ 1 = f θ1 ([ϕ, x]) where ϕ = Emb([p, c]) Prompt tuning Lester et al. (2021) f ′ 1 = f θ1 ([ϕ, x]) Multi-layer prompt tuning Li & Liang (2021) f ′ i = f θi ([ϕ i , x]) Parameter sharing Ruder (2017) f t ϕi = f s ϕi ∀i ∈ G Convolutional adapter Rebuffi et al. (2017) f ′ i = f ϕi (f θi (x)) where f ϕi (x) = F * x Transformer adapter Houlsby et al. (2019) f ′ i = f ϕi (f θi (x)) where f ϕi (x) = W d (σ(W u x)) Compacter Mahabadi et al. (2021a) f ′ i = f ϕi (f θi (x))$where

$f ϕi (x) = W d (σ(W u x)), W = n j=1 A j ⊗ B j Parallel adapter Rebuffi et al. (2018) f ′ i = f θi (x) + f ϕi (x) Rescaling Bilen & Vedaldi (2017) f ′ i = f θi (x) ⊙ ϕ Hypernetwork Platanios et al. (2018) f ′ i = (αW )x$
## Routing function

Fixed routing [Hampshire & Waibel (1992)](#b117)

$f ′ i = 1 |K| j f (ϕ j ) 1 j (K)$Top-1 learned routing Rosenbaum et al. ( [2018](#))

$f ′ i = f (x; θ i , ϕ j ) where j = argmax[α] Top-k learned routing Goyal et al. (2021) f ′ i = cat j∈ top k [α] f (x; θ i , ϕ j ) Variable-size (threshold) Rahaman et al. (2021) f ′ i = cat j∈M s.t. αj >t f (x; θ i , ϕ j ) Variable-size (soft partition) Ponti et al. (2022) f ′ i = 1 α j∈M s.t. αj =1 f (x; θ i , ϕ j ) Mixture of experts Jacobs et al. (1991b) f ′ i = j∈M α j f (x; θ i , ϕ j ) Weighted top-k routing Shazeer et al. (2017) f ′ i = j∈ top k [α] αj α f (x; θ i , ϕ j )$Aggregation function Sparse weight addition Ansell et al. (2022) f ′ = f θ0+ϕl+ϕt Representation averaging Ma et al. (2018)

$f ′ i = |Mi| j α j h j Input concatenation Vu et al. (2022b) f ′ i = f θ ([ϕ t , ϕ l , x]) Attention-based aggregation Pfeiffer et al. (2021a) f ′ i = Attn(h i-1 Q i , H i K i , H i V i ) Sequential aggregation Pfeiffer et al. (2020b) f ′ i = f ϕt (f ϕl (f θ0 (x)))$Table 2: An overview of representative computation, routing, and aggregation functions. Each method is paired with a representative reference. In computation functions, skip connections are omitted for simplicity. Definitions: the model f , a model's sub-function f i , model parameters θ, module parameters ϕ, parameters at initialisation θ 0 , parameters after training θ ⋆ , binary mask b ∈ {0, 1} |θ| , random matrix M, group G, input x, a model's embedding layer Emb(•), text prompt p, retrieved context c, filter bank F , routing scores or task embedding α, routing function r, subset of modules K, module inventory M .  A small separate neural network generates modular parameters conditioned on metadata. We show its application to function composition but it is compatible with all computation functions.

As pruning generally leads to a loss in performance due to the change in network connections, the non-pruned weights are typically re-wound to their initialisation value and re-trained. In practice, rather than pruning all weights in a single run, iterative pruning is carried out over multiple stages [(Han et al., 2015;](#b118)[Frankle & Carbin, 2019)](#b88). The models pruned in this fashion often retain-if not surpass -the performance of the original dense model. The existence of a subnetwork with this property in any given randomly initialised model is known as the Lottery Ticket Hypothesis (LTH; [Frankle & Carbin, 2019;](#b88)[Chen et al., 2020)](#b46). These 'winning tickets' have also been shown to exist in RL and NLP [(Yu et al., 2020)](#b343), as well as in computer vision [(Frankle et al., 2020)](#b89). Subnetworks achieve above-random performance even when kept fixed at their random initialisation [(Zhou et al., 2019;](#b354)[Wortsman et al., 2020;](#b333)[Zhao et al., 2020)](#b349), so f ′ = f θ0⊙b . In this case, they are known as supermasks.

Winning tickets also occur in pre-trained models, such as language models [(Chen et al., 2020;](#b46)[Prasanna et al., 2020)](#b244). These often outperform tickets from randomly initialised models [(Prasanna et al., 2020)](#b244) and are less sensitive to specific hyper-parameter choices [(Sun et al., 2020a)](#). Magnitude pruning, which relies on zeroth-order information (the absolute value of a weight), is sub-optimal in this setting as fine-tuned weights typically stay close to their pre-trained values. Thus, magnitude pruning selects a similar set of weights for pruning regardless of the downstream task. Pruning based on first-order (gradient-based) information better captures the task-specific relevance of each weight [(Molchanov et al., 2017)](#b214). For instance, movement pruning [(Sanh et al., 2020)](#b266) learns the mask b jointly with the parameters θ. As the mask is a discrete binary variable, they rely on straight-through estimators [(Bengio et al., 2013)](#b28). Alternatively, b can be first learned as a real-valued mask and then binarised via a thresholding function [(Mallya et al., 2018)](#).

In addition to pruning, sparsification techniques can be employed for adaptation. In particular, a sparse module ϕ can be merged with pre-trained parameters θ. For instance, in Sparse Fine-Tuning (SFT; [Ansell et al., 2022)](#b11) the LTH is re-purposed such that, instead of zeroing out weights with the lowest change in magnitude, they are simply frozen. Thus, only a subset of weights is fine-tuned. [7](#foot_5) The difference between these and the original pre-trained model results in a sparse module ϕ where

$ϕ i = 0 if b i = 0,$which can be Parameter Training Inference Performance Compositionality efficiency efficiency efficiency Parameter composition + -++ + + Input composition ++ ---+ Function composition -+ -++ + Table 3: Comparison of computation functions along different dimensions. See the end of § 3.1 (parameter composition), § 3.2 (input composition), and § 3.3 (function composition) for further explanation. Compositionality is discussed in § 5.

plugged in and out of the model as f ′ θ = f θ⊕ϕ . Diff pruning [(Guo et al., 2021)](#b109) instead obtains a sparse adapter by fine-tuning a dense difference vector ϕ regularised to be sparse with a differentiable approximation to the L 0 -norm penalty. [Sung et al. (2021)](#b292) induce a fixed sparse mask by selecting the top-k weights ranked according to (a diagonal approximation of) their Fisher information. This second-order information reveals the impact of the change of a parameter on the model predictions. Thus,

$b j = 1 if j ∈ top-k 1 n n i=1 Ey∼f θ ⋆ (y|xi) (∇ θ log f θ ⋆ (y | x i )) 2 0 otherwise (1)$Beyond the sparsification of individual weights, sparse model adaptation can also be structured. In this case, only a group of model sub-functions is fine-tuned, while the rest of the parameters remain frozen. The most common setting is for such a group to correspond to a subset of layers, e.g. the last one [(Donahue et al., 2014)](#b72). Groups can also relate to more fine-grained parts of the model. For instance, a group consisting of a model's bias parameters is a practical choice as this removes the need to store the model's intermediate activations [(Cai et al., 2020;](#b41)[Ben Zaken et al., 2022)](#b26). At the level of parameter tensors, some methods prune filters in CNNs [(Anwar et al., 2017;](#b12)[Newell et al., 2019)](#b221) whereas others prune attention heads in pre-trained Transformers [(Voita et al., 2019;](#b311)[Michel et al., 2019)](#b208). In structured diff pruning, members of a group are encouraged to share the same mask value [(Guo et al., 2021)](#b109).

Low-Rank Modules Similar to sparsity, another efficient solution is for the module parameters ϕ i to lie in a low-dimensional subspace. [Li et al. (2018)](#b174) show that models can be optimised in a low-dimensional, randomly oriented subspace rather than the full parameter space. In this setting, the module parameters ϕ ∈ R d are low-dimensional compared to the model parameters θ ∈ R D and d ≪ D. A random matrix M ∈ R d×D can be used to project from d to D: f ′ θ = f θ+ϕM . An efficient way to compute M is via the Fastfood transform [(Le et al., 2014)](#b167), which factorises M as random linear matrices. Specifically, M = HGΠHB consists of a Hadamard matrix H, a random diagonal matrix with independent standard normal entries G, a random diagonal matrix with equal probability ±1 entries B, and a random permutation matrix Π. [Li et al. (2018)](#b174) refer to the minimum d that achieves within 90% of the full-parameter model performance as the intrinsic dimensionality of a given task. [Aghajanyan et al. (2021)](#b0) investigate the intrinsic dimensionality of various NLP tasks with different pre-trained models. They observe that it decreases during pre-training and that larger models have lower values.

However, storing the random matrices results in a substantial memory overhead and is slow to train [(Mahabadi et al., 2021a)](#). If the weight matrix W ∈ R o×i is small enough, we can directly compose it into low-rank matrices W = λBA where A ∈ R k×i and B ∈ R o×k , where i is the input dimensionality, o is the output dimensionality, k is the rank of the matrix, and λ is a scaling hyper-parameter. To save space, the factorisation may be only applied to certain groups of parameters G. In LoRA [(Hu et al., 2022)](#b132), this group corresponds to the linear projections in the self-attention mechanisms of each Transformer layer:

$f ′ j = f θj +vec(Bj Aj ) ∀f ′ j ∈ G.$Overall, parameter composition methods (both sparse and low-rank) are very parameter-efficient and often require updating less than 0.5% of a model's parameters [(Guo et al., 2021)](#b109). At inference time, they keep the model size constant or even reduce it, if the resulting model is sparse. This is compelling as it enables a plug-in replacement of the original with the modular model without any changes to the underlying architecture. Sparse modules, however, increase the time complexity of optimisation as they typically require multiple iterations of re-training. Finally, state-of-the-art parameter composition methods, e.g., LoRA [(Hu et al., 2022)](#b132) and SFT [(Ansell et al., 2022)](#b11) achieve strong performance in zero-shot and few-shot transfer.

## Input Composition

Input composition methods augment a function's input x by concatenating it with a parameter vector ϕ i :

$f ′ i (x) = f θi ([ϕ i , x]).$The most common strategy is to augment the input fed to the model's first layer f 1 .

Prompting In a prompting setup with auto-regressive language models [(Brown et al., 2020)](#b38) or encoders [(Schick & Schütze, 2021a;](#)[b)](#), the input prompt p consists of (optional) instructions and (optional) in-context examples that have been converted to natural language. From a different perspective, the task-specific text prompt, when encoded using the model's embedding layer Emb(•), corresponds to modular parameters ϕ that elicit the desired behaviour [(Gao et al., 2021b;](#)[Liu et al., 2023)](#): Emb(p) = ϕ. More elaborate prompts p such as rationales have led to improved few-shot reasoning performance [(Wei et al., 2022c;](#)[Kojima et al., 2022;](#b156)[Shi et al., 2023)](#b280). However, models are ostensibly sensitive to the formulation of the prompt as well as to the set and order of the (few-shot) examples [(Zhao et al., 2021;](#b350)[Lu et al., 2022;](#b192)[Webson & Pavlick, 2022)](#b325).

Continuous Prompts Instead, a continuous prompt vector ϕ can be learned directly [(Lester et al., 2021;](#b169)[Liu et al., 2021b;](#)[Zhong et al., 2021;](#b352)[Hambardzumyan et al., 2021)](#b116). However, if ϕ is only concatenated with the first layer's input, the model has limited capacity to adapt to a specific task. As a result, such continuous (also called soft) prompts perform poorly at smaller model sizes and on some harder tasks [(Mahabadi et al., 2021a;](#)[Liu et al., 2022c)](#). To mitigate this, initialisation via multi-task learning has been proposed [(Vu et al., 2022c)](#). As an alternative, module vectors ϕ i can be learned for each layer of the model (Figure [2b](#fig_3); [Li & Liang, 2021;](#)[Liu et al., 2022c)](#). While this increases the number of parameters, it increases the modules' capacity to adapt to a given task. In practice, module parameters in the form of prefix vectors ϕ i = P i k , P i v ∈ R l×d are prepended to the keys and values of every multi-head attention layer. Attention is defined as

$f i (x) = Attn(xW i q , CW i k , CW i v )$where W q , W k , W v ∈ R d×d h are the projections that produce the queries, keys, and values, and C ∈ R m×d is a sequence of context vectors. Multi-layer prompt tuning thus takes the following form:

$f ′ i (x) = Attn(xW i q , [P i k , CW i k ], [P i v , CW i v ]).(2)$Retrieval Augmentation Beyond individual prompts, the input can be augmented with additional context c from a retrieval model. Retrieved documents are appended to the input and are used for conditioning the language model [(Guu et al., 2020;](#b114)[Lewis et al., 2020a)](#)

$: Emb([p, c]) = ϕ.$In summary, input composition is exceptionally parameter-efficient as it only adds a very small number of parameters. However, these parameters extend a model's context window, which makes them less efficient during training and inference. Prompt tuning methods also require large models to achieve decent performance.

## Function Composition

While parameter composition deals with individual weights and input composition methods act only on a function's input, function composition methods augment the model with new task-specific sub-functions (see Figure [2c](#fig_3)):

$f ′ i (x) = f ϕi • f θi (x) = f ϕi (f θi (x))$, where • stands for function composition.

Parameter Sharing Models in multi-task learning traditionally consist of shared layers f θ stacked under task-specific modules f ϕ [(Ruder, 2017)](#b260). Conversely, given models for tasks t and s expressed as a composition of functions f t ϕ1 • . . . • f t ϕ l and f s ϕ1 • . . . • f s ϕ l , respectively, a multi-task architecture can also be obtained by tying sets of parameters between the models: f t ϕi = f s ϕi ∀i ∈ G where the group G contains the set of shared  layer indices.[foot_6](#foot_6) Many multi-task neural architectures can be characterised in terms of their definition of G, which determines which modules are task-specific and which ones are shared. This is the case, for instance, of 'shared trunk' approaches in computer vision [(Zhang et al., 2014;](#b347)[Ma et al., 2018)](#b194) and approaches with supervision at different layers in NLP [(Søgaard & Goldberg, 2016;](#b282)[Sanh et al., 2019;](#b265)[Liu et al., 2019)](#b188).

Some approaches learn finer-grained interactions between pairs of modules. [Misra et al. (2016)](#b211) propose the cross-stitch unit, which linearly combines the inputs at every layer[foot_7](#foot_7) : ( x t , x s ) = W [x t , x s ] where

$x t ij x s ij = α tt α ts α st α ss x t ij x s ij$and α ∈ R. Sluice networks [(Ruder et al., 2019a)](#) extend cross-stitch units to multiple modules per layer and additionally employ a soft selection of the skip connections from all layers at the output layer l:

$x t⊤ =   β t 1 • • • β t l   ⊤ x t 1 ⊤ , . . . ,$x t l ⊤ and β ∈ R. On the other hand, Gao et al. (2019) fuse features from multiple tasks through a 1x1 convolution. Bragman et al. (2019) employ variational inference to assign filters in a CNN to task-specific or shared roles. Rather than learning which modules should be shared among which tasks, which is a combinatorially large problem, Lu et al. (2017) and Vandenhende et al. (2020) start with a fully shared model and then dynamically widen it during training, by cloning function f θi into new modules f ϕi,1 , . . . , f ϕ i,k shared among a smaller subset of tasks, in top-down order across layers. More information on parameter-sharing strategies in multi-task learning can be found in relevant surveys [(Ruder, 2017;](#b260)[Crawshaw, 2020)](#b61).

Adapter Layers As an alternative to parameter sharing, a new task-specific learnable function f ϕi can be composed with an (often frozen) shared function f θi . As the main purpose of such modules is adapting a pre-trained model to new tasks, they are also simply known as 'adapter layers'. We provide examples of different adapter layers in Figure [3](#fig_5).

The adapter's design and composition with the pre-trained model are often modality-specific. In computer vision, the adapter typically consists of a 1×1 convolution, i.e., f ϕi (x) = F * x where F is a bank of 1×1 filters and * is the convolution operation [(Rebuffi et al., 2017)](#b250). The module is then inserted between the convolutional blocks of a pre-trained model, such as a ResNet [(He et al., 2016)](#b123). In NLP, a bottleneck architecture has become popular which consists of a down-and up-projection, coupled with an intermediate activation function [d](#) x is the dimensionality of the input (typically the hidden dimension), and k is the bottleneck dimension. σ is commonly a non-linearity such as a ReLU unit (Figure [3a](#fig_5); [Houlsby et al., 2019;](#b129)[Pfeiffer et al., 2020b)](#). In a Transformer model, adapters are placed both after the multi-head attention and the feed-forward layer [(Houlsby et al., 2019)](#b129), just after the multi-head attention [(Bapna & Firat, 2019)](#b24), or just after the feed-forward layer [(Pfeiffer et al., 2020b)](#).

$σ: f ϕi (x) = W d (σ(W u x)) where W d ∈ R dx×k and W U ∈ R k×dx ,$Other variants for σ such as the identity function, standard multi-head attention, and multi-head attention with shared projection matrices have also been explored [(Stickland & Murray, 2019)](#b285). [Mahabadi et al. (2021a)](#) propose Compacter, a hyper-complex, low-rank adapter that reparameterises W in the adapter as:

$W = n i=1 A i ⊗ B i where A i ∈ R n×n is shared across layers (n is a hyper-parameter), B i ∈ R k n × d$n is parameterised as a low-rank matrix B i = s i t ⊤ i and ⊗ is the Kronecker product. Adapters can be routed sequentially or in parallel. Sequential adapters, are inserted between existing functions: [et al., 2017;](#)[Houlsby et al., 2019)](#b129). Parallel adapters are applied in parallel to a pretrained function: [(Figure 3b;](#)[Rebuffi et al., 2018;](#b251)[Stickland & Murray, 2019;](#b285)[He et al., 2022a)](#). Moreover, adapters involve two residual connections: between the output of f θi and the output of f ϕi , which is further added to x and normalised. Adapters have been shown to lead to increased sample efficiency, flatter minima, and more robustness to hyper-parameter choices compared to standard model fine-tuning [(Mahabadi et al., 2021b;](#)[He et al., 2021;](#b124)[Han et al., 2021)](#b120).

$f ′ i (x) = f ϕi (f θi (x)) (Rebuffi$$f ′ i (x) = x + f θi (x) + f ϕi (x)$Function Augmentation Adapters and more complex module designs can also be used to augment a base model with information and behaviour that it otherwise would not be able to access. This can be through adapter layers pre-trained on specific domains [(Wang et al., 2021a)](#) or other modalities [(Alayrac et al., 2022)](#). Modules can also be designed to attend over explicit key-value memory representations of entities and facts [(Verga et al., 2021)](#b309) and general domain knowledge [(Cheng et al., 2023)](#b51) to enable a model to perform certain types of operations such as arithmetic reasoning [(Trask et al., 2018;](#b300)[Andor et al., 2019)](#b6). More broadly, function composition enables the use of arbitrarily complex auxiliary modules. We highlight how function composition has been used to inject knowledge into models in §7.1.4. For an overview of how modules can be used to allow language models to use tools and to act, we direct the reader to [Mialon et al. (2023)](#).

## Rescaling

The output representations can also be directly transformed via element-wise multiplication with a vector of learned parameters:

$f ′ i (x) = f θi (x) ⊙ ϕ.$Crucially, this is equivalent to stacking the original function f θi with a linear transformation W = Iϕ. Such task-specific rescaling is typically applied to batch normalisation parameters in computer vision [(Bilen & Vedaldi, 2017)](#) and to layer normalisation parameters in NLP [(Houlsby et al., 2019)](#b129).

The adapter (IA) 3 (Figure [3c](#fig_5); [Liu et al., 2022b](#)) multiplies learned vectors with the keys and values in self-attention blocks and the intermediate activations in position-wise feedforward networks in the Transformer. Rescaling activations favours dimensions that are important for a given task. Multiplication with a binary mask is a special case of rescaling that incorporates sparsity: [Strezoski et al. (2019)](#b287) multiply a task-specific random binary mask b with a function's input x at every layer.

Overall, standard function composition methods such as adapter layers typically require more parameters as the new function depends on a model's input size and hidden size. While they do not require storing the gradients of the frozen parameters, they increase the number of operations at training and inference time. State-of-the-art function composition methods match or outperform standard fine-tuning.

## Hypernetworks

In the above-mentioned adapters, different modules ϕ 1 , . . . , ϕ |M | correspond to disjoint sets of parameters. However, the modules may benefit from sharing information. Rather than learning ϕ i directly, a (small) neural network W , known as a hypernetwork, can generate the module parameters instead, conditioned on an embedding α [(Ha et al., 2017;](#b115)[Platanios et al., 2018)](#b239). Thus, ϕ = W α. As a result, the modules are 'entangled', which violates the strong definition of modularity that postulates that modules are autonomous [(Goyal et al., 2021)](#b102). In fact, in hypernetworks, computation and routing are inseparably intertwined. In fact, foreshadowing our discussion in § 4.2.4, the embedding α can also be interpreted as unnormalised, learned routing scores for each task. In turn, the parameter generator weight would correspond to a set of modules stacked column-wise:

$W = [ϕ 1 , . . . , ϕ |M | ].$Hypernetworks can also be conditioned on inputs x (Figure [2d](#fig_3)). For instance, in conditional batch normalisation [(de Vries et al., 2017)](#b65), rescaling parameters are generated based on a representation of the model input obtained via an LSTM. Feature-wise linear modulation (FiLM; [Perez et al., 2018)](#b228) generates an element-wise affine transformation that is applied to image features, conditioned on the linguistic input of the model, for text-and-vision tasks. In self-modulation for Generative Adversarial Networks [(Chen et al., 2019)](#b48), the affine transformation is applied to hidden representations of the generator conditioned on the noise sample. [Bertinetto et al. (2016)](#b31) conditions the parameter generator on individual examples, in order to perform one-shot learning.

Hypernetworks have been used to generate a diverse set of module parameters, including classifier heads [(Ponti et al., 2021)](#), continuous prompts [(He et al., 2022c)](#), and adapter layers [(Üstün et al., 2020;](#b302)[Ansell et al., 2021;](#b10)[Mahabadi et al., 2021b)](#), most commonly conditioned on task [(Mahabadi et al., 2021b)](#) or language embeddings [(Üstün et al., 2020;](#b302)[Baziotis et al., 2022)](#b25). Such task or language embeddings α can themselves be learned directly from random initialisations or fixed as the typological features of a language [(Üstün et al., 2020;](#b302)[Ansell et al., 2021)](#b10). This is a strategy to integrate side (or metadata) information about the relationship among languages. Other examples of side information, such as the example label y, can be integrated into the hypernetwork input embedding via bi-linear interaction [(Chen et al., 2019)](#b48). Nevertheless, even the smallest possible module generator network is a linear projection W ∈ R d ϕ ×dα . To make the hypernetwork more parameter-efficient, it can be shared across layers by conditioning it on the module position in the neural architecture, in addition to the task index [(Mahabadi et al., 2021b)](#). In general, the hypernet can be conditioned on multiple (concatenated) embeddings: e.g., one corresponding to the task index and another to the language index. This allows the hypernetwork to generalise systematically to new task-language combinations at inference time. In particular, the hypernet can either generate a single module from all the embeddings [(Ponti et al., 2021)](#) or separate modules [(Ansell et al., 2021;](#b10)[Üstün et al., 2022)](#b304). In turn, the embedding combination chosen for any example is a form of hard routing (cf. § 4.2.2).

## Unifying Parameter, Input, and Function Composition

While the above methods may seem different, they all covertly share a similar functional form. [He et al. (2022a)](#) cast LoRA [(Hu et al., 2022)](#b132), prefix tuning [(Li & Liang, 2021)](#), and bottleneck adapters [(Houlsby et al., 2019)](#b129), representative methods of the three composition functions, into the same framework. We extend their framework to cover parameter composition, input composition, and function composition in general. Specifically, all modular computation functions can be reduced to function composition: the output of the function f θi of a model is added to a new term that depends on a learned function

$f ϕ : f ′ i (x) = f θ (x) + f ϕi (x).$For function composition methods, this form is the most natural. In the case of parallel adapters, for instance,

$f ′ i (x) = f θi (x) + f ϕi (x) where f θi (x) may be a multi-head attention module f θi (x) = MHA(C, x) = [head 1 , . . . , head h ]W o , with head j = Attn(xW j q , CW j k , CW j v ), and f ϕi (x) = W d (σ(W u x)$). In this setting, θ i and ϕ i are independent and must only agree regarding the dimensionality of their inputs and outputs.

For parameter composition methods, which modify the parameters directly, the dimensionality of the module parameters ϕ should match exactly the original parameters θ i . For instance, if we apply the module to a linear projection, then they should consist of weight matrices

$θ i = W i ∈ R dx×k and ϕ i = V i ∈ R dx×k ,$respectively. Because of linearity:

$f ′ i (x) = f θi⊕ϕ (x) = f W +V (x) = (W + V )x = W x + V x = f θi (x) + f ϕi (x)$For instance, in the case of LoRA [(Hu et al., 2022)](#b132), V = λB i A i . In the case of sparse adapters [(Ansell et al., 2022)](#b11), V is a sparse matrix.

For input composition methods, with the form

$f ′ i (x) = f θi ([ϕ i , x]$), the equivalence is derived as follows. Prefix tuning [(Li & Liang, 2021)](#) generalises other continuous prompt methods by concatenating prefix vectors ϕ i = P i k , P i v ∈ R l×d to the keys and values of self-attention. [He et al. (2022a)](#) show that prefix tuning can be expressed in the following way:

$f ′ i (x) = Attn(xW i q , [P i k , CW i k ], [P i v , CW i v ]) = (1 -λ(x))f θi (x) + λ(x) softmax(xW q P ⊤ k )P v$where λ(x) is a scalar that represents the sum of normalised attention weights on the prefixes and f θi (x) is the attention module in a Transformer. If we set, f ϕi (x) = softmax(xW q P ⊤ k )P v , then we obtain a function composition (1 -λ(x))f θi (x) + λ(x)f ϕi (x) that incorporates a weighted addition. For function and parameter composition, in contrast, the sum is unweighted.

Overall, despite their conceptual differences, most modular approaches are similar in their functional form and can be expressed as function composition. In practice, the way different methods are realised, however, leads to different trade-offs, which we illustrate in Table [3](#). Recent empirical studies [(Mahabadi et al., 2021a;](#)[He et al., 2022a;](#)[Liu et al., 2022b)](#) provide further evidence for the strengths and weaknesses of different methods. For instance, prompt tuning [(Vu et al., 2022a)](#) underperforms other methods due to limited capacity while intrinsic dimensionality [(Aghajanyan et al., 2021)](#b0) uses a very small number of parameters but leads to a large memory footprint and poor performance. Fine-tuning only biases [(Ben Zaken et al., 2022)](#b26) has a small memory footprint but achieves lower performance. Finally, function composition methods such as adapter layers [(Pfeiffer et al., 2021a) and](#)[compacter layers (Mahabadi et al., 2021a)](#), achieve the best performance, but add more parameters. (IA) 3 [(Liu et al., 2022b)](#) mitigates this by composing a lightweight linear diagonal weight. Modular deep learning architectures, however, have many other differences beyond their choice of computation function. In the following sections, we discuss the routing, aggregation, and training settings for the modules presented so far.

• Computation functions may consist of any neural module. Modules may modify the original parameters, be concatenated to the input, or composed with the original function.

• Parameter composition methods utilise sparsity or low-rank constraints. They are very parameter-efficient and efficient at inference time and show strong performance.

• Input composition methods concatenate a function's input with a parameter vector via prompting, continuous prompts, and retrieval augmentation. They are extremely parameterefficient but inefficient during training and inference and require large models.

• Function composition methods augment a model with arbitrary functions via parameter sharing, adapters, or rescaling. They require more parameters but often achieve the best performance.

• Rather than learning module parameters directly, hypernetworks can be used to generate module parameters, which enables sharing of information and conditioning on auxiliary information.

## Routing Function

In the previous section, we described how to compose a sub-function f i with shared weights θ with a single module function with weights ϕ. However, in a modular neural architecture, multiple modules are available from an inventory M = f ϕ1 , . . . , f ϕ |M | . A decision-making process is required to determine which modules are active, conditioned on the model input or auxiliary metadata. This process is implemented through a routing function r(•) that assigns a score α i to each module from the inventory M . These scores determine

Published in Transactions on Machine Learning Research (11/2023) which subset of modules is active, i.e. contributes to the computation. We provide an overview of different routing methods in Figure [4](#fig_6).

When metadata such as expert knowledge about sub-tasks (or skills) involved in a task is available, r(•) can be designed as a fixed function, that is, each routing decision can be made a priori (Figure [4a](#fig_6)). For instance, when using a language model to generate dialogue in Swahili, a task module for dialogue generation and a language module for Swahili can be selected. When no such prior information is available-for instance when modelling heterogeneous unlabelled data-routing of a given example needs to be learned (Figures [4b-4c](#fig_6)). In this case, the routing function can be conditioned on the current example x. [10](#foot_9)Unfortunately, learned routing is crucially under-constrained, as multiple possible ways of decomposing tasks into sub-tasks are reasonable [(Jacobs et al., 1991a)](#). In addition, it presents a series of unique challenges (see § 4.2.1). In an empirical study on synthetic data, [Mittal et al. (2022)](#) found that learned routing is sub-optimal compared to fixed routing, as it tends to under-utilise modules and to specialise them to a lesser degree. This behaviour is exacerbated as the number of tasks in the data grows. In real-world applications, [Muqeeth et al. (2022)](#b216) report similar results; however, Ponti et al. ( [2022](#)) find that learned routing may surpass expert module selection even in settings where tasks are procedurally constructed to require certain skills, such as instruction following in simulated environments.

Learning-to-route can roughly be split into hard routing and soft routing [(Rosenbaum et al., 2019)](#b257). Hard routing methods learn a binary selection of modules, similarly to the fixed routing scheme, where only a subset of modules is selected for each decision-making step (Figure [4b](#fig_6)). Inference for hard routing systems typically builds on score function estimators [(Williams, 1988;](#b329)[1992)](#b330) or stochastic re-parameterisation [(Jang et al., 2017)](#b140). On the other hand, soft routing methods learn a probability distribution over modules (Figure [4c](#fig_6); [Jacobs et al., 1991a)](#). While soft selection is more easily amenable to end-to-end learning via gradient descent, hard selection may lead to a sparse architectural design, owing to the fact that inactive modules are not part of the forward and backward computation graph. This reduces time complexity while augmenting the model capacity [(Bengio et al., 2013)](#b28).

While not the central focus of this paper, routing algorithms have recently garnered significant attention, due to their efficiency implications. There exists an intricate interplay between routing techniques and sparsity within modular models: When a modular architecture exhibits sparsity, signifying that only a select few modules are active during inference, a notable reduction in computational complexity during inference can be achieved [(Fedus et al., 2022)](#b75).[foot_10](#foot_10) Finally, [Shen et al. (2023a)](#); [Jang et al. (2023)](#b141) showcase how sparse models, when combined with instruction tuning techniques, can yield substantial gains in performance and efficiency over dense models, potentially reshaping the landscape of large language model design.

## Fixed Routing

Making the routing decision a priori-i.e. when we utilise metadata (e.g. task identity t) to make the discrete routing decisions before training-is referred to as fixed routing (Figure [4a](#fig_6)). Here the routing function r(•) simplifies to a selection of a subset of modules K ⊆ M for the examples with certain metadata:

$r(ϕ i ) = 1 if i ∈ K 0 otherwise (3)$This function defines a binary matrix A ∈ {0, 1} |T |×|M | , where the number of rows corresponds to possible tasks and the number of columns corresponds to the size of the module inventory.

One simple example of fixed routing in multi-task learning is when all parameters, except the final classification layer, are shared among all tasks [(Ruder, 2017)](#b260). Independently from the task identity, the examples are passed through the same network until after the penultimate layer. The penultimate layer's representations are then routed to their respective final classification layer according to the task identity. This boils down to setting |K| = 1, with the additional constraint that tasks cannot share modules, which results in the allocation matrix being an identity matrix, A = I.

While not immediately apparent, methods that adapt pre-trained models towards individual tasks [(Rebuffi et al., 2017;](#b250)[2018;](#b310)[Houlsby et al., 2019;](#b129)[Bapna & Firat, 2019;](#b24)[Li & Liang, 2021;](#)[Liu et al., 2022b;](#)[Hu et al., 2022;](#b132)[Ansell et al., 2022;](#b11)[Ben Zaken et al., 2022](#b26), inter alia)-as discussed in § 3-deterministically route representations through the newly introduced module f ϕ . Given that the pre-trained weights are frozen and modules trained on different tasks can be added or removed, the components become modular even if they are developed asynchronously and independently of each other [(Pfeiffer et al., 2021a)](#). In a sense, community-based hubs of pre-trained adapters such as AdapterHub [(Pfeiffer et al., 2020a](#)) can be considered as ever-evolving multi-task models, the development of whose components has been distributed throughout the community.[foot_11](#foot_11) Moreover, since newly introduced weights are encapsulated between frozen (shared) weights, adapted representations of intermediate layers are implicitly aligned as they are passed as input to the same frozen components. [Hampshire & Waibel (1992)](#b117) were possibly among the first to train independent experts for a series of sub-tasks known a priori. In this case, the (fixed-size) subset of experts K associated with each task t is assumed as given, resulting in the rows of A being k-way vectors. In cross-lingual transfer, any problem can be decomposed into a task and language variety. Fixed routing can select separate language and task components, and facilitate generalisation to new, unobserved combinations of tasks and languages at inference time [(Pfeiffer et al., 2020b;](#)[Ponti et al., 2021;](#)[Üstün et al., 2022)](#b304). In this case, |K| = 2. Similarly, in reinforcement learning, [Heess et al. (2016)](#b127) and [Devin et al. (2017)](#b66) design a modular policy that is composed of a robot-specific module and a task-specific module, which are instantiated as separate neural networks.

Composing these modules enables generalisation to unseen robot-task combinations.

Beyond task identity, routing can be performed based on other metadata such as language, domain, or modality information. [Pfeiffer et al. (2022b)](#) add adapters for each language to a multilingual language model during pre-training on unlabelled text. [Fan et al. (2021)](#b80) route deterministically for multilingual machine translation according to the language family: as a consequence, all languages in a family share the same expert. In a similar vein, [Gururangan et al. (2022)](#b113) add domain-specific adapters to language models, deterministically routing based on the text source domain. This concept was further extended by [Li et al. (2022b)](#), who proposed the branch-train-merge method: copies of the same model are trained on different domains and then averaged. Finally, modality can also inform fixed routing, such as in vision-and-language models [(Pfeiffer et al., 2022a)](#). This allows for adapting the encoders of different modality streams.

## Learned Routing

When the routing function r(•) is not known in advance, it can be implemented as a learnable neural network with parameters ρ. In input, it receives the example representation x or metadata such as the task t. In output, it returns routing scores α. Usually, r ρ is a linear projection or a Multi-Layer Perceptron. While the former represents a less expressive family of functions, the latter may collapse into ignoring the input features. Note that learning the routing function also implies that the specialisation of each module is unknown. Thus, modules are not trained on different sets of examples; rather, they are all trained jointly with the routing function.

## Challenges of Learned Routing

Learned routing introduces a number of challenges, including training instability, module collapse [(Kirsch et al., 2018)](#b155), and overfitting. These were first systematically described by [Rosenbaum et al. (2019)](#b257), and we follow a similar taxonomy. In general, they identify two root causes for all these challenges: first, the need to balance between exploration and exploitation [(Sutton, 1986)](#b294). More specifically, routing must find the optimal trade-off between allocating information to the most suitable modules versus under-explored modules. Second, routing must share modules across examples or tasks in such a way as to reap the benefits of positive transfer while avoiding negative interference. We elaborate on the individual challenges below.

Training Instability emerges especially in the early phases of training; at this point, modules are randomly initialised and have no clear functional specialisation. Thus, the router cannot make any principled decision in selecting modules. On the other hand, modules do not start specialising until they are consistently routed to different subsets of tasks or examples.

Curriculum learning can mitigate this challenge to some extent [(Chang et al., 2019)](#b45), as simpler tasks require simpler sets of skills. However, this assumes that information about task complexity is available and that the data can be ordered accordingly. As an alternative, the router parameters can be trained with a different learning rate than the module parameters, either lower [(Rosenbaum et al., 2018)](#b256) or higher [(Ponti et al., 2022)](#). These create two different dynamics: either the necessary skills for a task are determined after specialisation, or the relationship among tasks is figured out first and modules are updated accordingly.

Module Collapse describes scenarios where only a small number of modules (in the extreme case, one) from the available inventory are selected. This leaves the remaining modules untrained and negatively impacts their overall diversity. Often, this results from excessively favouring exploitation over exploration, which leads to sub-optimal results. To amend this, [Ahn et al. (2019)](#b1) use ϵ-greedy routing for initial exploration of all modules and afterwards switch to learned routing. Other strategies to avoid module collapse include auxiliary losses for load balancing [(Shazeer et al., 2017;](#b277)[Fedus et al., 2021)](#b82) and intrinsic rewards that encourage diversity in module selection [(Cases et al., 2019)](#b43). The choice of information that conditions the router also plays an important role: metadata, e.g. text genre [(Cases et al., 2019)](#b43) or task identity [(Kudugunta et al., 2021)](#b158), make routing more robust than individual examples. The diversity of training tasks also facilitates diversity in routing selections [(Chang et al., 2019;](#b45)[Caccia et al., 2022)](#b40). [Dua et al. (2022)](#b76) warms up the sampling temperature over training, in order to over-sample domains with fewer examples in unbalanced distributions.

Overfitting to noise is a risk faced by deep modular networks due to their ability to model subsets of examples independently [(Rosenbaum et al., 2019)](#b257). For instance, routing at the token level was shown to lead to performance drops in out-of-domain generalisation for MoEs [(Artetxe et al., 2022)](#b13). For a similar reason, gains in pre-training do not always translate into gains in fine-tuning for MoEs [(Fedus et al., 2021)](#b82). Increased robustness can be achieved by routing conditioned on metadata if available [(Chang et al., 2019;](#b45)[Cases et al., 2019;](#b43)[Kudugunta et al., 2021)](#b158). In addition, strategies that favour the combinatorial behaviour of modules yield superior generalisation [(Chang et al., 2019;](#b45)[Ponti et al., 2022)](#).

## Hard Learned Routing

A model may learn how to select modules through hard routing. This implies that the choice of whether a module is active or excluded from the computation graph is binary. Discrete decisions are not amenable to be learned through vanilla gradient descent: since small perturbations of parameters do not affect the selection of modules, the gradient of the loss with respect to the routing parameters is zero. Thus, various methods, including reinforcement learning, evolutionary algorithms, and stochastic re-parameterisation, have been proposed for inference. These are discussed separately below.

On the other hand, hard routing is more efficient than soft routing in terms of time and space complexity. In addition, binary selection implies that parameter updates are localised to a subset of modules. This reflects the intuition that the shifts in distribution of the variables in an environment are similarly local [(Parascandolo et al., 2018;](#b226)[Goyal et al., 2021)](#b102). Since the inactive module parameters are not affected, they remain invariant with respect to the distribution shift. On top of this, this type of routing may result in variable-size sets of active modules. This allocates model capacity according to task complexity, which follows the principle of conditional computation [(Bengio et al., 2015)](#b27). In fact, it is fair to assume that the skills required for complex tasks are a superset of those of simpler tasks. For instance, dialogue modelling requires (among others) intent detection, slot filling, and conditional response generation.

Reinforcement Learning In Routing Networks [(Rosenbaum et al., 2018)](#b256), Modular Networks [(Kirsch et al., 2018)](#b155), and the Compositional Recursive Learner (CRL; Chang et al., 2019), a router network is trained through reinforcement learning. Specifically, Routing Networks rely on multi-agent RL (MARL), Modular Networks rely on the score function estimator (REINFORCE), whereas the CRL relies on Proximal Policy Optimisation (PPO). Commonly, this family of methods alternate between a score function estimator for the routing parameters ρ and SGD for module parameters {ϕ 1 , . . . , ϕ |M | }. For a vanilla score function estimator, where routing is conditioned on the input example and m ∈ M , the update takes the form:

$∇ ρ E x,y p(y | x, θ, ϕ 1 , . . . , ϕ |M | , ρ) ≈ 1 n n i=0 [ p(y i | x i , θ, ϕ m ) ∇ ρ log p(m | x i )](4)$Under this lens, routing becomes a policy π(m | x). If applied layer-wise, each hidden representation at a given layer 1 ≥ t ≤ l constitutes a state h t ∈ H. The routing policy determines the action, i.e. the selection of a module index m. In particular, this assumes that the inventory M is shared across layers. [13](#foot_12) In turn, applying the transformation of the corresponding module on the input is equivalent to a transition function π : H → H, which returns the next layer's hidden state h t+1 . The loss function at the top layer corresponds to a (delayed) negative reward, i.e. L(•) = -R. [14](#foot_13) Crucially, in this setting the transition functions are non-stationary, as the module parameters are amenable to change. Because modules are applied sequentially based on the policy, the number of steps of computation in the model can vary when a special halting action is available.

Evolutionary Algorithms Alternatively, routing can be learned via a genetic algorithm. In PathNet [(Fernando et al., 2017)](#b84), the loss function indicates the fitness of a configuration of active modules K ⊆ M . For each task, two configurations are selected at random and trained until a stopping criterion is met. The one incurring the lower loss on a validation set overwrites the other. This copy, in turn, receives a random mutation, and then the procedure is repeated. In µNet [(Gesmundo & Dean, 2022a;](#)[b)](#), mutations involve cloning, insertion, and removal of layers. The fitness criteria include not only performance but also parameter efficiency. This approach has been extended to a multi-task setting where multiple agents update different modules asynchronously [(Gesmundo, 2022)](#). However, as is common for evolutionary algorithms, this search is brute-force and thus highly inefficient.

## Stochastic Re-parametrisation

Hard routing can also be performed via a continuous relaxation of the discrete latent variable α determining the module allocation. Several stochastic re-parameterisations such as Gumbel-Softmax [(Jang et al., 2017)](#b140) or the Concrete distribution [(Maddison et al., 2017)](#b195) have been proposed for this purpose. Compared to the score function estimator, stochastic re-parameterisations are biased but have lower variance. Moreover, they are differentiable, which makes a hard router trainable in an end-to-end fashion. For instance, AdaShare [(Sun et al., 2020b)](#) uses Gumbel-Sigmoid to learn a binary vector for each task that indicates whether a model layer should be included in the forward pass or skipped entirely. This may be interpreted as choosing between a parameterised module and an identity function at each layer.

Stochastic re-parameterisation also allows for selecting module subsets of varying sizes for each layer. In Neural Interpreters [(Rahaman et al., 2021)](#b247), this is based on a threshold. Each module is associated with a 'signature vector'. The dot product between this vector and the output of an unnormalised routing function ('type inference') conditioned on a token determines a score. If this surpasses a certain threshold, then the module is allowed to access the given token. As an alternative, variable-size module routing can be achieved by learning a soft clustering (a.k.a. soft partition) of modules [(Ponti et al., 2022;](#)[Caccia et al., 2022)](#b40). Thus, each entry α ij , which represents the routing of the j-th module to the i-th task, is constructed as follows:

$α i,j = sigmoid log sigmoid( αi,j ) u (1 -sigmoid(α i,j )) (1 -u) 1/τ u ∼ Uniform(0, 1). (5$$)$where αij represents the unnormalised routing score. This latent variable also admits priors such as the Indian Buffet Process (Griffiths & Ghahramani, 2011) to encourage both diversification and sharing of module subsets across tasks [(Ponti et al., 2022)](#). [Caccia et al. (2022)](#b40) extend this framework to multi-head routing, where different modules can be allocated to contiguous subsets of dimensions of the layer's input and output. While this just requires as many copies of α as the number of subsets of dimension, it provides higher expressivity to the routing function.

Top-k Selection Finally, hard selection can rely on top-k selection from (possibly unnormalised) scores α over modules. In the case of Independent Causal Mechanisms [(Parascandolo et al., 2018)](#b226), α is given by a discriminator that scores the outputs of a generator, and k = 1. In the case of Recurrent Independent Mechanisms [(Goyal et al., 2021)](#b102), the scores are derived from attention between modules and the input, and k > 1. These methods are grounded on the assumption that the competition among modules to be activated facilitates their specialisation (see § 8.3 for more details).

## Soft Learned Routing

Mixture of Experts To sidestep discrete selections of modules, several works propose soft routing methods, where all modules are selected and aggregated according to a weighted combination, i.e. a mixture of experts (MoE; [Jacobs et al., 1991b;](#)[Jordan & Jacobs, 1994)](#b145). [15](#foot_14) Here, the router learns a probability distribution over the available modules, i.e. p(M ) = r ρ (•). Hence, routing and aggregation take place as:

$f ′ i (x) = ϕj ∈M r(ϕ j ) f (x; θ i , ϕ j )(6)$In contrast to the discrete selection of hard routing methods, this setup is easily trained end-to-end via gradient descent. A number of works [(Eigen et al., 2014;](#b78)[Meyerson & Miikkulainen, 2018;](#b205)[Wortsman et al., 2020](#b333), inter alia) train a continuous weighting (i.e. a mixture) of all modules; however, this limits the degree of modularity as parameter updates are not local; instead, they always affect all modules. Additionally, activating all modules for each example significantly increases the computational cost for each forward and backward pass through the network. To circumvent this, [Shazeer et al. (2017)](#b277) and [Lepikhin et al. (2021)](#b168) only route to the top-k of |M | modules, where 1 < k < |M |. The output representations of the k active modules are averaged according to the respective routing weights, whose sum is re-normalised to 1. Thus, top-k MoEs stand between hard routing, as only a subset of modules is active, and soft routing, as their average is weighted by the routing scores. In practice, a layer performs the following computation:

$f ′ i (x) = ϕj ∈ top k [r(ϕ)] r(ϕ j ) k 1 r(ϕ) f (x; θ i , ϕ j )(7)$Fedus et al. ( [2021](#)) and [Clark et al. (2022)](#b57) demonstrate that even top-1 routing can achieve competitive results for language modelling.

Token-Level Routing MoEs have recently undergone a revival as part of the efforts to scale Transformers.

In particular, MoE Transformers route to a subset of Feed-Forward Network (FFN) modules per layer instead of a single FFN. The focus of these works is on computationally efficient training of very large models. This is achieved by splitting the input tokens across different (hardware) accelerators. The MoE routing algorithm is therefore required to (ideally) uniformly distribute the tokens of all the examples in an input batch across all accelerators, i.e. to load balance computation across "experts". The dominating routing strategy is for each token to choose the top-k experts [(Shazeer et al., 2017;](#b277)[Lepikhin et al., 2021;](#b168)[Fedus et al., 2021;](#b82)[Clark et al., 2022;](#b57)[Yang et al., 2021;](#b337)[Dua et al., 2022;](#b76)[Hazimeh et al., 2021;](#b121)[Rajbhandari et al., 2022;](#b248)[Riquelme et al., 2021;](#b254)[Du et al., 2022;](#b75)[Zoph et al., 2022)](#b75). Alternative approaches let each expert choose the top-k tokens [(You et al., 2022;](#b342)[Zhou et al., 2022b)](#) or globally determine the best routing path [(Lewis et al., 2021)](#b170). [16](#foot_15)However, since routing is conditioned on the token level, and the load balancing restriction limits the system from routing an entire example to a single module, the system potentially has to relearn similar concepts in multiple modules. Hence, load balancing hinders the router from selecting the single best module for longer (e.g., repetitive) sequences. This is investigated further by [Lewis et al. (2021)](#b170), who find that sparse models route syntactically and semantically similar words (in contrast to sentences or phrases) to the same modules. This sheds light on the limited expressiveness of modules which are learned on the token-level. Since scaling is the main focus of these works, their goals are orthogonal to modular approaches centred on parameter efficiency, transfer-interference trade-offs, and combinatorial generalisation. Since task identity (or other metadata) is not always given, especially in continual learning, it can be inferred through an auxiliary model. [Van de Ven & Tolias (2019)](#b305) refer to this scenario as 'class-incremental learning'.

## Example-Level Routing

For instance, the current task can be identified based on the lowest predictive uncertainty or an auxiliary task classifier [(von Oswald et al., 2020)](#b313). In these cases, routing can depend on the predicted task identity.

## Mitigating Module Collapse

To address the challenge of module collapse, which was previously discussed in §4.2.1, several strategies have been introduced to enhance the effectiveness of models in utilizing the available experts' capacity. One such approach, presented by [Shen et al. (2023b)](#), introduces a novel loss function centered on Mutual Information. This loss aims to maximize the mutual information between the input and the target module, effectively mitigating module collapse issues. Another innovative solution, put forward by [Chi et al. (2022)](#b52), involves the modification of the routing algorithm. This modification incorporates techniques like dimension reduction, L2 normalization, and adjustment of gating temperature, all designed to address the challenges associated with module collapse.

Puigcerver et al. (2023) employ a fully differentiable soft assignment mechanism by applying weighted combinations of representations to each module, allowing for enhanced model capacity without significantly increasing inference costs. Muqeeth et al. (2023) tackle module collapse by employing a weighted average-based merging approach of the module's parameters. 

## Hypernetworks

In addition to hard and soft routing, hypernetworks [(Ha et al., 2017)](#b115), as introduced in § 3.4, can be considered a third kind of routing, with unnormalised routing scores. More formally, the parameters θ t ∈ R d for a task t can be generated by a linear function Φα t . The task embedding α t ∈ R |M | can be interpreted as the output of a task-level routing function with unnormalised scores over |M | modules. In turn, the generator Φ ∈ R d×|M | can be considered a matrix of module parameters stacked column-wise, where each module has d parameters. Thus, the generated parameters θ t is a linear combination of the columns of the linear generator. This is also reminiscent of tensor factorisation models where parameters are factorised into shared tensors and task-specific tensors [(Yang & Hospedales, 2017)](#b339), which in hypernetworks correspond to the generator and the task embedding, respectively. However, hypernetworks learn both sets of parameters jointly rather than obtaining them from a factorisation of task-specific networks a posteriori.

## Level of Routing

Another aspect of designing a routing function is its level of granularity. Routing can select modules globally for the entire network, make different allocation decisions per layer, or even hierarchically select sub-routers. This last method is also referred to as 'dispatched routing' by [Rosenbaum et al. (2018)](#b256). A naive version of global routing (Figure [5b](#fig_8)) assumes that a single routing configuration is shared across layers. Allowing for different decisions per layer (Figure [5a](#fig_8)) is more challenging as the space of potential architectures grows exponentially as |M | l , where l is the number of layers or sub-functions of the network. In fact, to compute the posterior over parameters, one would need to marginalise over every possible configuration of A = [α 1 , . . . , α l ]. [Kirsch et al. (2018)](#b155) resort to Expectation Maximisation to make it tractable. Instead, per-layer routing (Figure [5c](#fig_8)) assumes conditional independence among decisions, thus facilitating scaling. Crucially, routing scores are sometimes employed not only to select a subset of modules but also to aggregate their outputs. This second purpose is addressed in more depth in § 5.

Most methods assume that routing decisions occur in a sequence, whose length is bounded or unbounded. This is the case where the output of every layer is fed into the next. However, routing may also involve defining both the selection of modules and their order of composition (i.e., the model architecture). For instance, in Neural Module Networks (NMNs; [Andreas et al., 2016b;](#)[2017)](#b260), the routing function consists of a parser that takes in a query and produces a dependency tree. This is post-processed and transformed into a tree graph where nodes are modules and directed edges control the flow of the information, i.e. route the output(s) of a subset of modules as input to another module. In Modular Meta Learning, [Alet et al. (2018)](#b4) alternate between sampling compositional graphs using simulated annealing [(Kirkpatrick et al., 1983)](#b154) and performing a step of gradient descent on the network parameters for a set of meta-training tasks.

• The Routing Function is a critical component in modular neural networks, responsible for determining how information flows through modules.

• Routing can be categorized into Fixed Routing, Learned Routing, and Hypernetworks.

-Fixed Routing uses predetermined rules to direct information flow.

-Learned Routing employs neural networks to dynamically allocate modules based on input data or task information. * Challenges in Learned Routing include Training Instability, Module Collapse, and Overfitting, which require specialized strategies for mitigation. * Hard Learned Routing involves discrete module selection, often requiring reinforcement learning or stochastic re-parameterization for training. * Soft Learned Routing often use weighted combinations of modules, where predominantly top-k routing strategies are employed for computational efficiency.

-Hypernetworks offer a flexible approach by generating task-specific parameters with unnormalized routing scores.

• Routing decisions can occur at different levels of granularity, including global, per-layer, and hierarchical routing.

## Aggregation Function

While in the previous section on routing we have covered the topic of how to select different modules during training, we will now focus on how we can aggregate these functions in order to combine the respective information. It is important to emphasise that, for the majority of current approaches, routing and aggregation are inseparable; that is, the selection and aggregation of modules are performed simultaneously.[foot_16](#foot_16) On the other hand, the strategies for aggregating functions in this section are reminiscent of the taxonomy previously discussed for computation functions (see §3); while in the latter we looked into the composition of shared components with modules, in this section we provide insights into the composition of multiple modules. This is often required when modules are recombined for zero-shot transfer or task-level generalisation (for more details on these applications, see § 7).

In particular, for a subset of active modules K ⊆ M i the aggregation of modular components can (similarly) be realised on the parameter level f

$′ i (x) = f ϕ1⊕•••⊕ϕ |K| (x), input level f ′ i (x) = f θi ([ϕ 1 , . . . , ϕ |K| , x]), as well as function level f ′ i (x) = f ϕ1 • ... • f ϕ |K| (x).$In addition, we cover output or representation level aggregation

$f ′ i (x) = f θi (x) ⊕ f ϕ1 (x) ⊕ • • • ⊕ f ϕ |K| (x).$Crucially, this differs from parameter aggregation if f is non-linear.

We discuss these different strategies in the following sections.

## Parameter Aggregation

Mode Connectivity A natural strategy to aggregate information from multiple modules is interpolating their weights. However, given that neural architectures differ, and that hidden representations might not necessarily be equivalent (e.g. under invariance to invertible linear transformations) even if the model architectures are the same [(Kornblith et al., 2019)](#b157), naively aggregating module weights may have catastrophic consequences. However, recent work on linear mode connectivity [(Frankle et al., 2020)](#b89) suggests that under certain conditions, it is in fact possible to interpolate between multiple models, which has positive ramifications for modular aggregation methods. To understand these conditions, we first provide a brief introduction to the constraints under which parameter aggregation is permissible.

The phenomenon where the minima found by two networks are connected by a path of non-increasing error, has been the subject of research for many years [(Freeman & Bruna, 2017;](#b90)[Draxler et al., 2018;](#b74)[Garipov et al., 2018;](#b96)[Nagarajan & Kolter, 2019)](#b218). However, most works demonstrate that mode paths are in fact not linear. While Nagarajan & Kolter (2019) find linear paths between networks, their experimental setup requires initialising models with the same set of weights. [Frankle et al. (2020)](#b89) and [Neyshabur et al. (2020)](#b222) demonstrate that this linear mode connectivity phenomenon is closely linked to the Lottery Ticket Hypothesis [(Frankle & Carbin, 2019)](#b88), which suggests that only a small subset of randomly initialised weights are the main drivers for the final performance of a model-the so-called winning tickets (see § 3.1). When interpolating between models trained on different tasks but initialised with the same set of weights, the models tend to stay in the same loss basin, indicated by the lack of a sudden increase in loss when interpolating the weights.

Consequently, it appears that the flatness of the basin of the loss landscape translates to better generalisation capabilities of a model. [Gueta et al. (2023)](#) find that fine-tuned models reside in distinct regions in weight space, and models within the same region exhibit high performance. On the other hand, Ainsworth et al.

(2022) argue that the success of such interpolation is strongly connected to the inherent bias of the optimiser being used, and not the neural network architecture itself.

Weight Interpolation Building on the findings of interpolating the weights of models, [Ansell et al. (2022)](#b11) propose Lottery Ticket Sparse Fine-Tuning (LT-SFT), described in § 3.1. In particular, they identify language, and task-specific sub-networks ϕ l and ϕ t . These can be aggregated by simply adding them to the base model parameters, i.e. θ ′ = θ 0 + ϕ l + ϕ t . Instead of identifying task adaptations on subsets of model parameters, [Ilharco et al. (2022)](#b334) propose to edit entire models with further arithmetic operations. For example, for toxic language generation and language modelling tasks, by performing the arithmetic negation operation θ ′ = θ 0 + (ϕ general -ϕ toxic ), their new model f θ ′ (x) generates less toxic text. This idea was influenced by the word analogy task (i.e., 'word arithmetics') [(Mikolov et al., 2013)](#b209).[foot_17](#foot_17)

Rather than interpolating sparse adapters, [Asai et al. (2022)](#b14) aggregate parameters of soft prompts learned via prefix tuning ( § 3.2). In order to generalise to new tasks, (frozen) modules from past tasks and a learnable module created for the new task are interpolated according to the weights of an attention mechanism between the modules and the input.

## Model Merging

Mode connectivity has enabled the fusion of entire models without extensive retraining, yielding performance improvements across a range of applications [(Choshen et al., 2022;](#b54)[Gupta et al., 2020;](#b112)[Yadav et al., 2023;](#b336)[Jin et al., 2023)](#b144). These developments have made frameworks, such as Git-Theta [(Kandpal et al., 2023)](#b146), which facilitate collaborative model development through version control, reasonable. Soft Merging of Experts with Adaptive Routing (SMEAR) [(Muqeeth et al., 2023)](#b217) introduces gradient-based training for sparsely activated models, offering specialization benefits.

## Representation Aggregation

Closely related to parameter aggregation, representation aggregation consists of interpolating the outputs of individual modules. Crucially, both operations are equivalent if the functions are linear:

$(α i Φ i + α j Φ j )x = α i Φ i x + α j Φ j x.$However, this does not hold true for non-linear functions, e.g. if the module is an adapter layer [(Houlsby et al., 2019)](#b129) or a feed-forward component of a Transformer layer [(Fedus et al., 2021)](#b82).

## Weighted Representation Averaging

At the i-th sub-function of the model, where multiple modules ϕ ∈ M i exist, the representations are passed through the (active) modules, outputting |K i | (latent) representations h 1 , . . . , h |Ki| . One way of performing aggregation is to learn the weights α to interpolate over the hidden representations:

$f ′ i (x) = |Ki| j α j h j (8)$with α j being a module-specific scalar weighting.

This aggregation is equivalent to Equation ( [6](#formula_37)) when interpreting each weight α j ∈ [0, 1] as the output of a soft router, i.e. α j = r(ϕ j ). Consequently, all soft-learned routing approaches (e.g. MoE) that do not perform top-1 routing (see § 4.2.3) also determine how to aggregate the representations of different modules.

As an extension to the traditional MoE aggregation/routing function, [Ma et al. (2018)](#b194) propose to learn one aggregation function per task t in a multi-task setup. [Gururangan et al. (2022)](#b113)

$f ′ i (x) = d∈D p(d | x) f ϕ d (x) (9)$This posterior is inferred through the Bayes rule. This does not require any auxiliary model, and only relies on the original d-conditioned language model. In fixed routing, module representations are often averaged without weighting [(Zhang et al., 2022a;](#)[Chronopoulou et al., 2022a)](#). Similarly, in hard routing methods, the representations of all active modules are averaged, such as in Polytropon [(Ponti et al., 2022)](#), or summed, as in PathNet [(Fernando et al., 2017)](#b84). [19](#foot_18)One disadvantage of simply learning gating parameters is that the weights do not depend on the hidden representations. Thus, they do not take into account their information content. This issue is tackled by attention-based aggregation functions.

## Attention-Based Representation Aggregation

Instead of inferring the weighting before a module has performed its transformation on the latent representation, the aggregation decision can take place afterwards. This allows for identifying whether or not the information added by the respective module is ancillary to the target task. In AdapterFusion, [Pfeiffer et al. (2021a)](#) propose an attention mechanism [(Bahdanau et al., 2015)](#b19) between the stacked hidden representations H i produced by the modules and their input x:

$f i (x) = Attn(xQ i , H i K i , H i V i )(10)$where Q, K, V ∈ R d×h are the projections that produce the queries, keys, and values, and x is the input representation to each of the modules (i.e., the output representation of the previous layer). H i ∈ R |M |×d is a matrix consisting of row-wise stacking of the output representations h 1 , . . . , h |Mi| of each module. In other words, the input of each module is interpreted as the query and the output of each module is interpreted as the value and key. The attention mechanism thus learns to attend over the module representations and weigh them according to their relevance for the current task.

Instead of aggregating module outputs into a single representation, Recurrent Independent Mechanisms [(Goyal et al., 2021)](#b102) concatenate the outputs of the top-k active modules. However, in between the application of recurrent computation functions, they exploit an attention mechanism over hidden representations to enable sparse communication among modules.

One major disadvantage of both weighted and attention-based representation averaging, is that-when used in combination with soft routing-they require a full forward pass through all modules, even if they contribute only marginally to the final aggregation. Thus, they incur significant increases in time and space complexity. While this can be mitigated by pruning (i.e., dropping) some modules during inference [(Rücklé et al., 2021)](#b258), latency still remains an issue for scalability. Thus, top-k hard routing offers a more efficient solution for both weighted averaging [(Shazeer et al., 2017;](#b277)[Lepikhin et al., 2021)](#b168) and attention-based aggregation [(Goyal et al., 2021)](#b102).

## Input Aggregation

Input aggregation lends itself naturally to adapters such as prompts or prefix tuning (Brown et al

., 2020; Lester et al., 2021; Li & Liang, 2021, see § 3.2). In prompting, we have a set of instructions or few-shot examples ϕ 1 , . . . , ϕ |K| . Given that the nature of prompting is to prepend the prompts to the input, aggregating the respective modules boils down to concatenating all prompts. That is, providing the model with multiple instructions, or with multiple examples (i.e. few-shot in-context learning) is a version of module input aggregation f ′ 1

$(x) = f θ1 ([ϕ 1 , . . . , ϕ |K| , x]$). This concept also extends to prefix-tuning, where we can simply concatenate all prefixes at every layer:

$f ′ i (x) = f θi ([ϕ 1 i , . . . , ϕ |K| i , x]$). In the context of prompting, [Schick et al. (2021)](#) leverage input aggregation by concatenating multiple textual descriptions of undesired behaviours of a language model to generate toxic text for model debiasing. In the context of prompt tuning, [Vu et al. (2022b)](#) learn separate task and language soft prompts that are recombined for zero-shot cross-lingual transfer in summarization. Nayak et al. ( [2022](#)) compose soft prompts of attributes and objects in visual tasks to generalise to new classes. Soft prompts can also be aggregated with methods different from concatenation such as attention-based parameter interpolation [(Asai et al., 2022)](#b14). Furthermore, input aggregation methods have found significant utility in retrieval augmented generation [(Lewis et al., 2020a)](#), a technique where retrieval models are employed to retrieve external knowledge for addressing knowledge-intensive NLP tasks. [20](#foot_19) In RAG methods, retrieved documents are appended to the input, essentially aggregating external information with the model's input. These methods facilitate knowledge injection and editing [(Verga et al., 2021;](#b309)[Cheng et al., 2023)](#b51), allowing models to access and incorporate information from external sources, which can be crucial for tasks demanding domain-specific knowledge or real-time data updates. This aligns with the broader theme of knowledge enhancement within modular neural architectures, extending their capabilities to handle complex and dynamic information needs.[foot_20](#foot_20) Hypernetworks Similarly to soft prompts, hypernetworks may aggregate information from different embeddings by combining them in the input to the parameter generator. For instance, in [(Ponti et al., 2021)](#) task and language embeddings are concatenated in the input when training a multilingual multi-task architecture where the encoder is fully shared and the hypernetwork generates the classifier head. By recombining embeddings appropriately, this method allows for inferring the parameters of unseen tasklanguage combinations. Combinations of embeddings have been used to generate adapters in multilingual [(Üstün et al., 2020)](#) and multi-task settings [(Mahabadi et al., 2021b;](#)[Pilault et al., 2021)](#b238).

## Function Aggregation

Finally, aggregation can be achieved on the function level;

$f ′ i (x) = f ϕ1 • f ϕ2 (x)$. Different aggregation methods infer either a sequence or a (tree) structure that determines the order of the aggregation.

Sequential Aggregation By performing a forward pass through multiple modules, where the input to the next module is the output of the previous one, the respective hidden representations are sequentially transformed:

$f ′ i (x) = f ϕ1 (f ϕ2 (. . . (f ϕ |M | (x)))$). This form of information aggregation is often chosen in conjunction with fixed routing, as discussed in § 4.1, given that the routing order is determined by the role of each module (e.g. language and task adapters). [Pfeiffer et al. (2020b;](#)[2021b)](#) propose a two-stage setup where language-specific components are disentangled from task-specific components, in order to perform zero-shot cross-lingual transfer. First, language (adapter) modules f ϕ ls and f ϕ l t are trained on monolingual unlabelled data for the source language s and the target language t, respectively. Then, in the second stage, the language component f ϕ ls is inserted but frozen, and a new (adapter) module is added for a task f ϕt and trained on annotated data for the source language: f ϕt (f ϕ ls (x)). Since this effectively disentangles language from task information, this also enables zero-shot inference on the target language t without annotated data. In particular, f ϕ ls is substituted with f ϕ l t , thereby hierarchically aggregating the information from the respective modular components: f ϕt (f ϕ l t (x)). [Similarly, Stickland et al. (2021)](#) perform function composition of a language module f ϕ l and a domain module f ϕ d for multilingual multi-domain machine translation. For more examples, see § 7.1.

## Hierarchical Aggregation

Alternatively, when global routers jointly determine the selection of modules and the model architecture, the order of function composition follows the structure of a tree. For instance, Neural Module Networks [(Andreas et al., 2016b](#)) leverage a semantic parse to infer a graphical structure for module aggregation. While all leaf nodes find objects by identifying regions of an image through attention, intermediate nodes either transform or combine these representations (depending on the arity of the node). The root then predicts the label by describing or measuring the attended objects.

• Aggregation functions play a crucial role in combining information from multiple modules in modular neural architectures.

• Parameter aggregation strategies interpolate weights of multiple modules and are influenced by concepts like linear mode connectivity.

• Weighted representation averaging and attention-based aggregation are methods for combining the outputs of modules, with attention mechanisms allowing for dynamic weighting based on relevance.

• Input aggregation methods, such as prompting and prefix tuning, involve concatenating instructions or examples to the input, enabling modular control over tasks and domains.

• Hypernetworks can also perform input aggregation by combining different embeddings.

• Function aggregation occurs on the function level and can be sequential or hierarchical, with different methods determining the order of aggregation.

• Sequential aggregation is often used with fixed routing, while hierarchical aggregation is employed when global routers jointly determine module selection and model architecture.

## Training Setting

Finally, we explore the training settings for modular architectures. We can identify three main strategies in the literature: 1) all modules are jointly trained for multi-task learning; 2) modules are introduced at different stages during continual learning; and 3) in transfer learning, modules are added post-hoc after pre-training, often as a way to fine-tune the model in a parameter-efficient fashion. Importantly, these strategies are not necessarily mutually exclusive and can be realised in combination.

## Joint Multitask Learning

In joint multi-task learning, there are two main settings. Firstly, task-specific parameterised components can be integrated into shared neural network architectures as a means to mitigate catastrophic forgetting or negative interference [(McCloskey & Cohen, 1989;](#b203)[French, 1999)](#b91) and as a way to scale the model size efficiently [(Kudugunta et al., 2021)](#b158). In these scenarios, modules are often optimised on individual tasks via fixed routing and specialise accordingly [(Hampshire & Waibel, 1992;](#b117)[Rajendran et al., 2017](#b249), inter alia; see § 4.1 for more details). As an alternative, the architecture can be fully modular, sharing only the parameters for learned routing [(Jacobs et al., 1991b;](#)[a;](#)[Rosenbaum et al., 2018;](#b256)[Kirsch et al., 2018;](#b155)[Chang et al., 2019, inter alia](#); see § 4.2.3 for more details).

Joint training can also be performed before post-hoc training: a shared base model can be pre-trained on multiple tasks as a warm-up before creating task-specific sparse subnetworks [(Sun et al., 2020a)](#) or as a way to provide a useful initialisation for modular parameters [(Vu et al., 2022c)](#). [Dua et al. (2022)](#b76) convert a dense language model pre-trained on text data into an MoE by decomposing the learned feed-forward layers. [Pfeiffer et al. (2022b)](#) add language-specific layers during multilingual pre-training of a language model. This prepares the model to be extended to more languages post-hoc; when new languages become available, a new (randomly initialised) learnable layer can be added to the inventory of modules, whereas the shared parameters remain untouched.

## Continual Learning

In a similar vein to countering negative interference in multi-task learning, continual learning-that is, continuously integrating new data into the model-often aims at mitigating catastrophic forgetting (i.e., the knowledge learned at early stages of training should not get overwritten by updates to the model later on).

Similar to the multi-task learning approaches discussed in § 6.1, new layers can be continuously introduced within the network which are only updated on the new data, keeping the others untouched. In methods like Progressive Networks [(Rusu et al., 2016)](#b264), PathNet [(Fernando et al., 2017)](#b84), and PackNet [(Mallya & Lazebnik, 2018)](#) when the model is trained on a new task, the parameters of the previous tasks are frozen; however, for new tasks, new modules may be learned, which connect to the existing set of modules. Often, the decision of inserting new modules at a given stage is made dynamically based on outlier detection [(Ostapenko et al., 2021)](#b223). Progressive Networks [(Rusu et al., 2016)](#b264), on the other hand, scale the model capacity linearly with the number of tasks. [Aljundi et al. (2017)](#b5) train separate experts for every task and route new examples based on the distribution of the reconstruction errors of task-specific auto-encoders.

Instead of adding new parameters to the model, other works in the continual learning landscape identify subnetworks for different tasks. For instance, some works identify subnetworks of the model, which have not been used by previous tasks. Consequently, updating these parts of the model will have little effect on the previously learned knowledge [(Javaloy & Valera, 2022)](#b142). Similarly, 'supermasks' ( §3.1; [Wortsman et al., 2020)](#b333), which learn a binary mask over a randomly initialised model, enable the extension to a potentially vast number of tasks during continual learning. Supermasks of previous tasks can be also linearly combined as a way to generalise to new tasks.

## Parameter-efficient Transfer Learning

Recently, transfer learning has become the dominant strategy for state-of-the-art results on most tasks. Auxiliary self-supervised objectives are utilised to pre-train models on a large amount of data. Subsequently, the model's weights are fine-tuned on the target tasks [(Howard & Ruder, 2018;](#b130)[Devlin et al., 2019)](#b67). Updating a small set of parameters of these large models has been demonstrated to perform equally well as full model fine-tuning, leading to the emergence of parameter-efficient fine-tuning strategies.

Most methods discussed in § 3 that are applied to large pre-trained models can be considered as post-hoc adaptation. Modularity can be achieved through parameter composition ( § 3.1) using sparse subnetworks [(Mehta, 2019;](#b204)[Chen et al., 2020;](#b46)[Donahue et al., 2014;](#b72)[Cai et al., 2020;](#b41)[Ben Zaken et al., 2022;](#b26)[Guo et al., 2021)](#b109), or low-rank adapters [(Li et al., 2018;](#b174)[Hu et al., 2022)](#b132), input composition ( § 3.2) by augmenting the function's input [(Brown et al., 2020;](#b38)[Li & Liang, 2021)](#), and function composition ( § 3.3) through adapter layers [(Rebuffi et al., 2017;](#b250)[Houlsby et al., 2019)](#b129) and rescaling [(Liu et al., 2022b)](#). Additionally, hypernetworks can be used to generate the parameters of any of the above-mentioned types of modules ( § 3.4).

Essentially, all of these methods are tightly connected as they share the same functional form ( § 3.5).

• There are three main training strategies: (1) Joint Multitask Learning, (2) Continual Learning, and (3) Parameter-efficient Transfer Learning.

• In Joint Multitask Learning, task-specific components are integrated into shared neural architectures, allowing modules to specialize via fixed or learned routing.

• Continual Learning methods aim to integrate new data while mitigating catastrophic forgetting, with options to introduce new modules dynamically or identify subnetworks for different tasks.

• Parameter-efficient Transfer Learning involves pre-training models on large datasets and finetuning on target tasks. Modular strategies can be applied post-hoc through various composition methods, including parameter composition, input composition, and function composition.

• These training strategies are not mutually exclusive and can be combined to achieve specific goals in modular neural architectures.

## Applications in Transfer Learning

Most applications of modular deep learning revolve around transfer learning. In particular, the two main purposes are: 1) parameter-efficient fine-tuning ( § 7.1), which achieves superior efficiency, prevents negative interference, and enables zero-shot transfer; and 2) zero/few-shot generalisation to new tasks [( § 7.2)](#). In what follows, we provide a quick overview of transfer learning applications of modular deep learning. For the in-depth discussions and illustrations of the key concepts, we will first focus on applications in NLP, and then draw direct analogies with other deep learning areas such as speech processing, computer vision, and multi-modal (representation) learning. In § 8, we will explore additional purposes of modular deep learning, including hierarchical reinforcement learning, programme simulation, and causal inference.

## Parameter-Efficient Fine-tuning

Regardless of the application area, one of the principal uses of modules has been to boost parameter efficiency and decrease model storage requirements of fine-tuning, eschewing so-called full model fine-tuning which requires storing a separate copy of the full model per task [(Howard & Ruder, 2018;](#b130)[Devlin et al., 2019)](#b67), see §6.3. In the simplest formulation, all task-specific updates are pushed to the parameters of the lightweight modules, while the parameters of the large base model are kept frozen throughout task fine-tuning. The modules then store task-specific knowledge that can be composed with the 'general-purpose' knowledge of the base model to adapt it to the task at hand. In NLP, this led to a number of research papers that introduced diverse modular architectures, as surveyed in § 3 and § 6. A typical evaluation protocol is fine-tuning a type of module on the popular GLUE and SuperGLUE benchmarks [(Wang et al., 2019)](#b318), comparing against full model fine-tuning or alternative modular architectures. The results usually corroborate either of two main goals: (i) improving performance with the same parameter budget versus (ii) maintaining performance with a smaller parameter budget [(Mahabadi et al., 2021a;](#)[Zhou et al., 2023)](#). In addition, modular adaptation has further benefits: first, it prevents negative interference between tasks [(Bapna & Firat, 2019)](#b24). Second, it allows for combining adapters to enable zero-shot transfer [(Pfeiffer et al., 2020b)](#). In light of the enormous size of state-of-the-art large language models (LLMs), parameter-efficient fine-tuning has emerged as the main way to update the pretrained models [(Hu et al., 2022)](#b132).

## Machine Translation

In the seminal work of [Bapna & Firat (2019)](#b24), bilingual (i.e., language-pair) adapters (see §3.3) were used to adapt a massively multilingual NMT model (spanning 103 languages) to a particular source-target translation direction. One benefit of such bilingual adapters is their ability to 'skew' the multilingual model to the language pair at hand without losing the benefits of massively multilingual training for low-resource languages.

Another positive effect of bilingual adapters concerns recovering the MT performance also for high-resource languages. High-resource languages might typically suffer from performance deterioration due to the particular interference phenomenon known as the 'curse of multilinguality' [(Conneau et al., 2020;](#b58)[Wang et al., 2020)](#b322): when (too) many languages compete for the fixed parameter budget of the model, the model's expressiveness and representation power deteriorates for all languages. The use of modules extends the parameter budget to recover the detrimental effects of multilingual inference through dedicated (i.e., modular) bilingual adaptation. Their work also demonstrates the superior performance of a multilingual model specialised towards a particular language pair over merely training a bilingual NMT model for the same pair from scratch.

However, fine-tuning bilingual adapters (or more generally, modules) for each translation direction assumes parallel data for all language pairs and requires n(n -1) modules to cater for all possible language pairs (one dedicated module in the encoder and another module in the decoder). Therefore, follow-up work [(Philip et al., 2020;](#b236)[Üstün et al., 2021)](#b303) aimed to learn monolingual (i.e., language-specific) adapters. Again assuming standard encoder-decoder architectures for MT such as mBART [(Liu et al., 2020)](#b189), this design requires only 2n modules in total. Besides improving parameter efficiency, this also bypasses the critical dependency on parallel data for all language pairs and allows for learning from monolingual data. Crucially, this design also enables translation to or from languages without parallel data, in a fully unsupervised way, and even to/from languages unseen by the base pre-trained encoder-decoder model. Put simply, when translating from language l s to l t , only the encoder adapters for l s plus the decoder adapters for l t are activated: the model is able to translate from l s to l t without seeing a single parallel l s to l t sentence. This application in the field of NMT exemplifies the power of modular design: available components, which were previously learned locally and asynchronously, can be recombined in novel ways to generalise systematically to unseen applications (i.e., in this particular case, to unseen translation directions). This is one of the main goals of modular deep learning ( § 1).

The separation into dedicated language-specific modules mitigates interference and catastrophic forgetting; however, it also hinders any positive transfer between modules of similar languages. The positive transfer can be achieved through the use of hypernetworks (see §3.4): [Baziotis et al. (2022)](#b25) learn to generate monolingual language-specific adapters for NMT. In fact, sharing the parameter generator takes advantage of language similarities [(Platanios et al., 2018)](#b239). As discussed in more detail later in §7.1.2, similar ideas of combining the modular design with hypernetworks have also been applied earlier and beyond NMT, e.g., for task fine-tuning with adapters in monolingual multi-task setups [(Mahabadi et al., 2021b)](#) and for cross-lingual transfer in single-task [(Ansell et al., 2021)](#b10) as well as in multi-task setups [(Ponti et al., 2021;](#)[Üstün et al., 2022)](#b304).

The curse of multilinguality and catastrophic interference in multilingual MT models have also been tackled through sparse sub-networks (see § 3.1). Lin et al. ( [2021](#)) extract sparse sub-networks for specific language pairs from a trained multilingual MT model via pruning. Subnetworks are then trained separately in order to specialise towards the particular translation direction. In fact, there exist dedicated small sub-networks (which can be obtained via standard masking) that store language pair-specific knowledge within the large network, where such knowledge should not interfere with other language pair-specific sub-networks [(Dua et al., 2022)](#b76). The same high-level idea has also been applied to domain adaptation of bilingual MT systems: e.g., [Liang et al. (2021)](#b178) show that it is possible to learn domain-specific sub-networks when fine-tuning the MT system on new domains, where a single large network (i.e., the full neural MT system) comprises multiple disjoint domain-specific sub-networks specialised to particular domains.

Another approach that leverages modularity for an increased language-specific capacity in MT is mixtureof-experts. Each expert is typically dedicated to a particular language or translation direction [(Kudugunta et al., 2021;](#b158)[Costa-jussà et al., 2022)](#). To maintain feasible decoding time, the procedure works as follows:

(i) during training, mix the inputs from different translation directions in the same batch, in order to learn the routing network and encourage positive transfer among related tasks; (ii) at inference time, different translation directions are decoded separately, and only the corresponding subset for elevant experts is loaded.

## Cross-Lingual Transfer

NMT focuses on translation as a single task and modularity was exploited mainly to carve language-specific and/or domain-specific modules that can support multilingual and multi-domain systems, respectively. In more general cross-lingual transfer setups, the aim is to transfer large models [(Devlin et al., 2019;](#b67)[Conneau et al., 2020)](#b58) fine-tuned for any task (e.g., sequence labelling tasks such as NER, text classification tasks such as NLI, sentiment analysis or intent detection for dialogue systems) on one or more source languages (where such task annotations exist) to one or more target languages [(Hu et al., 2020;](#b133)[Ruder et al., 2021)](#b263). Ideally, the transfer should be achieved without fine-tuning the full model [(Hu et al., 2020)](#b133), which results in catastrophic forgetting and negative interference, or requires the creation of separate model copies for each task.

The idea of training language modules thus largely follows what already outlined for MT in §7.1.1, with the addition of another set of dedicated modules that aim to capture task-related knowledge: task modules. Such language modules and task modules can then be combined to 1) favour zero-shot cross-lingual transfer for particular source-target directions [(Pfeiffer et al., 2020b;](#)[Ansell et al., 2021;](#b10)[2022;](#b269)[Parović et al., 2022)](#b227); 2) provide extra capacity to low-resource languages under-represented (or even not covered) in the large multilingual models such as mBERT or XLM-R [(Pfeiffer et al., 2021b;](#)[2022b;](#)[Ponti et al., 2020;](#b243)[Faisal & Anastasopoulos, 2022)](#b79), independently from task knowledge; and 3) enable handling unseen language-task or even language-domain-task configurations [(Ponti et al., 2021;](#)[Stickland et al., 2021)](#).

As an example of zero-shot cross-lingual transfer, the original MAD-X framework [(Pfeiffer et al., 2020b](#), Figure [1a](#)) relies on bottleneck adapters to implement language and task modules: In particular: 1) Language modules are inserted into each layer of the original neural model and are fine-tuned on (unsupervised) data of the particular language (e.g., via Masked Language Modelling) while the weights of the original model are kept

Published in Transactions on Machine Learning Research (11/2023) fixed. 2) After obtaining language modules, task modules are stacked on top of the source language module(s) and are fine-tuned relying on the task objective and task-annotated data in the source language(s), while both the original model and language modules are kept fixed. 3) At inference, source language module(s) are replaced with the desired target language module while retaining the task module: this enables zero-shot task inference in the target language. Recent work has introduced a spectrum of variations and enhancements to this core idea. For instance, inspired by the bilingual 'translation direction' adapters for NMT systems ( §7.1.1), [Parović et al. (2022)](#b227) learn bilingual adapters instead of single language adapters to boost transfer for a particular language pair. Faisal & Anastasopoulos (2022) and [Chronopoulou et al. (2022b)](#) learn language family adapters to reduce data sparsity for low-resource languages and capitalise on language similarity and cross-language sharing. Stickland et al. ( [2021](#)) decouple language and domain knowledge into dedicated modules (see also §7.1.3 later). Further, [Ansell et al. (2022)](#b11) implement dedicated modules as sparse sub-networks, the so-called language and task masks, which can be composed with the base model via parameter composition.

Following the analogy between language-specific and bilingual adapters, instead of learning separate language and task subnetworks, Foroutan et al. ( [2022](#)) learn dedicated task-language sub-networks, demonstrating the variance in the extracted subnetworks across different task-language combinations. The use of such language sub-networks as language modules, even without dedicated task modules, improves cross-lingual transfer for dependency parsing when used within a meta-learning setup [(Choenni et al., 2022)](#). [Litschko et al. (2022)](#b180) compare sparse sub-networks and bottleneck adapters for transferring ranking functions for information retrieval tasks across languages and find them both superior to full model fine-tuning.

Finally, a body of work again focuses on 'contextually generating' the modules via hypernetworks, aiming to increase efficiency and benefit from connections between different languages and tasks. A representative example is the Hyper-X framework [(Üstün et al., 2022)](#) provided in Figure [6](#fig_10), where the module parameter generation is conditioned on the (disentangled) task and language, and additionally on the index of the Transformer layer where the generated module is inserted. Each task and language are parameterised via separate embeddings, which enables adaptation to any tasklanguage combination, where these embeddings are low-dimensional vectors which are learned together with the parameters of the hypernetwork (see Figure [6](#fig_10) again). The framework thus leverages supervision and positive transfer from both multiple tasks and languages. Hyper-X can be seen as a more general variant of a series of precursors backed by the idea of contextual generation: Ponti et al. ( [2021](#)) condition the hypernetwork on both task and language embeddings but generates only the model's classifier head. Other methods generate modules but condition the hypernetwork only on tasks in a monolingual setup [(Mahabadi et al., 2021b)](#) or only on languages in a cross-lingual transfer setup [(Üstün et al., 2020;](#b302)[Ansell et al., 2021)](#b10).

## Domain Adaptation

As already hinted at in §7.1.1 and §7.1.2, dealing with different domains adds another tier to the modular design: domain-specific knowledge might be captured within dedicated domain modules. [22](#foot_21) This can again be accomplished through similar modular architectures as with language and task adapters. For instance, it is possible to inject domain-specific knowledge into (bottleneck) adapters [(Zhang et al., 2021;](#b346)[Chronopoulou et al., 2022a)](#) or to extract sparse domain-specific or task-specific sub-networks [(Thompson et al., 2018;](#b298)[Ke et al., 2021b)](#) for multi-domain and multi-task learning. Mixture-of-experts also enable multi-domain joint learning as well as domain adaptation [(Guo et al., 2018;](#b110)[Zhong et al., 2022)](#b351). Similar strategies have also been used in multi-domain and cross-domain speech processing and computer vision applications (see §7.1.5 and §7.1.6 later).

In domain adaptation, it is common to combine both shared parameters and domain modules that are learned jointly [(Bousmalis et al., 2016)](#b36). Beyond this standard setting, many approaches employ additional regularisation terms. The most common are 1) a domain-adversarial loss on the shared parameters in order to encourage them to be domain-invariant [(Ganin et al., 2016;](#b92)[Chen & Cardie, 2018)](#b49); 2) an orthogonality constraint on the domain modules to ensure that they capture different information [(Baktashmotlagh et al., 2013;](#b21)[Kim et al., 2017)](#b153); and 3) similarity constraints that bring representations of similar modules close together [(Bousmalis et al., 2016)](#b36).

## Knowledge Injection

Naturally, dedicated modules can also be assigned to inject and store external knowledge (e.g., from manually curated external knowledge bases), which can then interact with language, domain, or task knowledge. This idea has been explored with diverse external knowledge sources. For instance, [Lauscher et al. (2020)](#b164) aimed at complementing the distributional knowledge of large language models with conceptual and commonsense knowledge from ConceptNet [(Speer et al., 2017)](#b283). The external knowledge was captured within dedicated bottleneck adapters: they were fine-tuned via language modelling on synthetically created sentences from random walks over the ConceptNet graph structures. [Majewska et al. (2021)](#b199) stored verb-related knowledge from VerbNet [(Schuler, 2005)](#b275), a human-created verb classification repository, into bottleneck adapters, and demonstrated its usefulness in a range of tasks that require understanding of verb semantics. Along similar lines, [Wang et al. (2021a)](#) offered a generalisation of these approaches where different knowledge sources (e.g., Wikipedia, WikiData) are mapped to different dedicated adapters, which can be aggregated according to the task at hand. The same idea has been explored by [Lu et al. (2021)](#b191) in the biomedical domain, where the main knowledge sources were the UMLS Metathesaurus graph [(Bodenreider, 2004)](#b34) and biomedical Wikipedia articles. [Lu et al. (2021)](#b191) also introduce another component, the so-called knowledge controller, which can be seen as a standard attention-based function aggregator from §5.4. As an example of another relevant application, [Lauscher et al. (2021)](#b165) learned bottleneck adapters without manually curated external data, with the focus on model debiasing: the debiasing adapters were fine-tuned via standard language modelling on a counterfactually augmented corpus.

Finally, the idea of modular knowledge injection is also directly linked to the retrieval-augmented language models in text-only settings [(Lewis et al., 2020b)](#) as well as in multi-modal settings [(Yasunaga et al., 2023)](#b340) where the standalone retrieval module, detached from the 'main' language model, is responsible to fetch knowledge from some external memory or a knowledge base, and that knowledge is then used to condition the language model. In this design, the retrieval step and capability is made explicit and decoupled from the language model generation capability: as such, one can work directly on a retrieval module without the need to change the other components of the entire model [(Yu et al., 2023)](#b344). The ability of standard language models to use external tools is also sparked by the modular design: different external tools specialised for performing particular functions (e.g., conducting Web search, performing mathematical operations) are stored as separate modules accessed from the main model via external API calls. For a comprehensive overview of such augmented language models, we refer the reader to the recent survey [Mialon et al. (2023)](#). The use of modular deep learning for speech processing applications closely matches the ideas already exposed for NLP tasks. The landscape of the possible modular designs is exactly the same, where the only crucial differences are (i) the choice of the underlying large model, and (ii) the corresponding objective functions used to inject the specialised knowledge into the modules.

## Speech Processing

$Q K V f( ) .$
## CTC( ) . x12

For instance, the typical choice of the base model for automatic speech recognition (ASR) applications is one from the wav2vec family [(Baevski et al., 2020;](#b18)[Babu et al., 2022)](#b16), while the ASR-oriented objective function is the standard Connectionist Temporal Classification (CTC) loss [(Graves et al., 2006)](#b103). The high-level modular structure remains the same, as illustrated in Figure [7](#fig_11) with an example from [Thomas et al. (2022)](#b297), which utilises standard bottleneck adapters.

While in theory a large variety of possible modular configurations from § 3- § 6 can be applied to diverse speech processing tasks, the majority of current work in the area has indeed focused on the use of bottleneck (sequentially placed) adapters for ASR in monolingual and multilingual contexts. Before that, the concept of modularity can be traced to the work of [Swietojanski et al. (2016)](#b296), where the model re-weights hidden units using small amounts of unsupervised data to better adapt to a particular speaker or an environment. More recently, bottleneck adapters have been used to perform ASR adaptation to atypical and accented speech [(Tomanek et al., 2021)](#b299), unseen speakers with limited adaptation data [(Wang & Van hamme, 2022;](#b319)[Eeckt & Van hamme, 2022;](#)[Chen et al., 2023)](#b50), new domains and manners of speaking (e.g., children's speech) [(Fan & Alwan, 2022;](#)[Zhu et al., 2022)](#b357), or to perform further model customisation to specific speakers [(Biadsy et al., 2022;](#b32)[Sathyendra et al., 2022)](#b268) and for multilingual learning [(Kannan et al., 2019;](#b147)[Hou et al., 2022)](#b128). A notable exception, not resorting to adapter layers, is the method of (Winata et al., 2020) which aims to learn low-rank modules ( § 3.1), akin to the idea of LoRA [(Hu et al., 2022)](#b132), for end-to-end ASR.

Multi-task (where the term 'task' in this context can e.g. refer to different languages, domains, speakers, or accents) ASR setups have also witnessed the usage of mixture-of-experts, closely following the basic ideas already discussed for NMT ( §7.1.1) where different languages are assigned their dedicated modules through fixed routing. For instance, in speech processing, MoEs have been applied to multilingual ASR and cross-lingual ASR transfer [(Bai et al., 2022;](#b20)[Gaur et al., 2021;](#b97)[Kumatani et al., 2021)](#b160), while [You et al. (2022)](#b342) propose MoE for ASR with learned routing.

Beyond ASR, bottleneck adapters have also been used for speech translation [(Le et al., 2021)](#b166). Most recently, modular adapter-based approaches have been applied to text-to-speech methods (TTS) [(Hsieh et al., 2022;](#b131)[Morioka et al., 2022)](#b215), aiming to extend standard large multi-speaker TTS models such as FastPitch [(Lancucki, 2021)](#b162) to new speakers without compromising the TTS quality for the seen speakers. From a high-level perspective, one can see a direct analogy of this goal to the objectives in the MT literature of extending multilingual MT systems to unseen languages without compromising seen languages (see §7.1.1 again).

## Computer Vision and Cross-Modal Learning

In computer vision, similar to NLP and speech processing ( §7.1.5), dedicated modules are again used to enable parameter-efficient fine-tuning across multiple tasks and domains [(Rusu et al., 2016;](#b264)[Rebuffi et al., 2018;](#b251)[Berriel et al., 2019;](#b30)[He et al., 2022b, among others)](#). The core difference, again, is the choice of the actual neural architecture for the underlying model as well as for the modules: e.g., residual adapters [(Rebuffi et al., 2017)](#b250) consisted of simple 1 × 1 convolutions combined with the base ResNet neural model [(He et al., 2016)](#b123) while other work learned task-specific convolutional filters [(Newell et al., 2019;](#b221)[Bragman et al., 2019)](#b37). More recent work aims to exploit modular architectures from NLP (e.g., sequential or parallel adapters, LoRA, prefix tuning) with pretrained Vision Transformer (ViT) architectures [(Dosovitskiy et al., 2021](#b73)): e.g., [He et al. (2022b)](#) run a comparative empirical analysis of various modular architectures for vision tasks, while [Chen et al. (2021)](#b47) rely on sparse sub-networks.

Modular design lends itself naturally to cross-modal and multi-modal applications, where different modalities may be captured by modality-specific parameters and routing can also be modality-conditioned. For instance, in multilingual vision-and-language (V&L) settings, it is possible to conduct inference in languages that lack labelled task examples. In fact, language knowledge is again disentangled from the task and modality knowledge, and the knowledge for different input modality streams can be captured in dedicated modules. This idea has been heavily explored in recent work in multi-modal multi-task scenarios, both in monolingual [(Sung et al., 2022)](#b293) and multilingual contexts [(Bugliarello et al., 2022;](#b39)[Pfeiffer et al., 2022a)](#), for tasks such as image captioning [(Zhou et al., 2022a;](#)[Gao et al., 2021a](#)), text-to-image generation [(Maharana et al., 2022)](#b198), visual question answering [(Liu et al., 2022a;](#)[Sung et al., 2022)](#b293), visual reasoning [(Liu et al., 2021a)](#), etc. For instance, Flamingo [(Alayrac et al., 2022)](#) uses frozen pretrained vision and language models, and only trains adapter layers to handle sequences of arbitrarily interleaved visual and textual data. It is trained with a sequence modelling objective on Web-scale data [(Li et al., 2021)](#b175) and displays impressive zero-shot and few-shot capabilities. [Pfeiffer et al. (2022a)](#) use adapter modules to equip multilingual text-only models with the ability to also process the visual modality, as well as to equip monolingual multi-modal models to deal with input from multiple languages. [Papalampidi & Lapata (2022)](#) rely on hierarchical adapters (akin to hierarchical representation aggregation discussed in § 5) for the task of summarising long videos into textual descriptions. [Pan et al. (2022)](#b224) demonstrate that modular design also helps in image-to-video transfer tasks: they use adapter modules to equip a large image-based model without temporal knowledge with the ability to reason about dynamic video content.

We note that in this survey, we aim to list some exemplary applications and draw parallels between different yet similar application areas such as NLP, speech processing, and computer vision. While we acknowledge that there exists a wealth of other work in these areas, we have no pretence of exhaustiveness.

## Comparison and Design Principles

While a full-fledged comprehensive empirical study of the plethora of modular architectures across various application tasks and areas is still lacking, there exist initiatives such as the publicly available AdapterHub platform [(Pfeiffer et al., 2020a)](#): it provides (re)implementations of representative modular NLP architectures, within a unified framework tied to HuggingFace Transformers [(Wolf et al., 2020)](#b332). Among others, AdapterHub includes representatives of each computation method in § 3: LoRA [(Hu et al., 2022) (i.e., low-rank parameter composition)](#), prefix tuning of [Li & Liang (2021)](#) (input composition) and a number of bottleneck adapter configurations (function composition). The existence of AdapterHub delineates another crucial advantage of modularity: reusability of existing, already fine-tuned modules which can be (re)combined with the large neural models. In short, any practitioner can share or reuse a module specialised for a particular purpose (e.g., capturing specific task or language knowledge) with the community, facilitating community-wide sharing and thus avoiding time-and energy-costly repetitions of the same fine-tuning procedure. [23](#foot_22) As discussed in § 4, one can observe initiatives such as AdapterHub as continuously updating community-distributed multi-task models.

The discussion in this section also points to a more general principle: different end-goals even within the same end-application (e.g., NMT, cross-lingual transfer, domain adaptation) require rethinking the actual modular design, and the desired level and nature of modularity. For instance, if the goal in NMT (or cross-lingual transfer) is to boost performance for a particular translation or transfer direction, it might be useful to trade off some modularity for a better final performance by replacing language-specific monolingual modules with bilingual modules [(Bapna & Firat, 2019;](#b24)[Parović et al., 2022)](#b227). On the other hand, if the goal is to enable zero-shot or few-shot translation or transfer, the design with monolingual modules might be a better choice. In another example, if the focus is on MT or transfer for a particular low-resource language, the model designer should enable positive transfer to that language by 'opening' the flow of information from a module storing knowledge on high-resource languages similar to the target language if such languages exist (e.g., from Spanish to Galician) [(Üstün et al., 2021)](#), or by learning modules for families or groups of similar languages [(Chronopoulou et al., 2022b;](#)[Faisal & Anastasopoulos, 2022)](#b79). Analogously, related domains can also be grouped and hierarchically organised to enable positive transfer for domain adaptation [(Chronopoulou et al., 2022a)](#).

Other practical desiderata may also influence the selection of the actual modular design. If the final task performance is paramount, larger modules might be preferred, e.g., in order to offer enough extra capacity to store the wealth of language-specific information [(Ansell et al., 2022)](#b11). However, if model compactness is paramount, the criterion for choosing a specific design is instead the trade-off between efficiency (in terms of parameters and/or train and test time) and task performance; the optimisation of this trade-off has been the focus of recent research [(Rücklé et al., 2021;](#b258)[Mahabadi et al., 2021a;](#)[b;](#)[Sun et al., 2022)](#b290). In another example, if time efficiency during inference is a crucial requirement (e.g., real-time ASR in dialogue systems, low latency for information search systems) parameter composition methods such as sparse subnetworks or low-rank composition methods may be preferred over function composition methods as the latter increase the number of computations required during the forward pass, (see Table [3](#)). In yet another example, if storage requirements are a critical constraint, one cannot resort to huge mixture-of-expert models where billions of parameters must be stored [(Lepikhin et al., 2021)](#b168).

## Task Generalisation

The diverse applications of modular deep learning covered so far almost exclusively focus on learning modules associated with (arguably) well-formed and interpretable 'units of knowledge' such as languages, tasks, domains, dialects, accents, and speakers. However, modularity might also be achieved when such units are unknown. This relies on jointly learning arbitrarily sized inventories of so-called latent skills and a learned routing function ( § 4.2). Since such skills are learned end-to-end on a mixture of data from multiple tasks, they are often not straightforwardly interpretable. On the other hand, since arbitrary subsets of skills can be combined and each skill can be updated locally, these modular neural architectures are ideal for systematic generalisation to new tasks [(Zhang et al., 2022a;](#)[Ponti et al., 2022)](#).

In fact, another fundamental application in transfer learning is achieving zero-shot or few-shot generalisation to new tasks, where test examples are not i.i.d. with respect to training examples. The general experimental setup involves disjoint sets of training and test tasks. A model is pre-trained through multi-task learning on training tasks and then adapted to each new test task based on zero or few data points. Common examples of evaluation benchmarks for this setting include CrossFit [(Ye et al., 2021)](#b341), the T0 task suite [(Sanh et al., 2022)](#b267), or Natural Instructions [(Mishra et al., 2022)](#b210). While a common strategy to tackle this problem is instruction tuning [(Sanh et al., 2022;](#b267)[Wei et al., 2022a)](#), where models are fine-tuned prepending the instructions for each task, modular deep learning has emerged as a strong contender [(Alet et al., 2018;](#b4)[Kudugunta et al., 2021;](#b158)[Ponti et al., 2022)](#).

## Other Purposes of Modularity

In addition to scaling large models (for instance, through MoEs, as discussed in § 4.2.3) and facilitating transfer learning, which we covered in § 7, modularity serves multiple additional purposes. In particular, we devote this section to a cursory view of modularity for i) hierarchical reinforcement learning ( § 8.1); ii) neural programme simulation ( § 8.2); iii) neural causal inference ( § 8.3). While most of these applications predate the advent of neural networks, (modular) deep learning expands the scope and potential of these lines of research for a series of reasons. First, it holds promise to induce the relevant latent structures (such as options, programmes, or causal graphs, respectively) in an end-to-end fashion. Second, it marries these traditional problems with the ability to jointly model low-level perception, such as vision and language.  [(Andreas et al., 2017)](#b9). Two high-level policies Π corresponding to task instructions τ are illustrated. Each iteratively selects low-level policies π (options) corresponding to sub-tasks b from a shared inventory. These determine the choice of action given observations. In this case, options are implemented as predicate-argument pairs.

## Hierarchical Reinforcement Learning

The goal of reinforcement learning is to learn a policy, which predicts the next action of an agent based on past observation(s) from the environment, that maximises the return, i.e. the sum of future discounted rewards. However, many tasks span extremely dilated temporal horizons or provide only highly sparse or delayed rewards. In these cases, it becomes helpful to model intermediate abstractions between high-level goal specifications and low-level actions and observations [(Sutton et al., 1999;](#b295)[Precup, 2000)](#b245). This facilitates the planning abilities of the agent as well as their sample efficiency. In fact, the above-mentioned intermediate abstractions, known as options or skills, consist of sub-policies that are transferable across tasks.

More formally, each reinforcement learning task is a Markov Decision Process (MDP) consisting of states S and actions A, a transition function p : S × A × S → [0, 1] and a reward function r : S × A → R. We aim to learn a policy π : S × A → [0, 1]. We also define a value function as the expected (discounted) return from a given state s as

$V π (s) = E π [ ∞ t=0 γ t r t+1 | s 0 = s],$as well as a Q function from a state s and an action [Sutton et al. (1999)](#b295) and [Precup (2000)](#b245), each option ω ∈ Ω is defined as a tuple (I ω , π ω , β ω ), where I ω ⊆ S is the initiation set, π ω : S × Ω → [0, 1] the option-specific policy, and β ω : S → [0, 1] is the termination function. For simplicity, many works assume that ∀s ∈ S, ∀ω ∈ Ω, s ∈ I ω : in other words, all options are available at every state. Augmenting a task with options transforms it into a Semi-MDP, with corresponding functions V Ω (ω) and Q Ω (s, ω).

$a as Q π (s, a) = E π [ ∞ t=0 γ t r t+1 | s 0 = s, a 0 = a]. Following$Learning options involves a series of challenges [(Jiang et al., 2019)](#b143). Firstly, it is not trivial to specialise sub-policies towards distinct behaviours. This shortcoming is common to many modular architectures with learned routing [(Mittal et al., 2022, see § 4.2)](#). Not only this, the problem of hard learned routing has often been cast in a reinforcement learning framework ( § 4.2.2). Secondly, one must define the space where the actions of the high-level policy, which are latent variables, lie. In practice, one could treat them as a discrete, unordered set. In this case, a module from an inventory is chosen for a certain amount of time steps. However, alternative methods operate in structured spaces such as language, which is more transferable and scalable due to its combinatorial nature. Thirdly, training multiple options dilates the training time and requires collecting an appropriate amount of experiences for each of them. Fourthly, if trained jointly, options change simultaneously with the master policy, which is a source of non-stationarity. As a consequence, previous experiences for the master policy become invalid if the options have been updated in the meantime. Again, this is reminiscent of the challenges of learned routing exposed in § 4.2.

The simplest solution to circumvent end-to-end joint learning of the master policy and options is to provide separate supervision to both [(Sutton et al., 1999;](#b295)[Dayan & Hinton, 1992)](#b64). However, this may require extensive annotation, which is often not available. Thus, an alternative method is defining sub-goals, i.e. states an agent should reach as a stepping stone towards the high-level goal Dietterich [(2000)](#). Nevertheless, this similarly fails to scale due to the exponentially growing number of combinations of sub-goals some tasks may entail. Moreover, this does not eschew the need to train individual sub-policies for each sub-goal. A partial remedy is offered by hindsight learning, where an off-policy correction is introduced [(Nachum et al., 2018)](#). Specifically, the original target sub-goal of the current option is substituted with the one maximising the probability of the past sequence of low-level actions. Similarly, the master policy can be trained in hindsight through the currently predicted sequence of high-level sub-goals. Overall, relabelling past experiences significantly improves the model's sample efficiency.

A more radical solution to the challenge of scalability is jointly training both the master policy and options in an end-to-end fashion. To this end, [Bacon et al. (2017)](#b17) put forth a new architecture, the option-critic, that discovers options from data, without supervision for the intermediate abstractions. This architecture is trained based on policy gradient theorems [Bacon et al. (2017)](#b17) derive for options. Moreover, they augment the set of actions A available to each policy π ω with a special end-of-policy action eop instead of explicitly modelling β ω . Intuitively, formulating the execution as call-and-return, a master policy π Ω determines the active option ω, whose policy π ω is followed until the eop action is chosen. At this point, control returns to the master policy to choose the next option, and so on until termination. A downside of this method is that it is unstable and often diverges to degenerate solutions [(Jiang et al., 2019)](#b143). Thus, several inductive biases have been proposed to correct it. A popular method is leveraging intrinsic rewards: an auxiliary loss diversifies options by maximising the mutual information between each option and the next state conditioned on the current state [(Florensa et al., 2017;](#b85)[Kulkarni et al., 2016](#b159)).

An orthogonal question revolves around the ideal space for the option variables. In fact, compared to a discrete, unordered inventory of (possibly hard-coded) options, language affords more flexibility [(Andreas et al., 2017;](#b9)[Jiang et al., 2019)](#b143) as it solves many of the above-mentioned challenges. In fact, all sub-policies can be implemented through a single model conditioned on the linguistic label of the current option. This not only allows options to borrow statistical strength from each other but also makes options reusable in new tasks. Moreover, the nature of language (through its infinite use of finite means) is suitable to capture the extremely complex combination of sub-goals of many reinforcement learning tasks. Note that linguistic options can be interpreted as a generalisation of sub-goals, as every instruction implicitly corresponds to a subset of states [(Jiang et al., 2019)](#b143).

In practice, to learn linguistic options, [Andreas et al. (2017)](#b9) assumes that 'sketches' of options are provided for supervision (see Figure [8](#fig_12)). To induce them, subsequent methods rely instead on synthetic experiences through relabelling [(Jiang et al., 2019)](#b143), or restricted vocabularies and syntax such as predicate-argument pairs [(Das et al., 2018)](#b63). Recently, the master policy has been frequently implemented as a large language model. Since these are pre-trained on text, they already contain world knowledge that can serve as a powerful inductive bias for grounded learning. For instance, [Huang et al. (2022)](#b135) use frozen language models to generate options through prompting in a zero-shot fashion.

## Programme Simulation

Another distinct purpose of modular architectures is to model programmes, as a means to induce them from data or to simulate symbolic algorithms. The simplest (and least expressive) family of programmes are Finite State Automata (FSA). These receive a neural implementation in the Compositional Recursive Learner (CRL; [Chang et al., 2019)](#b45), similarly to Routing Networks [(Rosenbaum et al., 2018)](#b256) and Modular Networks [(Kirsch et al., 2018)](#b155). In these neural architectures, a loose equivalence can be drawn as follows: transformations induced by modules are transition functions (arcs in the graph), input and output representations are the states (nodes in the graph), and the input is the starting state. A memoryless routing function selects the transition based on the current state. Thus, the programme graph is constructed dynamically. The final states are defined as those reached after the router selects a special end-of-computation action.  [(Graves et al., 2016)](#b105). A recurrent neural controller iteratively receives an input from the environment, writes to / reads from memory, and produces an output. Read and write operations are based on attention between generated keys and memory entries. A special mechanism keeps track of memory usage and temporal links between entries.

On the other hand, a programme graph can be constructed globally based on the task description before processing the data. In particular, Neural Module Networks (NMNs; [Andreas et al., 2016b;](#)[a)](#) rely on an off-the-shelf semantic parser (and custom rules) to map a query in natural language into a tree graph. The nodes of this graph are learnable modules characterised by: 1) their types of input (raw image features and/or attention scores) and output (attention scores or labels); and 2) the particular instances of a type, indicated as an argument in the form of a natural language string. For instance, the module find[cat] takes an image and returns attention scores over the regions that contain cats. Compositionality is achieved by sharing weights across modules with the same type or instance. NMNs have been further extended to be amenable to end-to-end training without the aid of an external parser [(Hu et al., 2017)](#b134). In this case, the mapping from queries to programme graphs is learned by imitating expert demonstrations while the module parameters are learned based on the downstream loss of visual question answering.

In addition to the routing function and computation functions, a model can be extended with an external memory. In fact, these three mirror the fundamental components of a computer architecture: elementary operations, logical flow control, and a random-access memory that can be read and written to [(von Neumann, 1945;](#b312)[Graves et al., 2014)](#b104). While (appropriately wired) recurrent neural networks have been shown to be Turing-complete [(Siegelmann & Sontag, 1995)](#b281), separating the three functions into distinct components provides an inductive bias to simulate the workflow or a computer programme. Neural Turing Machines (NTMs; [Graves et al., 2014)](#b104) introduced a fully differentiable read-write memory matrix that interfaces with the main recurrent network through an attentional mechanism. In particular, this memory can be addressed both based on content (i.e., the match between its entries and the current input) and based on location, in order to store and retrieve temporally ordered information in contiguous entries. NTMs were further extended into the Differentiable Neural Computer (DNCs; [Graves et al., 2016, Figure 9)](#), which amended some of the limitations of NMTs, such as avoiding interference in the memory, freeing up previously written locations, and storing temporally ordered sequences in non-contiguous chunks. Another family of memory-augmented methods include the Neural Programmer Interpreter (NPI; [Reed & de Freitas, 2016)](#b252). This model is trained with full supervision from execution traces or through reinforcement learning [(Pierrot et al., 2019)](#b237). In particular, a core recurrent network receives information from a programme module, as well as representations from the environment module. In its output, it produces the index for the next sub-programme and its arguments (as well as a special termination symbol). Finally, a recent thread of research focused on simulating the behaviour of symbolic algorithms with vanilla (non-modular) neural networks. An example is neural algorithmic reasoning [(Veličković & Blundell, 2021)](#b308). First, a processor network is trained to emulate the output of a symbolic programme (e.g., Dijkstra's algorithm for shortest paths) that operates on abstract representations (e.g., weighted graphs). Second, encoder and decoder networks can be trained to operate on sensory real-world data while matching the input-output types expected by the processor network.

Among the main applications for programme simulations are settings where sub-problems are shared, such as multi-task or curriculum learning. By distilling the most common functionalities into modules, these can be reused to generalise compositionally to new sequences of sub-tasks. Another application is compositional reasoning, such as (visual) question answering [(Andreas et al., 2016b;](#)[a)](#). In general, external memory is useful for reasoning over complex data structures, such as graphs [(Graves et al., 2014;](#b104)[2016;](#)[Reed & de Freitas, 2016)](#b252). Finally, neural models can emulate symbolic algorithms to extend their capabilities to operate on sensory real-world data.

## Causal Discovery and Inference

Modularity in the design of a model may be assumed to reflect the modularity in the (physical) mechanisms of the world. In fact, a crucial assumption in causal inference [(Schölkopf et al., 2012)](#b273) is that such mechanisms underlying data generation are independent, as they do not influence each other, and reusable, as they may play a role in multiple distributions. Consequently, if one of the mechanisms, which defines a conditional distribution in the model graph, changes-possibly because of an intervention-the other modules remain invariant. If a machine learning model mirrors this modular structure, it is better suited to generalise in a sample-efficient way to new tasks: in fact, local distribution shifts require updating only the corresponding module parameters, which in turn results in faster adaptation [(Bengio et al., 2020;](#b29)[Mittal et al., 2022)](#).

The key challenge for this problem is how to specialise each module towards a specific mechanism based uniquely on observational data, especially when the number and nature of the mechanisms are unknown. Competition among the modules through top-k routing (see § 4.2.2) is a common feature of many proposed solutions. 24 Parascandolo et al. (2018) show how to invert causal independent mechanisms through a modular neural architecture, given data from the original distribution and an unlabelled mixture of their transformations (see Figure [10](#fig_14)). Their model consists of a mixture of experts and an adversarial discriminator, which enforces that the inverted transformation lies in the support of the original distribution. Another architecture relying on module competition and capable of modelling sequential data is Recurrent Independent Mechanisms (RIMs; [Goyal et al., 2021)](#b102). Here, the modules are recurrent networks with separate parameters, each representing a different transition dynamics. However, their states are not entirely independent, as active modules are allowed to communicate through attention. This reflects a second assumption, namely that the dependencies among variables are highly sparse [(Mittal et al., 2022)](#). Attention can also serve to direct the flow of bottom-up and top-down information [(Mittal et al., 2020)](#b212).

Another challenge of neural causal discovery is jointly inducing abstract latent variables (such as objects or entities) from low-level perception (e.g., pixels of an image) while simultaneously learning the causal graph underlying such variables, which determines how they interact [(Ke et al., 2021a)](#). The lacklustre abilities of vanilla neural models to understand the compositional properties of symbolic building blocks, i.e. their 'binding problem', arguably explains their current shortfalls in systematic generalisation [(Greff et al., 2020)](#b106).

Object-centric learning holds promise to mitigate these limitations. For instance, it can be facilitated by slot attention, which is a fully differentiable and iterative attention mechanism that interfaces between perceptual representations and slots, a set of unordered placeholder variables [(Locatello et al., 2020)](#b190). [(Didolkar et al., 2021)](#b68) propose Neural Production Systems, where rule templates can be bound to specific entities present in the working memory, in order to update their representations. In particular, rules are MLP modules and the matching with entities (triggering updates) is parameterised by attention.

Crucially, observational data alone is often[foot_24](#foot_24) insufficient to learn structural causal models as they may not be identifiable [(Pearl, 2009)](#). Hence the necessity to augment observation with interventions and counterfactuals. These allow for answering questions about cause-effect relationships rather than mere correlations. In real-world scenarios, however, the nature and number of interventions are unknown [Ke et al. (2021a)](#). In this setting, there is no formal guarantee that causal discovery succeeds. Yet, [Ke et al. (2019)](#b149) finds that DAG discovery on interventional data based on continuous optimisation recovers causal graphs reliably. In particular, modular architectures surpass both vanilla models and graph neural networks [(Ke et al., 2021a)](#). Recently, [Geffner et al. (2022)](#b98) perform causal inference in a deep non-linear additive noise structural equation model, based on autoregressive flows. Variational inference is used to learn a posterior over causal graphs. The learned functions can be further used to estimate conditional average treatment effects based on simulations.

The main purpose of these deep modular methods is causal inference and discovery, which has applications in several branches of medicine and economics [(Geffner et al., 2022)](#b98). In addition, these methods are particularly relevant in grounded settings, where the distribution of the observations from the environment changes as the agent learns better policies [(Goyal et al., 2021)](#b102). Moreover, causal discovery can be combined with model-based RL methods to learn a self-supervised model of the environment, i.e. its variables and their causal dependencies, from trajectories of observations, actions, and rewards. This allows for simulating the potential outcomes of a policy before execution and thus estimating better value functions, which dramatically improves sample efficiency in agents [(Ke et al., 2021a)](#). Another common application of this family of modular neural architectures is out-of-distribution generalisation: for instance, zero-shot transfer to images of different sizes or sequences of different lengths [(Goyal et al., 2021)](#b102).

## Conclusions

• Modularity is defined as the functional specialisation of the components of a system.

• Specialised sub-networks may emerge in vanilla neural modules (from multitask training or regularisation), but they are seldom reused and recombined.

• Deep modular architectures rest on the separation between computation functions on the one hand and routing and aggregation functions on the other.

• Computation functions may consist of any neural module. Modules may modify the original parameters, be concatenated to the input, or composed with the original function. • All composition strategies are equivalent to summing the original output with a term depending on the new module. In practice, however, they offer different trade-offs between efficiency (in time and space, during training and inference) and performance.

• Routing controls the flow of information, i.e., module selection. In fixed routing, it is determined a priori based on expert knowledge. When this is not available, routing parameters are learned. • Learned routing is challenging because of training instability, module collapse, and overfitting. Thus, learned routing often underperforms fixed routing. • Routing can be conditioned on (parts of) the input or metadata such as task identity. Routing can take place at different levels, such as globally for the whole model or layer-wise.

• Soft routing assigns every module a continuous score and performs a weighted combination of their outputs. It is amenable to being learned via gradient descent but is highly inefficient. • Hard routing activates only a subset of modules via top-1, top-k, or variable-size selection. It is learned via reinforcement learning, evolutionary algorithms, or stochastic re-parameterisation. It corresponds to the principles of conditional computation and information bottleneck in cognition.

• Hypernetworks can be interpreted as combining unnormalised routing (task embedding) with modules (generator). They can in turn generate parameters of other modules.

• If routing selects multiple modules, these must be aggregated via a function.

• Module parameters or outputs can be interpolated for aggregation, according to scores from the routing function, an attention mechanism, or via simple averaging.

• Alternatively, aggregation may involve composing the module functions, either sequentially or based on a tree graph obtained from global routing.

• The applications include parameter-efficient fine-tuning in NLP, computer vision, and speech processing. These rely on the same types of modules and fixed routing. In addition to increased efficiency, this prevents negative interference and enables zero-shot transfer.

• Modularity also serves the purpose of generalising to new tasks systematically, by recombining modules and locally updating them.

• Modular deep learning transcends the confines of private research: it enables community-driven sharing, expanding, reusing, and updating of the modules.

• In hierarchical reinforcement learning, modular options serve as abstractions between task goals and low-level actions and observations. They facilitate planning in long-horizon and sparse-reward tasks and increase sample efficiency due to transferability.

• In programme induction, the components of deep models can mirror a computer architecture: modules are elementary operations and routing is logical flow control. These are often augmented by an external read-write memory. Modules can also simulate symbolic algorithms.

• In causal discovery and inference, modules may be taken to correspond to physical mechanisms that are independent and reusable.

• Modular deep learning empowers these traditional applications by learning abstractions (options, programmes, causal graphs) end-to-end from perceptual stimuli.

## Future Work

While recently modularity has attracted increasing attention in research, there remain many interesting open research questions along the axes of modularity introduced in this survey. We provide an overview of some of these directions for future work.

Combination of Different Types of Computation Functions Existing computation functions (see § 3) are mostly associated with a single category: parameter composition, input composition, or function composition. There are a few exceptions such as compacter (Mahabadi et al., 2021a)-low-rank adapterswhich combine multiple types. In general, techniques from parameter composition that incorporate sparsity, a low-rank or structural constraint are agnostic of the form of the module. In practice, this should enable more efficient learning and aggregation.

Learned Module Structure Most modules used in current works share the same architecture, which is reused across different settings. Depending on the skill or knowledge that should be learned, a module may need to be structured differently and might require access to another component or other type of data. In the extreme, a model may require a special-purpose architecture to be able to perform a specific capability [(Andreas et al., 2016b)](#). As modules are more widely used, they may benefit from being learned in a more flexible manner, perhaps incorporating ideas from neural architecture search [(Negrinho et al., 2019)](#b220) in a module-specific space of architecture primitives.

Standardising Modularity Evaluation Depending on the dimension studied, modular approaches may be evaluated based on a variety of factors including downstream performance, memory footprint, number of parameters, latency, and compositional generalisation. In order to make progress on modular models in general, evaluation should be standardised. Current evaluation is additionally mainly based on existing datasets that are re-purposed to enable modular evaluation such as by framing them in a zero-shot transfer setting. Future work on modularity evaluation should design forward-looking evaluation benchmarks that are designed to test the capabilities of the next generation of modular models such as assessing the composition of skills and acquisition of new types of reasoning abilities.

Nature of Modular Representations While modular representations have been aggregated and composed, it remains mostly unclear how the inductive bias of a computation function influences the modular representation that is learned. In addition, it remains unclear how computation functions differ on a representation level. Beyond the computation function, it is also unclear how the other dimensions of our taxonomy, i.e., the routing function, the aggregation function, and the training setting influence the nature of the modular representations.

Hierarchical Modularity Current approaches mostly do not differentiate between high-level and low-level skills and how they relate to each other. It might also be possible to designate particular parts of the model or dedicated modules to capture a set of specialised skills or options, and clearly distinguish between other (sets of) skills. At fine-tuning, even more specialised sub-modules could be learned focused only on the previously designated modules. One example might be learning fine-grained specialised subnetworks over larger subnetworks of the original model, offering gradual module specialisation.

Learned Routing for Pre-training Fixed routing (see § 4) is the most common strategy to disentangle knowledge into modular parts of the model. However, fixed routing limits the usability of the proposed methods as they cannot be used on data, which lacks the metadata needed for fixed routing; for instance, when training on heterogeneous data, metadata such as domain information often does not exist. While learned routing methods do not require this metadata to perform routing a priori, they suffer from training difficulties (as discussed in § 4.2). This opens up research directions that enable modular pre-training with learned routing, which would make modular models applicable to a broader set of data.

## Modular Instruction Tuning

The main way in which current LLMs are specialised to particular downstream settings is via instruction tuning [(Wei et al., 2022b)](#), i.e., fine-tuning on a collection of tasks described via instructions. These tasks are increasingly defined based on a set of skills and capabilities that a model should learn, which opens the room to developing modular instruction tuning methods that enable the acquisition, updating, and composition of specialised knowledge.

Benchmarking Routing Methods Existing studies mainly evaluate routing methods based on performance but do not take into account how different routing strategies influence modular representations. In order to make progress on better routing methods, benchmarks and metrics are necessary that compare routing mechanisms from a modularity perspective across different settings.

Structured and Sparse Aggregation Current aggregation methods (see § 5) combine the information from multiple modular components by applying arithmetic operations such as addition and subtraction across all parameters, which likely includes parameters that should not be modified. Structured or sparse aggregation methods could focus on aggregating information within salient subnetworks or parameter groups, which might make aggregation more efficient and improve out-of-distribution generalisation.

Learned Aggregation Methods Most aggregation methods are based on arithmetic operations. Depending on the nature of the modular information, it may be useful to (non-)linearly transform the representations. More complex domain-specific aggregation methods can be learned in conjunction with the modular representations to enable better generalisation to new settings.

## Merging Modular Models

In recent work, merging models trained with different settings has led to improved performance [(Wortsman et al., 2022, inter alia)](#). Rather than requiring separate training runs of a model, a multi-task model can alternatively be trained with modular components that are designed to be merged at a later stage. This potentially allows for an architecture, which can be computationally efficiently trained while covering many modalities.

Extensible Multi-task Models Most approaches in multi-task learning have focused on training dense models, with a key limitation being that models cannot easily be extended to new settings. Focusing on training multi-task models with modular components ensures that the baseline models are much easier to adapt and extend to new settings. Given the trend of pre-training larger and larger models from scratch, modularising parts of such models and developing modular methods that can be shared across different architectures and model sizes may lead to more sustainable model development.

![. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.2 Input Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.3 Function Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.4 Hypernetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15]()

![Figure 1: Case studies of modular deep learning; best viewed in colour. Green components illustrate different routing functions (see § 4); shade-of-purple components illustrate modular computation functions (see §3). 1a) MAD-X (Pfeiffer et al., 2020b) uses Adapter layers with fixed routing for zero-shot cross-lingual transfer. 1b) Polytropon (Ponti et al., 2022) uses low-rank adapters (LoRA; Hu et al., 2022) with hard learned routing for few-shot task adaptation. 1c) MoE Transformers(Fedus et al., 2021; Clark et al., 2022, inter alia) use Multi-Layer Perceptrons with top-k soft routing, in order to scale to larger model sizes. The three representative models illustrated here are only a fraction of possible configurations from the 'configuration manifold' that can be created by varying the components surveyed in §3- §6.]()

![Figure 2: Different modular designs for Transformer architectures; best viewed in colour. Task-specific modular components are illustrated in magenta and purple, respectively. (a) Parameter Composition ( § 3.1): A sparse sub-network in the linear layer as part of multi-head-attention. (b) Input Composition ( § 3.2): Prefix-tuning (Li & Liang, 2021) extends the input by prepending embeddings to the key and value matrices in the Transformer layer. (c) Function Composition ( § 3.3): Task-specific bottleneck layers transforming the hidden representations are inserted in each layer (Houlsby et al., 2019). (d) Hypernetwork ( § 3.4):A small separate neural network generates modular parameters conditioned on metadata. We show its application to function composition but it is compatible with all computation functions.]()

![Figure 3: Different approaches of function composition. (a) Sequential Bottleneck Adapter:The first adapter architecture proposed for transformers which consists of two bottleneck layers placed after the multi-head attention (MHA) and feed-forward (FF) layers(Houlsby et al., 2019). (b) Parallel Bottleneck Adapter: Bottleneck layers processed in parallel to the MHA and FF layers of the pre-trained transformer components(Rebuffi et al., 2018;Stickland & Murray, 2019; He et al., 2022a). (c) (IA) 3 : Rescaling operations performed within the MHA and FF layers(Liu et al., 2022b).]()

![Figure 4: Different routing methods. (a) Fixed Routing: Examples are passed to modules based on a pre-defined logic, known a priori. (b) Hard Learned Routing: Learned hard selection modules. (c) Soft Learned Routing: Soft selection and weighting of modules.]()

![Nevertheless, one could imagine obtaining the best of both worlds by hybridising sparse MoE Transformers models with deterministic or learned routing strategies from § 4.1 and § 4.2.2. Instead of routing each individual token separately, all tokens of a single example can be routed to the same experts.Kudugunta et al. (2021) experiment with two versions of example-level routing for machine translation: In sentence-level routing, they average pool over the token embeddings, and condition the router on the resulting representation. In task-level routing, a task embedding is trained, based on which the router learns the distribution over modules. In a similar vein,Gupta et al. (2022) andXi et al. (2022) implement task-level routing across modular experts to improve the amount of knowledge sharing during multi-task learning in NLP and computer vision, respectively.]()

![Figure 5: Different routing levels. (a) Layer-wise Routing: The indices are chosen based on the input to the current layer. (b) Naive Global Routing: The same indices of modules are chosen for all the layers of the model. (c) Global Routing: The configuration (possibly different for each layer) is chosen globally.]()

![pre-train modular components for different textual domains d ∈ D. When utilising the pre-trained modules on unseen data, they weight the output representations h d of the respective domain modules ϕ d according to the posterior distribution over the input examples, i.e. α = p(D | x):]()

![Figure6: Hyper-X(Üstün et al., 2022): an example application of contextual module generation where a hypernetwork takes the concatenation of task, language and layer embeddings as input and generates a flat parameter vector. This is further reshaped into an adapter module within each Transformer layer. Learning independent layer embeddings and sharing a single hypernetwork across all layers(Ansell et al., 2021) (i) enables information sharing across layers, and (ii) reduces trainable parameters of the hyper-network by a factor corresponding to the number of layers.]()

![Figure 7: The structure of the wav2vec 2.0 model with taskspecific bottleneck adapters for parameter-efficient ASR fine-tuning from Thomas et al. (2022); f (•) denotes a convolutional encoder followed by 12 standard Transformer encoder blocks. For downstream ASR a linear classifier, CT C(•), is applied to the final encoder output.]()

![Figure 8: An example of Hierarchical Reinforcement Learning ( § 8.1), Policy Sketches(Andreas et al., 2017). Two high-level policies Π corresponding to task instructions τ are illustrated. Each iteratively selects low-level policies π (options) corresponding to sub-tasks b from a shared inventory. These determine the choice of action given observations. In this case, options are implemented as predicate-argument pairs.]()

![Figure 9: An example of Programme Simulation ( § 8.2): Differentiable Neural Computer(Graves et al., 2016). A recurrent neural controller iteratively receives an input from the environment, writes to / reads from memory, and produces an output. Read and write operations are based on attention between generated keys and memory entries. A special mechanism keeps track of memory usage and temporal links between entries.]()

![Figure 10: An example of Causal Inference ( § 8.3): Causal Independent Mechanisms (Parascandolo et al., 2018). A transformed example is routed to an expert which maps it to the original distribution. An adversarial discriminator attempts to distinguish between reconstructed and original examples.]()

These phenomena have also been referred to as spatial and temporal 'crosstalk'(Jacobs et al., 1991b).

[Lange et al. (2022)](#b163) found that clusters identified through downstream (output) information do not match with the clusters identified through upstream (input) information. They attribute this phenomenon to their different roles, namely disentanglement of the input structure and composition of the output structure, respectively.

We abuse notation by treating indexing over functions, f i , as identical to indexing over the parameters of a function, f θ i . In this survey, both are used interchangeably.

To avoid clutter in terminology, throughout this work we use the term composition to refer to the merger of computation functions ( § 3), and the term aggregation to refer to different approaches of combining the outputs of different modules ( §

5).

This is typically implemented by masking the gradient based on the binary mask b ⊙ ∇ θ L(f θ , D) where L is a loss function and D is a dataset[(Ansell et al., 2022)](#b11).

In this view, there is no clear differentiation between model parameters θ and module parameters ϕ.

We omit the layer index n to simplify the presentation.

Published in Transactions on Machine LearningResearch (11/2023)   

Alternative non-parametric routing strategies include random routing[(Zuo et al., 2022;](#b359) Wang et al., 2022)  or routing based on hash functions[(Roller et al., 2021)](#b255).

In §4.2.3 "Token-Level Routing" we provide a brief overview over recent works that focus on the efficiency aspect. However, it is important to note, that the focus of this paper is centred around modularity and not efficiency.

Alternatively, combining entire models stored in model repositories via distillation[(Khanuja et al., 2021)](#b152) or averaging[(Matena & Raffel, 2021)](#b202) can also help avoid negative interference(Don-Yehiya et al., 2022); however, this is usually less efficient and subject to limitations such as those discussed later in § 5.

This encourages module re-usage at different layers.

Intrinsic rewards can be added, for instance favouring diversity in the module selection across time steps[(Rosenbaum et al., 2018)](#b256).

In the following sections we use the term "expert" and "module" interchangeably to reflect common practice in the body of research on MoEs.

For more details on load balancing methods we refer to[Fedus et al. (2022)](#b75), Chapter 4.

Combining modules has the potential to significantly improve inference speed.

vec('King') -vec('Man') + vec('Woman') ≈ vec('Queen'), with vec(•) denoting word embeddings of the respective words.

Note that the latter strategy leads to high variance in the norms of hidden representations if the router can select variable-size subsets of modules.

Notably, RAG is also discussed in § 3.2 due to its dual capability of both input composition and aggregation, for instance when multiple documents are used in the retrieval process.

For further applications of RAG see § 7.1.4.

For instance, disentangling domain and language information yields benefits for NMT and cross-lingual transfer applications[(Vilar, 2018;](#b310) Cooper Stickland et al., 2021;[Pham et al., 2021;](#b235)[Saunders, 2022)](#b269).

The (concept of) reusability enabled by the modular design also positively impacts energy consumption[(Strubell et al., 2019)](#b288), making an important leap towards Green(er) AI[(Schwartz et al., 2020)](#b276).

In addition to causal inference, this strategy is also inspired by the global workspace theory[(Baars, 2005)](#b15). This theory postulates specialised modules compete to update a shared workspace, and the resulting communication bottleneck creates a crucial inductive bias in human cognition.

Unless specific assumptions are made about the data generating process, such as linear but non-Gaussian data.

