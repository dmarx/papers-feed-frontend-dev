# Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in Generative Adversarial Networks

## Abstract

## 

Owing to the growing concerns about privacy and regulatory compliance, it is desirable to regulate the output of generative models. To that end, the objective of this work is to prevent the generation of outputs containing undesired features from a pre-trained Generative Adversarial Network (GAN) where the underlying training data set is inaccessible. Our approach is inspired by the observation that the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features. However, such directions usually result in the degradation of the quality of generated samples. Our proposed two-stage method, known as 'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also maintaining the quality of generated samples. In the initial stage, we adapt a pre-trained GAN on a set of negative samples (containing undesired features) provided by the user. Subsequently, we train the original pre-trained GAN using positive samples, along with a repulsion regularizer. This regularizer encourages the learned model parameters to move away from the parameters of the adapted model (first stage) while not degrading the generation quality. We provide theoretical insights into the proposed method. To the best of our knowledge, our approach stands as the first method addressing unlearning within the realm of high-fidelity GANs (such as StyleGAN). We validate the effectiveness of our method through comprehensive experiments, encompassing both class-level unlearning on the MNIST and AFHQ dataset and feature-level unlearning tasks on the CelebA-HQ dataset. Our code and implementation is available at: [https://github.com/atriguha/Adapt_Unlearn](https://github.com/atriguha/Adapt_Unlearn).

## Introduction

## Unlearning

Recent advancements in deep generative models such as Generative Adversarial Networks (GANs) [(Goodfellow et al., 2014;](#b21)[Arjovsky et al., 2017;](#b1)[Karras et al., 2018b;](#)[a;](#)[2020)](#b20) and Diffusion models [(Ho et al., 2020;](#b26)[Song & Ermon, 2019;](#b62)[Song et al., 2021)](#b63) have showcased remarkable performance in diverse tasks, from generating high-fidelity images [(Karras et al., 2018a;](#)[2020;](#b20)[2021)](#) to text-to-image translations [(Ramesh et al., 2021;](#b58)[2022;](#)[Rombach et al., 2022)](#b60). Consequently, these models find application in various fields, including but not limited to medical imaging [(Celard et al., 2023;](#b7)[Varoquaux & Cheplygina, 2022;](#b68)[Tiwary et al., 2024a)](#), remote sensing [(Ball et al., 2017;](#b2)[Adegun et al., 2023)](#b0), hyperspectral imagery [(Jia et al., 2021;](#b29)[Wang et al., 2023)](#b71), and many others [(Choudhary et al., 2022;](#b10)[Yang & Xu, 2021;](#b82)[Liu et al., 2021;](#b42)[Tiwary et al., 2024b)](#). However, the extensive incorporation of data with possible undesired features or inherent biases cause these models to generate violent, racial, or explicit content which poses significant concerns [(Tommasi et al., 2017)](#b67). Thus, these models are subject to regulatory measures [(Voigt & dem Bussche, 2017;](#b69)[Goldman, 2020)](#b20). Identifying and eliminating these undesired features from the model's knowledge representation poses a challenging task. The framework of Machine Unlearning [(Xu et al., 2020;](#b81)[Nguyen et al., 2022b)](#) tries to solve this problem by removing specific training data points containing undesired feature from the pre-trained model. Specifically, machine unlearning refers to the task of forgetting the learned information [(Sekhari et al., 2021;](#b61)[Ma et al., 2022;](#b44)[Ye et al., 2022;](#b83)[Cao & Yang, 2015;](#b5)[Golatkar et al., 2021;](#b19)[2020a;](#)[Ginart et al., 2019;](#b15)[Golatkar et al., 2020b)](#), or erasing the influence of specific data subset of the training dataset from a trained model in response to a user request [(Wu et al., 2020a;](#)[Guo et al., 2020;](#b23)[Graves et al., 2021;](#b22)[Wu et al., 2022;](#b76)[2020b;](#)[Chourasia & Shah, 2023)](#b11).

The task of unlearning can be challenging because we aim to 'unlearn' a specific undesired feature without negatively impacting the previously acquired knowledge. In other words, unlearning could lead to Catastrophic Forgetting [(Ginart et al., 2019;](#b15)[Nguyen et al., 2022a;](#)[Golatkar et al., 2020b)](#) which would significantly deteriorate the performance of the model. Further, the level of difficulty faced in the process of unlearning may vary depending on the specific features of the data that one is required to unlearn. For example, unlearning a particular class (e.g. class of digit '9' in MNIST) could be relatively easier than unlearning a more subtle feature (e.g. beard feature in CelebA). This is because the classes in MNIST are quite distinct and don't necessarily share correlated features. Whereas, in the CelebA [(Liu et al., 2015)](#b43) dataset, the feature of having a beard is closely linked to the concept of gender. So, unlearning this subtle feature while retaining other correlated features such as gender, poses an increasingly difficult challenge. It is important to mention that re-training the model from scratch without the undesired input data is not feasible in this setting due unavailability of the training dataset.

## Motivation and Contribution

In this work, we try to solve the problem of unlearning undesired feature in pre-trained generative adversarial networks (GANs) without having access to the training data used for pre-training the GAN. We operate under the feedback-based unlearning framework, where we start with a pre-trained GAN. A user is given a set of generated samples from this GAN. The user chooses a subset of generated samples and identifies them as 'undesirable' (negative samples). The feedback-based approach is similar to RLHF in LLMs or human-inloop settings in general [(Ziegler et al., 2019;](#b84)[Christiano et al., 2017;](#b12)[Lambert et al., 2022)](#b37). The objective of the process of unlearning is to prevent the generation of undesirable characteristics, as identified by the user. We propose to unlearn the undesired features by following a two-step approach. In the first step, we adapt the pre-trained generator to the undesired features by using the samples marked as undesired by the user (negative samples). This ensures that the 'adapted' generator exclusively generates samples that possess the undesired features. In the next step, we unlearn the undesired features from the original GAN by using the samples that weren't marked as undesired by the user (positive samples). While unlearning, we add a repulsion loss that encourages the parameters of the generator to stay away from the parameters of the adapted generator (obtained from first step) while also making sure that the quality of generated samples does not deteriorate. We provide theoretical justification for the proposed method by using a bayesian framework. Particularly, we show that the proposed method leads to contrastive-divergence kind of objective desired for unlearning. We call the proposed two-stage process 'Adapt-then-Unlearn'. An overview of the proposed method is shown in figure [1 (a)](#fig_0).

Our approach hinges in realizing interpretable and meaningful directions within the parameter space of a pre-trained GAN generator, as discussed in [(Cherepkov et al., 2021)](#b8). In particular, the first stage of the proposed method leads to adapted parameters that exclusively generate negative samples. While the parameters of the original pre-trained generator generate both positive as well as negative samples. Hence, the difference between the parameters of adapted generator and the paramters of original generator can be interpreted as the direction in parameter space that leads to a decrease in the generation of negative samples. Given this, it is sensible to move away from the original parameters in this direction to further reduce the generation of negative samples. This observation is shown in figure 1 (b). However, it's worth noting that such extrapolation doesn't ensure the preservation of other image features' quality. In fact, deviations too far from the original parameters may hamper the smoothness of the latent space, potentially leading to a deterioration in the overall generation quality (see last columns of figure 1 (b)). Inspired by this observation, during unlearning stage, we propose to train the generator using adversarial loss while encouraging the generator parameters to be away from the parameters of the adapted generator by employing a repulsion regularization.

$G θ G G θ P G θ N G θ G Pre-$We provide a visual illustration of the proposed method on Mixture of Gaussian (MoG) dataset with eight centers in figure [1 (c](#fig_0)). The first column shows the original training dataset and the samples generated by the pre-trained GAN. The second column shows the negative samples provided during feedback and the samples generated by the adapted generator. We can see that the adapted generator exclusively generates samples from the negative modes of MoG. Lastly, in the third column, we see the positive samples and the samples generated after unlearning the negative modes. We clearly observe that after unlearning (via the proposed method), the generator unlearns the negative modes and generates samples from the rest of the modes. This gives a proof-of-concept for the proposed method.

We summarize our contribution as follows:

• We introduce a two-stage approach for machine unlearning in GANs. In the first stage, our method adapts the pre-trained GAN to the negative samples. In the second stage, we train the GAN using a repulsion loss, ensuring that the generator's parameters diverge from those of the adapted GAN in stage 1.

• By design, our method can operate in practical few-shot settings where the user provides a very small amount of negative samples.

• We provide theoretical justification for the proposed method by showing that the proposed regularization leads to contrastive-divergence kind of objectives appropriate for unlearning.

• The proposed method is thoroughly tested on multiple datasets, considering various types of unlearning scenarios such as class-level unlearning and feature-level unlearning. Throughout these tests, we empirically observe that the quality of the generated samples is not compromised.

## Related Work

## Machine Unlearning

Unlearning can be naively done by removing the unwanted data subset from the training dataset and then retraining the model from scratch. However, retraining is computationally costly and becomes impossible if the unlearning request comes recursively for single data points. The task of recursively 'unlearning' i.e. removing information of a single data point in an online manner (also known as decremental learning) for the SVM algorithm was introduced in [(Cauwenberghs & Poggio, 2000)](#b6). However, when multiple data points are added or removed, these algorithms become slow because they need to be applied to each data point individually. To address this, [(Karasuyama & Takeuchi, 2009)](#b30) introduced a newer type of SVM training algorithm that can efficiently update an SVM model when multiple data points are added or removed simultaneously. Later, inspired by the problem of protecting user privacy [(Cao & Yang, 2015)](#b5) developed efficient ways to delete data from certain statistical query algorithms and coined the term "machine unlearning". The works of [(Ginart et al., 2019)](#b15) extended the idea of unlearning to more complicated algorithms such k-means clustering and also proposed the first definition of effective data deletion that can be applied to randomized algorithms, in terms of statistical indistinguishability. Depending upon this statistical indistinguishability criteria machine unlearning processes are widely classified into exact unlearning [(Ginart et al., 2019;](#b15)[Brophy & Lowd, 2021)](#b4) and approximate unlearning methods [(Neel et al., 2021;](#b50)[Nguyen et al., 2020)](#b52). The goal of exact unlearning is to exactly match the parameter distributions of the unlearned model and the retrained model where as, in approximate unlearning, the distributions of the unlearned and retrained model's parameters are close to some small multiplicative and additive terms [(Neel et al., 2021)](#b50). To extend the idea of unlearning or efficient data deletion for non-convex models such as deep neural networks [(Golatkar et al., 2020b)](#) proposed a scrubbing mechanism for approximate unlearning in deep neural networks. A more efficient method of unlearning in deep networks is proposed by [(Goel et al., 2022)](#b16) where the initial layers of deep networks are frozen while the last few layers are finetuned on the filtered dataset. Further to achieve the goal of exact unlearning [(Jia et al., 2023)](#b28) exploit the model sparsification technique via weight pruning.

Even though all of these methods achieve unlearning in supervised deep networks, the generalization of these methods for state-of-the-art high-fidelity GANs is unexplored.

Few methods like cascaded unlearning [(Sun et al., 2023)](#b64) and data redaction [(Kong & Chaudhuri, 2023)](#b35) try to prevent generation of undesired features in GANs, however, their methods operate primarily on very primitive DC-GAN as opposed to high-fidelity GANs like StyleGAN which is the focus of this work. While [Sun et al. (2023)](#b64) also show result on StyleGAN, there are several significant differences compared to the proposed method. First, there is a fundamental difference in the unlearning setting between [Sun et al. (2023)](#b64) and our method. To reduce the generation of undesired samples, [Sun et al. (2023)](#b64) proposes to forget undesired samples from the training dataset. Specifically, they assume access to samples from the training dataset. This is somewhat restrictive since users typically don't have access to the training data [(Chundawat1 et al., 2023;](#b13)[Graves et al., 2021)](#b22). Further, in terms of methodology, [Sun et al. (2023)](#b64) propose to patch the latent space of the GAN with representative samples. They suggest various strategies for generating these representative samples, such as using 'average samples' or 'other class samples' (cf. Section 4.3 of their paper). However, imposing such constraints on the latent space may lead to suboptimal latent-space semantics, potentially harming the quality of generated images. To address this, we avoid manipulating the latent space directly. Instead, we focus on parameter-space semantics, where we identify generator parameters that produce undesired samples (Stage-1: Negative Adaptation), and then retrain the GAN to avoid these parameters (Stage-2: Unlearning Phase). Additionally, since the latent space naturally adjusts based on changes in the parameter space (as shown in figure 1 (b)), we find it sufficient to focus on parameter-space semantics alone, as it automatically handles latent-space semantics as well. To the best of our knowledge, these insights into parameter-space semantics have not been explored in the context of unlearning, making our approach novel.

## Few-Shot Generative Domain Adaptation

The area of few-shot generative domain adaptation deals with the problem where a pre-trained generative model is adapted to a target domain using very few samples. A general strategy to do this is to fine-tune the model on target data using appropriate regularizers. Eg. [Wang et al. (2018)](#b72) observed that using a single pre-trained GAN for fine-tuning is good enough for adaptation. However, due to the limited amount of target data, this could lead to mode collapse, hence [Noguchi & Harada (2019)](#b55) proposed to fine-tune only the batch statistics of the model. Although, such a strategy can be very restrictive in practice. To overcome this issue, [Wang et al. (2020)](#b73) proposed to append a 'miner' network before the generator. They propose a two-stage framework, where the miner network is first trained to transform the input latent space to capture the target domain distribution then the whole pipeline is re-trained using target data. While these fine-tuning based methods give equal weightage to all the parameters of the generator, [Li et al. (2020)](#b41) proposed to fine-tune the parameter using Elastic Weight Consolidation (EWC). Particularly, EWC is used to penalize large changes in important parameters. This importance is quantified using Fisher-information while adapting the pre-trained GAN. [Mo et al. (2020)](#b47) showed that fine-tuning a GAN by freezing the lower layers of discriminator is good enough in few-shot setting. Recently, a string of work [(Ojha et al., 2021;](#b56)[Xiao et al., 2022;](#b79)[Lee et al., 2021)](#b39) focuses on few-shot adaptation by preserving the cross-domain correspondence. Lastly, [Mondal et al. (2022)](#b48) suggested an inference-time optimization approach where a they prepend a latent-learner, and the latent-learner is optimized every time a new set of images are to be generated from target domain.

As mentioned earlier, our approach involves an adaptation stage, where we adapt the pre-trained GAN to the negative samples provided by the user. In practice, the amount of negative samples provided by the user is very less hence such an adaptation falls under the category of few-shot generative domain adaptation. Hence, we make use of EWC [(Li et al., 2020)](#b41) for this adaptation phase (cf. Section 3.2 for details).

3 Proposed Methodology

## Problem Formulation and Method Overview

Consider the generator G θ G of a pre-trained GAN with parameters θ G and an implicit generator distribution

$p G (y). The GAN is trained using a dataset D = {x i } |D| i=1$, where x i iid ∼ p data (x). Using the feedbackbased framework [(Moon et al., 2023)](#b49), we obtain a few negative and positive samples, marked by the user. Specifically, the user is provided with n samples S = {y i } n i=1 where y i are the generated samples from the pre-trained GAN, i.e., y i iid ∼ p G (y). The user identifies a subset of these samples S n = {y i } i∈sn , as negative samples or samples with undesired features, and the rest of the samples S p = {y i } i∈sp as positive samples or samples that don't possess the undesired features. Here, s p and s n are index sets such that

$s p ∪ s n = {1, 2, . . . , n} and s p ∩ s n = ϕ. Formally, {y i } i∈sn iid ∼ p N (y) and {y i } i∈sp iid ∼ p G\N (y), where, p N (y)$is the implicit generator distribution on negative samples and p G\N (y) is the implicit generator distribution after removing support of negative samples. Given this, the goal of unlearning is to learn the parameters θ P such that the generator G θ P generates only positive samples. In other words, the parameters θ P should lead to unlearning of the undesired features.

Our approach involves two stages: In Stage 1, we adapt the pre-trained generator G θ G on the user-marked negative samples. This step gives us the parameters θ N such that G θ N generates only negative samples. In Stage 2, we unlearn the undesired features by training the original generator G θ G on positive samples using the usual adversarial loss while adding an additional regularization term that makes sure that the learned parameter is far from θ N . We call this regularization term repulsion loss as it repels the learned parameters from θ N .

## Stage-1: Negative Adaptation

The aim of the first stage of our method is to obtain parameter θ N such that the generator G θ N only generates negative samples (S n ). However, one thing to note here is that the number of negative samples marked by the user |S n | might be much less in number (of the order of tens or a few hundred). Directly adapting a pretrained GAN with a much smaller amount of samples could lead to catastrophic forgetting [(McClelland et al., 1995;](#b45)[McCloskey & Cohen, 1989)](#b46). To address this issue, we employ a few-shot GAN adaptation technique, namely, Elastic Weight Consolidation (EWC) [(Li et al., 2020)](#b41), mainly because of its simplicity and ease of implementation. EWC-based adaptation relies on the simple observation that the 'rate of change' of weights is different for different layers. Further, this 'rate of change' is observed to be inversely proportional to the Fisher information, F of the corresponding weights, which can used for penalizing changes in weights in different layers.

In our context, we want to adapt the pre-trained GAN on the negative samples. Hence, the optimal parameter θ N for the adapted GAN can be obtained by solving the following optimization problem:

$θ N , ϕ N = arg min θ max ϕ L adv + γL adapt (1)$where,

$L adv = E x∼p N (x) [log D ϕ (x)] + E z∼p Z (z) [log(1 -D ϕ (G θ (z)))](2)$$L adapt = λ i F i (θ i -θ G,i ), F = E - ∂ 2 ∂θ 2 G L θ G (S n ) (3)$Here, p Z (z) is the standard Gaussian prior, and L θ G (S n ) refers to the log-likelihood function for the samples S n generated by the GAN with parameters θ G . Specifically, L θ G (S n ) = log p θ G (S n ) which is the log-likelihood of the negative samples under the generator's distribution with parameter θ G . This term can be estimated by calculating the binary cross-entropy of the discriminator's output, D ϕ (S n ), as shown in [(Li et al., 2020)](#b41). In practice, we train multiple instances of the generator to obtain multiple θ N . Specifically, given the negative samples S n , we adapt the pre-trained GAN k times to obtain {θ j N } k j=1 .

## Stage-2: Unlearning

In the second stage of our method, the actual unlearning of undesired features takes place. In particular, this stage is motivated by the observation that there exist meaningful directions in the parameter space of the generator, shown in figure 1 (b). However, such extrapolation-based schemes could lead to degradation in the quality of generated images.

Nevertheless, the above observation indicates that traversing away from θ N helps us to erase or unlearn the undesired features. Therefore, we ask the following question: Can we transverse in the parameter space of a generator in such a way the parameters remain far from θ N while making sure that the quality of generated samples doesn't degrade? To solve this problem, we make use of the positive samples S p provided by the user. Particularly, we propose to train the given GAN on the positive samples while incorporating a repulsion loss component that 'repels' or keeps the learned parameters away from θ N . Mathematically, we obtain the parameters after unlearning θ P by solving the following optimization problem:

$θ P , ϕ P = arg min θ max ϕ L ′ adv + γL repulsion (4)$where,

$L ′ adv = E x∼p G\N (x) [log D ϕ (x)] + E z∼p Z (z) [log(1 -D ϕ (G θ (z)))] (5)$Here, L repulsion is the repulsion loss. The repulsion loss is chosen such that it encourages the learned parameters to be far from θ N obtained from Stage-1. Further, L ′ adv encourages the parameters to capture the desired distribution p G\N (x). Hence, the combination of these two terms makes sure that we transverse in the parameter space maintaining the quality of generated samples while unlearning the undesired features as well.

We emphasize that L ′ adv is different from L adv used in Stage-1. Specifically, the two adversarial terms serve different purposes: (i) L adv is utilized during the Negative Adaptation Phase (Stage-1) to adapt the original GAN to the negative samples (S n ), and (ii) L ′ adv is applied during the Unlearning Phase (Stage-2) to retrain the original GAN on positive samples (S p ), which are the samples not marked as undesired.

Note: Our method requires users to identify or annotate negative samples, i.e., those containing undesired features. This annotation serves to adapt the GAN to negative samples during the Negative Adaptation phase and subsequently retrain it on positive samples during the Unlearning phase. While human feedback is one approach for obtaining these samples, other methods, such as curating datasets of positive and negative samples, can also be employed. However, curating such datasets can be challenging, especially when the feature, concept, or class to be unlearned is subtle or complex and is not readily annotated in standard datasets. In such scenarios, users may need to create a custom dataset. By contrast, our current approach leverages human feedback to annotate readily available samples generated by the GAN, reducing the need for external dataset creation. Nonetheless, if a pre-curated dataset of positive and negative samples is available, our method can be easily adapted to use it. The GAN can be trained on this dataset to obtain the negative parameters θ N , which can then be utilized in the Unlearning phase. We demonstrate few results using such approach in Appendix.

## Algorithm: Adapt-then-Unlearn

## Stage-1: Negative Adaptation

Stage-2: Unlearning Required: Pre-trained parameters (θ G , ϕ D ), Negative samples (S n ), Number of adapted models (k). Initialize: j ← 0

$Required: Pre-trained parameters (θ G , ϕ D ), Posi- tive samples (S p ), Adapted models (θ N = {θ j N } k j=1 ). Initialize: θ P ← θ G , ϕ P ← ϕ D 1: while j ≤ k do 2: θ ← θ G , ϕ ← ϕ D 3: repeat 4: Sample x ∼ S n and z ∼ N (0, I) 5: L adv ← log D ϕ (x) + log (1 -D ϕ (G θ (z))) 6: L adapt ← λ i F i (θ i -θ G,i ) 7: θ ← θ -η∇ θ (L adv + L adapt ) 8: until convergence 9: θ j N ← θ 10: j ← j + 1 11: end while 1: repeat 2:$Sample x ∼ S p and z ∼ N (0, I)

$3: L ′ adv ← log D ϕ (x) + log (1 -D ϕ (G θ (z))) 4:$Choose L repulsion from Eq. 6

$5: θ ← θ -η∇ θ (L ′ adv + L repulsion ) 6: until convergence$
## Choice of Repulsion Loss

As mentioned above, the repulsion loss should encourage the learned parameter to traverse away from θ N obtained from the negative adaptation stage. Here, we note that the repulsion term operates in parameterspace of the generator. There is a lineage of research work in Bayesian learning called Deep Ensembles, where multiple MAP estimates of a network are used to approximate full-data posterior [(Levin et al., 1990;](#b40)[Hansen & Salamon, 1990;](#b24)[Breiman, 1996;](#b3)[Lakshminarayanan et al., 2017;](#b36)[Ovadia et al., 2019;](#b57)[Wilson & Izmailov, 2020;](#b75)[D'Angelo & Fortuin, 2021)](#b14). However, if the members of an ensemble are not diverse enough, then the posterior approximation might not capture the multi-modal nature of full-data posterior. As a consequence, there are several methods proposed to increase the diversity of the members of the ensemble [(Huang et al., 2016;](#b27)[Von Oswald et al., 2020;](#b70)[D'Angelo & Fortuin, 2021;](#b14)[Wenzel et al., 2020;](#b74)[D'Angelo & Fortuin, 2021)](#b14).

Inspired by these developments, we make use of the technique proposed in D' [Angelo & Fortuin (2021)](#) where the members of an ensemble interact with each other through a repulsive force that encourages diversity in the ensemble. Particularly, we explore three choices for repulsion loss:

$L repulsion =      L IL2 repulsion = 1 ||θ-θ N || 2 2 (Inverse ℓ2 loss) L NL2 repulsion = -||θ -θ N || 2 2 (Negative ℓ2 loss) L EL2 repulsion = exp(-α||θ -θ N || 2 2 ) (Exponential negative ℓ2 loss) (6)$It can be seen that minimization of all of these choices will force θ to be away from θ N , consequently serving our purpose. In fact, in general, one can use any function of ∥θ -θ N ∥ 2 2 that has a global maxima at θ N as a choice for repulsion loss. In this work, we work with the above mentioned choices. An algorithmic overview of Stage-1 Negative Adaptation is presented in Algorithms 11 and Stage-2 Unlearning is presented in Algorithm 6

## Theoretical Discussion

In this section, we present theoretical insights into the proposed method. Inspired by the work in Nguyen et al. ( [2020](#)), we operate in Bayesian setting for these claims and make use of widely used Laplace approximation around relevant parameters. Specifically, we demonstrate that for an optimal discriminator, the proposed regularization term combined with the adversarial term results in a contrastive divergence-like objective (a difference of two divergence terms). This encourages the generator to capture the implicit distribution of the pre-trained generator without the support of negative samples while maximizing the divergence between the parameter distribution of the post-unlearning generator and that of the generator which produces negative samples (Theorem 1). This result is shown in . Further, we show the relation between the parameter space divergence and data space divergence (Claim 1).

For this, let Θ denote the parameter space of a generator network. Let θ G ∈ Θ be the parameter of a pre-trained generator with an implicit distribution p X G (x) over the data space[foot_0](#foot_0) . Further, consider two distributions p Θ N (θ) and p Θ U (θ) over Θ, where the latter is a learnable distribution and the former is such that for z ∼ p Z (z) the corresponding generated samples from manifested generator G θ (z) ∼ p X N (x). In other words, samples from p Θ N (θ) lead to the generation of negative samples. Theorem 1. Consider the distributions p Θ N (θ) and p Θ U (θ) to be Gaussian, i.e., p Θ N (θ) =

$1 |2πΣ| d/2 exp 1 2 (θ -θ N ) T Σ -1 (θ -θ N ) and p Θ U (θ) = 1 |2πΣ| d/2 exp 1 2 (θ -θ P ) T Σ -1 (θ -θ P )$, where Σ = I, θ N and θ P are the mean parameters and θ P is learnable. Then statements (1 -3) hold for the following optimization problem:

$min θ P max ϕ E x∼p G\N (x) [log D ϕ (x)] + E z∼p Z θ∼p U [log(1 -D ϕ (G θ (z)))] + L repulsion (7)$1. for L repulsion = L IL2 repulsion , solving Eq. 7 leads to p Θ U that minimizes

$D JSD (p X G\N || p X U ) + D KL (p Θ U || p Θ N ) -1$2. for L repulsion = L NL2 repulsion , solving Eq. 7 leads to p Θ U that minimizes

$D JSD (p X G\N || p X U ) -D KL (p Θ U || p Θ N )$3. for L repulsion = L EL2 repulsion , solving Eq. 7 leads to p Θ U that minimizes

$D JSD (p X G\N || p X U ) -D H (p Θ U || p Θ N )$where, D KL (• || •) and D H (• || •) denote KL divergence and Hellinger divergence, and G θ (z) ∼ p X U (•) denotes the implicit distribution of generator when z ∼ p Z and θ ∼ p Θ U .

The proof for the aforementioned result is relatively straightforward; for the sake of completeness, we include the proof in the Appendix. It is evident from the above result that the unlearning phase of the proposed method, assuming a Gaussian parameter distribution, achieves two objectives: (a) minimizing the Jensen-Shannon Divergence between the learned data distribution (p X U ) and the implicit generator distribution after removing the support of negative samples (p X G\N ), and (b) maximizing a suitable divergence measure between the parameter distribution during unlearning (p Θ U ) and the parameter distribution that leads to the generation of negative samples (p Θ N ). While (a) is relatively straightforward, (b) provides valuable insights into the effect of the regularization term, which is particularly interesting. The regularization term ensures that the current parameter distribution moves away from the one responsible for generating undesired features, effectively aligning with the primary goal of unlearning. Furthermore, this behavior has been shown to be crucial for unlearning in Bayesian settings, as demonstrated in [Nguyen et al. (2020)](#b52). Essentially, this aligns with the desired outcome of unlearning, ensuring that the model captures only the desired support of the distribution.

An interesting observation from the above result is that utilizing L NL2 repulsion or L EL2 repulsion results in an objective akin to contrastive divergence, i.e., it entails the difference between two divergences. However, these two divergence metrics operate on distributions in distinct spaces: the first divergence operates in the data space, while the second operates in parameter space. This prompts a natural question: how does the divergence in parameter space relate to the corresponding distribution in data space? We address this question in the following claim.

Claim 1. For any general f -divergence D f (• | •), and a given latent vector, the following inequality holds:

$D JSD (p X G\N || p X U ) -D f (p Θ U || p Θ N ) ≤ D JSD (p X G\N || p X U ) -D f (p X U || p X N )(8)$Above result relies on simple application of data-processing inequality. The proof is provided in Appendix.

Since, KL and Hellinger divergence are both instances of f -divergence, the above result holds for Statements 2 and 3 of Theorem 1. Hence, we see that the while using L NL2 repulsion or L EL2 repulsion , the corresponding data space objectives act as upper bounds to the parameter space objectives. With these insights, we end the theoretical discussion.

## Experiments and Results

## Datasets

An unlearning algorithm should ensure that the generator should not generate images containing the undesired (or unlearnt) feature. For our experiments, we consider two types of unlearning settings: (i) Class-level unlearning and (ii) Feature-level unlearning. The primary difference between the two type of unlearning lies in the nature of the associations. In feature-level unlearning, an image can exhibit multiple features simultaneously, whereas in class-level unlearning, an image from one class cannot belong to any other class. In other words, if each feature is treated as a class, feature-level unlearning allows an image to belong to multiple classes, while class-level unlearning restricts an image to a single class. For instance, a person with bangs can be either male or female, but a digit labeled as 'one' cannot simultaneously belong to any other class.

We use MNIST dataset [(LeCun et al., 1998)](#b38) and AFHQ dataset [(Choi et al., 2020)](#b9) for class-level unlearning. MNIST consists of 60, 000 28 × 28 dimensional black and white images of handwritten digits. For our experiments, we take three-digit classes: 1, 4, and 8 for unlearning. AFHQ consists of 15, 000 high-quality animal face images at 512 × 512 resolution with three categories: cat, dog and wildlife. We unlearn each class one at a time in our experiments. Similarly, we use CelebA-HQ dataset [(Liu et al., 2015)](#b43) for feature-level unlearning. CelebA-HQ contains 30, 000 RGB high-quality celebrity face images of dimension 256 × 256. Here, we unlearn the following subtle features: 

## Experimental Details

Training Details: We use one of the state-of-the-art and widely used high-fidelity StyleGAN2 [(Karras et al., 2020)](#b33) for demonstrating the performance of the proposed method on the tasks mentioned in previous section. The StyleGAN is trained on the entire MNIST, AFHQ and CelebA-HQ datasets to obtain the pre-trained GAN from which we desire to unlearn specific features. The training details of StyleGAN2 are given in Supplementary Section A.

## Unlearning Details:

In our experiments, we employ a pre-trained classifier as a proxy for human to obtain user feedback. Specifically, we pre-train the classifier to classify a given image as desired or undesired (depending upon the feature under consideration). We classify 1, 000 generated images from pre-trained GAN as positive and negative samples using the pre-trained classifier. The generated samples containing the undesired features are marked as negative samples and the rest of the images are marked as positive samples. These samples are then used in Stage-1 and Stage-2 of the proposed method for unlearning as described in Section 3. We evaluate our result using all the choices of repulsion loss as mentioned in Eq. 6. For reproducibility, we provide all the hyper-parameters and training details in Supplementary Section A. The original FID of the GAN after training is as follows-MNIST: 5.4, AFHQ: 8.1, CelebA-HQ: 5.3. We mention these in caption of each table wherever necessary.

## Baselines and Evaluation Metrics

Baselines: To the best of our knowledge, ours is one of the first works that addresses the problem of unlearning in high-fidelity generator models such as StyleGAN2. Hence, we evaluate and compare our method with all the candidates for repulsion loss presented in Eq. 6. Further, we also include the results with extrapolation in the parameter space as demonstrated in figure [1 (b](#fig_0)). We include recent unlearning baselines tailored for classification, easily adaptable to generative tasks. Specifically, we incorporate EU-k, CF-k, and ℓ1-sparse [(Goel et al., 2022;](#b16)[Jia et al., 2023)](#b28) for comparison, with detailed information in the Supplementary section A.4. Additionally, we assess our method against GAN adaptation to positive samples, utilizing recent generative few-shot adaptation methods like EWC, CDC, and RSSA [(Li et al., 2020;](#b41)[Ojha et al., 2021;](#b56)[Xiao et al., 2022)](#b79) as baselines. Apart from the above baselines, we also mention the results obtained from training a GAN from scratch on only desirable data present in the dataset. This model acts as the gold standard, however, due to unavailability of underlying dataset, this is not practical. Nonetheless, we mention it in our tables for completeness. We evaluate the performance of each method across three independent runs and report the result in the form of mean ± std. dev. × 100, where, (S n ) θ G and (S n ) θ P represent the number of negative samples generated by the original GAN and the GAN after unlearning respectively. We generate 15,000 random samples from both GANs and employ a pre-trained classifier (as detailed in Section 5.2) to identify the negative samples. PUL provides a quantitative measure of the extent of the unlearning algorithm in eliminating the undesired feature from the GAN.

## Original Samples Extrapolation

## Fréchet Inception Distance (FID):

While PUL quantifies the degree of unlearning, it does not assess the quality of samples generated by the GAN post-unlearning. Hence, we calculate the FID [(Heusel et al., 2017)](#b25) between the generated samples and the original dataset without the undesired samples.

## Retraining FID (Ret-FID):

To resemble the retrained GAN, we compute the FID between the outputs of the GAN after unlearning and the GAN trained from scratch on the dataset obtained after eliminating undesired features.

Please note that the original dataset is unavailable during the unlearning process. Consequently, the use of the original dataset is solely for evaluation purposes.

## Unlearning Results

We present our results and observations on MNIST, AFHQ and CelebA-HQ in Table [1](#tab_0), 2, 3 respectively. We observe that the choice of L EL2 repulsion as repulsion loss provides the highest PUL in most of the cases Table 3: PUL (↑), FID (↓), and Ret-FID (↓) after unlearning CelebA-HQ features. FID of pre-trained GAN: 5.3. Method Bangs Hat Bald Eyeglasses PUL FID Ret-FID PUL FID Ret-FID PUL FID Ret-FID PUL FID Ret-FID Retraining 84.47 ± 1.49 7.58 ± 0.06 N/A 98.65 ± 0.03 6.35 ± 0.10 N/A 72.13 ± 0.07 7.18 ± 0.08 N/A 58.57 ± 0.04 6.50 ± 0.77 N/A CF-k 18.60 ± 2.30 9.65 ± 0.15 7.70 ± 0.27 15.22 ± 0.05 9.58 ± 0.09 7.57 ± 0.05 48.17 ± 1.76 9.30 ± 0.12 7.37 ± 0.06 16.41 ± 0.73 9.49 ± 0.15 6.79 ± 0.06 EU-k 20.08 ± 1.35 9.03 ± 0.23 7.38 ± 0.06 17.96 ± 1.54 9.40 ± 0.21 7.16 ± 0.05 52.18 ± 0.53 9.03 ± 0.06 6.92 ± 0.05 16.41 ± 1.33 9.39 ± 0.04 6.85 ± 0.85 ℓ1-Sparse 75.78 ± 1.48 16.20 ± 0.24 13.77 ± 0.46 59.01 ± 1.67 8.61 ± 0.09 5.66 ± 0.09 67.26 ± 0.19 16.27 ± 0.73 14.14 ± 0.59 82.30 ± 1.96 15.95 ± 0.02 12.36 ± 0.32 EWC 76.24 ± 2.14 9.64 ± 0.05 6.69 ± 0.03 74.14 ± 0.78 9.06 ± 0.22 6.44 ± 0.08 55.70 ± 2.57 9.32 ± 0.07 6.77 ± 0.04 46.51 ± 0.57 9.19 ± 0.04 6.56 ± 0.01 CDC 83.76 ± 0.12 34.66 ± 1.94 24.67 ± 0.02 85.23 ± 0.77 23.48 ± 0.12 17.46 ± 0.22 90.10 ± 0.37 26.96 ± 0.02 17.99 ± 0.09 91.65 ± 0.31 36.66 ± 0.02 30.08 ± 0.27 RSSA 32.07 ± 0.58 17.79 ± 0.02 15.99 ± 0.08 22.70 ± 1.47 18.83 ± 0.08 14.87 ± 0.16 31.70 ± 1.89 20.35 ± 0.34 18.63 ± 0.01 26.21 ± 0.17 22.24 ± 0.16 18.09 ± 0.02 Extrapolation 89.54 ± 0.09 11.54 ± 0.07 11.02 ± 0.06 94.35 ± 0.12 12.18 ± 0.04 10.12 ± 0.07 94.44 ± 0.34 23.44 ± 0.02 26.40 ± 0.30 92.80 ± 0.14 23.70 ± 0.07 19.10 ± 0.10 L NL2 repulsion (Ours) 90.41 ± 0.19 11.92 ± 0.46 8.69 ± 0.05 93.99 ± 1.70 9.60 ± 0.25 6.44 ± 0.11 97.13 ± 1.42 14.70 ± 0.55 9.03 ± 0.13 83.76 ± 3.21 12.81 ± 0.88 7.93 ± 0.99 L IL2 repulsion (Ours) 84.05 ± 1.03 13.09 ± 0.10 9.07 ± 0.18 94.00 ± 0.75 11.31 ± 0.06 7.25 ± 0.13 83.51 ± 2.18 12.94 ± 0.89 9.87 ± 0.04 75.23 ± 6.25 13.12 ± 0.78 6.11 ± 0.24 L EL2 repulsion (Ours) 90.45 ± 1.02 11.16 ± 0.08 7.94 ± 0.32 94.40 ± 2.19 9.45 ± 0.96 6.31 ± 0.64 93.97 ± 2.65 11.07 ± 0.86 7.83 ± 0.05 93.63 ± 0.42 9.66 ± 0.58 9.84 ± 0.23

for both the datasets. Further, it also provides the best FID and Ret-FID as compared to other choices of repulsion loss. L NL2 repulsion is stands out to be the second best in these metrics for most of the cases. Further, we observe that across all datasets, the classification unlearning baselines perform very poorly on all metrics for unlearning in GANs. This tells us that methods proposed for unlearning in classification are not suited for unlearning in generative tasks. And lastly, we find that few-shot adaptation baselines, give relatively poor results when compared to the proposed method. This observation indicates that it is not enough to just adapt the GAN on the positive samples for unlearning, one needs to go further and use additional regularization to unlearn the undesired features.

For MNIST, we observe in Table [1](#tab_0) that the proposed method with L EL2 repulsion as repulsion loss consistently provides a PUL of above 95% while giving the best FID and Ret-FID compared to other methods. We also observe that Extrapolation in parameter space leads to significant PUL albeit the FID and Ret-FID are considerably worse compared to proposed method under different repulsion loss. This shows that the proposed method decently solves the task of unlearning at class-level.

We make similar observations for high-resolution AFHQ dataset as well. One can see that the proposed method provides highest PUL in all the cases, while maintaining the FID as well as Ret-FID. We observe highest PUL while unlearning the 'Cat' class while lowest PUL is observed in 'Dog' class.

Lastly, for feature-level unlearning results on CelebA-HQ, it can be seen that the proposed method with L EL2 repulsion as repulsion loss consistently provides a PUL of above 90%, illustrating significant unlearning of undesired features. Further, the FID and Ret-FID using L EL2 repulsion stand out to be the best among all the methods with significant PUL.

We also observe some drop in FID across all dataset after unlearning. E.g., the FID of the samples generated by the unlearnt GAN (on Hats) using L EL2 repulsion drops by about 4.15 points while it drops by 4.3 and 6.01 points while using L NL2 repulsion and L IL2 repulsion as compared to the pre-trained GAN. On the other hand, Extrapolation in parameter space leads to a drop of 6.88 poinits in FID. This further validates the need of repulsion regularizer to maintain the generation quality. This observation is consistent across all datasets and features. This supports our claim that extrapolation might unlearn the undesired feature, however, it deteriorates the quality of generated samples significantly.

Another interesting observation from the above results is that the classification unlearning baselines consistently provide lower PUL, albeit with slightly better FID and Ret-FID. This tells us that these baselines while capable of unlearning in classification tasks, fail to nudge the generator appropriately for desired unlearning task. Leading to a suboptimal generator which still generates undesired samples, without compromising on the quality of the generated samples.

The visual illustration of these methods for AFHQ and CelebA-HQ are shown in figure [2](#fig_1) and figure [3](#fig_2) respectively. Here, we observe that the proposed method effectively unlearns the undesired feature. Moreover, it can be seen that the unlearning through extrapolation leads to the unlearning of correlated features as well. E.g. Bangs are correlated with female attributes. It can be seen that the unlearning of Bangs through extrapolation also leads to the unlearning of female feature which is not desired. However, unlearning through the proposed method unlearns Bangs only, while keeping the other features as it is. Similar observations could be made for AFHQ, where extrapolation leads to some minor artifacts in generates samples, whereas proposed method generates plausible images without any artifacts. Similar visual results for MNIST is provided in Supplementary Section B. We also provide visualization of random samples generated from original GAN and the GAN after unlearning in Supplementary Section F to give a qualitative idea of the generation quality.

Another aspect of unlearning that we explore in our work is the effect of unlearning on other features. Particularly, unlearning an undesired feature should not disturb the other features. For this we generate samples from pre-trained and post-unlearning GANs. Then, we calculate the occurrence of specific features within the two GANs and report the percentage change in these numbers. These results could be found in Section E of Supplementary. As discussed in Section 3, the approach of first adapting the model to negative samples followed by applying the repulsion loss during the Unlearning phase can also be extended to scenarios where curated datasets of positive and negative samples are available. We demonstrate this in Section D.

## Comparison with DC-GAN Baselines

As previously mentioned, our proposed method operates with high-fidelity GANs. Nonetheless, in addition to the tailored baselines, a few previous methods aim to unlearn undesired features in more primitive GANs, such as DC-GANs, on low-resolution images. Notably, the methods proposed in [Sun et al. (2023)](#b64) and [Kong & Chaudhuri (2023)](#b35) are highly relevant to our work. However, they primarily operate on DC-GAN. To ensure a fair comparison, we implement our method on DC-GAN and evaluate it against these baselines using the MNIST dataset (with L EL2 repulsion ). Specifically, we compare our approach to the cascaded unlearning algorithm (CUA) from [Sun et al. (2023)](#b64) and the data redaction method using validity data (DRed) from [Kong & Chaudhuri (2023)](#b35). Our findings are summarized in

Table 4. The results indicate that our method Published in Transactions on Machine Learning Research (02/2025)  outperforms both baselines across all metrics in all scenarios. All methods performed well on PUL, with our method achieving the best PUL, followed by DRed and then CUA in most cases. A similar trend is observed in the FID scores. Although DRed is a close competitor to our proposed method, our approach yields a significantly better Ret-FID than DRed, suggesting that the post-unlearning GAN using our method is closer to the gold standard compared to DRed.

## Ablation Study

Lastly, we present the ablation study to observe the effect of repulsion loss. In particular, we see if adapting the pre-trained GAN only on the positive samples leads to desired levels of unlearning. Our observations on CelebA-HQ for Bangs and Hats are presented in Table [5](#tab_5). Here, we use L EL2 repulsion as repulsion loss. It can be seen that only using adversarial loss doesn't lead to significant unlearning of undesired feature. E.g. using repulsion loss provides and increase of about 10.56% and 9.72% in PUL. The FID increases by minor 0.66 point on Bangs while it decreases by 0.21 points on Hats. Hence, we conclude that repulsion loss is indeed crucial for unlearning.

## Conclusion

We propose a novel unlearning method designed for high-fidelity GANs. Our approach is distinguished via its unique ability to operate in zero-shot scenario, entirely independent of the original data on which GAN is trained. We operate under feedback-based framework in two stages. The initial stage adapts the pre-trained GAN on the negative samples whereas the later stage unlearns the undesired feature by adapting on positive samples along with a repulsion regularizer. A notable advantage of our approach is its capability to conduct the unlearning process without significantly impacting other desirable features. We firmly believe that our work represents a substantial advancement in the field of unlearning within deep generative models. This progress holds particular relevance in addressing critical societal concerns, particularly those related to the generation of biased, racial, or harmful content by these models.

## Limitation and Future Work:

We note that our study does not address aspects like 'toxicity' due to the absence of annotated datasets with explicit characteristics. Despite this limitation, it's crucial to emphasize that our explored features are subtle and interconnected. For example, subtle details like hairstyles (e.g., bangs) are strongly linked to gender, and characteristics like baldness correlate with physical appearance, also associated with gender. Additionally, attributes such as hats and eyeglasses are tied to accessories. Though not the primary focus for unlearning, these features hold potential utility for such purposes.

In future, we aspire to provide rigorous theoretical guarantees to such methods making them more dependable and safe for deployment. Moreover, the approach proposed in this work is inherently generic and can be applied to any model exhibiting parameter-space semantics. This opens up the possibility of extending such techniques to more powerful generative models, such as Diffusion models and Flow-based models. However, applying this method to iterative models like diffusion models would require a thorough investigation of their parameter-space semantics. Additionally, developing an effective strategy for negative adaptation in these models remains an open research challenge, unlike GANs, where few-shot generative adaptation is relatively well-studied. Nonetheless, this represents a promising avenue for modern generative models, and we leave this exploration for future research.

## Broader Impact Statement

Machine unlearning has emerged as a crucial tool for addressing privacy concerns and mitigating harmful biases in AI systems. Our method advances this field by enabling selective feature removal from pre-trained GANs without requiring access to the original training data. This capability is particularly valuable for correcting deployed models that may generate problematic or biased content, making AI systems more ethical and socially responsible. While our approach currently focuses on GANs, its success demonstrates the potential for developing similar techniques for other generative models, contributing to the broader goal of creating AI systems that can be refined and corrected post-deployment to better serve society's needs.

EU-k and CF-k [(Goel et al., 2022)](#b16) propose to train just the last k layers of the model from scratch (in EU-k) or from pre-trained initialization (in CF-k) for unlearning on the positive samples. We employ the same strategy to GANs directly with k = 10 layers. Further, ℓ1-sparse [(Jia et al., 2023)](#b28) proposes to use sparse weights for fine-tuning to unlearn the undesired features. To this end, they propose to use ℓ-1 regularization while fine-tuning. Hence, for our case, we fine-tune the model on positive samples by adding an ℓ-1 regularization on weights of the network.

For the few-shot adaptation baselines, we directly employ the provided open-source codebase of CDC[foot_4](#foot_4) and[foot_5](#foot_5) for adaptation on positive samples to obtain results.

## B MNIST Qualitative Results

We present visual illustration of images generated after unlearning using various methods in figure [4](#fig_3). Here, we unlearn class of digits 1, 4, and 8. We observe that all the proposed methods effectively unlearn the undesired classes. Moreover, it can be seen that although extrapolation leads to unlearning, it does so at the expense of the quality of the generated images. In contrast, the quality of the generated images after unlearning using the proposed method leads to unlearning with plausible image quality. We refer the reader the reader to main text for quantitative evaluation.

## Class 1 Class 4

Class 8 

## Original Samples Extrapolation

## C Proofs

Proof of Theorem 1. The said objective function is given by-

$min θ P max ϕ E x∼p X G\N [log D ϕ (x)] + E z∼p Z θ∼p Θ U [log(1 -D ϕ (G θ (z)))] + L repulsion ≡ min θ P max ϕ E x∼p X G\N [log D ϕ (x)] + E x∼p X U [log(1 -D ϕ (x))] + L repulsion (9)$Since ϕ depends only on the first two terms of Eq. 9, the optimal discriminator as obtained in Goodfellow et al.

(

$) is given as D ϕ * = p X G\N p X G\N +p X U .2014$Substituting this in Eq. 9 and using standard results from [Goodfellow et al. (2014)](#b21) gives -

$min θ P D JSD (p X G\N ||p X U ) + L repulsion (10) L repulsion = L IL2 repulsion : Since p Θ N (θ) = 1 |2πΣ| d/2 exp 1 2 (θ -θ N ) T Σ -1 (θ -θ N ) and p Θ U (θ) = 1 |2πΣ| d/2 exp 1 2 (θ -θ P ) T Σ -1 (θ -θ P ) are both Gaussian distributions, then D KL (p Θ U ||p Θ N ) = ∥θ P -θ N ∥ 2 =⇒ [D KL (p Θ U ||p Θ N )] -1 = 1 ∥θ P -θ N ∥ 2 = L IL2 repulsion .$Substituting in the above we get -

$min θ P D JSD (p X G\N ||p X U ) + [D KL (p Θ U ||p Θ N )] -1 (11) L repulsion = L NL2 repulsion : Similar to above argument, D KL (p Θ U ||p Θ N ) = ∥θ P -θ N ∥ 2 =⇒ -D KL (p Θ U ||p Θ N ) = -∥θ P -θ N ∥ 2 = L NL2$repulsion . Substituting in the above we get -

$min θ P D JSD (p X G\N ||p X U ) -D KL (p Θ U ||p Θ N )(12)$L repulsion = L EL2 repulsion : Again, since p Θ N and p Θ U follow Gaussian distribution, the Hellinger divergence between two Gaussian distribution is a shifted negative Manhabolis distance between the means of the two distributions, i.e., D

$H (p Θ U ||p Θ N ) = 1 -exp (-∥θ P -θ N ∥ 2 ) =⇒ 1 -D H (p Θ U ||p Θ N ) = exp (-∥θ P -θ N ∥ 2 ) = L EL2$repulsion . Substituting in above we get -

$min θ P D JSD (p X G\N ||p X U ) -D H (p Θ U ||p Θ N ) + 1 (13) ≡ min θ P D JSD (p X G\N ||p X U ) -D H (p Θ U ||p Θ N )(14)$This completes the proof of all the three statements.

Proof of Claim 1. For a given latent vector, θ → G θ (z) is a map from parameter space to generated sample in the data space. Hence, we can use data-processing inequality to obtain -

$D f (p X U ||p X N ) ≤ D f (p Θ U ||p Θ N ) (15) =⇒ D JSD (p X G\N ||p X U ) -D f (p Θ U ||p Θ N ) ≤ D JSD (p X G\N ||p X U ) -D f (p X U ||p X N )(16)$This completes the proof.

## D Results with Curated Datasets

As explained in the main paper, our method requires users to identify or annotate negative samples under feedback-based framework. This annotation is used to adapt the GAN to negative samples during the Negative Adaptation phase and subsequently retrain it on positive samples during the Unlearning phase.

While human feedback is one approach to obtain these samples, curated datasets of positive and negative samples can also serve this purpose.

Curating datasets, however, can be challenging, particularly when the feature, concept, or class to be unlearned is subtle or complex and not readily available in standard datasets. In such cases, users may need to create a custom dataset. By contrast, our human-feedback approach involves annotating samples generated by the GAN, reducing the need for external dataset creation. Nevertheless, if a curated dataset of positive and negative samples is available, our method can be seamlessly adapted to utilize it.

To demonstrate this, we conduct experiments on all three datasets (MNIST, AFHQ, and CelebA-HQ) using curated dataset samples in both the Negative Adaptation and Unlearning phases. Specifically, for the Negative Adaptation phase, we use samples from the original dataset that were pre-annotated with the undesired feature or label. For instance, in the CelebA-HQ dataset, we selected all samples with a positive label for the undesired feature (e.g., Bangs). The GAN is then adapted to these samples using the training objective described in Section 3.2 (Eq. 1 of the main paper) to obtain the parameters θ N . In the Unlearning phase, the GAN was retrained on the remaining dataset samples using the objective described in Section 3.3 (Eq. 4).

We performed these experiments on MNIST (unlearning 'Class 8'), AFHQ (unlearning 'Cat'), and CelebA-HQ (unlearning 'Bangs'). The results of these experiments are summarized in Table [6](#tab_6), comparing the performance of using curated dataset samples against GAN-generated samples. As shown in Table [6](#tab_6), using curated dataset samples during the Negative Adaptation phase achieves performance comparable to that of GAN-generated samples. This demonstrates the flexibility of our method, allowing users to choose the most convenient approach depending on the availability of datasets. These results will be included in the supplementary material to provide further clarity on this point.

## E Effect on other features after Unlearning

As previously discussed, the unlearning procedure aims to exclusively erase undesired features without impacting other features. Consequently, it becomes imperative to assess whether the unlearning process exerts any influence on other features. To address this concern, we introduce plots illustrating the percentage change in the presence of other features.

Specifically, we undertake the generation of 15, 000 random samples from both the pre-trained GAN and the GAN after the unlearning procedure. Subsequently, employing the pre-trained classifiers (for comprehensive details, please refer to the Supplementary), we calculate the occurrence of specific features within the two GANs. We report the percentage change in these numbers to demonstrate how has the unlearning process affected this feature. Hence, a lower percentage change is better as it means that the other features are not affected after unlearning. This experiment is repeated across multiple features, and our findings after unlearning Bangs in CelebA-HQ are depicted in figure [5](#). We observe that for all the features, unlearning via extrapolation leads to significant changes in other features. Whereas, unlearning via the proposed method leads to minor changes in the features. For instance, since Bangs are highly correlated with gender, we observe that unlearning Bangs via extrapolation leads to an increase in Males. However, unlearning via the proposed method leads to minor changes in the number of samples with Male features. This shows that the proposed method is effective in erasing the undesired feature while preserving other features. It can be observed that in majority of the cases, extrapolation leads to significant change in the features, indicating that unlearning via extrapolation leads to significant change in other features as well. This also indicates that extrapolation leads to unlearning of several correlated features. On the other hand, we observe that unlearning using the proposed method with L EL2 repulsion gives the least change in most of the cases. This illustrates the efficacy of the proposed method in preserving features other than the unlearnt feature.

![Figure 1: (a) Block diagram of the proposed method: Stage-1: Negative Adaptation of the GAN to negative samples received from user feedback and Stage-2: Unlearning of the original GAN using the positive samples with a repulsion loss. (b) Illustrating linear interpolation and extrapolation in parameter space for unlearning undesired features. We observe that in the extrapolation region, undesired features are suppressed, but the quality of generated samples deteriorates. (c) An example of results obtained using our method on Mixture of Gaussian (MoG) dataset, where we unlearn two centers provided in negative samples.]()

![Figure 2: Results of Unlearning different classes on AFHQ dataset.]()

![Figure 3: Results of Unlearning different features on CelebA dataset.]()

![Figure 4: Results of Unlearning undesired feature (class) via different methods. The undesired class contains digits 1(top row), 4(second row), 8 (bottom row).]()

![PUL (↑), FID (↓), and Ret-FID (↓) after unlearning MNIST classes. FID of pre-trained GAN: 5.4.]()

![PUL (↑), FID (↓), and Ret-FID (↓) after unlearning AFHQ classes. FID of pre-trained GAN: 8.1.]()

![Comparison of the proposed method against DC-GAN baselines]()

![Effect on PUL (↑), FID (↓), and Ret-FID (↓) with and without repulsion loss.]()

![Comparison of results using curated dataset samples versus GAN-generated samples for negative adaptation. PUL (↑), FID (↓), and Ret-FID (↓).± 1.20 7.79 ± 0.86 10.7 ± 0.52 94.26 ± 1.08 11.88 ± 0.81 6.13 ± 1.66 90.40 ± 0.91 10.04 ± 0.49 6.05 ± 1.25 w/ GAN samples (using L EL2 repulsion ) 95.22 ± 0.34 8.80 ± 0.52 5.68 ± 0.10 95.76 ± 0.25 16.50 ± 0.12 8.17 ± 0.14 90.45 ± 1.02 11.16 ± 0.08 7.94 ± 0.32]()

For convenience, we use superscript X and Θ to denote a distribution in data space and parameter space respectively. We use the data space distribution from previous section as it is with a superscript X for this distinction.

https://github.com/rosinality/stylegan2-pytorch

https://github.com/csinva/gan-vae-pretrained-pytorch/tree/master/mnist_classifier

https://github.com/rgkannan676/Recognition-and-Classification-of-Facial-Attributes/

https://github.com/utkarshojha/few-shot-gan-adaptation

https://github.com/StevenShaw1999/RSSA

