- **Objective**: Prevent generation of undesired features in pre-trained GANs without access to training data.
  
- **Methodology**: Introduces 'Adapt-then-Unlearn' two-stage approach:
  - **Stage 1**: Adapt pre-trained GAN using user-provided negative samples (undesired features).
  - **Stage 2**: Train original GAN with positive samples and a repulsion regularizer to maintain quality while moving away from adapted parameters.

- **Repulsion Regularizer**: Encourages divergence of generator parameters from adapted model parameters to suppress undesired features without degrading quality.

- **Theoretical Justification**: 
  - Utilizes a Bayesian framework to show that the method leads to a contrastive-divergence objective suitable for unlearning.
  
- **Challenges Addressed**: 
  - Catastrophic Forgetting: Avoids significant performance degradation while unlearning specific features.
  - Distinction between unlearning classes (e.g., digit '9' in MNIST) vs. subtle features (e.g., beard in CelebA).

- **Empirical Validation**: 
  - Tested on MNIST, AFHQ, and CelebA-HQ datasets for class-level and feature-level unlearning.
  - Results demonstrate that quality of generated samples is preserved post-unlearning.

- **Practical Application**: 
  - Operates effectively in few-shot settings, requiring minimal negative samples from users.

- **Key Contributions**:
  - First method addressing unlearning in high-fidelity GANs (e.g., StyleGAN).
  - Provides a framework for feedback-based unlearning similar to RLHF in LLMs.

- **Visual Illustrations**: 
  - Figure 1(a): Overview of the proposed method.
  - Figure 1(b): Parameter space interpretation showing the difference between adapted and original generator parameters.
  - Figure 1(c): MoG dataset example illustrating the unlearning process.

- **References**: 
  - Key papers on GANs and unlearning methodologies (Goodfellow et al., 2014; Xu et al., 2020; Ginart et al., 2019).

- **Code Availability**: Implementation can be accessed at [GitHub Repository](https://github.com/atriguha/Adapt_Unlearn).