<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual Information Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anca</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual Information Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A739E1FFD672AF436245BF549B2B8B8E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How can we train an assistive human-machine interface (e.g., an electromyographybased limb prosthesis) to translate a user's raw command signals into the actions of a robot or computer when there is no prior mapping, we cannot ask the user for supervision in the form of action labels or reward feedback, and we do not have prior knowledge of the tasks the user is trying to accomplish? The key idea in this paper is that, regardless of the task, when an interface is more intuitive, the user's commands are less noisy. We formalize this idea as a completely unsupervised objective for optimizing interfaces: the mutual information between the user's command signals and the induced state transitions in the environment. To evaluate whether this mutual information score can distinguish between effective and ineffective interfaces, we conduct a large-scale observational study on 540K examples of users operating various keyboard and eye gaze interfaces for typing, controlling simulated robots, and playing video games. The results show that our mutual information scores are predictive of the ground-truth task completion metrics in a variety of domains, with an average Spearman's rank correlation of ρ = 0.43. In addition to offline evaluation of existing interfaces, we use our unsupervised objective to learn an interface from scratch: we randomly initialize the interface, have the user attempt to perform their desired tasks using the interface, measure the mutual information score, and update the interface to maximize mutual information through reinforcement learning. We evaluate our method through a small-scale user study with 12 participants who perform a 2D cursor control task using a perturbed mouse, and an experiment with one expert user playing the Lunar Lander game using hand gestures captured by a webcam. The results show that we can learn an interface from scratch, without any user supervision or prior knowledge of tasks, with less than 30 minutes of human-in-the-loop training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Imagine communicating with an intelligent extraterrestrial for the first time. They are trying to get us to do something (e.g., synthesize a novel molecule and build a space elevator out of it), but we do not know what that task is, and we cannot understand their language. They might speak, write, wave their limbs at us in a video, or send us messages through some other modality. They might even be watching how we act on their messages, and adapting the content of their messages to our behavior. How do we translate their messages into the actions they want us to take?</p><p>To make progress on this problem in the absence of (known) alien signals, we study the related problem of helping humans communicate their intent to machines via arbitrary command signals; for example, controlling a robotic arm through a brain-computer interface <ref type="bibr" target="#b0">[1]</ref>. When designing human-machine interfaces, we typically either (a) use human ingenuity to design an interface that is</p><p>Code, data, and videos available at <ref type="url" target="https://sites.google.com/view/coadaptation">https://sites.google.com/view/coadaptation</ref> 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2205.12381v2 [cs.LG] 15 Sep 2022 a s d f ? ? ? ?</p><p>Interface's action Environment state</p><p>User's command "banan" "banana" "a" "a" "s" "d" "f" "banan" "bananf" "f" "d""f" "a" "s" The user is trying to perform a task (e.g., land a quadrotor), which the interface π cannot directly observe. The user communicates with the interface π by observing the environment state st, then providing a command signal xt (e.g., the raw audio of a voice command). The interface π observes the state st and command xt, then takes an action at that causes the next state st+1. We assume that the user partially adapts their command xt to any given interface π, and that the extent of this user adaptation depends on the intuitiveness of the interface. We search for an interface π that maximizes the rate of information transfer from user to the environment (orange) and from the environment to the user (blue), which we formalize as the mutual information I(xt, (st, st+1)). (b) The more intuitive the interface (e.g., a QWERTY keyboard where the 'a', 's', 'd', and 'f' keys are arranged in that order), the less noisy the user's commands (e.g., keypresses). The less intuitive the interface (e.g., a randomly-permuted keyboard where "asdf" is scrambled to "dfas"), the noisier the user.</p><p>intuitive for a human to operate; (b) get direct supervision on how commands should map to actions, so we can train the interface through supervised learning; or (c) get reward feedback on the system's performance, so that we can improve the interface through reinforcement learning (see Sec. 4 for prior work). Unfortunately, none of this is possible with alien users: we do not know what is intuitive for them; we cannot ask them to perform specific tasks so that we can collect labeled data for supervised learning; we do not know what tasks they are trying to accomplish; and we cannot understand any feedback they might give on the quality of our actions. Existing approaches are also inconvenient for human users who would like to perform highly-personalized tasks using non-traditional interfaces, such as controlling a vehicle through a microphone [2], but do not want to be interrupted and asked for supervision or feedback. 1   Without any source of supervision or prior knowledge about the user's desired tasks, it seems impossible to design a good interface. Recent work proposes optimizing for the user's ability to control their environment <ref type="bibr" target="#b1">[3]</ref>. This is useful, but there tend to be many different interfaces that enable the user to influence the environment, and most of them are unintuitive to operate (e.g., compare the QWERTY keyboard to a keyboard with randomly-permuted keybindings). How do we capture what is intuitive, not only when the user is a human, but also when the user is an alien whose prior over 'natural' interfaces may not overlap with those of a human? The key idea (and assumption) in this paper: the more intuitive the interface, the less noisy the user's commands (Fig. <ref type="figure" target="#fig_0">1b</ref>). Thus, a good interface should not only maximize the user's ability to influence the environment, but also minimize the entropy of the user's commands. We show that this corresponds to maximizing the mutual information between the user's command and the induced state transition (Sec. 2.1).</p><p>Of course, we cannot optimize this mutual information objective without interacting with the user. What we thus propose is a co-adaptation process, wherein we start with a random interface, the user learns about it as they use it, we measure the mutual information, and update the interface to maximize the mutual information through reinforcement learning. As the interface improves, the user's ability to complete their desired tasks using the interface also increases. We call this method the mutual information-maximizing interface (MIMI).</p><p>1 "'The van is under voice command,' [Ng] explains. 'I removed the steering-wheel-and-pedal interface because I found verbal commands more convenient. This is why I will sometimes make unfamiliar sounds with my voice -I am controlling the vehicle's systems...Ng makes a yapping sound, and the van pulls out onto the frontage road...Ng laughs sharply, like distant ack-ack, and the van almost swerves off the road...Ng sings a little song. A robot arm unfolds itself from the ceiling of the van, crisply yanks the vial from her hand, swings it around, and holds it in front of a video camera set into the dashboard." <ref type="bibr">[2]</ref> Our primary contribution is an unsupervised mutual information objective for evaluating interfaces that can be used even when the ground-truth reward function for the user's desired task is unknown. To evaluate whether this unsupervised objective can be used to distinguish between effective and ineffective interfaces, we analyze offline datasets of users operating various interfaces in five different domains from prior work, and measure the correlation between the ground-truth rewards for each task and our unsupervised mutual information scores (Sec. 5.1). The results show that, in each domain, the interface with the largest mutual information score also has the largest ground-truth reward (Fig. <ref type="figure">5</ref> in the appendix). We also contribute the MIMI algorithm for training an interface to understand human commands from scratch, without prior knowledge of the user's desired tasks or explicit feedback from the user. We evaluate MIMI through an online user study with 12 participants who use a perturbed mouse to perform a 2D cursor control task (Sec. 5.2). The results show that, in under 30 minutes of human-in-the-loop training, MIMI learns an interface that is intuitive for users to operate and enables users to reach their goals more quickly than a random interface. We also showcase MIMI's ability to scale to more complex tasks and command modalities through a study with one expert user (the first author) who uses hand gestures to play the Lunar Lander game (Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">An Unsupervised Mutual Information Objective for Evaluating Interfaces</head><p>In our problem setting, users cannot directly act in the environment, and must rely on an interface to take actions for them (e.g., due to a motor impairment, or because they are remotely operating space robots, or because they are aliens communicating with humans over long distances). The interface does not have direct access to the user's desired task, but can observe command signals that the user provides to the interface (e.g., through a webcam or microphone). We formulate the command-toaction translation problem as a contextual Markov decision process (CMDP) <ref type="bibr" target="#b2">[4]</ref>. Each observation is decomposed into two variables: the environment state s t (e.g., the position and orientation of a robot arm) and the user's command signal x t (e.g., a webcam image of their eye gaze). The interface π(a t |s t , x t ) takes an action a t given the state s t and command x t . We do not assume access to the user's desired task (which represents the 'context' in the CMDP), the space of desired tasks, the ground-truth reward function R(s t , a t ), or the state transition dynamics. We approach this problem by defining a surrogate reward function that correlates positively with the ground-truth reward. We define this surrogate to be the information rate of the interface, which we formalize as the mutual information between the user's command x t and the induced state transition (s t , s t+1 ). Fig. <ref type="figure" target="#fig_0">1a</ref> outlines our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Characterizing the Information Rate of the Interface</head><p>Viewing our system through the lens of information theory <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6]</ref>, the user interacts with the environment through a noisy communication channel that mediates perception (blue in Fig. <ref type="figure" target="#fig_0">1</ref>) and control (orange in Fig. <ref type="figure" target="#fig_0">1</ref>). When the environment sends a message s t to the user, the user's internal decision-making process noisily converts the message s t into a command x t . When the user sends a message x t to the interface, the interface converts the message x t into an action a t ∼ π(a t |s t , x t ), and the environment converts the action a t into a next state s t+1 ∼ p(s t+1 |s t , a t ) following unknown dynamics. In this section, we derive the information rate of this channel, and show how maximizing it enables the user to perform their desired tasks.</p><p>The amount of information that is transmitted from the user to the environment can be characterized as the conditional mutual information I(x t , s t+1 |s t ) (orange in Fig. <ref type="figure" target="#fig_0">1</ref>). Maximizing this term enables the user to influence the next state s t+1 by modulating their command x t . However, there can be many different interfaces that achieve an equally-high value of I(x t , s t+1 |s t ). For example, consider typing on a QWERTY keyboard. Randomly permuting the keybindings would preserve the value of I(x t , s t+1 |s t ), since there would still be a one-to-one mapping between physical keys and typed characters, but the resulting layout would probably not be as intuitive to use as QWERTY. In order to maximize the intuitiveness of the interface, we need to consider the amount of information that gets transmitted in the other direction: from the environment to the user.</p><p>The amount of information that gets transmitted from the environment to the user can be characterized as the mutual information I(s t , x t ) (blue in Fig. <ref type="figure" target="#fig_0">1</ref>). The main assumption in our method is that maximizing I(s t , x t ), which can be rewritten as H(x t ) -H(x t |s t ), leads to an intuitive interface. The idea behind penalizing the conditional entropy H(x t |s t ) is that, when the user operates an</p><p>Algorithm 1 MIMI-EVALUATE(π) 1: D ← ∅ 2: while |D|&lt; k do 3: x t ∼ p(x t |s t ; π) user gives command that is partially adapted to interface π 4: a t ∼ π(a t |s t , x t ) interface takes action 5: s t+1 ∼ p(s t+1 |s t , a t ) environment transitions to next state (following unknown dynamics) 6: D ← D ∪ {(s t , x t , s t+1 )} 7: Split D into training set D train and validation set D val 8: φ * , ψ * ← arg max φ,ψ I TUBA (φ, ψ; D train ) optimize MI lower bound in Eqn. 2 9: Return I TUBA (φ * , ψ * ; D val ) return optimized MI lower bound evaluated on validation set unintuitive interface, they provide noisier commands x t given the current state s t (similar to the maximum causal entropy model of noisily-rational user behavior <ref type="bibr" target="#b5">[7]</ref>). This is because an unintuitive interface can confuse users, cause them to provide random commands when they are under time pressure to perform tasks, or cause them to keep trying different commands to see which ones will trigger their desired actions. On the other hand, an intuitive interface makes it easy for the user to determine which command will induce their desired next state, leading to more deterministic user behavior (Fig. <ref type="figure" target="#fig_0">1b</ref>). Penalizing the conditional entropy H(x t |s t ) alone is not enough, however. For example, consider an interface that minimizes the conditional entropy to zero by frustrating the user and causing them to always provide the same command, no matter the state or desired task (e.g., a constant 'no op' signal). Hence, we must also maximize the marginal entropy H(x t ), which leads to an interface that encourages the user to provide a wide distribution of commands overall (e.g., by enabling the user to visit a variety of states).</p><p>Summing the two mutual information terms, we get the total amount of information that the user sends and receives, which simplifies to the mutual information between the user's command x t and the induced state transition (s t , s t+1 ),</p><formula xml:id="formula_0">R(π) I(s t , x t ) + I(x t , s t+1 |s t ) = (H(x t ) -H(x t |s t )) + (H(x t |s t ) -H(x t |s t , s t+1 )) = H(x t ) -H(x t |s t , s t+1 ) = I(x t , (s t , s t+1 )).<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Estimating Mutual Information Rewards</head><p>Computing the mutual information reward R(π) exactly would require taking an expectation with respect to the state transition dynamics of the environment as well as the user's policy for giving commands, both of which are unknown. Hence, we estimate the mutual information by collecting samples (s t , x t , s t+1 ) of the user operating the interface π. Alg. 1 outlines this sample-based evaluation of R(π).</p><p>To estimate I(x t , (s t , s t+1 )) from the samples in a dataset D in our experiments, we use the TUBA estimator <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9]</ref>. In particular, we fit a "statistics network" T φ and variational parameters a ψ to maximize the mutual information lower bound,</p><formula xml:id="formula_1">I TUBA (φ, ψ) (st,xt,st+1)∈D T φ (x t , s t , s t+1 ) -E x∼D e T φ (x,st,st+1) e a ψ (st,st+1) + a ψ (s t , s t+1 ) -1 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where x is sampled uniformly at random from the dataset D. Intuitively, T φ is a discriminator that learns to distinguish between realistic pairs (x t , (s t , s t+1 )) and unrealistic pairs (x, (s t , s t+1 )). In our experiments, we represent T φ and a ψ each as a feedforward neural network that outputs a scalar.</p><p>Two practical issues emerge when applying the standard TUBA estimator to our problem setting. First, we can only collect a small amount of data D to evaluate a given interface during our user studies, which can lead to high-variance estimates of the mutual information. To address this issue, we adopt the following approach from prior work on data-efficient mutual information estimation <ref type="bibr" target="#b8">[10]</ref>: instead of using the final training loss I TUBA as our mutual information estimate, we use the validation loss (line 9 in Alg. 1). Second, we must avoid substantial wall-clock delays to the user while we optimize the mutual information lower bound in line 8 of Alg. 1. Typically, in order to accurately estimate the mutual information, one must optimize the lower bound to convergence. This can take many steps of stochastic gradient descent, and blocks the user from proceeding to the next episode. In our setting, however, we find that optimizing to convergence is unnecessary. The key idea is that, since we ultimately use the mutual information estimates as rewards to compare interfaces, only the relative values of the estimates matter to us. Hence, we only take 1K gradient steps (with a small batch size of 64) to fit the estimator in all our experiments. We find that, even after only 1K steps, the mutual information estimates already tend to correlate positively with the ground-truth task rewards (Fig. <ref type="figure" target="#fig_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning an Interface through Mutual Information Maximization</head><p>In principle, one could represent the interface π in any manner (e.g., as a deep neural network) and use any reinforcement learning algorithm to optimize MIMI-EVALUATE(π). In our experiments, we parameterize the interface as a linear model π θ , where θ are the weights and biases, and maximize MIMI-EVALUATE(π) using an off-the-shelf Bayesian optimization algorithm (scikit-optimize) <ref type="bibr" target="#b9">[11]</ref> based on Gaussian process regression <ref type="bibr" target="#b10">[12]</ref>. Initially, we randomly sample the interface parameters θ 0 , have the user attempt to perform their desired tasks using the interface π θ0 for 10 episodes, and compute the mutual information reward MIMI-EVALUATE(π θ0 ). Given all pairs of interface parameters and corresponding mutual information rewards {(θ i , MIMI-EVALUATE(π θi ))} h i=0 evaluated so far, we fit a Gaussian process regression model that predicts the mutual information reward given the interface parameters, and select the next interface θ h+1 by optimizing one of three acquisition functions-expected improvement, lower confidence bound, or probability of improvement-chosen uniformly at random. Details in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Developing an assistive human-machine interface from scratch requires translating the unknown language of commands into actions. Translating unknown languages is a broad area of research, encompassing linguistics <ref type="bibr" target="#b11">[13]</ref>, cryptanalysis <ref type="bibr" target="#b12">[14]</ref>, and deep neural network interpretability <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17]</ref>. Prior work on multi-agent reinforcement learning proposes inductive biases that use mutual information maximization to promote positive signalling and listening in cooperative tasks <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b17">19]</ref>. This prior work studies the emergence of communication in autonomous agents, whereas MIMI is intended for the unsupervised learning of co-adaptive user interfaces. Our problem setting also differs in that we cannot train the speaker agent (i.e., the human), so we must craft an objective for the listener agent (i.e., the interface) that indirectly promotes informative signaling from the speaker.</p><p>The literature on adaptive interfaces spans multiple fields, including brain-computer interfaces <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b24">26]</ref>, natural language interfaces <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b26">28]</ref>, speech interfaces <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b28">30]</ref>, electronic musical instruments <ref type="bibr" target="#b29">[31]</ref>, and robotic teleoperation interfaces <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b34">36]</ref>. In particular, there is substantial prior work on human-machine co-adaptation <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b41">43]</ref>. In contrast to this prior work, MIMI does not require knowledge of the user's desired tasks or direct supervision. <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b43">45]</ref> propose an unsupervised co-adaptation method that uses principal component analysis to fit an interface that maps commands to actions, but it requires that the interface be a linear mapping, and it does not necessarily maximize the intuitiveness of the interface.</p><p>Prior work has trained reinforcement learning agents to optimize implicit user feedback contained in EEG signals <ref type="bibr" target="#b44">[46]</ref>, peripheral pulse measurements <ref type="bibr" target="#b45">[47]</ref>, facial expressions <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b47">49]</ref>, and clicks <ref type="bibr" target="#b48">[50]</ref>. These approaches typically assume access to a dataset of user observations (e.g., facial expressions) and explicit rewards (e.g., labels that indicate positive or negative emotion) in order to train a reward model that infers rewards from user observations. Other prior work on human-in-the-loop RL assumes direct access to explicit user feedback during the RL phase <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b32">34]</ref>. MIMI, in contrast, does not require access to explicit rewards at any time.</p><p>Mutual information maximization has previously been used to acquire diverse, complex behaviors through reinforcement learning in the absence of extrinsic rewards <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b58">60]</ref>. These prior methods aim to train an autonomous agent, whereas MIMI aims to train a user interface. Furthermore, MIMI differs in that it learns from a user who adaptively provides commands to the interface that try to induce the user's desired next states, whereas in these prior methods, the 'command signal' is a latent code that is randomly sampled from a fixed distribution. As a consequence, the latent skill space discovered by these prior methods is not necessarily intuitive for a user to control, or even human-interpretable.</p><p>Our mutual information objective in Eqn. 1 is related to empowerment <ref type="bibr" target="#b59">[61]</ref>, which measures the channel capacity between an agent's actions and the environment state. Empowerment has previously been used as an intrinsic motivation for autonomous reinforcement learning agents <ref type="bibr" target="#b60">[62]</ref>, and an auxiliary objective for assistive agents that preserve a user's ability to reach various states <ref type="bibr" target="#b1">[3]</ref>. Our objective differs in that it measures not only the user's ability to influence state transitions, but also the intuitiveness of the interface.</p><p>Our implicit model of the user's noisy rationality in Sec. 2.1 is similar to the widely-used maximum causal entropy model of user behavior <ref type="bibr" target="#b5">[7]</ref>, but departs from it in that we assume the user partially adapts their behavior to match the interface, and that the extent of this user adaptation depends on the intuitiveness of the interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>Our experiments focus on measuring MIMI's effectiveness at evaluating interfaces, learning interfaces from scratch, and scaling interface optimization to complex tasks and interface modalities where there is no clear, intuitive solution. We aim to answer the following questions. Q1 (Sec. 5.1): Can MIMI's mutual information score distinguish between effective and ineffective interfaces for a variety of users, interface modalities, environments, and tasks? Q2 (Sec. 5.2): Can we learn an interface from scratch in a completely unsupervised manner by maximizing mutual information rewards with MIMI? Q3 (Sec. 5.3): Does unsupervised human-in-the-loop reinforcement learning scale to more complex interface modalities and tasks? To answer Q1, we conduct an observational study on five datasets from prior work. To answer Q2, we conduct a user study with 12 participants who use a perturbed mouse to perform a 2D cursor control task. To answer Q3, we study how MIMI can be used to enable an expert user (the first author) to play the Lunar Lander game through hand gestures captured by a webcam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Offline Evaluation of Existing Interfaces</head><p>In this experiment, we aim to evaluate whether the mutual information score in MIMI can distinguish between effective and ineffective interfaces for a wide variety of users, interface modalities, environments, and tasks (Q1). We take data from prior work on adaptive interfaces in which the ground-truth rewards were measured, and check whether MIMI's unsupervised evaluation of those interfaces correlates with the true reward that users received when performing tasks via those interfaces. We examine data from four prior works: X2T <ref type="bibr" target="#b61">[63]</ref>, ASHA <ref type="bibr" target="#b62">[64]</ref>, shared autonomy via deep reinforcement learning (SAvDRL) <ref type="bibr" target="#b63">[65]</ref>, and internal-to-real dynamics transfer (ISQL) <ref type="bibr" target="#b64">[66]</ref>. In each of these prior works, users were asked to operate at least two different interfaces: a non-adaptive baseline interface, and an adaptive interface that learns to assist users over time. In the X2T experiments, participants used their eye gaze (a 128-dimensional command signal x t from their webcam) to select 1 of 8 buttons on a screen in order to type a phrase. In the ASHA experiments, participants used their eye gaze to control a 7-DoF simulated robotic arm to either flip a light switch or reach for a bottle. In the SAvDRL and ISQL experiments, participants used the directional keys on a keyboard to play the Lunar Lander game <ref type="bibr" target="#b65">[67]</ref>. The appendix describes each domain in detail. The data from these prior user studies totals over 540K timesteps.</p><p>For each domain, each user, and each experimental condition (e.g., non-adaptive vs. adaptive interface), we measure our mutual information score by averaging over 10 different random seeds used to fit the estimator in Eqn. 2. The ground-truth reward function for each domain is defined in Appendix A.3. The results in Fig. <ref type="figure" target="#fig_2">2</ref> show that, in 4 out of 5 domains, MIMI's mutual information score for an interface is predictive of the ground-truth task reward a user is able to obtain using that interface, with an average Spearman's rank correlation coefficient of ρ = 0.43. Furthermore, in each domain, the interface with the largest mutual information score also has the largest ground-truth reward (Fig. <ref type="figure">5</ref> in the appendix).</p><p>In analyzing the data from the SAvDRL experiments, we initially encountered an unexpected result: a strong negative correlation between our mutual information scores and the ground-truth rewards  (Fig. <ref type="figure" target="#fig_2">2d</ref>). The reason is that, in the SAvDRL experiments, the assistant tends to help the user play the Lunar Lander game by preventing them from crashing, which necessarily involves ignoring a large fraction of the user's commands. This leads to a low mutual information between the user's command x t and the one-step state transition (s t , s t+1 ). However, even though the assistant appears to reduce the user's influence over the next state, it actually increases the user's influence over later states in the episode by preventing the user from crashing immediately. This leads us to propose a generalized mutual information objective, I(x t , (s t , s t+∆ )), in which the time offset ∆ is a hyperparameter than can be set to a value other than ∆ = 1.</p><p>Fig. <ref type="figure" target="#fig_2">2d</ref> shows how modifying the time offset ∆ in the generalized mutual information objective I(x t , (s t , s t+∆ )) affects the correlation between the mutual information scores and the ground-truth rewards in the SAvDRL experiments: for low values of ∆ (including the default value of ∆ = 1), there is a negative correlation, whereas for sufficiently high values of ∆ (i.e., the maximum episode length of 10 3 steps), there is a positive correlation. In Figures <ref type="figure" target="#fig_2">2e</ref> and <ref type="figure" target="#fig_2">2f</ref>, we set ∆ to the maximum episode length, so that the MIMI objective I(x t , (s t , s T )) always measures mutual information with respect to the final state s T of each episode. In general, choosing the value of ∆ requires prior knowledge of the timescale of the user's desired influence over the system (i.e., does the user's command indicate what should happen at the next timestep, or what should happen by the end of the episode?). This is the primary limitation on the generality of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">User Study: 2D Cursor Control with Perturbed Mouse</head><p>In the previous experiment, we showed that MIMI's mutual information score correlates positively with the ground-truth task rewards in a variety of offline datasets (Q1). However, it is not clear from this result alone that maximizing mutual information rewards will yield an interface that also performs well in terms of the ground-truth task rewards (Q2). To answer Q2, we conduct a small-scale user study with 12 participants who use a perturbed mouse to perform a simple 2D cursor control task. The goal of the task is to drive a cursor from the center of the screen to a randomly-sampled target position (Fig. <ref type="figure" target="#fig_3">3e</ref>). The ground-truth reward is the average negative distance to target throughout the episode. The user's 2D mouse position command is used to control the 2D velocity of the cursor (details in Appendix A.3). The interface π θ applies a rotation to the 2D mouse position to get the 2D velocity (i.e., θ consists of a single parameter: the rotation angle). The initial interface π θ0 is a random rotation, and is usually difficult for the user to operate (see scattered trajectories in Fig. <ref type="figure" target="#fig_3">3f</ref>). Users struggle to anticipate and adjust for the rotation, which leads to them initially moving away from the target, oscillating around the target as they approach it because they keep steering in the wrong direction, or getting stuck on the boundaries of the environment because they forget how to steer away from the boundary.</p><p>Each of the 12 participants completed two phases of experiments: A and B. In phase A, they operate 5 interfaces with randomly-sampled parameters θ for 10 episodes each. In phase B, they operate an adaptive interface optimized by MIMI (Sec. 3). Each interface proposed by MIMI is evaluated for 10 episodes, and we halt the experiment after at most 30 interfaces have been evaluated, yielding a total of at most 300 episodes per participant. To avoid the confounding effect of overall user improvement or fatigue over time, we counterbalance the order of phases A and B. Details in Appendix A.1.</p><p>The results in Fig. <ref type="figure" target="#fig_3">3</ref> show that, even though MIMI starts from scratch with a random interface and only aims to maximize mutual information rewards (blue), it enables users to achieve substantially higher ground-truth rewards (orange) than the random baseline interfaces (gray). Fig. <ref type="figure" target="#fig_3">3d</ref> shows that MIMI tends to converge to one of two interfaces: moving the cursor in the same direction as the  intuitive interface (one episode illustrated above) that enables occasional successful landings in a challenging game where constantly crashing is the default behavior. Success rates are averaged over 3 different random seeds, and smoothed using a moving average with a window size of 10 episodes. Hand tracking is performed using a webcam and MediaPipe <ref type="bibr" target="#b66">[68]</ref>. See Fig. <ref type="figure">6</ref> in the appendix for plots of the mutual information reward. Videos available at <ref type="url" target="https://sites.google.com/view/coadaptation">https://sites.google.com/view/coadaptation</ref> mouse, or rotating the mouse direction 180 degrees (which tends to be easy for users to understand and invert; see the appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Lunar Lander with Hand Gestures</head><p>In the previous experiment, we showed that MIMI is capable of learning an interface from scratch through human-in-the-loop RL with the mutual information objective, albeit on a simple task and interface modality (2D cursor control via 2D mouse commands). In this section, we demonstrate that this result extends to a more complex task and interface modality (Q3): an expert user (the first author) playing the Lunar Lander game <ref type="bibr" target="#b65">[67]</ref> using hand gestures. To win the game, the user must land between the flags without crashing. The user's hand pose command is a 4D signal that consists of the distances from the thumb tip to each of the four other fingertips. The Lunar Lander game has a 2D continuous action space that controls the main engine and lateral thrusters (details in Appendix A.3). The interface π θ is a linear transformation from the 4D hand pose command to the 2D thruster action space (i.e., θ consists of 8 parameters). The initial interface π θ0 is a random linear function, and is usually difficult for the user to operate (e.g., because it requires making difficult hand gestures, or never fires the lateral thrusters). Lunar Lander is a challenging game, even when played with a joystick or keyboard. What makes it more challenging in this setting is that, unlike 2D cursor control, where there is an obvious prior mapping that moves the cursor in the direction of the mouse without any rotation, there is no obvious mapping from hand gestures to thruster actions that the user would have in mind.</p><p>The results in Fig. <ref type="figure" target="#fig_5">4</ref> show that, after 200 episodes of online training, MIMI learns an interface that enables the expert user to maneuver the lander, usually avoid crashing, and occasionally land successfully. The interface enables the user to fire the right thruster by touching their index finger to their thumb, fire the left thruster and main engine by touching their ring finger to their thumb, and fire the main engine by slightly curling all fingers. The 8 interface parameter values θ learned through MIMI would have been difficult to determine in advance, given uncertainty about the range of comfortable hand pose commands, and the non-obvious intuitiveness of using individual fingers for lateral thrusters vs. all fingers for the main engine. Interestingly, the final interface does not enable the user to fire the left thruster in isolation (i.e., all gestures that fire the left thruster also fire the main engine simultaneously). This turns out to be okay, since the user wants to descend, and alternating between firing the left thruster and main engine together then firing the right thruster without the main engine is a reasonable strategy for descending without tilting. This would have been difficult to anticipate in advance, but nevertheless emerged through co-adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We presented a proof of concept that, through unsupervised human-in-the-loop reinforcement learning, we can learn an assistive interface from scratch in less than 30 minutes without any explicit user feedback or prior knowledge of the user's desired tasks. Our experiments show that, for a variety of users (12 participants in our user study, and 12 in each of the offline evaluations), command modalities (high-dimensional eye gaze and hand gestures, and low-dimensional keyboard commands), environments (typing, simulated robotic control, playing a video game, and cursor control), and tasks (flipping light switches and reaching for bottles with the same simulated robotic arm), MIMI's mutual information objective can be used to rank interfaces without observing ground-truth rewards for the user's desired task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations</head><p>MIMI is limited by the fact that the correlation between the mutual information objective I(x t , (s t , s t+∆ )) and the ground-truth reward function can depend on the time offset ∆ (Sec. 5.1). Since we do not assume access to ground-truth rewards, choosing the value of the hyperparameter ∆ requires prior knowledge of the timescale of the user's desired control over the system (i.e., whether the user wants to be able to control what happens next, or what happens by the end of the episode).</p><p>The main limitation of our experiments is that the largest interface only has 8 parameters, due to constraints on the duration of a user study and the data efficiency of the Bayesian optimization algorithm we use for RL. One direction for future work is to instantiate MIMI with a data-efficient deep RL algorithm that scales to learning high-dimensional policies, such as REDQ <ref type="bibr" target="#b67">[69]</ref> or policy evaluation networks <ref type="bibr" target="#b68">[70]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future Work</head><p>Another direction for future work is evaluating MIMI on real-world systems in which explicit user feedback is sparse or unavailable, such as brain-computer interfaces for users with total locked-in syndrome. MIMI could also be helpful for designing creative tools for artists and musicians where the 'task' or 'ground-truth reward function' is inherently difficult to specify, such as a tool that helps users navigate the high-dimensional latent space of a generative model of images using physical gestures and virtual reality <ref type="bibr" target="#b69">[71]</ref>, an instrument that provides users with a playable, low-dimensional latent space of music audio <ref type="bibr" target="#b70">[72]</ref>, or a tool that helps designers carve out discriminable gestures from low-dimensional embeddings <ref type="bibr" target="#b71">[73]</ref>.</p><p>There may be cases where the user can observe the state s t , but the interface cannot (e.g., the user can look around with their eyes, but the interface is not equipped with a camera). A useful extension of MIMI would extract implicit feedback from the stream of command signals x t alone, without assuming access to the states s t .</p><p>Biological neurons maximize the mutual information between their inputs and outputs through local, unsupervised learning-known as the infomax principle <ref type="bibr" target="#b72">[74]</ref>. Ideas from recent work on greedy, self-supervised representation learning based on the infomax principle <ref type="bibr" target="#b73">[75]</ref> could be useful for scaling MIMI to deep neural network interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This After Phase B: Sometimes I could figure out that there was some offset angle and then I would try to adjust my control based on that. Sometimes I couldn't tell what the transformation of my input was and then I was just making small adjustments until the dot headed in the right direction. Sometimes after a particularly easy block (when I just had to point at the green dot and it would go to it) I got unfocused and had a harder time controlling some of the more difficult blocks right after.</p><p>To account for differences in the number of online training episodes for each of the 12 participants in Fig. <ref type="figure" target="#fig_3">3a</ref>, we pad the sequence of true rewards with the final true reward, up to the maximum sequence length across participants. We use the same padding scheme to account for differences in the number of online training episodes for each of the 3 training runs in Fig. <ref type="figure" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation</head><p>In our cursor control and Lunar Lander user studies in Sections 5.2 and 5.3, we simplify the policy architecture to ignore the environment state s t , so that π(a t |x t ) is only conditioned on the user's command x t .</p><p>All user studies and experiments (including model training) were performed on a consumer-grade MacBook Pro laptop computer. During the user study, the user is repeatedly presented with a new interface and asked to operate it for 10 episodes. During those 10 episodes, the user becomes more proficient at using that fixed interface. When presented with a random interface, the user takes a longer time to learn to use the interface and achieves worse final performance, compared to when they are presented with an interface that is being optimized by MIMI.</p><p>We use the scikit-optimize library<ref type="foot" target="#foot_0">foot_0</ref> to implement the Bayesian optimization algorithm for mutual information maximization described in Sec. 3. In the cursor control user study, the first 5 interfaces are sampled uniformly at random, then the subsequent policies are selected using acquisition functions that measure expected improvement, lower confidence bound, and probability of improvement. In the Lunar Lander experiments, the first 3 interfaces are sampled uniformly at random. In both domains, we have the user operate each interface for 10 episodes.</p><p>Code and data are available at <ref type="url" target="https://github.com/rddy/mimi">https://github.com/rddy/mimi</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Environments</head><p>The X2T experiments formulate typing as a contextual bandit problem. Hence, we format the dataset such that each attempt to press a button is a separate episode that consists of only one state transition, from the all-zeros initial state s 0 (which represents no buttons being pressed) to a one-hot encoding s 1 of the button that was pressed. In the ASHA experiments, the state s t is either a 142-dimensional vector (switch domain) or 151-dimensional vector (bottle domain) that includes the 7 joint positions of the arm and the 2D position and 4D orientation of the end effector-the full list of state variables can be found in Sec. B.3 of the appendix in <ref type="bibr" target="#b62">[64]</ref>. In our analysis of the SAvDRL and ISQL data, we set the state s t to be the 1D horizontal position of the lander.</p><p>In X2T, ground-truth rewards correspond to the classification accuracy of the interface, which predicts the user's desired button (1 of 8) given the user's eye gaze signals. There were three conditions in the X2T experiments: the user first operated the non-adaptive baseline interface, then operated the adaptive X2T interface, then returned to operating the non-adaptive baseline interface. There were 12 participants in the X2T user study, yielding a total of 12 • 3 = 36 data points in the scatter plot in Fig. <ref type="figure" target="#fig_2">2</ref> (one for each user in each condition). In the ASHA switch experiments, ground-truth rewards penalize distance from the robot end effector to the position of the target switch. In the ISQL data, there were only two conditions: the user playing the Lunar Lander game on their own, and with assistance. In the Lunar Lander game, ground-truth rewards penalize crashing, and give a bonus for landing between the flags. In the ASHA bottle experiments, ground-truth rewards give a bonus for opening the door in front of the desired bottle and for reaching the desired bottle.</p><p>In the 2D cursor control environment, the ground-truth reward function is the average negative distance to target throughout the episode. The episode terminates when the user reaches the target, so naively computing this reward would penalize the user for reaching the target quickly. To address this issue, we treat reaching the target as entering an absorbing state, and re-weight the reward as r ← |τ |•r + (T -|τ |) • 0, where r is the average negative distance to target throughout the trajectory τ , |τ | is the length of the trajectory, and T is the maximum episode length. The maximum episode length is T = 300 timesteps.</p><p>In the Lunar Lander experiments in Sec. 5.3, we set the state s t to be the 3D position and orientation of the lander. The maximum episode length is 500 timesteps. The location of the landing zone is sampled uniformly at random at the beginning of each episode.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: (a) The user is trying to perform a task (e.g., land a quadrotor), which the interface π cannot directly observe. The user communicates with the interface π by observing the environment state st, then providing a command signal xt (e.g., the raw audio of a voice command). The interface π observes the state st and command xt, then takes an action at that causes the next state st+1. We assume that the user partially adapts their command xt to any given interface π, and that the extent of this user adaptation depends on the intuitiveness of the interface. We search for an interface π that maximizes the rate of information transfer from user to the environment (orange) and from the environment to the user (blue), which we formalize as the mutual information I(xt, (st, st+1)). (b) The more intuitive the interface (e.g., a QWERTY keyboard where the 'a', 's', 'd', and 'f' keys are arranged in that order), the less noisy the user's commands (e.g., keypresses). The less intuitive the interface (e.g., a randomly-permuted keyboard where "asdf" is scrambled to "dfas"), the noisier the user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In 4 out of 5 domains, our mutual information score (y-axis) is predictive of the ground-truth reward (x-axis) that a user is able to obtain with a given interface. (a) In X2T, ground-truth rewards correspond to the classification accuracy of the interface, which predicts the user's desired button (1 of 8) given the user's eye gaze signals. There were three conditions in the X2T experiments: the user first operated the non-adaptive baseline interface, then operated the adaptive X2T interface, then returned to operating the non-adaptive baseline interface. There were 12 participants in the user study, yielding a total of 12 • 3 = 36 data points in the scatter plot (one for each user in each condition). Spearman's rank correlation coefficient ρ = 0.45 between the ground-truth rewards and the estimated mutual information scores. (b) Ground-truth rewards penalize distance from the robot end effector to the position of the target switch. (c) In the internal-to-real dynamics transfer data, there were only two conditions: the user playing the Lunar Lander game on their own, and with assistance. (d) In the SAvDRL data, the rank correlation between the ground-truth reward and the mutual information score depends on the time offset ∆ in the generalized mutual information objective I(xt, (st, st+∆)). (e) Following the results in (d), we set the time offset ∆ to the maximum episode length of 10 3 . Ground-truth rewards penalize crashing, and give a bonus for landing between the flags. (f) Ground-truth rewards give a bonus for opening the door in front of the desired bottle and for reaching the desired bottle. As in (e), we set the time offset ∆ to the maximum episode length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Through online training, MIMI (orange) learns an interface that substantially outperforms baseline interfaces with random parameters (gray), and approaches the performance of an oracle agent that always moves straight to the target (green). (b) As expected, MIMI is indeed maximizing mutual information rewards through human-in-the-loop RL. (c) Each point in the scatter plot represents a distinct user operating a distinct interface. The ground-truth task rewards (negative average distance to target) and the mutual information rewards are highly positively correlated (Spearman's rank correlation ρ = 0.87), as in the offline experiments in Sec. 5.1. (d)A polar histogram of the final interface parameter θ that MIMI converges to, for each of the 12 users. Whereas the random baseline samples θ from a uniform distribution over perturbation angles [0, 2π), MIMI converges to a highly non-uniform distribution over θ. In particular, MIMI tends to converge to one of two interfaces: no perturbation of the user's mouse (θ ≈ 0), or inversion of the user's mouse (θ ≈ π). (e-g) In this center-out cursor control task, the user initially tends to move in curved trajectories that wander away from the target. After 150 episodes of online training, the user tends to move in straight lines to the target.</figDesc><graphic coords="8,238.07,168.45,112.22,95.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: MIMI can learn an effective mapping from hand pose commands to thruster actions in the Lunar Lander game. During the initial training episodes when the interface is randomly sampled, the interface can require uncomfortable hand contortions in order to trigger certain actions, or have a very sharp threshold between poses that trigger one action vs. a different action. Towards the end of training, MIMI learns a comfortable, intuitive interface (one episode illustrated above) that enables occasional successful landings in a challenging game where constantly crashing is the default behavior. Success rates are averaged over 3 different random seeds, and smoothed using a moving average with a window size of 10 episodes. Hand tracking is performed using a webcam and MediaPipe<ref type="bibr" target="#b66">[68]</ref>. See Fig.6in the appendix for plots of the mutual information reward. Videos available at https://sites.google.com/view/coadaptation</figDesc><graphic coords="9,389.73,196.61,58.84,57.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: (a) From Fig. 4a. (b) Analogous to Fig. 3b, but for Lunar Lander instead of cursor control. (c) Analogous to Fig. 3c, but for Lunar Lander instead of cursor control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>work was supported by Berkeley Existential Risk Initiative, NSF IIS-1651843, ARL DCIST CRA W911NF-17-2-0181, Weill Neurohub, and NSF CAREER. Thanks to Natasha Jaques for giving feedback on this project during its early stages and pointing us to related work. Thanks to lauren, Quantum1000, guillefix, and the Transhumanists in VR community for many stimulating discussions about MIMI, human-computer interfaces, and multi-agent learning. Thanks to anonymous NeurIPS reviewers eRaa, phLR, and evLC for their constructive feedback.Try to predict the angle of the offset, then course adjust After Phase B: Initially, I twirled my mouse around to find the correct heading, eventually I was able to predict the offset well enough to directly go to the goal position.If it moved in the direction of cursor, then I just clicked the green dot. If it was 180 rotation, I imagined just doing the opposite of what I normally did. If it was 90, it was the hardest.Initially, my strategy was unchanged from Phase 1. After 30 episodes I did not perceive any perturbation between my cursor and the direction of the circle. Figure 5: In each domain, the interface with the highest mutual information score is also the interface that enables users to achieve the highest ground-truth reward. Analogous to Fig. 2 in the main paper, but averaging the mutual information scores and ground-truth rewards across all 12 user study participants in each experimental condition. Subjective Evaluations from User Study Participants p</figDesc><table><row><cell>User 3:</cell></row><row><cell>After Phase A:</cell></row><row><cell>User 4:</cell></row><row><cell>After Phase A:</cell></row><row><cell>I looked at where the circle went in the first few seconds and adjusted</cell></row><row><cell>After Phase B:</cell></row><row><cell>Look at where the circle went in the first few seconds, then adjust the angle offset from there</cell></row><row><cell>User 5:</cell></row><row><cell>After Phase A:</cell></row><row><cell>I recalibrated as it started moving, it was easier to see what direction you're going in</cell></row><row><cell>after you start moving</cell></row><row><cell>After Phase B:</cell></row><row><cell>User 6:</cell></row><row><cell>After Phase A:</cell></row><row><cell>tried to identify the angle offset between curse and green dot so that it went straight</cell></row><row><cell>to green dot</cell></row><row><cell>After Phase B:</cell></row><row><cell>Put the cursor where the green dot was</cell></row><row><cell>User 7:</cell></row><row><cell>After Phase A:</cell></row><row><cell>try to figure out the angle over time</cell></row><row><cell>After Phase B:</cell></row><row><cell>again predicting the angle and adjusting on fly as need being. As algorithm seemed</cell></row><row><cell>to get better, began just hovering mouse over the target</cell></row><row><cell>User 8:</cell></row><row><cell>After Phase A:</cell></row><row><cell>I first tried to figure out what the constant angular offset was, and then tried to adjust</cell></row><row><cell>my steering accordingly</cell></row><row><cell>After Phase B:</cell></row><row><cell>Pretty much the same as the first phase, trying to figure out the offset, and then steering</cell></row><row><cell>accordingly</cell></row><row><cell>User 9:</cell></row><row><cell>After Phase A:</cell></row><row><cell>Try to learn the perturbation in the first couple episodes then adjust the cursor angle</cell></row><row><cell>accordingly afterwards.</cell></row><row><cell>After Phase B:</cell></row><row><cell>User 10:</cell></row><row><cell>After Phase A:</cell></row><row><cell>same as phase B</cell></row><row><cell>After Phase B:</cell></row><row><cell>Trying to determine the angle between the goal and where i should point my cursor,</cell></row><row><cell>for each batch of 10</cell></row><row><cell>User 11:</cell></row><row><cell>After Phase A:</cell></row></table><note><p>I tried to figure out the angle that was being added to my input and then adjusted my control to be minus that angle from the direction I actually wanted the dot to go. The angles that were close to either 0 or 180 were easier to control than those closer to 90/270 degrees.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization. html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 User Study</head><p>We recruited 10 male, 1 female, and 1 non-binary participants, with an average age of 27. We obtained informed consent from each participant, as well as IRB approval for our study. Each participant was provided with the rules of the task, and a short practice period of 10 episodes to familiarize themselves with the 2D cursor control environment. Each participant was compensated with a $10 Amazon gift card.</p><p>When prompted to "please describe your command strategy", participants responded as follows.</p><p>User 0: After Phase A: Use 1-2 trials to get a sense of the angle difference After Phase B: same as phase A User 1: After Phase A: I positioned the cursor opposite of the triangle and the green dot since most of the physics made it such that that moved the triangle in the desired direction After Phase B: same as phase A User 2: After Phase A: steering wheel After Phase B: same as phase A</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Autonomy infused teleoperation with application to BCI manipulation</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Sebastien</forename><surname>Valois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Javdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Collinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.05451</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Tiomkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Polani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14796</idno>
		<title level="m">Pieter Abbeel, and Anca Dragan. AvE: Assistance via empowerment</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Hallak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dotan</forename><surname>Di Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02259</idno>
		<title level="m">Contextual Markov decision processes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication. The Bell System Technical</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">J C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling interaction via the principle of maximum causal entropy</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brian D Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The IM algorithm: a variational approach to information maximization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Agakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On variational lower bounds of mutual information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair ; Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Aäron van den Oord</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Indranil</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Samuel A Nastase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><surname>Amer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03319</idno>
		<title level="m">Data-efficient mutual information neural estimator</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The decipherment of Linear B</title>
		<author>
			<persName><forename type="first">John</forename><surname>Chadwick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Sinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Feil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Elementary cryptanalysis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06960</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Translating neuralese. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swee Kiat</forename><surname>Lim</surname></persName>
		</author>
		<ptr target="https://distill.pub/2020/circuits" />
		<imprint/>
	</monogr>
	<note>Thread: Circuits. Distill, 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Biases for emergent communication in multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05676</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Social influence as intrinsic motivation for multi-agent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Strouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A high-performance neural prosthesis enabled by control algorithm design</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Gilja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Nuyujukian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><forename type="middle">A</forename><surname>Chestek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu Byron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joline</forename><forename type="middle">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Design and analysis of closed-loop decoder adaptation algorithms for brain-machine interfaces</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">L</forename><surname>Orsborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><forename type="middle">G</forename><surname>Moorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Carmena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi-agent control framework for co-adaptation in brain-computer interfaces</title>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">S</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous closed-loop decoder adaptation with a recursive maximum likelihood algorithm allows for rapid performance acquisition in brain-machine interfaces</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><forename type="middle">G</forename><surname>Moorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">L</forename><surname>Orsborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Shanechi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Carmena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04156</idno>
		<title level="m">Neuroprosthetic decoder training as imitation learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech synthesis from neural decoding of spoken sentences</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gopala K Anumanchipalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">F</forename><surname>Chartier</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-performance brain-to-text communication via imagined handwriting</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">T</forename><surname>Francis R Willett</surname></persName>
		</author>
		<author>
			<persName><surname>Avansino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaimie</forename><forename type="middle">M</forename><surname>Leigh R Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02447</idno>
		<title level="m">Learning language games through interaction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning adaptive language interfaces through decomposition</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05190</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove-Talk: A neural network interface between a data-glove and a speech synthesizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sidney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove-Talk II-a neural-network interface which maps gestures to parallel formant speech synthesizer controls</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sidney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mapping performer parameters to synthesis engines</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><forename type="middle">M</forename><surname>Wanderley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Organised Sound</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning</title>
		<author>
			<persName><surname>Patrick M Pilarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Michael R Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farbod</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">P</forename><surname>Fahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Rehabilitation Robotics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning models for shared control of human-machine systems with unknown dynamics</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Broad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>David Murphey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brenna</forename><forename type="middle">Dee</forename><surname>Argall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01744</idno>
		<title level="m">Shared autonomy via deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><surname>Schaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05097</idno>
		<title level="m">Residual policy learning for shared autonomy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeon</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">P</forename><surname>Losey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03210</idno>
		<title level="m">Shared autonomy with learned latent actions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploiting co-adaptation for the design of symbiotic neuroprosthetic assistants</title>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">C</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Digiovanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Progressive co-adaptation in humanmachine interaction</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Gallina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Bellotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Informatics in Control, Automation and Robotics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human-robot mutual adaptation in collaborative tasks: Models and experiments</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model and experiments to optimize co-adaptation in a simplified myoelectric control system</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Couraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cattaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Paclet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aymar</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rugy</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human-agent co-adaptation using error-related potentials</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A computational model of human decision making and learning for assessment of co-adaptation in neuro-adaptive human-robot interaction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Errors in human-robot interactions and their effects on robot learning</title>
		<author>
			<persName><forename type="first">Su</forename><surname>Kyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Elsa</forename><forename type="middle">Andrea</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Schloßmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Kirchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised coadaptation of an assistive interface to facilitate sensorimotor learning of redundant control</title>
		<author>
			<persName><forename type="first">Dalia</forename><surname>De Santis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrycja</forename><surname>Dzialecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdinando</forename><forename type="middle">A</forename><surname>Mussa-Ivaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biomedical Robotics and Biomechatronics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A framework for optimizing co-adaptation in body-machine interfaces</title>
		<author>
			<persName><forename type="first">Dalia</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Accelerating reinforcement learning agent with EEG-based implicit human feedback</title>
		<author>
			<persName><forename type="first">Duo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faramarz</forename><surname>Fekri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghupathy</forename><surname>Sivakumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16498</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visceral machines: Reinforcement learning with intrinsic physiological rewards</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning via social awareness: Improving a deep generative sketching model with facial feedback</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Mccleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04877</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The empathic framework for task learning from implicit human feedback</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Allievi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Niekum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Knox</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13649</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evaluating the robustness of learning from implicit feedback</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno>arXiv preprint cs/0605036</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interactively shaping agents via human reinforcement: The TAMER framework</title>
		<author>
			<persName><forename type="first">Knox</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Capture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Macglashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Loftin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Michael L Littman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06049</idno>
		<title level="m">Interactive learning from policy-dependent human feedback</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Active preference-based learning of reward functions</title>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Anca D Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjit</forename><forename type="middle">A</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Seshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical relative entropy policy search</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07507</idno>
		<title level="m">Variational intrinsic control</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Diversity is all you need: Learning skills without a reward function</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06070</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Van De Wiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11359</idno>
		<title level="m">Unsupervised control through non-parametric discriminative rewards</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dynamicsaware unsupervised discovery of skills</title>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01657</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Van De Wiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05030</idno>
		<title level="m">Fast task inference with variational intrinsic successor features</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">CIC: Contrastive intrinsic control for unsupervised skill discovery</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00161</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Empowerment: A universal agent-centric measure of control</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Alexander S Klyubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chrystopher</forename><forename type="middle">L</forename><surname>Polani</surname></persName>
		</author>
		<author>
			<persName><surname>Nehaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezende</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08731</idno>
		<title level="m">Variational information maximisation for intrinsically motivated reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">X2T: Training an x-to-text typing interface with online learning from user feedback</title>
		<author>
			<persName><forename type="first">Jensen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Berseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhilesh</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karunesh</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">ASHA: Assistive teleoperation via human-in-the-loop reinforcement learning</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jensen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Berseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02465</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Shared autonomy via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>Arxiv 1802.01744</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Where do you think you&apos;re going?: Inferring beliefs about dynamics from behavior</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>Arxiv 1805.08010</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">OpenAI Gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Bazarevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Vakunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Tkachenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuo-Ling</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10214</idno>
		<title level="m">MediaPipe hands: On-device real-time hand tracking</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Randomized ensembled double Qlearning: Learning fast without a model</title>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05982</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11833</idno>
		<title level="m">Policy evaluation networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Developing embodied familiarity with hyperphysical phenomena</title>
		<author>
			<persName><forename type="first">Gray</forename><surname>Crawford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">MusicVAE: Creating a palette for musical scores with machine learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Low-dimensional embeddings for interaction design</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mihai Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Svenja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Schött</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albrecht</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roderick</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><surname>Murray-Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Advanced Intelligent Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Putting an end to end-to-end: Gradientisolated learning of representations</title>
		<author>
			<persName><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastiaan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Veeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
