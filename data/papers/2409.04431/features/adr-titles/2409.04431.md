- Decision to explore alternatives to softmax attention
- Choice of sigmoid activation for attention mechanism
- Implementation of FLASH-SIGMOID for hardware efficiency
- Selection of bias term for sigmoid activation
- Theoretical proof of universal function approximation for SigmoidAttn
- Analysis of regularity and Lipschitz constant for SigmoidAttn
- Comparison of computational complexity between sigmoid and softmax attention
- Design of multi-head attention mechanism with SigmoidAttn
- Decision to conduct empirical analysis across multiple domains (language, vision, speech)
- Choice of normalization techniques for attention weights
- Implementation of tiling and kernel fusion strategies in FLASHSIGMOID
- Decision to focus on stabilization of attention norms during training
- Choice of hyperparameters for SigmoidAttn
- Decision to unify prior art and establish best practices for sigmoid attention
- Selection of evaluation metrics for performance comparison with softmax attention