# Theory, Analysis, and Best Practices for Sigmoid Self-Attention

## Abstract

## 

Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASH-SIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs 2 . Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.

## Introduction

The success of modern machine learning can be largely attributed to the attention mechanism [(Bahdanau et al., 2015;](#b0)[Vaswani et al., 2017)](#b55). Attention uses a sequence-to-sequence (seq-to-seq) map to build context-aware token representations. Classically, attention relies on the softmax function (SoftmaxAttn) to recover token representations as data-dependent convex combinations of values.

Despite its widespread use and effectiveness, softmax in SoftmaxAttn is not without limitations. For instance, the softmax function can sometimes lead to a concentration of attention on just a few features [(Yang et al., 2018;](#b59)[Ganea et al., 2019)](#b20), potentially neglecting other informative aspects of the input data. Moreover, applying SoftmaxAttn requires performing a row-wise reduction along the length of the input sequence, which in the case of efficient attention kernels [(Dao et al., 2022;](#b15)[Dao, 2023)](#), slows down computations. In this work, we relax this constraint by substituting the row-wise softmax operation with an element-wise sigmoid nonlinearity. We highlight that the central problem with naïve sigmoid attention (SigmoidAttn) is that of large initial attention norms and propose solutions to alleviate it. Our contributions are as follows:

(1) We prove SigmoidAttn is a universal function approximator on seq-to-seq tasks (Sec. 3.1).

(2) We analyze SigmoidAttn's regularity and provide its worst-case Jacobian bound (Sec. 3.2).

(3) We extend FLASHATTENTION2 [(Dao et al., 2022;](#b15)[Dao, 2023)](#) with the sigmoid kernel, reducing kernel inference wall-clock time by up to 17% and real world inference by up to 8% (Sec. [4)](#). ( [4](#formula_2)) We show that SigmoidAttn matches SoftmaxAttn in various tasks and domains (Sec. 5).

## Sigmoid Attention

Let X ∈ R n×d be the input sequence of n vectors, where each vector has dimension d. We define three learnable weight matrices W q ∈ R d×d qk , W k ∈ R d×d qk , and W v ∈ R d×dv , which are used to compute the queries Q ∈ R n×d qk , keys K ∈ R n×d qk , and values V ∈ R n×dv as follows:

$Q = XW q , K = XW k , and V = XW v .$(1)

Self-attention [(Bahdanau et al., 2015;](#b0)[Vaswani et al., 2017)](#b55) can be compactly written as

$SoftmaxAttn(X) = Softmax(QK T / d qk )V ,(2)$where the Softmax function normalizes each row of the input matrix. We replace the Softmax with SigmoidAttn(X) = σ(QK T / d qk )V , with σ : u → sigmoid(u + b) := (1 + e -(u+b) ) -1 .

(3)

Here, σ is applied element-wise to the input matrix in (3). The activation function σ has a hyperparameter b ∈ R. In App. E, we discuss an intuitive way to choose the order-optimal bias term, resulting in b =log(n). This choice of b allows us to make sense of SigmoidAttn for any sequence length. Indeed, letting (y 1 , . . . , y n ) = SigmoidAttn(X) be the output sequence, we have

$y i = n j=1 exp(⟨W q x i , W k x j ⟩) exp(⟨W q x i , W k x j ⟩) + n W v x j -----→ n→+∞ exp(⟨W q x i , W k x⟩)W v xdµ(x),(4)$where µ = 1 n n j=1 δ xj is the empirical measure corresponding to X. Notably, ( [4](#formula_2)) still makes sense in the infinite length limit, where the measure µ is not a sum of Diracs. [Wortsman et al. (2023a)](#) do not use a bias, and propose a n -1 normalization for various attention activations, such as sigmoid and ReLU, but leave the reason as an open question. Our variable bias has a similar effect in the large n limit, and we posit that recovering a finite output limit as n increases is the why it works in practice.

A multi-head version of ( [3](#)) is obtained by combining the outputs of several SigmoidAttn, as follows:

$[SigmoidAttn 1 (X), . . . , SigmoidAttn h (X)] W o ,(5)$for a learnable output weight matrix W o ∈ R hdv×d , where h denotes the number of heads.

## Theoretical Properties of Sigmoid Attention

We analyze SigmoidAttn, with two objectives: [(1)](#) showing that a transformer architecture remains a universal function approximator when SigmoidAttn replaces SoftmaxAttn, and (2) recovering a measure of regularity of SigmoidAttn by computing its Lipschitz constant.

3.1 Are Transformers with Sigmoid Attention Universal Approximators? [Yun et al. (2020)](#b60) demonstrate that classical transformers can approximate continuous sequenceto-sequence functions to arbitrary precision, a property known as the Universal Approximation Property (UAP). UAP is highly desirable as it provides proof of an architecture's generalizability and representation capability. As SigmoidAttn modifies the transformer architecture, it is crucial to theoretically guarantee that this modification does not impact the representation capability and that UAP is retained. We provide this guarantee with the following theorem.

Theorem 3.1 (UAP for SigmoidAttn). We denote with T h,dv,r σ the class of transformer networks obtainable by combining an arbitrary number of SigmoidAttn layers (each of h heads of dimension d v ) followed by FFN layers of hidden dimension r. For any given continuous, permutation-equivariant function f : Ω ⊂ R n×d → R n×d with compact support Ω, and for any arbitrarily small error ε, there exists a transformer network g ∈ T 4,1,4   σ such that

$Ω ∥f (X) -g(X)∥ p p dX ≤ ε, for 1 ≤ p < ∞.(6)$Theorem 3.1 is the exact counterpart of [(Yun et al., 2020, Thm. 2)](#), which shows UAP for classical transformers. Our proof largely follows the same path, an outline of the original proof provided in App. C. Here, we present an overview of the main adaptations required to prove Thm. 3.1 for SigmoidAttn, with further details in App. C.1 and C.2.

Sigmoid Attention layers can implement contextual mappings: A key step in proving Thm. 3.1 is showing that, even with SigmoidAttn, a sequence of transformer blocks can implement a Contextual Mapping [(Yun et al., 2020, Def. 3.1)](#). A contextual mapping characterizes a function that maps each input sequence element to an output uniquely dependent on the whole sequence. This property allows a transformer to capture and store global context within each token, even if each layer only performs pairwise comparisons. Subsequent layers can then use this global information to map individual tokens to the correct output, ultimately approximating any arbitrary sequence-to-sequence function.

In [Yun et al. (2020)](#b60), the contextual mapping is assembled by modifying individual transformer blocks: each block is tuned to react to a specific input token. By stacking a sequence of these blocks, a transformer can be turned into an accumulator, mapping a given input token sequence to a unique global index. This outcome is achieved via a selective shift layer [(Yun et al., 2020, App. B.5)](#):

$Ψ(X; b, b ′ ) i,1 := max k X k,1 -min k X k,1 if b < X i,1 < b ′ 0 otherwise,(7)$and can be approximated using classic attention. Although SigmoidAttn cannot directly approximate [(7)](#b64), our accumulator definition relies on an equivalent selective shift operation:

$Ψ σ (X; b, b ′ ) i,1 := k:X k,1 >b ′ X k,1 if b < X i,1 < b ′ 0 otherwise,(8)$which can be approximated by SigmoidAttn (described in App. C.1). In App. C.2.4, we show that ( [8](#formula_6)) shares similar properties with [(7)](#b64), allowing us to use the original proof framework in [Yun et al. (2020)](#b60) and demonstrate that UAP holds in our case as well.

Our proof is largely equivalent to that in [Yun et al. (2020)](#b60), with two relevant differences: to approximate ( [8](#formula_6)), we require SigmoidAttn with at least four heads and shifts included in both query and key definitions. In contrast, SoftmaxAttn requires at least two heads to approximate [(7)](#b64), with shifts only in the query definition. However, this is primarily a theoretical requirement for the proof and does not affect performance. Notably, the total number of parameters required by both architectures for the approximation follows the same tight scaling of [Yun et al. (2020)](#b60).

## Regularity of Sigmoid Attention

As with any layer in a neural network, the regularity of SigmoidAttn is important to study, as it gives insights into the robustness of the corresponding network and the ease of optimizing it. The most standard way to quantify the regularity of a layer function ϕ is to compute its Lipschitz constant over a set X , that is a constant C > 0 such that for all X, Y ∈ X , it holds ∥ϕ(X)ϕ(Y

$)∥ ≤ C∥X -Y ∥,$where ∥ • ∥ is the standard Frobenius norm. The local Lipschitz constant is the spectral norm of the Jacobian of ϕ at X. The two are related: the Lipschitz constant of ϕ over X is the greatest local Lipschitz constant for all X ∈ X . We turn to the theorem giving the regularity of SigmoidAttn:

Theorem 3.2. Define A = {⟨W q x i W k x j ⟩|, i, j ∈ {1, . . . , n}} ⊂ R the set of attention weights, and the scaled activation norms σ ∞ = n × sup u∈A |σ(u)| and σ ′ ∞ = n × sup u∈A |σ ′ (u)|. Then, the Jacobian of SigmoidAttn at X = (x 1 , . . . , x n ) has a spectral norm of at most:

$∥W v ∥ 2 σ ∞ + 2σ ′ ∞ ∥W T q W k ∥ 2 1 n n i=1 ∥x i ∥ 2 2 . (9$$)$The proof is found in App. D. In SigmoidAttn, if we assume that the attention weights ⟨W q x i , W k x j ⟩ are all bounded by a constant µ -this is true, e.g., if the activations are bounded -we get σ ∞ ≤ exp(µ) and σ ′ ∞ ≤ exp(µ) thanks to the choice of b =log(n). The bound in Thm. 3.2 depends only on the average squared-norm of the input sequence x i , while classical results for the study of attention all rely on the largest value of ∥x i ∥ 2 2 [(Kim et al., 2021;](#b35)[Castin et al., 2023)](#b6). This is another consequence of the simplicity of sigmoid attention and is due to the removal of the normalizing constant in SoftmaxAttn. Our result implies that if all x i are within a ball of radius R then the Lipschitz constant of SigmoidAttn grows at most like R 2 , but it is stronger since we can apply this to unbounded distributions x i ; it matters only that the second moment is bounded. This result contrasts sharply with the bounds obtained for SoftmaxAttn: [Castin et al. (2023, Thm. 3.4.)](#) show that there exists a sequence X = (x 1 , . . . , x n ) with ∥x i ∥ 2 ≤ R for all i such that the spectral norm of the Jacobian of Attn at X is at least cR 2 exp(cR 2 ) for some constant c > 0. On the other hand, our bound scales in R 2 : this means that the local Lipschitz constant of SigmoidAttn is much lower than the worst local Lipschitz constant of SoftmaxAttn.

## Computational Complexity of Sigmoid and Softmax.

Table [1](#): Forward floating operations per token per attention head. n ctx and d head are the context length and head dimension respectively. ∆ measures the compute difference between sigmoid and softmax as a multiple of the floating operations for computing the attention logits. c accounts for causal (c = (n ctx + 1)/2n ctx ∼ 1/2), or standard (c = 1) attention. Typical values are taken from the 1B LLM results (n ctx = 2048, d head = 64). The difference in floating operations between Sigmoid and Softmax attention mechanisms is subleading (∼ 1%) compared to other operations in the attention mechanism like computing attention logits L (shown below), and the attention matrix × values operation. This analysis precludes hardware aware improvements (Section 4). Memory speed has not kept pace with recent gains in computation speed [(Choquette, 2023;](#b9)[Jouppi et al., 2017;](#b33)[Hannun et al., 2023)](#b25). Consequently, attention computations on modern architectures have been IO-bound by memory accesses [(Ivanov et al., 2021)](#b31). FLASHATTENTION [(Dao et al., 2022)](#b15) and FLASHATTENTION2 [(Dao, 2023)](#) address these shortcomings by optimizing GPU memory hierarchy utilization to accelerate attention computations. Motivated by the speed boost provided by these approaches, we develop FLASHSIGMOID, a hardware-aware implementation of SigmoidAttn. Like previous works, FLASHSIGMOID employs three core ideas:

Tiling: Divide and Conquer Approach to Attention: Similar to FLASHATTENTION and FLASHATTENTION2, FLASHSIGMOID processes input parts in parallel to compute attention outputs in blocks, efficiently combining partial results to generate the final attention output.

Kernel Fusion: Like FLASHATTENTION and FLASHATTENTION2, FLASHSIGMOID implements the computational steps of both forward and backward passes of SigmoidAttn as single GPU kernels, minimizing memory accesses and improving memory efficiency by avoiding materialization of intermediate activations on High-Bandwidth Memory (HBM). Figure [1](#): Average kernel speed-up for FLASHSIGMOID over FLASHATTENTION2 for sequence lengths 64-78k. Inference is 17.39% faster for self-attention and 18.76% for causal attention.

Training is 6.53% faster for self-attention and 9.46% for causal attention.

Activation Recomputation: The backward pass of sigmoid attention requires the sigmoid activation matrix, which, if materialized on GPU HBM, results in slower implementation and memory inefficiencies. FLASHSIGMOID addresses this by retaining only query, key, and value tensors for re-computation of the sigmoid activation matrix during the backward pass. Despite increased FLOPs, this approach proves faster in wall-clock time as well as more memory-efficient than the alterantive approach of materializing and retaining the attention matrix.

The forward and backward pass algorithms of FLASHSIGMOID can be found in App. F.1. Here, we highlight key differences between FLASHSIGMOID and FLASHATTENTION/FLASHATTENTION2.

The point-wise nature of SigmoidAttn results in a faster and more memory-efficient implementation by removing the need to compute the softmax normalization and materialize it to HBM. A reduction in the number of kernel dispatches also speeds up FLASHSIGMOID. Further, FLASHSIGMOID does not require accumulation and tracking of intermediate variables (row-sum and maximum of blocks) in the forward and backward passes which saves computation cost and reduces register pressure. We use sigmoid (x) = 0.5 • (1 + tanh (0.5 • x)) to optimize the sigmoid computation on GPU. The speed up in FLASHSIGMOID compared to FLASHATTENTION arises from optimizing hardware bottlenecks; theoretically, SigmoidAttn is slower than SoftmaxAttn (Sec. 3.3).

To measure the performance improvements of FLASHSIGMOID, we compare the timings of the kernels in its forward and backward passes against those of FLASHATTENTION2. The details of this benchmarking on H100 and A100 GPUs can be found in App. F.2. Measuring GPU computation time, we observe a 17.39% speed-up during inference and a 6.53% speed-up during training for attention over randomly initialized data on H100 GPU (Fig. [1](#)). In practice, these gains may be affected by other bottlenecks, such as movement of tensors between CPU or GPU memory, computations in other layers, and communication overhead in distributed training and inference. However, we demonstrate that FLASHSIGMOID speeds up training by ∼4% and inference by ∼8% in a realistic end-to-end setup. The details of wall-clock time improvements with FLASHSIGMOID are in App. F.3. We also note that practical machine learning workflows are dominated by inference rather than training.

## Experiments

To empirically validate SigmoidAttn, we evaluate across several domains: supervised image classification using vision transformers [(Dosovitskiy et al., 2021)](#b18), self-supervised image representation learning with SimCLR [(Chen et al., 2020;](#b7)[Zhai et al., 2023a)](#), Bootstrap Your Own Latent (BYOL) [(Grill et al., 2020;](#b22)[Busbridge et al., 2023)](#b3) and Masked AutoEncoders (MAE) [(He et al., 2022)](#b26) as well as automatic speech recognition (ASR) [(Synnaeve et al., 2020;](#b53)[Gulati et al., 2020b)](#) and auto-regressive language modeling (LM) [(Brown et al., 2020)](#b2). We also validate sequence length generalization on TED-LIUM v3 [(Hernandez et al., 2018)](#b27) for ASR and in small scale synthetic experiments in App. G.5.4. Across all these domains and algorithms, we demonstrate that SigmoidAttn matches the performance of SoftmaxAttn (Fig. [2](#fig_2) and 21), while offering training and inference speed-ups as highlighted in Sec. 4. Empirically we make the following observations: 0 200 400 600 Epoch -0.0 0.1 0.3 0.5 0.7 Train Loss BYOL ViT-B/16 0 100 200 300 Epoch 0.8 2.8 4.8 6.8 8.8 SimCLR ViT-B/16 0 100 200 300 400 Epoch 0.4 0.5 0.6 0.8 0.9 MAE ViT-L/16 0 100 200 300 Epoch 2.3 3.5 4.7 5.9 7.1 Train Loss Supervised ViT-B/16 100000 200000 300000 Steps 23.0 32.2 41.5 50.8 60.0 255M ASR Transformer 0 100000 200000 300000 Steps 1.9 2.3 2.7 3.2 3.6 1B LM (n = 2048) Softmax Sigmoid (1) SigmoidAttn is effective for vision tasks without a bias (except MAE), but relies on LayerScale to match the performance of the baseline SoftmaxAttn (Fig. [9](#)-a) in a hyper-parameter free manner. [3](#foot_0) All results presented for SoftmaxAttn also fairly add LayerScale unless specified.

(2) LM and ASR are sensitive to the initial norm ||σ(QK T / d qk )V ||. Modulation is required via (a) relative positional embeddings like ALiBi [(Press et al., 2022)](#b48), which reduces the initial attention norm by shifting logit mass to the zero regime under SigmoidAttn, (b) appropriate initialization of b to achieve the same effect -enabling usage of any positional embedding.

## Ablations

We begin with ablations to dissect the benefits of each of our introduced components. To gain intuition about SigmoidAttn, we developed a research-friendly auto-regressive (AR) LM training framework to measure all components of attention and validate the effects of LayerScale, LayerNorm applied to Q and K (QK norm), different positional embedding techniques, and initialization values for b.

## Mitigating Large Attention Norms

We train a single layer AR transformer block (E=3072, D_FF=12288) on the realnews split of C4 [(Raffel et al., 2020)](#b49). We train for 2 16 steps using a batch size of 6 and max sequence length of 4096 using a single cycle cosine learning rate (LR) schedule without weight decay. SigmoidAttn initially underperformed SoftmaxAttn when using absolute sinusoidal (SinCos) (Fig. [3](#fig_3)) or relative (Fig. [4](#fig_4)) positional embeddings (PE), which we attribute to high initial attention Frobenius norms, ∥σ(QK T / √ d)V ∥. A corresponding evolution of the attention distribution and sparsity can be seen in Appendix Fig. [31](#fig_3) and Fig. [32](#fig_3) on a synthetic task. To address these larger attention norms, we propose: (a) using ALiBi [(Press et al., 2022)](#b48) whose relative bias moves initial attention logit mass to the zero region under the sigmoid activation, producing equivalent train negative log-likelihoods (Fig. [5](#fig_5)); or (b) set the attention logit bias b to a negative offset proportional to the sequence length, b ∝ln n (see App. G.1.2 for an ablation on b). This enables the usage of other PE techniques like RoPE [(Su et al., 2024)](#) (Fig. [6](#fig_6)).    

$∈ {3 × 10 -4 , 1 × 10 -3 , 3 × 10 -3 , 1 × 10 -2 , 3 × 10 -2 , 1 × 10 -1 , 3 × 10 -1 }. LR sensitivity is defined as E η∈[a,b] [min(ℓ(A(η)), ℓ 0 ) -ℓ * ] where ℓ(A(η))$is the loss achieved by the learning algorithm A with LR η, ℓ 0 is the loss at initialization, and ℓ * is the loss achieved by the best LR. LayerScale is initialized at 10 -[foot_1](#foot_1) . Unlike vision tasks, where LayerScale improves performance (Fig. [9](#)-a), in LM, we observe that SoftmaxAttn slightly benefits from LayerScale, while the performance of SigmoidAttn remains largely unaffected.

Stability with QK Norm Theorem 3.2 indicates that the Jacobian of SigmoidAttn has favorable properties compared to SoftmaxAttn. We explore this by repeating the analysis of [Wortsman et al. (2023b)](#), as described in the LayerScale analysis, to investigate the impact of QK norm [(Dehghani et al., 2023)](#b16). For language modeling, both SigmoidAttn and SoftmaxAttn exhibit sensitivity to learning rate changes without QK norm. However, incorporating QK norm significantly stabilizes performance (Fig. [8](#)). In vision tasks, SigmoidAttn demonstrates robustness with and without QK norm (Fig. [9](#)-a) and without the need for n -α normalization from [Wortsman et al. (2023a)](#). Final loss use_layer_scale=False which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate use_layer_scale=True which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 7 10 8

Non-embed Params (M)

10 1 LR sensitivity which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax  Final loss qk_norm=False which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate qk_norm=True which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 7 10 8 Non-embed Params (M) 10 1 10 0 LR sensitivity which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax Figure 8: LR sensitivity QK norm ablation. 50 100 150 200 250 300 Epoch (a) 65 70 75 80 Test Top-1% +LayerScale,+QKNorm +LayerScale,-QKNorm -LayerScale,+QKNorm n -α ,-LayerScale,+QKNorm n -α ,-LayerScale,-QKNorm 50 100 150 200 250 300 Epoch (b) MHA Softmax MHA Sigmoid MQA Softmax MQA Sigmoid 50 100 150 200 250 300 Epoch (c) Softmax, +LayerScale,+QKNorm Sigmoid, +LayerScale,+QKNorm TanH, +LayerScale,+QKNorm ReLU, +LayerScale,+QKNorm GeLU, +LayerScale,+QKNorm ReLU 2 , +LayerScale, +QKNorm ReLU 2 , -LayerScale, -QKNorm Figure 9: ImageNet1k ViT-B/16 classification. (a) SigmoidAttn is robust without QK norm (+Lay-erScale, -QKNorm). Removing LayerScale reduces accuracy by 1.0% (-LayerScale, +/-QKNorm). n -α normalization (Wortsman et al., 2023a) underperforms without LayerScale. (b) SigmoidAttn multi-query attention (MQA) (Shazeer, 2019) with one head matches multi-head attention (MHA). (c) Sigmoid with LayerScale and QK norm performs comparably to other activations, except TanH. ReLU 2 (Hua et al., 2022) underperforms without LayerScale and QK norm.

Multi-query attention (MQA) In Fig. [9](#)-b we explore MQA [(Shazeer, 2019)](#b50) for vision using only one head for {K, V }. We find that both SigmoidAttn and SoftmaxAttn perform equally well with or without multiple heads even at the small scale of ViT-B/16.

Activation Function Ablations As in [Wortsman et al. (2023a)](#), various activation functions, when combined with LayerScale and QK norm, perform equally well for vision tasks (Fig. [9-c](#)).

However, for sequence-critical tasks like ASR, activation functions such as ReLU pose instabilities and underperform. In the same figure, we also compare to the ReLU 2 proposal from Hua et al. ( [2022](#)) and find that it underperforms without LayerScale and QK norm.

## Supervised Image Classification

Vision transformers [(Dosovitskiy et al., 2021)](#b18) extend transformers [(Vaswani et al., 2017)](#b55) to treat K × K image grids as disparate tokens. All tokens are refined through sequential layers of selfattention, pooled using a CLS token or global average pooling layer, and optimized using the negative log likelihood, ln p(y|x). We train ViT-B/16 models using R 224×224×3 images for 300 epochs using the recipe provided in App. G.2.4. We use the same set of training hyper-parameters for both SoftmaxAttn and SigmoidAttn, changing only the activation function between trials. The train negative log-likelihood is reported in Fig. [2](#fig_2) and the test top-1% is reported in Fig. [21](#fig_25). We find that SigmoidAttn matches both the training dynamics and the evaluation performance of SoftmaxAttn. 

## Automatic Speech Recognition (ASR)

We benchmark ASR using LibriSpeech data [(Panayotov et al., 2015)](#b45) on 100h and 960h settings of paired speech and text transcriptions. Our PyTorch implementations of encoder-based vanilla transformer [(Synnaeve et al., 2020)](#b53) and conformer [(Gulati et al., 2020a)](#) are trained with Connectionist Temporal Classification (CTC) [(Graves et al., 2006)](#b21) w/ BF16 mixed precision, w/o QK norm and w/o LayerScale. After extensively tuning SoftmaxAttn baselines, we switch to SigmoidAttn per (3) without any other changes. We investigate the effects of post/pre-LayerNorm, model depth, optimizer type, small data regime, and connection to local attention, with details in App. G.4. Our main findings are: i) CAPE (Likhomanenko et al., 2021) PE is the most unstable for SigmoidAttn; ii) post-LayerNorm models with SoftmaxAttn are hard to match with stable SigmoidAttn; iii) w/o QK norm SigmoidAttn is unstable and significant spikes happen in both gradient norms and training loss; iv) LayerScale is needed for generalization; v) learnable bias b = -10 gives no loss and gradient norms spikes while matching the SoftmaxAttn (which does not benefit from the improved throughput of FLASHSIGMOID); vi) adding a learnable bias, b = -5, to Q instead of the attention logits also solves the initial large attention norms for CAPE and ALiBi but not for RoPE; vii) b =log n gives rare (2-5 times) marginal gradient norms spikes with smooth loss while matching SoftmaxAttn.

Table 2 shows the main result for pre-LayerNorm transformers with CAPE, RoPE, and ALiBi, where SigmoidAttn uses LayerScale, QK norm, b =log n, and no sequence normalization. The bias is ablated with learnable bias (one per layer) in attention or Q with or without sequence normalization. SigmoidAttn is stabilized with bias while matching SoftmaxAttn, and b =log n works well. In most cases, bias allows generalization to longer sequences without sequence normalization, except for RoPE where it helps for longer sequences but hurts overall performance. 

## Autoregressive Large Language Modeling

We initially iterated at the 85M scale, as it served as a proxy for larger scale training. Our findings show that: i) attention bias is required for stability, which can be learnable, but setting it tolog(n),

where n is the maximum training sequence length of 4096, works well and is faster; ii) RoPE is more challenging to stabilize; iii) the final setting exhibits smooth loss curves, but still shows gradient norm fluctuations. We then turn our attention to validating SigmoidAttn at scale.

We train a 1B language model using the Llama2 [(Touvron et al., 2023)](#b54) recipe with ALiBi instead of RoPE positional embedding, and the RedPajama (Computer, 2023) dataset (see App. G.3). At sequence length 4096, SigmoidAttn achieves a 1.23× step-time improvement over SoftmaxAttn in JAX without FLASHATTENTION (Tab. 3). All LLMs are trained using the AXLearn framework, which include the recipe and SigmoidAttn implementation.[foot_2](#foot_2)

SoftmaxAttn and SigmoidAttn have matching train and validation NLL at 85M (Fig. [26](#fig_6)) and at 1B scale when using 2048 sequence length (Fig. [2](#fig_2)). However, a slight disparity is observed at 1B scale when using 4096 sequence length, which we leave for future investigation (more details in App. G.3).

## Related Work

Recent studies in supervised image classification [(Wightman et al., 2021)](#b56) and self-supervised learning (SSL), including approaches like SigLIP [(Zhai et al., 2023b)](#), are shifting large-scale machine learning training from output conditional categorical distributions, traditionally parameterized by softmax functions, to richer pointwise Bernoulli conditionals parameterized by sigmoid functions. In this study, our focus shifts to refining the model's internal mechanics, specifically by substituting the softmax component of the attention mechanism with a pointwise sigmoid function.

Previous work has explored the replacing softmax with the ReLU activation in both practical [(Shen et al., 2023;](#)[Hron et al., 2020)](#b28) and theoretical settings [(Bai et al., 2023;](#b1)[Fu et al., 2023)](#b19). Other works explores using the ReLU 2 activation [(Hua et al., 2022)](#b29), exploring purely linear attention [(Katharopoulos et al., 2020;](#b34)[Lu et al., 2021;](#b41)[Koohpayegani & Pirsiavash, 2024)](#b36) or cosine-similarity based attention [(Luo et al., 2018;](#b42)[Liu et al., 2022)](#b39). Our work builds upon these explorations, particularly [Wortsman et al. (2023a)](#), which replaces softmax with various activation functions scaled by n -α , where n corresponds to the sequence length and α, a hyper-parameter. However, we find that their formulation does not match expected performance without proper b initialization and the use of LayerScale (Fig. [9](#)-a, App. G.1.1).

## Conclusion

In this work, we present a comprehensive theoretical and empirical study of sigmoid attention as an alternative to softmax attention in transformers. We prove that transformers with sigmoid attention are universal function approximators with improved regularity, and identify LayerScale and prevention of large initial attention norms as key factors for successful training. We introduce FLASHSIGMOID, a memory-efficient variant providing a 17% inference kernel speed-up. Extensive experiments across language, vision, and speech demonstrate that properly normalized sigmoid attention matches softmax attention performance on various tasks and scales. Our findings establish sigmoid attention as a viable alternative, unifying prior work and establishing best practices for its application in transformers. 

## H Contributions

## A Limitations

While our work demonstrates that SigmoidAttn can serve as a viable drop-in replacement for SoftmaxAttn in many domains and scales, there are a few key limitations to note:

(1) In large-scale (1B parameter, 4096 context length) language modeling, we observed some gradient norm spikes and a slight performance gap between SigmoidAttn and SoftmaxAttn (Table [3](#tab_7)). While runs at smaller context lengths (1B parameter, n=2048) were stable and matched SoftmaxAttn performance, further research is needed to fully close the performance gap and ensure training stability for very large language models using SigmoidAttn.

(2) Our theoretical analysis proves that transformers with SigmoidAttn are universal function approximators and have improved regularity compared to SoftmaxAttn. However, the bounds we derive, while tighter than those for SoftmaxAttn, may not be maximally tight.

There could be room for further theoretical refinements.

(3) We focused our empirical evaluation on standard benchmarks in language, vision, and speech domains. Performance on more niche or emerging applications remains to be validated. ( [4](#formula_2)) In automatic speech recognition experiments, we observed that SigmoidAttn can be sensitive to the choice of positional embeddings and may require careful initialization of the attention bias term to ensure stable training. Specifically, we found that the CAPE positional embedding was the most unstable for SigmoidAttn. Further work is needed to develop robust initialization schemes that work well across different positional embeddings. Moreover we found that w/o QK norm or with post-LayerNorm SigmoidAttn is unstable and can underperforms SoftmaxAttn, thus further investigation is needed.

(5) FLASHSIGMOID demonstrates promising inference and training speed-ups by exploiting SigmoidAttn's simpler kernel structure compared to SoftmaxAttn. However, realizing these gains at scale in distributed training setups may require additional engineering to optimize communication bottlenecks.

Despite these limitations, we believe this work establishes a strong foundation for SigmoidAttn, unifying prior art and demonstrating its potential as a drop-in SoftmaxAttn replacement. We hope our theoretical grounding and empirical results motivate further research into this simple yet effective architectural variation.

## B Broader Impact

The development of efficient and theoretically grounded attention mechanisms has the potential for significant positive impact across a range of applications. By establishing SigmoidAttn as a viable alternative to SoftmaxAttn, our work expands the toolkit of architectural choices available to researchers and practitioners. Positive impacts of this work may include:

(1) Improved computational efficiency: FLASHSIGMOID's faster kernel implementation could lead to more efficient training and inference for attention-based models, reducing energy consumption and enabling deployment on resource-constrained devices. This could democratize access to powerful models.

(2) Theoretical understanding: Our universal approximation results and tighter bounds on the regularity of SigmoidAttn contribute to a deeper theoretical understanding of this key component. A stronger theoretical foundation can guide principled model design and architectural search.

(3) Application-specific benefits: Across language, vision, and speech domains, SigmoidAttn's performance could translate into improved user experiences, such as more natural language interactions, enhanced image understanding, and robust speech recognition. These advancements could have positive societal impacts, such as improved accessibility tools and more effective educational technologies.

However, as with any foundational machine learning advance, there are also risks of negative impacts that must be considered and mitigated:

(1) Fairness and bias considerations: As with any machine learning model, it is important to carefully evaluate SigmoidAttn based models for fairness and potential biases when applied to sensitive use cases. The unique properties of SigmoidAttn may have unexpected interactions with data biases. Researchers and practitioners should follow best practices for auditing and mitigating unwanted biases to ensure equitable outcomes.

(2) Environmental impact: While FLASHSIGMOID is more computationally efficient than FLASHATTENTION, the overall trend of scaling up attention-based models has significant energy costs. Further efficiency improvements and the use of renewable energy sources are important to mitigate environmental harms.

We believe that the benefits of SigmoidAttn outweigh the risks, but it is crucial for the research community to actively consider and address these potential negative impacts. By doing so, we can work towards a future where the efficiency and expressivity of SigmoidAttn are used for societal benefit.

## C Universal Approximation Property for Sigmoid Attention

This section is dedicated to the proof for the Universal Approximation Property for attention equipped with sigmoid nonlinearity. The proof follows closely the one provided in [Yun et al. (2020, Sec. 3)](#), of which we inherit much of the notation, and we encourage the interested reader to refer to the original source for a more comprehensive understanding of its details. Here we first provide context by outlining the main steps in the original proof, before proceeding to adapt its key components to the SigmoidAttn case.

The proof aims at showing that a transformer network can approximate to arbitrary accuracy any continuous, permutation-equivariant function with compact support. The proof is constructive in nature, in that it explicitly defines the architecture (and particularly, the sequence of self-attention and feed-forward layers) that can approximate a given target function. To do so, it proceeds in steps (see [Yun et al. (2020, Sec.](#) 3.2)):

(1) prove that any continuous function with compact support can be approximated to arbitrary accuracy by a piecewise constant function

(2) prove that an aptly-constructed modified transformer network, (where the softmax nonlinearity is substituted with a hardmax nonlinearity), can exactly represent such piecewise constant function. This step is further divided into three sub-steps (see [Yun et al. (2020, Sec. 4)](#)):

(a) prove that a series of feed-forward layers can quantize any input to a specific discretization grid in the compact domain (b) prove that a series of self-attention layers can implement a contextual mapping (see [Yun et al. (2020, Def.](#) 3.1)) (c) prove that a series of feed-forward layers can map the output of the contextual mapping to the desired output of the target piecewise-constant approximation

(3) prove that a (classical) transformer network can approximate such modified transformer network to arbitrary accuracy Fortunately, some of the steps outlined above do not rely on a specific nonlinear function being used within the attention mechanism, and can be directly reused in our proof, virtually unchanged. Notice however that Steps (2-b) and ( [3](#)) are directly impacted by modifications to the attention layer, and hence require adaptation in our case. This is the focus of the next sections.

## C.1 Proof of Step (3): Sigmoid Transformers can Approximate Modified Sigmoid Transformers

In [Yun et al. (2020)](#b60), to implement contextual mappings, the authors rely on a modified version of transformers, for the sake of simplifying the analysis. In their modified version, the (row-wise) softmax operation is substituted with a (row-wise) hardmax operation. This substitution is valid because a classical transformer can still be made arbitrarily close to such modified transformer, in light of the fact that softmax(λX) λ→∞ ----→ hardmax(X). ( [10](#)) In our proof, we follow a similar strategy to define our modified sigmoid transformer (and in particular, its self-attention mechanism). We have that

$σ(λX) λ→∞ ----→ H(X),(11)$where σ(x) = (1 + e -x ) -1 is the (elementwise) sigmoid function, while

$H(x) =    1 x > 0 1 2 x = 0 0 x < 0 (12)$denotes the (elementwise) Heaviside step function. This allows us to define our modified sigmoid self-attention layer, as follows.

Definition C.1 (Modified sigmoid self-attention layer). Given an input X ∈ R d×n , the action of a modified sigmoid self-attention layer with shifts and a single one-dimensional head is defined as

$X → X + ψ(X; q, b q , k, b k , v, o), where ψ(X; q, b q , k, b k , v, o) = o v T X H q T X -b T q T k T X -b T k (13)$with q, k, v ∈ R d representing the query, key, and value vectors, b q , b k ∈ R n the corresponding query and key bias vectors, while o ∈ R d denotes the output vector.

Analogously to ( [10](#)), ( [11](#formula_11)) guarantees that sigmoid attention can approximate modified sigmoid attention by simply increasing the magnitude of its inner parameters.

Here and in the following, the length of the input sequence is denoted as n, while d represents the dimensionality of the tokens. Notice that we are considering the input tensor X ∈ R d×n , (as opposed to ∈ R n×d ) to better align out notation with the one used in [Yun et al. (2020)](#b60).

## C.2 Proof of Step (2-b): Modified Sigmoid Transformers can Implement Contextual Mappings

The core of the proof consists in showing how, by opportunely combining the operations in ( [13](#)), one can build an architecture capable of implementing a contextual mapping. For completeness, we report next the definition of such a map (see also [Yun et al. (2020, Def.](#) 3.1)).

Definition C.2 (Contextual mapping). A map q : L → R n from a finite set L ⊂ R d×n is said to be a contextual mapping if both the following conditions hold:

$(i) q i (X) ̸ = q j (X), ∀i ̸ = j and ∀X ∈ L (ii) q i (X) ̸ = q j (X ′ ), ∀i, j and ∀X, X ′ ∈ L, with X ̸ = X ′$where q i (X) denotes the i-th component of q(X).

Namely, a contextual mapping is such that it transforms each token in an input sequence to a value depending uniquely on the whole sequence. By satisfying this property, we can ensure that any element of the quantization of the input domain (achieved by Step (2-a)) can be mapped to a unique identifying value (depending on the whole input) via a sequence of modified sigmoid self-attention layers. It is then up to the MLP (in Step (2-c)) to correctly map this value to the corresponding output value in the piece-wise constant approximation.

In particular, after defining a uniform discretization (characterized by the parameter δ) of the unitary hypercube

$[0, 1] d ⊂ R d , namely G δ := {g : g i ∈ {0, δ, 2δ, . . . , 1 -δ}, ∀i = 1 . . . d},(14)$we consider as input a tensor X (composed of columns

$X = [x i ] n i=1 ) such that X ∈ L := {X : x i ∈ G δ ∀i = 1 . . . n, and x i ̸ = x j ∀i ̸ = j} ⊂ R d×n ,(15)$that is, a 2D tensor whose columns are element of the discretization G δ , and that all differ from each other (at least for one element). We want to build a contextual mapping acting on L, by stacking layers parameterized according to Def. C.1. In App. C.2.1 we define the basic building blocks of our architecture; in App. C.2.2 we describe how to stack them, and the effect the architecture has on a given input; finally, in App. C.2.4 we prove that this architecture indeed implements a contextual mapping.

## C.2.1 Basic Building Blocks of Contextual Mapping

The strategy we follow to assemble a contextual mapping consists in sequentially looking at each column of the input, progressively updating and storing information regarding its content in a uniquely identifiable manner, and finally broadcasting this information back to every element in the sequence. The difficulty lies in the fact that each of these updates must be carried on while relying solely on applications of the modified SigmoidAttn layer in Def. C.1. In the following, we describe how we can tweak its parameters to achieve exactly this.

From d-dimensional quantized vectors to scalars As a first simplification, we can get rid of the d-dimension in the X tensor by mapping each of its columns to a corresponding identifying scalar, uniquely defined by the specific column components. This step is also performed in [Yun et al. (2020, App. B.5)](#), and can be achieved rather straightforwardly, by defining

$v ≡ q ≡ k ≡ u := [1, δ -1 , δ -2 , . . . , δ -d+1 ] T . (16$$)$Notice in fact that, since each column x i belongs to G δ , it can equivalently be written in the form

$x i = δ • [id 0,i , id 1,i , . . . , id d-1,i ]$T , where id j,i ∈ {0, 1, 2, . . . , δ -1 -1} represents the (indexed) coordinate of the discretization along the j-th dimension. Scalar-multiplying X by u in ( [16](#formula_17)), then, turns this tuple of indices into a single one, in a bijective fashion[foot_3](#foot_3) .

This allows us to equivalently consider a single vector u T X ∈ R n , rather than the whole tensor X ∈ R d×n in the remainder of our analysis. Analogously, choosing o ≡ e 0 := [1, 0, . . . , 0] T in ( [13](#)) constraints the effect of the layer application to impact only the first row of the tensor: the goal is then to store in this row the result of the target contextual mapping q in Def. C.2. To slim our notation, in the following we often refer to u T X as the vector l ∈ R n , with components l i .

In light of the simplification above, we can rewrite (13) more compactly, as follows:

$ψ(X; q = k = v ≡ u, o ≡ e 0 ; b q , b k ) = e 0 l T H ((l -b q ) ⊗ (l -b k ))(17)$Notice that, since the elements of both X and u are always non-negative, so are those of l, too. Moreover, since we are interested in permutation-equivariant functions with respect to the columns of X, without loss of generality we can consider the elements of l = u T X to be ordered: 0 ≤ l i < l j , ∀i < j.

Selective shift operation for sigmoid attention Since we aim to recover a contextual map by sequentially updating the elements of l, we proceed by designing a modification of ( [17](#formula_20)) which affects only a certain selected element at a time. This is were our second simplification comes into play, and this time it pertains the roles of the bias vectors b q and b k . Since l ≥ 0, these vectors have the effect of tweaking the sign of the inner arguments of the Heaviside function in ( [17](#formula_20)), hence directly impacting when its application outputs 0 or 1. By aptly selecting the values of b k and b q , then, we can explicitly decide when a specific layer triggers an update, which elements are affected by the update, and what elements to consider to compute the update itself.

More in detail, take b q = 1b q and b v = 1b v , for some scalars b q , b v , and with 1 being the all-one vector. Plugging this into (17), we have

$ψ(X; b q , b k ) := ψ(X; q = k = v ≡ u, o ≡ e 0 , b q = 1b q , b k = 1b k ) = e 0 l T H ((l -1b q ) ⊗ (l -1b k )) = e 0 i:li<bv l i if l j < b k i:li>bv l i if l j > b k ;(18)$notice how b q determines what elements of l compose the update (as it impacts the indices considered in the sum), while b k defines the elements impacted by the update itself [7](#foot_4) . If we opportunely combine four modified sigmoid self-attention heads ψ(X; b q , b k ), we recover, for a given index i = 0 . . . δ -d -1,

$Ψ (i) (X) :=X + 1 2 c     ψ X; b q = 0, b k = i -1 2 δ -ψ X; b q = 0, b k = i + 1 2 δ -ψ X; b q = b k = i + 1 2 δ + ψ X; b q = i + 1 2 , b k = i -1 2 δ     =X + 1 2 ce 0 l T     H l ⊗ l -i -1 2 δ -H l ⊗ l -i + 1 2 δ -H l -i + 1 2 δ ⊗ l -i + 1 2 δ +H l -i + 1 2 δ ⊗ l -i -1 2 δ     =⇒Ψ (i) 1,j (X) = X 1,j + c k:l k >iδ l k if l j = iδ 0 otherwise =⇒Ψ (i) k>1,j (X) = X k,j ,(22)$where c ≡ c(δ, d, n) is a multiplicative constant which will be chosen later.

The operator assembled in ( [22](#formula_22)) defines the basic layer of the architecture that we use in our proof. Notice Ψ (i) (X) has the effect of modifying only the column x j which has index l j = u T x j = iδ (if at all present in the input X). This layer covers a similar role to the selective shift operation 

## C.2.2 Result of Applying a Sequence of Selective Shifts

Ultimately we want to show how, by stacking a sequence of selective shift layers ( [22](#formula_22)) for increasing i = 0 . . . δ -d -1 and one additional global shift, we can build an architecture capable of representing

$lj < b k lj > b k H (l ⊗ (l -1b k )) =   0 • • • 0 1 • • • 1 . . . . . . . . . . . . . . . . . . 0 • • • 0 1 • • • 1   .(19)$This shows how, by modifying b k , one can decide which columns will receive an update: namely, all those with index lj > b k . By combining two such operators with b

$k = i -1 2 δ and b k = i + 1 2 δ, we then recover lj = iδ H l ⊗ l -1 i -1 2 δ -H l ⊗ l -1 i + 1 2 δ =   0 • • • 0 1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 1 0 • • • 0   ,(20)$which allows us to limit the update to only one specific column: the one with index lj = iδ.

The parameter bq acts analogously, but varies the output of the Heaviside function as we move down the rows, rather than the columns. The same operator as in ( [20](#formula_24)), but with bq = i + 1 2 δ gives us in fact:

$lj = iδ H l -1 i + 1 2 δ ⊗ l -1 i -1 2 δ -H l -1 i + 1 2 δ ⊗ l -1 i + 1 2 δ =         0 • • • 0 -1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 -1 0 • • • 0 0 • • • 0 1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 1 0 • • • 0         lj < iδ lj > iδ .(21)$Finally, ( [22](#formula_22)) can be recovered by combining ( [20](#formula_24)) and ( [21](#formula_25)): this has the effect of removing the -1's in [(21)](#b72).

a contextual mapping. As a preliminary step, in this section we provide an explicit formula for the result of applying such an architecture. Once again, we are proceeding analogously to [Yun et al. (2020, App. B.5.1)](#).

After the first selective shift application Consider a quantized input sequence X ∈ L as defined in [(15)](#b68), with its columns ordered according to their scalar indices l = u T X. The sequence of selective shift layers Ψ (0) , Ψ (1) , . . . initially has no effect on the input itself, and it leaves it unchanged until we hit the layer corresponding to the index of the first column in the input, Ψ ( î) , where l 1 = u T x 1 = îδ. At this point, following ( [22](#formula_22)), the first column of the input is modified into

$x 1 → Ψ ( î) |,1 (X) = x 1 + ce 0 k:l k >l1 l k = x 1 + ce 0 n k=1 l k -l 1(23)$while the other columns are still left untouched. In the following, we compactly refer to the quantities

$n k=1 l k -l i as s i : s = [s 1 , s 2 , . . . , s n ] T := n k=1 l k -l 1 , n k=1 l k -l 2 , . . . , n k=1 l k -l n T . (24$$)$According to ( [23](#formula_26)), the index l 1 of column x 1 is then analogously mapped to

$l 1 = u T x 1 → l1 := u T Ψ ( î) |,1 (X) = u T x 1 + cs 1 = l 1 + cs 1 .(25)$Notice that, by choosing c > 1, we can ensure

$c > 1 =⇒ l1 > l 1 + n k=1 l k - l 1 > n k=1 > l i ∀i,(26)$and particularly l1 > l 2 , implying that at the next (effective) application of the selective shift operation, this term, too, will contribute to the update.

Subsequent selective shift applications Following similar considerations, the next effective update will be applied by the layer Ψ ( î) with l 2 = u T x 2 = îδ. At this point, the second column index is updated as follows:

$l 2 = u T x 2 → l2 :=u T Ψ ( î) |,2 (X) = u T x 2 + c k:l k >l2 l k + l1 =l 2 + c n k=1 l k -l 2 - l 1 + l 1 + cs 1 = l 2 + cs 2 + c 2 s 1 (27$$)$where l1 is also included in light of ( [26](#formula_30)), and we used the definitions ( [24](#formula_27)) and ( [25](#formula_29)). Continuing to apply Ψ (i) (X), for increasing i, and unrolling the recursion, we recover [)](#) which eventually allows us to write the general formula 8

$l3 = l 3 + c n k=1 l k -l 1 -l 2 -l 3 + l1 + l2 = l 3 + cs 3 + c 2 (s 2 + s 1 ) + c 3 s 1 l4 = l 4 + c n k=1 l k -l 1 -l 2 -l 3 -l 4 + l1 + l2 + l3 = l 4 + cs 4 + c 2 (s 3 + s 2 + s 1 ) + c 3 (s 2 + 2s 1 ) + c 4 s 1 l5 = l 5 + c n k=1 l k -l 1 -l 2 -l 3 -l 4 -l 5 + l1 + l2 + l3 + l4 = l 5 + cs 5 + c 2 (s 4 + s 3 + s 2 + s 1 ) + c 3 (s 3 + 2s 2 + 3s 1 ) + c 4 (s 2 + 3s 1 ) + c 5 s 1 . . . (28$$lj := l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 , j = 1 . . . n.(29)$
## C.2.3 Result of Applying One Last Global Shift Layer

After the last selective shift layer, the original input X has been mapped to a modified one X whereby each column xj is characterized by the index lj = u T xj given in [(29)](#b86). Remember our goal is to recover a contextual mapping, but notice that these lj indices are not uniquely defined by the input 9 ; in other words, they do not satisfy property (2) in Def. C.2. The only exception to this is the last index ln , as (loosely speaking) it has "seen" all the previous updates -and indeed in App. C.2.4 we prove this rigorously, under some assumption on the yet-undefined coefficient c(δ, d, n).

A straightforward way to recover a one-to-one mapping for the whole sequence, then, is to update every index lj via a quantity directly depending on ln . This is precisely what the last global shift layer Ψ(X) aims to accomplish. This last layer is also defined starting from the simplified modified sigmoid attention ( [18](#formula_21)), by picking b k = 0 and b q = c(δ, d, n) n + 1 2 δ: if, for any input, we can guarantee that lj ≤ c(δ, d, n) n δ j < n and ln > c(δ, d, n) n δ, (30) then the application of the global shift layer would result in 10 :

$Ψ( X) := X + c n+1 ψ X; b q = c n + 1 2 δ, b k = 0 =⇒ Ψ1,j ( X) = X1,j + c n+1 ln =⇒ Ψk>1,j ( X) = Xk,j .(32)$The global shift ( [32](#formula_35)) is the last layer we need to define our candidate contextual mapping. Collecting the results from this section together, our architecture is defined by sequentially composing the selective shift layers with the global shift one,

$Ψ(X) := Ψ • Ψ (δ -d -1) • • • • • Ψ (2) • Ψ (1) (X).$(33) After being scalar-multiplied by u, this results in a sequence q(X) := u T Ψ(X) = l + c n+1 1 ln (34) which we aim to prove is a contextual mapping. This is shown in the next section.

8 From ( [28](#formula_33)), we can notice that, for a given lk , the coefficients a (k) i,j appearing in front of the various s k-i for each of the c j terms, are first given by a list of ones, a

i,1 = 1, then a list of increasing numbers a

$(k) i,2 = i =⇒ a (k) -,2 = cumsum(a (k) -,1 ), then a list of triangular numbers a (k) i,3 = i(i + 1)/2 =⇒ a (k) -,3 = cumsum(a (k)$-,2 ), and so on:

$a (k) -,j = cumsum(a (k)$-,j-1 ). The result of iterated applications of cumsum, starting from an all-one vector, can be compactly described via the binomial coefficient: we have in fact

$ai,j = [cumsum j ([1, 1, . . . ])]i = i + j -2 j -1 .$The actual formula ( [29](#formula_34)) can be recovered after a few algebraic steps, by rearranging the summation indices. 9 To convince ourselves of this, it suffices to look at the formula for ( [25](#formula_29)): two sequences with different elements l ̸ = l ′ , but such that l1 = l ′ 1 and s1 = s ′ 1 (that is, with n i=1 li = n i=1 l ′ i ) would map to the same l1 = l′ 1 . 10 As in footnote 7, this is also better seen by considering the resulting modified sigmoid attention matrix. With b k = 0 and bq = c(δ, d, n) n + 1 2 δ, in fact, if condition ( [30](#)) is verified, this matrix is given by

$H l -1 c n + 1 2 δ ⊗ l =     0 • • • 0 . . . . . . . . . 0 • • • 0 1 • • • 1     lj, j < n ln .(31)$
## C.2.4 A Sequence of Selective Shifts Followed by a Global Shift Produces a Contextual Mapping

To complete the proof, it remains to show that the recovered sequence (34) represents a contextual mapping and, in particular, that it is (i) one-to-one in L, and that (ii) all of its elements are distinct for different inputs. To do so, we need a few preparatory lemmas. The first few are needed to show that each of the basic components of ( [34](#)) is indeed a one-to-one map.

Lemma C.3. The map l → s in ( [24](#formula_27)) is one-to-one.

Proof. The target map can be compactly represented as a linear operator S:

$l → s := 1 n k=1 l k -l = (1 ⊗ 1 -I)l =: Sl(35)$which is invertible [11](#foot_5) , denoting that l → s is bijective.

Lemma C.4. The map l → ln in ( [29](#formula_34)) is one-to-one, under the condition

$c(δ, d, n) > (n -1)(δ -d -1) n -1 n-1 2 . (36$$)$Proof. Consider two vectors of column indices l, l ′ differing for at least one element. We have by definition ( [29](#formula_34)) that

$ln -l′ n = (l n -l ′ n ) + c(s n -s ′ n ) + n-2 i=0 c i+2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 )(37)$By absurd, assume ln -l′ n = 0 even though ∃i :

$l i ̸ = l ′ i .$We have then that it must hold

$(l ′ n -l n ) = c(s n -s ′ n ) + n-2 i=0 c i+2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) = c (s n -s ′ n ) + n-2 i=0 c i+1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 )(38)$Notice that, for c(δ, d, n) large enough, the right-hand side does not have enough granularity to counter the left-hand side: in fact, since l n ∈ {0, δ, 2δ, . . . , δ -d+1 -δ}, the left-hand side can attain values l ′ nl n ∈ {0, ±δ, ±2δ, . . . , ±(δ -d+1δ)} (39) while the former, in light of the presence of the c(δ, d, n) factor, can only attain values ∈ {0, ±cδ, ±2cδ, . . . }. Picking c > δ -d -1, then, ensures that equality between the two sides of (38) can only be achieved if they are both 0. In this case, we need to impose

$c(s ′ n -s n ) = n-2 i=0 c i+1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) ⇐⇒ s ′ n -s n = c n-2 i=0 c i n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) .(40)$Similarly, notice that[foot_6](#foot_6) , ∀i,

$|s i -s ′ i | = n k=1 (l k -l ′ k ) -(l i -l ′ i ) = n k=1,k̸ =i (l k -l ′ k ) < (n -1)(δ -d+1 -δ),(41)$implying that s ′ ns n ∈ {0, ±δ, ±2δ, . . . , ±(n -1)(δ -d -1)δ}. Again, by picking c(δ, d, n) > (n -1)(δ -d -1) we ensure that the right-hand side does not have enough granularity, and hence

$c(δ, d, n) > (n -1)(δ -d -1) =⇒ s ′ n -s n = 0, (42) implying c n-2 i=0 c i n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) = 0 ⇐⇒ n-2 k=0 k 0 (s ′ k+1 -s k+1 ) = c n-2 i=1 c i-1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) ⇐⇒ n-2 k=0 (s ′ k+1 -s k+1 ) = c n-2 i=1 c i-1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) .(43)$Following a similar reasoning as the one applied above shows us that picking

$c(δ, d, n) > (n -1) 2 (δ -d -1) =⇒ n-2 k=0 (s k+1 -s ′ k+1 ) = 0,(44)$and requires us to satisfy

$c n-2 i=1 c i-1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) = 0 ⇐⇒ n-2 k=1 k 1 (s ′ k -s k ) = c n-2 i=2 c i-2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) ⇐⇒ n-2 k=1 k(s ′ k -s k ) = c n-2 i=2 c i-2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) .(45)$Once again, then, by choosing

$c(δ, d, n) > (n -2)(n -1) 2 2 (δ -d -1) =⇒ n-2 k=1 k(s k -s ′ k ) = 0. (46$$)$This reasoning can be repeated recursively: at each step i of the recursion, by imposing a stricter and stricter bound on c(δ, d, n) we gain more and more conditions that the quantity s ′s needs to satisfy:

$c(δ, d, n) > (n -1)(δ -d -1) n-2 k=i k i =⇒ n-2 k=i k i (s k-i+1 -s ′ k-1+1 ) = 0. (47$$)$Notice that, every time we increase i = 0 . . . n -2, these conditions involve one less term s k-i+1s ′ k-i+1 , k = i . . . n -2: if we were to collect all these conditions within a single linear system, the system would have an upper-triangular structure, and hence be non-singular. This implies that for the set of n independent conditions on ss ′ to hold (we have n -1 in ( [47](#formula_55)), plus one more in ( [42](#))), the only possibility is that s ≡ s ′ . Because of Lemma C.3, though, this also implies l ≡ l ′ : we have finally reached a contradiction, and proven that indeed l → ln is one-to-one, under an opportune condition on c(δ, d, n). Such condition can be promptly recovered[foot_7](#foot_7) by ( [47](#formula_55)):

$max i=0...n-2 n-2 k=i k i = max i=0...n-2 n -1 i + 1 = n -1 n-1 2 . (48$$)$Substituting this in (47), we recover that it suffices to impose

$c(δ, d, n) > (n -1)(δ -d -1) n -1 n-1 2 . (49$$)$The next few lemmas are needed to bound the elements in the lj sequence, which in turn are used to prove property (ii) in Def. C.2.

Lemma C.5. lj in ( [29](#formula_34)) is an increasing sequence.

Proof. This can be proven directly: we have in fact, by definition ( [29](#formula_34)),

$lj > lj-1 ⇐⇒ l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 > l j-1 + cs j-1 + j-3 i=0 c i+2 j-3 k=i k i s k-i+1 combine sums ⇐⇒ (l j -l j-1 )(1 -c) + j-2 i=0 c i+2 j -2 i s j-1-i > 0 j-2 i ≥ 1, c i+2 ≥ c 2 ⇐= (l j -l j-1 )(1 -c) + c 2 j-2 i=0 s j-1-i > 0 (24) ⇐⇒ (l j -l j-1 )(1 -c) + c 2 j-2 i=0 n k=1 l k -l j-1-i > 0 ⇐⇒ (l j -l j-1 )(1 -c) + c 2 (j -1) n k=1 l k - j-1 k=1 l k > 0 ⇐⇒ (1 -c)l j + (c -1)l j-1 + c 2 (j -2) n k=1 l k + c 2 n k=j l k > 0 ⇐⇒ (c 2 -c + 1)l j + (c -1)l j-1 + c 2 (j -2) n k=1 l k + c 2 n k=j+1 l k > 0(50)$Already with c > 1, all the coefficients are positive (and at least one is non-zero), implying that the condition above is always satisfied and that indeed lj is an increasing sequence.

Lemma C.6. Under constraint (36), each term lj , j > 1 in ( [29](#formula_34)) is bounded from below by lj > c j δ, and each term lj , 1 < j < n is bounded from above by lj < c j+1 δ.

Proof. We start by proving the lower bound. By definition ( [29](#formula_34)), we have

$lj = l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 = l j + cs j + c j s 1 + j-3 i=0 c i+2 j-2 k=i k i s k-i+1 . (51$$)$Since by assumption l j is an ordered sequence without repetitions, for j > 1 we necessarily have l j > l 1 ≥ 0, and hence l j ≥ δ. All the other terms in ( [51](#formula_62)) are non-negative, so we can safely claim that lj

$≥ δ + c j δ > c j δ ∀j > 1,(52)$which confirms the lower bound.

For the upper bound, we start again from the definition of lj :

$lj = l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 < (δ -d -1)δ + c(n -1)(δ -d -1)δ + s 1 j-2 i=0 c i+2 j -1 i + 1 ≤ (n -1)(δ -d -1) j -1 j-1 2 δ j i=0 c i = (n -1)(δ -d -1) j -1 j-1 2 δ 1 -c j+1 1 -c ,(53)$where we used relationship ( [48](#formula_57)) and collected all c terms within the sum. Notice that, for a given a > 1 we have that

$1 -c j+1 1 -c ≤ ac j ,(54)$provided that c ≥ a a-1 . In fact,

$1 -c j+1 1 -c ≤ ac j ⇐⇒ 1 -c j+1 -ac j + ac j+1 1 -c ≤ 0 ⇐= 1 a -1 + c - a a -1 c j ≥ 0 ⇐= 1 a -1 ≥ 0(55)$which is always satisfied. After substituting ( [54](#formula_66)) in ( [53](#formula_65)), this allows us to write

$lj < a(n -1)(δ -d -1) j -1 j-1 2 δc j .(56)$To prove that lj < δc j+1 , then, it remains to show that

$c ≥ a(n -1)(δ -d -1) j -1 j-1 2 ∀1 < j < n.(57)$Substituting condition (36) in the inequality above, we are left with proving

$n -1 n-1 2 ≥ max j=2...n-1 a j -1 j-1 2 = a n -2 n-2 2 . (58$$)$The outcome depends on the parity of n. For n odd, we have

$n -1 n-1 2 ≥ a n -2 n-2 2 ⇐⇒ 2 n -1 n -1 ≥ a,(59)$to satisfy which it suffices to pick a = 2. This requires having c ≥ a a-1 = 2, which is automatically satisfied. For n even, on the other hand, the binomial coefficients simplify to

$n -1 n-1 2 ≥ a n -2 n-2 2 ⇐⇒ 2 n -1 n ≥ a.(60)$To satisfy this, we need to pick a = 2 n-1 n , which requires c ≥ a a-1 = 2 n-1 n-2 ; however, this too is automatically satisfied by (36) provided n ≥ 4. This completes the proof.

Lemma C.7. Under the constraint (36), condition (30) holds.

Proof. We remind that condition (30) is necessary for the correct "functioning" of the global shift layer, and it composes of two parts. The first part requires that lj < c n δ ∀j < n. Thanks to Lemma C.5, it suffices to show that ln-1 < c n δ, but this is already granted by the upper bound in Lemma C.6. Analogously, for the second part, we need to show that ln > c n δ: for this too we can use the lower bound in Lemma C.6.

We finally have all the ingredients to prove the main theorem of this section:

Theorem C.8. The map in (34), given by

$X → q(X) = u T Ψ(X)$represents a contextual mapping.

Proof. As defined in Def. C.2, a contextual mapping must satisfy two conditions. The first one is that

$q i (X) ̸ = q j (X), ∀i ̸ = j and ∀X ∈ L.(61)$This is directly proven by considering Lemma C.5: since lj is a (strictly) increasing sequence, all its elements are already distinct. The action of the last global shift layer merely translates all these elements by a same quantity, but they remain distinct nonetheless.

The second condition for a contextual mapping is given by

$q i (X) ̸ = q j (X ′ ), ∀i, j and ∀X, X ′ ∈ L, with X ̸ = X ′ . (62$$)$We prove that this holds for (34) by directly considering the difference between two components i, j for different inputs:

$q i (X) -q j (X ′ ) = li -l′ j + c n+1 ln -l′ n = 0 ⇐⇒ li -l′ j = c n+1 l′ n -ln . (63$$)$Notice that, due to Lemma C.4, we have ln -l′ n ̸ = 0 and particularly, | ln -l′ n | ≥ δ. On the other hand, in light of the bounds in Lemma C.6, we have that the left-hand side | ljli | < c n δ. Consequently, the two sides can never cancel each other out, and the proof is complete.

## D Lipschitzness of Sigmoid Attention

In the following, we report the proof for the recovering the Lipschitzness constant associated with SigmoidAttn, as stated in Thm. 3.2. Letting A = W T q W k , and calling σ ij = σ(⟨W q x i , W k x j ⟩) and σ ′ ij = σ ′ (⟨W q x i , W k x j ⟩), we find that the Jacobian of ϕ in the direction (δ 1 , . . . , δ n ) for the sample x i is given by:

$Jac i =   n j=1 σ ′ ij x j x T j A T   δ i + n j=1 σ ′ ij x j x T i A + σ ij I p δ j ,(64)$We see that this Jacobian is the sum of two terms. To control its norm, we can control each norm individually.

The first term, n j=1 σ ′ ij x j x T j A T δ i is of the form U i δ i with U i a matrix. Its squared-norm is therefore:

$n i=1 ∥U i δ i ∥ 2 ≤ max i ∥U i ∥ 2 2 ∥δ∥ F .(65)$Hence, its squared spectral norm is bounded by max i ∥U i ∥ 2 2 . We now let σ ′ ∞ be a bound on n × |σ ′ |; We have:

$∥U i ∥ 2 ≤ n j=1 ∥σ ′ ij x j x ⊤ j A∥ 2 (66) ≤ σ ′ ∞ ∥A∥ 2 1 n n j=1 ∥x j ∥ 2 (67) ≤ σ ′ ∞ ∥A∥ 2 E[∥x j ∥ 2 ].(68)$We see that if the points x i have norm ≤ R, then the Jacobian grows at most like R 2 , because it is "quadratic" in x. However, we see that the quadratic term is likely to be mitigated by the σ ′ (a ij ) term that goes to 0 if a ij is large.

The second term, n j=1 σ ′ ij x j x T i A + σ ij I p δ j , is the sum of two terms. Here, too, we use the triangular inequality to control their norm individually. We get:

$∥ n j=1 σ ij δ j ∥ 2 = ∥δ T σ i ∥ 2 (69) ≤ ∥δ∥ 2 F ∥σ i ∥ 2 , (70$$)$where σ i ∈ R p is the i-th column of σ ij , and δ ∈ R n×p . and by summing, letting σ ∞ an upper bound on n × |σ(x)|:

$n i=1 ∥ n j=1 σ ij δ j ∥ 2 ≤ σ 2 ∞ ∥δ∥ 2 F .(71)$So that σ ∞ upper bounds the spectral norm of the last term.

For the final term, n j=1 σ ′ ij x j x T i Aδ j , define δ = δA T . We get:

$n j=1 σ ′ ij x j x T i Aδ j = n j=1 σ ′ ij ⟨x i , δj ⟩x j .(72)$Hence, letting M the matrix of entries M ij = σ ′ ij ⟨x i , δj ⟩, we see that the previous term is simply x T M T i , so that we get the upper bound on the norm of the term:

$n i=1 ∥x T M T i ∥ 2 ≤ ∥x∥ 2 F ∥M ∥ 2 F (73$$)$and

$∥M ∥ 2 F = ij (σ ′ ij ) 2 ⟨x i , δj ⟩ 2 ≤ 1 n 2 σ ′ ∞ ∥x∥ 2 F ∥A∥ 2 2 ∥δ∥ 2 F , giving overall: n i=1 ∥x T M T i ∥ 2 ≤ σ ′ ∞ ∥A∥ 2 E[∥x j ∥ 2 ]∥δ∥ F .(74)$Notice how this quantity matches the one in (68).

Finally, summing all together gives:

$∥Jac∥ 2 ≤ 2σ ′ ∞ ∥A∥ 2 E[∥x j ∥ 2 ] + σ ∞ ,(75)$which completes the proof.

## Remark:

The previous upper bound might not be tight. Indeed, intuitively, if the x i are large, then the term σ ′ ij should be exponentially small (provided, of course, that W q x i and W k x j are not orthogonal), which would even remove the dependency on the variance in the sigmoid attention.

## E The Bias Term of Sigmoid Attention

One of the differences between SigmoidAttn and SoftmaxAttn is the normalization constant. In SigmoidAttn, one way to emulate the effect of a normalization constant (which links all the elements of the input together and defines a distribution over them), is to include a bias term in the definition as proposed in (3).

For an input vector z ∈ R n , the output of the sigmoid with bias b is

$σ b (z) i := exp(z i ) exp(z i ) + exp(-b)$Contrary to the softmax, this output cannot always sum to one because there is no normalization. We therefore seek a value for b that approximately normalizes σ b (z), i.e., such that We can also look for a bias term b, which helps to approximate the softmax function by the sigmoid function.

We assume that softmax provides us with the true distribution p ⋆ , where p ⋆ i = e z i e z i + j̸ =i e z j . The goal is to find the bias term b such that sigmoid function with weights over all elements denoted by p, where p i = σ b (z) i , approximates p ⋆ . Note that, as mentioned before, p is not necessarily a distribution, i.e. n i=1 p i is not always equal to one. In technical terms, we aim to estimate the normalizing factor Z = n i=1 e zi . The existing approaches for estimating Z is compute-expensive for high dimensions and requires resampling methods. Also, the optimal value of b would depend on the exact values of z, which is unknown beforehand. Therefore, we propose a more intuitive way to estimate the order of bias but possibly with larger disparity. To distribute the independent masses in SigmoidAttn, we assume that each element has uniform weight for the model apriori, which means that none of the elements of the input vector z has any known importance over the others. In the simplest case when softmax is a uniform distribution, we ideally want to have the same order of values for sigmoid as of softmax, which should be 1 n . Therefore, we can write down the following:

$∀i p i = 1 1 + e -(zi+b) ≃ 1 n = p ⋆ i(76)$Ideally, we would like to have 1 + e -(zi+b) ≃ n. Requiring that p = p * in the case where all the z i are 0 gives exp(-b) = n -1, i.e. b ≃log(n) for large n. In the case that all the z i are bounded,

$|z i | ≤ M < ∞ for some constant M , then b ≃ -(M + log(n)) ≈ -max{M, log(n)}.$However, in most cases we do not know M . When the sequence length n is large enough, the constant M loses its importance while in short sequence length, it impacts distributing the weights over elements more. To resolve this issue, we assume that z i are sampled from a standard Gaussian distribution, i.e. z i ∼ N (0, σ 2 ) where σ = 1. Note that this assumption comes from the fact that z i in our problem is one of the elements of QK T / d qk , which is the sum of d qk random variables. Using Central Limit Theorem, we can assume that z i is sampled from a Gaussian distribution. The idea is to estimate M , such that with high probability, |z i | ≤ M , i.e. P (|z i | > M ) ≤ ϵ for a desired ϵ. Therefore, we have

$P (|z i | > M ) = P |z i | > M σ σ ≤ 1 ( M σ ) 2 = σ 2 M 2 ≤ ϵ,(77)$where the inequality is resulted from Chebychev's inequality. Setting σ = 1, we have M ≃ 1/ϵ. Therefore, the order-optimal value would be b ≃ -max{ 1/ϵ, log(n)}, and for long sequence length, b ≃log(n). For example, if we want 90% accuracy in our estimation, M ≈ 3σ = 3, which means b ≃ -max{3, log(n)}. Note that this approximation also follows the intuition that as n grows, we expect the SigmoidAttn without bias term overestimate the mass on each point, so we need to normalize the mass according to n at each point as well.

On another side, one may be more interested in the gradients of p ⋆ and p with respect to z i to behave similarly. We show that b ≃log(n) is still a good choice in this scenario. Let us derive the derivative of SigmoidAttn and SoftmaxAttn with respect to the input. We note that for any i, both functions can be written as e z i e z i +Z-i where Z -i is the share of normalization factor except element i of z. For SoftmaxAttn, Z -i = j̸ =i e zj and for SigmoidAttn, Z -i = e -b . Now, we have

$∂ ∂z i e zi e zi + Z -i = e zi Z -i (e zi + Z -i ) 2 . (78$$)$Therefore, we have the following

$∂p ⋆ i ∂z i = p ⋆ i (1 -p ⋆ i )(79)$$∂p i ∂z i = p i (1 -p i ).(80)$We can see that if p i ≃ p ⋆ i , then ∂pi ∂zi ≃ ∂p ⋆ i ∂zi . So, the previous choice of bias term b ≃log(n) approximates the order of gradients as well. In fact, this is the only valid choice even though we have a quadratic term.

$∂p i ∂z i ≃ ∂p ⋆ i ∂z i ⇐⇒ p ⋆ i (1 -p ⋆ i ) = p i (1 -p i ) (81) ⇐⇒ (p i -p ⋆ i ) (p i -(1 -p ⋆ i )) = 0.(82)$Which means either

$p i ≃ p ⋆ i or p i ≃ 1 -p ⋆ i .$The first one provides us with b ≃log(n) while the second one cannot happen since the nominator of p i is dependent on z i while the nominator of 1p ⋆ i is independent of z i .

## F Details of FLASHSIGMOID

This appendix provides details of the FLASHSIGMOID algorithm. We begin by discussing the implementation details of FLASHSIGMOID, which we build as an extension of FLASHATTENTION2, followed by a benchmark of the performance of the involved kernels. We show that the kernels of FLASHSIGMOID provide a considerable performance boost in model inference over those of FLASHATTENTION2 and a modest performance boost for model training. Further, we demonstrate that the kernel speed boosts also reflect in a considerable performance gain in realistic end-to-end experiments, with an example of training vision transformers [(Dosovitskiy et al., 2021)](#b18) on the ImageNet dataset [(Deng et al., 2009)](#b17). Finally, we also provide kernel benchmarking details of FLASHSIGMOID implementation by taking into account ALiBi slopes [(Press et al., 2022)](#b48), which is one of the important components of SigmoidAttn as seen in the main text of the paper.

## F.1 Details of FLASHSIGMOID Algorithm

Softmax vs. Sigmoid Attention: In this subsection, we discuss the implementation details of FLASHSIGMOID algorithm, which is a hardware-aware implementation of SigmoidAttn approach. We begin with the expressions of the forward and backward passes of softmax and sigmoid attention mechanisms. Let Q, K, and V represent the query, key, and value tensors. Then, the desired forward and backward pass expressions are reported in Tab. 4. The application of sigmoid and

$SOFTMAX SIGMOID FORWARD BACKWARD FORWARD BACKWARD S = Q • K ⊤ √ d dV = P ⊤ • dO S = Q • K ⊤ √ d dV = P ⊤ • dO P = SOFTMAX (S) dP = dO • V ⊤ P = σ (S) dP = dO • V ⊤ O = P • V dS = P ⊙ (dP -ROWSUM (dO ⊙ O)) O = P • V dS = P ⊙ (1 -P ) ⊙ dP dQ = √ d • dS • K dQ = √ d • dS • K dK = √ d • dS ⊤ • Q dK = √ d • dS ⊤ • Q$Table [4](#): Description of the forward and backward passes of softmax and sigmoid attention. With ⊙, we denote Hadamard (element-wise) multiplication.

softmax activation functions, as highlighted in orange color in Tab. 4, is the only implementation difference in the forward passes. Similarly, the expressions for the gradients of the preactivation (dS), as highlighted in purple color in the table above, is the only implementation difference in the backward passes. In light of this, we implement the FLASHSIGMOID algorithm as an extension of the FLASHATTENTION2 [(Dao, 2023)](#) algorithm, which is a highly optimized hardware-aware implementation of SoftmaxAttn.

Flash Attention in Brief: As pointed at in the main text, the FLASHATTENTION [(Dao et al., 2022)](#b15) and FLASHATTENTION2 [(Dao, 2023)](#) algorithms provide hardware-aware implementations of exact attention mechanism by optimizing for bottlenecks of modern accelerators [(Choquette et al., 2021;](#b10)[Choquette, 2023)](#b9). These GPUs possess massive amounts (e.g., ∼ 80GB) of High-Bandwidth Memory (HBM), which stores large tensors but is slow in moving the data to the accelerators. On the other hand, they have smaller amounts (e.g., ∼ 20 MB) of SRAM, which is often more than an order magnitude faster for carrying out actual computations using the registers/tensor cores of the GPU. This trade-off between memory size and computation speed across hierarchies results in the attention mechanism computation being bottlenecked by memory accesses between the HBM and the SRAM [(Ivanov et al., 2021)](#b31). Consequently, flash algorithms optimize for memory accesses across the hierarchy of GPU memory types in order to accelerate computation of attention mechanism and its gradients. FLASHSIGMOID is no exception to this approach.

Algorithm 1 describes the forward pass and Alg. 2 describes the backward pass of the FLASHSIGMOID algorithm. We highlight in orange color the steps in the forward pass of FLASHSIGMOID that differ from those in FLASHATTENTION2 by virtue of sigmoid activation. Similarly, we highlight in purple color the differences in the backward pass. Finally, we highlight in blue color the salient points of FLASHSIGMOID that further help minimize bottlenecking factors on modern accelerators.

Fewer Tensor Allocations, Fewer Memory Accesses, Fast-Tanh: In FLASHATTENTION and FLASHATTENTION2, the attention mechanism is computed by splitting the attention matrix into blocks. Since softmax activation requires a row-wise reduction to compute its normalization factor (i.e., the denominator), one needs to properly compute and track such factor across blocks. Moreover, in FLASHATTENTION this normalization factor is stored after being computed in the forward pass, to have it easily accessible to further speed-up the backward pass. By contrast, substituting sigmoid to softmax eliminates the need to allocate and move across the GPU memory hierarchy the tensors related to the normalization factor (i.e., moving the logsumexp tensor L ∈ R n on HBM in the forward and backward passes). In addition, applying softmax in a stable manner requires tracking the row-max variable m i on chip, which instead is not needed for sigmoid activation. This further helps reducing some on-chip operations and lowering register pressure in FLASHSIGMOID.

Moving on to the backward pass (described in Alg. 2), FLASHATTENTION2 requires computing rowsum (dO ⊙ O), which is needed to backpropagate the gradients of softmax attention outputs 31:

Store query gradient block dQ i from chip back to HBM.

## 32:

On chip, update key gradient block:

$dK j ← dK j + √ d • dS ⊤ ij • Q i . 33:$end for 34:

Store dK j , dV j from chip to HBM as the j-th blocks of dK, dV matrices respectively.

## 35:

end for 36:

return matrices dQ, dK, dV . 37: end procedure to the preactivations. However, since sigmoid activation is applied element-wise, its gradients also backpropagate across sigmoid element-wise, eliminating the need of the row-sum variable and the movement of its blocks across the memory hierarchy. Another optimization of FLASHATTENTION and FLASHATTENTION2 consists of partially re-computing the forward pass of attention mechanism in the backward pass to avoid bottlenecks and speed-up the implementation. To keep the backward pass implementation fast, they require the logsumexp variable to be available and transferred between HBM and SRAM in the backward pass. FLASHSIGMOID, being an element-wise activation, eliminates the need of this variable from the backward pass, and consequently, from the entire algorithm. Finally, a major component in our implementation is the usage of GPU-based implementation of the tanh activation. Sigmoid activation is related to Tanh activation via the following relation: σ (x) = 0.5 • (1 + tanh (0.5 • x)). We utilize the fast GPU-implementation of Tanh activation, which trades off some precision for better speed, in order to compute sigmoid activation in both the forward and the backward pass. This provides a considerable speed-boost in both the forward and backward passes of FLASHSIGMOID, while maintaining parity in performance with a naïve implementation of sigmoid attention. Based on these points of modification, we extend FLASHATTENTION2 to obtain FLASHSIGMOID, a hardware-aware implementation of SigmoidAttn.

## F.2 Benchmarking of FLASHSIGMOID Kernels

Benchmarking Setup: Having seen the details of the FLASHSIGMOID algorithm, we next consider the benchmarking of its kernels. For this, we create a small model in PyTorch [(Paszke et al., 2019)](#b47) that inputs query, key, and value tensors (all of shape [batch, tokens, heads, features]) and passes these through a number of attention layers. Mimicking the design of vision transformers (ViTB-16/224) [(Dosovitskiy et al., 2021)](#b18), we set the number of heads and per-head features as 12 and 64, respectively. We set a batch size of 32, and consider a 10-layer architecture. Then, for the number of tokens sampled from a wide range of [[64, 78k]](#), we compute the forward and backward passes of this model. For these computations, we measure the kernel GPU time using PyTorch's profiler. We carry out our experiments on both H100 [(Choquette, 2023)](#b9) and A100 [(Choquette et al., 2021)](#b10) GPUs.   Results: Figures [10](#fig_14) and [11](#fig_15) show the GPU time comparisons of kernels in inference mode and training mode of FLASHSIGMOID and FLASHATTENTION2 respectively. We observe that we obtain a large average speed-boost for inference and a modest average speed-boost for training. Note that the speed-ups in all the subsequent figures are obtained by averaging the performances for tokens sampled in the range of [[64, 78k]](#).

Details of Individual Kernels: Next, we also show the performance of individual flash kernels of FLASHSIGMOID and FLASHATTENTION2. Note that inference mode involves only the forward pas of the model, while training mode involves both the forward and the backward pass of the model. The forward pass of both these approaches involves one kernel, which we term flash_fwd_kernel, and the backward pass of both these approaches is made up of three kernels, which we term bwd_dq_dk_dv, bwd_dot_do_o, and bwd_convert_dq. In code, the real names of these kernels are as follows.

fwd := flash_fwd_kernel bwd_dq_dk_dv := flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel bwd_dot_do_o := flash_bwd_dot_do_o_kernel bwd_convert_dq := flash_bwd_convert_dq_kernel ( [83](#))

Here, we first provide a brief description of the tasks performed by each of these kernels; for a detailed explanation, we refer the reader to FLASHATTENTION2 [(Dao, 2023)](#) paper and code. The fwd kernel computes the full forward pass of the model as shown in Tab. 4. The bulk of computations of the backward pass happen in the bwd_dq_dk_dv kernel, which performs re-computation of attention matrix and reduction of key and value gradient tensors (dK, dV ). Again, the exact steps carried out in the backward pass can be checked from Tab. 4. The bwd_convert_dq kernel performs the reduction of query gradient tensor (dQ). Finally, note that the bwd_dot_do_o kernel in FLASHATTENTION2 performs the task of computing the rowsum(dO ⊙ O) tensor along with clearing of the accumulators of query gradients (dQ). Although FLASHSIGMOID does not require this row-sum tensor, the clearing of accumulators of query gradients is still needed. For this reason, bwd_dot_do_o kernel also appears in the profiling of FLASHSIGMOID.

Performance of Individual Kernels: Figures [12](#fig_2) and [13](#fig_3) show the performance comparison of each flash kernel in FLASHSIGMOID with the corresponding kernel in FLASHATTENTION2 when tested on an H100 GPU and an A100 GPU respectively. We observe that on both the H100 and A100 GPU architectures, the fwd kernel of FLASHSIGMOID is significantly faster than that of FLASHATTENTION2 and the bwd_dq_dk_dv kernel of FLASHSIGMOID has a modest average speed boost over FLASHATTENTION2. The bwd_dot_do_o kernel in FLASHSIGMOID is significantly faster on A100 GPUs. Note that even though the bwd_dot_do_o kernel of FLASHSIGMOID appears to be slower on average on H100 GPUs, the kernel time of bwd_dot_do_o (∼ 5ms) is negligible compared to that of the main bwd_dq_dk_dv kernel (∼ 5000ms). Thus, the combined backward pass kernel in FLASHSIGMOID time does not suffer from this slowdown. Finally, note that for bwd_convert_dq, FLASHSIGMOID and FLASHATTENTION2 have identical performance. This is expected, since the task of this kernel is to reduce the gradient of the queries dQ, which is a common step in both the approaches and is not modified in FLASHSIGMOID. FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) fwd: 17.39% faster for self-attention and 18.76% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1000 2000 3000 4000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dq_dk_dv: 3.29% faster for self-attention and 6.97% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1 2 3 4 5 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dot_do_o: 2.24% slower for self-attention and 2.17% for causal. bwd_convert_dq: 0.03% faster for self-attention, 0.02% slower for causal.

Figure [12](#fig_2): FLASHSIGMOID and FLASHATTENTION2 kernel comparison on H100 GPUs.

## F.3 Speed Boosts of FLASHSIGMOID in Realistic Settings

In this section, we demonstrate how the performance boosts measured in App. F.2 for the individual kernels of FLASHSIGMOID contributes to speeding-up realistic runs with end-to-end training.

Setup: As a target experiment, we consider training a vision transformer [(Dosovitskiy et al., 2021)](#b18) on the ImageNet dataset [(Deng et al., 2009)](#b17). We create two vision transformer model variants-one

0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 500 1000 1500 2000 2500 3000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) fwd: 14.33% faster for self-attention and 16.92% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1000 2000 3000 4000 5000 6000 7000 8000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dq_dk_dv: 3.50% faster for self-attention and 1.39% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 2 4 6 8 10 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dot_do_o: 7.95% faster for self-attention and 8.00% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1 2 3 4 5 6 7 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal)

bwd_convert_dq: 0.01% faster for self-attention, 0.03% slower for causal.

Figure [13](#fig_3): FLASHSIGMOID and FLASHATTENTION2 kernel comparison on A100 GPUs.

with FLASHATTENTION2 attention and the other with FLASHSIGMOID attention. We carry out the training of these models with a distributed data-parallel (DDP) setup using PyTorch [(Paszke et al., 2019)](#b47). We perform two sets of experiments-i. the first performs DDP training on four nodes of H100 GPUs with eight GPUs per node and EFA/RDMA interconnect for the nodes, and ii. the second performs DDP training on four nodes of A100 GPUs with eight GPUs per node. In each set of experiments, we use three different image sizes (64 × 64, 90 × 90, and 100 × 100), along with patch size of 1 to result in different number of tokens for the underlying attention mechanism in the vision transformer model (64 × 64 = 4096, 90 × 90 = 8100, and 100 × 100 = 10000 tokens). For each of these configurations, we select batch sizes so that the GPU memory utilization would be greater than 80%. These considerations are in order to minimize, if not eliminate, other confounders that can unfairly affect estimation speed-ups in realistic runs. For instance, a low GPU utilization would lead to a larger number of updates, which in turn would incur unnecessary delays, variations, and slow-downs due to across-nodes communications.

## Results:

The results of the runs on H100 nodes and A100 nodes are shown in Tab. 5 and 6 respectively. There, we show how the kernel GPU times for forward and backward passes vary according to the number of tokens considered, and include the wall-clock time of the end-to-end runs as explained above. We observe that the kernel speed-up reflects significantly in the speed-up of inference of the models (during testing) and modestly in the training of the models. We observe ∼ 8% speed-up in wall-clock time of inference and ∼ 4% speed-up in wall-clock time of training.

TOKENS KERNEL GPU TIME COMPARISON FULL RUN WALL-CLOCK TIME COMPARISON KERNELS FLASHATTENTION2 (MS) FLASHSIGMOID (MS) MODE FLASHATTENTION2 (S) FLASHSIGMOID (S) 4096 FWD 4.98±0.01 4.17±0.01 (-16.31%) INFERENCE 11.17±0.18 10.68±0.18 (-4.42%) FWD + BWD 19.58±0.06 18.12±0.04 (-7.45%) TRAINING 1563.39±1.30 1521.68±2.27 (-2.67%) 8100 FWD 20.46±0.05 16.73±0.05 (-18.22%) INFERENCE 28.21±0.18 25.93±0.17 (-8.06%) FWD + BWD 77.63±0.13 72.70±0.12 (-6.35%) TRAINING 4282.75±2.14 4129.25±4.14 (-3.58%) 10000 FWD 31.17±0.07 25.49±0.05 (-18.20%) INFERENCE 38.71±0.19 35.37±0.17 (-8.62%) FWD + BWD 117.53±0.13 109.87±0.12 (-6.52%) TRAINING 5990.72±2.21 5751.43±5.77 (-3.99%) Table 6: FLASHSIGMOID vs. FLASHATTENTION2 on A100 nodes. The kernel GPU time for both the approaches is reported in milliseconds and wall-clock times is reported in seconds per epoch.

Connection of Wall-Clock Time Speed-Up and Kernel Speed-Up: From Tab. 5 and 6, it is clear that the speed-up in kernels is larger than that in the wall-clock times of the full runs. In fact, the speed-up in kernels is the upper bound for the speed-up that we would see in wall-clock times. To see why, let us denote by τ sm and τ σ the total kernel GPU time for softmax attention and sigmoid attention respectively. Then, the kernel speed-up is given by s kernel := 1 -τσ τsm . However, in a full run, the total wall clock time also incorporates the time required to load data, time taken by other layers of the underlying models, time required to communicate gradients and other data across GPUs and across nodes, and so on. For our corresponding sigmoid and softmax runs, these extra factors are designed to add, upon expectation, in the same extra time τ . Thus, the wall-clock time speed-up of a full run with end-to-end training is s wall-clock := 1 -τσ+τ τsm+τ . Since we have faster sigmoid kernels, we have τ σ < τ sm , which in turn shows that s wall-clock = 1 -τσ+τ τsm+τ < 1 -τσ τsm = s kernel . This explains the speed boost trends in kernel time versus full run wall-clock time for each setting in Tab. 5 and 6. However, in particular, if a model performs attention mechanism over large number of tokens, the attention mechanism, and hence the corresponding kernel time, starts to dominate the other computations in the network. In that case, we see that the wall-clock time speed-boost is closer to the kernel speed-boost. Mathematically, if τ σ , τ sm > > τ , we have: τ σ + τ ≈ τ σ , τ sm + τ ≈ τ sm . Thus, s kernel ≈ s wall-clock , thereby making s wall-clock /s kernel → 1.  Significance of Wall-Clock Speed-Up of Inference: Although FLASHSIGMOID provides only modest gains during training, the speed-up in inference is significant (> 15% for underlying kernels and 5 -10% during inference of full runs). We posit that this speed-up in inference is extremely critical as well. Contemporary large-scale models, once trained, spend a huge portion of the rest their lifetime in inference mode [(OpenAI, 2023)](#). Thus, significant performance boosts in inference mode have immense potential for saving resources in deployment of large models for inference.

## F.4 FLASHSIGMOID with ALiBi

It is evident from the main text of the paper that improved positional embeddings, like ALiBi [(Press et al., 2022)](#b48), can be crucial for certain tasks and data modalities. Thus, we also provide a FLASH-SIGMOID implementation that incorporates ALiBi. We compare the FLASHSIGMOID with ALiBi implementation with the FLASHATTENTION2 with ALiBi implementation [(Dao, 2023)](#). [Figures 14 and 15](#) show the kernel GPU time for the forward and backward pass kernels of FLASHSIGMOID with ALiBi implementation versus FLASHATTENTION2 with ALiBi implementation. Again, we observe that FLASHSIGMOID kernels for inference have significant speed-up in wall-clock time over those in FLASHATTENTION2 and the kernels for training also have modest wall-clock improvements. Figure [15](#fig_5): On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 12.28% faster than FLASHATTENTION2 for self-attention and 5.30% for causal attention. The training mode kernels of FLASHSIGMOID are 14.64% faster than FLASHATTENTION2 for self-attention and 6.80% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.

## F.5 Directions for Future Work on FLASHSIGMOID

In this section, we discussed FLASHSIGMOID, a hardware-aware implementation of the SigmoidAttn algorithm. Then, we demonstrated via kernel benchmarking and realistic setting runs that FLASH-SIGMOID provides significant gains in inference as well as modest gains in training of models with attention mechanism. In this subsection we further discuss additional avenues for improving the implementation of FLASHSIGMOID, and point out some interesting directions for future work.

Optimization of Block Shapes for Different Input and GPU Settings: As stated before, our FLASHSIGMOID implementation builds on FLASHATTENTION2 by adding functionality for forward and backward pass of sigmoid attention in place of the standard softmax attention. In particular, for all FLASHSIGMOID results discussed so far, we inherit directly from FLASHATTENTION2 the details of optimal block shapes, grid shapes, and other kernel launch parameters, and keep them unchanged in our implementation. For instance, this is the case for the block sizes B r , B c in Alg. 1 and 2, which are identical in FLASHATTENTION2 and FLASHSIGMOID. This choice is dictated by the need to ensure a fair comparison between the two implementations, and allows us to demonstrate the speed-up of sigmoid attention by minimizing confounders associated with parallel computations on different GPU architectures for different input shapes.

Although FLASHSIGMOID kernels lead to speed-ups in inference and training for both H100 and A100 GPUs, we observe that the kernel timing speed-ups on A100 are not uniform across sequence lengths: for a small subset of these, our kernel provides significantly lower speed-up compared to the overall trend for other sequence lengths. Ideally, the implementation of attention mechanisms should not assume any information on the token count in input, and it is then desirable to have uniform speed-ups across all input lengths. Here, we show that this is achievable by simply updating the block shape information in FLASHSIGMOID to values that are different than those in FLASHATTENTION2.

Note that the implementation of FLASHATTENTION2 is templated according to block shapes, grid shapes, and other kernel launch parameters.

Note that FLASHATTENTION2 provides various tailored implementations, optimized for different input shapes (e.g., different ranges of feature dimension per head), input types (e.g., causal attention vs. self-attention, ALiBi vs. no ALiBi in attention, etc.), and GPU types (e.g., A100 vs. H100 via checking shared memory size on GPUs). This is achieved by opportunely selecting the kernel template parameters defining block shapes, grid shapes, and other kernel launch parameters for parallel computation on GPUs.

In our case, we create a variant of FLASHSIGMOID, denoted by FLASHSIGMOID † , where we update the block sizes for query and key tensors from (B r , B c ) = (128, 128) of FLASHSIGMOID to (B r , B c ) = (128, 64) of FLASHSIGMOID † only for our input setting (template with features per head being 64). 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 500 1000 1500 2000 2500 3000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Full) (a) Inference mode kernels on A100. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 2000 4000 6000 8000 10000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Full) (b) Training mode kernels on A100. Figure 16: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID † is 14.82% faster than FLASHATTENTION2 for self-attention and 18.02% for causal attention. The training mode kernels of FLASHSIGMOID † are 6.18% faster than FLASHATTENTION2 for self-attention and 5.76% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model. TOKENS KERNEL GPU TIME COMPARISON KERNELS FLASHATTENTION2 (MS) FLASHSIGMOID (MS) FLASHSIGMOID † (MS) 4096 FWD 8.32±0.02 7.84±0.03 (-5.79%) 7.26±0.02 (-13.21%) FWD + BWD 31.81±0.08 31.11±0.08 (-2.19%) 30.62±0.09 (-4.03%) 8100 FWD 33.65±0.09 27.92±0.07 (-17.04%) 28.54±0.07 (-15.50%) FWD + BWD 128.18±0.13 119.04±0.12 (-7.13%) 119.85±0.13 (-6.81%) 10000 FWD 51.17±0.07 42.49±0.06 (-16.96%) 43.53±0.09 (-15.32%) FWD + BWD 194.54±0.14 180.59±0.15 (-7.17%) 181.97±0.17 (-6.87%) 16384 FWD 134.19±0.12 125.43±0.10 (-6.53%) 116.75±0.10 (-13.40%) FWD + BWD 494.65±0.28 482.08±0.23 (-2.54%) 474.52±0.28 (-4.48%)

Table [7](#): FLASHATTENTION2 vs. FLASHSIGMOID vs. FLASHSIGMOID † on A100 nodes. The kernel GPU time for all three approaches are reported in milliseconds. We observe that FLASHSIGMOID † provides better and more uniform speed-ups across all example tokens.

Experimentation and Results: For this variant, we perform kernel benchmarking as described in App. F.2, and report the corresponding results in Fig. [16](#fig_6). Comparing the plots for kernel timing with FLASHSIGMOID plots from Fig. [11](#fig_15), we observe that FLASHSIGMOID † not only provides a more uniform inference and training kernel speed-up on all sequence lengths, but also improves the average of these speed-ups across all lengths. To further bolster our observations, Tab. 7 shows the inference mode and training mode kernel speed-ups for a subset of sequence lengths under consideration. This experiment indicates that it is possible to obtain higher and more uniform speed-ups in kernel timings across a wide range of tokens by investigating optimal block shape, grid shape, and other kernel launch parameters for each input setting and GPU type. We leave this optimization for future work.

## G Experiments

G.1 Extra Ablations G.1.1 The Effect of Multiplicative Sequence Length Normalization [Wortsman et al. (2023a)](#) notes that models trained with sigmoid or ReLU attention require scaling by the sequence length, n -α σ(QK T / d qk )V . We ablate this by comparing the scaled solution to the one we propose in App. E. We also generalize the variant proposed in [(Wortsman et al., 2023a)](#) to variadic sequence lengths such that it works with auto-regressive (AR) training, for example for  Figure [19](#): n -0.5 normalization.

$n = 3:   1 1 1 0.5 -α 0.5 -α 1 0.33 -α 0.33 -α 0.33 -α   n -α ⊙ 1 0 0 1 1 0 1 1 1 Causal Mask M ⊙ σ(QK T / d qk )V . (84$$)$We repeat the experiment from Fig. [5](#fig_5), using ALiBi positional embeddings for all trials. We apply α = {1, 0.5} AR normalization proposed in (84). While there is an observable difference in terms of the attention norm, ∥σ(QK T / d qk )V ∥, we find that the train NLL is slightly worse for both normalized variants (Fig. [18](#fig_22) and [19)](#b70) in comparison to the b =ln n variant in Fig. [17](#fig_21).

## G.1.2 Attention Bias Stability Ablation

To validate the stabilizing effects of attention bias we repeat the experiment from Fig. [7](#fig_8) and [8](#), keeping all of the same hyper-parameters, while enabling QK norm and LayerScale (initialized at 10 -4 ). We train with a range of constant bias offsets, b ∈ {-15, -10, -6, -4, -1} and visualize the results below in Fig. [20](#fig_24). We observe a systematic increase in stability (and lower SigmoidAttn NLL) for which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-10 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-6 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-4 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-1 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 7 10 8 Non-embed Params (M) 10 1 LR sensitivity which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax values less than -1 up till -10, after which the -15 plot shows an over-regularizing effect with decreased performance.

## G.2 Vision

## G.2.1 Test ImageNet1k Top-1%

Fig. [21](#fig_25) reports the test linear probe results for the ViT-B/16 BYOL [(Grill et al., 2020;](#b22)[Busbridge et al., 2023)](#b3), ViT-B/16 SimCLR [(Chen et al., 2020;](#b7)[Zhai et al., 2023a)](#) and the finetuned performance for the ViT-L/16 MAE [(He et al., 2022)](#b26) and the test top-1% results for for ViT-B/16 supervised model [(Dosovitskiy et al., 2021)](#b18). Across these wide range of SSL and supervised learning tasks, trained with contrastive (SimCLR), EMA distillation (BYOL) and reconstructive objectives (MAE), we find that SigmoidAttn not only matches the training dynamics (Fig. [2](#fig_2)), but also the linear probe and finetuned performance of the baseline SoftmaxAttn.

G.2.2 LayerScale Free Sigmoid Attention While Fig. [22](#fig_2) demonstrates the possibility of learning SigmoidAttn without LayerScale, it involves task specific tuning of {t, b}. We also explored gating attention from learning (through a simple multiply by zero) for ∼25 epochs and were able to match the t = 10, b = -10 training curves from above. However, we opted for the LayerScale method due to its simplicity.  [& Hutter (2017)](#b40). In addition, to confirm that applying QK-Norm does not hurt the baseline, we show training parity with and without QK-Norm in Fig. [24](#fig_4).  

## G.3.2 Gradient Norm

While a SigmoidAttn based LM using aforementation hyper-parameters has a smooth loss curve, we do see more gradient norm fluctuations. See Fig. [25](#fig_5), where spikes larger than 0.5 are not visible in the SoftmaxAttn equivalent.

G.

4 Automatic Speech Recognition G.4.1 Training Details All acoustic models are fed 80 channel log-mel filterbanks with a 25ms sliding window strided by 10ms. The transformer-based encoder model has 255M parameters: 1D convolution of kernel 7 and stride 3 followed by CAPE positional embedding if it is used and 36 transformer blocks with pre-LayerNorm, an embedding dimension of 768, 4 heads, 3072 units in the MLP layers. The model is trained with CTC loss and a character vocabulary, including apostrophe ('). In additional experiments, we vary the depth to 12 and 24 layers, and change pre-LayerNorm to post-LayerNorm. We implemented our own conformer-based encoder model, also trained with a CTC loss and a character vocabulary. The conformer model has 104M parameters and consists of 1D convolution of kernel 7 and stride 3 followed by 16 conformer blocks with an embedding dimension of 512, 4 heads, 2048 units in the MLP layers. Variational noise is not used and RoPE is used as a relative positional embedding instead of relative sinusoidal positional embedding. For all models, SpecAugment (Park et al., 2019) is used for augmentation with 2 frequency masks (max width 30) and 10 time masks (max width 50, ratio 0.1). All models are trained with dynamic batching and mixed precision with BF16. Models are trained with different configurations of optimizers and hyperparameters to have diverse coverage of use-cases. We first optimize every configuration for SoftmaxAttn and then change only attention to the introduced configuration of SigmoidAttn while all other parameters are kept the same. Detailed configurations are shown in Table 11. We train models until the greedy WER stops improving on the validation sets (dev-clean, dev-other) and report final test sets (test-clean, test-other) greedy WER without integration of any external language model.

For the bias term b =log n in SigmoidAttn, we do not use max sequence length as in language model experiments. Instead, for every audio sample we use its own duration as a bias terms resulting into non-trainable bias vector for the minibatch. For experiments with sequence normalization, we also use not the max sequence length in the minibatch but rather the ground truth sample duration to properly normalize encoder attention.

To evaluate behaviour for length generalization we use TED-LIUM v3 dataset [Hernandez et al. (2018)](#b27) as its validation and test sets have longer audio duration than LibriSpeech: LibriSpeech has in average 10-15s duration, while in TED-LIUM there are audio longer than 30s (the max duration of LibriSpeech). To perform evaluation on TED-LIUM v3, we combine together validation and test sets of TED-LIUM v3 (we don't use them for training and hyper-parameters search and just perform final evaluation) and split them into 4 datasets according to the duration: 0-10s, 10-20s, 20-30s, and 30s+.

For positional embeddings we use not only CAPE, but change it to AliBi or RoPE. As ALiBi was originally introduced for the decoder only models and there is no official adoption of it yet [14](#foot_8)for the encoder models (without causal masking), we follow the best practices found in [https: //iclr-blogposts.github.io/2024/blog/alibi-mlm/](https://iclr-blogposts.github.io/2024/blog/alibi-mlm/) of nonsymmetric ALiBi with different slopes instead of symmetric version used by [(Lee et al., 2022)](#b37).

## G.4.2 Results and Ablations

Initial investigation on post-LayerNorm and pre-LayerNorm transformers on both LibriSpeech 100h and 960h revealed that SigmoidAttn without any bias is unstable resulting in huge and frequent gradient norm and training loss spikes throughout the training which in turn result in spikes of   Further experiments with bias term in the SigmoidAttn definition for post-LayerNorm transformers on LibriSpeech 100h reveal that training is now stable (only few marginal spikes in gradient norm occur, while train loss is smooth all the time). However, both LayerScale and QK norm restrict model capacity thus not matching SoftmaxAttn. Moreover, some combination of them is needed for the stable training, though w/o both of them we got the best performance for SigmoidAttn (still behind SoftmaxAttn), see Table [13](#tab_22). We believe, further adaptation and deeper investigation is needed for SigmoidAttn and post-LayerNorm, though recent advances in machine learning do not use post-LayerNorm models due to high training instability even for SoftmaxAttn.

Switching to pre-LayerNorm transformers and varying the depth of the models lead to stable training with SigmoidAttn and bias term b =log n with few (2-5 times) spikes in the gradient norm and smooth loss. In this case, SigmoidAttn matches results for SoftmaxAttn and they both generalize to TED-LIUM data similarly, see

Table 12. If the bias term is removed, SigmoidAttn can still match SoftmaxAttn but large spikes in gradient norm and loss can occur.  Finally, we experiment with a conformer model, in Table [14](#tab_23). Again, we found that bias term b =log n stabilizes training. The learnable b = -10 works though we see significant gradient norm spikes while the train loss remains smooth. Besides, b =log n generalizes well to longer sequences while learnable b = -10 fails to do so with RoPE for conformer. Overall, SigmoidAttn is able to match SoftmaxAttn having stable training with b =log n.

In experiments with different variants of bias term for SigmoidAttn, the bias b =log n is found to be the most stable (only few marginal gradient norm spikes are observed with the train loss being smooth) and it provides similar performance as SoftmaxAttn in most settings. The source of instability is coming from the larger attention output norms (80k for CAPE, 40k for RoPE and 20k for AliBi while being 200 for SoftmaxAttn). This happens due to high attention weight of every token which can be biased towards zero with a bias term in SigmoidAttn definition. Preliminary results to connect this to the local attention property needed at the beginning of the training for stable training failed, as local attention did not converge well at all (it is deactivated after some initial training).

To fully benefit from the improved throughput of FLASHSIGMOID, for the bias term b =log n in SigmoidAttn, we experimented with configuration when the maximum audio duration in the minibatch is used as n resulting into non-trainable bias scalar which changes between minibatches as we use dynamic batching. Comparison between the bias vector with per sample own duration normalization and the bias scalar as maximum duration in the minibatch is shown in Table [15](#tab_25): final model performance is similar and stability is same (only 2-3 minor spikes in CAPE for gradient norms are observed). Thus, per batch maximum audio duration can be used with b =log n as the final configuration.

## G.5 Simple Experiments

## G.5.1 k-Summation Problem Definition

Here we look at a synthetic, simple task in order to investigate the behavior of softmax and sigmoid attention activations. The problem chosen is to minimize the MSE loss of a R n → R target function.

In the first half of each input are samples from a N (0, 1) distribution, and the second half is a k-hot binary vector indicating which values in the first half to sum.

The results presented here are for the n = 40 problem with various values for k. Where a transformer is used, the transformer is a single layer to aid visualization. In all cases (unless noted otherwise), the optimizer is Adam with a constant learning rate of 0.001, and the training data is continuously generated to preclude over-fitting.

A few examples for n = 10 (not drawn from N (0, 1)) are shown below. Inputs in the second half of the input are show in orange only as a visual aid.

1 2 3 4 5 0 0 0 0 1 → 5 1 2 3 4 5 1 0 0 0 1 → 6 8 1 2 0 5 0 1 1 1 0 → 3 2 0 2 2 2 1 1 0 1 0 → 4

## G.5.2 Comparison to Softmax

In Figure [30](#fig_30), we see the performance of three architectures on the k-summation problem as k increases. The sigmoid activated transformer has similar scaling to the softmax activation. 

## G.5.3 Attention Evolution

In Figures [31](#fig_3) and [32](#fig_3), forty samples are used to monitor the single head, single layer post-activation attention matrix as training progresses. In Figure [31](#fig_3), the distribution of values is visualized over time; note the sigmoid attention is more variable but reaches comparable values at convergence. The main difference at convergence is that the sigmoid has fewer high magnitude values than softmax indicating a more distributed attention.

In Figure [32](#fig_3), metrics on the post-activation attention matrices are used and show comparable behavior in the first half of training. In the second half of training, the SigmoidAttn can be seen to reduce in norm and in sparsity. (see following discussion of Figure [33](#fig_31) for further insights). rate schedule with 5% linear warmup and a maximum learning rate of 1e-3 is used with the Adam optimizer.

In this result, we see the sigmoid activation has higher data efficiency and similar fall-off in the out of distribution cases. From shorter runs, we estimate that the softmax network would fit the training with 4-5x more data. Our conjecture is that the two layer transformer more easily learns the pair finding task with sigmoid because softmax is biased to focus on single values, though it is unclear why multiple heads are not able to compensate for this proposed cause in the softmax case. 

![Figure 2: Train losses comparing SigmoidAttn with SoftmaxAttn.]()

![Figure 3: SigmoidAttn with SinCos.]()

![Figure 4: SigmoidAttn with RoPE.]()

![Figure 5: SigmoidAttn with ALiBi.Figure 6: SigmoidAttn with RoPE, b = -10.]()

![Figure 5: SigmoidAttn with ALiBi.Figure 6: SigmoidAttn with RoPE, b = -10. LayerScale To validate the need for LayerScale, we follow Wortsman et al. (2023b) to quantify the impact on stability. All models are trained with RoPE with b ∝ln n, using AdamW (Loshchilov & Hutter, 2017) on the realnews split of C4 with (β 1 , β 2 ) = (0.9, 0.95), ϵ = 10 -8 , wd = 0, batch size 24, maximum token sequence length of 512 from the T5 tokenizer (Raffel et al., 2020), cosine LR schedule of 2 14 steps including a linear warmup of 2 10 steps. Models have n heads = κ, n layers = 2 × κ, d model = 64 × κ and d feed-forward = 256 × κ for a scaling value κ ∈ {1, 2, 4, 8, 16} leading to models with {2.2, 4.9, 15.0, 67.0, 440.0}M trainable non-embedding parameters. Following Wortsman et al. (2023b), we sweep learning rates η∈ {3 × 10 -4 , 1 × 10 -3 , 3 × 10 -3 , 1 × 10 -2 , 3 × 10 -2 , 1 × 10 -1 , 3 × 10 -1 }. LR sensitivity is defined as E η∈[a,b] [min(ℓ(A(η)), ℓ 0 )ℓ * ] where ℓ(A(η))is the loss achieved by the learning algorithm A with LR η, ℓ 0 is the loss at initialization, and ℓ * is the loss achieved by the best LR. LayerScale is initialized at 10 -4 . Unlike vision tasks, where LayerScale improves performance (Fig.9-a), in LM, we observe that SoftmaxAttn slightly benefits from LayerScale, while the performance of SigmoidAttn remains largely unaffected.]()

![Figure 7: LR sensitivity LayerScale ablation.]()

![Property for Sigmoid Attention C.1 Proof of Step (3): Sigmoid Transformers can Approximate Modified Sigmoid Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Proof of Step (2-b): Modified Sigmoid Transformers can Implement Contextual Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 Basic Building Blocks of Contextual Mapping . . . . . . . . . . . . . . . C.2.2 Result of Applying a Sequence of Selective Shifts . . . . . . . . . . . . . . C.2.3 Result of Applying One Last Global Shift Layer . . . . . . . . . . . . . . C.2.4 A Sequence of Selective Shifts Followed by a Global Shift Produces a Contextual Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D Lipschitzness of Sigmoid Attention E The Bias Term of Sigmoid Attention F Details of FLASHSIGMOID F.1 Details of FLASHSIGMOID Algorithm . . . . . . . . . . . . . . . . . . . . . . . . F.2 Benchmarking of FLASHSIGMOID Kernels . . . . . . . . . . . . . . . . . . . . . F.3 Speed Boosts of FLASHSIGMOID in Realistic Settings . . . . . . . . . . . . . . . F.4 FLASHSIGMOID with ALiBi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Directions for Future Work on FLASHSIGMOID . . . . . . . . . . . . . . . . . . . G Experiments G.1 Extra Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1.1 The Effect of Multiplicative Sequence Length Normalization . . . . . . . . G.1.2 Attention Bias Stability Ablation . . . . . . . . . . . . . . . . . . . . . . . G.2 Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2.1 Test ImageNet1k Top-1% . . . . . . . . . . . . . . . . . . . . . . . . . . G.2.2 LayerScale Free Sigmoid Attention . . . . . . . . . . . . . . . . . . . . . G.2.3 Sigmoid Attention vs. Attention Relaxations . . . . . . . . . . . . . . . . G.2.4 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3.1 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3.2 Gradient Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 Automatic Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.1 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.2 Results and Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5 Simple Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.1 k-Summation Problem Definition . . . . . . . . . . . . . . . . . . . . . . G.5.2 Comparison to Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.3 Attention Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.4 Pair Repeat Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .]()

![introduced inYun et al. (2020, App. B.5), but it has been adapted to account for the presence of a sigmoid nonlinearity: notice this required us to use 4-headed attention, while in Yun et al. (2020) a 2-headed version is sufficient.]()

![n i=1 σ b (z) i ≃ 1. We have Proposition E.1. Let z ∈ R n , and take m, M ∈ R such that for all i, it holds m ≤ z i ≤ M . Then, the equation n i=1 σ b (z) i = 1 with variable b has a single solution b * with log(n -1) -M ≤ b * ≤log(n -1)m . Proof. The function ϕ : b → n i=1 σ b (z) i is smooth and monotonically increasing, and we have ϕ(log(n -1) -M ) ≤ 1 and ϕ(log(n -1)m) ≥ 1. This shows the existence of b * as well as the advertised bound on b * . This suggests using a b of the order oflog(n); in practice we use b =log(n).]()

![Figure10: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 17.39% faster than FLASHATTENTION2 for self-attention and 18.76% for causal attention. The training mode kernels of FLASHSIGMOID are 6.53% faster than FLASHATTENTION2 for self-attention and 9.46% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.]()

![Figure11: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 14.33% faster than FLASHATTENTION2 for self-attention and 16.92% for causal attention. The training mode kernels of FLASHSIGMOID are 6.02% faster than FLASHATTENTION2 for self-attention and 5.27% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.]()

![Figure14: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 17.04% faster than FLASHATTENTION2 for self-attention and 10.87% for causal attention. The training mode kernels of FLASHSIGMOID are 8.91% faster than FLASHATTENTION2 for self-attention and 4.72% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.]()

![Figure 17: b =ln n.]()

![Figure 18: n -1 normalization.Figure19: n -0.5 normalization.]()

![Figure 20: Attention bias ablation.]()

![Figure 21: ImageNet1k test top-1% for SoftmaxAttn vs. SigmoidAttn using models from Fig. 2.]()

![Figure 22: A competitive SigmoidAttn ViT-B/16 model can be learned without LayerScale or QK norm using a large initial learnable scalar temperature t = 10 and bias b = -10 (similar to SigLIP (Zhai et al., 2023b)): σ(e t [QK T / d qk ] + b)V , {b, t} ∈ R. This regularizes the model, as it must move the temperature to a learnable regime. The t = 10, b = -10 curve makes no progress in train NLL or test top-1 for ∼25 epochs (near max LR), but ultimately outperforms baselines.]()

![Figure 24: 1B SoftmaxAttn LLM training with and without QK Norm, converging to the same loss.]()

![Figure 28: ASR Transformer model (255M) training with post-LayerNorm (left) and pre-LayerNorm (right) on LibriSpeech 960h with SigmoidAttn (w/ bias term, b = 0, w/o QK norm, w/ LayerScale) or with SoftmaxAttn. Huge gradient norms and training loss spikes are observed for SigmoidAttn which can result in worse final model performance hence models for SigmoidAttn are unstable.]()

![Figure 29: ASR Transformer model (255M) training with pre-LayerNorm on LibriSpeech 960h with SigmoidAttn (w/ bias term, b =log n, w/ QK norm, w/ LayerScale) and different positional embeddings CAPE, RoPE, ALiBi. The bias b is able to stabilize SigmoidAttn training: smooth training loss and only marginal rare spikes in gradient norms are observed.]()

![Figure 30: Final loss is shown after training convergence as k-summation problem complexity increases. The ReLU MLP has two hidden layers (900, 300) for 307k parameters, while the transformer has an embedding dimension of 120, 8 heads, and an MLP ratio of 4, giving 187k parameters. The SigmoidAttn is applied after a learned offset initialized to -4, A+param(-4).]()

![Figure 33: For 8 samples, the post-activation attentions is visualized as training progresses on the k = 1, n = 40 summation problem. The model has one head to simplify the visualization. The attention is shown in pairs for each sample with softmax attention is in black and sigmoid is in blue. A 2 × 2 block structure is evident in both cases, resulting from each halve of the input containing different information.]()

![Figure 34: Validation accuracy for out of distribution sequence lengths is shows after 5M samples of training; trained lengths are shown with vertical lines. Quartiles and means are shown from six trials. The MLP has two hidden layers, ReLU activation, and a similar number of parameters. The sigmoid transformer has a learned offset initialized to -4.]()

![Word error rate (%) on LibriSpeech test sets and TED-LIUM v3(Hernandez et al., 2018) ("TED", joint validation and test sets split according to duration) for transformer (255M params) with either SoftmaxAttn or SigmoidAttn (LayerScale and QK norm are used with b =log n) trained on LibriSpeech 960h data (mean duration is 10-15s). Hyper-parameters are in App. G.4.]()

![1B LLM English evaluation.]()

![FLASHSIGMOID vs. FLASHATTENTION2 on H100 nodes. The kernel GPU time for both the approaches is reported in milliseconds and wall-clock times is reported in seconds per epoch.]()

![SigmoidAttn SimCLR and BYOL ViT-B/16 hyperparameters.]()

![SigmoidAttn Supervised ViT-B/16 and MAE ViT-L/16 hyperparameters.]()

![Training details for the Llama-style 1B LM training.]()

![Training details for the ASR models on LibriSpeech 100h (LS-100) and LibriSpeech 960h (LS-960) for transformers and conformers.]()

![Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3(Hernandez et al.]()

![Word error rate (%) on LibriSpeech dev/test sets for post-LayerNorm transformer (255M) with either SoftmaxAttn (w/o QK norm) or SigmoidAttn (by default w/ LayerScale, w/ QK norm, w/ b =log n) trained on LibriSpeech 100h data. Hyper-parameters can be found in Table11.]()

![Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3(Hernandez et al., 2018) ("TED", joint validation and test sets with split according to audio duration) for conformer (104M) with RoPE and with either SoftmaxAttn or SigmoidAttn (w/ LayerScale, w/ QK norm, w/ b =log n) trained on LibriSpeech 960h data (average duration is 10-15s). Hyper-parameters can be found in Table11.]()

![Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3(Hernandez et al., 2018) ("TED", joint validation and test sets split according to duration) for transformer (255M params) with either SoftmaxAttn or SigmoidAttn (LayerScale and QK norm are used with b =log n) trained on LibriSpeech 960h data (mean duration is 10-15s). Hyper-parameters are in App. G.4.]()

Appendix G.2.2 demonstrates that supervised vision tasks using SigmoidAttn without LayerScale can match baseline SoftmaxAttn performance by relying on learnable scalar bias and temperature: {b, t} ∈ R.

We ablate multiplicative sequence length scaling in more detail in App. G.1.1.

https://github.com/apple/axlearn

For example, consider d = 3 and the column defined as xi = [3δ, 10δ, 2δ] T , that is, the column identified by the triplet of indices[3, 10, 2]. Multiplying by u would then give the scalar u T xi = (3 + 10N + 2N 2 )δ, where N = δ -1 , which is uniquely identified by the single index (3 + 10N + 2N 2 ).

This can be better seen by considering independently the effects of the two parameters b k , bq on the modified sigmoid attention matrix H ((l -1bq) ⊗ (l -1b k )). We have in fact, with bq = 0,

Indeed its inverse can be explicitly recovered by directly applying Sherman-Morrison formula.

This is a direct consequence of the definition of operator S in (35): since it has 1's everywhere but on its diagonal, its ∞-norm is simply n -1.

This is a consequence of some useful properties of the binomial coefficient, namely the Hockey stick identity[Jones (1994)](#b32), and the symmetry of k i with respect to i.

See discussion in https://github.com/ofirpress/attention_with_linear_ biases/issues/5.

