<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theory, Analysis, and Best Practices for Sigmoid Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-06">6 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
							<email>jramapuram@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Danieli</surname></persName>
							<email>f_danieli@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Eeshan</forename><surname>Dhekane</surname></persName>
							<email>eeshan@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Floris</forename><surname>Weers</surname></persName>
							<email>floris_weers@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
							<email>dbusbridge@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Ablin</surname></persName>
							<email>p_ablin@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jagrit</forename><surname>Digani</surname></persName>
							<email>digani@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Zijin</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amitis</forename><surname>Shidani</surname></persName>
							<email>amitis_shidani@apple.com</email>
						</author>
						<author>
							<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
							<email>rwebb@apple.com</email>
						</author>
						<title level="a" type="main">Theory, Analysis, and Best Practices for Sigmoid Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-06">6 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">42E6ED2C89D3DA4A86B13921C3129A56</idno>
					<idno type="arXiv">arXiv:2409.04431v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASH-SIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs 2 . Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of modern machine learning can be largely attributed to the attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b55">Vaswani et al., 2017)</ref>. Attention uses a sequence-to-sequence (seq-to-seq) map to build context-aware token representations. Classically, attention relies on the softmax function (SoftmaxAttn) to recover token representations as data-dependent convex combinations of values.</p><p>Despite its widespread use and effectiveness, softmax in SoftmaxAttn is not without limitations. For instance, the softmax function can sometimes lead to a concentration of attention on just a few features <ref type="bibr" target="#b59">(Yang et al., 2018;</ref><ref type="bibr" target="#b20">Ganea et al., 2019)</ref>, potentially neglecting other informative aspects of the input data. Moreover, applying SoftmaxAttn requires performing a row-wise reduction along the length of the input sequence, which in the case of efficient attention kernels <ref type="bibr" target="#b15">(Dao et al., 2022;</ref><ref type="bibr">Dao, 2023)</ref>, slows down computations. In this work, we relax this constraint by substituting the row-wise softmax operation with an element-wise sigmoid nonlinearity. We highlight that the central problem with naïve sigmoid attention (SigmoidAttn) is that of large initial attention norms and propose solutions to alleviate it. Our contributions are as follows:</p><p>(1) We prove SigmoidAttn is a universal function approximator on seq-to-seq tasks (Sec. 3.1).</p><p>(2) We analyze SigmoidAttn's regularity and provide its worst-case Jacobian bound (Sec. 3.2).</p><p>(3) We extend FLASHATTENTION2 <ref type="bibr" target="#b15">(Dao et al., 2022;</ref><ref type="bibr">Dao, 2023)</ref> with the sigmoid kernel, reducing kernel inference wall-clock time by up to 17% and real world inference by up to 8% (Sec. <ref type="bibr">4)</ref>. ( <ref type="formula" target="#formula_2">4</ref>) We show that SigmoidAttn matches SoftmaxAttn in various tasks and domains (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sigmoid Attention</head><p>Let X ∈ R n×d be the input sequence of n vectors, where each vector has dimension d. We define three learnable weight matrices W q ∈ R d×d qk , W k ∈ R d×d qk , and W v ∈ R d×dv , which are used to compute the queries Q ∈ R n×d qk , keys K ∈ R n×d qk , and values V ∈ R n×dv as follows:</p><formula xml:id="formula_0">Q = XW q , K = XW k , and V = XW v .</formula><p>(1)</p><p>Self-attention <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b55">Vaswani et al., 2017)</ref> can be compactly written as</p><formula xml:id="formula_1">SoftmaxAttn(X) = Softmax(QK T / d qk )V ,<label>(2)</label></formula><p>where the Softmax function normalizes each row of the input matrix. We replace the Softmax with SigmoidAttn(X) = σ(QK T / d qk )V , with σ : u → sigmoid(u + b) := (1 + e -(u+b) ) -1 .</p><p>(3)</p><p>Here, σ is applied element-wise to the input matrix in (3). The activation function σ has a hyperparameter b ∈ R. In App. E, we discuss an intuitive way to choose the order-optimal bias term, resulting in b =log(n). This choice of b allows us to make sense of SigmoidAttn for any sequence length. Indeed, letting (y 1 , . . . , y n ) = SigmoidAttn(X) be the output sequence, we have</p><formula xml:id="formula_2">y i = n j=1 exp(⟨W q x i , W k x j ⟩) exp(⟨W q x i , W k x j ⟩) + n W v x j -----→ n→+∞ exp(⟨W q x i , W k x⟩)W v xdµ(x),<label>(4)</label></formula><p>where µ = 1 n n j=1 δ xj is the empirical measure corresponding to X. Notably, ( <ref type="formula" target="#formula_2">4</ref>) still makes sense in the infinite length limit, where the measure µ is not a sum of Diracs. <ref type="bibr">Wortsman et al. (2023a)</ref> do not use a bias, and propose a n -1 normalization for various attention activations, such as sigmoid and ReLU, but leave the reason as an open question. Our variable bias has a similar effect in the large n limit, and we posit that recovering a finite output limit as n increases is the why it works in practice.</p><p>A multi-head version of ( <ref type="formula">3</ref>) is obtained by combining the outputs of several SigmoidAttn, as follows:</p><formula xml:id="formula_3">[SigmoidAttn 1 (X), . . . , SigmoidAttn h (X)] W o ,<label>(5)</label></formula><p>for a learnable output weight matrix W o ∈ R hdv×d , where h denotes the number of heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Properties of Sigmoid Attention</head><p>We analyze SigmoidAttn, with two objectives: <ref type="bibr">(1)</ref> showing that a transformer architecture remains a universal function approximator when SigmoidAttn replaces SoftmaxAttn, and (2) recovering a measure of regularity of SigmoidAttn by computing its Lipschitz constant.</p><p>3.1 Are Transformers with Sigmoid Attention Universal Approximators? <ref type="bibr" target="#b60">Yun et al. (2020)</ref> demonstrate that classical transformers can approximate continuous sequenceto-sequence functions to arbitrary precision, a property known as the Universal Approximation Property (UAP). UAP is highly desirable as it provides proof of an architecture's generalizability and representation capability. As SigmoidAttn modifies the transformer architecture, it is crucial to theoretically guarantee that this modification does not impact the representation capability and that UAP is retained. We provide this guarantee with the following theorem.</p><p>Theorem 3.1 (UAP for SigmoidAttn). We denote with T h,dv,r σ the class of transformer networks obtainable by combining an arbitrary number of SigmoidAttn layers (each of h heads of dimension d v ) followed by FFN layers of hidden dimension r. For any given continuous, permutation-equivariant function f : Ω ⊂ R n×d → R n×d with compact support Ω, and for any arbitrarily small error ε, there exists a transformer network g ∈ T 4,1,4   σ such that</p><formula xml:id="formula_4">Ω ∥f (X) -g(X)∥ p p dX ≤ ε, for 1 ≤ p &lt; ∞.<label>(6)</label></formula><p>Theorem 3.1 is the exact counterpart of <ref type="bibr">(Yun et al., 2020, Thm. 2)</ref>, which shows UAP for classical transformers. Our proof largely follows the same path, an outline of the original proof provided in App. C. Here, we present an overview of the main adaptations required to prove Thm. 3.1 for SigmoidAttn, with further details in App. C.1 and C.2.</p><p>Sigmoid Attention layers can implement contextual mappings: A key step in proving Thm. 3.1 is showing that, even with SigmoidAttn, a sequence of transformer blocks can implement a Contextual Mapping <ref type="bibr">(Yun et al., 2020, Def. 3.1)</ref>. A contextual mapping characterizes a function that maps each input sequence element to an output uniquely dependent on the whole sequence. This property allows a transformer to capture and store global context within each token, even if each layer only performs pairwise comparisons. Subsequent layers can then use this global information to map individual tokens to the correct output, ultimately approximating any arbitrary sequence-to-sequence function.</p><p>In <ref type="bibr" target="#b60">Yun et al. (2020)</ref>, the contextual mapping is assembled by modifying individual transformer blocks: each block is tuned to react to a specific input token. By stacking a sequence of these blocks, a transformer can be turned into an accumulator, mapping a given input token sequence to a unique global index. This outcome is achieved via a selective shift layer <ref type="bibr">(Yun et al., 2020, App. B.5)</ref>:</p><formula xml:id="formula_5">Ψ(X; b, b ′ ) i,1 := max k X k,1 -min k X k,1 if b &lt; X i,1 &lt; b ′ 0 otherwise,<label>(7)</label></formula><p>and can be approximated using classic attention. Although SigmoidAttn cannot directly approximate <ref type="bibr" target="#b64">(7)</ref>, our accumulator definition relies on an equivalent selective shift operation:</p><formula xml:id="formula_6">Ψ σ (X; b, b ′ ) i,1 := k:X k,1 &gt;b ′ X k,1 if b &lt; X i,1 &lt; b ′ 0 otherwise,<label>(8)</label></formula><p>which can be approximated by SigmoidAttn (described in App. C.1). In App. C.2.4, we show that ( <ref type="formula" target="#formula_6">8</ref>) shares similar properties with <ref type="bibr" target="#b64">(7)</ref>, allowing us to use the original proof framework in <ref type="bibr" target="#b60">Yun et al. (2020)</ref> and demonstrate that UAP holds in our case as well.</p><p>Our proof is largely equivalent to that in <ref type="bibr" target="#b60">Yun et al. (2020)</ref>, with two relevant differences: to approximate ( <ref type="formula" target="#formula_6">8</ref>), we require SigmoidAttn with at least four heads and shifts included in both query and key definitions. In contrast, SoftmaxAttn requires at least two heads to approximate <ref type="bibr" target="#b64">(7)</ref>, with shifts only in the query definition. However, this is primarily a theoretical requirement for the proof and does not affect performance. Notably, the total number of parameters required by both architectures for the approximation follows the same tight scaling of <ref type="bibr" target="#b60">Yun et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regularity of Sigmoid Attention</head><p>As with any layer in a neural network, the regularity of SigmoidAttn is important to study, as it gives insights into the robustness of the corresponding network and the ease of optimizing it. The most standard way to quantify the regularity of a layer function ϕ is to compute its Lipschitz constant over a set X , that is a constant C &gt; 0 such that for all X, Y ∈ X , it holds ∥ϕ(X)ϕ(Y</p><formula xml:id="formula_7">)∥ ≤ C∥X -Y ∥,</formula><p>where ∥ • ∥ is the standard Frobenius norm. The local Lipschitz constant is the spectral norm of the Jacobian of ϕ at X. The two are related: the Lipschitz constant of ϕ over X is the greatest local Lipschitz constant for all X ∈ X . We turn to the theorem giving the regularity of SigmoidAttn:</p><p>Theorem 3.2. Define A = {⟨W q x i W k x j ⟩|, i, j ∈ {1, . . . , n}} ⊂ R the set of attention weights, and the scaled activation norms σ ∞ = n × sup u∈A |σ(u)| and σ ′ ∞ = n × sup u∈A |σ ′ (u)|. Then, the Jacobian of SigmoidAttn at X = (x 1 , . . . , x n ) has a spectral norm of at most:</p><formula xml:id="formula_8">∥W v ∥ 2 σ ∞ + 2σ ′ ∞ ∥W T q W k ∥ 2 1 n n i=1 ∥x i ∥ 2 2 . (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>The proof is found in App. D. In SigmoidAttn, if we assume that the attention weights ⟨W q x i , W k x j ⟩ are all bounded by a constant µ -this is true, e.g., if the activations are bounded -we get σ ∞ ≤ exp(µ) and σ ′ ∞ ≤ exp(µ) thanks to the choice of b =log(n). The bound in Thm. 3.2 depends only on the average squared-norm of the input sequence x i , while classical results for the study of attention all rely on the largest value of ∥x i ∥ 2 2 <ref type="bibr" target="#b35">(Kim et al., 2021;</ref><ref type="bibr" target="#b6">Castin et al., 2023)</ref>. This is another consequence of the simplicity of sigmoid attention and is due to the removal of the normalizing constant in SoftmaxAttn. Our result implies that if all x i are within a ball of radius R then the Lipschitz constant of SigmoidAttn grows at most like R 2 , but it is stronger since we can apply this to unbounded distributions x i ; it matters only that the second moment is bounded. This result contrasts sharply with the bounds obtained for SoftmaxAttn: <ref type="bibr">Castin et al. (2023, Thm. 3.4.)</ref> show that there exists a sequence X = (x 1 , . . . , x n ) with ∥x i ∥ 2 ≤ R for all i such that the spectral norm of the Jacobian of Attn at X is at least cR 2 exp(cR 2 ) for some constant c &gt; 0. On the other hand, our bound scales in R 2 : this means that the local Lipschitz constant of SigmoidAttn is much lower than the worst local Lipschitz constant of SoftmaxAttn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computational Complexity of Sigmoid and Softmax.</head><p>Table <ref type="table">1</ref>: Forward floating operations per token per attention head. n ctx and d head are the context length and head dimension respectively. ∆ measures the compute difference between sigmoid and softmax as a multiple of the floating operations for computing the attention logits. c accounts for causal (c = (n ctx + 1)/2n ctx ∼ 1/2), or standard (c = 1) attention. Typical values are taken from the 1B LLM results (n ctx = 2048, d head = 64). The difference in floating operations between Sigmoid and Softmax attention mechanisms is subleading (∼ 1%) compared to other operations in the attention mechanism like computing attention logits L (shown below), and the attention matrix × values operation. This analysis precludes hardware aware improvements (Section 4). Memory speed has not kept pace with recent gains in computation speed <ref type="bibr" target="#b9">(Choquette, 2023;</ref><ref type="bibr" target="#b33">Jouppi et al., 2017;</ref><ref type="bibr" target="#b25">Hannun et al., 2023)</ref>. Consequently, attention computations on modern architectures have been IO-bound by memory accesses <ref type="bibr" target="#b31">(Ivanov et al., 2021)</ref>. FLASHATTENTION <ref type="bibr" target="#b15">(Dao et al., 2022)</ref> and FLASHATTENTION2 <ref type="bibr">(Dao, 2023)</ref> address these shortcomings by optimizing GPU memory hierarchy utilization to accelerate attention computations. Motivated by the speed boost provided by these approaches, we develop FLASHSIGMOID, a hardware-aware implementation of SigmoidAttn. Like previous works, FLASHSIGMOID employs three core ideas:</p><p>Tiling: Divide and Conquer Approach to Attention: Similar to FLASHATTENTION and FLASHATTENTION2, FLASHSIGMOID processes input parts in parallel to compute attention outputs in blocks, efficiently combining partial results to generate the final attention output.</p><p>Kernel Fusion: Like FLASHATTENTION and FLASHATTENTION2, FLASHSIGMOID implements the computational steps of both forward and backward passes of SigmoidAttn as single GPU kernels, minimizing memory accesses and improving memory efficiency by avoiding materialization of intermediate activations on High-Bandwidth Memory (HBM). Figure <ref type="figure">1</ref>: Average kernel speed-up for FLASHSIGMOID over FLASHATTENTION2 for sequence lengths 64-78k. Inference is 17.39% faster for self-attention and 18.76% for causal attention.</p><p>Training is 6.53% faster for self-attention and 9.46% for causal attention.</p><p>Activation Recomputation: The backward pass of sigmoid attention requires the sigmoid activation matrix, which, if materialized on GPU HBM, results in slower implementation and memory inefficiencies. FLASHSIGMOID addresses this by retaining only query, key, and value tensors for re-computation of the sigmoid activation matrix during the backward pass. Despite increased FLOPs, this approach proves faster in wall-clock time as well as more memory-efficient than the alterantive approach of materializing and retaining the attention matrix.</p><p>The forward and backward pass algorithms of FLASHSIGMOID can be found in App. F.1. Here, we highlight key differences between FLASHSIGMOID and FLASHATTENTION/FLASHATTENTION2.</p><p>The point-wise nature of SigmoidAttn results in a faster and more memory-efficient implementation by removing the need to compute the softmax normalization and materialize it to HBM. A reduction in the number of kernel dispatches also speeds up FLASHSIGMOID. Further, FLASHSIGMOID does not require accumulation and tracking of intermediate variables (row-sum and maximum of blocks) in the forward and backward passes which saves computation cost and reduces register pressure. We use sigmoid (x) = 0.5 • (1 + tanh (0.5 • x)) to optimize the sigmoid computation on GPU. The speed up in FLASHSIGMOID compared to FLASHATTENTION arises from optimizing hardware bottlenecks; theoretically, SigmoidAttn is slower than SoftmaxAttn (Sec. 3.3).</p><p>To measure the performance improvements of FLASHSIGMOID, we compare the timings of the kernels in its forward and backward passes against those of FLASHATTENTION2. The details of this benchmarking on H100 and A100 GPUs can be found in App. F.2. Measuring GPU computation time, we observe a 17.39% speed-up during inference and a 6.53% speed-up during training for attention over randomly initialized data on H100 GPU (Fig. <ref type="figure">1</ref>). In practice, these gains may be affected by other bottlenecks, such as movement of tensors between CPU or GPU memory, computations in other layers, and communication overhead in distributed training and inference. However, we demonstrate that FLASHSIGMOID speeds up training by ∼4% and inference by ∼8% in a realistic end-to-end setup. The details of wall-clock time improvements with FLASHSIGMOID are in App. F.3. We also note that practical machine learning workflows are dominated by inference rather than training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To empirically validate SigmoidAttn, we evaluate across several domains: supervised image classification using vision transformers <ref type="bibr" target="#b18">(Dosovitskiy et al., 2021)</ref>, self-supervised image representation learning with SimCLR <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr">Zhai et al., 2023a)</ref>, Bootstrap Your Own Latent (BYOL) <ref type="bibr" target="#b22">(Grill et al., 2020;</ref><ref type="bibr" target="#b3">Busbridge et al., 2023)</ref> and Masked AutoEncoders (MAE) <ref type="bibr" target="#b26">(He et al., 2022)</ref> as well as automatic speech recognition (ASR) <ref type="bibr" target="#b53">(Synnaeve et al., 2020;</ref><ref type="bibr">Gulati et al., 2020b)</ref> and auto-regressive language modeling (LM) <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>. We also validate sequence length generalization on TED-LIUM v3 <ref type="bibr" target="#b27">(Hernandez et al., 2018)</ref> for ASR and in small scale synthetic experiments in App. G.5.4. Across all these domains and algorithms, we demonstrate that SigmoidAttn matches the performance of SoftmaxAttn (Fig. <ref type="figure" target="#fig_2">2</ref> and 21), while offering training and inference speed-ups as highlighted in Sec. 4. Empirically we make the following observations: 0 200 400 600 Epoch -0.0 0.1 0.3 0.5 0.7 Train Loss BYOL ViT-B/16 0 100 200 300 Epoch 0.8 2.8 4.8 6.8 8.8 SimCLR ViT-B/16 0 100 200 300 400 Epoch 0.4 0.5 0.6 0.8 0.9 MAE ViT-L/16 0 100 200 300 Epoch 2.3 3.5 4.7 5.9 7.1 Train Loss Supervised ViT-B/16 100000 200000 300000 Steps 23.0 32.2 41.5 50.8 60.0 255M ASR Transformer 0 100000 200000 300000 Steps 1.9 2.3 2.7 3.2 3.6 1B LM (n = 2048) Softmax Sigmoid (1) SigmoidAttn is effective for vision tasks without a bias (except MAE), but relies on LayerScale to match the performance of the baseline SoftmaxAttn (Fig. <ref type="figure">9</ref>-a) in a hyper-parameter free manner. <ref type="foot" target="#foot_0">3</ref> All results presented for SoftmaxAttn also fairly add LayerScale unless specified.</p><p>(2) LM and ASR are sensitive to the initial norm ||σ(QK T / d qk )V ||. Modulation is required via (a) relative positional embeddings like ALiBi <ref type="bibr" target="#b48">(Press et al., 2022)</ref>, which reduces the initial attention norm by shifting logit mass to the zero regime under SigmoidAttn, (b) appropriate initialization of b to achieve the same effect -enabling usage of any positional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablations</head><p>We begin with ablations to dissect the benefits of each of our introduced components. To gain intuition about SigmoidAttn, we developed a research-friendly auto-regressive (AR) LM training framework to measure all components of attention and validate the effects of LayerScale, LayerNorm applied to Q and K (QK norm), different positional embedding techniques, and initialization values for b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mitigating Large Attention Norms</head><p>We train a single layer AR transformer block (E=3072, D_FF=12288) on the realnews split of C4 <ref type="bibr" target="#b49">(Raffel et al., 2020)</ref>. We train for 2 16 steps using a batch size of 6 and max sequence length of 4096 using a single cycle cosine learning rate (LR) schedule without weight decay. SigmoidAttn initially underperformed SoftmaxAttn when using absolute sinusoidal (SinCos) (Fig. <ref type="figure" target="#fig_3">3</ref>) or relative (Fig. <ref type="figure" target="#fig_4">4</ref>) positional embeddings (PE), which we attribute to high initial attention Frobenius norms, ∥σ(QK T / √ d)V ∥. A corresponding evolution of the attention distribution and sparsity can be seen in Appendix Fig. <ref type="figure" target="#fig_3">31</ref> and Fig. <ref type="figure" target="#fig_3">32</ref> on a synthetic task. To address these larger attention norms, we propose: (a) using ALiBi <ref type="bibr" target="#b48">(Press et al., 2022)</ref> whose relative bias moves initial attention logit mass to the zero region under the sigmoid activation, producing equivalent train negative log-likelihoods (Fig. <ref type="figure" target="#fig_5">5</ref>); or (b) set the attention logit bias b to a negative offset proportional to the sequence length, b ∝ln n (see App. G.1.2 for an ablation on b). This enables the usage of other PE techniques like RoPE <ref type="bibr">(Su et al., 2024)</ref> (Fig. <ref type="figure" target="#fig_6">6</ref>).    </p><formula xml:id="formula_10">∈ {3 × 10 -4 , 1 × 10 -3 , 3 × 10 -3 , 1 × 10 -2 , 3 × 10 -2 , 1 × 10 -1 , 3 × 10 -1 }. LR sensitivity is defined as E η∈[a,b] [min(ℓ(A(η)), ℓ 0 ) -ℓ * ] where ℓ(A(η))</formula><p>is the loss achieved by the learning algorithm A with LR η, ℓ 0 is the loss at initialization, and ℓ * is the loss achieved by the best LR. LayerScale is initialized at 10 -<ref type="foot" target="#foot_1">foot_1</ref> . Unlike vision tasks, where LayerScale improves performance (Fig. <ref type="figure">9</ref>-a), in LM, we observe that SoftmaxAttn slightly benefits from LayerScale, while the performance of SigmoidAttn remains largely unaffected.</p><p>Stability with QK Norm Theorem 3.2 indicates that the Jacobian of SigmoidAttn has favorable properties compared to SoftmaxAttn. We explore this by repeating the analysis of <ref type="bibr">Wortsman et al. (2023b)</ref>, as described in the LayerScale analysis, to investigate the impact of QK norm <ref type="bibr" target="#b16">(Dehghani et al., 2023)</ref>. For language modeling, both SigmoidAttn and SoftmaxAttn exhibit sensitivity to learning rate changes without QK norm. However, incorporating QK norm significantly stabilizes performance (Fig. <ref type="figure">8</ref>). In vision tasks, SigmoidAttn demonstrates robustness with and without QK norm (Fig. <ref type="figure">9</ref>-a) and without the need for n -α normalization from <ref type="bibr">Wortsman et al. (2023a)</ref>. Final loss use_layer_scale=False which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate use_layer_scale=True which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 7 10 8</p><p>Non-embed Params (M)</p><p>10 1 LR sensitivity which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax  Final loss qk_norm=False which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate qk_norm=True which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 7 10 8 Non-embed Params (M) 10 1 10 0 LR sensitivity which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax Figure 8: LR sensitivity QK norm ablation. 50 100 150 200 250 300 Epoch (a) 65 70 75 80 Test Top-1% +LayerScale,+QKNorm +LayerScale,-QKNorm -LayerScale,+QKNorm n -α ,-LayerScale,+QKNorm n -α ,-LayerScale,-QKNorm 50 100 150 200 250 300 Epoch (b) MHA Softmax MHA Sigmoid MQA Softmax MQA Sigmoid 50 100 150 200 250 300 Epoch (c) Softmax, +LayerScale,+QKNorm Sigmoid, +LayerScale,+QKNorm TanH, +LayerScale,+QKNorm ReLU, +LayerScale,+QKNorm GeLU, +LayerScale,+QKNorm ReLU 2 , +LayerScale, +QKNorm ReLU 2 , -LayerScale, -QKNorm Figure 9: ImageNet1k ViT-B/16 classification. (a) SigmoidAttn is robust without QK norm (+Lay-erScale, -QKNorm). Removing LayerScale reduces accuracy by 1.0% (-LayerScale, +/-QKNorm). n -α normalization (Wortsman et al., 2023a) underperforms without LayerScale. (b) SigmoidAttn multi-query attention (MQA) (Shazeer, 2019) with one head matches multi-head attention (MHA). (c) Sigmoid with LayerScale and QK norm performs comparably to other activations, except TanH. ReLU 2 (Hua et al., 2022) underperforms without LayerScale and QK norm.</p><p>Multi-query attention (MQA) In Fig. <ref type="figure">9</ref>-b we explore MQA <ref type="bibr" target="#b50">(Shazeer, 2019)</ref> for vision using only one head for {K, V }. We find that both SigmoidAttn and SoftmaxAttn perform equally well with or without multiple heads even at the small scale of ViT-B/16.</p><p>Activation Function Ablations As in <ref type="bibr">Wortsman et al. (2023a)</ref>, various activation functions, when combined with LayerScale and QK norm, perform equally well for vision tasks (Fig. <ref type="figure">9-c</ref>).</p><p>However, for sequence-critical tasks like ASR, activation functions such as ReLU pose instabilities and underperform. In the same figure, we also compare to the ReLU 2 proposal from Hua et al. ( <ref type="formula">2022</ref>) and find that it underperforms without LayerScale and QK norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supervised Image Classification</head><p>Vision transformers <ref type="bibr" target="#b18">(Dosovitskiy et al., 2021)</ref> extend transformers <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> to treat K × K image grids as disparate tokens. All tokens are refined through sequential layers of selfattention, pooled using a CLS token or global average pooling layer, and optimized using the negative log likelihood, ln p(y|x). We train ViT-B/16 models using R 224×224×3 images for 300 epochs using the recipe provided in App. G.2.4. We use the same set of training hyper-parameters for both SoftmaxAttn and SigmoidAttn, changing only the activation function between trials. The train negative log-likelihood is reported in Fig. <ref type="figure" target="#fig_2">2</ref> and the test top-1% is reported in Fig. <ref type="figure" target="#fig_25">21</ref>. We find that SigmoidAttn matches both the training dynamics and the evaluation performance of SoftmaxAttn. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Automatic Speech Recognition (ASR)</head><p>We benchmark ASR using LibriSpeech data <ref type="bibr" target="#b45">(Panayotov et al., 2015)</ref> on 100h and 960h settings of paired speech and text transcriptions. Our PyTorch implementations of encoder-based vanilla transformer <ref type="bibr" target="#b53">(Synnaeve et al., 2020)</ref> and conformer <ref type="bibr">(Gulati et al., 2020a)</ref> are trained with Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b21">(Graves et al., 2006)</ref> w/ BF16 mixed precision, w/o QK norm and w/o LayerScale. After extensively tuning SoftmaxAttn baselines, we switch to SigmoidAttn per (3) without any other changes. We investigate the effects of post/pre-LayerNorm, model depth, optimizer type, small data regime, and connection to local attention, with details in App. G.4. Our main findings are: i) CAPE (Likhomanenko et al., 2021) PE is the most unstable for SigmoidAttn; ii) post-LayerNorm models with SoftmaxAttn are hard to match with stable SigmoidAttn; iii) w/o QK norm SigmoidAttn is unstable and significant spikes happen in both gradient norms and training loss; iv) LayerScale is needed for generalization; v) learnable bias b = -10 gives no loss and gradient norms spikes while matching the SoftmaxAttn (which does not benefit from the improved throughput of FLASHSIGMOID); vi) adding a learnable bias, b = -5, to Q instead of the attention logits also solves the initial large attention norms for CAPE and ALiBi but not for RoPE; vii) b =log n gives rare (2-5 times) marginal gradient norms spikes with smooth loss while matching SoftmaxAttn.</p><p>Table 2 shows the main result for pre-LayerNorm transformers with CAPE, RoPE, and ALiBi, where SigmoidAttn uses LayerScale, QK norm, b =log n, and no sequence normalization. The bias is ablated with learnable bias (one per layer) in attention or Q with or without sequence normalization. SigmoidAttn is stabilized with bias while matching SoftmaxAttn, and b =log n works well. In most cases, bias allows generalization to longer sequences without sequence normalization, except for RoPE where it helps for longer sequences but hurts overall performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Autoregressive Large Language Modeling</head><p>We initially iterated at the 85M scale, as it served as a proxy for larger scale training. Our findings show that: i) attention bias is required for stability, which can be learnable, but setting it tolog(n),</p><p>where n is the maximum training sequence length of 4096, works well and is faster; ii) RoPE is more challenging to stabilize; iii) the final setting exhibits smooth loss curves, but still shows gradient norm fluctuations. We then turn our attention to validating SigmoidAttn at scale.</p><p>We train a 1B language model using the Llama2 <ref type="bibr" target="#b54">(Touvron et al., 2023)</ref> recipe with ALiBi instead of RoPE positional embedding, and the RedPajama (Computer, 2023) dataset (see App. G.3). At sequence length 4096, SigmoidAttn achieves a 1.23× step-time improvement over SoftmaxAttn in JAX without FLASHATTENTION (Tab. 3). All LLMs are trained using the AXLearn framework, which include the recipe and SigmoidAttn implementation.<ref type="foot" target="#foot_2">foot_2</ref> </p><p>SoftmaxAttn and SigmoidAttn have matching train and validation NLL at 85M (Fig. <ref type="figure" target="#fig_6">26</ref>) and at 1B scale when using 2048 sequence length (Fig. <ref type="figure" target="#fig_2">2</ref>). However, a slight disparity is observed at 1B scale when using 4096 sequence length, which we leave for future investigation (more details in App. G.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Recent studies in supervised image classification <ref type="bibr" target="#b56">(Wightman et al., 2021)</ref> and self-supervised learning (SSL), including approaches like SigLIP <ref type="bibr">(Zhai et al., 2023b)</ref>, are shifting large-scale machine learning training from output conditional categorical distributions, traditionally parameterized by softmax functions, to richer pointwise Bernoulli conditionals parameterized by sigmoid functions. In this study, our focus shifts to refining the model's internal mechanics, specifically by substituting the softmax component of the attention mechanism with a pointwise sigmoid function.</p><p>Previous work has explored the replacing softmax with the ReLU activation in both practical <ref type="bibr">(Shen et al., 2023;</ref><ref type="bibr" target="#b28">Hron et al., 2020)</ref> and theoretical settings <ref type="bibr" target="#b1">(Bai et al., 2023;</ref><ref type="bibr" target="#b19">Fu et al., 2023)</ref>. Other works explores using the ReLU 2 activation <ref type="bibr" target="#b29">(Hua et al., 2022)</ref>, exploring purely linear attention <ref type="bibr" target="#b34">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b41">Lu et al., 2021;</ref><ref type="bibr" target="#b36">Koohpayegani &amp; Pirsiavash, 2024)</ref> or cosine-similarity based attention <ref type="bibr" target="#b42">(Luo et al., 2018;</ref><ref type="bibr" target="#b39">Liu et al., 2022)</ref>. Our work builds upon these explorations, particularly <ref type="bibr">Wortsman et al. (2023a)</ref>, which replaces softmax with various activation functions scaled by n -α , where n corresponds to the sequence length and α, a hyper-parameter. However, we find that their formulation does not match expected performance without proper b initialization and the use of LayerScale (Fig. <ref type="figure">9</ref>-a, App. G.1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present a comprehensive theoretical and empirical study of sigmoid attention as an alternative to softmax attention in transformers. We prove that transformers with sigmoid attention are universal function approximators with improved regularity, and identify LayerScale and prevention of large initial attention norms as key factors for successful training. We introduce FLASHSIGMOID, a memory-efficient variant providing a 17% inference kernel speed-up. Extensive experiments across language, vision, and speech demonstrate that properly normalized sigmoid attention matches softmax attention performance on various tasks and scales. Our findings establish sigmoid attention as a viable alternative, unifying prior work and establishing best practices for its application in transformers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations</head><p>While our work demonstrates that SigmoidAttn can serve as a viable drop-in replacement for SoftmaxAttn in many domains and scales, there are a few key limitations to note:</p><p>(1) In large-scale (1B parameter, 4096 context length) language modeling, we observed some gradient norm spikes and a slight performance gap between SigmoidAttn and SoftmaxAttn (Table <ref type="table" target="#tab_7">3</ref>). While runs at smaller context lengths (1B parameter, n=2048) were stable and matched SoftmaxAttn performance, further research is needed to fully close the performance gap and ensure training stability for very large language models using SigmoidAttn.</p><p>(2) Our theoretical analysis proves that transformers with SigmoidAttn are universal function approximators and have improved regularity compared to SoftmaxAttn. However, the bounds we derive, while tighter than those for SoftmaxAttn, may not be maximally tight.</p><p>There could be room for further theoretical refinements.</p><p>(3) We focused our empirical evaluation on standard benchmarks in language, vision, and speech domains. Performance on more niche or emerging applications remains to be validated. ( <ref type="formula" target="#formula_2">4</ref>) In automatic speech recognition experiments, we observed that SigmoidAttn can be sensitive to the choice of positional embeddings and may require careful initialization of the attention bias term to ensure stable training. Specifically, we found that the CAPE positional embedding was the most unstable for SigmoidAttn. Further work is needed to develop robust initialization schemes that work well across different positional embeddings. Moreover we found that w/o QK norm or with post-LayerNorm SigmoidAttn is unstable and can underperforms SoftmaxAttn, thus further investigation is needed.</p><p>(5) FLASHSIGMOID demonstrates promising inference and training speed-ups by exploiting SigmoidAttn's simpler kernel structure compared to SoftmaxAttn. However, realizing these gains at scale in distributed training setups may require additional engineering to optimize communication bottlenecks.</p><p>Despite these limitations, we believe this work establishes a strong foundation for SigmoidAttn, unifying prior art and demonstrating its potential as a drop-in SoftmaxAttn replacement. We hope our theoretical grounding and empirical results motivate further research into this simple yet effective architectural variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Broader Impact</head><p>The development of efficient and theoretically grounded attention mechanisms has the potential for significant positive impact across a range of applications. By establishing SigmoidAttn as a viable alternative to SoftmaxAttn, our work expands the toolkit of architectural choices available to researchers and practitioners. Positive impacts of this work may include:</p><p>(1) Improved computational efficiency: FLASHSIGMOID's faster kernel implementation could lead to more efficient training and inference for attention-based models, reducing energy consumption and enabling deployment on resource-constrained devices. This could democratize access to powerful models.</p><p>(2) Theoretical understanding: Our universal approximation results and tighter bounds on the regularity of SigmoidAttn contribute to a deeper theoretical understanding of this key component. A stronger theoretical foundation can guide principled model design and architectural search.</p><p>(3) Application-specific benefits: Across language, vision, and speech domains, SigmoidAttn's performance could translate into improved user experiences, such as more natural language interactions, enhanced image understanding, and robust speech recognition. These advancements could have positive societal impacts, such as improved accessibility tools and more effective educational technologies.</p><p>However, as with any foundational machine learning advance, there are also risks of negative impacts that must be considered and mitigated:</p><p>(1) Fairness and bias considerations: As with any machine learning model, it is important to carefully evaluate SigmoidAttn based models for fairness and potential biases when applied to sensitive use cases. The unique properties of SigmoidAttn may have unexpected interactions with data biases. Researchers and practitioners should follow best practices for auditing and mitigating unwanted biases to ensure equitable outcomes.</p><p>(2) Environmental impact: While FLASHSIGMOID is more computationally efficient than FLASHATTENTION, the overall trend of scaling up attention-based models has significant energy costs. Further efficiency improvements and the use of renewable energy sources are important to mitigate environmental harms.</p><p>We believe that the benefits of SigmoidAttn outweigh the risks, but it is crucial for the research community to actively consider and address these potential negative impacts. By doing so, we can work towards a future where the efficiency and expressivity of SigmoidAttn are used for societal benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Universal Approximation Property for Sigmoid Attention</head><p>This section is dedicated to the proof for the Universal Approximation Property for attention equipped with sigmoid nonlinearity. The proof follows closely the one provided in <ref type="bibr">Yun et al. (2020, Sec. 3)</ref>, of which we inherit much of the notation, and we encourage the interested reader to refer to the original source for a more comprehensive understanding of its details. Here we first provide context by outlining the main steps in the original proof, before proceeding to adapt its key components to the SigmoidAttn case.</p><p>The proof aims at showing that a transformer network can approximate to arbitrary accuracy any continuous, permutation-equivariant function with compact support. The proof is constructive in nature, in that it explicitly defines the architecture (and particularly, the sequence of self-attention and feed-forward layers) that can approximate a given target function. To do so, it proceeds in steps (see <ref type="bibr">Yun et al. (2020, Sec.</ref> 3.2)):</p><p>(1) prove that any continuous function with compact support can be approximated to arbitrary accuracy by a piecewise constant function</p><p>(2) prove that an aptly-constructed modified transformer network, (where the softmax nonlinearity is substituted with a hardmax nonlinearity), can exactly represent such piecewise constant function. This step is further divided into three sub-steps (see <ref type="bibr">Yun et al. (2020, Sec. 4)</ref>):</p><p>(a) prove that a series of feed-forward layers can quantize any input to a specific discretization grid in the compact domain (b) prove that a series of self-attention layers can implement a contextual mapping (see <ref type="bibr">Yun et al. (2020, Def.</ref> 3.1)) (c) prove that a series of feed-forward layers can map the output of the contextual mapping to the desired output of the target piecewise-constant approximation</p><p>(3) prove that a (classical) transformer network can approximate such modified transformer network to arbitrary accuracy Fortunately, some of the steps outlined above do not rely on a specific nonlinear function being used within the attention mechanism, and can be directly reused in our proof, virtually unchanged. Notice however that Steps (2-b) and ( <ref type="formula">3</ref>) are directly impacted by modifications to the attention layer, and hence require adaptation in our case. This is the focus of the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Step (3): Sigmoid Transformers can Approximate Modified Sigmoid Transformers</head><p>In <ref type="bibr" target="#b60">Yun et al. (2020)</ref>, to implement contextual mappings, the authors rely on a modified version of transformers, for the sake of simplifying the analysis. In their modified version, the (row-wise) softmax operation is substituted with a (row-wise) hardmax operation. This substitution is valid because a classical transformer can still be made arbitrarily close to such modified transformer, in light of the fact that softmax(λX) λ→∞ ----→ hardmax(X). ( <ref type="formula">10</ref>) In our proof, we follow a similar strategy to define our modified sigmoid transformer (and in particular, its self-attention mechanism). We have that</p><formula xml:id="formula_11">σ(λX) λ→∞ ----→ H(X),<label>(11)</label></formula><p>where σ(x) = (1 + e -x ) -1 is the (elementwise) sigmoid function, while</p><formula xml:id="formula_12">H(x) =    1 x &gt; 0 1 2 x = 0 0 x &lt; 0 (12)</formula><p>denotes the (elementwise) Heaviside step function. This allows us to define our modified sigmoid self-attention layer, as follows.</p><p>Definition C.1 (Modified sigmoid self-attention layer). Given an input X ∈ R d×n , the action of a modified sigmoid self-attention layer with shifts and a single one-dimensional head is defined as</p><formula xml:id="formula_13">X → X + ψ(X; q, b q , k, b k , v, o), where ψ(X; q, b q , k, b k , v, o) = o v T X H q T X -b T q T k T X -b T k (13)</formula><p>with q, k, v ∈ R d representing the query, key, and value vectors, b q , b k ∈ R n the corresponding query and key bias vectors, while o ∈ R d denotes the output vector.</p><p>Analogously to ( <ref type="formula">10</ref>), ( <ref type="formula" target="#formula_11">11</ref>) guarantees that sigmoid attention can approximate modified sigmoid attention by simply increasing the magnitude of its inner parameters.</p><p>Here and in the following, the length of the input sequence is denoted as n, while d represents the dimensionality of the tokens. Notice that we are considering the input tensor X ∈ R d×n , (as opposed to ∈ R n×d ) to better align out notation with the one used in <ref type="bibr" target="#b60">Yun et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Step (2-b): Modified Sigmoid Transformers can Implement Contextual Mappings</head><p>The core of the proof consists in showing how, by opportunely combining the operations in ( <ref type="formula">13</ref>), one can build an architecture capable of implementing a contextual mapping. For completeness, we report next the definition of such a map (see also <ref type="bibr">Yun et al. (2020, Def.</ref> 3.1)).</p><p>Definition C.2 (Contextual mapping). A map q : L → R n from a finite set L ⊂ R d×n is said to be a contextual mapping if both the following conditions hold:</p><formula xml:id="formula_14">(i) q i (X) ̸ = q j (X), ∀i ̸ = j and ∀X ∈ L (ii) q i (X) ̸ = q j (X ′ ), ∀i, j and ∀X, X ′ ∈ L, with X ̸ = X ′</formula><p>where q i (X) denotes the i-th component of q(X).</p><p>Namely, a contextual mapping is such that it transforms each token in an input sequence to a value depending uniquely on the whole sequence. By satisfying this property, we can ensure that any element of the quantization of the input domain (achieved by Step (2-a)) can be mapped to a unique identifying value (depending on the whole input) via a sequence of modified sigmoid self-attention layers. It is then up to the MLP (in Step (2-c)) to correctly map this value to the corresponding output value in the piece-wise constant approximation.</p><p>In particular, after defining a uniform discretization (characterized by the parameter δ) of the unitary hypercube</p><formula xml:id="formula_15">[0, 1] d ⊂ R d , namely G δ := {g : g i ∈ {0, δ, 2δ, . . . , 1 -δ}, ∀i = 1 . . . d},<label>(14)</label></formula><p>we consider as input a tensor X (composed of columns</p><formula xml:id="formula_16">X = [x i ] n i=1 ) such that X ∈ L := {X : x i ∈ G δ ∀i = 1 . . . n, and x i ̸ = x j ∀i ̸ = j} ⊂ R d×n ,<label>(15)</label></formula><p>that is, a 2D tensor whose columns are element of the discretization G δ , and that all differ from each other (at least for one element). We want to build a contextual mapping acting on L, by stacking layers parameterized according to Def. C.1. In App. C.2.1 we define the basic building blocks of our architecture; in App. C.2.2 we describe how to stack them, and the effect the architecture has on a given input; finally, in App. C.2.4 we prove that this architecture indeed implements a contextual mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Basic Building Blocks of Contextual Mapping</head><p>The strategy we follow to assemble a contextual mapping consists in sequentially looking at each column of the input, progressively updating and storing information regarding its content in a uniquely identifiable manner, and finally broadcasting this information back to every element in the sequence. The difficulty lies in the fact that each of these updates must be carried on while relying solely on applications of the modified SigmoidAttn layer in Def. C.1. In the following, we describe how we can tweak its parameters to achieve exactly this.</p><p>From d-dimensional quantized vectors to scalars As a first simplification, we can get rid of the d-dimension in the X tensor by mapping each of its columns to a corresponding identifying scalar, uniquely defined by the specific column components. This step is also performed in <ref type="bibr">Yun et al. (2020, App. B.5)</ref>, and can be achieved rather straightforwardly, by defining</p><formula xml:id="formula_17">v ≡ q ≡ k ≡ u := [1, δ -1 , δ -2 , . . . , δ -d+1 ] T . (<label>16</label></formula><formula xml:id="formula_18">)</formula><p>Notice in fact that, since each column x i belongs to G δ , it can equivalently be written in the form</p><formula xml:id="formula_19">x i = δ • [id 0,i , id 1,i , . . . , id d-1,i ]</formula><p>T , where id j,i ∈ {0, 1, 2, . . . , δ -1 -1} represents the (indexed) coordinate of the discretization along the j-th dimension. Scalar-multiplying X by u in ( <ref type="formula" target="#formula_17">16</ref>), then, turns this tuple of indices into a single one, in a bijective fashion<ref type="foot" target="#foot_3">foot_3</ref> .</p><p>This allows us to equivalently consider a single vector u T X ∈ R n , rather than the whole tensor X ∈ R d×n in the remainder of our analysis. Analogously, choosing o ≡ e 0 := [1, 0, . . . , 0] T in ( <ref type="formula">13</ref>) constraints the effect of the layer application to impact only the first row of the tensor: the goal is then to store in this row the result of the target contextual mapping q in Def. C.2. To slim our notation, in the following we often refer to u T X as the vector l ∈ R n , with components l i .</p><p>In light of the simplification above, we can rewrite (13) more compactly, as follows:</p><formula xml:id="formula_20">ψ(X; q = k = v ≡ u, o ≡ e 0 ; b q , b k ) = e 0 l T H ((l -b q ) ⊗ (l -b k ))<label>(17)</label></formula><p>Notice that, since the elements of both X and u are always non-negative, so are those of l, too. Moreover, since we are interested in permutation-equivariant functions with respect to the columns of X, without loss of generality we can consider the elements of l = u T X to be ordered: 0 ≤ l i &lt; l j , ∀i &lt; j.</p><p>Selective shift operation for sigmoid attention Since we aim to recover a contextual map by sequentially updating the elements of l, we proceed by designing a modification of ( <ref type="formula" target="#formula_20">17</ref>) which affects only a certain selected element at a time. This is were our second simplification comes into play, and this time it pertains the roles of the bias vectors b q and b k . Since l ≥ 0, these vectors have the effect of tweaking the sign of the inner arguments of the Heaviside function in ( <ref type="formula" target="#formula_20">17</ref>), hence directly impacting when its application outputs 0 or 1. By aptly selecting the values of b k and b q , then, we can explicitly decide when a specific layer triggers an update, which elements are affected by the update, and what elements to consider to compute the update itself.</p><p>More in detail, take b q = 1b q and b v = 1b v , for some scalars b q , b v , and with 1 being the all-one vector. Plugging this into (17), we have</p><formula xml:id="formula_21">ψ(X; b q , b k ) := ψ(X; q = k = v ≡ u, o ≡ e 0 , b q = 1b q , b k = 1b k ) = e 0 l T H ((l -1b q ) ⊗ (l -1b k )) = e 0 i:li&lt;bv l i if l j &lt; b k i:li&gt;bv l i if l j &gt; b k ;<label>(18)</label></formula><p>notice how b q determines what elements of l compose the update (as it impacts the indices considered in the sum), while b k defines the elements impacted by the update itself <ref type="foot" target="#foot_4">7</ref> . If we opportunely combine four modified sigmoid self-attention heads ψ(X; b q , b k ), we recover, for a given index i = 0 . . . δ -d -1,</p><formula xml:id="formula_22">Ψ (i) (X) :=X + 1 2 c     ψ X; b q = 0, b k = i -1 2 δ -ψ X; b q = 0, b k = i + 1 2 δ -ψ X; b q = b k = i + 1 2 δ + ψ X; b q = i + 1 2 , b k = i -1 2 δ     =X + 1 2 ce 0 l T     H l ⊗ l -i -1 2 δ -H l ⊗ l -i + 1 2 δ -H l -i + 1 2 δ ⊗ l -i + 1 2 δ +H l -i + 1 2 δ ⊗ l -i -1 2 δ     =⇒Ψ (i) 1,j (X) = X 1,j + c k:l k &gt;iδ l k if l j = iδ 0 otherwise =⇒Ψ (i) k&gt;1,j (X) = X k,j ,<label>(22)</label></formula><p>where c ≡ c(δ, d, n) is a multiplicative constant which will be chosen later.</p><p>The operator assembled in ( <ref type="formula" target="#formula_22">22</ref>) defines the basic layer of the architecture that we use in our proof. Notice Ψ (i) (X) has the effect of modifying only the column x j which has index l j = u T x j = iδ (if at all present in the input X). This layer covers a similar role to the selective shift operation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Result of Applying a Sequence of Selective Shifts</head><p>Ultimately we want to show how, by stacking a sequence of selective shift layers ( <ref type="formula" target="#formula_22">22</ref>) for increasing i = 0 . . . δ -d -1 and one additional global shift, we can build an architecture capable of representing</p><formula xml:id="formula_23">lj &lt; b k lj &gt; b k H (l ⊗ (l -1b k )) =   0 • • • 0 1 • • • 1 . . . . . . . . . . . . . . . . . . 0 • • • 0 1 • • • 1   .<label>(19)</label></formula><p>This shows how, by modifying b k , one can decide which columns will receive an update: namely, all those with index lj &gt; b k . By combining two such operators with b</p><formula xml:id="formula_24">k = i -1 2 δ and b k = i + 1 2 δ, we then recover lj = iδ H l ⊗ l -1 i -1 2 δ -H l ⊗ l -1 i + 1 2 δ =   0 • • • 0 1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 1 0 • • • 0   ,<label>(20)</label></formula><p>which allows us to limit the update to only one specific column: the one with index lj = iδ.</p><p>The parameter bq acts analogously, but varies the output of the Heaviside function as we move down the rows, rather than the columns. The same operator as in ( <ref type="formula" target="#formula_24">20</ref>), but with bq = i + 1 2 δ gives us in fact:</p><formula xml:id="formula_25">lj = iδ H l -1 i + 1 2 δ ⊗ l -1 i -1 2 δ -H l -1 i + 1 2 δ ⊗ l -1 i + 1 2 δ =         0 • • • 0 -1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 -1 0 • • • 0 0 • • • 0 1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 1 0 • • • 0         lj &lt; iδ lj &gt; iδ .<label>(21)</label></formula><p>Finally, ( <ref type="formula" target="#formula_22">22</ref>) can be recovered by combining ( <ref type="formula" target="#formula_24">20</ref>) and ( <ref type="formula" target="#formula_25">21</ref>): this has the effect of removing the -1's in <ref type="bibr" target="#b72">(21)</ref>.</p><p>a contextual mapping. As a preliminary step, in this section we provide an explicit formula for the result of applying such an architecture. Once again, we are proceeding analogously to <ref type="bibr">Yun et al. (2020, App. B.5.1)</ref>.</p><p>After the first selective shift application Consider a quantized input sequence X ∈ L as defined in <ref type="bibr" target="#b68">(15)</ref>, with its columns ordered according to their scalar indices l = u T X. The sequence of selective shift layers Ψ (0) , Ψ (1) , . . . initially has no effect on the input itself, and it leaves it unchanged until we hit the layer corresponding to the index of the first column in the input, Ψ ( î) , where l 1 = u T x 1 = îδ. At this point, following ( <ref type="formula" target="#formula_22">22</ref>), the first column of the input is modified into</p><formula xml:id="formula_26">x 1 → Ψ ( î) |,1 (X) = x 1 + ce 0 k:l k &gt;l1 l k = x 1 + ce 0 n k=1 l k -l 1<label>(23)</label></formula><p>while the other columns are still left untouched. In the following, we compactly refer to the quantities</p><formula xml:id="formula_27">n k=1 l k -l i as s i : s = [s 1 , s 2 , . . . , s n ] T := n k=1 l k -l 1 , n k=1 l k -l 2 , . . . , n k=1 l k -l n T . (<label>24</label></formula><formula xml:id="formula_28">)</formula><p>According to ( <ref type="formula" target="#formula_26">23</ref>), the index l 1 of column x 1 is then analogously mapped to</p><formula xml:id="formula_29">l 1 = u T x 1 → l1 := u T Ψ ( î) |,1 (X) = u T x 1 + cs 1 = l 1 + cs 1 .<label>(25)</label></formula><p>Notice that, by choosing c &gt; 1, we can ensure</p><formula xml:id="formula_30">c &gt; 1 =⇒ l1 &gt; l 1 + n k=1 l k - l 1 &gt; n k=1 &gt; l i ∀i,<label>(26)</label></formula><p>and particularly l1 &gt; l 2 , implying that at the next (effective) application of the selective shift operation, this term, too, will contribute to the update.</p><p>Subsequent selective shift applications Following similar considerations, the next effective update will be applied by the layer Ψ ( î) with l 2 = u T x 2 = îδ. At this point, the second column index is updated as follows:</p><formula xml:id="formula_31">l 2 = u T x 2 → l2 :=u T Ψ ( î) |,2 (X) = u T x 2 + c k:l k &gt;l2 l k + l1 =l 2 + c n k=1 l k -l 2 - l 1 + l 1 + cs 1 = l 2 + cs 2 + c 2 s 1 (<label>27</label></formula><formula xml:id="formula_32">)</formula><p>where l1 is also included in light of ( <ref type="formula" target="#formula_30">26</ref>), and we used the definitions ( <ref type="formula" target="#formula_27">24</ref>) and ( <ref type="formula" target="#formula_29">25</ref>). Continuing to apply Ψ (i) (X), for increasing i, and unrolling the recursion, we recover <ref type="bibr">)</ref> which eventually allows us to write the general formula 8</p><formula xml:id="formula_33">l3 = l 3 + c n k=1 l k -l 1 -l 2 -l 3 + l1 + l2 = l 3 + cs 3 + c 2 (s 2 + s 1 ) + c 3 s 1 l4 = l 4 + c n k=1 l k -l 1 -l 2 -l 3 -l 4 + l1 + l2 + l3 = l 4 + cs 4 + c 2 (s 3 + s 2 + s 1 ) + c 3 (s 2 + 2s 1 ) + c 4 s 1 l5 = l 5 + c n k=1 l k -l 1 -l 2 -l 3 -l 4 -l 5 + l1 + l2 + l3 + l4 = l 5 + cs 5 + c 2 (s 4 + s 3 + s 2 + s 1 ) + c 3 (s 3 + 2s 2 + 3s 1 ) + c 4 (s 2 + 3s 1 ) + c 5 s 1 . . . (<label>28</label></formula><formula xml:id="formula_34">lj := l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 , j = 1 . . . n.<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3 Result of Applying One Last Global Shift Layer</head><p>After the last selective shift layer, the original input X has been mapped to a modified one X whereby each column xj is characterized by the index lj = u T xj given in <ref type="bibr" target="#b86">(29)</ref>. Remember our goal is to recover a contextual mapping, but notice that these lj indices are not uniquely defined by the input 9 ; in other words, they do not satisfy property (2) in Def. C.2. The only exception to this is the last index ln , as (loosely speaking) it has "seen" all the previous updates -and indeed in App. C.2.4 we prove this rigorously, under some assumption on the yet-undefined coefficient c(δ, d, n).</p><p>A straightforward way to recover a one-to-one mapping for the whole sequence, then, is to update every index lj via a quantity directly depending on ln . This is precisely what the last global shift layer Ψ(X) aims to accomplish. This last layer is also defined starting from the simplified modified sigmoid attention ( <ref type="formula" target="#formula_21">18</ref>), by picking b k = 0 and b q = c(δ, d, n) n + 1 2 δ: if, for any input, we can guarantee that lj ≤ c(δ, d, n) n δ j &lt; n and ln &gt; c(δ, d, n) n δ, (30) then the application of the global shift layer would result in 10 :</p><formula xml:id="formula_35">Ψ( X) := X + c n+1 ψ X; b q = c n + 1 2 δ, b k = 0 =⇒ Ψ1,j ( X) = X1,j + c n+1 ln =⇒ Ψk&gt;1,j ( X) = Xk,j .<label>(32)</label></formula><p>The global shift ( <ref type="formula" target="#formula_35">32</ref>) is the last layer we need to define our candidate contextual mapping. Collecting the results from this section together, our architecture is defined by sequentially composing the selective shift layers with the global shift one,</p><formula xml:id="formula_36">Ψ(X) := Ψ • Ψ (δ -d -1) • • • • • Ψ (2) • Ψ (1) (X).</formula><p>(33) After being scalar-multiplied by u, this results in a sequence q(X) := u T Ψ(X) = l + c n+1 1 ln (34) which we aim to prove is a contextual mapping. This is shown in the next section.</p><p>8 From ( <ref type="formula" target="#formula_33">28</ref>), we can notice that, for a given lk , the coefficients a (k) i,j appearing in front of the various s k-i for each of the c j terms, are first given by a list of ones, a</p><p>i,1 = 1, then a list of increasing numbers a</p><formula xml:id="formula_38">(k) i,2 = i =⇒ a (k) -,2 = cumsum(a (k) -,1 ), then a list of triangular numbers a (k) i,3 = i(i + 1)/2 =⇒ a (k) -,3 = cumsum(a (k)</formula><p>-,2 ), and so on:</p><formula xml:id="formula_39">a (k) -,j = cumsum(a (k)</formula><p>-,j-1 ). The result of iterated applications of cumsum, starting from an all-one vector, can be compactly described via the binomial coefficient: we have in fact</p><formula xml:id="formula_40">ai,j = [cumsum j ([1, 1, . . . ])]i = i + j -2 j -1 .</formula><p>The actual formula ( <ref type="formula" target="#formula_34">29</ref>) can be recovered after a few algebraic steps, by rearranging the summation indices. 9 To convince ourselves of this, it suffices to look at the formula for ( <ref type="formula" target="#formula_29">25</ref>): two sequences with different elements l ̸ = l ′ , but such that l1 = l ′ 1 and s1 = s ′ 1 (that is, with n i=1 li = n i=1 l ′ i ) would map to the same l1 = l′ 1 . 10 As in footnote 7, this is also better seen by considering the resulting modified sigmoid attention matrix. With b k = 0 and bq = c(δ, d, n) n + 1 2 δ, in fact, if condition ( <ref type="formula">30</ref>) is verified, this matrix is given by</p><formula xml:id="formula_41">H l -1 c n + 1 2 δ ⊗ l =     0 • • • 0 . . . . . . . . . 0 • • • 0 1 • • • 1     lj, j &lt; n ln .<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.4 A Sequence of Selective Shifts Followed by a Global Shift Produces a Contextual Mapping</head><p>To complete the proof, it remains to show that the recovered sequence (34) represents a contextual mapping and, in particular, that it is (i) one-to-one in L, and that (ii) all of its elements are distinct for different inputs. To do so, we need a few preparatory lemmas. The first few are needed to show that each of the basic components of ( <ref type="formula">34</ref>) is indeed a one-to-one map.</p><p>Lemma C.3. The map l → s in ( <ref type="formula" target="#formula_27">24</ref>) is one-to-one.</p><p>Proof. The target map can be compactly represented as a linear operator S:</p><formula xml:id="formula_42">l → s := 1 n k=1 l k -l = (1 ⊗ 1 -I)l =: Sl<label>(35)</label></formula><p>which is invertible <ref type="foot" target="#foot_5">11</ref> , denoting that l → s is bijective.</p><p>Lemma C.4. The map l → ln in ( <ref type="formula" target="#formula_34">29</ref>) is one-to-one, under the condition</p><formula xml:id="formula_43">c(δ, d, n) &gt; (n -1)(δ -d -1) n -1 n-1 2 . (<label>36</label></formula><formula xml:id="formula_44">)</formula><p>Proof. Consider two vectors of column indices l, l ′ differing for at least one element. We have by definition ( <ref type="formula" target="#formula_34">29</ref>) that</p><formula xml:id="formula_45">ln -l′ n = (l n -l ′ n ) + c(s n -s ′ n ) + n-2 i=0 c i+2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 )<label>(37)</label></formula><p>By absurd, assume ln -l′ n = 0 even though ∃i :</p><formula xml:id="formula_46">l i ̸ = l ′ i .</formula><p>We have then that it must hold</p><formula xml:id="formula_47">(l ′ n -l n ) = c(s n -s ′ n ) + n-2 i=0 c i+2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) = c (s n -s ′ n ) + n-2 i=0 c i+1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 )<label>(38)</label></formula><p>Notice that, for c(δ, d, n) large enough, the right-hand side does not have enough granularity to counter the left-hand side: in fact, since l n ∈ {0, δ, 2δ, . . . , δ -d+1 -δ}, the left-hand side can attain values l ′ nl n ∈ {0, ±δ, ±2δ, . . . , ±(δ -d+1δ)} (39) while the former, in light of the presence of the c(δ, d, n) factor, can only attain values ∈ {0, ±cδ, ±2cδ, . . . }. Picking c &gt; δ -d -1, then, ensures that equality between the two sides of (38) can only be achieved if they are both 0. In this case, we need to impose</p><formula xml:id="formula_48">c(s ′ n -s n ) = n-2 i=0 c i+1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) ⇐⇒ s ′ n -s n = c n-2 i=0 c i n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) .<label>(40)</label></formula><p>Similarly, notice that<ref type="foot" target="#foot_6">foot_6</ref> , ∀i,</p><formula xml:id="formula_49">|s i -s ′ i | = n k=1 (l k -l ′ k ) -(l i -l ′ i ) = n k=1,k̸ =i (l k -l ′ k ) &lt; (n -1)(δ -d+1 -δ),<label>(41)</label></formula><p>implying that s ′ ns n ∈ {0, ±δ, ±2δ, . . . , ±(n -1)(δ -d -1)δ}. Again, by picking c(δ, d, n) &gt; (n -1)(δ -d -1) we ensure that the right-hand side does not have enough granularity, and hence</p><formula xml:id="formula_50">c(δ, d, n) &gt; (n -1)(δ -d -1) =⇒ s ′ n -s n = 0, (42) implying c n-2 i=0 c i n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) = 0 ⇐⇒ n-2 k=0 k 0 (s ′ k+1 -s k+1 ) = c n-2 i=1 c i-1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) ⇐⇒ n-2 k=0 (s ′ k+1 -s k+1 ) = c n-2 i=1 c i-1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) .<label>(43)</label></formula><p>Following a similar reasoning as the one applied above shows us that picking</p><formula xml:id="formula_51">c(δ, d, n) &gt; (n -1) 2 (δ -d -1) =⇒ n-2 k=0 (s k+1 -s ′ k+1 ) = 0,<label>(44)</label></formula><p>and requires us to satisfy</p><formula xml:id="formula_52">c n-2 i=1 c i-1 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) = 0 ⇐⇒ n-2 k=1 k 1 (s ′ k -s k ) = c n-2 i=2 c i-2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) ⇐⇒ n-2 k=1 k(s ′ k -s k ) = c n-2 i=2 c i-2 n-2 k=i k i (s k-i+1 -s ′ k-i+1 ) .<label>(45)</label></formula><p>Once again, then, by choosing</p><formula xml:id="formula_53">c(δ, d, n) &gt; (n -2)(n -1) 2 2 (δ -d -1) =⇒ n-2 k=1 k(s k -s ′ k ) = 0. (<label>46</label></formula><formula xml:id="formula_54">)</formula><p>This reasoning can be repeated recursively: at each step i of the recursion, by imposing a stricter and stricter bound on c(δ, d, n) we gain more and more conditions that the quantity s ′s needs to satisfy:</p><formula xml:id="formula_55">c(δ, d, n) &gt; (n -1)(δ -d -1) n-2 k=i k i =⇒ n-2 k=i k i (s k-i+1 -s ′ k-1+1 ) = 0. (<label>47</label></formula><formula xml:id="formula_56">)</formula><p>Notice that, every time we increase i = 0 . . . n -2, these conditions involve one less term s k-i+1s ′ k-i+1 , k = i . . . n -2: if we were to collect all these conditions within a single linear system, the system would have an upper-triangular structure, and hence be non-singular. This implies that for the set of n independent conditions on ss ′ to hold (we have n -1 in ( <ref type="formula" target="#formula_55">47</ref>), plus one more in ( <ref type="formula">42</ref>)), the only possibility is that s ≡ s ′ . Because of Lemma C.3, though, this also implies l ≡ l ′ : we have finally reached a contradiction, and proven that indeed l → ln is one-to-one, under an opportune condition on c(δ, d, n). Such condition can be promptly recovered<ref type="foot" target="#foot_7">foot_7</ref> by ( <ref type="formula" target="#formula_55">47</ref>):</p><formula xml:id="formula_57">max i=0...n-2 n-2 k=i k i = max i=0...n-2 n -1 i + 1 = n -1 n-1 2 . (<label>48</label></formula><formula xml:id="formula_58">)</formula><p>Substituting this in (47), we recover that it suffices to impose</p><formula xml:id="formula_59">c(δ, d, n) &gt; (n -1)(δ -d -1) n -1 n-1 2 . (<label>49</label></formula><formula xml:id="formula_60">)</formula><p>The next few lemmas are needed to bound the elements in the lj sequence, which in turn are used to prove property (ii) in Def. C.2.</p><p>Lemma C.5. lj in ( <ref type="formula" target="#formula_34">29</ref>) is an increasing sequence.</p><p>Proof. This can be proven directly: we have in fact, by definition ( <ref type="formula" target="#formula_34">29</ref>),</p><formula xml:id="formula_61">lj &gt; lj-1 ⇐⇒ l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 &gt; l j-1 + cs j-1 + j-3 i=0 c i+2 j-3 k=i k i s k-i+1 combine sums ⇐⇒ (l j -l j-1 )(1 -c) + j-2 i=0 c i+2 j -2 i s j-1-i &gt; 0 j-2 i ≥ 1, c i+2 ≥ c 2 ⇐= (l j -l j-1 )(1 -c) + c 2 j-2 i=0 s j-1-i &gt; 0 (24) ⇐⇒ (l j -l j-1 )(1 -c) + c 2 j-2 i=0 n k=1 l k -l j-1-i &gt; 0 ⇐⇒ (l j -l j-1 )(1 -c) + c 2 (j -1) n k=1 l k - j-1 k=1 l k &gt; 0 ⇐⇒ (1 -c)l j + (c -1)l j-1 + c 2 (j -2) n k=1 l k + c 2 n k=j l k &gt; 0 ⇐⇒ (c 2 -c + 1)l j + (c -1)l j-1 + c 2 (j -2) n k=1 l k + c 2 n k=j+1 l k &gt; 0<label>(50)</label></formula><p>Already with c &gt; 1, all the coefficients are positive (and at least one is non-zero), implying that the condition above is always satisfied and that indeed lj is an increasing sequence.</p><p>Lemma C.6. Under constraint (36), each term lj , j &gt; 1 in ( <ref type="formula" target="#formula_34">29</ref>) is bounded from below by lj &gt; c j δ, and each term lj , 1 &lt; j &lt; n is bounded from above by lj &lt; c j+1 δ.</p><p>Proof. We start by proving the lower bound. By definition ( <ref type="formula" target="#formula_34">29</ref>), we have</p><formula xml:id="formula_62">lj = l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 = l j + cs j + c j s 1 + j-3 i=0 c i+2 j-2 k=i k i s k-i+1 . (<label>51</label></formula><formula xml:id="formula_63">)</formula><p>Since by assumption l j is an ordered sequence without repetitions, for j &gt; 1 we necessarily have l j &gt; l 1 ≥ 0, and hence l j ≥ δ. All the other terms in ( <ref type="formula" target="#formula_62">51</ref>) are non-negative, so we can safely claim that lj</p><formula xml:id="formula_64">≥ δ + c j δ &gt; c j δ ∀j &gt; 1,<label>(52)</label></formula><p>which confirms the lower bound.</p><p>For the upper bound, we start again from the definition of lj :</p><formula xml:id="formula_65">lj = l j + cs j + j-2 i=0 c i+2 j-2 k=i k i s k-i+1 &lt; (δ -d -1)δ + c(n -1)(δ -d -1)δ + s 1 j-2 i=0 c i+2 j -1 i + 1 ≤ (n -1)(δ -d -1) j -1 j-1 2 δ j i=0 c i = (n -1)(δ -d -1) j -1 j-1 2 δ 1 -c j+1 1 -c ,<label>(53)</label></formula><p>where we used relationship ( <ref type="formula" target="#formula_57">48</ref>) and collected all c terms within the sum. Notice that, for a given a &gt; 1 we have that</p><formula xml:id="formula_66">1 -c j+1 1 -c ≤ ac j ,<label>(54)</label></formula><p>provided that c ≥ a a-1 . In fact,</p><formula xml:id="formula_67">1 -c j+1 1 -c ≤ ac j ⇐⇒ 1 -c j+1 -ac j + ac j+1 1 -c ≤ 0 ⇐= 1 a -1 + c - a a -1 c j ≥ 0 ⇐= 1 a -1 ≥ 0<label>(55)</label></formula><p>which is always satisfied. After substituting ( <ref type="formula" target="#formula_66">54</ref>) in ( <ref type="formula" target="#formula_65">53</ref>), this allows us to write</p><formula xml:id="formula_68">lj &lt; a(n -1)(δ -d -1) j -1 j-1 2 δc j .<label>(56)</label></formula><p>To prove that lj &lt; δc j+1 , then, it remains to show that</p><formula xml:id="formula_69">c ≥ a(n -1)(δ -d -1) j -1 j-1 2 ∀1 &lt; j &lt; n.<label>(57)</label></formula><p>Substituting condition (36) in the inequality above, we are left with proving</p><formula xml:id="formula_70">n -1 n-1 2 ≥ max j=2...n-1 a j -1 j-1 2 = a n -2 n-2 2 . (<label>58</label></formula><formula xml:id="formula_71">)</formula><p>The outcome depends on the parity of n. For n odd, we have</p><formula xml:id="formula_72">n -1 n-1 2 ≥ a n -2 n-2 2 ⇐⇒ 2 n -1 n -1 ≥ a,<label>(59)</label></formula><p>to satisfy which it suffices to pick a = 2. This requires having c ≥ a a-1 = 2, which is automatically satisfied. For n even, on the other hand, the binomial coefficients simplify to</p><formula xml:id="formula_73">n -1 n-1 2 ≥ a n -2 n-2 2 ⇐⇒ 2 n -1 n ≥ a.<label>(60)</label></formula><p>To satisfy this, we need to pick a = 2 n-1 n , which requires c ≥ a a-1 = 2 n-1 n-2 ; however, this too is automatically satisfied by (36) provided n ≥ 4. This completes the proof.</p><p>Lemma C.7. Under the constraint (36), condition (30) holds.</p><p>Proof. We remind that condition (30) is necessary for the correct "functioning" of the global shift layer, and it composes of two parts. The first part requires that lj &lt; c n δ ∀j &lt; n. Thanks to Lemma C.5, it suffices to show that ln-1 &lt; c n δ, but this is already granted by the upper bound in Lemma C.6. Analogously, for the second part, we need to show that ln &gt; c n δ: for this too we can use the lower bound in Lemma C.6.</p><p>We finally have all the ingredients to prove the main theorem of this section:</p><p>Theorem C.8. The map in (34), given by</p><formula xml:id="formula_74">X → q(X) = u T Ψ(X)</formula><p>represents a contextual mapping.</p><p>Proof. As defined in Def. C.2, a contextual mapping must satisfy two conditions. The first one is that</p><formula xml:id="formula_75">q i (X) ̸ = q j (X), ∀i ̸ = j and ∀X ∈ L.<label>(61)</label></formula><p>This is directly proven by considering Lemma C.5: since lj is a (strictly) increasing sequence, all its elements are already distinct. The action of the last global shift layer merely translates all these elements by a same quantity, but they remain distinct nonetheless.</p><p>The second condition for a contextual mapping is given by</p><formula xml:id="formula_76">q i (X) ̸ = q j (X ′ ), ∀i, j and ∀X, X ′ ∈ L, with X ̸ = X ′ . (<label>62</label></formula><formula xml:id="formula_77">)</formula><p>We prove that this holds for (34) by directly considering the difference between two components i, j for different inputs:</p><formula xml:id="formula_78">q i (X) -q j (X ′ ) = li -l′ j + c n+1 ln -l′ n = 0 ⇐⇒ li -l′ j = c n+1 l′ n -ln . (<label>63</label></formula><formula xml:id="formula_79">)</formula><p>Notice that, due to Lemma C.4, we have ln -l′ n ̸ = 0 and particularly, | ln -l′ n | ≥ δ. On the other hand, in light of the bounds in Lemma C.6, we have that the left-hand side | ljli | &lt; c n δ. Consequently, the two sides can never cancel each other out, and the proof is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Lipschitzness of Sigmoid Attention</head><p>In the following, we report the proof for the recovering the Lipschitzness constant associated with SigmoidAttn, as stated in Thm. 3.2. Letting A = W T q W k , and calling σ ij = σ(⟨W q x i , W k x j ⟩) and σ ′ ij = σ ′ (⟨W q x i , W k x j ⟩), we find that the Jacobian of ϕ in the direction (δ 1 , . . . , δ n ) for the sample x i is given by:</p><formula xml:id="formula_80">Jac i =   n j=1 σ ′ ij x j x T j A T   δ i + n j=1 σ ′ ij x j x T i A + σ ij I p δ j ,<label>(64)</label></formula><p>We see that this Jacobian is the sum of two terms. To control its norm, we can control each norm individually.</p><p>The first term, n j=1 σ ′ ij x j x T j A T δ i is of the form U i δ i with U i a matrix. Its squared-norm is therefore:</p><formula xml:id="formula_81">n i=1 ∥U i δ i ∥ 2 ≤ max i ∥U i ∥ 2 2 ∥δ∥ F .<label>(65)</label></formula><p>Hence, its squared spectral norm is bounded by max i ∥U i ∥ 2 2 . We now let σ ′ ∞ be a bound on n × |σ ′ |; We have:</p><formula xml:id="formula_82">∥U i ∥ 2 ≤ n j=1 ∥σ ′ ij x j x ⊤ j A∥ 2 (66) ≤ σ ′ ∞ ∥A∥ 2 1 n n j=1 ∥x j ∥ 2 (67) ≤ σ ′ ∞ ∥A∥ 2 E[∥x j ∥ 2 ].<label>(68)</label></formula><p>We see that if the points x i have norm ≤ R, then the Jacobian grows at most like R 2 , because it is "quadratic" in x. However, we see that the quadratic term is likely to be mitigated by the σ ′ (a ij ) term that goes to 0 if a ij is large.</p><p>The second term, n j=1 σ ′ ij x j x T i A + σ ij I p δ j , is the sum of two terms. Here, too, we use the triangular inequality to control their norm individually. We get:</p><formula xml:id="formula_83">∥ n j=1 σ ij δ j ∥ 2 = ∥δ T σ i ∥ 2 (69) ≤ ∥δ∥ 2 F ∥σ i ∥ 2 , (<label>70</label></formula><formula xml:id="formula_84">)</formula><p>where σ i ∈ R p is the i-th column of σ ij , and δ ∈ R n×p . and by summing, letting σ ∞ an upper bound on n × |σ(x)|:</p><formula xml:id="formula_85">n i=1 ∥ n j=1 σ ij δ j ∥ 2 ≤ σ 2 ∞ ∥δ∥ 2 F .<label>(71)</label></formula><p>So that σ ∞ upper bounds the spectral norm of the last term.</p><p>For the final term, n j=1 σ ′ ij x j x T i Aδ j , define δ = δA T . We get:</p><formula xml:id="formula_86">n j=1 σ ′ ij x j x T i Aδ j = n j=1 σ ′ ij ⟨x i , δj ⟩x j .<label>(72)</label></formula><p>Hence, letting M the matrix of entries M ij = σ ′ ij ⟨x i , δj ⟩, we see that the previous term is simply x T M T i , so that we get the upper bound on the norm of the term:</p><formula xml:id="formula_87">n i=1 ∥x T M T i ∥ 2 ≤ ∥x∥ 2 F ∥M ∥ 2 F (<label>73</label></formula><formula xml:id="formula_88">)</formula><p>and</p><formula xml:id="formula_89">∥M ∥ 2 F = ij (σ ′ ij ) 2 ⟨x i , δj ⟩ 2 ≤ 1 n 2 σ ′ ∞ ∥x∥ 2 F ∥A∥ 2 2 ∥δ∥ 2 F , giving overall: n i=1 ∥x T M T i ∥ 2 ≤ σ ′ ∞ ∥A∥ 2 E[∥x j ∥ 2 ]∥δ∥ F .<label>(74)</label></formula><p>Notice how this quantity matches the one in (68).</p><p>Finally, summing all together gives:</p><formula xml:id="formula_90">∥Jac∥ 2 ≤ 2σ ′ ∞ ∥A∥ 2 E[∥x j ∥ 2 ] + σ ∞ ,<label>(75)</label></formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark:</head><p>The previous upper bound might not be tight. Indeed, intuitively, if the x i are large, then the term σ ′ ij should be exponentially small (provided, of course, that W q x i and W k x j are not orthogonal), which would even remove the dependency on the variance in the sigmoid attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E The Bias Term of Sigmoid Attention</head><p>One of the differences between SigmoidAttn and SoftmaxAttn is the normalization constant. In SigmoidAttn, one way to emulate the effect of a normalization constant (which links all the elements of the input together and defines a distribution over them), is to include a bias term in the definition as proposed in (3).</p><p>For an input vector z ∈ R n , the output of the sigmoid with bias b is</p><formula xml:id="formula_91">σ b (z) i := exp(z i ) exp(z i ) + exp(-b)</formula><p>Contrary to the softmax, this output cannot always sum to one because there is no normalization. We therefore seek a value for b that approximately normalizes σ b (z), i.e., such that We can also look for a bias term b, which helps to approximate the softmax function by the sigmoid function.</p><p>We assume that softmax provides us with the true distribution p ⋆ , where p ⋆ i = e z i e z i + j̸ =i e z j . The goal is to find the bias term b such that sigmoid function with weights over all elements denoted by p, where p i = σ b (z) i , approximates p ⋆ . Note that, as mentioned before, p is not necessarily a distribution, i.e. n i=1 p i is not always equal to one. In technical terms, we aim to estimate the normalizing factor Z = n i=1 e zi . The existing approaches for estimating Z is compute-expensive for high dimensions and requires resampling methods. Also, the optimal value of b would depend on the exact values of z, which is unknown beforehand. Therefore, we propose a more intuitive way to estimate the order of bias but possibly with larger disparity. To distribute the independent masses in SigmoidAttn, we assume that each element has uniform weight for the model apriori, which means that none of the elements of the input vector z has any known importance over the others. In the simplest case when softmax is a uniform distribution, we ideally want to have the same order of values for sigmoid as of softmax, which should be 1 n . Therefore, we can write down the following:</p><formula xml:id="formula_92">∀i p i = 1 1 + e -(zi+b) ≃ 1 n = p ⋆ i<label>(76)</label></formula><p>Ideally, we would like to have 1 + e -(zi+b) ≃ n. Requiring that p = p * in the case where all the z i are 0 gives exp(-b) = n -1, i.e. b ≃log(n) for large n. In the case that all the z i are bounded,</p><formula xml:id="formula_93">|z i | ≤ M &lt; ∞ for some constant M , then b ≃ -(M + log(n)) ≈ -max{M, log(n)}.</formula><p>However, in most cases we do not know M . When the sequence length n is large enough, the constant M loses its importance while in short sequence length, it impacts distributing the weights over elements more. To resolve this issue, we assume that z i are sampled from a standard Gaussian distribution, i.e. z i ∼ N (0, σ 2 ) where σ = 1. Note that this assumption comes from the fact that z i in our problem is one of the elements of QK T / d qk , which is the sum of d qk random variables. Using Central Limit Theorem, we can assume that z i is sampled from a Gaussian distribution. The idea is to estimate M , such that with high probability, |z i | ≤ M , i.e. P (|z i | &gt; M ) ≤ ϵ for a desired ϵ. Therefore, we have</p><formula xml:id="formula_94">P (|z i | &gt; M ) = P |z i | &gt; M σ σ ≤ 1 ( M σ ) 2 = σ 2 M 2 ≤ ϵ,<label>(77)</label></formula><p>where the inequality is resulted from Chebychev's inequality. Setting σ = 1, we have M ≃ 1/ϵ. Therefore, the order-optimal value would be b ≃ -max{ 1/ϵ, log(n)}, and for long sequence length, b ≃log(n). For example, if we want 90% accuracy in our estimation, M ≈ 3σ = 3, which means b ≃ -max{3, log(n)}. Note that this approximation also follows the intuition that as n grows, we expect the SigmoidAttn without bias term overestimate the mass on each point, so we need to normalize the mass according to n at each point as well.</p><p>On another side, one may be more interested in the gradients of p ⋆ and p with respect to z i to behave similarly. We show that b ≃log(n) is still a good choice in this scenario. Let us derive the derivative of SigmoidAttn and SoftmaxAttn with respect to the input. We note that for any i, both functions can be written as e z i e z i +Z-i where Z -i is the share of normalization factor except element i of z. For SoftmaxAttn, Z -i = j̸ =i e zj and for SigmoidAttn, Z -i = e -b . Now, we have</p><formula xml:id="formula_95">∂ ∂z i e zi e zi + Z -i = e zi Z -i (e zi + Z -i ) 2 . (<label>78</label></formula><formula xml:id="formula_96">)</formula><p>Therefore, we have the following</p><formula xml:id="formula_97">∂p ⋆ i ∂z i = p ⋆ i (1 -p ⋆ i )<label>(79)</label></formula><formula xml:id="formula_98">∂p i ∂z i = p i (1 -p i ).<label>(80)</label></formula><p>We can see that if p i ≃ p ⋆ i , then ∂pi ∂zi ≃ ∂p ⋆ i ∂zi . So, the previous choice of bias term b ≃log(n) approximates the order of gradients as well. In fact, this is the only valid choice even though we have a quadratic term.</p><formula xml:id="formula_99">∂p i ∂z i ≃ ∂p ⋆ i ∂z i ⇐⇒ p ⋆ i (1 -p ⋆ i ) = p i (1 -p i ) (81) ⇐⇒ (p i -p ⋆ i ) (p i -(1 -p ⋆ i )) = 0.<label>(82)</label></formula><p>Which means either</p><formula xml:id="formula_100">p i ≃ p ⋆ i or p i ≃ 1 -p ⋆ i .</formula><p>The first one provides us with b ≃log(n) while the second one cannot happen since the nominator of p i is dependent on z i while the nominator of 1p ⋆ i is independent of z i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Details of FLASHSIGMOID</head><p>This appendix provides details of the FLASHSIGMOID algorithm. We begin by discussing the implementation details of FLASHSIGMOID, which we build as an extension of FLASHATTENTION2, followed by a benchmark of the performance of the involved kernels. We show that the kernels of FLASHSIGMOID provide a considerable performance boost in model inference over those of FLASHATTENTION2 and a modest performance boost for model training. Further, we demonstrate that the kernel speed boosts also reflect in a considerable performance gain in realistic end-to-end experiments, with an example of training vision transformers <ref type="bibr" target="#b18">(Dosovitskiy et al., 2021)</ref> on the ImageNet dataset <ref type="bibr" target="#b17">(Deng et al., 2009)</ref>. Finally, we also provide kernel benchmarking details of FLASHSIGMOID implementation by taking into account ALiBi slopes <ref type="bibr" target="#b48">(Press et al., 2022)</ref>, which is one of the important components of SigmoidAttn as seen in the main text of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Details of FLASHSIGMOID Algorithm</head><p>Softmax vs. Sigmoid Attention: In this subsection, we discuss the implementation details of FLASHSIGMOID algorithm, which is a hardware-aware implementation of SigmoidAttn approach. We begin with the expressions of the forward and backward passes of softmax and sigmoid attention mechanisms. Let Q, K, and V represent the query, key, and value tensors. Then, the desired forward and backward pass expressions are reported in Tab. 4. The application of sigmoid and</p><formula xml:id="formula_101">SOFTMAX SIGMOID FORWARD BACKWARD FORWARD BACKWARD S = Q • K ⊤ √ d dV = P ⊤ • dO S = Q • K ⊤ √ d dV = P ⊤ • dO P = SOFTMAX (S) dP = dO • V ⊤ P = σ (S) dP = dO • V ⊤ O = P • V dS = P ⊙ (dP -ROWSUM (dO ⊙ O)) O = P • V dS = P ⊙ (1 -P ) ⊙ dP dQ = √ d • dS • K dQ = √ d • dS • K dK = √ d • dS ⊤ • Q dK = √ d • dS ⊤ • Q</formula><p>Table <ref type="table">4</ref>: Description of the forward and backward passes of softmax and sigmoid attention. With ⊙, we denote Hadamard (element-wise) multiplication.</p><p>softmax activation functions, as highlighted in orange color in Tab. 4, is the only implementation difference in the forward passes. Similarly, the expressions for the gradients of the preactivation (dS), as highlighted in purple color in the table above, is the only implementation difference in the backward passes. In light of this, we implement the FLASHSIGMOID algorithm as an extension of the FLASHATTENTION2 <ref type="bibr">(Dao, 2023)</ref> algorithm, which is a highly optimized hardware-aware implementation of SoftmaxAttn.</p><p>Flash Attention in Brief: As pointed at in the main text, the FLASHATTENTION <ref type="bibr" target="#b15">(Dao et al., 2022)</ref> and FLASHATTENTION2 <ref type="bibr">(Dao, 2023)</ref> algorithms provide hardware-aware implementations of exact attention mechanism by optimizing for bottlenecks of modern accelerators <ref type="bibr" target="#b10">(Choquette et al., 2021;</ref><ref type="bibr" target="#b9">Choquette, 2023)</ref>. These GPUs possess massive amounts (e.g., ∼ 80GB) of High-Bandwidth Memory (HBM), which stores large tensors but is slow in moving the data to the accelerators. On the other hand, they have smaller amounts (e.g., ∼ 20 MB) of SRAM, which is often more than an order magnitude faster for carrying out actual computations using the registers/tensor cores of the GPU. This trade-off between memory size and computation speed across hierarchies results in the attention mechanism computation being bottlenecked by memory accesses between the HBM and the SRAM <ref type="bibr" target="#b31">(Ivanov et al., 2021)</ref>. Consequently, flash algorithms optimize for memory accesses across the hierarchy of GPU memory types in order to accelerate computation of attention mechanism and its gradients. FLASHSIGMOID is no exception to this approach.</p><p>Algorithm 1 describes the forward pass and Alg. 2 describes the backward pass of the FLASHSIGMOID algorithm. We highlight in orange color the steps in the forward pass of FLASHSIGMOID that differ from those in FLASHATTENTION2 by virtue of sigmoid activation. Similarly, we highlight in purple color the differences in the backward pass. Finally, we highlight in blue color the salient points of FLASHSIGMOID that further help minimize bottlenecking factors on modern accelerators.</p><p>Fewer Tensor Allocations, Fewer Memory Accesses, Fast-Tanh: In FLASHATTENTION and FLASHATTENTION2, the attention mechanism is computed by splitting the attention matrix into blocks. Since softmax activation requires a row-wise reduction to compute its normalization factor (i.e., the denominator), one needs to properly compute and track such factor across blocks. Moreover, in FLASHATTENTION this normalization factor is stored after being computed in the forward pass, to have it easily accessible to further speed-up the backward pass. By contrast, substituting sigmoid to softmax eliminates the need to allocate and move across the GPU memory hierarchy the tensors related to the normalization factor (i.e., moving the logsumexp tensor L ∈ R n on HBM in the forward and backward passes). In addition, applying softmax in a stable manner requires tracking the row-max variable m i on chip, which instead is not needed for sigmoid activation. This further helps reducing some on-chip operations and lowering register pressure in FLASHSIGMOID.</p><p>Moving on to the backward pass (described in Alg. 2), FLASHATTENTION2 requires computing rowsum (dO ⊙ O), which is needed to backpropagate the gradients of softmax attention outputs 31:</p><p>Store query gradient block dQ i from chip back to HBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32:</head><p>On chip, update key gradient block:</p><formula xml:id="formula_102">dK j ← dK j + √ d • dS ⊤ ij • Q i . 33:</formula><p>end for 34:</p><p>Store dK j , dV j from chip to HBM as the j-th blocks of dK, dV matrices respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>35:</head><p>end for 36:</p><p>return matrices dQ, dK, dV . 37: end procedure to the preactivations. However, since sigmoid activation is applied element-wise, its gradients also backpropagate across sigmoid element-wise, eliminating the need of the row-sum variable and the movement of its blocks across the memory hierarchy. Another optimization of FLASHATTENTION and FLASHATTENTION2 consists of partially re-computing the forward pass of attention mechanism in the backward pass to avoid bottlenecks and speed-up the implementation. To keep the backward pass implementation fast, they require the logsumexp variable to be available and transferred between HBM and SRAM in the backward pass. FLASHSIGMOID, being an element-wise activation, eliminates the need of this variable from the backward pass, and consequently, from the entire algorithm. Finally, a major component in our implementation is the usage of GPU-based implementation of the tanh activation. Sigmoid activation is related to Tanh activation via the following relation: σ (x) = 0.5 • (1 + tanh (0.5 • x)). We utilize the fast GPU-implementation of Tanh activation, which trades off some precision for better speed, in order to compute sigmoid activation in both the forward and the backward pass. This provides a considerable speed-boost in both the forward and backward passes of FLASHSIGMOID, while maintaining parity in performance with a naïve implementation of sigmoid attention. Based on these points of modification, we extend FLASHATTENTION2 to obtain FLASHSIGMOID, a hardware-aware implementation of SigmoidAttn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Benchmarking of FLASHSIGMOID Kernels</head><p>Benchmarking Setup: Having seen the details of the FLASHSIGMOID algorithm, we next consider the benchmarking of its kernels. For this, we create a small model in PyTorch <ref type="bibr" target="#b47">(Paszke et al., 2019)</ref> that inputs query, key, and value tensors (all of shape [batch, tokens, heads, features]) and passes these through a number of attention layers. Mimicking the design of vision transformers (ViTB-16/224) <ref type="bibr" target="#b18">(Dosovitskiy et al., 2021)</ref>, we set the number of heads and per-head features as 12 and 64, respectively. We set a batch size of 32, and consider a 10-layer architecture. Then, for the number of tokens sampled from a wide range of <ref type="bibr">[64, 78k]</ref>, we compute the forward and backward passes of this model. For these computations, we measure the kernel GPU time using PyTorch's profiler. We carry out our experiments on both H100 <ref type="bibr" target="#b9">(Choquette, 2023)</ref> and A100 <ref type="bibr" target="#b10">(Choquette et al., 2021)</ref> GPUs.   Results: Figures <ref type="figure" target="#fig_14">10</ref> and <ref type="figure" target="#fig_15">11</ref> show the GPU time comparisons of kernels in inference mode and training mode of FLASHSIGMOID and FLASHATTENTION2 respectively. We observe that we obtain a large average speed-boost for inference and a modest average speed-boost for training. Note that the speed-ups in all the subsequent figures are obtained by averaging the performances for tokens sampled in the range of <ref type="bibr">[64, 78k]</ref>.</p><p>Details of Individual Kernels: Next, we also show the performance of individual flash kernels of FLASHSIGMOID and FLASHATTENTION2. Note that inference mode involves only the forward pas of the model, while training mode involves both the forward and the backward pass of the model. The forward pass of both these approaches involves one kernel, which we term flash_fwd_kernel, and the backward pass of both these approaches is made up of three kernels, which we term bwd_dq_dk_dv, bwd_dot_do_o, and bwd_convert_dq. In code, the real names of these kernels are as follows.</p><p>fwd := flash_fwd_kernel bwd_dq_dk_dv := flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel bwd_dot_do_o := flash_bwd_dot_do_o_kernel bwd_convert_dq := flash_bwd_convert_dq_kernel ( <ref type="formula">83</ref>)</p><p>Here, we first provide a brief description of the tasks performed by each of these kernels; for a detailed explanation, we refer the reader to FLASHATTENTION2 <ref type="bibr">(Dao, 2023)</ref> paper and code. The fwd kernel computes the full forward pass of the model as shown in Tab. 4. The bulk of computations of the backward pass happen in the bwd_dq_dk_dv kernel, which performs re-computation of attention matrix and reduction of key and value gradient tensors (dK, dV ). Again, the exact steps carried out in the backward pass can be checked from Tab. 4. The bwd_convert_dq kernel performs the reduction of query gradient tensor (dQ). Finally, note that the bwd_dot_do_o kernel in FLASHATTENTION2 performs the task of computing the rowsum(dO ⊙ O) tensor along with clearing of the accumulators of query gradients (dQ). Although FLASHSIGMOID does not require this row-sum tensor, the clearing of accumulators of query gradients is still needed. For this reason, bwd_dot_do_o kernel also appears in the profiling of FLASHSIGMOID.</p><p>Performance of Individual Kernels: Figures <ref type="figure" target="#fig_2">12</ref> and <ref type="figure" target="#fig_3">13</ref> show the performance comparison of each flash kernel in FLASHSIGMOID with the corresponding kernel in FLASHATTENTION2 when tested on an H100 GPU and an A100 GPU respectively. We observe that on both the H100 and A100 GPU architectures, the fwd kernel of FLASHSIGMOID is significantly faster than that of FLASHATTENTION2 and the bwd_dq_dk_dv kernel of FLASHSIGMOID has a modest average speed boost over FLASHATTENTION2. The bwd_dot_do_o kernel in FLASHSIGMOID is significantly faster on A100 GPUs. Note that even though the bwd_dot_do_o kernel of FLASHSIGMOID appears to be slower on average on H100 GPUs, the kernel time of bwd_dot_do_o (∼ 5ms) is negligible compared to that of the main bwd_dq_dk_dv kernel (∼ 5000ms). Thus, the combined backward pass kernel in FLASHSIGMOID time does not suffer from this slowdown. Finally, note that for bwd_convert_dq, FLASHSIGMOID and FLASHATTENTION2 have identical performance. This is expected, since the task of this kernel is to reduce the gradient of the queries dQ, which is a common step in both the approaches and is not modified in FLASHSIGMOID. FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) fwd: 17.39% faster for self-attention and 18.76% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1000 2000 3000 4000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dq_dk_dv: 3.29% faster for self-attention and 6.97% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1 2 3 4 5 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dot_do_o: 2.24% slower for self-attention and 2.17% for causal. bwd_convert_dq: 0.03% faster for self-attention, 0.02% slower for causal.</p><p>Figure <ref type="figure" target="#fig_2">12</ref>: FLASHSIGMOID and FLASHATTENTION2 kernel comparison on H100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Speed Boosts of FLASHSIGMOID in Realistic Settings</head><p>In this section, we demonstrate how the performance boosts measured in App. F.2 for the individual kernels of FLASHSIGMOID contributes to speeding-up realistic runs with end-to-end training.</p><p>Setup: As a target experiment, we consider training a vision transformer <ref type="bibr" target="#b18">(Dosovitskiy et al., 2021)</ref> on the ImageNet dataset <ref type="bibr" target="#b17">(Deng et al., 2009)</ref>. We create two vision transformer model variants-one</p><p>0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 500 1000 1500 2000 2500 3000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) fwd: 14.33% faster for self-attention and 16.92% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1000 2000 3000 4000 5000 6000 7000 8000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dq_dk_dv: 3.50% faster for self-attention and 1.39% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 2 4 6 8 10 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal) bwd_dot_do_o: 7.95% faster for self-attention and 8.00% for causal. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 1 2 3 4 5 6 7 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Causal)</p><p>bwd_convert_dq: 0.01% faster for self-attention, 0.03% slower for causal.</p><p>Figure <ref type="figure" target="#fig_3">13</ref>: FLASHSIGMOID and FLASHATTENTION2 kernel comparison on A100 GPUs.</p><p>with FLASHATTENTION2 attention and the other with FLASHSIGMOID attention. We carry out the training of these models with a distributed data-parallel (DDP) setup using PyTorch <ref type="bibr" target="#b47">(Paszke et al., 2019)</ref>. We perform two sets of experiments-i. the first performs DDP training on four nodes of H100 GPUs with eight GPUs per node and EFA/RDMA interconnect for the nodes, and ii. the second performs DDP training on four nodes of A100 GPUs with eight GPUs per node. In each set of experiments, we use three different image sizes (64 × 64, 90 × 90, and 100 × 100), along with patch size of 1 to result in different number of tokens for the underlying attention mechanism in the vision transformer model (64 × 64 = 4096, 90 × 90 = 8100, and 100 × 100 = 10000 tokens). For each of these configurations, we select batch sizes so that the GPU memory utilization would be greater than 80%. These considerations are in order to minimize, if not eliminate, other confounders that can unfairly affect estimation speed-ups in realistic runs. For instance, a low GPU utilization would lead to a larger number of updates, which in turn would incur unnecessary delays, variations, and slow-downs due to across-nodes communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The results of the runs on H100 nodes and A100 nodes are shown in Tab. 5 and 6 respectively. There, we show how the kernel GPU times for forward and backward passes vary according to the number of tokens considered, and include the wall-clock time of the end-to-end runs as explained above. We observe that the kernel speed-up reflects significantly in the speed-up of inference of the models (during testing) and modestly in the training of the models. We observe ∼ 8% speed-up in wall-clock time of inference and ∼ 4% speed-up in wall-clock time of training.</p><p>TOKENS KERNEL GPU TIME COMPARISON FULL RUN WALL-CLOCK TIME COMPARISON KERNELS FLASHATTENTION2 (MS) FLASHSIGMOID (MS) MODE FLASHATTENTION2 (S) FLASHSIGMOID (S) 4096 FWD 4.98±0.01 4.17±0.01 (-16.31%) INFERENCE 11.17±0.18 10.68±0.18 (-4.42%) FWD + BWD 19.58±0.06 18.12±0.04 (-7.45%) TRAINING 1563.39±1.30 1521.68±2.27 (-2.67%) 8100 FWD 20.46±0.05 16.73±0.05 (-18.22%) INFERENCE 28.21±0.18 25.93±0.17 (-8.06%) FWD + BWD 77.63±0.13 72.70±0.12 (-6.35%) TRAINING 4282.75±2.14 4129.25±4.14 (-3.58%) 10000 FWD 31.17±0.07 25.49±0.05 (-18.20%) INFERENCE 38.71±0.19 35.37±0.17 (-8.62%) FWD + BWD 117.53±0.13 109.87±0.12 (-6.52%) TRAINING 5990.72±2.21 5751.43±5.77 (-3.99%) Table 6: FLASHSIGMOID vs. FLASHATTENTION2 on A100 nodes. The kernel GPU time for both the approaches is reported in milliseconds and wall-clock times is reported in seconds per epoch.</p><p>Connection of Wall-Clock Time Speed-Up and Kernel Speed-Up: From Tab. 5 and 6, it is clear that the speed-up in kernels is larger than that in the wall-clock times of the full runs. In fact, the speed-up in kernels is the upper bound for the speed-up that we would see in wall-clock times. To see why, let us denote by τ sm and τ σ the total kernel GPU time for softmax attention and sigmoid attention respectively. Then, the kernel speed-up is given by s kernel := 1 -τσ τsm . However, in a full run, the total wall clock time also incorporates the time required to load data, time taken by other layers of the underlying models, time required to communicate gradients and other data across GPUs and across nodes, and so on. For our corresponding sigmoid and softmax runs, these extra factors are designed to add, upon expectation, in the same extra time τ . Thus, the wall-clock time speed-up of a full run with end-to-end training is s wall-clock := 1 -τσ+τ τsm+τ . Since we have faster sigmoid kernels, we have τ σ &lt; τ sm , which in turn shows that s wall-clock = 1 -τσ+τ τsm+τ &lt; 1 -τσ τsm = s kernel . This explains the speed boost trends in kernel time versus full run wall-clock time for each setting in Tab. 5 and 6. However, in particular, if a model performs attention mechanism over large number of tokens, the attention mechanism, and hence the corresponding kernel time, starts to dominate the other computations in the network. In that case, we see that the wall-clock time speed-boost is closer to the kernel speed-boost. Mathematically, if τ σ , τ sm &gt; &gt; τ , we have: τ σ + τ ≈ τ σ , τ sm + τ ≈ τ sm . Thus, s kernel ≈ s wall-clock , thereby making s wall-clock /s kernel → 1.  Significance of Wall-Clock Speed-Up of Inference: Although FLASHSIGMOID provides only modest gains during training, the speed-up in inference is significant (&gt; 15% for underlying kernels and 5 -10% during inference of full runs). We posit that this speed-up in inference is extremely critical as well. Contemporary large-scale models, once trained, spend a huge portion of the rest their lifetime in inference mode <ref type="bibr">(OpenAI, 2023)</ref>. Thus, significant performance boosts in inference mode have immense potential for saving resources in deployment of large models for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 FLASHSIGMOID with ALiBi</head><p>It is evident from the main text of the paper that improved positional embeddings, like ALiBi <ref type="bibr" target="#b48">(Press et al., 2022)</ref>, can be crucial for certain tasks and data modalities. Thus, we also provide a FLASH-SIGMOID implementation that incorporates ALiBi. We compare the FLASHSIGMOID with ALiBi implementation with the FLASHATTENTION2 with ALiBi implementation <ref type="bibr">(Dao, 2023)</ref>. <ref type="bibr">Figures 14 and 15</ref> show the kernel GPU time for the forward and backward pass kernels of FLASHSIGMOID with ALiBi implementation versus FLASHATTENTION2 with ALiBi implementation. Again, we observe that FLASHSIGMOID kernels for inference have significant speed-up in wall-clock time over those in FLASHATTENTION2 and the kernels for training also have modest wall-clock improvements. Figure <ref type="figure" target="#fig_5">15</ref>: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 12.28% faster than FLASHATTENTION2 for self-attention and 5.30% for causal attention. The training mode kernels of FLASHSIGMOID are 14.64% faster than FLASHATTENTION2 for self-attention and 6.80% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Directions for Future Work on FLASHSIGMOID</head><p>In this section, we discussed FLASHSIGMOID, a hardware-aware implementation of the SigmoidAttn algorithm. Then, we demonstrated via kernel benchmarking and realistic setting runs that FLASH-SIGMOID provides significant gains in inference as well as modest gains in training of models with attention mechanism. In this subsection we further discuss additional avenues for improving the implementation of FLASHSIGMOID, and point out some interesting directions for future work.</p><p>Optimization of Block Shapes for Different Input and GPU Settings: As stated before, our FLASHSIGMOID implementation builds on FLASHATTENTION2 by adding functionality for forward and backward pass of sigmoid attention in place of the standard softmax attention. In particular, for all FLASHSIGMOID results discussed so far, we inherit directly from FLASHATTENTION2 the details of optimal block shapes, grid shapes, and other kernel launch parameters, and keep them unchanged in our implementation. For instance, this is the case for the block sizes B r , B c in Alg. 1 and 2, which are identical in FLASHATTENTION2 and FLASHSIGMOID. This choice is dictated by the need to ensure a fair comparison between the two implementations, and allows us to demonstrate the speed-up of sigmoid attention by minimizing confounders associated with parallel computations on different GPU architectures for different input shapes.</p><p>Although FLASHSIGMOID kernels lead to speed-ups in inference and training for both H100 and A100 GPUs, we observe that the kernel timing speed-ups on A100 are not uniform across sequence lengths: for a small subset of these, our kernel provides significantly lower speed-up compared to the overall trend for other sequence lengths. Ideally, the implementation of attention mechanisms should not assume any information on the token count in input, and it is then desirable to have uniform speed-ups across all input lengths. Here, we show that this is achievable by simply updating the block shape information in FLASHSIGMOID to values that are different than those in FLASHATTENTION2.</p><p>Note that the implementation of FLASHATTENTION2 is templated according to block shapes, grid shapes, and other kernel launch parameters.</p><p>Note that FLASHATTENTION2 provides various tailored implementations, optimized for different input shapes (e.g., different ranges of feature dimension per head), input types (e.g., causal attention vs. self-attention, ALiBi vs. no ALiBi in attention, etc.), and GPU types (e.g., A100 vs. H100 via checking shared memory size on GPUs). This is achieved by opportunely selecting the kernel template parameters defining block shapes, grid shapes, and other kernel launch parameters for parallel computation on GPUs.</p><p>In our case, we create a variant of FLASHSIGMOID, denoted by FLASHSIGMOID † , where we update the block sizes for query and key tensors from (B r , B c ) = (128, 128) of FLASHSIGMOID to (B r , B c ) = (128, 64) of FLASHSIGMOID † only for our input setting (template with features per head being 64). 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 500 1000 1500 2000 2500 3000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Full) (a) Inference mode kernels on A100. 0 10000 20000 30000 40000 50000 60000 70000 Tokens 0 2000 4000 6000 8000 10000 Kernel GPU Time (ms) FlashAttention2 (Full) FlashSigmoid (Full) FlashAttention2 (Causal) FlashSigmoid (Full) (b) Training mode kernels on A100. Figure 16: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID † is 14.82% faster than FLASHATTENTION2 for self-attention and 18.02% for causal attention. The training mode kernels of FLASHSIGMOID † are 6.18% faster than FLASHATTENTION2 for self-attention and 5.76% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model. TOKENS KERNEL GPU TIME COMPARISON KERNELS FLASHATTENTION2 (MS) FLASHSIGMOID (MS) FLASHSIGMOID † (MS) 4096 FWD 8.32±0.02 7.84±0.03 (-5.79%) 7.26±0.02 (-13.21%) FWD + BWD 31.81±0.08 31.11±0.08 (-2.19%) 30.62±0.09 (-4.03%) 8100 FWD 33.65±0.09 27.92±0.07 (-17.04%) 28.54±0.07 (-15.50%) FWD + BWD 128.18±0.13 119.04±0.12 (-7.13%) 119.85±0.13 (-6.81%) 10000 FWD 51.17±0.07 42.49±0.06 (-16.96%) 43.53±0.09 (-15.32%) FWD + BWD 194.54±0.14 180.59±0.15 (-7.17%) 181.97±0.17 (-6.87%) 16384 FWD 134.19±0.12 125.43±0.10 (-6.53%) 116.75±0.10 (-13.40%) FWD + BWD 494.65±0.28 482.08±0.23 (-2.54%) 474.52±0.28 (-4.48%)</p><p>Table <ref type="table">7</ref>: FLASHATTENTION2 vs. FLASHSIGMOID vs. FLASHSIGMOID † on A100 nodes. The kernel GPU time for all three approaches are reported in milliseconds. We observe that FLASHSIGMOID † provides better and more uniform speed-ups across all example tokens.</p><p>Experimentation and Results: For this variant, we perform kernel benchmarking as described in App. F.2, and report the corresponding results in Fig. <ref type="figure" target="#fig_6">16</ref>. Comparing the plots for kernel timing with FLASHSIGMOID plots from Fig. <ref type="figure" target="#fig_15">11</ref>, we observe that FLASHSIGMOID † not only provides a more uniform inference and training kernel speed-up on all sequence lengths, but also improves the average of these speed-ups across all lengths. To further bolster our observations, Tab. 7 shows the inference mode and training mode kernel speed-ups for a subset of sequence lengths under consideration. This experiment indicates that it is possible to obtain higher and more uniform speed-ups in kernel timings across a wide range of tokens by investigating optimal block shape, grid shape, and other kernel launch parameters for each input setting and GPU type. We leave this optimization for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Experiments</head><p>G.1 Extra Ablations G.1.1 The Effect of Multiplicative Sequence Length Normalization <ref type="bibr">Wortsman et al. (2023a)</ref> notes that models trained with sigmoid or ReLU attention require scaling by the sequence length, n -α σ(QK T / d qk )V . We ablate this by comparing the scaled solution to the one we propose in App. E. We also generalize the variant proposed in <ref type="bibr">(Wortsman et al., 2023a)</ref> to variadic sequence lengths such that it works with auto-regressive (AR) training, for example for  Figure <ref type="figure">19</ref>: n -0.5 normalization.</p><formula xml:id="formula_103">n = 3:   1 1 1 0.5 -α 0.5 -α 1 0.33 -α 0.33 -α 0.33 -α   n -α ⊙ 1 0 0 1 1 0 1 1 1 Causal Mask M ⊙ σ(QK T / d qk )V . (<label>84</label></formula><formula xml:id="formula_104">)</formula><p>We repeat the experiment from Fig. <ref type="figure" target="#fig_5">5</ref>, using ALiBi positional embeddings for all trials. We apply α = {1, 0.5} AR normalization proposed in (84). While there is an observable difference in terms of the attention norm, ∥σ(QK T / d qk )V ∥, we find that the train NLL is slightly worse for both normalized variants (Fig. <ref type="figure" target="#fig_22">18</ref> and <ref type="bibr" target="#b70">19)</ref> in comparison to the b =ln n variant in Fig. <ref type="figure" target="#fig_21">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1.2 Attention Bias Stability Ablation</head><p>To validate the stabilizing effects of attention bias we repeat the experiment from Fig. <ref type="figure" target="#fig_8">7</ref> and <ref type="figure">8</ref>, keeping all of the same hyper-parameters, while enabling QK norm and LayerScale (initialized at 10 -4 ). We train with a range of constant bias offsets, b ∈ {-15, -10, -6, -4, -1} and visualize the results below in Fig. <ref type="figure" target="#fig_24">20</ref>. We observe a systematic increase in stability (and lower SigmoidAttn NLL) for which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-10 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-6 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-4 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 3 10 2 10 1 Learning rate attn_bias=-1 which_attn_act_name sigmoid softmax Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 Non-embed Params (M) 2.2 4.9 15.0 67.0 440.0 10 7 10 8 Non-embed Params (M) 10 1 LR sensitivity which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax 10 7 10 8 Non-embed Params (M) which_attn_act_name sigmoid softmax which_attn_act_name sigmoid softmax values less than -1 up till -10, after which the -15 plot shows an over-regularizing effect with decreased performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Vision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.1 Test ImageNet1k Top-1%</head><p>Fig. <ref type="figure" target="#fig_25">21</ref> reports the test linear probe results for the ViT-B/16 BYOL <ref type="bibr" target="#b22">(Grill et al., 2020;</ref><ref type="bibr" target="#b3">Busbridge et al., 2023)</ref>, ViT-B/16 SimCLR <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr">Zhai et al., 2023a)</ref> and the finetuned performance for the ViT-L/16 MAE <ref type="bibr" target="#b26">(He et al., 2022)</ref> and the test top-1% results for for ViT-B/16 supervised model <ref type="bibr" target="#b18">(Dosovitskiy et al., 2021)</ref>. Across these wide range of SSL and supervised learning tasks, trained with contrastive (SimCLR), EMA distillation (BYOL) and reconstructive objectives (MAE), we find that SigmoidAttn not only matches the training dynamics (Fig. <ref type="figure" target="#fig_2">2</ref>), but also the linear probe and finetuned performance of the baseline SoftmaxAttn.</p><p>G.2.2 LayerScale Free Sigmoid Attention While Fig. <ref type="figure" target="#fig_2">22</ref> demonstrates the possibility of learning SigmoidAttn without LayerScale, it involves task specific tuning of {t, b}. We also explored gating attention from learning (through a simple multiply by zero) for ∼25 epochs and were able to match the t = 10, b = -10 training curves from above. However, we opted for the LayerScale method due to its simplicity.  <ref type="bibr" target="#b40">&amp; Hutter (2017)</ref>. In addition, to confirm that applying QK-Norm does not hurt the baseline, we show training parity with and without QK-Norm in Fig. <ref type="figure" target="#fig_4">24</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.2 Gradient Norm</head><p>While a SigmoidAttn based LM using aforementation hyper-parameters has a smooth loss curve, we do see more gradient norm fluctuations. See Fig. <ref type="figure" target="#fig_5">25</ref>, where spikes larger than 0.5 are not visible in the SoftmaxAttn equivalent.</p><p>G.</p><p>4 Automatic Speech Recognition G.4.1 Training Details All acoustic models are fed 80 channel log-mel filterbanks with a 25ms sliding window strided by 10ms. The transformer-based encoder model has 255M parameters: 1D convolution of kernel 7 and stride 3 followed by CAPE positional embedding if it is used and 36 transformer blocks with pre-LayerNorm, an embedding dimension of 768, 4 heads, 3072 units in the MLP layers. The model is trained with CTC loss and a character vocabulary, including apostrophe ('). In additional experiments, we vary the depth to 12 and 24 layers, and change pre-LayerNorm to post-LayerNorm. We implemented our own conformer-based encoder model, also trained with a CTC loss and a character vocabulary. The conformer model has 104M parameters and consists of 1D convolution of kernel 7 and stride 3 followed by 16 conformer blocks with an embedding dimension of 512, 4 heads, 2048 units in the MLP layers. Variational noise is not used and RoPE is used as a relative positional embedding instead of relative sinusoidal positional embedding. For all models, SpecAugment (Park et al., 2019) is used for augmentation with 2 frequency masks (max width 30) and 10 time masks (max width 50, ratio 0.1). All models are trained with dynamic batching and mixed precision with BF16. Models are trained with different configurations of optimizers and hyperparameters to have diverse coverage of use-cases. We first optimize every configuration for SoftmaxAttn and then change only attention to the introduced configuration of SigmoidAttn while all other parameters are kept the same. Detailed configurations are shown in Table 11. We train models until the greedy WER stops improving on the validation sets (dev-clean, dev-other) and report final test sets (test-clean, test-other) greedy WER without integration of any external language model.</p><p>For the bias term b =log n in SigmoidAttn, we do not use max sequence length as in language model experiments. Instead, for every audio sample we use its own duration as a bias terms resulting into non-trainable bias vector for the minibatch. For experiments with sequence normalization, we also use not the max sequence length in the minibatch but rather the ground truth sample duration to properly normalize encoder attention.</p><p>To evaluate behaviour for length generalization we use TED-LIUM v3 dataset <ref type="bibr" target="#b27">Hernandez et al. (2018)</ref> as its validation and test sets have longer audio duration than LibriSpeech: LibriSpeech has in average 10-15s duration, while in TED-LIUM there are audio longer than 30s (the max duration of LibriSpeech). To perform evaluation on TED-LIUM v3, we combine together validation and test sets of TED-LIUM v3 (we don't use them for training and hyper-parameters search and just perform final evaluation) and split them into 4 datasets according to the duration: 0-10s, 10-20s, 20-30s, and 30s+.</p><p>For positional embeddings we use not only CAPE, but change it to AliBi or RoPE. As ALiBi was originally introduced for the decoder only models and there is no official adoption of it yet <ref type="foot" target="#foot_8">14</ref>for the encoder models (without causal masking), we follow the best practices found in <ref type="url" target="https://iclr-blogposts.github.io/2024/blog/alibi-mlm/">https: //iclr-blogposts.github.io/2024/blog/alibi-mlm/</ref> of nonsymmetric ALiBi with different slopes instead of symmetric version used by <ref type="bibr" target="#b37">(Lee et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4.2 Results and Ablations</head><p>Initial investigation on post-LayerNorm and pre-LayerNorm transformers on both LibriSpeech 100h and 960h revealed that SigmoidAttn without any bias is unstable resulting in huge and frequent gradient norm and training loss spikes throughout the training which in turn result in spikes of   Further experiments with bias term in the SigmoidAttn definition for post-LayerNorm transformers on LibriSpeech 100h reveal that training is now stable (only few marginal spikes in gradient norm occur, while train loss is smooth all the time). However, both LayerScale and QK norm restrict model capacity thus not matching SoftmaxAttn. Moreover, some combination of them is needed for the stable training, though w/o both of them we got the best performance for SigmoidAttn (still behind SoftmaxAttn), see Table <ref type="table" target="#tab_22">13</ref>. We believe, further adaptation and deeper investigation is needed for SigmoidAttn and post-LayerNorm, though recent advances in machine learning do not use post-LayerNorm models due to high training instability even for SoftmaxAttn.</p><p>Switching to pre-LayerNorm transformers and varying the depth of the models lead to stable training with SigmoidAttn and bias term b =log n with few (2-5 times) spikes in the gradient norm and smooth loss. In this case, SigmoidAttn matches results for SoftmaxAttn and they both generalize to TED-LIUM data similarly, see</p><p>Table 12. If the bias term is removed, SigmoidAttn can still match SoftmaxAttn but large spikes in gradient norm and loss can occur.  Finally, we experiment with a conformer model, in Table <ref type="table" target="#tab_23">14</ref>. Again, we found that bias term b =log n stabilizes training. The learnable b = -10 works though we see significant gradient norm spikes while the train loss remains smooth. Besides, b =log n generalizes well to longer sequences while learnable b = -10 fails to do so with RoPE for conformer. Overall, SigmoidAttn is able to match SoftmaxAttn having stable training with b =log n.</p><p>In experiments with different variants of bias term for SigmoidAttn, the bias b =log n is found to be the most stable (only few marginal gradient norm spikes are observed with the train loss being smooth) and it provides similar performance as SoftmaxAttn in most settings. The source of instability is coming from the larger attention output norms (80k for CAPE, 40k for RoPE and 20k for AliBi while being 200 for SoftmaxAttn). This happens due to high attention weight of every token which can be biased towards zero with a bias term in SigmoidAttn definition. Preliminary results to connect this to the local attention property needed at the beginning of the training for stable training failed, as local attention did not converge well at all (it is deactivated after some initial training).</p><p>To fully benefit from the improved throughput of FLASHSIGMOID, for the bias term b =log n in SigmoidAttn, we experimented with configuration when the maximum audio duration in the minibatch is used as n resulting into non-trainable bias scalar which changes between minibatches as we use dynamic batching. Comparison between the bias vector with per sample own duration normalization and the bias scalar as maximum duration in the minibatch is shown in Table <ref type="table" target="#tab_25">15</ref>: final model performance is similar and stability is same (only 2-3 minor spikes in CAPE for gradient norms are observed). Thus, per batch maximum audio duration can be used with b =log n as the final configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 Simple Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.1 k-Summation Problem Definition</head><p>Here we look at a synthetic, simple task in order to investigate the behavior of softmax and sigmoid attention activations. The problem chosen is to minimize the MSE loss of a R n → R target function.</p><p>In the first half of each input are samples from a N (0, 1) distribution, and the second half is a k-hot binary vector indicating which values in the first half to sum.</p><p>The results presented here are for the n = 40 problem with various values for k. Where a transformer is used, the transformer is a single layer to aid visualization. In all cases (unless noted otherwise), the optimizer is Adam with a constant learning rate of 0.001, and the training data is continuously generated to preclude over-fitting.</p><p>A few examples for n = 10 (not drawn from N (0, 1)) are shown below. Inputs in the second half of the input are show in orange only as a visual aid.</p><p>1 2 3 4 5 0 0 0 0 1 → 5 1 2 3 4 5 1 0 0 0 1 → 6 8 1 2 0 5 0 1 1 1 0 → 3 2 0 2 2 2 1 1 0 1 0 → 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.2 Comparison to Softmax</head><p>In Figure <ref type="figure" target="#fig_30">30</ref>, we see the performance of three architectures on the k-summation problem as k increases. The sigmoid activated transformer has similar scaling to the softmax activation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.3 Attention Evolution</head><p>In Figures <ref type="figure" target="#fig_3">31</ref> and <ref type="figure" target="#fig_3">32</ref>, forty samples are used to monitor the single head, single layer post-activation attention matrix as training progresses. In Figure <ref type="figure" target="#fig_3">31</ref>, the distribution of values is visualized over time; note the sigmoid attention is more variable but reaches comparable values at convergence. The main difference at convergence is that the sigmoid has fewer high magnitude values than softmax indicating a more distributed attention.</p><p>In Figure <ref type="figure" target="#fig_3">32</ref>, metrics on the post-activation attention matrices are used and show comparable behavior in the first half of training. In the second half of training, the SigmoidAttn can be seen to reduce in norm and in sparsity. (see following discussion of Figure <ref type="figure" target="#fig_31">33</ref> for further insights). rate schedule with 5% linear warmup and a maximum learning rate of 1e-3 is used with the Adam optimizer.</p><p>In this result, we see the sigmoid activation has higher data efficiency and similar fall-off in the out of distribution cases. From shorter runs, we estimate that the softmax network would fit the training with 4-5x more data. Our conjecture is that the two layer transformer more easily learns the pair finding task with sigmoid because softmax is biased to focus on single values, though it is unclear why multiple heads are not able to compensate for this proposed cause in the softmax case. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Train losses comparing SigmoidAttn with SoftmaxAttn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SigmoidAttn with SinCos.</figDesc><graphic coords="7,108.00,267.82,190.08,176.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SigmoidAttn with RoPE.</figDesc><graphic coords="7,311.43,267.82,190.08,176.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SigmoidAttn with ALiBi.Figure 6: SigmoidAttn with RoPE, b = -10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: SigmoidAttn with ALiBi.Figure 6: SigmoidAttn with RoPE, b = -10. LayerScale To validate the need for LayerScale, we follow Wortsman et al. (2023b) to quantify the impact on stability. All models are trained with RoPE with b ∝ln n, using AdamW (Loshchilov &amp; Hutter, 2017) on the realnews split of C4 with (β 1 , β 2 ) = (0.9, 0.95), ϵ = 10 -8 , wd = 0, batch size 24, maximum token sequence length of 512 from the T5 tokenizer (Raffel et al., 2020), cosine LR schedule of 2 14 steps including a linear warmup of 2 10 steps. Models have n heads = κ, n layers = 2 × κ, d model = 64 × κ and d feed-forward = 256 × κ for a scaling value κ ∈ {1, 2, 4, 8, 16} leading to models with {2.2, 4.9, 15.0, 67.0, 440.0}M trainable non-embedding parameters. Following Wortsman et al. (2023b), we sweep learning rates η∈ {3 × 10 -4 , 1 × 10 -3 , 3 × 10 -3 , 1 × 10 -2 , 3 × 10 -2 , 1 × 10 -1 , 3 × 10 -1 }. LR sensitivity is defined as E η∈[a,b] [min(ℓ(A(η)), ℓ 0 )ℓ * ] where ℓ(A(η))is the loss achieved by the learning algorithm A with LR η, ℓ 0 is the loss at initialization, and ℓ * is the loss achieved by the best LR. LayerScale is initialized at 10 -4 . Unlike vision tasks, where LayerScale improves performance (Fig.9-a), in LM, we observe that SoftmaxAttn slightly benefits from LayerScale, while the performance of SigmoidAttn remains largely unaffected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: LR sensitivity LayerScale ablation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Property for Sigmoid Attention C.1 Proof of Step (3): Sigmoid Transformers can Approximate Modified Sigmoid Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Proof of Step (2-b): Modified Sigmoid Transformers can Implement Contextual Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 Basic Building Blocks of Contextual Mapping . . . . . . . . . . . . . . . C.2.2 Result of Applying a Sequence of Selective Shifts . . . . . . . . . . . . . . C.2.3 Result of Applying One Last Global Shift Layer . . . . . . . . . . . . . . C.2.4 A Sequence of Selective Shifts Followed by a Global Shift Produces a Contextual Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D Lipschitzness of Sigmoid Attention E The Bias Term of Sigmoid Attention F Details of FLASHSIGMOID F.1 Details of FLASHSIGMOID Algorithm . . . . . . . . . . . . . . . . . . . . . . . . F.2 Benchmarking of FLASHSIGMOID Kernels . . . . . . . . . . . . . . . . . . . . . F.3 Speed Boosts of FLASHSIGMOID in Realistic Settings . . . . . . . . . . . . . . . F.4 FLASHSIGMOID with ALiBi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Directions for Future Work on FLASHSIGMOID . . . . . . . . . . . . . . . . . . . G Experiments G.1 Extra Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1.1 The Effect of Multiplicative Sequence Length Normalization . . . . . . . . G.1.2 Attention Bias Stability Ablation . . . . . . . . . . . . . . . . . . . . . . . G.2 Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2.1 Test ImageNet1k Top-1% . . . . . . . . . . . . . . . . . . . . . . . . . . G.2.2 LayerScale Free Sigmoid Attention . . . . . . . . . . . . . . . . . . . . . G.2.3 Sigmoid Attention vs. Attention Relaxations . . . . . . . . . . . . . . . . G.2.4 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3.1 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3.2 Gradient Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 Automatic Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.1 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.2 Results and Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5 Simple Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.1 k-Summation Problem Definition . . . . . . . . . . . . . . . . . . . . . . G.5.2 Comparison to Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.3 Attention Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.4 Pair Repeat Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>introduced inYun et al. (2020, App. B.5), but it has been adapted to account for the presence of a sigmoid nonlinearity: notice this required us to use 4-headed attention, while in Yun et al. (2020) a 2-headed version is sufficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>n i=1 σ b (z) i ≃ 1. We have Proposition E.1. Let z ∈ R n , and take m, M ∈ R such that for all i, it holds m ≤ z i ≤ M . Then, the equation n i=1 σ b (z) i = 1 with variable b has a single solution b * with log(n -1) -M ≤ b * ≤log(n -1)m . Proof. The function ϕ : b → n i=1 σ b (z) i is smooth and monotonically increasing, and we have ϕ(log(n -1) -M ) ≤ 1 and ϕ(log(n -1)m) ≥ 1. This shows the existence of b * as well as the advertised bound on b * . This suggests using a b of the order oflog(n); in practice we use b =log(n).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 17.39% faster than FLASHATTENTION2 for self-attention and 18.76% for causal attention. The training mode kernels of FLASHSIGMOID are 6.53% faster than FLASHATTENTION2 for self-attention and 9.46% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 14.33% faster than FLASHATTENTION2 for self-attention and 16.92% for causal attention. The training mode kernels of FLASHSIGMOID are 6.02% faster than FLASHATTENTION2 for self-attention and 5.27% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: On average, for sequence lengths between [64, 78k], the inference mode kernel of FLASHSIGMOID is 17.04% faster than FLASHATTENTION2 for self-attention and 10.87% for causal attention. The training mode kernels of FLASHSIGMOID are 8.91% faster than FLASHATTENTION2 for self-attention and 4.72% for causal attention. Note that inference involves only the forward pass of the model and training involves both the forward and the backward pass of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: b =ln n.</figDesc><graphic coords="42,108.00,77.69,122.76,114.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: n -1 normalization.Figure19: n -0.5 normalization.</figDesc><graphic coords="42,244.62,77.45,122.76,114.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Attention bias ablation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: ImageNet1k test top-1% for SoftmaxAttn vs. SigmoidAttn using models from Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><figDesc>Figure 22: A competitive SigmoidAttn ViT-B/16 model can be learned without LayerScale or QK norm using a large initial learnable scalar temperature t = 10 and bias b = -10 (similar to SigLIP (Zhai et al., 2023b)): σ(e t [QK T / d qk ] + b)V , {b, t} ∈ R. This regularizes the model, as it must move the temperature to a learnable regime. The t = 10, b = -10 curve makes no progress in train NLL or test top-1 for ∼25 epochs (near max LR), but ultimately outperforms baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 24 :Figure 25 :Figure 26 :Figure 27 :</head><label>24252627</label><figDesc>Figure 24: 1B SoftmaxAttn LLM training with and without QK Norm, converging to the same loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: ASR Transformer model (255M) training with post-LayerNorm (left) and pre-LayerNorm (right) on LibriSpeech 960h with SigmoidAttn (w/ bias term, b = 0, w/o QK norm, w/ LayerScale) or with SoftmaxAttn. Huge gradient norms and training loss spikes are observed for SigmoidAttn which can result in worse final model performance hence models for SigmoidAttn are unstable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: ASR Transformer model (255M) training with pre-LayerNorm on LibriSpeech 960h with SigmoidAttn (w/ bias term, b =log n, w/ QK norm, w/ LayerScale) and different positional embeddings CAPE, RoPE, ALiBi. The bias b is able to stabilize SigmoidAttn training: smooth training loss and only marginal rare spikes in gradient norms are observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Final loss is shown after training convergence as k-summation problem complexity increases. The ReLU MLP has two hidden layers (900, 300) for 307k parameters, while the transformer has an embedding dimension of 120, 8 heads, and an MLP ratio of 4, giving 187k parameters. The SigmoidAttn is applied after a learned offset initialized to -4, A+param(-4).</figDesc><graphic coords="50,187.20,364.00,237.60,175.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 33 :</head><label>33</label><figDesc>Figure 33: For 8 samples, the post-activation attentions is visualized as training progresses on the k = 1, n = 40 summation problem. The model has one head to simplify the visualization. The attention is shown in pairs for each sample with softmax attention is in black and sigmoid is in blue. A 2 × 2 block structure is evident in both cases, resulting from each halve of the input containing different information.</figDesc><graphic coords="52,108.00,71.99,395.99,177.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 34 :</head><label>34</label><figDesc>Figure 34: Validation accuracy for out of distribution sequence lengths is shows after 5M samples of training; trained lengths are shown with vertical lines. Quartiles and means are shown from six trials. The MLP has two hidden layers, ReLU activation, and a similar number of parameters. The sigmoid transformer has a learned offset initialized to -4.</figDesc><graphic coords="52,167.40,425.86,277.19,170.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Word error rate (%) on LibriSpeech test sets and TED-LIUM v3<ref type="bibr" target="#b27">(Hernandez et al., 2018)</ref> ("TED", joint validation and test sets split according to duration) for transformer (255M params) with either SoftmaxAttn or SigmoidAttn (LayerScale and QK norm are used with b =log n) trained on LibriSpeech 960h data (mean duration is 10-15s). Hyper-parameters are in App. G.4.</figDesc><table><row><cell>ATTN</cell><cell>PE</cell><cell>TEST-CLEAN</cell><cell>TEST-OTHER</cell><cell>TED 0-10S</cell><cell>TED 10-20S</cell><cell cols="2">TED 20-30S</cell><cell>TED 30S+</cell></row><row><cell>SOFTMAX SIGMOID -QK NORM -LAYERSCALE SIGMOID (b = -10, LEARNABLE) SIGMOID (b = -5 IN Q, LEARNABLE) -QK NORM</cell><cell>CAPE</cell><cell>2.3 2.4 2.5 2.3 2.3</cell><cell cols="4">5.7 5.5 UNSTABLE, GRADIENT NORM AND LOSS SPIKES 12.4 10.5 12.4 10.3 6.1 13.6 11.5 5.5 12.1 10.5 5.4 12.2 10.8 UNSTABLE, GRADIENT NORM AND LOSS SPIKES</cell><cell>11.9 12.3 13.4 13.0 12.4</cell><cell>9.1 9.7 8.9 9.3 9.9</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID (b = -10, LEARNABLE) + α = 1 SIGMOID (b = -5 IN Q, LEARNABLE)</cell><cell>ROPE</cell><cell>2.2 2.3 2.2 2.7</cell><cell cols="4">5.5 5.4 5.2 6.6 UNSTABLE, GRADIENT NORM AND LOSS SPIKES 12.7 10.6 12.3 10.1 12.4 10.5 14.1 12.0</cell><cell>12.8 12.3 12.3 14.5</cell><cell>9.5 8.6 21.8 14.9</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID (b = -10, LEARNABLE) + α = 1 SIGMOID (b = -5 IN Q, LEARNABLE)</cell><cell>ALIBI</cell><cell>2.2 2.3 2.2 2.6 2.2</cell><cell>5.4 5.1 5.2 6.6 5.2</cell><cell>12.3 12.3 12.4 13.9 12.1</cell><cell>10.7 10.5 10.4 11.9 10.4</cell><cell></cell><cell>12.1 12.6 11.7 14.2 12.0</cell><cell>8.6 9.1 9.1 8.6 8.2</cell></row><row><cell cols="4">5.3 Self-Supervised Image Representation Learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Self-supervised representation learning (SSL) exploits vast quantities of unlabeled data to learn semantic representations based on inductive biases such as augmentation invariance (SimCLR Chen et al. (2020), BYOL (Grill et al., 2020)) or reconstruction from compressed representations (MAE (He et al., 2022)). We employ vision transformer training recipes from Zhai et al. (2023a) and Busbridge et al. (2023) (App. G.2.4) for SimCLR and BYOL. As with supervised learning, we use the same set of training hyper-parameters for both SoftmaxAttn and SigmoidAttn, changing only the activation function between trials. Figure 2 reports the train losses, and Fig. 21 highlights the linear probe and finetuned test top-1%. Despite the diverse training objectives in SSL, SigmoidAttn matches SoftmaxAttn while improving training and inference throughput (Sec. 4).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>1B LLM English evaluation.</figDesc><table><row><cell>MODEL</cell><cell>SEQ. LEN.</cell><cell>ARC EASY</cell><cell>ARC CHALLENGE</cell><cell>HELLA-SWAG</cell><cell>PIQA SCIQ</cell><cell>WINO-GRANDE</cell><cell>LAMBADA OPENAI</cell><cell>TRIVIAQA (1-SHOT)</cell><cell>WEBQS (1-SHOT)</cell><cell>AVG</cell><cell>STEP TIME (S)</cell></row><row><cell cols="2">SOFTMAX (ALIBI) 2K SIGMOID (ALIBI) 2K</cell><cell cols="2">62.2 26.8 62.8 28.8</cell><cell>42.4 42.5</cell><cell cols="2">59.0 72.3 88.1 59.7 70.3 88.6</cell><cell>58.4 59.7</cell><cell>19.9 19.1</cell><cell>15.4 13.8</cell><cell cols="2">49.4 0.38 49.5 0.34</cell></row><row><cell cols="2">SOFTMAX (ROPE) 4K SOFTMAX (ALIBI) 4K SIGMOID (ALIBI) 4K</cell><cell cols="2">63.3 29.3 62.6 27.7 60.5 27.3</cell><cell>43.3 42.4 41.3</cell><cell cols="2">58.1 71.3 86.9 58.6 71.1 88.2 57.8 70.5 87.0</cell><cell>58.8 58.6 57.6</cell><cell>20.4 18.9 18.9</cell><cell>15.6 14.7 12.6</cell><cell cols="2">49.7 0.84 49.2 0.84 48.2 0.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>FLASHSIGMOID vs. FLASHATTENTION2 on H100 nodes. The kernel GPU time for both the approaches is reported in milliseconds and wall-clock times is reported in seconds per epoch.</figDesc><table><row><cell>TOKENS</cell><cell></cell><cell cols="2">KERNEL GPU TIME COMPARISON</cell><cell></cell><cell cols="2">FULL RUN WALL-CLOCK TIME COMPARISON</cell></row><row><cell></cell><cell>KERNELS</cell><cell>FLASHATTENTION2 (MS)</cell><cell>FLASHSIGMOID (MS)</cell><cell>MODE</cell><cell>FLASHATTENTION2 (S)</cell><cell>FLASHSIGMOID (S)</cell></row><row><cell>4096</cell><cell>FWD</cell><cell>8.32±0.02</cell><cell>7.84±0.03 (-5.79%)</cell><cell>INFERENCE</cell><cell>19.05±0.22</cell><cell>18.74±0.19 (-1.65%)</cell></row><row><cell></cell><cell>FWD + BWD</cell><cell>31.81±0.08</cell><cell>31.11±0.08 (-2.19%)</cell><cell>TRAINING</cell><cell>2795.03±2.35</cell><cell>2769.44±5.10 (-0.92%)</cell></row><row><cell>8100</cell><cell>FWD</cell><cell>33.65±0.09</cell><cell>27.92±0.07 (-17.04%)</cell><cell>INFERENCE</cell><cell>47.35±0.20</cell><cell>44.05±0.17 (-6.96%)</cell></row><row><cell></cell><cell>FWD + BWD</cell><cell>128.18±0.13</cell><cell>119.04±0.12 (-7.13%)</cell><cell>TRAINING</cell><cell>7519.64±4.21</cell><cell>7254.84±12.64 (-3.52%)</cell></row><row><cell>10000</cell><cell>FWD</cell><cell>51.17±0.07</cell><cell>42.49±0.06 (-16.96%)</cell><cell>INFERENCE</cell><cell>64.61±0.32</cell><cell>59.55±0.18 (-7.82%)</cell></row><row><cell></cell><cell>FWD + BWD</cell><cell>194.54±0.14</cell><cell>180.59±0.15 (-7.17%)</cell><cell>TRAINING</cell><cell>10455.64±8.85</cell><cell>10052.04±18.87 (-3.86%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>SigmoidAttn SimCLR and BYOL ViT-B/16 hyperparameters.</figDesc><table><row><cell cols="9">G.2.3 Sigmoid Attention vs. Attention Relaxations</cell></row><row><cell></cell><cell>80.0</cell><cell></cell><cell></cell><cell cols="3">Supervised ViT-B/16</cell><cell></cell><cell>Figure 23: Supervised ViT-B/16 ImageNet1k classification. We contrast SigmoidAttn and SoftmaxAttn against (a) linear attention with</cell></row><row><cell>Test Top-1%</cell><cell>0.0 20.0 40.0 60.0</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150 Epoch</cell><cell>200</cell><cell>250 Softmax 300 Sigmoid Performer Linear</cell><cell>no activation: QK T / d qk and (b) fast atten-tion via positive orthogonal random features, used in Performer (Choromanski et al., 2021). SigmoidAttn, like SoftmaxAttn, differs from attention relaxations like Performer which uses low-rank representations of the attention ma-trix. SigmoidAttn maintains performance parity with SoftmaxAttn, while outperforming other efficient attention variants.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>SigmoidAttn Supervised ViT-B/16 and MAE ViT-L/16 hyperparameters.</figDesc><table><row><cell>Parameter</cell><cell>Supervised</cell><cell>MAE</cell></row><row><cell>Attention bias LayerScale Init QK Norm Pos Embed</cell><cell>None 10 -4 Yes Learnable</cell><cell>b = -ln n 10 -4 Yes Learnable</cell></row><row><cell>Architecture Mask Ratio Freeze Patcher Weight init Normalization LR schedule LR warmup Min LR Training duration Optimizer Optimizer scaling rule Base Adam (β1, β2) Base LR Base batch size Total batch size Weight decay Weight decay skip bias Numerical precision Stochastic depth Augmentation stack</cell><cell>ViT-B/16 -No trunc_normal(.02) LayerNorm Single Cycle Cosine 20 Epochs 1 × 10 -6 300 Epochs AdamW Linear (0.9, 0.95) 1 × 10 -4 256 4096 0.3 Yes bf16 0.28 RandAug (Cubuk et al., 2020)</cell><cell>ViT-L/16 0.75 No trunc_normal(.02) LayerNorm Single Cycle Cosine 40 Epochs 0.0 400 Epochs AdamW Linear (0.9, 0.95) 1.5 × 10 -4 256 4096 0.05 Yes bf16 0.0 RRC + HFLIP</cell></row><row><cell>G.3 Language Model</cell><cell></cell><cell></cell></row><row><cell>G.3.1 Hyper-Parameters</cell><cell></cell><cell></cell></row></table><note><p>Tab. 10 shows the hyper-parameters for the final comparison. MuP-simple(Wortsman et al., 2023b)   </p><p>is used, where the peak learning rate is set to 1e-2. Weight decay is decoupled, following Loshchilov</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Training details for the Llama-style 1B LM training.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell cols="2">Params Context Length Total Tokens Batch size LR Schedule LR Warmup Steps Peak LR Final LR Optimizer Optimizer momentum 0.9, 0.95 1B 2048 300B 4M tokens Cosine 5000 1e-2 10% of peak AdamW Weight decay 1e-4 Gradient clipping 1.0 Position encoding ALiBi Q/K Norm Applied Num layers 24 Num heads 32 Hidden dim 2048</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Training details for the ASR models on LibriSpeech 100h (LS-100) and LibriSpeech 960h (LS-960) for transformers and conformers.</figDesc><table><row><cell>Parameter</cell><cell>Transformer LS-960</cell><cell cols="2">Conformer LS-960 Transformer LS-100 Transformer LS-100</cell></row><row><cell cols="3">Params LayerNorm Dropout Layer drop Training steps Batch size LR schedule SpecAugment start LR Warmup Steps Peak LR LR start decay LR decay step Optimizer Optimizer momentum Weight decay Gradient clipping Position encoding Q/K Norm SoftmaxAttn Not Applied 255M pre 0.1 0.1 400k 3.56h step-wise 0k 64k 1e-3 250k 50k AdamW 0.9, 0.999 1e-6 1.0 CAPE / ALiBi / RoPE RoPE 104M pre + post 0.1 0.0 400k 4.44h step-wise 10k 10k 2e-3 250k 50k AdamW 0.9, 0.98 1e-6 0.5 Not Applied Q/K Norm SigmoidAttn Applied Applied Num layers 36 16 Num heads 4 4</cell><cell>255M / 170M / 85M 255M pre post 0.3 0.3 0.3 0.3 400k 500k 1.1h 1.1h step-wise step-wise 0k 0k 64k 64k 0.1 0.03 200k 330k 30k 50k Adagrad Adagrad --0 0 1.0 1.0 CAPE CAPE / ALiBi / RoPE Not Applied Not Applied Not Applied Applied 36 / 24 / 12 36 4 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12 :</head><label>12</label><figDesc>Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3(Hernandez et al.</figDesc><table><row><cell>ATTN</cell><cell># LAYERS</cell><cell>DEV-CLEAN</cell><cell>TEST-CLEAN</cell><cell>DEV-OTHER</cell><cell>TEST-OTHER</cell><cell>TED 0-10S</cell><cell>TED 10-20S</cell><cell>TED 20-30S</cell><cell>TED 30S+</cell></row><row><cell>SOFTMAX SIGMOID b = 0</cell><cell>36 36 36</cell><cell>6.7 7.0 6.8</cell><cell>7.1 7.3 7.1</cell><cell>20.0 20.3 19.8</cell><cell>20.4 20.5 20.3</cell><cell>26.4 26.2</cell><cell>22.4 23.4</cell><cell>23.3 23.6</cell><cell>21.8 21.8</cell></row><row><cell>SOFTMAX SIGMOID b = 0</cell><cell>24 24 24</cell><cell>6.4 7.1 6.7</cell><cell>6.8 7.3 6.9</cell><cell>20.2 21.0 20.2</cell><cell>20.5 21.3 20.7</cell><cell>25.4 26.6</cell><cell>22.1 23.3</cell><cell>23.3 24.0</cell><cell>21.8 22.0</cell></row><row><cell>SOFTMAX SIGMOID b = 0</cell><cell>12 12 12</cell><cell>8.2 8.3 8.7</cell><cell>8.7 8.7 8.5</cell><cell>25.0 24.8 24.4</cell><cell>25.4 25.2 24.7</cell><cell>29.0 29.0</cell><cell>25.6 25.7</cell><cell>27.1 26.3</cell><cell>27.4 25.5</cell></row></table><note><p>, 2018) ("TED", joint validation and test sets with split according to audio duration) for pre-LayerNorm transformer (255M / 170M / 85M params) with CAPE and with either SoftmaxAttn or SigmoidAttn (w/ LayerScale, w/o QK norm, w/ b =log n) trained on LibriSpeech 100h data (average duration is 10-15s). Hyper-parameters can be found in Table11</p><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13 :</head><label>13</label><figDesc>Word error rate (%) on LibriSpeech dev/test sets for post-LayerNorm transformer (255M) with either SoftmaxAttn (w/o QK norm) or SigmoidAttn (by default w/ LayerScale, w/ QK norm, w/ b =log n) trained on LibriSpeech 100h data. Hyper-parameters can be found in Table11.</figDesc><table><row><cell>ATTN</cell><cell>PE</cell><cell>DEV-CLEAN</cell><cell>TEST-CLEAN</cell><cell>DEV-OTHER</cell><cell cols="2">TEST-OTHER</cell></row><row><cell>SOFTMAX + QK NORM SIGMOID -QK NORM -LAYERSCALE -QK NORM -LAYERSCALE SIGMOID (b = -10, LEARNABLE)</cell><cell>CAPE</cell><cell cols="4">6.4 6.1 8.0 7.5 UNSTABLE, GRADIENT NORM AND LOSS SPIKES 6.5 18.4 6.3 18.2 8.4 22.7 7.9 22.1 6.5 6.9 19.9 8.7 9.4 23.5</cell><cell>18.2 18.1 22.7 27.6 20.1 24.0</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID (b = -10, LEARNABLE)</cell><cell>ROPE</cell><cell>6.6 6.8 8.7</cell><cell>6.9 7.1 9.4</cell><cell>18.3 20.8 23.5</cell><cell></cell><cell>18.5 20.8 24.0</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID (b = -10, LEARNABLE)</cell><cell>ALIBI</cell><cell>6.4 6.9 6.8</cell><cell>6.9 7.2 7.1</cell><cell>18.3 20.8 20.4</cell><cell></cell><cell>18.3 21.1 20.5</cell></row></table><note><p>validation and test WER, see Figure28</p><p>. Neither LayerScale nor QK norm were able to stabilize the training, though we did not observe any model divergence.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 14 :</head><label>14</label><figDesc>Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3<ref type="bibr" target="#b27">(Hernandez et al., 2018)</ref> ("TED", joint validation and test sets with split according to audio duration) for conformer (104M) with RoPE and with either SoftmaxAttn or SigmoidAttn (w/ LayerScale, w/ QK norm, w/ b =log n) trained on LibriSpeech 960h data (average duration is 10-15s). Hyper-parameters can be found in Table11.</figDesc><table><row><cell>ATTN</cell><cell>DEV-CLEAN</cell><cell>TEST-CLEAN</cell><cell>DEV-OTHER</cell><cell>TEST-OTHER</cell><cell>TED 0-10S</cell><cell>TED 10-20S</cell><cell>TED 20-30S</cell><cell>TED 30S+</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID (b = -10, LEARNABLE)</cell><cell>2.2 2.3 2.4</cell><cell>2.5 2.5 2.7</cell><cell>5.4 5.6 5.8</cell><cell>5.6 5.8 5.8</cell><cell>13.0 13.5 12.9</cell><cell>11.1 10.8 11.1</cell><cell>13.2 13.3 14.1</cell><cell>7.1 10.2 54.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3<ref type="bibr" target="#b27">(Hernandez et al., 2018)</ref> ("TED", joint validation and test sets split according to duration) for transformer (255M params) with either SoftmaxAttn or SigmoidAttn (LayerScale and QK norm are used with b =log n) trained on LibriSpeech 960h data (mean duration is 10-15s). Hyper-parameters are in App. G.4.</figDesc><table><row><cell>ATTN</cell><cell>PE</cell><cell>DEV-CLEAN</cell><cell>TEST-CLEAN</cell><cell>DEV-OTHER</cell><cell>TEST-OTHER</cell><cell>TED 0-10S</cell><cell>TED 10-20S</cell><cell>TED 20-30S</cell><cell>TED 30S+</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID, b = -log(maxbatch n)</cell><cell>CAPE</cell><cell>2.2 2.2 2.1</cell><cell>2.3 2.4 2.3</cell><cell>5.6 5.2 5.2</cell><cell>5.7 5.5 5.3</cell><cell>12.4 12.4 12.2</cell><cell>10.5 10.3 10.6</cell><cell>11.9 12.3 12.0</cell><cell>9.1 9.7 9.3</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID, b = -log(maxbatch n)</cell><cell>ROPE</cell><cell>2.2 2.0 2.1</cell><cell>2.2 2.3 2.3</cell><cell>5.4 5.2 5.0</cell><cell>5.5 5.4 5.1</cell><cell>12.7 12.3 12.3</cell><cell>10.6 10.1 10.1</cell><cell>12.8 12.3 12.1</cell><cell>9.5 8.6 10.4</cell></row><row><cell>SOFTMAX SIGMOID SIGMOID, b = -log(maxbatch n)</cell><cell>ALIBI</cell><cell>2.1 2.1 2.0</cell><cell>2.2 2.3 2.3</cell><cell>5.3 5.0 5.2</cell><cell>5.4 5.1 5.2</cell><cell>12.3 12.3 12.3</cell><cell>10.7 10.5 10.5</cell><cell>12.1 12.6 11.9</cell><cell>8.6 9.1 10.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Appendix G.2.2 demonstrates that supervised vision tasks using SigmoidAttn without LayerScale can match baseline SoftmaxAttn performance by relying on learnable scalar bias and temperature: {b, t} ∈ R.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We ablate multiplicative sequence length scaling in more detail in App. G.1.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://github.com/apple/axlearn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>For example, consider d = 3 and the column defined as xi = [3δ, 10δ, 2δ] T , that is, the column identified by the triplet of indices[3, 10, 2]. Multiplying by u would then give the scalar u T xi = (3 + 10N + 2N 2 )δ, where N = δ -1 , which is uniquely identified by the single index (3 + 10N + 2N 2 ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>This can be better seen by considering independently the effects of the two parameters b k , bq on the modified sigmoid attention matrix H ((l -1bq) ⊗ (l -1b k )). We have in fact, with bq = 0,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5"><p>Indeed its inverse can be explicitly recovered by directly applying Sherman-Morrison formula.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>This is a direct consequence of the definition of operator S in (35): since it has 1's everywhere but on its diagonal, its ∞-norm is simply n -1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>This is a consequence of some useful properties of the binomial coefficient, namely the Hockey stick identity<ref type="bibr" target="#b32">Jones (1994)</ref>, and the symmetry of k i with respect to i.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8"><p>See discussion in https://github.com/ofirpress/attention_with_linear_ biases/issues/5.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>We thank <rs type="person">Zakaria Aldeneh</rs>, <rs type="person">Samy Bengio</rs>, <rs type="person">Navdeep Jaitly</rs>, <rs type="person">David Koski</rs>, <rs type="person">Pau Rodriguez Lopez</rs>, <rs type="person">Hadi Pouransari</rs>, and <rs type="person">Skyler Seto</rs> for their helpful feedback and critical discussions throughout the process of writing this paper; <rs type="person">Okan Akalin</rs>, <rs type="person">Hassan Babaie</rs>, <rs type="person">Michael Brooks</rs>, <rs type="person">Brian Gamp</rs>, <rs type="person">Denise Hui</rs>, <rs type="person">Mubarak Seyed Ibrahim</rs>, <rs type="person">Li Li</rs>, <rs type="person">Rajat Phull</rs>, <rs type="person">Evan Samanas</rs>, <rs type="person">Guillaume Seguin</rs>, and the wider Apple infrastructure team for assistance with developing and running scalable, fault tolerant code. Names are in alphabetical order by last name within group.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Figure <ref type="figure">33</ref>, we see post-activation attention values for eight samples at training progresses. The most notable difference between the activations is, that by the end of training, the SigmoidAttn is less sparse in the N (0, 1) self-attention in the upper-left quadrant. We can see that softmax tends to produce sparser values (as it is designed to) while sigmoid controls the magnitude and location of peak attention independently, leading to a less sparse attention at the end of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.4 Pair Repeat Problem</head><p>We define a synthetic task of identifying if the first two symbols in a sequence repeat. The symbols, s i below come from a fixed vocabulary of size K, and the repeat location (when present) is uniformly distributed in the sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Contributions</head><p>All authors contributed to writing this paper, designing the experiments, discussing results at each stage of the project.</p><p>Preliminary work Preliminary viability of SigmoidAttn done by Jason Ramapuram.</p><p>Universal Function Approximation Proof of UFA (Section 3.1 and App. C) sculpted by Federico Danieli.</p><p>Lipschitzness of Sigmoid Attention Lipschitzness analysis (Section 3.2 and App. D) molded by Pierre Ablin.</p><p>FlashSigmoid Implementation and analysis driven by Eeshan Dhekane in collaboration with Jagrit Digani (Section 4 and App. F).</p><p>Bias Analysis Theoretical grounding for bias (Appendix E) done by Amitis Shidani in discussion with Pierre Ablin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modeling Results</head><p>All large scale language model pretraining and evaluation (Section 5.5 and App. G.3) driven by Floris Weers.</p><p>Stability Analysis QK norm (Figure <ref type="figure">8</ref>), LayerScale (Figure <ref type="figure">7</ref>) and bias (Figure <ref type="figure">20</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transformers as statisticians: Provable in-context learning with in-context algorithm selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/b" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>2e63e36c57e153b9015fece2352a9f9-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How to scale your EMA</title>
		<author>
			<persName><forename type="first">D</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Dhekane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>e7681dd6fe16052433ab68cd1555bdc9-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">October 10-17, 2021</date>
			<biblScope unit="page" from="9630" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00951</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00951" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding the regularity of self-attention with optimal transport</title>
		<author>
			<persName><forename type="first">V</forename><surname>Castin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14820</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00950</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00950" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">October 10-17, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NVIDIA hopper H100 GPU: scaling performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choquette</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2023.3256796</idno>
		<ptr target="https://doi.org/10.1109/MM.2023.3256796" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="9" to="17" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NVIDIA A100 tensor core GPU: performance and innovation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krashinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2021.3061394</idno>
		<ptr target="https://doi.org/10.1109/MM.2021.3061394" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ua6zuk0WRH" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Redpajama: An open source recipe to reproduce llama training dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Computer</surname></persName>
		</author>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.08691</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.08691" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/67" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note>d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to 22 billion parameters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pavetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v202/dehghani23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="7480" to="7512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-06">2009. 2009. June 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What can a single attention layer learn? A study through the random features lens</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>274db6bf1b01d8b4f07feaeb8c46f474-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck via learnable monotonic pointwise non-linearities</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/ganea19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2073" to="2082" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143891</idno>
		<ptr target="https://doi.org/10.1145/1143844.1143891" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</editor>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">June 25-29, 2006. 2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Á</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/f" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>3ada80d5c4ee70142b17b8192b2958e-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-3015</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2020-3015" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</editor>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10-29">25-29 October 2020. 2020</date>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">MLX: Efficient and flexible machine learning on apple silicon</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Digani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<ptr target="https://github.com/ml-explore" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01553</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.01553" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">June 18-24, 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2022</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer: 20th International Conference</title>
		<meeting><address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 18-22, 2018. 2018</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
	<note>SPECOM 2018</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Infinite attention: NNGP and NTK for deep attention networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/hron20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4376" to="4386" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformer quality in linear time</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/hua22a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9099" to="9117" />
		</imprint>
	</monogr>
	<note>ICML PMLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Comparing measures of sparsity</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Rickard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data movement is all you need: A case study on optimizing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper/2021/hash/c" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems 2021, MLSys 2021, virtual</meeting>
		<imprint>
			<date type="published" when="2021">April 5-9, 2021. mlsys.org, 2021</date>
		</imprint>
	</monogr>
	<note>9e1074f5b3f9fc8ea15d152add07294-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generalized hockey stick identities and n-dimensional blockwalking</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:2088017" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080246</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA 2017</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA 2017<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">June 24-28, 2017. 2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/katharopoulos20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The lipschitz constant of self-attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5562" to="5571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple softmax-free attention for vision transformers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Koohpayegani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><surname>Sima</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV57701.2024.00259</idno>
		<ptr target="https://doi.org/10.1109/WACV57701.2024.00259" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2024</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">January 3-8, 2024</date>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient faster &amp; longer transformer for question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Littlebird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5261" to="5277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CAPE: encoding relative positions with continuous augmented positional embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogozhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Vaughan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/865" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="16079" to="16092" />
		</imprint>
	</monogr>
	<note>bf46435bd84fa5d89f64cf3ba7347-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Swin transformer V2: scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01170</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.01170" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">June 18-24, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SOFT: softmaxfree transformer with linear complexity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/b1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="21297" to="21309" />
		</imprint>
	</monogr>
	<note>d10e7bafa4421218a51b1e1f1b0ba2-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2018 -27th International Conference on Artificial Neural Networks</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Kurková</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Iliadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Maglogiannis</surname></persName>
		</editor>
		<meeting><address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">October 4-7, 2018</date>
			<biblScope unit="volume">11139</biblScope>
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01418-6_38</idno>
		<idno>978-3-030-01418-6_38</idno>
		<ptr target="https://doi.org/10.1007/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">GPT-4 technical report</title>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<imprint>
			<date type="published" when="2023">CoRR, abs/2303.08774, 2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><surname>Librispeech</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2015.7178964</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2015.7178964" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>South Brisbane, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04-19">2015. April 19-24, 2015. 2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2680</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2019-2680" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2019, 20th Annual Conference of the International Speech Communication Association</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Kubin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kacic</surname></persName>
		</editor>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-19">15-19 September 2019. 2019</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/bdbca" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>288fee7f92f2bfa9f7012727740-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=R8sQPpGCv0" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="140" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02150</idno>
		<ptr target="http://arxiv.org/abs/1911.02150" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A study on relu and softmax in transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.06461</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.06461" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2023.127063</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2023.127063" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end ASR: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OSVxDDc360z" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Self-supervision in Audio and Speech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>CoRR, abs/2110.00476</idno>
		<ptr target="https://arxiv.org/abs/2110.00476" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Replacing softmax with ReLU in vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.08586</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.14322</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.14322" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkwZSG-CZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Are transformers universal approximators of sequence-to-sequence functions?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stabilizing transformer training by preventing attention entropy collapse</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v202/zhai23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="40770" to="40803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sigmoid loss for language image pre-training</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.15343</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.15343" />
	</analytic>
	<monogr>
		<title level="m">Algorithm 1 FLASHSIGMOID Forward Pass 1: procedure FORWARD</title>
		<imprint>
			<date type="published" when="2023">CoRR, abs/2303.15343. 2023</date>
		</imprint>
	</monogr>
	<note>3: inputs: Matrices Q, K, V ∈ R n×d are on HBM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Integers B r and B c are the block size for queries and key-values respectively</title>
	</analytic>
	<monogr>
		<title level="m">outputs: Matrix O ∈ R n×d on HBM of the GPU</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m"># No need to output logsumexp vector L ∈ R n on HBM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m">9: Divide Q into T r := ⌈ n Br ⌉ blocks: Q 1 , • • • , Q Tr with Q i ∈ R Br×d</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Divide K into T c := ⌈ n Bc ⌉ blocks: K 1</title>
		<author>
			<persName><forename type="first">•</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tc With K I ∈ R Bc×d</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Divide V Into T C Blocks ; • • •</surname></persName>
		</author>
		<author>
			<persName><surname>Tc With V I ∈ R Bc×d</surname></persName>
		</author>
		<title level="m">Tr with O i ∈ R Br×d . 13: for i = 1, • •</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Load block Q i from HBM to SRAM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m">On chip, initialize O i with zeros: O i ← 0 Br×d</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main"># No allocation of either row-sum ℓ i ∈ R Br or row-max m i ∈ R Br on chip. 17: for j = 1 •</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>Load blocks K j , V j from HBM to SRAM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">On chip, evaluate pre-activations: S ij</title>
		<editor>← Q i • K ⊤ j / √ d ∈ R Br×Bc</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">On chip, evaluate sigmoid attention: P ij ← σ</title>
		<imprint/>
	</monogr>
	<note>S ij</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">On chip, update output block: O i ← O i + P ij</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">L i blocks on chip</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">27: end for 28: return matrix O. 29: end procedure Algorithm 2 FLASHSIGMOID Backward Pass 1: procedure BACKWARD</title>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">No movement of L i block from chip to HBM</title>
		<imprint/>
	</monogr>
	<note>3: inputs: Matrices Q, K, V , dO ∈ R n×d are on HBM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Integers B r and B c are the block size for queries and key-values respectively</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note># No need of logsumexp vector L ∈ R n to be saved for the backward pass 6: 7: outputs: Matrices dQ, dK, dV ∈ R n×d on HBM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m">9: Divide Q into T r := ⌈ n Br ⌉ blocks: Q 1 , • • • , Q Tr with Q i ∈ R Br×d</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Divide K into T c := ⌈ n Bc ⌉ blocks: K 1</title>
		<author>
			<persName><forename type="first">•</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tc With K I ∈ R Bc×d</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Divide V Into T C Blocks ; • • •</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; • • •</forename><surname>Tc With V I ∈ R Bc×d</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tr With V I ∈ R Br×d</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; ∈ R</forename><surname>Br×d</surname></persName>
		</author>
		<title level="m">Divide dO into T r := ⌈ n Br ⌉ blocks: dO 1 , • •</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Allocate dQ on HBM and divide into T r blocks: dQ 1 , • • • , dQ Tr with dQ i ∈ R Br×d . 15: Allocate dK on HBM and divide into T c blocks: dK 1 , • • • , dK Tc with dK i ∈ R Bc×d</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>Allocate dV on HBM and divide into T c blocks: dV 1 , • • • , dV Tc with dV i ∈ R Bc×d . 17: # No need to compute rowsum (dO ⊙ O) as sigmoid and its gradients are pointwise Load blocks K j , V j from HBM to SRAM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m">On chip, initialize dK j , dV j with zeros: dK j ← 0 Bc×d ; dV j ← 0 Bc×d . 21: for i = 1 • • • T r do 22: Load blocks Q i</title>
		<imprint/>
	</monogr>
	<note>dO i , dQ i from HBM to SRAM of the GPU</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">On chip, evaluate pre-activations: S ij</title>
	</analytic>
	<monogr>
		<title level="s">No need of movement of blocks rowsum (dO ⊙ O) i and logsumexp L i</title>
		<editor>
			<persName><surname>← Q I • K ⊤ J / √ D ∈ R Br×bc</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">On chip, evaluate sigmoid attention: P ij ← σ</title>
		<imprint/>
	</monogr>
	<note>S ij</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">On chip, update gradient of values: dV i ← dV i + P ⊤ ij</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">On chip, compute gradients of attention matrix: dP ij ← dO i • V ⊤ i ∈ R Br×Bc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">On chip, compute gradients of pre-activations: dS ij ← P ij ⊙ (1 -P ij ) ⊙ dP ij</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Load query gradient block dQ i from HBM to SRAM, and then on to chip. 30: Update query gradient block on chip: dQ i ← dQ i + √ d • dS ij</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
