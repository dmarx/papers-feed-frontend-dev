- Decision to use a camera-only approach for closed-loop driving
- Choice of LLaVA vision encoder as the backbone for the model
- Adoption of a semi-disentangled output representation for path and waypoint predictions
- Implementation of an efficient training recipe to optimize data usage
- Selection of high-resolution input images for improved feature extraction
- Use of multi-view input configuration to enhance model performance
- Decision to utilize LLaMA architecture for the language model decoder
- Strategy for dataset collection focusing on diverse driving scenarios
- Approach to handle distribution shifts in target point predictions
- Implementation of data buckets to prioritize interesting training samples
- Choice of loss functions for training (e.g., MSE loss for waypoints)
- Decision to generate language commentary alongside driving outputs
- Use of temporal and spatial encodings for input representation
- Strategy for downsampling feature maps to reduce computational overhead
- Decision to leverage pre-trained models for fine-tuning on driving tasks
- Approach to augment training data with random weather conditions
- Decision to use learnable queries for generating path and waypoints
- Implementation of early stopping criteria based on performance metrics
- Choice of evaluation metrics for assessing model performance on the CARLA Leaderboard
- Strategy for managing complex driving scenarios in the simulation environment