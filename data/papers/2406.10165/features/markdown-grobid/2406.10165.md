# CarLLaVA: Vision language models for camera-only closed-loop driving

## Abstract

## 

In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. Car-LLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as backbone, achieving state-ofthe-art closed-loop driving performance with only camera input and without the need for complex or expensive labels. Additionally, we show preliminary results on predicting language commentary alongside the driving output. CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control. We propose an efficient training recipe to train on large driving datasets without wasting compute on easy, trivial data. CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving Challenge 2.0 outperforming the previous state-of-theart by 458% and the best concurrent submission by 32.6%.

## Introduction

The trend in autonomous driving is shifting towards end-toend solutions, showed by recent advances in industry [[33]](#b32) and the state-of-the-art performance on the CARLA Leaderboard 1.0 [[6,](#b5)[15,](#b14)[27,](#b26)[30,](#b29)[39]](#b38). Most of the top-performing entries on the CARLA Leaderboard 1.0 [[1]](#b0) rely on expensive LiDAR sensors, with the exception of TCP [[39]](#b38), which employs a camera-only approach. Additionally, multi-task learning has emerged as a common strategy for enhancing performance [[9]](#b8). However, this requires access to labels, such as BEV semantics, depth, or semantic segmentation, which are expensive to obtain in the real world. This makes it hard to transfer insights from research using simulators to real world driving in a scalable and cost-efficient way. Car-LLaVA in contrast only relies on commonly available and easy to obtain driving data such as camera images and driving trajectory and is a camera only method. Additionally, most state-of-the-art CARLA methods use ResNet-style backbones pretrained on ImageNet [[15,](#b14)[27,](#b26)[30,](#b29)[39]](#b38). However, recent progress in pretraining techniques, such as CLIP [[23]](#b22), MAE [[13]](#b12), and DINO, have demonstrated the advantages of using Vision Transformers (ViTs) [[31]](#b30) over traditional CNN-encoders for improved feature learning. Moreover, state-of-the-art VLMs [[8,](#b7)[17,](#b16)[20]](#b19) that fine-tune the CLIP encoder exhibit nuanced image understanding, indicating the existence of strong vision features. CarLLaVA makes use of this by using the vision encoder of LLaVA-NeXT [[19]](#b18)[[20]](#b19)[[21]](#b20) which is pre-trained on internetscale vision-language data. While the size of modern VLMs could be viewed as a concern for inference time when deployed on real vehicles, several recent works showed that this is a solvable engineering problem [[2,](#b1)[3,](#b2)[35]](#b34). In this technical report, we describe the details of our driving model CarLLaVA, which includes the following properties and advantages: Camera only without expensive labels: Our method only uses camera input, eliminating the need for additional expensive labels such as Bird's Eye View (BEV), depth, or semantic segmentation. This label-free approach reduces dependency on extensive labeled datasets, making deployment on real cars more feasible. Vision-Language Pretraining: Our approach leverages a vision encoder pre-trained on internet-scale visionlanguage data. We demonstrate that this pretraining can be effectively transferred to the task of driving, resulting in improved driving performance compared to training from scratch on driving data. High-resolution input: We noticed that the default resolution of the CLIP vision encoder is not sufficient for quality driving. Similar to LLaVA [[21]](#b20), we split input images into patches to allow the VLM access smaller details in the driving images such as distant traffic lights and pedestrians. In contrast to LLaVA we do not use the small resolution global patch to reduce the number of tokens. Efficient Training Recipe: We propose an efficient training recipe that makes more use of interesting training samples, significantly reducing training time. Semi-Disentangled Output Representation: We propose a semi-disentangled representation with both timeconditioned waypoints and space-conditioned path waypoints, leading to better control.

## Related Work

Foundation models for driving. Recently, large language models (LLMs) have been integrated into driving systems to leverage their reasoning capabilities for addressing longtail scenarios. Multi-modal LLM-based driving frameworks such as LLM-Driver [[7]](#b6), DriveGPT4 [[40]](#b39), and DriveLM [[32]](#b31) utilize foundation models with the inputs from different modalities for driving. GPT-Driver [[22]](#b21) and Lan-guageMPC [[25]](#b24) fine-tune ChatGPT as a motion planner using text. Knowledge-driven approaches [[12,](#b11)[37]](#b36) are also adopted to make decisions based on common-sense knowledge and evolve continuously. However, most of these works have been evaluated primarily through qualitative analysis or in open-loop settings. The most similar works leveraging foundation models for closed-loop driving in CARLA are DriveMLM [[36]](#b35) and LMDrive [[28]](#b27), which utilize multi-modal LLMs. However, these approaches rely on image and LiDAR inputs with customized encoders, without leveraging the power of vision-language pretraining and focused on tasks like instruction following. In comparison we focus on pure closed-loop driving performance to provide a baseline that can solve basic driving behaviors to enable future research on VLMs for driving. End-to-end closed-loop driving in CARLA. End-to-end training based on Imitation Learning (IL) is the dominant approach for state-of-the-art methods on the CARLA Leaderboard 1.0 [[5,](#b4)[15,](#b14)[26,](#b25)[38]](#b37). Those methods are mostly incorporate numerous auxiliary outputs and rely on expensive sensors like LiDAR. In contrast, we build a model that only relies on camera images and the driving trajectory. The dominant output representation is predicting waypoints with a GRU and using PID-controllers for lateral and longitudinal control [[5,](#b4)[10,](#b9)[15,](#b14)[16,](#b15)[24,](#b23)[26,](#b25)[29,](#b28)[38,](#b37)[41]](#b40). TCP [[38]](#b37) showed that waypoints perform poorly in turns, but predicting direct control performs worse in avoiding collisions. They propose a situation-based fusion strategy of those representations. Interfuser [[26]](#b25) proposed predicting path waypoints together with a combination of forecasting and heuristics to obtain control. TF++ [[15]](#b14) uses path waypoints for lateral control and target speed classes for longitudinal control. In our work we leverage the path representation for improved steering together with the standard waypoints for longitudinal control avoiding heuristics or the need for predefined classes. Additionally directly predict the waypoints from the output features of the transformer without using GRU.

## Method

In the following sections, we provide a comprehensive overview of our architecture and training methodology. mediate target points. The map includes diverse environments such as highways, urban streets, residential areas, and rural settings, all of which must be navigated under various weather conditions, including clear daylight, sunset, rain, fog, and nighttime scenarios. Along the way the agent must manage various complex scenarios such as encountering pedestrians, navigating parking exits, executing unprotected turns, merging into ongoing traffic, passing construction sites or avoiding vehicles with opening doors.

Architecture. An overview of our base architecture can be seen in Fig. [1](#fig_0). Input/Output Representation. The model inputs include camera images, the next two target points, and the ego vehicle's speed. We tested several configurations: (1) the base model (C1T1) with a single front view image, (2) the temporal model (C1T2) which includes image features from the previous timestep, and (3) the multi-view model (C2T1) which adds a low-resolution rear-view camera to the high-resolution front view. For the output, we use a semi-disentangled representation with both time-conditioned waypoints with a PID controller for longitudinal control and space-conditioned path waypoints with a PID controller for lateral control. Early experiments with entangled waypoints led to steering errors, especially during turns or when swerving around obstacles. By using path waypoints, we achieve denser supervision, as we also predict the path when the vehicle is stationary, leading to improved steering behaviour. For longitudinal control we use standard time-conditioned waypoints to make use of the better collision avoidance compared to directly predicting control [[38]](#b37). We also experimented with target speed classification and GRUs, but these methods did not perform as well, although we lack official performance metrics. HD-Vision Encoder. To encode the camera images, we use the LLaVA-NeXT vision encoder, specifically the CLIPViT-L-336px model, which is the highest resolution trained CLIP model. High-resolution images are crucial for driving because important information, such as traffic lights at large intersections, may only be visible in a few pixels. To leverage CLIP pre-training at higher resolutions than 336x336, we use LLaVA's anyres technique [[21]](#b20). We divide high-resolution images into multiple large patches of up to 336x336 pixels, encoding each independently, and then concatenating the resulting features in spatial dimension to form a single large feature map for the original image. Using a VLM not only provides strong features, but also offers the advantage of easily query the VLM to identify what information are captured in the image features. More specifically, we queried the VLM for example for the state of traffic lights at different input resolutions to determine the optimal resolution and therefore the number of patches. Adapter. To reduce computation overhead due to the nature of the quadratic complexity of the LLaMA transformer, we downsample the feature map to half the number of tokens. After flattening, we employ a linear projection layer to map the vision features to the embedding space of the language model. To encode the target points and ego speed, we utilize a multi-layer perceptron (MLP) following a normalization layer. Additionally, we add camera encodings for the different views (model C2T1) and temporal encodings when using images from multiple time steps (only for model C1T2). LM-Decoder. We use the LLaMA architecture as a decoder. In addition to the sensor input tokens, we use learnable queries to generate the path and waypoints. An MLP on top of the output features generates waypoint differences. The cumulative sum of these differences yields the final waypoints, which are supervised during training using mean squared error (MSE) loss. For our preliminary results on generating language explanations we auto-regressively sample the language explanation after generating the path and waypoints. During training we feed the tokenized explanations and use a standard language modelling (LM) loss. We use the tokenizer and LM-head of the pretrained Tiny-LLaMA model.

Efficient training of large models. Our models have between 350M and 1.3B parameter. To finetune these large models on our task we rely on models pretrained on internet-scale data, a large dataset and an efficient training recipe which is described in the following. Dataset We utilize the privileged rule-based expert PDM- light [[4]](#b3) to collect a dataset. We divide the official CARLA routes of Town 12 and Town 13 into shorter segments centered around scenarios to reduce trivial data (e.g., driving straight without any hazardous events) and simplify data management. We use short routes with a single scenario as proposed by [[9,](#b8)[18]](#b17), however with the introduction of Leaderboard 2.0, the maximum distance between target points increased from 50 meters to 200 meters. The short routes often fall within this distance, causing a distribution shift, as the next target point is the end of the route (i.e, closer than 200m) rather than the position that would be used when having long routes. Consequently, we employ a second set of routes featuring three scenarios per route.

To ensure balance, we adjust the number of routes per scenario and apply random weather augmentation and modify the parameter distance for scenarios by ±10%. Overall, we collect 2.9 million samples at 5 fps.

For the language generation experiment we use the logic of the rule-based expert to generate explanations. More precisely, we use the leading object obtained from the experts' Intelligent Driver Model (IDM) [[34]](#b33) as well as information about changing the path to swerve around objects. In addition, we use heuristics based on the ego waypoints to distinguish between driving intentions like starting from stop or keep driving at the same speed. As this experiment is only intended to showcase the potential of using LLMs for driving, we do not add detailed statistics of the obtained labels and keep it for future work.

Buckets. The majority of driving involves straight, uneventful segments. To maximize the collection of interesting scenarios during data collection, we focus on capturing a diverse range of challenging situations. However, some ratio of easy and uneventful data is inevitable. Training models on the entire dataset revealed that straight driving without hazards is effectively learned in the early epochs, resulting in wasted compute in later epochs as the models continue to train on these uninteresting samples. To address this issue, we create data buckets containing only the interesting samples and sample from these buckets during training in-

DS ↑ Stat ↓ WPs 3.21 0.68 +Path 4.49 0.0 (a) Output. DS ↑ LLaVA 6.87 -pretraining 0.45 Resnet-34 2.71 (b) Vision encoder. DS ↑ 1300 3.93 1800 4.49 2100 6.87 2400 6.35 (c) Early stopping.

Table 2. Ablations of different parts of our model, showcasing the superiority of the semi-disentangled output representation and the large impact of the correct threshold for early stopping. The score of the default configuration is highlighted in gray. All numbers are official Leaderboard scores.

stead of the entire dataset. We use: (1) five buckets for different amount of acceleration and deceleration with one specifically for starting from stop, excluding samples with acceleration between -1 and 1, (2) two buckets for steering, excluding samples for going straight, (3) three buckets for vehicle hazard with vehicles coming from different directions, ( [4](#)) one for stop sign, red light and walker hazards each, ( [5](#)) one bucket for swerving around obstacles and ( [6](#)) one bucket that samples from the whole dataset to keep a small portion of uneventful data such as driving straight. This approach reduces the number of samples per epoch 650,000.

## Experiments

Benchmarks. Leaderboard2.0. We use the official test server with secret routes under different weather conditions. 10xShort. For the models where we could not get Leaderboard results, we use a local evaluation on short routes with one scenario per route to evaluate the models ability to solve each scenario type. We use maximum 10 routes per scenario which are randomly sampled from the whole set.

Metrics. We report the official CARLA metrics, Driving Score (DS), Route Completion (RC) and Infraction Score (IS). DS is calculated in a way that the reduction due to infractions does not linearly correlate with the increase in DS due to higher RC (i.e., with a constant infraction per km the DS gets much worse for higher RC for models that can solve the scenarios below a certain percentage). Forcing the agent to stop a route early can maximize DS. Implementation Details. We use a learning rate of 3e-5 with a cosine annealing schedule. The batch size of our base model is 20, while for specific configurations, we use a batch size of 10 for C1T2 and a batch size of 12 for C2T1. The AdamW optimizer is employed with a weight decay of 0.1. Our vision encoder consists of 305 million parameters. We experiment with the LLaMA architecture in three configurations: LLaMA-50M, LLaMA-350M (both trained from scratch), and a 1B TinyLLaMA with LoRA finetuning [[14]](#b13), applied to all linear layers as demonstrated to be effective by QLoRA [[11]](#b10). We apply the same data augmentation techniques as TF++ [[15]](#b14) but with more aggressive shift and rotation augmentation (shift: 1.5m, rot: 20 deg). Additionally, we add histogram enhancements to improve the contrast and quality of input images for night time driving. DeepSpeed v2 is utilized for optimizing training effi-ciency and memory usage. We train for 30 epochs. Our base model, C1T1, trains in approximately 27 hours using 8xA100 40GB GPUs. During inference we apply early stopping to counter the nature of DS described in the metric section. We track the travelled distance and stop driving after a specified distance when the steering angle is close to zero to prevent stopping in the middle of an intersection where it could happen that other vehicles crash into us.

## Results.

Leaderboard state of the art. We present the official Leaderboard results in Tab.

1. With our base-model CarLLaVA C1T1 we outperfrom the state of the art (5.18 vs 6.87 DS). However, we observed a high variance on the Leaderboard score, detailed results on mean and standard deviation can be found in the supplementary (the official Leaderboard numbers are our first submissions of the models, the repetitions to calculate mean and std happened after the challenge deadline). It is also noteworthy that, to the best of our knowledge, our model is the only model on the leaderboard working only with camera images and without the usage of additional auxiliary labels (note: for the new entry greatone we do not know what their method is). Output representation. Tab. 2a compares the DS on the Leaderboard for the different output representations. As the goal of the additional path prediction is improved lateral control, we also report the collisions with static layout as this is mainly caused due to bad steering. With the semidisentangled representation we can reduce the layout collision from 0.68 to 0.0 showcasing the strength of additional path predictions. Vision-Language and CLIP pretraining. We ablate the pretraining of the vision encoder and train the same model from scratch. Tab. 2b '-pretraining' shows that the pretraining stage is essential for good driving performance (more tuning of the training hyperparameters can further improve the performance but is unlikely to reach the performance of the pretrained model). Additionally, we show a comparison to the widely used Resnet-34 pretrained on ImageNet. The decreased performance (2.71 vs. 6.87 DS) indicates the importance of the larger ViT and the internet-scale imagelanguage pretraining. Early stopping. We ablate the thresholds for the early stopping as it is not trivial to calculate the perfect trade-off as the routes and density of scenarios are secret (however a rough function of the expected DS can be caluculated which we DS S ↑ 50M 90.40 350M 92.49 1B pt LoRA 90.03 1B s LoRA 89.57 (a) Scale. DS S ↑ default 90.40 + temporal 90.37 + back 88.81 -pretraining 75.43 (b) Input.

Table 3. Further ablations of different parts of our model. The score of the default configuration is highlighted in gray. DSS is performance on the 10xShort benchmark.

used to get a rough idea). Tab. 2c shows the Leaderboard DS for a given travelled distance in meters. This hyperparameter has a big impact on the final score.

Preliminary Results.

In addition to our ablations we show preliminary results to showcase the potential to extend to multiple views and temporal input, scaling our model and adding language predictions. Leaderboard variance. We submitted our base model Car-LLaVA C1T1 with an early stopping threshold of 2100 and 2400 three times to the leadboard to get an estimate of the evlauation variance. For the 2100 model we obtain the following scores: 5.5, 6.8 and 5.3 resulting in a mean DS of 5.87 with a standard deviation of 0.81. The base model with a threshold of 2400 obtained 6.3, 6.3 and 4.8 resuting in a mean of 5.8 with standard deviation of 0.87. Scale. In an additional experiment we scale up the LLaMA architecture (Tab. 3a). Training a 350M parameter model from scratch improves performance slightly. However scaling to 1B parameter and finetuning with LoRA resulted in worse performance for using a pretrained LLM (pt) and training from scratch (s). We suspect that this may be due to the use of LoRA finetuning and not fully tuned hyperparameters, but further investigation is needed. This remains an interesting research question for future work. Extending the input. To be able to fully solve autonomous driving, information from more than one camera (especially for camera-only arhcitectures) and temporal information are needed. In Tab. 3b we show results for a model with temporal information and one with added back camera. Qualitative investigations showed improvements in the expected scenarios (less rear-end collisions for +temporal and improved lane-change behaviour for +back). Interestingly the overall score does not increase. Language explanations. With the additional language training our model is able to produce commentary that comments the current driving behaviour (Fig. [2](#fig_1)). This is not intended as an actual explanation as the training misses an important grounding step (i.e., commentary is not always aligned with the actions the model takes). We leave this for future work. Failure cases. The most common failure cases of our model are rear end collision, which can be reduced by using the The ego vehicle is waiting for a gap in the traffic before changing lanes to the lane with oncoming traffic to go around the construction site.

The ego vehicle is changing to the oncoming lane to go around the construction site, because the gap is large enough.

The ego vehicle remains stopped due to a pedestrian crossing its path. 

## Conclusion

In this report, we present CarLLaVA the winning entry in the CARLA Autonomous Driving Challenge 2.0 2024, which leverages vision-language pretraining and uses only camera images as input. By utilizing a semi-disentangled output representation and an efficient training approach, CarLLaVA demonstrates superior performance in both lateral and longitudinal control. Its ability to operate without expensive labels or sensors makes it a scalable and costeffective solution. The results indicate a substantial improvement over previous methods, showcasing the potential of vision-language models in real-world autonomous driving applications.

![Figure 1. CarLLaVA base model architecture. (C1T1) The images are split in two, and each split is independently encoded and then concatenated, downsampled and projected into a pre-trained large language model. The output utilises a semi-disentangled representation with both time-conditioned waypoints and spaceconditioned path waypoints for improved lateral control.]()

![Figure 2. Qualitative examples of generated language. Red: predicted path, Green: predicted waypoints, Blue: Target Points]()

![Leaderboard 2.0 Results.]()

