{
  "1310.6753": {
    "id": "1310.6753",
    "title": "Romantic Partnerships and the Dispersion of Social Ties: A Network   Analysis of Relationship Status on Facebook",
    "authors": "Lars Backstrom, Jon Kleinberg",
    "abstract": "A crucial task in the analysis of on-line social-networking systems is to identify important people --- those linked by strong social ties --- within an individual's network neighborhood. Here we investigate this question for a particular category of strong ties, those involving spouses or romantic partners. We organize our analysis around a basic question: given all the connections among a person's friends, can you recognize his or her romantic partner from the network structure alone? Using data from a large sample of Facebook users, we find that this task can be accomplished with high accuracy, but doing so requires the development of a new measure of tie strength that we term `dispersion' --- the extent to which two people's mutual friends are not themselves well-connected. The results offer methods for identifying types of structurally significant people in on-line applications, and suggest a potential expansion of existing theories of tie strength.",
    "url": "https://arxiv.org/abs/1310.6753",
    "arxivId": "1310.6753",
    "last_visited": "2025-01-03T18:28:19.059Z",
    "last_read": "2025-01-04T06:52:00.641659",
    "total_reading_time_seconds": 79,
    "published_date": "2013-10-24T20:00:18Z",
    "arxiv_tags": [
      "cs.SI",
      "physics.soc-ph",
      "H.2.8"
    ]
  },
  "1411.1792": {
    "id": "1411.1792",
    "title": "How transferable are features in deep neural networks?",
    "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson",
    "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
    "url": "https://arxiv.org/abs/1411.1792",
    "arxivId": "1411.1792",
    "last_visited": "2024-12-24T02:43:09.950Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2014-11-06T23:09:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE"
    ]
  },
  "1503.02531": {
    "id": "1503.02531",
    "title": "Distilling the Knowledge in a Neural Network",
    "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
    "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
    "url": "https://arxiv.org/abs/1503.02531",
    "arxivId": "1503.02531",
    "last_visited": "2024-12-21T16:06:57.364Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2015-03-09T15:44:49Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "cs.NE"
    ]
  },
  "1503.03585": {
    "id": "1503.03585",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
    "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
    "url": "https://arxiv.org/abs/1503.03585",
    "arxivId": "1503.03585",
    "last_visited": "2024-12-22T07:09:20.505Z",
    "last_read": "2025-01-05T08:23:44.500651",
    "total_reading_time_seconds": 60,
    "published_date": "2015-03-12T04:51:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "q-bio.NC",
      "stat.ML"
    ]
  },
  "1506.06579": {
    "id": "1506.06579",
    "title": "Understanding Neural Networks Through Deep Visualization",
    "authors": "Jason Yosinski, Jeff Clune, Anh Nguyen and 2 others",
    "abstract": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.",
    "url": "https://arxiv.org/abs/1506.06579",
    "arxivId": "1506.06579",
    "last_visited": "2024-12-24T02:47:02.227Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2015-06-22T12:57:15Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ]
  },
  "1602.03483": {
    "id": "1602.03483",
    "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
    "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen",
    "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",
    "url": "https://arxiv.org/abs/1602.03483",
    "arxivId": "1602.03483",
    "last_visited": "2025-01-03T20:13:43.540Z",
    "last_read": "2025-01-04T06:51:57.840814",
    "total_reading_time_seconds": 24,
    "published_date": "2016-02-10T18:49:58Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "1705.07831": {
    "id": "1705.07831",
    "title": "Stabilizing GAN Training with Multiple Random Projections",
    "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti",
    "abstract": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",
    "url": "https://arxiv.org/abs/1705.07831",
    "arxivId": "1705.07831",
    "last_visited": "2025-01-11T08:02:03.936Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2017-05-22T16:23:26Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ]
  },
  "1705.08039": {
    "id": "1705.08039",
    "title": "Poincaré Embeddings for Learning Hierarchical Representations",
    "authors": "Maximilian Nickel, Douwe Kiela",
    "abstract": "Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\\'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.",
    "url": "https://arxiv.org/abs/1705.08039",
    "arxivId": "1705.08039",
    "last_visited": "2024-12-29T01:44:01.360000+00:00",
    "last_read": "2025-01-04T14:49:42.247780",
    "total_reading_time_seconds": 20,
    "published_date": "2017-05-22T23:14:36Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  "1705.10359": {
    "id": "1705.10359",
    "title": "Neural Embeddings of Graphs in Hyperbolic Space",
    "authors": "Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth",
    "abstract": "Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.",
    "url": "https://arxiv.org/abs/1705.10359",
    "arxivId": "1705.10359",
    "last_visited": "2024-12-29T01:43:53.617000+00:00",
    "last_read": "2025-01-04T14:49:42.248652",
    "total_reading_time_seconds": 11,
    "published_date": "2017-05-29T18:47:30Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1706.05806": {
    "id": "1706.05806",
    "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning   Dynamics and Interpretability",
    "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
    "abstract": "We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/",
    "url": "https://arxiv.org/abs/1706.05806",
    "arxivId": "1706.05806",
    "last_visited": "2024-12-24T02:53:44.603Z",
    "last_read": "2025-01-05T08:23:29.503537",
    "total_reading_time_seconds": 20,
    "published_date": "2017-06-19T07:09:20Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1710.09412": {
    "id": "1710.09412",
    "title": "mixup: Beyond Empirical Risk Minimization",
    "authors": "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
    "url": "https://arxiv.org/abs/1710.09412",
    "arxivId": "1710.09412",
    "last_visited": "2024-12-22T17:42:57.473Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2017-10-25T18:30:49Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "1711.11586": {
    "id": "1711.11586",
    "title": "Toward Multimodal Image-to-Image Translation",
    "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak and 4 others",
    "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
    "url": "https://arxiv.org/abs/1711.11586",
    "arxivId": "1711.11586",
    "last_visited": "2025-01-12T06:42:01.228Z",
    "last_read": "2025-01-12T06:43:53.633334",
    "total_reading_time_seconds": 6,
    "published_date": "2017-11-30T18:59:01Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR",
      "stat.ML"
    ]
  },
  "1802.03426": {
    "id": "1802.03426",
    "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension   Reduction",
    "authors": "Leland McInnes, John Healy, James Melville",
    "abstract": "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",
    "url": "https://arxiv.org/abs/1802.03426",
    "arxivId": "1802.03426",
    "last_visited": "2024-12-30T03:35:58.743Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-02-09T19:39:33Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.CG",
      "cs.LG"
    ]
  },
  "1804.03329": {
    "id": "1804.03329",
    "title": "Representation Tradeoffs for Hyperbolic Embeddings",
    "authors": "Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala",
    "abstract": "Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.",
    "url": "https://arxiv.org/abs/1804.03329",
    "arxivId": "1804.03329",
    "last_visited": "2024-12-29T02:34:35.575Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-04-10T03:39:16Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "1807.00734": {
    "id": "1807.00734",
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "authors": "Alexia Jolicoeur-Martineau",
    "abstract": "In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs.   We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function.   Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.",
    "url": "https://arxiv.org/abs/1807.00734",
    "arxivId": "1807.00734",
    "last_visited": "2025-01-12T17:59:03.697Z",
    "last_read": "2025-01-11T08:07:25.239318",
    "total_reading_time_seconds": 16,
    "published_date": "2018-07-02T15:11:23Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  "1810.00363": {
    "id": "1810.00363",
    "title": "A Kernel Perspective for Regularizing Deep Neural Networks",
    "authors": "Alberto Bietti, Grégoire Mialon, Dexiong Chen, Julien Mairal",
    "abstract": "We propose a new point of view for regularizing deep neural networks by using the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various practical strategies. Specifically, this perspective (i) provides a common umbrella for many existing regularization principles, including spectral norm and gradient penalties, or adversarial training, (ii) leads to new effective regularization penalties, and (iii) suggests hybrid strategies combining lower and upper bounds to get better approximations of the RKHS norm. We experimentally show this approach to be effective when learning on small datasets, or to obtain adversarially robust models.",
    "url": "https://arxiv.org/abs/1810.00363",
    "arxivId": "1810.00363",
    "last_visited": "2024-12-29T10:35:11.499Z",
    "last_read": "2025-01-04T14:49:18.223709",
    "total_reading_time_seconds": 14,
    "published_date": "2018-09-30T11:40:59Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1810.01588": {
    "id": "1810.01588",
    "title": "Interpreting Layered Neural Networks via Hierarchical Modular   Representation",
    "authors": "Chihiro Watanabe",
    "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. In this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values.",
    "url": "https://arxiv.org/abs/1810.01588",
    "arxivId": "1810.01588",
    "last_visited": "2024-12-24T03:07:38.277Z",
    "last_read": "2025-01-04T15:03:30.857351",
    "total_reading_time_seconds": 6,
    "published_date": "2018-10-03T05:38:26Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "1904.08779": {
    "id": "1904.08779",
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech   Recognition",
    "authors": "Daniel S. Park, William Chan, Yu Zhang and 4 others",
    "abstract": "We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
    "url": "https://arxiv.org/abs/1904.08779",
    "arxivId": "1904.08779",
    "last_visited": "2024-12-17T14:41:05.563Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-04-18T17:53:38Z",
    "arxiv_tags": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "stat.ML"
    ]
  },
  "1906.01563": {
    "id": "1906.01563",
    "title": "Hamiltonian Neural Networks",
    "authors": "Sam Greydanus, Misko Dzamba, Jason Yosinski",
    "abstract": "Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",
    "url": "https://arxiv.org/abs/1906.01563",
    "arxivId": "1906.01563",
    "last_visited": "2024-12-24T02:49:04.570Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-06-04T16:27:55Z",
    "arxiv_tags": [
      "cs.NE"
    ]
  },
  "1906.04358": {
    "id": "1906.04358",
    "title": "Weight Agnostic Neural Networks",
    "authors": "Adam Gaier, David Ha",
    "abstract": "Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/",
    "url": "https://arxiv.org/abs/1906.04358",
    "arxivId": "1906.04358",
    "last_visited": "2024-12-24T03:27:56.957Z",
    "last_read": "2025-01-04T15:03:24.866258",
    "total_reading_time_seconds": 20,
    "published_date": "2019-06-11T02:40:11Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ]
  },
  "1906.05433": {
    "id": "1906.05433",
    "title": "Tackling Climate Change with Machine Learning",
    "authors": "David Rolnick, Priya L. Donti, Lynn H. Kaack and 19 others",
    "abstract": "Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.",
    "url": "https://arxiv.org/abs/1906.05433",
    "arxivId": "1906.05433",
    "last_visited": "2024-12-22T06:23:13.395Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-06-10T17:51:47Z",
    "arxiv_tags": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  "1912.02757": {
    "id": "1912.02757",
    "title": "Deep Ensembles: A Loss Landscape Perspective",
    "authors": "Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan",
    "abstract": "Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.",
    "url": "https://arxiv.org/abs/1912.02757",
    "arxivId": "1912.02757",
    "last_visited": "2024-12-24T03:01:38.243Z",
    "last_read": "2025-01-05T08:23:23.618098",
    "total_reading_time_seconds": 6,
    "published_date": "2019-12-05T17:48:18Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ]
  },
  "2001.04063": {
    "id": "2001.04063",
    "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence   Pre-training",
    "authors": "Weizhen Qi, Yu Yan, Yeyun Gong and 5 others",
    "abstract": "This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.",
    "url": "https://arxiv.org/abs/2001.04063",
    "arxivId": "2001.04063",
    "last_visited": "2024-12-26T17:17:59.219Z",
    "last_read": "2025-01-04T15:03:09.858274",
    "total_reading_time_seconds": 3,
    "published_date": "2020-01-13T05:12:38Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2001.04451": {
    "id": "2001.04451",
    "title": "Reformer: The Efficient Transformer",
    "authors": "Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
    "url": "https://arxiv.org/abs/2001.04451",
    "arxivId": "2001.04451",
    "last_visited": "2025-01-05T20:08:29.858000+00:00",
    "last_read": "2025-01-05T20:09:30.079520",
    "total_reading_time_seconds": 3,
    "published_date": "2020-01-13T18:38:28Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ]
  },
  "2006.11120": {
    "id": "2006.11120",
    "title": "From Discrete to Continuous Convolution Layers",
    "authors": "Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani",
    "abstract": "A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.",
    "url": "https://arxiv.org/abs/2006.11120",
    "arxivId": "2006.11120",
    "last_visited": "2024-12-25T05:29:13.001Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-06-19T13:16:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ]
  },
  "2006.14769": {
    "id": "2006.14769",
    "title": "Supermasks in Superposition",
    "authors": "Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu and 4 others",
    "abstract": "We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.",
    "url": "https://arxiv.org/abs/2006.14769",
    "arxivId": "2006.14769",
    "last_visited": "2024-12-24T02:37:38.778Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-06-26T03:16:44Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  "2009.10195": {
    "id": "2009.10195",
    "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving   Out-of-Domain Robustness",
    "authors": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi",
    "abstract": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
    "url": "https://arxiv.org/abs/2009.10195",
    "arxivId": "2009.10195",
    "last_visited": "2024-12-21T18:19:38.104Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-09-21T22:02:33Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ]
  },
  "2010.11929": {
    "id": "2010.11929",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale",
    "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov and 9 others",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
    "url": "https://arxiv.org/abs/2010.11929",
    "arxivId": "2010.11929",
    "last_visited": "2025-01-04T15:01:19.005Z",
    "last_read": "2025-01-04T15:01:58.106137",
    "total_reading_time_seconds": 16,
    "published_date": "2020-10-22T17:55:59Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2012.13255": {
    "id": "2012.13255",
    "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model   Fine-Tuning",
    "authors": "Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta",
    "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
    "url": "https://arxiv.org/abs/2012.13255",
    "arxivId": "2012.13255",
    "last_visited": "2024-12-24T02:33:31.614000+00:00",
    "last_read": "2025-01-05T08:23:41.555214",
    "total_reading_time_seconds": 7,
    "published_date": "2020-12-22T07:42:30Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2103.13413": {
    "id": "2103.13413",
    "title": "Vision Transformers for Dense Prediction",
    "authors": "René Ranftl, Alexey Bochkovskiy, Vladlen Koltun",
    "abstract": "We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
    "url": "https://arxiv.org/abs/2103.13413",
    "arxivId": "2103.13413",
    "last_visited": "2025-01-04T14:48:49.256000+00:00",
    "last_read": "2025-01-04T15:01:58.107814",
    "total_reading_time_seconds": 5,
    "published_date": "2021-03-24T18:01:17Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2105.01601": {
    "id": "2105.01601",
    "title": "MLP-Mixer: An all-MLP Architecture for Vision",
    "authors": "Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov and 9 others",
    "abstract": "Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. \"mixing\" the per-location features), and one with MLPs applied across patches (i.e. \"mixing\" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.",
    "url": "https://arxiv.org/abs/2105.01601",
    "arxivId": "2105.01601",
    "last_visited": "2025-01-05T20:12:13.323Z",
    "last_read": "2025-01-05T20:13:29.393146",
    "total_reading_time_seconds": 22,
    "published_date": "2021-05-04T16:17:21Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2105.05720": {
    "id": "2105.05720",
    "title": "Breaking the Computation and Communication Abstraction Barrier in   Distributed Machine Learning Workloads",
    "authors": "Abhinav Jangda, Jun Huang, Guodong Liu and 6 others",
    "abstract": "Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone.   Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.",
    "url": "https://arxiv.org/abs/2105.05720",
    "arxivId": "2105.05720",
    "last_visited": "2024-12-16T05:53:22.033Z",
    "last_read": "2025-01-05T18:41:15.658266",
    "total_reading_time_seconds": 158,
    "published_date": "2021-05-12T15:13:43Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.LG",
      "cs.PL"
    ]
  },
  "2105.08050": {
    "id": "2105.08050",
    "title": "Pay Attention to MLPs",
    "authors": "Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le",
    "abstract": "Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.",
    "url": "https://arxiv.org/abs/2105.08050",
    "arxivId": "2105.08050",
    "last_visited": "2025-01-05T23:07:26.844000+00:00",
    "last_read": "2025-01-05T23:08:39.880665",
    "total_reading_time_seconds": 45,
    "published_date": "2021-05-17T17:55:04Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ]
  },
  "2106.10165": {
    "id": "2106.10165",
    "title": "The Principles of Deep Learning Theory",
    "authors": "Daniel A. Roberts, Sho Yaida, Boris Hanin",
    "abstract": "This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",
    "url": "https://arxiv.org/abs/2106.10165",
    "arxivId": "2106.10165",
    "last_visited": "2024-12-29T22:46:13.679Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2021-06-18T15:00:00Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "hep-th",
      "stat.ML"
    ]
  },
  "2107.11817": {
    "id": "2107.11817",
    "title": "Go Wider Instead of Deeper",
    "authors": "Fuzhao Xue, Ziji Shi, Futao Wei and 3 others",
    "abstract": "More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference.   In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on average and surpass BERT using factorized embedding parameterization by $0.8\\%$ with fewer parameters.",
    "url": "https://arxiv.org/abs/2107.11817",
    "arxivId": "2107.11817",
    "last_visited": "2025-01-05T18:44:31.871Z",
    "last_read": "2025-01-05T18:40:55.338983",
    "total_reading_time_seconds": 13,
    "published_date": "2021-07-25T14:44:24Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2110.09456": {
    "id": "2110.09456",
    "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
    "authors": "Sam Shleifer, Jason Weston, Myle Ott",
    "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .",
    "url": "https://arxiv.org/abs/2110.09456",
    "arxivId": "2110.09456",
    "last_visited": "2025-01-07T23:17:21.742Z",
    "last_read": "2025-01-07T23:18:44.187174",
    "total_reading_time_seconds": 36,
    "published_date": "2021-10-18T16:47:45Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2111.11418": {
    "id": "2111.11418",
    "title": "MetaFormer Is Actually What You Need for Vision",
    "authors": "Weihao Yu, Mi Luo, Pan Zhou and 5 others",
    "abstract": "Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in Transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in Transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of \"MetaFormer\", a general architecture abstracted from Transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent Transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer.",
    "url": "https://arxiv.org/abs/2111.11418",
    "arxivId": "2111.11418",
    "last_visited": "2025-01-05T20:25:32.615Z",
    "last_read": "2025-01-05T20:26:49.250252",
    "total_reading_time_seconds": 5,
    "published_date": "2021-11-22T18:52:03Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2112.04215": {
    "id": "2112.04215",
    "title": "Self-Supervised Models are Continual Learners",
    "authors": "Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda and 3 others",
    "abstract": "Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings.",
    "url": "https://arxiv.org/abs/2112.04215",
    "arxivId": "2112.04215",
    "last_visited": "2024-12-28T07:29:58.974Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2021-12-08T10:39:13Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ]
  },
  "2112.14569": {
    "id": "2112.14569",
    "title": "Fine-Tuning Transformers: Vocabulary Transfer",
    "authors": "Vladislav Mosin, Igor Samenko, Alexey Tikhonov and 2 others",
    "abstract": "Transformers are responsible for the vast majority of recent advances in natural language processing. The majority of practical natural language processing applications of these models are typically enabled through transfer learning. This paper studies if corpus-specific tokenization used for fine-tuning improves the resulting performance of the model. Through a series of experiments, we demonstrate that such tokenization combined with the initialization and fine-tuning strategy for the vocabulary tokens speeds up the transfer and boosts the performance of the fine-tuned model. We call this aspect of transfer facilitation vocabulary transfer.",
    "url": "https://arxiv.org/abs/2112.14569",
    "arxivId": "2112.14569",
    "last_visited": "2025-01-10T01:59:25.704Z",
    "last_read": "2025-01-10T02:01:59.118581",
    "total_reading_time_seconds": 28,
    "published_date": "2021-12-29T14:22:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50, 91F20",
      "I.2.7"
    ]
  },
  "2203.02155": {
    "id": "2203.02155",
    "title": "Training language models to follow instructions with human feedback",
    "authors": "Long Ouyang, Jeff Wu, Xu Jiang and 17 others",
    "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
    "url": "https://arxiv.org/abs/2203.02155",
    "arxivId": "2203.02155",
    "last_visited": "2025-01-06T23:15:51.499Z",
    "last_read": "2025-01-06T23:16:42.092303",
    "total_reading_time_seconds": 5,
    "published_date": "2022-03-04T07:04:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2204.00595": {
    "id": "2204.00595",
    "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate   Training",
    "authors": "Tri Dao, Beidi Chen, Nimit Sohoni and 7 others",
    "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called \"reverse sparsification,\" Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
    "url": "https://arxiv.org/abs/2204.00595",
    "arxivId": "2204.00595",
    "last_visited": "2024-12-28T06:07:58.885Z",
    "last_read": "2025-01-04T15:02:51.856832",
    "total_reading_time_seconds": 35,
    "published_date": "2022-04-01T17:37:29Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2205.12381": {
    "id": "2205.12381",
    "title": "First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual   Information Maximization",
    "authors": "Siddharth Reddy, Sergey Levine, Anca D. Dragan",
    "abstract": "How can we train an assistive human-machine interface (e.g., an electromyography-based limb prosthesis) to translate a user's raw command signals into the actions of a robot or computer when there is no prior mapping, we cannot ask the user for supervision in the form of action labels or reward feedback, and we do not have prior knowledge of the tasks the user is trying to accomplish? The key idea in this paper is that, regardless of the task, when an interface is more intuitive, the user's commands are less noisy. We formalize this idea as a completely unsupervised objective for optimizing interfaces: the mutual information between the user's command signals and the induced state transitions in the environment. To evaluate whether this mutual information score can distinguish between effective and ineffective interfaces, we conduct an observational study on 540K examples of users operating various keyboard and eye gaze interfaces for typing, controlling simulated robots, and playing video games. The results show that our mutual information scores are predictive of the ground-truth task completion metrics in a variety of domains, with an average Spearman's rank correlation of 0.43. In addition to offline evaluation of existing interfaces, we use our unsupervised objective to learn an interface from scratch: we randomly initialize the interface, have the user attempt to perform their desired tasks using the interface, measure the mutual information score, and update the interface to maximize mutual information through reinforcement learning. We evaluate our method through a user study with 12 participants who perform a 2D cursor control task using a perturbed mouse, and an experiment with one user playing the Lunar Lander game using hand gestures. The results show that we can learn an interface from scratch, without any user supervision or prior knowledge of tasks, in under 30 minutes.",
    "url": "https://arxiv.org/abs/2205.12381",
    "arxivId": "2205.12381",
    "last_visited": "2025-01-06T11:04:10.770Z",
    "last_read": "2025-01-06T11:07:41.188548",
    "total_reading_time_seconds": 13,
    "published_date": "2022-05-24T21:57:18Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.HC",
      "cs.RO"
    ]
  },
  "2205.13509": {
    "id": "2205.13509",
    "title": "The role of disorder in the motion of chiral swimmers in the presence of   obstacles",
    "authors": "Danne M. van Roon, Giorgio Volpe, Margarida M. Telo da Gama, Nuno A. M. Araújo",
    "abstract": "The presence of obstacles is intuitively expected to hinder the diffusive transport of micro-swimmers. However, for chiral micro-swimmers, a low density of obstacles near a surface can enhance their diffusive behavior, due to the rectification of the chiral motion by the obstacles. Here, we study numerically the role that disorder plays in determining the transport dynamics of chiral micro-swimmers on surfaces with obstacles. We consider different densities of regularly spaced obstacles and distinct types of disorder: noise in the dynamics of the micro-swimmer, quenched noise in the positions of the obstacles as well as obstacle size polydispersity. We show that, depending on the type and strength of the disorder, the presence of obstacles can either enhance or hinder transport, and discuss implications for the control of active transport in disordered media.",
    "url": "https://arxiv.org/abs/2205.13509",
    "arxivId": "2205.13509",
    "last_visited": "2024-12-28T07:25:12.900Z",
    "last_read": "2025-01-04T14:49:54.238526",
    "total_reading_time_seconds": 5,
    "published_date": "2022-05-26T17:22:50Z",
    "arxiv_tags": [
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ]
  },
  "2207.10342": {
    "id": "2207.10342",
    "title": "Language Model Cascades",
    "authors": "David Dohan, Winnie Xu, Aitor Lewkowycz and 9 others",
    "abstract": "Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.",
    "url": "https://arxiv.org/abs/2207.10342",
    "arxivId": "2207.10342",
    "last_visited": "2024-12-28T09:09:37.944Z",
    "last_read": "2025-01-04T14:49:42.250385",
    "total_reading_time_seconds": 51,
    "published_date": "2022-07-21T07:35:18Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2208.02554": {
    "id": "2208.02554",
    "title": "Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add   Data",
    "authors": "Priyanka Singh, Vladislav D. Mosin, Ivan P. Yamshchikov",
    "abstract": "Working within specific NLP subdomains presents significant challenges, primarily due to a persistent deficit of data. Stringent privacy concerns and limited data accessibility often drive this shortage. Additionally, the medical domain demands high accuracy, where even marginal improvements in model performance can have profound impacts. In this study, we investigate the potential of vocabulary transfer to enhance model performance in biomedical NLP tasks. Specifically, we focus on vocabulary extension, a technique that involves expanding the target vocabulary to incorporate domain-specific biomedical terms. Our findings demonstrate that vocabulary extension, leads to measurable improvements in both downstream model performance and inference time.",
    "url": "https://arxiv.org/abs/2208.02554",
    "arxivId": "2208.02554",
    "last_visited": "2025-01-10T02:00:26.390000+00:00",
    "last_read": "2025-01-10T02:01:59.117896",
    "total_reading_time_seconds": 59,
    "published_date": "2022-08-04T09:53:22Z",
    "arxiv_tags": [
      "cs.CL",
      "I.2.7"
    ]
  },
  "2208.11665": {
    "id": "2208.11665",
    "title": "Statistical exploration of the Manifold Hypothesis",
    "authors": "Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy",
    "abstract": "The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. These procedures operate under minimal assumptions and make use of well known, scaleable graph-analytic algorithms.",
    "url": "https://arxiv.org/abs/2208.11665",
    "arxivId": "2208.11665",
    "last_visited": "2024-12-29T02:26:31.276Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-08-24T17:00:16Z",
    "arxiv_tags": [
      "stat.ME",
      "cs.LG",
      "stat.ML",
      "62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30"
    ]
  },
  "2209.02740": {
    "id": "2209.02740",
    "title": "Emergent hypernetworks in weakly coupled oscillators",
    "authors": "Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu and 2 others",
    "abstract": "Networks of weakly coupled oscillators had a profound impact on our understanding of complex systems. Studies on model reconstruction from data have shown prevalent contributions from hypernetworks with triplet and higher interactions among oscillators, in spite that such models were originally defined as oscillator networks with pairwise interactions. Here, we show that hypernetworks can spontaneously emerge even in the presence of pairwise albeit nonlinear coupling given certain triplet frequency resonance conditions. The results are demonstrated in experiments with electrochemical oscillators and in simulations with integrate-and-fire neurons. By developing a comprehensive theory, we uncover the mechanism for emergent hypernetworks by identifying appearing and forbidden frequency resonant conditions. Furthermore, it is shown that microscopic linear (difference) coupling among units results in coupled mean fields, which have sufficient nonlinearity to facilitate hypernetworks. Our findings shed light on the apparent abundance of hypernetworks and provide a constructive way to predict and engineer their emergence.",
    "url": "https://arxiv.org/abs/2209.02740",
    "arxivId": "2209.02740",
    "last_visited": "2024-12-22T05:30:04.148Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-09-06T18:02:12Z",
    "arxiv_tags": [
      "math.DS"
    ]
  },
  "2211.14453": {
    "id": "2211.14453",
    "title": "Transform Once: Efficient Operator Learning in Frequency Domain",
    "authors": "Michael Poli, Stefano Massaroli, Federico Berto and 4 others",
    "abstract": "Spectral analysis provides one of the most effective paradigms for information-preserving dimensionality reduction, as simple descriptions of naturally occurring signals are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequency-domain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: transform once (T1). To enable efficient, direct learning in the frequency domain we derive a variance-preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of 3x to 10x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke. T1 models improve on the test performance of FDMs while requiring significantly less computation (5 hours instead of 32 for our large-scale experiment), with over 20% reduction in average predictive error across tasks.",
    "url": "https://arxiv.org/abs/2211.14453",
    "arxivId": "2211.14453",
    "last_visited": "2024-12-28T07:23:40.569000+00:00",
    "last_read": "2025-01-04T15:02:00.950202",
    "total_reading_time_seconds": 4,
    "published_date": "2022-11-26T01:56:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ]
  },
  "2212.07677": {
    "id": "2212.07677",
    "title": "Transformers learn in-context by gradient descent",
    "authors": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo and 4 others",
    "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "url": "https://arxiv.org/abs/2212.07677",
    "arxivId": "2212.07677",
    "last_visited": "2024-12-21T08:19:06.235Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-12-15T09:21:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2212.14052": {
    "id": "2212.14052",
    "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
    "authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab and 3 others",
    "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
    "url": "https://arxiv.org/abs/2212.14052",
    "arxivId": "2212.14052",
    "last_visited": "2024-12-28T07:22:13.066Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-12-28T17:56:03Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2302.04222": {
    "id": "2302.04222",
    "title": "Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models",
    "authors": "Shawn Shan, Jenna Cryan, Emily Wenger and 3 others",
    "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after \"fine-tuning\" on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply \"style cloaks\" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
    "url": "https://arxiv.org/abs/2302.04222",
    "arxivId": "2302.04222",
    "last_visited": "2024-12-22T16:19:38.213Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-02-08T17:45:23Z",
    "arxiv_tags": [
      "cs.CR"
    ]
  },
  "2302.10866": {
    "id": "2302.10866",
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
    "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen and 6 others",
    "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
    "url": "https://arxiv.org/abs/2302.10866",
    "arxivId": "2302.10866",
    "last_visited": "2024-12-28T07:17:27.699Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-02-21T18:29:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2302.11529": {
    "id": "2302.11529",
    "title": "Modular Deep Learning",
    "authors": "Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, Edoardo Maria Ponti",
    "abstract": "Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.",
    "url": "https://arxiv.org/abs/2302.11529",
    "arxivId": "2302.11529",
    "last_visited": "2024-12-24T03:05:18.719Z",
    "last_read": "2025-01-05T08:23:20.689362",
    "total_reading_time_seconds": 26,
    "published_date": "2023-02-22T18:11:25Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2302.13714": {
    "id": "2302.13714",
    "title": "On the Design of Codes for DNA Computing: Secondary Structure Avoidance   Codes",
    "authors": "Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah and 2 others",
    "abstract": "In this work, we investigate a challenging problem, which has been considered to be an important criterion in designing codewords for DNA computing purposes, namely secondary structure avoidance in single-stranded DNA molecules. In short, secondary structure refers to the tendency of a single-stranded DNA sequence to fold back upon itself, thus becoming inactive in the computation process. While some design criteria that reduces the possibility of secondary structure formation has been proposed by Milenkovic and Kashyap (2006), the main contribution of this work is to provide an explicit construction of DNA codes that completely avoid secondary structure of arbitrary stem length. Formally, given codeword length n and arbitrary integer m>=2, we provide efficient methods to construct DNA codes of length n that avoid secondary structure of any stem length more than or equal to m. Particularly, when m = 3, our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the highest rate found in the prior art was 1.1609 bits/nt. In addition, for m>=3log n + 4, we provide an efficient encoder that incurs only one redundant symbol.",
    "url": "https://arxiv.org/abs/2302.13714",
    "arxivId": "2302.13714",
    "last_visited": "2024-12-28T07:13:40.029Z",
    "last_read": "2025-01-04T15:02:09.852499",
    "total_reading_time_seconds": 6,
    "published_date": "2023-02-27T12:22:07Z",
    "arxiv_tags": [
      "cs.IT",
      "math.CO",
      "math.IT"
    ]
  },
  "2303.00383": {
    "id": "2303.00383",
    "title": "Ordinal Poincaré Sections: Reconstructing the First Return Map from an   Ordinal Segmentation of Time Series",
    "authors": "Zahra Shahriari, Shannon Dee Algar, David M. Walker, Michael Small",
    "abstract": "We propose a robust and computationally efficient algorithm to generically construct first return maps of dynamical systems from time series without the need for embedding. Typically, a first return map is constructed using a heuristic convenience (maxima or zero-crossings of the time series, for example) or a computationally delicate geometric approach (explicitly constructing a Poincar\\'e section from a hyper-surface normal to the flow and then interpolating to determine intersections with trajectories). Our approach relies on ordinal partitions of the time series and builds the first return map from successive intersections with particular ordinal sequences. Generically, we can obtain distinct first return maps for each ordinal sequence. We define entropy-based measures to guide our selection of the ordinal sequence for a ``good'' first return map and show that this method can robustly be applied to time series from classical chaotic systems to extract the underlying first return map dynamics. The results are shown on several well-known dynamical systems (Lorenz, R{\\\"o}ssler and Mackey-Glass in chaotic regimes).",
    "url": "https://arxiv.org/abs/2303.00383",
    "arxivId": "2303.00383",
    "last_visited": "2025-01-03T08:49:43.332Z",
    "last_read": "2025-01-04T06:52:03.683895",
    "total_reading_time_seconds": 4,
    "published_date": "2023-03-01T10:09:57Z",
    "arxiv_tags": [
      "math.DS"
    ]
  },
  "2303.03667": {
    "id": "2303.03667",
    "title": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",
    "authors": "Jierun Chen, Shiu-hong Kao, Hao He and 4 others",
    "abstract": "To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise convolution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is $2.8\\times$, $3.3\\times$, and $2.4\\times$ faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being $2.9\\%$ more accurate. Our large FasterNet-L achieves impressive $83.5\\%$ top-1 accuracy, on par with the emerging Swin-B, while having $36\\%$ higher inference throughput on GPU, as well as saving $37\\%$ compute time on CPU. Code is available at \\url{https://github.com/JierunChen/FasterNet}.",
    "url": "https://arxiv.org/abs/2303.03667",
    "arxivId": "2303.03667",
    "last_visited": "2025-01-05T20:19:32.266000+00:00",
    "last_read": "2025-01-05T20:21:34.744107",
    "total_reading_time_seconds": 35,
    "published_date": "2023-03-07T06:05:30Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2303.08500": {
    "id": "2303.08500",
    "title": "The Devil's Advocate: Shattering the Illusion of Unexploitable Data   using Diffusion Models",
    "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie",
    "abstract": "Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data \"unexploitable.\" This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR.",
    "url": "https://arxiv.org/abs/2303.08500",
    "arxivId": "2303.08500",
    "last_visited": "2024-12-22T16:41:41.995Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-15T10:20:49Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  "2303.09489": {
    "id": "2303.09489",
    "title": "Effectively Modeling Time Series with Simple Discrete State Spaces",
    "authors": "Michael Zhang, Khaled K. Saab, Michael Poli and 3 others",
    "abstract": "Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a \"closed-loop\" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\\ell$ and state-space size $d$, we go from $\\tilde{O}(d \\ell)$ na\\\"ively to $\\tilde{O}(d + \\ell)$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.",
    "url": "https://arxiv.org/abs/2303.09489",
    "arxivId": "2303.09489",
    "last_visited": "2024-12-28T07:09:58.237Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-16T17:08:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2304.02234": {
    "id": "2304.02234",
    "title": "JPEG Compressed Images Can Bypass Protections Against AI Editing",
    "authors": "Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein",
    "abstract": "Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.",
    "url": "https://arxiv.org/abs/2304.02234",
    "arxivId": "2304.02234",
    "last_visited": "2024-12-22T16:44:11.039Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-05T05:30:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  "2304.15004": {
    "id": "2304.15004",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "authors": "Rylan Schaeffer, Brando Miranda, Sanmi Koyejo",
    "abstract": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",
    "url": "https://arxiv.org/abs/2304.15004",
    "arxivId": "2304.15004",
    "last_visited": "2024-12-21T06:06:11.226Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-28T17:52:11Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.LG"
    ]
  },
  "2305.06161": {
    "id": "2305.06161",
    "title": "StarCoder: may the source be with you!",
    "authors": "Raymond Li, Loubna Ben Allal, Yangtian Zi and 64 others",
    "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
    "url": "https://arxiv.org/abs/2305.06161",
    "arxivId": "2305.06161",
    "last_visited": "2024-12-28T06:14:43.367Z",
    "last_read": "2025-01-04T15:02:21.867203",
    "total_reading_time_seconds": 12,
    "published_date": "2023-05-09T08:16:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ]
  },
  "2305.13169": {
    "id": "2305.13169",
    "title": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data   Age, Domain Coverage, Quality, & Toxicity",
    "authors": "Shayne Longpre, Gregory Yauney, Emily Reif and 8 others",
    "abstract": "Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.",
    "url": "https://arxiv.org/abs/2305.13169",
    "arxivId": "2305.13169",
    "last_visited": "2024-12-30T20:20:50.556Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-05-22T15:57:53Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2307.08691": {
    "id": "2307.08691",
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work   Partitioning",
    "authors": "Tri Dao",
    "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
    "url": "https://arxiv.org/abs/2307.08691",
    "arxivId": "2307.08691",
    "last_visited": "2024-12-28T06:14:32.268Z",
    "last_read": "2025-01-04T15:02:28.119002",
    "total_reading_time_seconds": 10,
    "published_date": "2023-07-17T17:50:36Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2307.09288": {
    "id": "2307.09288",
    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "authors": "Hugo Touvron, Louis Martin, Kevin Stone and 65 others",
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
    "url": "https://arxiv.org/abs/2307.09288",
    "arxivId": "2307.09288",
    "last_visited": "2024-12-29T10:07:22.174Z",
    "last_read": "2025-01-04T14:49:21.229139",
    "total_reading_time_seconds": 26,
    "published_date": "2023-07-18T14:31:57Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2307.12868": {
    "id": "2307.12868",
    "title": "Understanding the Latent Space of Diffusion Models through the Lens of   Riemannian Geometry",
    "authors": "Yong-Hyun Park, Mingi Kwon, Jaewoong Choi and 2 others",
    "abstract": "Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\\mathbf{x}_t \\in \\mathcal{X}$, we analyze them from a geometrical perspective. Our approach involves deriving the local latent basis within $\\mathcal{X}$ by leveraging the pullback metric associated with their encoding feature maps. Remarkably, our discovered local latent basis enables image editing capabilities by moving $\\mathbf{x}_t$, the latent space of DMs, along the basis vector at specific timesteps. We further analyze how the geometric structure of DMs evolves over diffusion timesteps and differs across different text conditions. This confirms the known phenomenon of coarse-to-fine generation, as well as reveals novel insights such as the discrepancy between $\\mathbf{x}_t$ across timesteps, the effect of dataset complexity, and the time-varying influence of text prompts. To the best of our knowledge, this paper is the first to present image editing through $\\mathbf{x}$-space traversal, editing only once at specific timestep $t$ without any additional training, and providing thorough analyses of the latent structure of DMs. The code to reproduce our experiments can be found at https://github.com/enkeejunior1/Diffusion-Pullback.",
    "url": "https://arxiv.org/abs/2307.12868",
    "arxivId": "2307.12868",
    "last_visited": "2025-01-10T20:55:12.357Z",
    "last_read": "2025-01-10T20:56:46.517220",
    "total_reading_time_seconds": 18,
    "published_date": "2023-07-24T15:06:42Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2308.06259": {
    "id": "2308.06259",
    "title": "Self-Alignment with Instruction Backtranslation",
    "authors": "Xian Li, Ping Yu, Chunting Zhou and 5 others",
    "abstract": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
    "url": "https://arxiv.org/abs/2308.06259",
    "arxivId": "2308.06259",
    "last_visited": "2024-12-30T04:57:10.471Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-08-11T17:47:54Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2308.10718": {
    "id": "2308.10718",
    "title": "Backdooring Textual Inversion for Concept Censorship",
    "authors": "Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang",
    "abstract": "Recent years have witnessed success in AIGC (AI Generated Content). People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language. More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references. However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations. Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement.   In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance. TI crafts the word embedding that contains detailed information about a specific object. Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization. To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings. Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use. In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept.   To demonstrate the effectiveness of our approach, we conduct extensive experiments on Stable Diffusion, a prevailing open-sourced text-to-image model. Our code, data, and results are available at https://concept-censorship.github.io.",
    "url": "https://arxiv.org/abs/2308.10718",
    "arxivId": "2308.10718",
    "last_visited": "2024-12-22T16:47:15.076Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-08-21T13:39:04Z",
    "arxiv_tags": [
      "cs.CR",
      "cs.CV"
    ]
  },
  "2308.10792": {
    "id": "2308.10792",
    "title": "Instruction Tuning for Large Language Models: A Survey",
    "authors": "Shengyu Zhang, Linfeng Dong, Xiaoya Li and 8 others",
    "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
    "url": "https://arxiv.org/abs/2308.10792",
    "arxivId": "2308.10792",
    "last_visited": "2025-01-06T23:25:42.652000+00:00",
    "last_read": "2025-01-06T23:26:59.522369",
    "total_reading_time_seconds": 20,
    "published_date": "2023-08-21T15:35:16Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2308.13561": {
    "id": "2308.13561",
    "title": "Project Aria: A New Tool for Egocentric Multi-Modal AI Research",
    "authors": "Jakob Engel, Kiran Somasundaram, Michael Goesele and 71 others",
    "abstract": "Egocentric, multi-modal data as available on future augmented reality (AR) devices provides unique challenges and opportunities for machine perception. These future devices will need to be all-day wearable in a socially acceptable form-factor to support always available, context-aware and personalized AI applications. Our team at Meta Reality Labs Research built the Aria device, an egocentric, multi-modal data recording and streaming device with the goal to foster and accelerate research in this area. In this paper, we describe the Aria device hardware including its sensor configuration and the corresponding software tools that enable recording and processing of such data.",
    "url": "https://arxiv.org/abs/2308.13561",
    "arxivId": "2308.13561",
    "last_visited": "2025-01-10T06:30:22.302Z",
    "last_read": "2025-01-10T06:32:03.790467",
    "total_reading_time_seconds": 19,
    "published_date": "2023-08-24T20:42:21Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.CV"
    ]
  },
  "2309.06180": {
    "id": "2309.06180",
    "title": "Efficient Memory Management for Large Language Model Serving with   PagedAttention",
    "authors": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang and 6 others",
    "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm",
    "url": "https://arxiv.org/abs/2309.06180",
    "arxivId": "2309.06180",
    "last_visited": "2025-01-15T19:11:27.895Z",
    "last_read": "2025-01-15T19:12:38.377163",
    "total_reading_time_seconds": 11,
    "published_date": "2023-09-12T12:50:04Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.DC"
    ]
  },
  "2309.07965": {
    "id": "2309.07965",
    "title": "A survey on relative Lipschitz saturation of algebras and its relation   with radicial algebras",
    "authors": "Thiago da Silva, Guilherme Schultz Netto",
    "abstract": "In this work, we introduce the concept of relative Lipschitz saturation, along with its key categorical and algebraic properties, and demonstrate how such a structure always gives rise to a radicial algebra.",
    "url": "https://arxiv.org/abs/2309.07965",
    "arxivId": "2309.07965",
    "last_visited": "2024-12-28T07:12:00.313000+00:00",
    "last_read": "2025-01-04T15:02:13.219648",
    "total_reading_time_seconds": 6,
    "published_date": "2023-09-14T18:02:12Z",
    "arxiv_tags": [
      "math.AC",
      "13B22"
    ]
  },
  "2309.12032": {
    "id": "2309.12032",
    "title": "Human-in-the-Loop Causal Discovery under Latent Confounding using   Ancestral GFlowNets",
    "authors": "Tiago da Silva, Eliezer Silva, António Góis and 4 others",
    "abstract": "Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid.",
    "url": "https://arxiv.org/abs/2309.12032",
    "arxivId": "2309.12032",
    "last_visited": "2024-12-28T06:14:26.797Z",
    "last_read": "2025-01-04T15:02:28.120081",
    "total_reading_time_seconds": 12,
    "published_date": "2023-09-21T12:53:45Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ]
  },
  "2309.14556": {
    "id": "2309.14556",
    "title": "Art or Artifice? Large Language Models and the False Promise of   Creativity",
    "authors": "Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal and 2 others",
    "abstract": "Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.",
    "url": "https://arxiv.org/abs/2309.14556",
    "arxivId": "2309.14556",
    "last_visited": "2024-12-30T14:42:55.576Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-09-25T22:02:46Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ]
  },
  "2310.01889": {
    "id": "2310.01889",
    "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
    "authors": "Hao Liu, Matei Zaharia, Pieter Abbeel",
    "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
    "url": "https://arxiv.org/abs/2310.01889",
    "arxivId": "2310.01889",
    "last_visited": "2025-01-02T07:40:21.565Z",
    "last_read": "2025-01-04T06:52:27.608961",
    "total_reading_time_seconds": 40,
    "published_date": "2023-10-03T08:44:50Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2310.16410": {
    "id": "2310.16410",
    "title": "Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in   AlphaZero",
    "authors": "Lisa Schut, Nenad Tomasev, Tom McGrath and 3 others",
    "abstract": "Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.",
    "url": "https://arxiv.org/abs/2310.16410",
    "arxivId": "2310.16410",
    "last_visited": "2025-01-17T18:09:38.173Z",
    "last_read": "2025-01-17T18:10:52.310417",
    "total_reading_time_seconds": 56,
    "published_date": "2023-10-25T06:49:26Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ]
  },
  "2310.17157": {
    "id": "2310.17157",
    "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
    "authors": "Zichang Liu, Jue Wang, Tri Dao and 8 others",
    "abstract": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.",
    "url": "https://arxiv.org/abs/2310.17157",
    "arxivId": "2310.17157",
    "last_visited": "2024-12-28T06:14:06.334Z",
    "last_read": "2025-01-04T15:02:30.861843",
    "total_reading_time_seconds": 19,
    "published_date": "2023-10-26T05:01:09Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2312.00330": {
    "id": "2312.00330",
    "title": "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style   Adapter",
    "authors": "Gongye Liu, Menghan Xia, Yong Zhang and 6 others",
    "abstract": "Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image. Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy. Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors.",
    "url": "https://arxiv.org/abs/2312.00330",
    "arxivId": "2312.00330",
    "last_visited": "2024-12-15T20:33:49.865Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-01T03:53:21Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI"
    ]
  },
  "2312.00752": {
    "id": "2312.00752",
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "authors": "Albert Gu, Tri Dao",
    "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
    "url": "https://arxiv.org/abs/2312.00752",
    "arxivId": "2312.00752",
    "last_visited": "2024-12-28T06:13:33.669Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-01T18:01:34Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2312.07731": {
    "id": "2312.07731",
    "title": "A Response to Glaze Purification via IMPRESS",
    "authors": "Shawn Shan, Stanley Wu, Haitao Zheng, Ben Y. Zhao",
    "abstract": "Recent work proposed a new mechanism to remove protective perturbation added by Glaze in order to again enable mimicry of art styles from images protected by Glaze. Despite promising results shown in the original paper, our own tests with the authors' code demonstrated several limitations of the proposed purification approach. The main limitations are 1) purification has a limited effect when tested on artists that are not well-known historical artists already embedded in original training data, 2) problems in evaluation metrics, and 3) collateral damage on mimicry result for clean images. We believe these limitations should be carefully considered in order to understand real world usability of the purification attack.",
    "url": "https://arxiv.org/abs/2312.07731",
    "arxivId": "2312.07731",
    "last_visited": "2024-12-22T16:49:35.138Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-12T20:52:27Z",
    "arxiv_tags": [
      "cs.CR"
    ]
  },
  "2312.17127": {
    "id": "2312.17127",
    "title": "Probabilistic programming interfaces for random graphs: Markov   categories, graphons, and nominal sets",
    "authors": "Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar and 5 others",
    "abstract": "We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.   We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi graphons. In this way, we build new models of graph probabilistic programming from graphons.",
    "url": "https://arxiv.org/abs/2312.17127",
    "arxivId": "2312.17127",
    "last_visited": "2024-12-29T19:53:34.293000+00:00",
    "last_read": "2025-01-04T14:49:15.404627",
    "total_reading_time_seconds": 10,
    "published_date": "2023-12-28T17:04:50Z",
    "arxiv_tags": [
      "cs.PL",
      "cs.LO",
      "math.PR"
    ]
  },
  "2401.00649": {
    "id": "2401.00649",
    "title": "Linear Model and Extensions",
    "authors": "Peng Ding",
    "abstract": "I developed the lecture notes based on my ``Linear Model'' course at the University of California Berkeley over the past seven years. This book provides an intermediate-level introduction to the linear model. It balances rigorous proofs and heuristic arguments. This book provides R code to replicate all simulation studies and case studies.",
    "url": "https://arxiv.org/abs/2401.00649",
    "arxivId": "2401.00649",
    "last_visited": "2025-01-14T15:24:20.284000+00:00",
    "last_read": "2025-01-14T15:20:47.368469",
    "total_reading_time_seconds": 308,
    "published_date": "2024-01-01T03:34:17Z",
    "arxiv_tags": [
      "stat.ME",
      "stat.AP"
    ]
  },
  "2401.10774": {
    "id": "2401.10774",
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple   Decoding Heads",
    "authors": "Tianle Cai, Yuhong Li, Zhengyang Geng and 4 others",
    "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.",
    "url": "https://arxiv.org/abs/2401.10774",
    "arxivId": "2401.10774",
    "last_visited": "2024-12-28T07:11:26.727Z",
    "last_read": "2025-01-04T15:02:15.846437",
    "total_reading_time_seconds": 20,
    "published_date": "2024-01-19T15:48:40Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2402.03239": {
    "id": "2402.03239",
    "title": "Bluesky and the AT Protocol: Usable Decentralized Social Media",
    "authors": "Martin Kleppmann, Paul Frazee, Jake Gold and 6 others",
    "abstract": "Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 10 million registered users by October 2024. In this paper we introduce the architecture of Bluesky and the AT Protocol, and explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation.",
    "url": "https://arxiv.org/abs/2402.03239",
    "arxivId": "2402.03239",
    "last_visited": "2024-12-22T05:41:38.653Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-05T17:55:51Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.SI"
    ]
  },
  "2402.09949": {
    "id": "2402.09949",
    "title": "Multi-word Tokenization for Sequence Compression",
    "authors": "Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini",
    "abstract": "Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.",
    "url": "https://arxiv.org/abs/2402.09949",
    "arxivId": "2402.09949",
    "last_visited": "2025-01-10T01:40:44.674Z",
    "last_read": "2025-01-10T01:42:15.861971",
    "total_reading_time_seconds": 14,
    "published_date": "2024-02-15T13:52:23Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2402.09977": {
    "id": "2402.09977",
    "title": "Fast Vocabulary Transfer for Language Model Compression",
    "authors": "Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni",
    "abstract": "Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.",
    "url": "https://arxiv.org/abs/2402.09977",
    "arxivId": "2402.09977",
    "last_visited": "2025-01-10T02:00:48.166000+00:00",
    "last_read": "2025-01-10T02:01:59.117108",
    "total_reading_time_seconds": 65,
    "published_date": "2024-02-15T14:37:07Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2402.10193": {
    "id": "2402.10193",
    "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
    "authors": "James Liu, Guangxuan Xiao, Kai Li and 4 others",
    "abstract": "Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.",
    "url": "https://arxiv.org/abs/2402.10193",
    "arxivId": "2402.10193",
    "last_visited": "2024-12-28T06:12:55.272Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-15T18:50:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2402.16670": {
    "id": "2402.16670",
    "title": "Pay Attention: a Call to Regulate the Attention Market and Prevent   Algorithmic Emotional Governance",
    "authors": "Franck Michel, Fabien Gandon",
    "abstract": "Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.",
    "url": "https://arxiv.org/abs/2402.16670",
    "arxivId": "2402.16670",
    "last_visited": "2024-12-27T22:07:23.174Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-26T15:46:43Z",
    "arxiv_tags": [
      "cs.SI"
    ]
  },
  "2402.19173": {
    "id": "2402.19173",
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "authors": "Anton Lozhkov, Raymond Li, Loubna Ben Allal and 63 others",
    "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
    "url": "https://arxiv.org/abs/2402.19173",
    "arxivId": "2402.19173",
    "last_visited": "2024-12-28T06:12:42.484Z",
    "last_read": "2025-01-04T15:02:39.855254",
    "total_reading_time_seconds": 11,
    "published_date": "2024-02-29T13:53:35Z",
    "arxiv_tags": [
      "cs.SE",
      "cs.AI"
    ]
  },
  "2403.08540": {
    "id": "2403.08540",
    "title": "Language models scale reliably with over-training and on downstream   tasks",
    "authors": "Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar and 22 others",
    "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
    "url": "https://arxiv.org/abs/2403.08540",
    "arxivId": "2403.08540",
    "last_visited": "2024-12-30T19:37:31.972Z",
    "last_read": "2025-01-04T06:53:18.648664",
    "total_reading_time_seconds": 22,
    "published_date": "2024-03-13T13:54:00Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2403.10304": {
    "id": "2403.10304",
    "title": "KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge   Sources",
    "authors": "Guilherme Lima, João M. B. Rodrigues, Marcelo Machado and 6 others",
    "abstract": "We present a Wikidata-based framework, called KIF, for virtually integrating heterogeneous knowledge sources. KIF is written in Python and is released as open-source. It leverages Wikidata's data model and vocabulary plus user-defined mappings to construct a unified view of the underlying sources while keeping track of the context and provenance of their statements. The underlying sources can be triplestores, relational databases, CSV files, etc., which may or may not use the vocabulary and RDF encoding of Wikidata. The end result is a virtual knowledge base which behaves like an \"extended Wikidata\" and which can be queried using a simple but expressive pattern language, defined in terms of Wikidata's data model. In this paper, we present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF",
    "url": "https://arxiv.org/abs/2403.10304",
    "arxivId": "2403.10304",
    "last_visited": "2024-12-28T06:12:17.716Z",
    "last_read": "2025-01-04T15:02:42.885145",
    "total_reading_time_seconds": 24,
    "published_date": "2024-03-15T13:46:36Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.DB"
    ]
  },
  "2403.14554": {
    "id": "2403.14554",
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering",
    "authors": "Antoine Guédon, Vincent Lepetit",
    "abstract": "We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: https://anttwo.github.io/frosting/",
    "url": "https://arxiv.org/abs/2403.14554",
    "arxivId": "2403.14554",
    "last_visited": "2025-01-03T02:52:11.974Z",
    "last_read": "2025-01-04T06:52:06.631879",
    "total_reading_time_seconds": 3,
    "published_date": "2024-03-21T16:53:03Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR"
    ]
  },
  "2404.13320": {
    "id": "2404.13320",
    "title": "Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than   We Think",
    "authors": "Haotian Xue, Yongxin Chen",
    "abstract": "Adversarial examples for diffusion models are widely used as solutions for safety concerns. By adding adversarial perturbations to personal images, attackers can not edit or imitate them easily. However, it is essential to note that all these protections target the latent diffusion model (LDMs), the adversarial examples for diffusion models in the pixel space (PDMs) are largely overlooked. This may mislead us to think that the diffusion models are vulnerable to adversarial attacks like most deep models. In this paper, we show novel findings that: even though gradient-based white-box attacks can be used to attack the LDMs, they fail to attack PDMs. This finding is supported by extensive experiments of almost a wide range of attacking methods on various PDMs and LDMs with different model structures, which means diffusion models are indeed much more robust against adversarial attacks. We also find that PDMs can be used as an off-the-shelf purifier to effectively remove the adversarial patterns that were generated on LDMs to protect the images, which means that most protection methods nowadays, to some extent, cannot protect our images from malicious attacks. We hope that our insights will inspire the community to rethink the adversarial samples for diffusion models as protection methods and move forward to more effective protection. Codes are available in https://github.com/xavihart/PDM-Pure.",
    "url": "https://arxiv.org/abs/2404.13320",
    "arxivId": "2404.13320",
    "last_visited": "2024-12-22T16:33:17.796Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-04-20T08:28:43Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI"
    ]
  },
  "2404.16698": {
    "id": "2404.16698",
    "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society   of LLM Agents",
    "authors": "Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner and 3 others",
    "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
    "url": "https://arxiv.org/abs/2404.16698",
    "arxivId": "2404.16698",
    "last_visited": "2025-01-05T20:11:03.137000+00:00",
    "last_read": "2025-01-05T20:12:12.476203",
    "total_reading_time_seconds": 24,
    "published_date": "2024-04-25T15:59:16Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2405.04434": {
    "id": "2405.04434",
    "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts   Language Model",
    "authors": "DeepSeek-AI, Aixin Liu, Bei Feng and 154 others",
    "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
    "url": "https://arxiv.org/abs/2405.04434",
    "arxivId": "2405.04434",
    "last_visited": "2024-12-26T13:01:17.331Z",
    "last_read": "2025-01-04T15:03:12.861833",
    "total_reading_time_seconds": 46,
    "published_date": "2024-05-07T15:56:43Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2405.04517": {
    "id": "2405.04517",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "authors": "Maximilian Beck, Korbinian Pöppel, Markus Spanring and 6 others",
    "abstract": "In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.",
    "url": "https://arxiv.org/abs/2405.04517",
    "arxivId": "2405.04517",
    "last_visited": "2024-12-30T20:10:41.007000+00:00",
    "last_read": "2025-01-04T06:53:15.667071",
    "total_reading_time_seconds": 31,
    "published_date": "2024-05-07T17:50:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  "2405.06865": {
    "id": "2405.06865",
    "title": "Disrupting Style Mimicry Attacks on Video Imagery",
    "authors": "Josephine Passananti, Stanley Wu, Shawn Shan and 2 others",
    "abstract": "Generative AI models are often used to perform mimicry attacks, where a pretrained model is fine-tuned on a small sample of images to learn to mimic a specific artist of interest. While researchers have introduced multiple anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence points to a growing trend of mimicry models using videos as sources of training data. This paper presents our experiences exploring techniques to disrupt style mimicry on video imagery. We first validate that mimicry attacks can succeed by training on individual frames extracted from videos. We show that while anti-mimicry tools can offer protection when applied to individual frames, this approach is vulnerable to an adaptive countermeasure that removes protection by exploiting randomness in optimization results of consecutive (nearly-identical) frames. We develop a new, tool-agnostic framework that segments videos into short scenes based on frame-level similarity, and use a per-scene optimization baseline to remove inter-frame randomization while reducing computational cost. We show via both image level metrics and an end-to-end user study that the resulting protection restores protection against mimicry (including the countermeasure). Finally, we develop another adaptive countermeasure and find that it falls short against our framework.",
    "url": "https://arxiv.org/abs/2405.06865",
    "arxivId": "2405.06865",
    "last_visited": "2024-12-22T16:37:12.908Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-11T01:40:19Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.CR"
    ]
  },
  "2405.07883": {
    "id": "2405.07883",
    "title": "Zero-Shot Tokenizer Transfer",
    "authors": "Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vulić",
    "abstract": "Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.",
    "url": "https://arxiv.org/abs/2405.07883",
    "arxivId": "2405.07883",
    "last_visited": "2025-01-10T02:01:14.129000+00:00",
    "last_read": "2025-01-10T02:01:59.116135",
    "total_reading_time_seconds": 15,
    "published_date": "2024-05-13T16:17:10Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2405.12399": {
    "id": "2405.12399",
    "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
    "authors": "Eloi Alonso, Adam Jelley, Vincent Micheli and 4 others",
    "abstract": "World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. We further demonstrate that DIAMOND's diffusion world model can stand alone as an interactive neural game engine by training on static Counter-Strike: Global Offensive gameplay. To foster future research on diffusion for world modeling, we release our code, agents, videos and playable world models at https://diamond-wm.github.io.",
    "url": "https://arxiv.org/abs/2405.12399",
    "arxivId": "2405.12399",
    "last_visited": "2024-12-26T22:06:50.951Z",
    "last_read": "2025-01-04T15:03:00.857912",
    "total_reading_time_seconds": 12,
    "published_date": "2024-05-20T22:51:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2405.16567": {
    "id": "2405.16567",
    "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
    "authors": "Minseon Kim, Hyomin Lee, Boqing Gong and 2 others",
    "abstract": "Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 12% and 17% of the attacks with naive prompts, respectively, while ChatGPT blocks 84% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0% block rate, making it generate copyrighted contents in 76% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.",
    "url": "https://arxiv.org/abs/2405.16567",
    "arxivId": "2405.16567",
    "last_visited": "2024-12-22T16:31:57.733Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-26T13:32:24Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CR"
    ]
  },
  "2405.17472": {
    "id": "2405.17472",
    "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via   Selective Tensor Freezing",
    "authors": "Kai Huang, Haoming Wang, Wei Gao",
    "abstract": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations. The source code is available at: https://github.com/pittisl/FreezeAsGuard.",
    "url": "https://arxiv.org/abs/2405.17472",
    "arxivId": "2405.17472",
    "last_visited": "2024-12-22T18:25:59.099Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-24T03:23:51Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ]
  },
  "2405.20053": {
    "id": "2405.20053",
    "title": "Would I Lie To You? Inference Time Alignment of Language Models using   Direct Preference Heads",
    "authors": "Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic",
    "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.",
    "url": "https://arxiv.org/abs/2405.20053",
    "arxivId": "2405.20053",
    "last_visited": "2024-12-15T17:25:17.781Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-30T13:38:52Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2405.21060": {
    "id": "2405.21060",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms   Through Structured State Space Duality",
    "authors": "Tri Dao, Albert Gu",
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "url": "https://arxiv.org/abs/2405.21060",
    "arxivId": "2405.21060",
    "last_visited": "2024-12-28T06:11:12.620Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-31T17:50:01Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2406.01981": {
    "id": "2406.01981",
    "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
    "authors": "Yury Tokpanov, Beren Millidge, Paolo Glorioso and 4 others",
    "abstract": "The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.",
    "url": "https://arxiv.org/abs/2406.01981",
    "arxivId": "2406.01981",
    "last_visited": "2024-12-30T20:04:46.858Z",
    "last_read": "2025-01-04T06:53:18.650339",
    "total_reading_time_seconds": 8,
    "published_date": "2024-06-04T05:47:17Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2406.06158": {
    "id": "2406.06158",
    "title": "Get rich quick: exact solutions reveal how unbalanced initializations   promote rapid feature learning",
    "authors": "Daniel Kunin, Allan Raventós, Clémentine Dominé and 4 others",
    "abstract": "While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this rich feature learning regime remain elusive, with much of our theoretical understanding stemming from the opposing lazy regime. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs from balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.",
    "url": "https://arxiv.org/abs/2406.06158",
    "arxivId": "2406.06158",
    "last_visited": "2024-12-22T07:10:41.441Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-06-10T10:42:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  "2406.07522": {
    "id": "2406.07522",
    "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context   Language Modeling",
    "authors": "Liliang Ren, Yang Liu, Yadong Lu and 3 others",
    "abstract": "Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall recent memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and demonstrate that it significantly outperforms state-of-the-art models across a variety of benchmarks. Pretrained on sequences of 4K length, Samba shows improved perplexity in context lengths of up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits superior retrieval extrapolation on the challenging Phonebook task compared to full-attention models. As a linear-time sequence model, Samba achieves a 3.73x higher throughput compared to Transformers with grouped-query attention for user prompts of 128K length, and a 3.64x speedup when generating 64K tokens with unlimited streaming. Our code for training on open source data is publicly available at https://github.com/microsoft/Samba.",
    "url": "https://arxiv.org/abs/2406.07522",
    "arxivId": "2406.07522",
    "last_visited": "2025-01-17T05:35:26.692000+00:00",
    "last_read": "2025-01-17T05:37:04.997640",
    "total_reading_time_seconds": 12,
    "published_date": "2024-06-11T17:50:51Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2406.09162": {
    "id": "2406.09162",
    "title": "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal   Prompts",
    "authors": "Yucheng Han, Rui Wang, Chi Zhang and 4 others",
    "abstract": "Recent advancements in image generation have enabled the creation of high-quality images from text conditions. However, when facing multi-modal conditions, such as text combined with reference appearances, existing methods struggle to balance multiple conditions effectively, typically showing a preference for one modality over others. To address this challenge, we introduce EMMA, a novel image generation model accepting multi-modal prompts built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA seamlessly incorporates additional modalities alongside text to guide image generation through an innovative Multi-modal Feature Connector design, which effectively integrates textual and supplementary modal information using a special attention mechanism. By freezing all parameters in the original T2I diffusion model and only adjusting some additional layers, we reveal an interesting finding that the pre-trained T2I diffusion model can secretly accept multi-modal prompts. This interesting property facilitates easy adaptation to different existing frameworks, making EMMA a flexible and effective tool for producing personalized and context-aware images and even videos. Additionally, we introduce a strategy to assemble learned EMMA modules to produce images conditioned on multiple modalities simultaneously, eliminating the need for additional training with mixed multi-modal prompts. Extensive experiments demonstrate the effectiveness of EMMA in maintaining high fidelity and detail in generated images, showcasing its potential as a robust solution for advanced multi-modal conditional image generation tasks.",
    "url": "https://arxiv.org/abs/2406.09162",
    "arxivId": "2406.09162",
    "last_visited": "2024-12-28T08:40:07.461Z",
    "last_read": "2025-01-04T14:49:45.266317",
    "total_reading_time_seconds": 24,
    "published_date": "2024-06-13T14:26:43Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2406.10670": {
    "id": "2406.10670",
    "title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language   Model Pre-training",
    "authors": "David Brandfonbrener, Hanlin Zhang, Andreas Kirsch and 2 others",
    "abstract": "Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.   In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.   Code: https://github.com/davidbrandfonbrener/color-filter-olmo   Filtered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4",
    "url": "https://arxiv.org/abs/2406.10670",
    "arxivId": "2406.10670",
    "last_visited": "2024-12-30T20:02:17.082Z",
    "last_read": "2025-01-04T06:53:27.615183",
    "total_reading_time_seconds": 7,
    "published_date": "2024-06-15T15:28:02Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2406.12027": {
    "id": "2406.12027",
    "title": "Adversarial Perturbations Cannot Reliably Protect Artists From   Generative AI",
    "authors": "Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr",
    "abstract": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections -- with millions of downloads -- and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that all existing protections can be easily bypassed, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative non-technological solutions.",
    "url": "https://arxiv.org/abs/2406.12027",
    "arxivId": "2406.12027",
    "last_visited": "2024-12-22T16:38:53.342Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-06-17T18:51:45Z",
    "arxiv_tags": [
      "cs.CR"
    ]
  },
  "2407.01392": {
    "id": "2407.01392",
    "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
    "authors": "Boyuan Chen, Diego Marti Monso, Yilun Du and 3 others",
    "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
    "url": "https://arxiv.org/abs/2407.01392",
    "arxivId": "2407.01392",
    "last_visited": "2024-12-26T22:05:31.374Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-01T15:43:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "cs.RO"
    ]
  },
  "2407.01492": {
    "id": "2407.01492",
    "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
    "authors": "Qian Liu, Xiaosen Zheng, Niklas Muennighoff and 5 others",
    "abstract": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at https://github.com/sail-sg/regmix.",
    "url": "https://arxiv.org/abs/2407.01492",
    "arxivId": "2407.01492",
    "last_visited": "2024-12-30T20:17:16.629000+00:00",
    "last_read": "2025-01-04T06:53:09.611828",
    "total_reading_time_seconds": 42,
    "published_date": "2024-07-01T17:31:03Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2407.05872": {
    "id": "2407.05872",
    "title": "Scaling Exponents Across Parameterizations and Optimizers",
    "authors": "Katie Everett, Lechao Xiao, Mitchell Wortsman and 8 others",
    "abstract": "Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes tens of thousands of models trained with all combinations of three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 26.8B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose Adam-atan2, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely.",
    "url": "https://arxiv.org/abs/2407.05872",
    "arxivId": "2407.05872",
    "last_visited": "2024-12-25T10:23:47.480000+00:00",
    "last_read": "2025-01-04T15:03:18.849005",
    "total_reading_time_seconds": 4,
    "published_date": "2024-07-08T12:32:51Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2407.08608": {
    "id": "2407.08608",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision",
    "authors": "Jay Shah, Ganesh Bikshandi, Ying Zhang and 3 others",
    "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention.",
    "url": "https://arxiv.org/abs/2407.08608",
    "arxivId": "2407.08608",
    "last_visited": "2024-12-28T06:10:45.698Z",
    "last_read": "2025-01-04T15:02:45.957021",
    "total_reading_time_seconds": 39,
    "published_date": "2024-07-11T15:44:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2407.21783": {
    "id": "2407.21783",
    "title": "The Llama 3 Herd of Models",
    "authors": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri and 558 others",
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "url": "https://arxiv.org/abs/2407.21783",
    "arxivId": "2407.21783",
    "last_visited": "2024-12-30T21:58:38.440Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-31T17:54:27Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  "2407.21787": {
    "id": "2407.21787",
    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
    "authors": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich and 4 others",
    "abstract": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
    "url": "https://arxiv.org/abs/2407.21787",
    "arxivId": "2407.21787",
    "last_visited": "2024-12-29T20:31:06.783Z",
    "last_read": "2025-01-04T14:49:12.237648",
    "total_reading_time_seconds": 17,
    "published_date": "2024-07-31T17:57:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2408.03314": {
    "id": "2408.03314",
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than   Scaling Model Parameters",
    "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
    "url": "https://arxiv.org/abs/2408.03314",
    "arxivId": "2408.03314",
    "last_visited": "2024-12-17T13:48:02.489Z",
    "last_read": "2025-01-04T14:49:12.236101",
    "total_reading_time_seconds": 39,
    "published_date": "2024-08-06T17:35:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2408.06072": {
    "id": "2408.06072",
    "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
    "authors": "Zhuoyi Yang, Jiayan Teng, Wendi Zheng and 16 others",
    "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.",
    "url": "https://arxiv.org/abs/2408.06072",
    "arxivId": "2408.06072",
    "last_visited": "2024-12-30T15:18:13.864Z",
    "last_read": "2025-01-04T06:53:30.624413",
    "total_reading_time_seconds": 26,
    "published_date": "2024-08-12T11:47:11Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2408.11810": {
    "id": "2408.11810",
    "title": "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain   Diffusion Models",
    "authors": "Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao and 3 others",
    "abstract": "Diffusion Models have emerged as powerful generative models for high-quality image synthesis, with many subsequent image editing techniques based on them. However, the ease of text-based image editing introduces significant risks, such as malicious editing for scams or intellectual property infringement. Previous works have attempted to safeguard images from diffusion-based editing by adding imperceptible perturbations. These methods are costly and specifically target prevalent Latent Diffusion Models (LDMs), while Pixel-domain Diffusion Models (PDMs) remain largely unexplored and robust against such attacks. Our work addresses this gap by proposing a novel attacking framework with a feature representation attack loss that exploits vulnerabilities in denoising UNets and a latent optimization strategy to enhance the naturalness of protected images. Extensive experiments demonstrate the effectiveness of our approach in attacking dominant PDM-based editing methods (e.g., SDEdit) while maintaining reasonable protection fidelity and robustness against common defense methods. Additionally, our framework is extensible to LDMs, achieving comparable performance to existing approaches.",
    "url": "https://arxiv.org/abs/2408.11810",
    "arxivId": "2408.11810",
    "last_visited": "2024-12-22T16:45:08.960Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-08-21T17:56:34Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2408.14837": {
    "id": "2408.14837",
    "title": "Diffusion Models Are Real-Time Game Engines",
    "authors": "Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter",
    "abstract": "We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.",
    "url": "https://arxiv.org/abs/2408.14837",
    "arxivId": "2408.14837",
    "last_visited": "2024-12-26T21:43:51.081Z",
    "last_read": "2025-01-04T15:03:06.880492",
    "total_reading_time_seconds": 14,
    "published_date": "2024-08-27T07:46:07Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2409.04431": {
    "id": "2409.04431",
    "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
    "authors": "Jason Ramapuram, Federico Danieli, Eeshan Dhekane and 8 others",
    "abstract": "Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.",
    "url": "https://arxiv.org/abs/2409.04431",
    "arxivId": "2409.04431",
    "last_visited": "2025-01-02T19:38:28.512000+00:00",
    "last_read": "2025-01-04T06:52:15.620261",
    "total_reading_time_seconds": 8,
    "published_date": "2024-09-06T17:53:26Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2409.04580": {
    "id": "2409.04580",
    "title": "GRB 221009A: the B.O.A.T Burst that Shines in Gamma Rays",
    "authors": "M. Axelsson, M. Ajello, M. Arimoto and 151 others",
    "abstract": "We present a complete analysis of Fermi Large Area Telescope (LAT) data of GRB 221009A, the brightest Gamma-Ray Burst (GRB) ever detected. The burst emission above 30 MeV detected by the LAT preceded by 1 s the low-energy (< 10 MeV) pulse that triggered the Fermi Gamma-Ray Burst Monitor (GBM), as has been observed in other GRBs. The prompt phase of GRB 221009A lasted a few hundred seconds. It was so bright that we identify a Bad Time Interval (BTI) of 64 seconds caused by the extremely high flux of hard X-rays and soft gamma rays, during which the event reconstruction efficiency was poor and the dead time fraction quite high. The late-time emission decayed as a power law, but the extrapolation of the late-time emission during the first 450 seconds suggests that the afterglow started during the prompt emission. We also found that high-energy events observed by the LAT are incompatible with synchrotron origin, and, during the prompt emission, are more likely related to an extra component identified as synchrotron self-Compton (SSC). A remarkable 400 GeV photon, detected by the LAT 33 ks after the GBM trigger and directionally consistent with the location of GRB 221009A, is hard to explain as a product of SSC or TeV electromagnetic cascades, and the process responsible for its origin is uncertain. Because of its proximity and energetic nature, GRB 221009A is an extremely rare event.",
    "url": "https://arxiv.org/abs/2409.04580",
    "arxivId": "2409.04580",
    "last_visited": "2025-01-13T06:26:26.740000+00:00",
    "last_read": "2025-01-13T06:27:58.445781",
    "total_reading_time_seconds": 25,
    "published_date": "2024-09-06T19:42:28Z",
    "arxiv_tags": [
      "astro-ph.HE"
    ]
  },
  "2409.05816": {
    "id": "2409.05816",
    "title": "Improving Pretraining Data Using Perplexity Correlations",
    "authors": "Tristan Thrush, Christopher Potts, Tatsunori Hashimoto",
    "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.",
    "url": "https://arxiv.org/abs/2409.05816",
    "arxivId": "2409.05816",
    "last_visited": "2024-12-30T20:20:51.220Z",
    "last_read": "2025-01-04T06:53:00.604590",
    "total_reading_time_seconds": 3,
    "published_date": "2024-09-09T17:23:29Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ]
  },
  "2409.08514": {
    "id": "2409.08514",
    "title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration",
    "authors": "Kai Li, Yi Luo",
    "abstract": "Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at https://github.com/JusperLee/Apollo.",
    "url": "https://arxiv.org/abs/2409.08514",
    "arxivId": "2409.08514",
    "last_visited": "2024-12-30T05:02:38.295Z",
    "last_read": "2025-01-04T14:48:42.234855",
    "total_reading_time_seconds": 4,
    "published_date": "2024-09-13T03:25:34Z",
    "arxiv_tags": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ]
  },
  "2409.08861": {
    "id": "2409.08861",
    "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with   Memoryless Stochastic Optimal Control",
    "authors": "Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen",
    "abstract": "Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.",
    "url": "https://arxiv.org/abs/2409.08861",
    "arxivId": "2409.08861",
    "last_visited": "2025-01-13T06:50:13.853000+00:00",
    "last_read": "2025-01-13T06:52:55.474406",
    "total_reading_time_seconds": 21,
    "published_date": "2024-09-13T14:22:14Z",
    "arxiv_tags": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ]
  },
  "2409.11321": {
    "id": "2409.11321",
    "title": "SOAP: Improving and Stabilizing Shampoo using Adam",
    "authors": "Nikhil Vyas, Depen Morwani, Rosie Zhao and 4 others",
    "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient approximation of Adam -- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: $\\textbf{S}$hampo$\\textbf{O}$ with $\\textbf{A}$dam in the $\\textbf{P}$reconditioner's eigenbasis (SOAP).   With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.",
    "url": "https://arxiv.org/abs/2409.11321",
    "arxivId": "2409.11321",
    "last_visited": "2025-01-05T18:45:46.230Z",
    "last_read": "2025-01-05T18:45:46.713842",
    "total_reading_time_seconds": 9,
    "published_date": "2024-09-17T16:18:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2409.13731": {
    "id": "2409.13731",
    "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented   Generation",
    "authors": "Lei Liang, Mengshu Sun, Zhengke Gui and 16 others",
    "abstract": "The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.",
    "url": "https://arxiv.org/abs/2409.13731",
    "arxivId": "2409.13731",
    "last_visited": "2025-01-02T15:07:25.587Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-10T02:00:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2409.16986": {
    "id": "2409.16986",
    "title": "Harnessing Diversity for Important Data Selection in Pretraining Large   Language Models",
    "authors": "Chi Zhang, Huaping Zhong, Kuan Zhang and 10 others",
    "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores. However, this approach has several limitations. (1) Computing the influence of all available data is time-consuming. (2) The selected data instances are not diverse enough, which may hinder the pre-trained model's ability to generalize effectively to various downstream tasks. In this paper, we introduce \\texttt{Quad}, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pre-training results. In particular, noting that attention layers capture extensive semantic details, we have adapted the accelerated $iHVP$ computation methods for attention layers, enhancing our ability to evaluate the influence of data, $i.e.,$ its quality. For the diversity, \\texttt{Quad} clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. To determine which clusters to select, we utilize the classic Multi-Armed Bandit method, treating each cluster as an arm. This approach favors clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.",
    "url": "https://arxiv.org/abs/2409.16986",
    "arxivId": "2409.16986",
    "last_visited": "2024-12-30T20:04:00.391Z",
    "last_read": "2025-01-04T06:53:18.651949",
    "total_reading_time_seconds": 20,
    "published_date": "2024-09-25T14:49:29Z",
    "arxiv_tags": [
      "cs.AI"
    ]
  },
  "2409.19256": {
    "id": "2409.19256",
    "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
    "authors": "Guangming Sheng, Chi Zhang, Zilingfeng Ye and 6 others",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53$\\times$~20.57$\\times$ throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at https://github.com/volcengine/verl.",
    "url": "https://arxiv.org/abs/2409.19256",
    "arxivId": "2409.19256",
    "last_visited": "2025-01-05T08:22:18.012Z",
    "last_read": "2025-01-05T08:23:20.687827",
    "total_reading_time_seconds": 14,
    "published_date": "2024-09-28T06:20:03Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.DC",
      "I.2"
    ]
  },
  "2409.19606": {
    "id": "2409.19606",
    "title": "Hyper-Connections",
    "authors": "Defa Zhu, Hongzhi Huang, Zihao Huang and 5 others",
    "abstract": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",
    "url": "https://arxiv.org/abs/2409.19606",
    "arxivId": "2409.19606",
    "last_visited": "2025-01-02T08:05:54.690Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-29T07:57:07Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.NE"
    ]
  },
  "2410.00286": {
    "id": "2410.00286",
    "title": "Fermi-GBM Team Analysis on The Ravasio Line",
    "authors": "Eric Burns, Stephen Lesage, Adam Goldstein and 20 others",
    "abstract": "The prompt spectra of gamma-ray bursts are known to follow broadband continuum behavior over decades in energy. GRB 221009A, given the moniker the brightest of all time (BOAT), is the brightest gamma-ray burst identified in half a century of observations, and was first identified by the Fermi Gamma-ray Burst Monitor (GBM). On behalf of the Fermi-GBM Team, Lesage et al. (2023) described the initial GBM analysis. Ravasio et al. (2024) report the identification of a spectral line in part of the prompt emission of this burst, which they describe as evolving over 80 s from $\\sim$12 MeV to 6 MeV. We report a GBM Team analysis on the Ravasio Line: 1) We cannot identify an instrumental effect that could have produced this signal, and 2) our method of calculating the statistical significance of the line shows it easily exceeds the 5$\\sigma$ discovery threshold. We additionally comment on the claim of the line beginning at earlier time intervals, up to 37 MeV, as reported in Zhang et al. (2024). We find that it is reasonable to utilize these measurements for characterization of the line evolution, with caution. We encourage theoretical studies exploring this newly discovered gamma-ray burst spectral feature, unless any rigorous alternative explanation unrelated to the emission from GRB 221009A is identified.",
    "url": "https://arxiv.org/abs/2410.00286",
    "arxivId": "2410.00286",
    "last_visited": "2025-01-13T06:06:36.272Z",
    "last_read": "2025-01-13T06:08:24.046881",
    "total_reading_time_seconds": 21,
    "published_date": "2024-09-30T23:43:52Z",
    "arxiv_tags": [
      "astro-ph.HE",
      "stat.AP"
    ]
  },
  "2410.01131": {
    "id": "2410.01131",
    "title": "nGPT: Normalized Transformer with Representation Learning on the   Hypersphere",
    "authors": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg",
    "abstract": "We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.",
    "url": "https://arxiv.org/abs/2410.01131",
    "arxivId": "2410.01131",
    "last_visited": "2025-01-07T23:23:22.376Z",
    "last_read": "2025-01-07T23:26:00.695853",
    "total_reading_time_seconds": 122,
    "published_date": "2024-10-01T23:50:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2410.02423": {
    "id": "2410.02423",
    "title": "PnP-Flow: Plug-and-Play Image Restoration with Flow Matching",
    "authors": "Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl",
    "abstract": "In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pre-trained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the data-fidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.",
    "url": "https://arxiv.org/abs/2410.02423",
    "arxivId": "2410.02423",
    "last_visited": "2024-12-22T06:43:50.011Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-03T12:13:56Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ]
  },
  "2410.05437": {
    "id": "2410.05437",
    "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
    "authors": "Charbel Sakr, Brucek Khailany",
    "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.",
    "url": "https://arxiv.org/abs/2410.05437",
    "arxivId": "2410.05437",
    "last_visited": "2025-01-10T20:41:20.568Z",
    "last_read": "2025-01-10T20:44:15.236395",
    "total_reading_time_seconds": 23,
    "published_date": "2024-10-07T18:59:22Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2410.08800": {
    "id": "2410.08800",
    "title": "Data Processing for the OpenGPT-X Model Family",
    "authors": "Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick and 19 others",
    "abstract": "This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.",
    "url": "https://arxiv.org/abs/2410.08800",
    "arxivId": "2410.08800",
    "last_visited": "2024-12-30T20:06:07.375Z",
    "last_read": "2025-01-04T06:52:45.622955",
    "total_reading_time_seconds": 47,
    "published_date": "2024-10-11T13:34:24Z",
    "arxiv_tags": [
      "cs.CL",
      "H.3.1; I.2.7"
    ]
  },
  "2410.10792": {
    "id": "2410.10792",
    "title": "Semantic Image Inversion and Editing using Rectified Stochastic   Differential Equations",
    "authors": "Litu Rout, Yujia Chen, Nataniel Ruiz and 3 others",
    "abstract": "Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.",
    "url": "https://arxiv.org/abs/2410.10792",
    "arxivId": "2410.10792",
    "last_visited": "2024-12-30T15:14:34.707Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-14T17:56:24Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ]
  },
  "2410.13835": {
    "id": "2410.13835",
    "title": "Active-Dormant Attention Heads: Mechanistically Demystifying   Extreme-Token Phenomena in LLMs",
    "authors": "Tianyu Guo, Druv Pai, Yu Bai and 3 others",
    "abstract": "Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called \"sink tokens\" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.   We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures -- transformers with one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.",
    "url": "https://arxiv.org/abs/2410.13835",
    "arxivId": "2410.13835",
    "last_visited": "2024-12-30T22:17:13.815000+00:00",
    "last_read": "2025-01-04T06:52:39.614755",
    "total_reading_time_seconds": 16,
    "published_date": "2024-10-17T17:54:06Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2410.15468": {
    "id": "2410.15468",
    "title": "What Emergence Can Possibly Mean",
    "authors": "Sean M. Carroll, Achyuth Parola",
    "abstract": "We consider emergence from the perspective of dynamics: states of a system evolving with time. We focus on the role of a decomposition of wholes into parts, and attempt to characterize relationships between levels without reference to whether higher-level properties are \"novel\" or \"unexpected.\" We offer a classification of different varieties of emergence, with and without new ontological elements at higher levels.",
    "url": "https://arxiv.org/abs/2410.15468",
    "arxivId": "2410.15468",
    "last_visited": "2024-12-22T05:48:56.244Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-20T18:45:11Z",
    "arxiv_tags": [
      "physics.hist-ph",
      "cond-mat.stat-mech"
    ]
  },
  "2410.21265": {
    "id": "2410.21265",
    "title": "Modular Duality in Deep Learning",
    "authors": "Jeremy Bernstein, Laker Newhouse",
    "abstract": "An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers -- the latter two methods are based on a rectangular Newton-Schulz iteration (Kovarik, 1970; Bj\\\"orck & Bowie, 1971). A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.",
    "url": "https://arxiv.org/abs/2410.21265",
    "arxivId": "2410.21265",
    "last_visited": "2025-01-02T19:43:53.760Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-28T17:57:31Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ]
  },
  "2410.24054": {
    "id": "2410.24054",
    "title": "EigenVI: score-based variational inference with orthogonal function   expansions",
    "authors": "Diana Cai, Chirag Modi, Charles C. Margossian and 3 others",
    "abstract": "We develop EigenVI, an eigenvalue-based approach for black-box variational inference (BBVI). EigenVI constructs its variational approximations from orthogonal function expansions. For distributions over $\\mathbb{R}^D$, the lowest order term in these expansions provides a Gaussian variational approximation, while higher-order terms provide a systematic way to model non-Gaussianity. These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them. EigenVI can also model other types of random variables (e.g., nonnegative, bounded) by constructing variational approximations from different families of orthogonal functions. Within these families, EigenVI computes the variational approximation that best matches the score function of the target distribution by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms. (Gradient-based methods can be sensitive to learning rates, termination criteria, and other tunable hyperparameters.) We use EigenVI to approximate a variety of target distributions, including a benchmark suite of Bayesian models from posteriordb. On these distributions, we find that EigenVI is more accurate than existing methods for Gaussian BBVI.",
    "url": "https://arxiv.org/abs/2410.24054",
    "arxivId": "2410.24054",
    "last_visited": "2024-12-22T05:52:47.117Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-31T15:48:34Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ]
  },
  "2411.04282": {
    "id": "2411.04282",
    "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning   Capabilities via Self-Rewarding",
    "authors": "Haolin Chen, Yihao Feng, Zuxin Liu and 8 others",
    "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at \\url{https://github.com/SalesforceAIResearch/LaTRO}.",
    "url": "https://arxiv.org/abs/2411.04282",
    "arxivId": "2411.04282",
    "last_visited": "2024-12-22T21:35:41.978Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-06T22:02:30Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML",
      "I.2.7"
    ]
  },
  "2411.04330": {
    "id": "2411.04330",
    "title": "Scaling Laws for Precision",
    "authors": "Tanishq Kumar, Zachary Ankner, Benjamin F. Spector and 6 others",
    "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
    "url": "https://arxiv.org/abs/2411.04330",
    "arxivId": "2411.04330",
    "last_visited": "2024-12-30T20:16:37.184Z",
    "last_read": "2025-01-04T06:53:12.612468",
    "total_reading_time_seconds": 11,
    "published_date": "2024-11-07T00:10:10Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2411.05899": {
    "id": "2411.05899",
    "title": "Streaming Bayes GFlowNets",
    "authors": "Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita",
    "abstract": "Bayes' rule naturally allows for inference refinement in a streaming fashion, without the need to recompute posteriors from scratch whenever new data arrives. In principle, Bayesian streaming is straightforward: we update our prior with the available data and use the resulting posterior as a prior when processing the next data chunk. In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation. For continuous state spaces, variational inference (VI) is particularly convenient due to its scalability and the tractability of variational posteriors. For discrete state spaces, however, state-of-the-art VI results in analytically intractable approximations that are ill-suited for streaming settings. To enable streaming Bayesian inference over discrete parameter spaces, we propose streaming Bayes GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed GFlowNets -- a powerful class of amortized samplers for discrete compositional objects. Notably, SB-GFlowNet approximates the initial posterior using a standard GFlowNet and subsequently updates it using a tailored procedure that requires only the newly observed data. Our case studies in linear preference learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets in sampling from an unnormalized posterior in a streaming setting. As expected, we also observe that SB-GFlowNets is significantly faster than repeatedly training a GFlowNet from scratch to sample from the full posterior.",
    "url": "https://arxiv.org/abs/2411.05899",
    "arxivId": "2411.05899",
    "last_visited": "2024-12-28T06:09:10.852Z",
    "last_read": "2025-01-04T15:02:48.872465",
    "total_reading_time_seconds": 24,
    "published_date": "2024-11-08T15:53:56Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2411.06068": {
    "id": "2411.06068",
    "title": "Zyda-2: a 5 Trillion Token High-Quality Dataset",
    "authors": "Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge",
    "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset for language model pretraining. Zyda-2 was used to train our Zamba2 series of models which are state-of-the-art for their weight class. We build Zyda-2 by collating high-quality open-source tokens such as FineWeb and DCLM, then distilling them to the highest-quality subset via cross-deduplication and model-based quality filtering. Zyda-2 is released under a permissive open license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2",
    "url": "https://arxiv.org/abs/2411.06068",
    "arxivId": "2411.06068",
    "last_visited": "2024-12-30T22:18:39.627000+00:00",
    "last_read": "2025-01-04T06:52:36.612646",
    "total_reading_time_seconds": 6,
    "published_date": "2024-11-09T04:57:41Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2411.12372": {
    "id": "2411.12372",
    "title": "RedPajama: an Open Dataset for Training Large Language Models",
    "authors": "Maurice Weber, Daniel Fu, Quentin Anthony and 16 others",
    "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",
    "url": "https://arxiv.org/abs/2411.12372",
    "arxivId": "2411.12372",
    "last_visited": "2024-12-30T20:18:27.856Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-19T09:35:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ]
  },
  "2411.18933": {
    "id": "2411.18933",
    "title": "Efficient Track Anything",
    "authors": "Yunyang Xiong, Chong Zhou, Xiaoyu Xiang and 10 others",
    "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.",
    "url": "https://arxiv.org/abs/2411.18933",
    "arxivId": "2411.18933",
    "last_visited": "2024-12-15T20:13:52.905Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-28T05:52:10Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2411.19108": {
    "id": "2411.19108",
    "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "authors": "Feng Liu, Shiwei Zhang, Xiaofeng Wang and 6 others",
    "abstract": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.",
    "url": "https://arxiv.org/abs/2411.19108",
    "arxivId": "2411.19108",
    "last_visited": "2024-12-30T15:10:27.225Z",
    "last_read": "2025-01-04T14:48:33.229442",
    "total_reading_time_seconds": 40,
    "published_date": "2024-11-28T12:50:05Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2411.19722": {
    "id": "2411.19722",
    "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text",
    "authors": "Michael Tschannen, André Susano Pinto, Alexander Kolesnikov",
    "abstract": "Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer - JetFormer - which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds.",
    "url": "https://arxiv.org/abs/2411.19722",
    "arxivId": "2411.19722",
    "last_visited": "2024-12-15T22:16:44.245Z",
    "last_read": "2025-01-05T18:41:15.660935",
    "total_reading_time_seconds": 300,
    "published_date": "2024-11-29T14:14:59Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  "2412.00733": {
    "id": "2412.00733",
    "title": "Hallo3: Highly Dynamic and Realistic Portrait Image Animation with   Diffusion Transformer Networks",
    "authors": "Jiahao Cui, Hui Li, Yun Zhan and 7 others",
    "abstract": "Existing methodologies for animating portrait images face significant challenges, particularly in handling non-frontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds. In this paper, we introduce the first application of a pretrained transformer-based video generative model that demonstrates strong generalization capabilities and generates highly dynamic, realistic videos for portrait animation, effectively addressing these challenges. The adoption of a new video backbone model makes previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation inapplicable. To address this limitation, we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers, ensuring consistent facial identity across video sequences. Additionally, we investigate various speech audio conditioning and motion frame mechanisms to enable the generation of continuous video driven by speech audio. Our method is validated through experiments on benchmark and newly proposed wild datasets, demonstrating substantial improvements over prior methods in generating realistic portraits characterized by diverse orientations within dynamic and immersive scenes. Further visualizations and the source code are available at: https://fudan-generative-vision.github.io/hallo3/.",
    "url": "https://arxiv.org/abs/2412.00733",
    "arxivId": "2412.00733",
    "last_visited": "2025-01-10T18:21:47.668Z",
    "last_read": "2025-01-10T18:23:26.487447",
    "total_reading_time_seconds": 20,
    "published_date": "2024-12-01T08:54:30Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ]
  },
  "2412.01023": {
    "id": "2412.01023",
    "title": "Learning Structured Representations with Hyperbolic Embeddings",
    "authors": "Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao",
    "abstract": "Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013]. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss, and can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios. For a better understanding of structured representation, we perform eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at \\url{https://github.com/uiuctml/HypStructure}.",
    "url": "https://arxiv.org/abs/2412.01023",
    "arxivId": "2412.01023",
    "last_visited": "2024-12-29T02:32:00.838000+00:00",
    "last_read": "2025-01-04T14:49:39.264976",
    "total_reading_time_seconds": 16,
    "published_date": "2024-12-02T00:56:44Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ]
  },
  "2412.02595": {
    "id": "2412.02595",
    "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset",
    "authors": "Dan Su, Kezhi Kong, Ying Lin and 6 others",
    "abstract": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html",
    "url": "https://arxiv.org/abs/2412.02595",
    "arxivId": "2412.02595",
    "last_visited": "2024-12-30T20:02:25.502Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-03T17:28:50Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.03603": {
    "id": "2412.03603",
    "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
    "authors": "Weijie Kong, Qi Tian, Zijian Zhang and 49 others",
    "abstract": "Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.",
    "url": "https://arxiv.org/abs/2412.03603",
    "arxivId": "2412.03603",
    "last_visited": "2024-12-30T15:16:27.366Z",
    "last_read": "2025-01-04T14:48:27.533139",
    "total_reading_time_seconds": 74,
    "published_date": "2024-12-03T23:52:37Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.04384": {
    "id": "2412.04384",
    "title": "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D   Occupancy Prediction",
    "authors": "Yuanhui Huang, Amonnut Thammatadatrakoon, Wenzhao Zheng and 3 others",
    "abstract": "3D semantic occupancy prediction is an important task for robust vision-centric autonomous driving, which predicts fine-grained geometry and semantics of the surrounding scene. Most existing methods leverage dense grid-based scene representations, overlooking the spatial sparsity of the driving scenes. Although 3D semantic Gaussian serves as an object-centric sparse alternative, most of the Gaussians still describe the empty region with low efficiency. To address this, we propose a probabilistic Gaussian superposition model which interprets each Gaussian as a probability distribution of its neighborhood being occupied and conforms to probabilistic multiplication to derive the overall geometry. Furthermore, we adopt the exact Gaussian mixture model for semantics calculation to avoid unnecessary overlapping of Gaussians. To effectively initialize Gaussians in non-empty region, we design a distribution-based initialization module which learns the pixel-aligned occupancy distribution instead of the depth of surfaces. We conduct extensive experiments on nuScenes and KITTI-360 datasets and our GaussianFormer-2 achieves state-of-the-art performance with high efficiency. Code: https://github.com/huang-yh/GaussianFormer.",
    "url": "https://arxiv.org/abs/2412.04384",
    "arxivId": "2412.04384",
    "last_visited": "2025-01-14T18:28:38.121Z",
    "last_read": "2025-01-14T18:29:51.193732",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-05T17:59:58Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2412.04619": {
    "id": "2412.04619",
    "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
    "authors": "Tian Qin, Naomi Saphra, David Alvarez-Melis",
    "abstract": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
    "url": "https://arxiv.org/abs/2412.04619",
    "arxivId": "2412.04619",
    "last_visited": "2024-12-22T17:55:18.862Z",
    "last_read": "2025-01-05T08:23:53.657116",
    "total_reading_time_seconds": 60,
    "published_date": "2024-12-05T21:12:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ]
  },
  "2412.06264": {
    "id": "2412.06264",
    "title": "Flow Matching Guide and Code",
    "authors": "Yaron Lipman, Marton Havasi, Peter Holderrieth and 7 others",
    "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
    "url": "https://arxiv.org/abs/2412.06264",
    "arxivId": "2412.06264",
    "last_visited": "2024-12-22T06:52:20.913Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-09T07:22:38Z",
    "arxiv_tags": [
      "cs.LG"
    ]
  },
  "2412.06769": {
    "id": "2412.06769",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "authors": "Shibo Hao, Sainbayar Sukhbaatar, DiJia Su and 4 others",
    "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
    "url": "https://arxiv.org/abs/2412.06769",
    "arxivId": "2412.06769",
    "last_visited": "2025-01-02T19:39:10.844Z",
    "last_read": "2025-01-04T06:52:12.618753",
    "total_reading_time_seconds": 32,
    "published_date": "2024-12-09T18:55:56Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.06771": {
    "id": "2412.06771",
    "title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under   Uncertainty",
    "authors": "Meera Hahn, Wenjun Zeng, Nithish Kannen and 4 others",
    "abstract": "User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn T2I generation. Demo: https://github.com/google-deepmind/proactive_t2i_agents.",
    "url": "https://arxiv.org/abs/2412.06771",
    "arxivId": "2412.06771",
    "last_visited": "2025-01-09T14:42:27.418000+00:00",
    "last_read": "2025-01-09T14:43:31.362013",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-09T18:56:32Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  "2412.06845": {
    "id": "2412.06845",
    "title": "Fully Open Source Moxin-7B Technical Report",
    "authors": "Pu Zhao, Xuan Shen, Zhenglun Kong and 13 others",
    "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be \"open-source,\" which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of \"open science\" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.",
    "url": "https://arxiv.org/abs/2412.06845",
    "arxivId": "2412.06845",
    "last_visited": "2024-12-30T20:04:21.071Z",
    "last_read": "2025-01-04T06:53:18.651113",
    "total_reading_time_seconds": 53,
    "published_date": "2024-12-08T02:01:46Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2412.09621": {
    "id": "2412.09621",
    "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
    "authors": "Linyi Jin, Richard Tucker, Zhengqi Li and 3 others",
    "abstract": "Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3R to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io",
    "url": "https://arxiv.org/abs/2412.09621",
    "arxivId": "2412.09621",
    "last_visited": "2024-12-16T08:58:26.647Z",
    "last_read": "2025-01-05T18:41:15.656598",
    "total_reading_time_seconds": 60,
    "published_date": "2024-12-12T18:59:54Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.10271": {
    "id": "2412.10271",
    "title": "Benchmarking Linguistic Diversity of Large Language Models",
    "authors": "Yanzhu Guo, Guokan Shang, Chloé Clavel",
    "abstract": "The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an in-depth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.",
    "url": "https://arxiv.org/abs/2412.10271",
    "arxivId": "2412.10271",
    "last_visited": "2024-12-30T20:03:18.882Z",
    "last_read": "2025-01-04T06:53:18.652874",
    "total_reading_time_seconds": 52,
    "published_date": "2024-12-13T16:46:03Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.11766": {
    "id": "2412.11766",
    "title": "Interplay of epidemic spreading and vaccine uptake under complex social   contagion",
    "authors": "Alfonso de Miguel-Arribas, Alberto Aleta, Yamir Moreno",
    "abstract": "Modeling human behavior is essential to accurately predict epidemic spread, with behaviors like vaccine hesitancy complicating control efforts. While epidemic spread is often treated as a simple contagion, vaccine uptake may follow complex contagion dynamics, where individuals' decisions depend on multiple social contacts. Recently, the concept of complex contagion has received strong theoretical underpinnings thanks to the generalization of spreading phenomena from pairwise to higher-order interactions. Although several potential applications have been suggested, examples of complex contagions motivated by real data remain scarce. Surveys on COVID-19 vaccine hesitancy in the US suggest that vaccination attitudes may indeed depend on the vaccination status of social peers, aligning with complex contagion principles. In this work, we examine the interactions between epidemic spread, vaccination, and vaccine uptake attitudes under complex contagion. Using the SIR model with a dynamic, threshold-based vaccination campaign, we simulate scenarios on an age-structured multilayer network informed by US contact data. Our results offer insights into the role of social dynamics in shaping vaccination behavior and epidemic outcomes.",
    "url": "https://arxiv.org/abs/2412.11766",
    "arxivId": "2412.11766",
    "last_visited": "2024-12-28T05:44:44.089Z",
    "last_read": "2025-01-04T15:02:54.892474",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-16T13:37:27Z",
    "arxiv_tags": [
      "physics.soc-ph",
      "cs.SI"
    ]
  },
  "2412.11768": {
    "id": "2412.11768",
    "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
    "authors": "Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen",
    "abstract": "In this work, we question the necessity of adaptive gradient methods for training deep neural networks. SGD-SaI is a simple yet effective enhancement to stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning rate Scaling at Initialization (SaI) to distinct parameter groups, guided by their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning rates without relying on adaptive second-order momentum, SGD-SaI helps prevent training imbalances from the very first iteration and cuts the optimizer's memory usage by half compared to AdamW. Despite its simplicity and efficiency, SGD-SaI consistently matches or outperforms AdamW in training a variety of Transformer-based tasks, effectively overcoming a long-standing challenge of using SGD for training Transformers. SGD-SaI excels in ImageNet-1K classification with Vision Transformers(ViT) and GPT-2 pretraining for large language models (LLMs, transformer decoder-only), demonstrating robustness to hyperparameter variations and practicality for diverse applications. We further tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion models, where it consistently outperforms state-of-the-art optimizers. From a memory efficiency perspective, SGD-SaI achieves substantial memory savings for optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters) and 25.15 GB for Llama2-7B compared to AdamW in full-precision training settings.",
    "url": "https://arxiv.org/abs/2412.11768",
    "arxivId": "2412.11768",
    "last_visited": "2025-01-05T23:10:17.881Z",
    "last_read": "2025-01-05T23:11:31.444118",
    "total_reading_time_seconds": 83,
    "published_date": "2024-12-16T13:41:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ]
  },
  "2412.12095": {
    "id": "2412.12095",
    "title": "Causal Diffusion Transformers for Generative Modeling",
    "authors": "Chaorui Deng, Deyao Zhu, Kunchang Li and 2 others",
    "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.",
    "url": "https://arxiv.org/abs/2412.12095",
    "arxivId": "2412.12095",
    "last_visited": "2025-01-10T21:02:08.602000+00:00",
    "last_read": "2025-01-10T21:03:37.384735",
    "total_reading_time_seconds": 36,
    "published_date": "2024-12-16T18:59:29Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.13061": {
    "id": "2412.13061",
    "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
    "authors": "Anni Tang, Tianyu He, Junliang Guo and 3 others",
    "abstract": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.",
    "url": "https://arxiv.org/abs/2412.13061",
    "arxivId": "2412.13061",
    "last_visited": "2025-01-07T08:25:03.828000+00:00",
    "last_read": "2025-01-07T08:26:18.055954",
    "total_reading_time_seconds": 8,
    "published_date": "2024-12-17T16:27:11Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2412.13145": {
    "id": "2412.13145",
    "title": "Agnosticism About Artificial Consciousness",
    "authors": "Tom McClelland",
    "abstract": "Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism - that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.",
    "url": "https://arxiv.org/abs/2412.13145",
    "arxivId": "2412.13145",
    "last_visited": "2024-12-22T05:22:42.487Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-17T18:11:12Z",
    "arxiv_tags": [
      "cs.AI"
    ]
  },
  "2412.13663": {
    "id": "2412.13663",
    "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for   Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "authors": "Benjamin Warner, Antoine Chaffin, Benjamin Clavié and 11 others",
    "abstract": "Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",
    "url": "https://arxiv.org/abs/2412.13663",
    "arxivId": "2412.13663",
    "last_visited": "2024-12-22T06:30:57.155Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-18T09:39:44Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2412.14294": {
    "id": "2412.14294",
    "title": "TRecViT: A Recurrent Video Transformer",
    "authors": "Viorica Pătrăucean, Xu Owen He, Joseph Heyward and 10 others",
    "abstract": "We propose a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having $3\\times$ less parameters, $12\\times$ smaller memory footprint, and $5\\times$ lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit.",
    "url": "https://arxiv.org/abs/2412.14294",
    "arxivId": "2412.14294",
    "last_visited": "2025-01-11T07:35:29.299000+00:00",
    "last_read": "2025-01-11T07:38:52.726106",
    "total_reading_time_seconds": 9,
    "published_date": "2024-12-18T19:44:30Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ]
  },
  "2412.15285": {
    "id": "2412.15285",
    "title": "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase   Pretraining",
    "authors": "Steven Feng, Shrimai Prabhumoye, Kezhi Kong and 4 others",
    "abstract": "Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.",
    "url": "https://arxiv.org/abs/2412.15285",
    "arxivId": "2412.15285",
    "last_visited": "2024-12-30T20:04:52.739Z",
    "last_read": "2025-01-04T06:53:18.649544",
    "total_reading_time_seconds": 49,
    "published_date": "2024-12-18T18:41:18Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  "2412.17758": {
    "id": "2412.17758",
    "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
    "authors": "Łukasz Borchmann",
    "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
    "url": "https://arxiv.org/abs/2412.17758",
    "arxivId": "2412.17758",
    "last_visited": "2024-12-29T08:28:16.461Z",
    "last_read": "2025-01-04T14:49:27.230757",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-23T18:14:36Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ]
  },
  "2412.17805": {
    "id": "2412.17805",
    "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
    "authors": "Yazhou Xing, Yang Fei, Yingqing He and 4 others",
    "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.",
    "url": "https://arxiv.org/abs/2412.17805",
    "arxivId": "2412.17805",
    "last_visited": "2025-01-02T08:03:08.915Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-23T18:58:24Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2412.17847": {
    "id": "2412.17847",
    "title": "Bridging the Data Provenance Gap Across Text, Speech and Video",
    "authors": "Shayne Longpre, Nikhil Singh, Manuel Cherep and 40 others",
    "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
    "url": "https://arxiv.org/abs/2412.17847",
    "arxivId": "2412.17847",
    "last_visited": "2024-12-30T20:03:13.039Z",
    "last_read": "2025-01-04T06:53:21.617516",
    "total_reading_time_seconds": 5,
    "published_date": "2024-12-19T01:30:19Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "cs.MM"
    ]
  },
  "2412.18069": {
    "id": "2412.18069",
    "title": "Improving Factuality with Explicit Working Memory",
    "authors": "Mingda Chen, Yang Li, Karthik Padthe and 5 others",
    "abstract": "Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce EWE (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing EWE to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.",
    "url": "https://arxiv.org/abs/2412.18069",
    "arxivId": "2412.18069",
    "last_visited": "2025-01-05T19:03:17.946Z",
    "last_read": "2025-01-05T19:04:47.338057",
    "total_reading_time_seconds": 27,
    "published_date": "2024-12-24T00:55:59Z",
    "arxiv_tags": [
      "cs.CL"
    ]
  },
  "2412.18082": {
    "id": "2412.18082",
    "title": "Prompt Tuning for Item Cold-start Recommendation",
    "authors": "Yuezihan Jiang, Gaode Chen, Wenhan Zhang and 6 others",
    "abstract": "The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of the positive feedback to the model, which is the core of the cold-start problem that hinders the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that compared to the content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios",
    "url": "https://arxiv.org/abs/2412.18082",
    "arxivId": "2412.18082",
    "last_visited": "2024-12-30T04:50:12.821000+00:00",
    "last_read": "2025-01-04T14:48:54.231751",
    "total_reading_time_seconds": 19,
    "published_date": "2024-12-24T01:38:19Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.AI"
    ]
  },
  "2412.18860": {
    "id": "2412.18860",
    "title": "Bootstrap Your Own Context Length",
    "authors": "Liang Wang, Nan Yang, Xingxing Zhang and 2 others",
    "abstract": "We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.",
    "url": "https://arxiv.org/abs/2412.18860",
    "arxivId": "2412.18860",
    "last_visited": "2024-12-30T04:37:40.310Z",
    "last_read": "2025-01-04T14:48:54.232387",
    "total_reading_time_seconds": 44,
    "published_date": "2024-12-25T10:08:54Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.IR"
    ]
  },
  "2412.18956": {
    "id": "2412.18956",
    "title": "Musings About the Future of Search: A Return to the Past?",
    "authors": "Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne",
    "abstract": "When you have a question, the most effective way to have the question answered is to directly connect with experts on the topic and have a conversation with them. Prior to the invention of writing, this was the only way. Although effective, this solution exhibits scalability challenges. Writing allowed knowledge to be materialized, preserved, and replicated, enabling the development of different technologies over the centuries to connect information seekers with relevant information. This progression ultimately culminated in the ten-blue-links web search paradigm we're familiar with, just before the recent emergence of generative AI. However, we often forget that consuming static content is an imperfect solution. With the advent of large language models, it has become possible to develop a superior experience by allowing users to directly engage with experts. These interactions can of course satisfy information needs, but expert models can do so much more. This coming future requires reimagining search.",
    "url": "https://arxiv.org/abs/2412.18956",
    "arxivId": "2412.18956",
    "last_visited": "2024-12-30T04:52:50.157000+00:00",
    "last_read": "2025-01-04T14:48:51.239805",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-25T18:09:34Z",
    "arxiv_tags": [
      "cs.IR"
    ]
  },
  "2412.19442": {
    "id": "2412.19442",
    "title": "A Survey on Large Language Model Acceleration based on KV Cache   Management",
    "authors": "Haoyang Li, Yiming Li, Anxin Tian and 7 others",
    "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
    "url": "https://arxiv.org/abs/2412.19442",
    "arxivId": "2412.19442",
    "last_visited": "2024-12-30T04:35:23.041Z",
    "last_read": "2025-01-04T14:48:57.232180",
    "total_reading_time_seconds": 20,
    "published_date": "2024-12-27T04:17:57Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.DC"
    ]
  },
  "2412.19792": {
    "id": "2412.19792",
    "title": "InfAlign: Inference-aware language model alignment",
    "authors": "Ananth Balashankar, Ziteng Sun, Jonathan Berant and 9 others",
    "abstract": "Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inference-time methods. We then modify the alignment objective and propose a framework for inference-aware alignment (IAPO). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the inference-time win rate of the aligned policy against the reference policy is the solution to the typical RLHF problem with a transformation of the reward. This motivates us to provide the KL-regularized calibrate-and-transform RL (CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-N sampling and best-of-N jailbreaking, where N responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform baselines that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.",
    "url": "https://arxiv.org/abs/2412.19792",
    "arxivId": "2412.19792",
    "last_visited": "2025-01-03T08:59:10.679Z",
    "last_read": "2025-01-04T06:52:00.645791",
    "total_reading_time_seconds": 49,
    "published_date": "2024-12-27T18:45:36Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ]
  },
  "2412.20292": {
    "id": "2412.20292",
    "title": "An analytic theory of creativity in convolutional diffusion models",
    "authors": "Mason Kamb, Surya Ganguli",
    "abstract": "We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \\sim 0.75$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.",
    "url": "https://arxiv.org/abs/2412.20292",
    "arxivId": "2412.20292",
    "last_visited": "2025-01-01T16:00:07.088Z",
    "last_read": "2025-01-04T06:52:30.604171",
    "total_reading_time_seconds": 3,
    "published_date": "2024-12-28T22:33:29Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "q-bio.NC",
      "stat.ML",
      "I.2.10"
    ]
  },
  "2501.00663": {
    "id": "2501.00663",
    "title": "Titans: Learning to Memorize at Test Time",
    "authors": "Ali Behrouz, Peilin Zhong, Vahab Mirrokni",
    "abstract": "Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.",
    "url": "https://arxiv.org/abs/2501.00663",
    "arxivId": "2501.00663",
    "last_visited": "2025-01-17T02:36:23.950Z",
    "last_read": "2025-01-10T08:07:02.307560",
    "total_reading_time_seconds": 11,
    "published_date": "2024-12-31T22:32:03Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  "2501.02976": {
    "id": "2501.02976",
    "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution",
    "authors": "Rui Xie, Yinhong Liu, Penghao Zhou and 7 others",
    "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (\\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce\\textbf{~\\name} (\\textbf{S}patial-\\textbf{T}emporal \\textbf{A}ugmentation with T2V models for \\textbf{R}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate\\textbf{~\\name}~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
    "url": "https://arxiv.org/abs/2501.02976",
    "arxivId": "2501.02976",
    "last_visited": "2025-01-10T06:25:50.587Z",
    "last_read": "2025-01-10T06:28:39.452667",
    "total_reading_time_seconds": 46,
    "published_date": "2025-01-06T12:36:21Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2501.03006": {
    "id": "2501.03006",
    "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
    "authors": "Luozhou Wang, Yijun Li, Zhifei Chen and 5 others",
    "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
    "url": "https://arxiv.org/abs/2501.03006",
    "arxivId": "2501.03006",
    "last_visited": "2025-01-10T05:23:00.328000+00:00",
    "last_read": "2025-01-10T05:24:23.109329",
    "total_reading_time_seconds": 23,
    "published_date": "2025-01-06T13:32:16Z",
    "arxiv_tags": [
      "cs.CV"
    ]
  },
  "2501.03082": {
    "id": "2501.03082",
    "title": "A two-hump spectrum in the prompt emission of GRB 240825A",
    "authors": "Hai-Ming Zhang, Zi-Qi Wang, Cui-Yuan Dai and 4 others",
    "abstract": "An extra hard spectral component that extends to GeV energies, in additional to the typical sub- MeV Band component, appears in several gamma-ray burst (GRBs) detected by Fermi Large Area Telescopes (LAT). Only in one case (i.e., GRB 090926A), a spectral break feature at the high energy end is identified in the extra hard component, but the photon counts are not enough to distinguish between the cutoff model and the broken power law model for the spectral break. In this work, we report the detection of an extra hard component showing the spectral break in GRB 240825A. We find that a broken power-law model fits the spectral data of the extra component better than a single power-law with an exponential cutoff in the time resolved spectrum for the second emission pulse, with a break at about 50 MeV. This spectral feature disfavors the gamma-ray opacity to pair creation as the origin of the spectral break, but points to an intrinsic peak for the extra component. The low ratio between the peak of the extra hard component and that of the Band component challenges the synchrotron self-Compton origin for the extra component. Alternative scenarios, such as the inverse Compton scattering of the photosphere emission, are discussed. In addition, we find a clear transition from the prompt emission to afterglow emission at GeV energies in GRB 240825A, manifested by a temporal steep decay and an unique spectral evolution.",
    "url": "https://arxiv.org/abs/2501.03082",
    "arxivId": "2501.03082",
    "last_visited": "2025-01-13T06:07:18.596Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-06T15:25:59Z",
    "arxiv_tags": [
      "astro-ph.HE"
    ]
  },
  "2501.03847": {
    "id": "2501.03847",
    "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video   Generation Control",
    "authors": "Zekai Gu, Rui Yan, Jiahao Lu and 9 others",
    "abstract": "Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.",
    "url": "https://arxiv.org/abs/2501.03847",
    "arxivId": "2501.03847",
    "last_visited": "2025-01-10T05:52:18.660Z",
    "last_read": "2025-01-10T05:54:15.298855",
    "total_reading_time_seconds": 22,
    "published_date": "2025-01-07T15:01:58Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ]
  },
  "2501.04227": {
    "id": "2501.04227",
    "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
    "authors": "Samuel Schmidgall, Yusheng Su, Ze Wang and 6 others",
    "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
    "url": "https://arxiv.org/abs/2501.04227",
    "arxivId": "2501.04227",
    "last_visited": "2025-01-10T05:47:26.623Z",
    "last_read": "2025-01-10T05:49:05.434811",
    "total_reading_time_seconds": 44,
    "published_date": "2025-01-08T01:58:42Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  "2501.04697": {
    "id": "2501.04697",
    "title": "Grokking at the Edge of Numerical Stability",
    "authors": "Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal",
    "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the na\\\"ive loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and $\\perp$Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods. Code for this paper is available at https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
    "url": "https://arxiv.org/abs/2501.04697",
    "arxivId": "2501.04697",
    "last_visited": "2025-01-17T18:17:21.137Z",
    "last_read": "2025-01-17T18:18:33.078619",
    "total_reading_time_seconds": 18,
    "published_date": "2025-01-08T18:58:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ]
  },
  "2501.05242": {
    "id": "2501.05242",
    "title": "Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and   Photorealistic Mapping",
    "authors": "Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun",
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.",
    "url": "https://arxiv.org/abs/2501.05242",
    "arxivId": "2501.05242",
    "last_visited": "2025-01-11T09:05:35.504Z",
    "last_read": "2025-01-11T09:07:19.697839",
    "total_reading_time_seconds": 3,
    "published_date": "2025-01-09T13:50:26Z",
    "arxiv_tags": [
      "cs.CV",
      "68T40(Primary)68T45, 68U99 (Secondary)",
      "I.4.8; I.3.7"
    ]
  },
  "2501.05441": {
    "id": "2501.05441",
    "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
    "authors": "Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin",
    "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.",
    "url": "https://arxiv.org/abs/2501.05441",
    "arxivId": "2501.05441",
    "last_visited": "2025-01-12T17:53:23.123Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-09T18:53:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ]
  },
  "2501.05743": {
    "id": "2501.05743",
    "title": "Recurrent Features of Amplitudes in Planar $\\mathcal{N}=4$ Super   Yang-Mills Theory",
    "authors": "Tianji Cai, François Charton, Kyle Cranmer and 3 others",
    "abstract": "The planar three-gluon form factor for the chiral stress tensor operator in planar maximally supersymmetric Yang-Mills theory is an analog of the Higgs-to-three-gluon scattering amplitude in QCD. The amplitude (symbol) bootstrap program has provided a wealth of high-loop perturbative data about this form factor, with results up to eight loops available. The symbol of the form factor at $L$ loops is given by words of length $2L$ in six letters with associated integer coefficients. In this paper, we analyze this data, describing patterns of zero coefficients and relations between coefficients. We find many sequences of words whose coefficients are given by closed-form expressions which we expect to be valid at any loop order. Moreover, motivated by our previous machine-learning analysis, we identify simple recursion relations that relate the coefficient of a word to the coefficients of particular lower-loop words. These results open an exciting door for understanding scattering amplitudes at all loop orders.",
    "url": "https://arxiv.org/abs/2501.05743",
    "arxivId": "2501.05743",
    "last_visited": "2025-01-13T04:39:53.737000+00:00",
    "last_read": "2025-01-13T04:41:06.637199",
    "total_reading_time_seconds": 78,
    "published_date": "2025-01-10T06:19:48Z",
    "arxiv_tags": [
      "hep-th",
      "hep-ph"
    ]
  },
  "2501.06252": {
    "id": "2501.06252",
    "title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
    "authors": "Qi Sun, Edoardo Cetin, Yujin Tang",
    "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce $\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, $\\text{Transformer}^2$ employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. $\\text{Transformer}^2$ represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.",
    "url": "https://arxiv.org/abs/2501.06252",
    "arxivId": "2501.06252",
    "last_visited": "2025-01-15T05:04:30.386000+00:00",
    "last_read": "2025-01-15T05:05:34.185386",
    "total_reading_time_seconds": 48,
    "published_date": "2025-01-09T01:19:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  }
}