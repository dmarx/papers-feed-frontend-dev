- **Title**: Timestep Embedding Aware Cache (TeaCache) for Video Diffusion Models
- **Objective**: Develop a training-free caching strategy to enhance inference speed in video diffusion models by leveraging the correlation between model inputs and outputs.
- **Key Contributions**:
  - Introduction of TeaCache, compatible with DiT diffusion models.
  - Two-stage strategy for estimating model output differences using input embeddings.
  - Significant speedup in state-of-the-art (SOTA) generation models (Open-Sora, Open-Sora-Plan, Latte) with negligible quality loss.
  
- **Diffusion Model Overview**:
  - **Forward Process**: 
    - $x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} z_t$ for $t = 1, \ldots, T$.
  - **Reverse Process**: 
    - $p_\theta(x_{t-1} | x_t) = N(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$.
  
- **Timestep Embedding**:
  - Transformed via sinusoidal embedding and MLP: 
    - $T_t = MLP(\text{sinusoidal}(t))$.
  - Modulates inputs/outputs in Transformer blocks, affecting model output magnitude.

- **Caching Strategy**:
  - **Naive Caching**: 
    - Use accumulated relative L1 distance to decide on caching:
      - $t_b - 1 \text{ if } L1_{rel}(F, t) \leq \delta$.
  - **Rescaled Caching**: 
    - Introduces polynomial fitting to correct scaling bias between input and output differences.

- **Performance Metrics**:
  - Relative L1 distance for output embedding:
    - $L1_{rel}(O, t) = \frac{\|O_t - O_{t+1}\|_1}{\|O_{t+1}\|_1}$.
  - Caching threshold $\delta$ impacts trade-off between speed and visual quality.

- **Empirical Findings**:
  - Strong correlation between timestep embedding modulated noisy input and model output.
  - Distinct output difference patterns observed across models (e.g., 'U' shape in Open Sora).

- **Figures**:
  - **Figure 1**: Quality-latency comparison of TeaCache vs. PAB.
  - **Figure 2**: Caching mechanism illustration.
  - **Figure 3**: Output difference patterns across models.

- **Conclusion**: TeaCache effectively reduces redundant computations in video diffusion models, enhancing inference speed while maintaining output quality.