# Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model

## Abstract

## 

Figure 1. Quality-latency comparison of video diffusion models. Visual quality versus latency curves of the proposed TeaCache approach and PAB [59] using Latte [29]. TeaCache significantly outperforms PAB in both visual quality and efficiency. Latency is evaluated on a single A800 GPU for 16-frame video generation under 512 × 512 resolution.

## Introduction

Recent years have witnessed the emergence of diffusion models [[13,](#b12)[16,](#b15)[44,](#b42)[46]](#b44), as a fundamental backbone for visual generation. The model architecture has evolved from U-Net [[4,](#b3)[33,](#b32)[35]](#b34) to diffusion transformers (DiT) [[32]](#b31), which greatly increased model capacities. Empowered by DiT, video generation models [[1,](#b0)[2,](#b1)[21,](#b20)[29,](#b28)[56,](#b54)[60]](#b58) have reached a groundbreaking level. Despite of the substantial efficacy of these powerful models, their inference speed remains a pivotal impediment to wider adoption [[24]](#b23). This core limitation arises from the sequential denoising procedure inherent to their reverse phase, which inhibits parallel decoding [[40]](#). Moreover, as model parameters scale up and the requirements for higher resolution and longer durations of videos escalate [[9,](#b8)[56]](#b54), the inference process experiences a further decline in speed.

To accelerate the visual generation procedure, distillation [[30,](#b29)[37,](#b36)[51]](#b49) and post-training [[10,](#b9)[28]](#b27) are employed. However, these methods typically require extra training, which implies substantial computational cost and data resources. An alternative technological pathway is to leverage the caching mechanism [[3,](#b2)[14,](#b13)[41]](#b39), which does not require additional training to maintain the performance of diffusion models. These methods [[11,](#b10)[38,](#b37)[55,](#b53)[59](#b57)] find that the model outputs are similar between the consecutive timesteps when denoising and propose to reduce redundancy by caching model outputs in a uniform way, Fig. [2](#fig_0)(upper). Nevertheless, when the output difference between consecutive timesteps varies, the uniform caching strategy lacks flexibility to maximize the cache utilization.

In this study, we aim to develop an novel caching approach by fully utilizing the fluctuating differences among outputs of the diffusion model across timesteps. The primary challenge is: when can we reuse cached output to substitute the current timestep's output? Intuitively, this is possible when the current output is similar with the cached output, Fig. [2](#fig_0)(upper). Unfortunately, such difference is not predictable before the current output is computed. Consequently, without the guidance of difference, the uniformly cached outputs becomes redundant and the inference efficiency remains low.

To conquer this challenge, we propose Timestep Embedding Aware Cache (TeaCache), a training-free caching strategy. TeaCache leverages the following prior: There exists a strong correlation between a model's inputs and outputs. If a transformation relationship can be established between the input and output difference, one can utilizes the difference among inputs as an indicator of whether the corresponding outputs need to be cached, Fig. [2](#fig_0)(lower). Since inputs are readily accessible, this approach would significantly reduce computation cost. We then delve into the inputs of diffusion models: a noisy input, a timestep embedding, and a text embedding. The text embedding remains constant throughout the denoising process and cannot be used to measure the difference of input across timesteps. As for the timestep embedding, it changes as timesteps progress but is independent of the noisy input and text embedding, making it difficult to fully reflect the input information. The noisy input, on the other hand, is gradually updated during the denoising process and contains information from the text embedding, but it is not sensitive to timesteps. To accurately describe the model inputs and ensure their strong correlation with the outputs, TeaCache follows the inference process of diffusion and employ the timestep-embedding modulated noisy input as the final input embeddings, among which the difference are then used to estimated the output difference.

It is noteworthy that the input difference estimated above still exhibits a scaling bias relative to the output difference, which has been observed through empirical studies. That is because this strategy only captures the correlation trend between input difference and output difference. Considering that both input and output differences are already scalars, TeaCache further introduces a simple polynomial fitting procedure to estimate the scaling factors between them. With the correction of the scaling factors, the input difference can accurately reflect the output difference and is ultimately used as an indicator of whether the outputs need to be cached, Fig. [2](#fig_0)

## (lower).

The contributions of this paper include: • We propose TeaCache, a training-free approach which is completely compatible with DiT diffusion models, to estimate the difference of model outputs, selectively cache model outputs and speed up the inference process. • We propose a simple-yet-effective two-stage strategy to estimate the difference of model output through model input. The proposed strategy uses timestep-embedding modulated noisy input to perform coarse estimation and a polynomial fitting procedure for refinement. • TeaCache speeds up SOTA generation models, Open-Sora [[60]](#b58), Open-Sora-Plan [[21]](#b20), and Latte [[29]](#b28),

(PAB [[59]](#b57)) with large margins at negligible quality cost, Fig. [1](#).

## Related Work

## Diffusion Model

In the realm of generative models, diffusion models [[16,](#b15)[44]](#b42) have become foundational due to their exceptional ability to produce high-quality and diverse outputs. Initially developed with the U-Net architecture, these models have demonstrated impressive performance in image and video generation [[6, 7, 17, 33-35, 50, 52, 53]](#). However, the scalability of U-Net-based diffusion models is inherently constrained, posing challenges for applications requiring larger model capacities for enhanced performance. To address this limitation, Diffusion transformers (DiT) [[32]](#b31) represent a significant advancement. By utilizing the scalable architecture of transformers [[48]](#b46), DiT provides an effective means to increase model capacity. A notable achievement in this field is the advancement in generating long videos through the large-scale training of Sora [[31]](#b30), which employs a transformer-based Diffusion architecture for comprehensive simulations of the physical world. This underscores the considerable impact of scaling transformerbased Diffusion models. An increasing number of studies have adopted the Diffusion transformer as the noise estimation network [[8,](#b7)[9,](#b8)[21,](#b20)[29,](#b28)[56,](#b54)[60]](#b58).

## Diffusion Model Acceleration

Despite the notable performance of Diffusion models in image and video synthesis, their significant inference costs hinder practical applications. Efforts to accelerate Diffusion model inference fall into two primary categories. First, techniques such as DDIM [[45]](#b43) allow for fewer sampling steps without sacrificing quality. Additional research has focused on efficient ODE or SDE solvers [[19,](#b18)[20,](#b19)[26,](#b25)[27,](#b26)[46]](#b44), using pseudo numerical methods for faster sampling. Second, approaches include distillation [[36,](#b35)[51]](#b49), quantization [[15,](#b14)[25,](#b24)[39,](#b38)[43]](#b41), and distributed inference [[22]](#b21) are employed to reduce the workload and inference time.

However, these methods often demand additional resources for fine-tuning or optimization. Some training-free approaches [[5,](#b4)[49]](#b47) streamline the sampling process by reducing input tokens, thereby eliminating redundancy in image synthesis. Other methods reuse intermediate features between successive timesteps to avoid redundant computations [[42,](#b40)[54,](#b52)[58]](#b56). DeepCache [[55]](#b53) and Faster Diffusion [[23]](#b22) utilize feature caching to modify the UNet Diffusion, thus enhancing acceleration. FORA [[38]](#b37) and △-DiT [[11]](#b10) adapts this mechanism to DiT by caching residuals between attention layers. PAB [[59]](#b57) caches and broadcasts intermediate features at various timestep intervals based on different attention block characteristics for video synthesis. While these methods have improved Diffusion efficiency, enhancements for DiT in visual synthesis remain limited.

## Methodology

## Preliminaries

Denoising Diffusion Models. Diffusion models simulate visual generation through a sequence of iterative denoising steps. The core idea is to start with random noise and progressively refine it until it approximates a sample from the target distribution. During the forward diffusion process, Gaussian noise is incrementally added over T steps to a data point x 0 sampled from the real distribution q(x):

$x t = √ α t x t-1 + √ 1 -α t z t for t = 1, . . . , T(1)$where α t ∈ [0, 1] governs the noise level, and z t ∼ N (0, I) represents Gaussian noise. As t increases, x t becomes progressively noisier, ultimately resembling a normal distribution N (0, I) when t = T . The reverse diffusion process is designed to reconstruct the original data from its noisy counterpart:

$p θ (x t-1 | x t ) = N (x t-1 ; µ θ (x t , t), Σ θ (x t , t)),(2)$where µ θ and Σ θ are learned parameters defining the mean and covariance. Timestep Embedding in Diffusion Models. The diffusion procedures are usually splitted to one thousand timesteps during training phase and dozens of timesteps during inference phase. Timestep defines the strength of noise to be added or removed in the diffusion procedures, which is an important input of the diffusion model. Specifically, the scalar timestep t is firstly transformed to timestep embedding through sinusoidal embedding and multilayer perception module:

$T t = M LP (sinusoidal(t)) for t = 1, . . . , T. (3)$Timestep embedding then modulates the input and output of the Self Attention Layer and Feed Forward Network (FFN) in each Transformer block, as shown in Fig. [4](#). Thus, timestep embedding can significantly affect the magnitude of the model output.

## Analysis

To investigate the correlation between model output and input, we perform an in-depth analysis of their behaviors during the diffusion process.

Model outputs: Ideally, if we could obtain the model outputs in advance, we could directly measure the difference between outputs at adjacent timesteps and decide whether to cache the outputs based on their difference. Following [[54]](#b52), we use the relative L1 distance as our metric. For instance, the relative L1 distance L1 rel (O, t) for output embedding O t at timestep t is calculated as follows:

$L1 rel (O, t) = ∥O t -O t+1 ∥ 1 ∥O t+1 ∥ 1(4)$where a large L1 rel (O, t) indicates that O t is informative relative to O t+1 and should be cached; otherwise, a small L1 rel (O, t) indicates that O t+1 and O t are similar to each other and therefore O t+1 could be reused to replace O t .

Therefore, Eq. 4 can be used to define a criterion for determining whether the model outputs should be cached. However, in most cases, the model outputs cannot be obtained in advance, making the above approach infeasible. To address this issue, an intuitive idea is that if we can efficiently estimate the difference of the model outputs, we can leverage it to design a caching strategy. Fortunately, it is well-known that the model inputs and outputs are strongly correlated. Based on this insight, we analyzed the model inputs and conducted detailed experiments to investigate their correlation with the model outputs.

## Model inputs:

We consider the inputs of diffusion model: text embedding, timestep embedding, and noisy input, as shown in Fig. [4](#). Since the text embedding remains constant throughout the diffusion process, it cannot be used to measure the difference of inputs across timestep. Therefore, text embedding is excluded from analysis. As for the timestep embedding, it changes as timesteps progress but is independent of the noisy input and text embedding, making it difficult to fully reflect the information of the input. The noisy input, on the other hand, is gradually updated during the denoising process and contains information from the text embedding, but it is not sensitive to timesteps. To comprehensively represent the model inputs and ensure their correlation with the outputs, we ultimately utilized the timestep embedding modulated noisy input at the Transformer's input stage as the final input embedding, as illustrated in the Fig. [4](#).

Experimental analysis: To derive a robust conclusion, we make analysis using the metric defined in Eq. 4 to compute the difference of model inputs and outputs on three distinct video generation models: Open Sora [[60]](#b58), Latte [[29]](#b28), and OpenSora Plan [[21]](#b20). As illustrated in Fig. [3](#), the difference of outputs exhibit distinct patterns across various models. In Open Sora, the pattern forms a 'U' shape, whereas in Latte and OpenSora-Plan, it resembles a horizontally flipped 'L'. Additionally, OpenSora-Plan features multiple peaks because its scheduler samples certain timesteps twice. The noisy input across consecutive timesteps changes minimally and shows little correlation with the model output. In contrast, both the timestep embedding and the timestep embedding modulated noisy input demonstrate a strong correlation with the model output. Given that the timestep embedding modulated noisy inputs exhibits superior generation capabilities (e.g., in Open Sora) and effectively leverages the dynamics of input, we select it as the indicator to determine whether the model output at the current step is similar to that of the previous timestep. 

## TeaCache

As illustrated in Fig. [3](#), adjacent timesteps conduct redundant computations where model outputs exhibit minimal change. To minimize these redundancies and accelerate inference, we propose the Timestep Embedding Aware Cache (TeaCache). Rather than computing new outputs at each timestep, we reuse cached outputs from previous timesteps. Our caching technique can be applied to nearly all recent diffusion models based on Transformers. Naive Caching Strategy. To determine whether to reuse the cached model output from a previous timestep, we employ the accumulated relative L1 distance as an indicator.

$t b -1 t=ta L1 rel (F, t) ≤ δ < t b t=ta L1 rel (F, t)(5)$where L1 rel is defined in Eq. 4. F can be timestep embedding or timestep embedding modulated noisy inputs and δ is the caching threshold. Specifically, after computing the model output at timestep t a and caching it, we accumulate the relative L1 distance

$t b -1 t=ta L1 rel (F, t) for subsequent timesteps. If, at timestep t b (> t a ), t b -1$t=ta L1 rel (F, t) is less than the caching threshold δ, we reuse the cached model output; otherwise, we compute the new model output and set the accumulated relative L1 distance to zero. A smaller threshold δ results in more frequent refreshing of cached outputs, while a larger threshold speeds up visual generation but may adversely affect image appearance. The threshold δ should be chosen to enhance inference speed without compromising visual quality.

Rescaled Caching Strategy. Although timestep embedding modulated noisy inputs exhibit a strong correlation with model outputs, the differences in consecutive timesteps are inconsistent. Directly using the difference of timestep embedding-modulated noisy input to estimate model output difference leads to a scaling bias. Such bias may cause suboptimal timestep selection. Considering that these differences are scalars, we apply simple polynomial fitting to rescale them to reduce the bias. The polynomial fitting is then performed between model inputs (timestep embedding modulated noisy inputs) and outputs, which is formulated as

$y = f (x) = a 0 + a 1 x + a 2 x 2 + • • • + a n x n ,(6)$where y represents estimated difference of model output and x signifies the difference of timestep embeddingmodulated noisy inputs. This can be efficiently solved using the poly1d function from the numpy package. With polynomial fitting, the rescaled difference in timestep embeddingmodulated noisy inputs better estimates model output difference, as shown in Fig. [5](#). The final caching indicator is formulated as

$t b -1 t=ta f (L1 rel (F, t)) ≤ δ < t b t=ta f (L1 rel (F, t))(7)$
## Discussion

Caching Mechanism v.s. Reducing Timesteps. Assume that both of the caching mechanism and reducing timesteps stregegies reduces half of the timesteps. The differences between them can be concluded in three aspects: 

## Experiment

## Settings

Base Models and Compared Methods. To demonstrate the effectiveness of our method, we apply our acceleration technique to various video, such as Open-Sora 1.2 [[60]](#b58),

Open-Sora-Plan [[21]](#b20) and Latte [[29]](#b28). We compare our base models with recent efficient video synthesis techniques, including PAB [[59]](#b57), T-GATE [[58]](#b56) and ∆-DiT [[11]](#b10), to highlight the advantages of our approach. Notably, ∆-DiT and T-GATE are originally designed as an acceleration method for image synthesis; PAB adapted them for video synthesis to facilitate comparison.

Evaluation Metrics and Datasets. To assess the performance of video synthesis acceleration methods, we focus on two primary aspects: inference efficiency and visual quality. For evaluating inference efficiency, we use Floating Point Operations (FLOPs) and inference latency as metrics. For visual quality evaluation, we employ VBench [[18]](#b17), LPIPS [[57]](#b55), PSNR, and SSIM. VBench serves as a comprehensive benchmark suite for video generative models, aligning well with human perceptions and offering valuable insights from multiple perspectives. LPIPS, PSNR, and SSIM evaluate the similarity between videos produced by the accelerated sampling method and the original model. PSNR assesses pixel-level fidelity, LPIPS measures perceptual consistency, and SSIM evaluates structural similarity.  [[59]](#b57) in both visual quality and efficiency. Latency is evaluated on a single A800 GPU. Video generation specifications: Open-Sora [[60]](#b58) (51 frames, 480p), Latte [[29]](#b28) (16 frames, 512×512), Open-Sora-Plan [[21]](#b20) (65 frames , 512×512). Best-viewed with zoom-in.

Generally, higher similarity scores imply better fidelity and visual quality. The details of evaluation metrics are presented in Appendix.

Implementation Detail All experiments are carried out on the NVIDIA A800 80GB GPUs with Pytorch. We enable FlashAttention [[12]](#b11) by default for all experiments. To obtain robust polynomial fitting, we sample 70 texts from T2V-CompBench [[47]](#b45) to generate videos, assessing seven desired attributes of generated videos. 10 prompts are sampled for each attributes. δ is 0.1 for TeaCache-slow and 0.2 for TeaCache-fast.

## Main Results

Quantitative Comparison. Tab. 1 presents a quantitative evaluation of efficiency and visual quality using the VBench benchmark [[18]](#b17). We examine two variants of TeaCache: a slow variant and a fast variant with greater speedup. Compared to other training-free acceleration methods, TeaCache consistently achieves superior efficiency and better visual quality across different base models, sampling schedulers, video resolutions, and lengths. In evaluating the Latte [[29]](#b28) baseline, the TeaCache-slow model demonstrates superior performance across all visual quality metrics, achieving a 1.86× speedup compared to PAB [[59]](#b57), which provides a 1.34× speedup. TeaCache-fast achieves the highest acceleration at 3.28×, albeit with a slight reduction in visual quality. With the OpenSora [[60]](#b58) baseline, we obtain the optimal speedup of 2.25× as compared to the previous 1.40×, and the highest overall quality with a speedup of 1.55×. Additionally, using Open-Sora-Plan [[21]](#b20), TeaCache achieves the highest speedup of 6.83×, surpassing the previously best 1.49× offered by PAB, while also delivering the highest quality at a speedup of 4.41×.

Visualization. Fig. [7](#fig_3) compares the videos generated by TeaCache against those by the original model and PAB. The results demonstrate that TeaCache outperforms PAB in visual quality with lower latency. More visual results can be found in the Appendix.

## Ablation Studies

Scaling to multiple GPUs. Aligned with previous research employing Dynamic Sequence Parallelism (DSP) [[59]](#b57) for supporting high-resolution long-video generation across multiple GPUs, we assess the performance of TeaCache in these scenarios. The results of this study are presented in Tab. 4. We utilize Open-Sora [[60]](#b58) (480p -192 frames at 30 timesteps) and Open-Sora-Plan [[21]](#b20) (512×512 -221 frames at 150 timesteps) as baselines and compare them against the prior method PAB [[59]](#b57) regarding latency measurements on A800 GPUs. As the number of GPUs increases, TeaCache consistently improves inference speed across various base models and outperforms PAB.

Performance at different Length and Resolution. To assess the effectiveness of our method in accelerating sam-

Original 1 GPU 2 GPU 4 GPU 8 GPU 0 5 10 15 20 25 30 35 40 45 42.49 28.51 12.18 7.81 4.54 1.49x 3.49x 5.44x 9.36x (a) 480P, 48 frames Original 1 GPU 2 GPU 4 GPU 8 GPU 0 25 50 75 100 125 150 175 200 188.87 114.01 47.03 24.64 14.41 1.66x 4.02x 7.67x 13.1x (b) 480P, 192 frames Original 1 GPU 2 GPU 4 GPU 8 GPU 0 20 40 60 80 100 120 140 129.68 86.63 31.28 16.45 9.51 1.5x 4.16x 7.88x 13.64x (c) 360P, 240 frames Original 1 GPU 2 GPU 4 GPU 8 GPU 0 20 40 60 80 100 106.29 67.53 27.77 16.36 8.77 1.57x 3.82x 6.49x 12.11x (d) 720P, 48 frames pling for videos with varying sizes, we perform tests across different video lengths and resolutions. The results, presented in Fig. [8](#fig_4), demonstrate that our method sustains consistent acceleration performance, even with increases in video resolution and frame count. This consistency highlights the method's potential to accelerate sampling processes for longer and higher-resolution videos, meeting practical demands. Quality-Efficiency trade-off. In Fig. [1](#), we compare the quality-latency trade-off of TeaCache with PAB [[59]](#b57). Our analysis reveals that TeaCache achieves significantly higher reduction rates, indicated by lower absolute latency, compared to PAB. Additionally, across a wide range of latency configurations, TeaCache consistently outperforms PAB on all quality metrics. This is particularly evident in the reference-free metric VBench score [[18]](#b17), which aligns more closely with human preferences. Although there is a decline in reference-based scores such as PSNR and SSIM at extreme reduction rates, qualitative results suggest that the outputs remain satisfactory, despite not perfectly matching the reference.

Choice of Indicator. When determining the caching schedule, we evaluate various indicators to estimate the differences in model outputs across consecutive timesteps. These indicators include timestep embedding and timestep embedding-modulated noisy input. As illustrated in Fig. [3](#), the timestep embedding-modulated noisy input demonstrates a stronger correlation with model output compared to the timestep embedding, particularly in the Open-Sora. Moreover, the selection of timesteps by the timestep embedding-modulated noisy input adapts dynamically to different prompts, whereas the timestep embedding selects the same timesteps for all prompts. This observation is validated by the results presented in Tab. 2, where the timestep embedding-modulated noisy input consistently surpasses the timestep embedding across various models, especially in OpenSora.

Effect of Rescaling. Tab.3 illustrates the impact of rescaling. A first-order polynomial fitting outperforms the original data by 0.24% under Vbench score metric, as well as in LPIPS, SSIM, and PSNR metrics. Performance gains  tend to saturate with a fourth-order polynomial fitting.

## Conclusion

In this study, we introduce TeaCache, a novel, training-free approach designed to significantly accelerate video synthesis inference while maintaining high-quality output. We analyze the correlation between model input and output, observing that similarity of timsetep embedding modulated noisy input in consecutive timesteps shows strong correlation with similarity of model output. We propose to utilize similarity of timsetep embedding modulated noisy input as an indicator of output similarity, allowing for dynamic caching of model outputs. Further, we propose a rescaling strategy to refine the estimation of model output similarity, optimizing the selection process for timestep caching.

Extensive experiments demonstrate TeaCache's robust performance in terms of both efficiency and visual quality across diverse video generation models and image generation models, sampling schedules, video lengths, and resolutions, underscoring its potential for real-world applications.

![Figure 2. Comparison of the proposed TeaCache and the conventional uniform caching strategy for DiT models during inference. TeaCache is capable of selectively caching informative intermediate model outputs during the inference process, and therefore accelerates the DiT models while maintaining its performance. F and t respectively denote the model inputs of noisy input and timestep embedding. L1 rel and f are difference estimation functions of model inputs. δ is an indicator threshold of whether to cache a model output or not.]()

![Figure 3. Visualization of input differences and output differences in consecutive timesteps of Open Sora, Latte, and OpenSora-Plan. Timestep embedding and Timestep embedding modulated noisy input have strong correlation with model output.]()

![Figure 5. Visualization of corelation of input differences and output differences in consecutive timesteps of Open Sora, Latte, and OpenSora-Plan. The original data points deviate a lot from the linear corelation. Polynomial fitting reduces the gap.]()

![Figure 7. Comparison of visual quality and efficiency (denoted by latency) with the competing method. TeaCache outperforms PAB[59] in both visual quality and efficiency. Latency is evaluated on a single A800 GPU. Video generation specifications: Open-Sora[60] (51 frames, 480p), Latte[29] (16 frames, 512×512), Open-Sora-Plan[21] (65 frames , 512×512). Best-viewed with zoom-in.]()

![Figure 8. Inference efficiency and visual quality of TeaCache at different video lengths and resolutions.]()

![]()

![Quantitative evaluation of inference efficiency and visual quality in video generation models. TeaCache consistently achieves superior efficiency and better visual quality across different base models, sampling schedulers, video resolutions, and lengths.Speedup ↑ Latency (s) ↓ VBench ↑ LPIPS ↓ SSIM ↑ PSNR ↑]()

![Ablation study of caching indicator. 'Timestep': timestep embedding. 'Input': timestep embedding-modulated noisy input.]()

![Ablation study of polynomial fitting. Rescaling with polynomial fitting outperforms original data. Higher-order fitting obtains better performance and saturates in 4-order fitting.]()

![Inference efficiency and visual quality when scaling to multiple GPUs with Dynamic Sequence Parallelism (DSP).]()

