<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Timestep Embedding Tells: It&apos;s Time to Cache for Video Diffusion Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-28">28 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujie</forename><surname>Wei</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haonan</forename><surname>Qiu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Nanyang Technological University Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuzhong</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Timestep Embedding Tells: It&apos;s Time to Cache for Video Diffusion Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-28">28 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">9A97915EF4CACCA15118320DF68EBF2D</idno>
					<idno type="arXiv">arXiv:2411.19108v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Open-Sora-Plan Original (w/o Speed up) PAB TeaCache (Ours) Open-Sora Latte Latency: 99</term>
					<term>65s Latency: 73</term>
					<term>41s Latency: 22</term>
					<term>62s (Speed up 4</term>
					<term>41x) Latency: 44</term>
					<term>56s Latency: 31</term>
					<term>85s Latency: 28</term>
					<term>78s (Speed up 1</term>
					<term>55x) Latency: 26</term>
					<term>90s Latency: 19</term>
					<term>98s Latency: 14</term>
					<term>46s (Speed up 1</term>
					<term>86x)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1. Quality-latency comparison of video diffusion models. Visual quality versus latency curves of the proposed TeaCache approach and PAB [59] using Latte [29]. TeaCache significantly outperforms PAB in both visual quality and efficiency. Latency is evaluated on a single A800 GPU for 16-frame video generation under 512 × 512 resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the emergence of diffusion models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b44">46]</ref>, as a fundamental backbone for visual generation. The model architecture has evolved from U-Net <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> to diffusion transformers (DiT) <ref type="bibr" target="#b31">[32]</ref>, which greatly increased model capacities. Empowered by DiT, video generation models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b58">60]</ref> have reached a groundbreaking level. Despite of the substantial efficacy of these powerful models, their inference speed remains a pivotal impediment to wider adoption <ref type="bibr" target="#b23">[24]</ref>. This core limitation arises from the sequential denoising procedure inherent to their reverse phase, which inhibits parallel decoding <ref type="bibr">[40]</ref>. Moreover, as model parameters scale up and the requirements for higher resolution and longer durations of videos escalate <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">56]</ref>, the inference process experiences a further decline in speed.</p><p>To accelerate the visual generation procedure, distillation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">51]</ref> and post-training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> are employed. However, these methods typically require extra training, which implies substantial computational cost and data resources. An alternative technological pathway is to leverage the caching mechanism <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">41]</ref>, which does not require additional training to maintain the performance of diffusion models. These methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b57">59</ref>] find that the model outputs are similar between the consecutive timesteps when denoising and propose to reduce redundancy by caching model outputs in a uniform way, Fig. <ref type="figure" target="#fig_0">2</ref>(upper). Nevertheless, when the output difference between consecutive timesteps varies, the uniform caching strategy lacks flexibility to maximize the cache utilization.</p><p>In this study, we aim to develop an novel caching approach by fully utilizing the fluctuating differences among outputs of the diffusion model across timesteps. The primary challenge is: when can we reuse cached output to substitute the current timestep's output? Intuitively, this is possible when the current output is similar with the cached output, Fig. <ref type="figure" target="#fig_0">2</ref>(upper). Unfortunately, such difference is not predictable before the current output is computed. Consequently, without the guidance of difference, the uniformly cached outputs becomes redundant and the inference efficiency remains low.</p><p>To conquer this challenge, we propose Timestep Embedding Aware Cache (TeaCache), a training-free caching strategy. TeaCache leverages the following prior: There exists a strong correlation between a model's inputs and outputs. If a transformation relationship can be established between the input and output difference, one can utilizes the difference among inputs as an indicator of whether the corresponding outputs need to be cached, Fig. <ref type="figure" target="#fig_0">2</ref>(lower). Since inputs are readily accessible, this approach would significantly reduce computation cost. We then delve into the inputs of diffusion models: a noisy input, a timestep embedding, and a text embedding. The text embedding remains constant throughout the denoising process and cannot be used to measure the difference of input across timesteps. As for the timestep embedding, it changes as timesteps progress but is independent of the noisy input and text embedding, making it difficult to fully reflect the input information. The noisy input, on the other hand, is gradually updated during the denoising process and contains information from the text embedding, but it is not sensitive to timesteps. To accurately describe the model inputs and ensure their strong correlation with the outputs, TeaCache follows the inference process of diffusion and employ the timestep-embedding modulated noisy input as the final input embeddings, among which the difference are then used to estimated the output difference.</p><p>It is noteworthy that the input difference estimated above still exhibits a scaling bias relative to the output difference, which has been observed through empirical studies. That is because this strategy only captures the correlation trend between input difference and output difference. Considering that both input and output differences are already scalars, TeaCache further introduces a simple polynomial fitting procedure to estimate the scaling factors between them. With the correction of the scaling factors, the input difference can accurately reflect the output difference and is ultimately used as an indicator of whether the outputs need to be cached, Fig. <ref type="figure" target="#fig_0">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(lower).</head><p>The contributions of this paper include: • We propose TeaCache, a training-free approach which is completely compatible with DiT diffusion models, to estimate the difference of model outputs, selectively cache model outputs and speed up the inference process. • We propose a simple-yet-effective two-stage strategy to estimate the difference of model output through model input. The proposed strategy uses timestep-embedding modulated noisy input to perform coarse estimation and a polynomial fitting procedure for refinement. • TeaCache speeds up SOTA generation models, Open-Sora <ref type="bibr" target="#b58">[60]</ref>, Open-Sora-Plan <ref type="bibr" target="#b20">[21]</ref>, and Latte <ref type="bibr" target="#b28">[29]</ref>,</p><p>(PAB <ref type="bibr" target="#b57">[59]</ref>) with large margins at negligible quality cost, Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Diffusion Model</head><p>In the realm of generative models, diffusion models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">44]</ref> have become foundational due to their exceptional ability to produce high-quality and diverse outputs. Initially developed with the U-Net architecture, these models have demonstrated impressive performance in image and video generation <ref type="bibr">[6, 7, 17, 33-35, 50, 52, 53]</ref>. However, the scalability of U-Net-based diffusion models is inherently constrained, posing challenges for applications requiring larger model capacities for enhanced performance. To address this limitation, Diffusion transformers (DiT) <ref type="bibr" target="#b31">[32]</ref> represent a significant advancement. By utilizing the scalable architecture of transformers <ref type="bibr" target="#b46">[48]</ref>, DiT provides an effective means to increase model capacity. A notable achievement in this field is the advancement in generating long videos through the large-scale training of Sora <ref type="bibr" target="#b30">[31]</ref>, which employs a transformer-based Diffusion architecture for comprehensive simulations of the physical world. This underscores the considerable impact of scaling transformerbased Diffusion models. An increasing number of studies have adopted the Diffusion transformer as the noise estimation network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b58">60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Diffusion Model Acceleration</head><p>Despite the notable performance of Diffusion models in image and video synthesis, their significant inference costs hinder practical applications. Efforts to accelerate Diffusion model inference fall into two primary categories. First, techniques such as DDIM <ref type="bibr" target="#b43">[45]</ref> allow for fewer sampling steps without sacrificing quality. Additional research has focused on efficient ODE or SDE solvers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">46]</ref>, using pseudo numerical methods for faster sampling. Second, approaches include distillation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">51]</ref>, quantization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">43]</ref>, and distributed inference <ref type="bibr" target="#b21">[22]</ref> are employed to reduce the workload and inference time.</p><p>However, these methods often demand additional resources for fine-tuning or optimization. Some training-free approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">49]</ref> streamline the sampling process by reducing input tokens, thereby eliminating redundancy in image synthesis. Other methods reuse intermediate features between successive timesteps to avoid redundant computations <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b56">58]</ref>. DeepCache <ref type="bibr" target="#b53">[55]</ref> and Faster Diffusion <ref type="bibr" target="#b22">[23]</ref> utilize feature caching to modify the UNet Diffusion, thus enhancing acceleration. FORA <ref type="bibr" target="#b37">[38]</ref> and △-DiT <ref type="bibr" target="#b10">[11]</ref> adapts this mechanism to DiT by caching residuals between attention layers. PAB <ref type="bibr" target="#b57">[59]</ref> caches and broadcasts intermediate features at various timestep intervals based on different attention block characteristics for video synthesis. While these methods have improved Diffusion efficiency, enhancements for DiT in visual synthesis remain limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Denoising Diffusion Models. Diffusion models simulate visual generation through a sequence of iterative denoising steps. The core idea is to start with random noise and progressively refine it until it approximates a sample from the target distribution. During the forward diffusion process, Gaussian noise is incrementally added over T steps to a data point x 0 sampled from the real distribution q(x):</p><formula xml:id="formula_0">x t = √ α t x t-1 + √ 1 -α t z t for t = 1, . . . , T<label>(1)</label></formula><p>where α t ∈ [0, 1] governs the noise level, and z t ∼ N (0, I) represents Gaussian noise. As t increases, x t becomes progressively noisier, ultimately resembling a normal distribution N (0, I) when t = T . The reverse diffusion process is designed to reconstruct the original data from its noisy counterpart:</p><formula xml:id="formula_1">p θ (x t-1 | x t ) = N (x t-1 ; µ θ (x t , t), Σ θ (x t , t)),<label>(2)</label></formula><p>where µ θ and Σ θ are learned parameters defining the mean and covariance. Timestep Embedding in Diffusion Models. The diffusion procedures are usually splitted to one thousand timesteps during training phase and dozens of timesteps during inference phase. Timestep defines the strength of noise to be added or removed in the diffusion procedures, which is an important input of the diffusion model. Specifically, the scalar timestep t is firstly transformed to timestep embedding through sinusoidal embedding and multilayer perception module:</p><formula xml:id="formula_2">T t = M LP (sinusoidal(t)) for t = 1, . . . , T. (3)</formula><p>Timestep embedding then modulates the input and output of the Self Attention Layer and Feed Forward Network (FFN) in each Transformer block, as shown in Fig. <ref type="figure">4</ref>. Thus, timestep embedding can significantly affect the magnitude of the model output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis</head><p>To investigate the correlation between model output and input, we perform an in-depth analysis of their behaviors during the diffusion process.</p><p>Model outputs: Ideally, if we could obtain the model outputs in advance, we could directly measure the difference between outputs at adjacent timesteps and decide whether to cache the outputs based on their difference. Following <ref type="bibr" target="#b52">[54]</ref>, we use the relative L1 distance as our metric. For instance, the relative L1 distance L1 rel (O, t) for output embedding O t at timestep t is calculated as follows:</p><formula xml:id="formula_3">L1 rel (O, t) = ∥O t -O t+1 ∥ 1 ∥O t+1 ∥ 1<label>(4)</label></formula><p>where a large L1 rel (O, t) indicates that O t is informative relative to O t+1 and should be cached; otherwise, a small L1 rel (O, t) indicates that O t+1 and O t are similar to each other and therefore O t+1 could be reused to replace O t .</p><p>Therefore, Eq. 4 can be used to define a criterion for determining whether the model outputs should be cached. However, in most cases, the model outputs cannot be obtained in advance, making the above approach infeasible. To address this issue, an intuitive idea is that if we can efficiently estimate the difference of the model outputs, we can leverage it to design a caching strategy. Fortunately, it is well-known that the model inputs and outputs are strongly correlated. Based on this insight, we analyzed the model inputs and conducted detailed experiments to investigate their correlation with the model outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model inputs:</head><p>We consider the inputs of diffusion model: text embedding, timestep embedding, and noisy input, as shown in Fig. <ref type="figure">4</ref>. Since the text embedding remains constant throughout the diffusion process, it cannot be used to measure the difference of inputs across timestep. Therefore, text embedding is excluded from analysis. As for the timestep embedding, it changes as timesteps progress but is independent of the noisy input and text embedding, making it difficult to fully reflect the information of the input. The noisy input, on the other hand, is gradually updated during the denoising process and contains information from the text embedding, but it is not sensitive to timesteps. To comprehensively represent the model inputs and ensure their correlation with the outputs, we ultimately utilized the timestep embedding modulated noisy input at the Transformer's input stage as the final input embedding, as illustrated in the Fig. <ref type="figure">4</ref>.</p><p>Experimental analysis: To derive a robust conclusion, we make analysis using the metric defined in Eq. 4 to compute the difference of model inputs and outputs on three distinct video generation models: Open Sora <ref type="bibr" target="#b58">[60]</ref>, Latte <ref type="bibr" target="#b28">[29]</ref>, and OpenSora Plan <ref type="bibr" target="#b20">[21]</ref>. As illustrated in Fig. <ref type="figure">3</ref>, the difference of outputs exhibit distinct patterns across various models. In Open Sora, the pattern forms a 'U' shape, whereas in Latte and OpenSora-Plan, it resembles a horizontally flipped 'L'. Additionally, OpenSora-Plan features multiple peaks because its scheduler samples certain timesteps twice. The noisy input across consecutive timesteps changes minimally and shows little correlation with the model output. In contrast, both the timestep embedding and the timestep embedding modulated noisy input demonstrate a strong correlation with the model output. Given that the timestep embedding modulated noisy inputs exhibits superior generation capabilities (e.g., in Open Sora) and effectively leverages the dynamics of input, we select it as the indicator to determine whether the model output at the current step is similar to that of the previous timestep. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TeaCache</head><p>As illustrated in Fig. <ref type="figure">3</ref>, adjacent timesteps conduct redundant computations where model outputs exhibit minimal change. To minimize these redundancies and accelerate inference, we propose the Timestep Embedding Aware Cache (TeaCache). Rather than computing new outputs at each timestep, we reuse cached outputs from previous timesteps. Our caching technique can be applied to nearly all recent diffusion models based on Transformers. Naive Caching Strategy. To determine whether to reuse the cached model output from a previous timestep, we employ the accumulated relative L1 distance as an indicator.</p><formula xml:id="formula_4">t b -1 t=ta L1 rel (F, t) ≤ δ &lt; t b t=ta L1 rel (F, t)<label>(5)</label></formula><p>where L1 rel is defined in Eq. 4. F can be timestep embedding or timestep embedding modulated noisy inputs and δ is the caching threshold. Specifically, after computing the model output at timestep t a and caching it, we accumulate the relative L1 distance</p><formula xml:id="formula_5">t b -1 t=ta L1 rel (F, t) for subsequent timesteps. If, at timestep t b (&gt; t a ), t b -1</formula><p>t=ta L1 rel (F, t) is less than the caching threshold δ, we reuse the cached model output; otherwise, we compute the new model output and set the accumulated relative L1 distance to zero. A smaller threshold δ results in more frequent refreshing of cached outputs, while a larger threshold speeds up visual generation but may adversely affect image appearance. The threshold δ should be chosen to enhance inference speed without compromising visual quality.</p><p>Rescaled Caching Strategy. Although timestep embedding modulated noisy inputs exhibit a strong correlation with model outputs, the differences in consecutive timesteps are inconsistent. Directly using the difference of timestep embedding-modulated noisy input to estimate model output difference leads to a scaling bias. Such bias may cause suboptimal timestep selection. Considering that these differences are scalars, we apply simple polynomial fitting to rescale them to reduce the bias. The polynomial fitting is then performed between model inputs (timestep embedding modulated noisy inputs) and outputs, which is formulated as</p><formula xml:id="formula_6">y = f (x) = a 0 + a 1 x + a 2 x 2 + • • • + a n x n ,<label>(6)</label></formula><p>where y represents estimated difference of model output and x signifies the difference of timestep embeddingmodulated noisy inputs. This can be efficiently solved using the poly1d function from the numpy package. With polynomial fitting, the rescaled difference in timestep embeddingmodulated noisy inputs better estimates model output difference, as shown in Fig. <ref type="figure">5</ref>. The final caching indicator is formulated as</p><formula xml:id="formula_7">t b -1 t=ta f (L1 rel (F, t)) ≤ δ &lt; t b t=ta f (L1 rel (F, t))<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Caching Mechanism v.s. Reducing Timesteps. Assume that both of the caching mechanism and reducing timesteps stregegies reduces half of the timesteps. The differences between them can be concluded in three aspects: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Base Models and Compared Methods. To demonstrate the effectiveness of our method, we apply our acceleration technique to various video, such as Open-Sora 1.2 <ref type="bibr" target="#b58">[60]</ref>,</p><p>Open-Sora-Plan <ref type="bibr" target="#b20">[21]</ref> and Latte <ref type="bibr" target="#b28">[29]</ref>. We compare our base models with recent efficient video synthesis techniques, including PAB <ref type="bibr" target="#b57">[59]</ref>, T-GATE <ref type="bibr" target="#b56">[58]</ref> and ∆-DiT <ref type="bibr" target="#b10">[11]</ref>, to highlight the advantages of our approach. Notably, ∆-DiT and T-GATE are originally designed as an acceleration method for image synthesis; PAB adapted them for video synthesis to facilitate comparison.</p><p>Evaluation Metrics and Datasets. To assess the performance of video synthesis acceleration methods, we focus on two primary aspects: inference efficiency and visual quality. For evaluating inference efficiency, we use Floating Point Operations (FLOPs) and inference latency as metrics. For visual quality evaluation, we employ VBench <ref type="bibr" target="#b17">[18]</ref>, LPIPS <ref type="bibr" target="#b55">[57]</ref>, PSNR, and SSIM. VBench serves as a comprehensive benchmark suite for video generative models, aligning well with human perceptions and offering valuable insights from multiple perspectives. LPIPS, PSNR, and SSIM evaluate the similarity between videos produced by the accelerated sampling method and the original model. PSNR assesses pixel-level fidelity, LPIPS measures perceptual consistency, and SSIM evaluates structural similarity.  <ref type="bibr" target="#b57">[59]</ref> in both visual quality and efficiency. Latency is evaluated on a single A800 GPU. Video generation specifications: Open-Sora <ref type="bibr" target="#b58">[60]</ref> (51 frames, 480p), Latte <ref type="bibr" target="#b28">[29]</ref> (16 frames, 512×512), Open-Sora-Plan <ref type="bibr" target="#b20">[21]</ref> (65 frames , 512×512). Best-viewed with zoom-in.</p><p>Generally, higher similarity scores imply better fidelity and visual quality. The details of evaluation metrics are presented in Appendix.</p><p>Implementation Detail All experiments are carried out on the NVIDIA A800 80GB GPUs with Pytorch. We enable FlashAttention <ref type="bibr" target="#b11">[12]</ref> by default for all experiments. To obtain robust polynomial fitting, we sample 70 texts from T2V-CompBench <ref type="bibr" target="#b45">[47]</ref> to generate videos, assessing seven desired attributes of generated videos. 10 prompts are sampled for each attributes. δ is 0.1 for TeaCache-slow and 0.2 for TeaCache-fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>Quantitative Comparison. Tab. 1 presents a quantitative evaluation of efficiency and visual quality using the VBench benchmark <ref type="bibr" target="#b17">[18]</ref>. We examine two variants of TeaCache: a slow variant and a fast variant with greater speedup. Compared to other training-free acceleration methods, TeaCache consistently achieves superior efficiency and better visual quality across different base models, sampling schedulers, video resolutions, and lengths. In evaluating the Latte <ref type="bibr" target="#b28">[29]</ref> baseline, the TeaCache-slow model demonstrates superior performance across all visual quality metrics, achieving a 1.86× speedup compared to PAB <ref type="bibr" target="#b57">[59]</ref>, which provides a 1.34× speedup. TeaCache-fast achieves the highest acceleration at 3.28×, albeit with a slight reduction in visual quality. With the OpenSora <ref type="bibr" target="#b58">[60]</ref> baseline, we obtain the optimal speedup of 2.25× as compared to the previous 1.40×, and the highest overall quality with a speedup of 1.55×. Additionally, using Open-Sora-Plan <ref type="bibr" target="#b20">[21]</ref>, TeaCache achieves the highest speedup of 6.83×, surpassing the previously best 1.49× offered by PAB, while also delivering the highest quality at a speedup of 4.41×.</p><p>Visualization. Fig. <ref type="figure" target="#fig_3">7</ref> compares the videos generated by TeaCache against those by the original model and PAB. The results demonstrate that TeaCache outperforms PAB in visual quality with lower latency. More visual results can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Scaling to multiple GPUs. Aligned with previous research employing Dynamic Sequence Parallelism (DSP) <ref type="bibr" target="#b57">[59]</ref> for supporting high-resolution long-video generation across multiple GPUs, we assess the performance of TeaCache in these scenarios. The results of this study are presented in Tab. 4. We utilize Open-Sora <ref type="bibr" target="#b58">[60]</ref> (480p -192 frames at 30 timesteps) and Open-Sora-Plan <ref type="bibr" target="#b20">[21]</ref> (512×512 -221 frames at 150 timesteps) as baselines and compare them against the prior method PAB <ref type="bibr" target="#b57">[59]</ref> regarding latency measurements on A800 GPUs. As the number of GPUs increases, TeaCache consistently improves inference speed across various base models and outperforms PAB.</p><p>Performance at different Length and Resolution. To assess the effectiveness of our method in accelerating sam-</p><p>Original 1 GPU 2 GPU 4 GPU 8 GPU 0 5 10 15 20 25 30 35 40 45 42.49 28.51 12.18 7.81 4.54 1.49x 3.49x 5.44x 9.36x (a) 480P, 48 frames Original 1 GPU 2 GPU 4 GPU 8 GPU 0 25 50 75 100 125 150 175 200 188.87 114.01 47.03 24.64 14.41 1.66x 4.02x 7.67x 13.1x (b) 480P, 192 frames Original 1 GPU 2 GPU 4 GPU 8 GPU 0 20 40 60 80 100 120 140 129.68 86.63 31.28 16.45 9.51 1.5x 4.16x 7.88x 13.64x (c) 360P, 240 frames Original 1 GPU 2 GPU 4 GPU 8 GPU 0 20 40 60 80 100 106.29 67.53 27.77 16.36 8.77 1.57x 3.82x 6.49x 12.11x (d) 720P, 48 frames pling for videos with varying sizes, we perform tests across different video lengths and resolutions. The results, presented in Fig. <ref type="figure" target="#fig_4">8</ref>, demonstrate that our method sustains consistent acceleration performance, even with increases in video resolution and frame count. This consistency highlights the method's potential to accelerate sampling processes for longer and higher-resolution videos, meeting practical demands. Quality-Efficiency trade-off. In Fig. <ref type="figure">1</ref>, we compare the quality-latency trade-off of TeaCache with PAB <ref type="bibr" target="#b57">[59]</ref>. Our analysis reveals that TeaCache achieves significantly higher reduction rates, indicated by lower absolute latency, compared to PAB. Additionally, across a wide range of latency configurations, TeaCache consistently outperforms PAB on all quality metrics. This is particularly evident in the reference-free metric VBench score <ref type="bibr" target="#b17">[18]</ref>, which aligns more closely with human preferences. Although there is a decline in reference-based scores such as PSNR and SSIM at extreme reduction rates, qualitative results suggest that the outputs remain satisfactory, despite not perfectly matching the reference.</p><p>Choice of Indicator. When determining the caching schedule, we evaluate various indicators to estimate the differences in model outputs across consecutive timesteps. These indicators include timestep embedding and timestep embedding-modulated noisy input. As illustrated in Fig. <ref type="figure">3</ref>, the timestep embedding-modulated noisy input demonstrates a stronger correlation with model output compared to the timestep embedding, particularly in the Open-Sora. Moreover, the selection of timesteps by the timestep embedding-modulated noisy input adapts dynamically to different prompts, whereas the timestep embedding selects the same timesteps for all prompts. This observation is validated by the results presented in Tab. 2, where the timestep embedding-modulated noisy input consistently surpasses the timestep embedding across various models, especially in OpenSora.</p><p>Effect of Rescaling. Tab.3 illustrates the impact of rescaling. A first-order polynomial fitting outperforms the original data by 0.24% under Vbench score metric, as well as in LPIPS, SSIM, and PSNR metrics. Performance gains  tend to saturate with a fourth-order polynomial fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we introduce TeaCache, a novel, training-free approach designed to significantly accelerate video synthesis inference while maintaining high-quality output. We analyze the correlation between model input and output, observing that similarity of timsetep embedding modulated noisy input in consecutive timesteps shows strong correlation with similarity of model output. We propose to utilize similarity of timsetep embedding modulated noisy input as an indicator of output similarity, allowing for dynamic caching of model outputs. Further, we propose a rescaling strategy to refine the estimation of model output similarity, optimizing the selection process for timestep caching.</p><p>Extensive experiments demonstrate TeaCache's robust performance in terms of both efficiency and visual quality across diverse video generation models and image generation models, sampling schedules, video lengths, and resolutions, underscoring its potential for real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison of the proposed TeaCache and the conventional uniform caching strategy for DiT models during inference. TeaCache is capable of selectively caching informative intermediate model outputs during the inference process, and therefore accelerates the DiT models while maintaining its performance. F and t respectively denote the model inputs of noisy input and timestep embedding. L1 rel and f are difference estimation functions of model inputs. δ is an indicator threshold of whether to cache a model output or not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Visualization of input differences and output differences in consecutive timesteps of Open Sora, Latte, and OpenSora-Plan. Timestep embedding and Timestep embedding modulated noisy input have strong correlation with model output.</figDesc><graphic coords="4,106.75,72.00,118.79,118.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Visualization of corelation of input differences and output differences in consecutive timesteps of Open Sora, Latte, and OpenSora-Plan. The original data points deviate a lot from the linear corelation. Polynomial fitting reduces the gap.</figDesc><graphic coords="5,106.75,72.00,118.79,118.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of visual quality and efficiency (denoted by latency) with the competing method. TeaCache outperforms PAB<ref type="bibr" target="#b57">[59]</ref> in both visual quality and efficiency. Latency is evaluated on a single A800 GPU. Video generation specifications: Open-Sora<ref type="bibr" target="#b58">[60]</ref> (51 frames, 480p), Latte<ref type="bibr" target="#b28">[29]</ref> (16 frames, 512×512), Open-Sora-Plan<ref type="bibr" target="#b20">[21]</ref> (65 frames , 512×512). Best-viewed with zoom-in.</figDesc><graphic coords="7,73.41,241.77,78.66,78.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Inference efficiency and visual quality of TeaCache at different video lengths and resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="1,58.50,234.73,495.00,123.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of inference efficiency and visual quality in video generation models. TeaCache consistently achieves superior efficiency and better visual quality across different base models, sampling schedulers, video resolutions, and lengths.Speedup ↑ Latency (s) ↓ VBench ↑ LPIPS ↓ SSIM ↑ PSNR ↑</figDesc><table><row><cell>Method</cell><cell cols="3">Efficiency FLOPs (P) ↓ Latte (16 frames, 512×512)</cell><cell></cell><cell cols="2">Visual Quality</cell><cell></cell></row><row><cell>Latte (T = 50)</cell><cell>3.36</cell><cell>1×</cell><cell>26.90</cell><cell>77.40%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>∆-DiT [11]</cell><cell>3.36</cell><cell>1.02×</cell><cell>-</cell><cell>52.00%</cell><cell>0.8513</cell><cell>0.1078</cell><cell>8.65</cell></row><row><cell>T-GATE [58]</cell><cell>2.99</cell><cell>1.13×</cell><cell>-</cell><cell>75.42%</cell><cell>0.2612</cell><cell>0.6927</cell><cell>19.55</cell></row><row><cell>PAB-slow [59]</cell><cell>2.70</cell><cell>1.21×</cell><cell>22.16</cell><cell>76.32%</cell><cell>0.2669</cell><cell>0.7014</cell><cell>19.71</cell></row><row><cell>PAB-fast [59]</cell><cell>2.52</cell><cell>1.34×</cell><cell>19.98</cell><cell>73.13%</cell><cell>0.3903</cell><cell>0.6421</cell><cell>17.16</cell></row><row><cell>TeaCache-slow</cell><cell>1.86</cell><cell>1.86 ×</cell><cell>14.46</cell><cell>77.40%</cell><cell>0.1901</cell><cell>0.7786</cell><cell>22.09</cell></row><row><cell>TeaCache-fast</cell><cell>1.12</cell><cell>3.28 ×</cell><cell>8.20</cell><cell>76.69%</cell><cell>0.3133</cell><cell>0.6678</cell><cell>18.62</cell></row><row><cell></cell><cell></cell><cell cols="2">Open-Sora 1.2 (51 frames, 480P)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Open-Sora 1.2 (T = 30)</cell><cell>3.15</cell><cell>1×</cell><cell>44.56</cell><cell>79.22%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>∆-DiT [11]</cell><cell>3.09</cell><cell>1.03×</cell><cell>-</cell><cell>78.21%</cell><cell>0.5692</cell><cell>0.4811</cell><cell>11.91</cell></row><row><cell>T-GATE [58]</cell><cell>2.75</cell><cell>1.19×</cell><cell>-</cell><cell>77.61%</cell><cell>0.3495</cell><cell>0.6760</cell><cell>15.50</cell></row><row><cell>PAB-slow [59]</cell><cell>2.55</cell><cell>1.33×</cell><cell>33.40</cell><cell>77.64%</cell><cell>0.1471</cell><cell>0.8405</cell><cell>24.50</cell></row><row><cell>PAB-fast [59]</cell><cell>2.50</cell><cell>1.40×</cell><cell>31.85</cell><cell>76.95%</cell><cell>0.1743</cell><cell>0.8220</cell><cell>23.58</cell></row><row><cell>TeaCache-slow</cell><cell>2.40</cell><cell>1.55×</cell><cell>28.78</cell><cell>79.28%</cell><cell>0.1316</cell><cell>0.8415</cell><cell>23.62</cell></row><row><cell>TeaCache-fast</cell><cell>1.64</cell><cell>2.25×</cell><cell>19.84</cell><cell>78.48%</cell><cell>0.2511</cell><cell>0.7477</cell><cell>19.10</cell></row><row><cell></cell><cell></cell><cell cols="3">OpenSora-Plan (65 frames, 512×512)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OpenSora-Plan (T = 150)</cell><cell>11.75</cell><cell>1×</cell><cell>99.65</cell><cell>80.39%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>∆-DiT [11]</cell><cell>11.74</cell><cell>1.01×</cell><cell>-</cell><cell>77.55%</cell><cell>0.5388</cell><cell>0.3736</cell><cell>13.85</cell></row><row><cell>T-GATE [58]</cell><cell>2.75</cell><cell>1.18×</cell><cell>-</cell><cell>80.15%</cell><cell>0.3066</cell><cell>0.6219</cell><cell>18.32</cell></row><row><cell>PAB-slow [59]</cell><cell>8.69</cell><cell>1.36×</cell><cell>73.41</cell><cell>80.30%</cell><cell>0.3059</cell><cell>0.6550</cell><cell>18.80</cell></row><row><cell>PAB-fast [59]</cell><cell>8.35</cell><cell>1.56×</cell><cell>65.38</cell><cell>71.81%</cell><cell>0.5499</cell><cell>0.4717</cell><cell>15.47</cell></row><row><cell>TeaCache-slow</cell><cell>3.13</cell><cell>4.41×</cell><cell>22.62</cell><cell>80.32%</cell><cell>0.2145</cell><cell>0.7414</cell><cell>21.02</cell></row><row><cell>TeaCache-fast</cell><cell>2.06</cell><cell>6.83×</cell><cell>14.60</cell><cell>79.72%</cell><cell>0.3155</cell><cell>0.6589</cell><cell>18.95</cell></row><row><cell cols="3">(1) Timesteps. Our caching strategy dynamically selects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">timesteps with large difference for caching and reusing in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the following timesteps, whereas the strategy of reducing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">timesteps is conducted uniformly, lacking awareness of dy-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">namic differences among different timesteps. (2) Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">output. In our caching strategy, only the residual signal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(i.e., Output minus Input) in the diffusion transformer is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">cached, therefore the model output at the next timestep is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">updated. In contrast, reducing timesteps can be considered</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">as keeping the output constant in the next timestep. (3) Pa-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">rameter α t . Reducing timesteps results in a coarser-grained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">α t , which suffers from deteriorated visual quality. In com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">parison, TeaCache is able to maintain the visual quality, as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>illustrated in Fig. 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of caching indicator. 'Timestep': timestep embedding. 'Input': timestep embedding-modulated noisy input.</figDesc><table><row><cell>Indicator</cell><cell>VBench ↑</cell><cell>LPIPS ↓</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell></row><row><cell>OpenSora</cell><cell>79.22%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Timestep</cell><cell>77.01%</cell><cell>0.3425</cell><cell>0.6934</cell><cell>15.86</cell></row><row><cell>Input</cell><cell>78.21%</cell><cell>0.2549</cell><cell>0.7457</cell><cell>19.05</cell></row><row><cell>Latte</cell><cell>77.40%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Timestep</cell><cell>77.05%</cell><cell>0.2653</cell><cell>0.7073</cell><cell>19.76</cell></row><row><cell>Input</cell><cell>77.17%</cell><cell>0.2558</cell><cell>0.7164</cell><cell>20.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of polynomial fitting. Rescaling with polynomial fitting outperforms original data. Higher-order fitting obtains better performance and saturates in 4-order fitting.</figDesc><table><row><cell>Order</cell><cell>VBench ↑</cell><cell>LPIPS ↓</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell></row><row><cell>OpenSora</cell><cell>79.22%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Original</cell><cell>78.21%</cell><cell>0.2549</cell><cell>0.7457</cell><cell>19.05</cell></row><row><cell>1-order</cell><cell>78.45%</cell><cell>0.2517</cell><cell>0.7478</cell><cell>19.10</cell></row><row><cell>2-order</cell><cell>78.48%</cell><cell>0.2513</cell><cell>0.7477</cell><cell>19.09</cell></row><row><cell>4-order</cell><cell>78.48%</cell><cell>0.2511</cell><cell>0.7477</cell><cell>19.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Inference efficiency and visual quality when scaling to multiple GPUs with Dynamic Sequence Parallelism (DSP).</figDesc><table><row><cell>Method</cell><cell>1 × A800</cell><cell>2 × A800</cell><cell>4 × A800</cell><cell>8 × A800</cell></row><row><cell></cell><cell cols="3">Open-Sora (192 frames, 480P)</cell><cell></cell></row><row><cell>Baseline</cell><cell>188.87(1×)</cell><cell>72.86(2.59×)</cell><cell>39.26(4.81×)</cell><cell>22.18(8.52×)</cell></row><row><cell>PAB</cell><cell>142.23(1.33×)</cell><cell>53.74(3.51×)</cell><cell>29.19(6.47×)</cell><cell>16.88(11.19×)</cell></row><row><cell>TeaCache</cell><cell>114.01(1.66×)</cell><cell>47.03(4.02×)</cell><cell>24.64(7.67×)</cell><cell>14.41(13.10×)</cell></row><row><cell></cell><cell cols="3">Open-Sora-Plan (221 frames, 512×512)</cell><cell></cell></row><row><cell>Baseline</cell><cell>324.41(1×)</cell><cell>166.94(1.94×)</cell><cell>88.18(3.68×)</cell><cell>47.79(6.79×)</cell></row><row><cell>PAB</cell><cell>207.70(1.56×)</cell><cell>110.06(2.95×)</cell><cell>58.07(5.59×)</cell><cell>31.92(10.16×)</cell></row><row><cell>TeaCache</cell><cell>48.22(6.73×)</cell><cell>26.99(12.02×)</cell><cell>15.91(20.39×)</cell><cell>10.13 (32.02×)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mochi 1</title>
		<ptr target="https://www.genmo.ai.1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Parallel transformer for scaling up video diffusion models</title>
		<idno>Vchitect. vchitect-2.0</idno>
		<ptr target="https://github.com/Vchitect/Vchitect-2.0.1" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selective cache ways: On-demand cache resource allocation</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-32. Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Token merging for fast stable diffusion</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4599" to="4603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Videocrafter1: Open diffusion models for high-quality video generation</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19512</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Videocrafter2: Overcoming data limitations for high-quality video diffusion models</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7310" to="7320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00426</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04692</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Q-dit: Accurate post-training quantization for diffusion transformers</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17343</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Delta dit: A training-free acceleration method tailored for diffusion transformers</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjun</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos-Savvas</forename><surname>Bouganis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01125</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Atri and Christopher Ré. Flashattention: Fast and memory-efficient exact at-tention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
	<note>Diffusion models beat gans on image synthesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using cache memory to reduce processor-memory traffic</title>
		<author>
			<persName><surname>James R Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th annual international symposium on Computer architecture</title>
		<meeting>the 10th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ptqd: Accurate post-training quantization for diffusion models</title>
		<author>
			<persName><forename type="first">Yefei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vbench: Comprehensive benchmark suite for video generative models</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattapol</forename><surname>Chanpaisit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gotta go fast when generating data with score-based models</title>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Elucidating the design space of diffusion-based generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26565" to="26577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Pku-Yuan</forename><surname>Lab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Tuzhan</surname></persName>
		</author>
		<author>
			<persName><surname>Etc</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo</idno>
		<ptr target="https://doi.org/10.5281/zenodo" />
		<imprint>
			<date type="published" when="2007">2024. 10948109. 1, 3, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distrifusion: Distributed parallel inference for high-resolution diffusion models</title>
		<author>
			<persName><forename type="first">Muyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7183" to="7193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Senmao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taihang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Faster diffusion: Rethinking the role of unet encoder in diffusion models. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2312</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Snapfusion: Text-to-image diffusion model on mobile devices within two seconds</title>
		<author>
			<persName><forename type="first">Yanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Chemerys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Q-dm: An efficient low-bit quantized diffusion model</title>
		<author>
			<persName><forename type="first">Yanjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5775" to="5787" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01095</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning-to-cache: Accelerating diffusion transformer via layer caching</title>
		<author>
			<persName><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01733</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Latte: Latent diffusion transformer for video generation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengyun</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03048</idno>
		<imprint>
			<date type="published" when="2007">2024. 1, 3, 4, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On distillation of guided diffusion models</title>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14297" to="14306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Sora</surname></persName>
		</author>
		<ptr target="https://openai.com/index/sora/.3" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.17042</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial diffusion distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Pratheba</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.01425</idno>
		<title level="m">Fora: Fast-forward caching in diffusion transformer acceleration</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Post-training quantization on diffusion models</title>
		<author>
			<persName><forename type="first">Yuzhang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1972">1972-1981, 2023. 3 [40. 2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cache memories</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="530" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Frdiff: Feature reuse for universal training-free acceleration of diffusion models</title>
		<author>
			<persName><forename type="first">Junhyuk</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03517</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal dynamic quantization for diffusion models</title>
		<author>
			<persName><forename type="first">Junhyuk</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daehyun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">T2v-compbench: A comprehensive benchmark for compositional text-to-video generation</title>
		<author>
			<persName><forename type="first">Kaiyue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.14505</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention-driven training-free efficiency enhancement of diffusion models</title>
		<author>
			<persName><forename type="first">Hongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niraj</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="16080" to="16089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06571</idno>
		<title level="m">Modelscope text-to-video technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09109</idno>
		<title level="m">Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Composing your dream videos with customized subject and motion</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><surname>Dreamvideo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6537" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13830</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cache me if you can: Accelerating diffusion models through block caching</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wimbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Schoenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>eed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deepcache: Principled cache for mobile deep vision</title>
		<author>
			<persName><forename type="first">Mengwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengze</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Xiaozhu Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanzhe</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international conference on mobile computing and networking</title>
		<meeting>the 24th annual international conference on mobile computing and networking</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Crossattention makes inference cumbersome in text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Wentian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Faccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.02747</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Real-time video generation with pyramid attention broadcast</title>
		<author>
			<persName><forename type="first">Xuanlei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12588</idno>
		<imprint>
			<date type="published" when="2008">2024. 1, 2, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<ptr target="https://github.com/hpcaitech/Open-Sora.1" />
		<imprint>
			<date type="published" when="2007">2024. 3, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
