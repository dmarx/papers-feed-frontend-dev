- **Nested Optimization Problem**: Hyperparameter optimization is a nested optimization problem defined as:
  \[
  \text{argmin } \lambda L_{\text{Valid}} \quad \text{argmin } w L_{\text{Train}}(w, \lambda)
  \]

- **Hypertraining Method**: A method to learn a mapping from hyperparameters \(\lambda\) to optimal weights \(w\) using a hypernetwork:
  \[
  w = w_\phi(\lambda)
  \]
  where \(\phi\) are the hypernetwork parameters.

- **Gradient Update Rule**: Update hypernetwork weights using the chain rule:
  \[
  \frac{\partial L_{\text{Train}}(w_\phi)}{\partial w_\phi} \frac{\partial w_\phi}{\partial \phi}
  \]

- **Best-Response Function**: The function \(w^*(\lambda)\) outputs optimal weights for hyperparameters, aiming for convergence to this function.

- **Advantages of Hypertraining**:
  - Eliminates the need to fully train models for each hyperparameter set.
  - Utilizes the differentiable nature of validation loss \(L_{\text{Valid}}(w_\phi(\lambda))\).

- **Limitations**:
  - Cannot optimize inner optimization parameters due to lack of internal training loop.
  - Local changes to hyperparameters only; does not explore uncertainty-based methods.

- **Asymptotic Convergence**: Theorem 2.1 states that sufficiently powerful hypernetworks can learn continuous best-response functions:
  \[
  L_{\text{Train}}(w_\phi^*(\lambda), \lambda) = \min_w L_{\text{Train}}(w, \lambda)
  \]

- **Algorithm Overview**:
  - **Algorithm 1**: Standard cross-validation with stochastic optimization.
  - **Algorithm 2**: Optimization of hypernetwork followed by hyperparameters.
  - **Algorithm 3**: Joint optimization of hypernetwork and hyperparameters.

- **Conditional Hyperparameter Distribution**: Define \(p(\lambda | \lambda)\) to focus on promising hyperparameter values.

- **Visualization of Weights**:
  ```mermaid
  graph TD;
      A[Optimal Weights w*] --> B[Hyperparameters λ*];
      A --> C[Approximate Weights wφ*];
      B --> D[Hyperparameter Distribution p(λ)];
      C --> E[Validation Loss LValid];
  ```

- **Comparison with Bayesian Optimization**: Hypertraining is more efficient as it does not require full model training for each hyperparameter set, unlike Bayesian optimization which treats validation loss as a black-box function.

- **Practical Considerations**: The choice of hyperparameter distribution \(p(\lambda)\) is crucial for effective learning of the best-response function.