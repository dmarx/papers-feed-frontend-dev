<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Hyperparameter Optimization through Hypernetworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Lorraine</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
						</author>
						<title level="a" type="main">Stochastic Hyperparameter Optimization through Hypernetworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5A99580941CF2C7193B11766548BDFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-28T01:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters. We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters. Our process trains a neural network to output approximately optimal weights as a function of hyperparameters. We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernetworks. We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Model selection and hyperparameter tuning is a significant bottleneck in designing predictive models. Hyperparameter optimization is a nested optimization: The inner optimization finds model parameters w which minimize the training loss LTrain given hyperparameters λ. The outer optimization chooses λ to reduce a validation loss LValid.:</p><formula xml:id="formula_0">argmin λ L Valid. argmin w L Train (w, λ)<label>(1)</label></formula><p>Standard practice in machine learning solves (1) by gradient-free optimization of hyperparameters, such as grid search or random search. Each set of hyperparameters is evaluated by re-initializing weights and training the model to completion. Re-training a model from scratch is wasteful if the hyperparameters change by a small amount. Some approaches, such as Hyperband <ref type="bibr" target="#b15">(Li et al., 2016)</ref> and freezethaw Bayesian optimization <ref type="bibr" target="#b24">(Swersky et al., 2014)</ref>, resume model training and do not waste this effort. However, these methods often scale poorly beyond 10 to 20 dimensions.</p><p>How can we avoid re-training from scratch each time? Note that the optimal parameters w are a deterministic function 1 Department of Computer Science, University of Toronto, Toronto, Canada. Correspondence to: Jonathan Lorraine &lt;lorraine@cs.toronto.edu&gt;. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation Hyper-training</head><p>Figure <ref type="figure">1</ref>: Left: A typical computational graph for crossvalidation, where α are the optimizer parameters, and λ are training loss hyperparameters. It is expensive to differentiate through the entire training procedure. Right: The proposed computational graph with our changes in red, where φ are the hypernetwork parameters. We can cheaply differentiate through the hypernetwork to optimize the validation loss LValid. with respect to hyperparameters λ. We use x, t, and y to refer to a data point, its label, and a prediction respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter λ</head><p>Validation Loss L Valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation Optimized hypernetwork</head><p>Optimal hyperparameter λ * </p><p>We propose to learn this function. Specifically, we train a neural network that takes hyperparameters as input, and outputs an approximately optimal set of weights.</p><p>This formulation provides two major benefits: First, we can train the hypernetwork to convergence using stochastic gradient descent (SGD) without training any particular model to completion. Second, differentiating through the hypernetwork allows us to optimize hyperparameters with stochastic gradient-based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Training a network to output optimal weights</head><p>How can we teach a hypernetwork <ref type="bibr" target="#b11">(Ha et al., 2016)</ref> to output approximately optimal weights to another neural network? The basic idea is that at each iteration, we ask a hypernetwork to output a set of weights given some hyperparameters: w = w φ (λ). Instead of updating the weights w using the training loss gradient ∂ LTrain(w) /∂w, we update the hypernetwork weights φ using the chain rule:</p><p>∂ LTrain(wφ) ∂w φ ∂w φ ∂φ . This formulation allows us to optimize the hyperparameters λ with the validation loss gradient</p><formula xml:id="formula_2">∂ LValid.(wφ(λ)) ∂w φ (λ) ∂w φ (λ)</formula><p>∂λ . We call this method hypertraining and contrast it with standard training methods.</p><p>We call the function w * (λ) that outputs optimal weights for hyperparameters a best-response function. At convergence, we want our hypernetwork w φ (λ) to match the bestresponse function closely.</p><p>Our method is closely related to the concurrent work of <ref type="bibr" target="#b2">Brock et al. (2017)</ref>, whose SMASH algorithm also approximates the optimal weights as a function of model architectures, to perform a gradient-free search over discrete model structures. Their work focuses on efficiently estimating the performance of a variety of model architectures, while we focus on efficiently exploring continuous spaces of models. We further extend this idea by formulating an algorithm to optimize the hypernetwork and hyperparameters jointly. Joint optimization of parameters and hyperparameters addresses one of the main weaknesses of SMASH, which is that the the hypernetwork must be very large to learn approximately optimal weights for many different settings. During joint optimization, the hypernetwork need only model approximately optimal weights for the neighborhood around the current hyperparameters, allowing us to use even linear hypernetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Advantages of hypernetwork-based optimization</head><p>Hyper-training is a method to learn a mapping from hyperparameters to validation loss which is differentiable and cheap to evaluate. We can compare hyper-training to other model-based hyperparameter schemes. Bayesian optimization (e.g., <ref type="bibr" target="#b16">Lizotte (2008)</ref>; <ref type="bibr" target="#b23">Snoek et al. (2012)</ref>) builds a model of the validation loss as a function of hyperparameters, usually using a Gaussian process (e.g., <ref type="bibr" target="#b22">Rasmussen &amp; Williams (2006)</ref>) to track uncertainty. This approach has several disadvantages compared to hyper-training.</p><p>First, obtaining data for standard Bayesian optimization requires optimizing models from initialization for each set of hyperparameters. In contrast, hyper-training never needs to optimize any one model fully removing choices like how many models to train and for how long.</p><p>Second, standard Bayesian optimization treats the validation loss as a black-box function:</p><p>LValid. (λ) = f (λ). In contrast, hyper-training takes advantage of the fact that the validation loss is a known, differentiable function:</p><p>LValid. (λ) = LValid.(wφ(λ)). This information removes the need to learn a model of the validation loss. This function can also be evaluated stochastically by sampling points from the validation set.</p><p>Hyper-training has a benefit of learning hyperparameter to optimized weight mapping, which is substituted into the validation loss. This often has a better inductive bias for learning hyperparameter to validation loss than directly learning the loss. Also, the hypernetwork learns continuous best-responses, which may be a beneficial prior for finding weights by enforcing stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Limitations of hypernetwork-based optimization</head><p>We can apply this method to unconstrained continuous bilevel optimization problems with an inner loss function with inner parameters, and an outer loss function with outer parameters. What sort of parameters can be optimized by our approach? Hyperparameters typically fall into two broad categories: 1) Optimization hyperparameters, such as learning rates, which affect the choice of locally optimal point converged to, and 2) regularization or model architecture parameters which change the set of locally optimal points. Hyper-training does not have inner optimization parameters because there is no internal training loop, so we can not optimize these. However, we must still choose optimization parameters for the fused optimization loop. In principle, hyper-training can handle discrete hyperparameters, but does not offer particular advantages for optimization over continuous hyperparameters.</p><p>Another limitation is that our approach only proposes making local changes to the hyperparameters, and does not do uncertainty-based exploration. Uncertainty can be incorpo- (w)</p><formula xml:id="formula_3">λ * w * (λ * ) λ φ * w φ * (λ φ * )</formula><p>Figure <ref type="figure">3</ref>: A visualization of exact (blue) and approximate (red) optimal weights as a function of hyperparameters. The approximately optimal weights w φ * are output by a linear model fit at λ. The true optimal hyperparameter is λ * , while the hyperparameter estimated using approximately optimal weights is nearby at λ φ * . rated into the hypernetwork by using stochastic variational inference as in <ref type="bibr" target="#b1">Blundell et al. (2015)</ref>, and we leave this for future work. Finally, it is not obvious how to choose the training distribution of hyperparameters p(λ). If we do not sample a sufficient range of hyperparameters, the implicit estimated gradient of the validation loss w.r.t. the hyperparameters may be inaccurate. We discuss several approaches to this problem in section 2.4.</p><p>A clear difficulty of this approach is that hypernetworks can require several times as many parameters as the original model. For example, training a fully-connected hypernetwork with 1 hidden layer of H units to output D parameters requires at least D × H hypernetwork parameters. To address this problem, in section 2.4, we propose an algorithm that only trains a linear model mapping hyperparameters to model weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Asymptotic convergence properties</head><p>Algorithm 2 trains a hypernetwork using SGD, drawing hyperparameters from a fixed distribution p(λ). This section proves that Algorithm 2 converges to a local best-response under mild assumptions. In particular, we show that, for a sufficiently large hypernetwork, the choice of p(λ) does not matter as long as it has sufficient support. Notation as if w φ has a unique solution for φ or w is used for simplicity, but is not true in general.</p><p>Theorem 2.1. Sufficiently powerful hypernetworks can learn continuous best-response functions, which minimizes the expected loss for all hyperparameter distributions with convex support.</p><p>There exists φ * , such that for all λ ∈ support(p (λ)) ,</p><formula xml:id="formula_4">L Train (w φ * (λ) , λ) = min w L Train (w, λ) and φ * = argmin φ E p(λ ) L Train (w φ (λ ), λ )</formula><p>Proof. If w φ is a universal approximator <ref type="bibr" target="#b12">(Hornik, 1991)</ref> and the best-response is continuous in λ (which allows approximation by w φ ), then there exists optimal hypernetwork parameters φ * such that for all hyperparameters λ, w φ * (λ) = argmin w LTrain(w, λ). Thus, LTrain(wφ * (λ) , λ) = min w LTrain(w, λ). In other words, universal approximator hypernetworks can learn continuous best-responses.</p><p>Substituting φ * into the training loss gives</p><formula xml:id="formula_5">Ep(λ)[LTrain(wφ * (λ), λ)] = Ep(λ)[minφ LTrain(wφ(λ), λ)]. By Jensen's inequality, min φ Ep(λ)[LTrain(wφ(λ), λ)] ≥ Ep(λ)[minφ LTrain(wφ(λ), λ)].</formula><p>To satisfy Jensen's requirements, we have min φ as our convex function on the convex vector space of functions {L Train (w φ (λ), λ) for λ ∈ support(p (λ))}. To guarantee convexity of the vector space we require that support(p (λ)) is convex and</p><formula xml:id="formula_6">LTrain(w, λ) = Ex∼Train[LPred(x, w)] + LReg(w, λ) with LReg(w, λ) = λ • L(w). Thus, φ * = argmin φ Ep(λ)[LTrain(wφ(λ), λ)].</formula><p>In other words, if the hypernetwork learns the best-response it will simultaneously minimize the loss for every point in support(p (λ)).</p><p>Thus, having a universal approximator and a continuous best-response implies for all λ ∈ support(p (λ)), Algorithm 1 Standard cross-validation with stochastic optimization</p><formula xml:id="formula_7">for i = 1, . . . , T outer do initialize w λ = hyperopt λ (1:i) , LValid. w (1:i) loop x ∼ Training data w -= α∇ w LTrain(w, λ, x) λ i , w i = λ, w i = argmin i LTrain(w (i) , λ (i) , x) Return λ (i) , w (i) Algorithm 2 Optimization of hyper- network, then hyperparameters initialize φ initialize λ loop x ∼ Training data, λ ∼ p (λ) φ -= α∇ φ LTrain(wφ(λ), λ, x) loop x ∼ Validation data λ -= β∇λ LValid.(wφ( λ), x) Return λ, w φ ( λ)</formula><p>Algorithm 3 Joint optimization of hypernetwork and hyperparameters</p><formula xml:id="formula_8">initialize φ initialize λ loop x ∼ Training data, λ ∼ p(λ| λ) φ -= α∇ φ LTrain(wφ(λ), λ, x) x ∼ Validation data λ -= β∇λ LValid.(wφ( λ), x) Return λ, w φ ( λ)</formula><p>A comparison of standard hyperparameter optimization, our first algorithm, and our joint algorithm. Here, hyperopt refers to a generic hyperparameter optimization. Instead of updating weights w using the loss gradient ∂L(w) /∂w, we update hypernetwork weights φ and hyperparameters λ using the chain rule:</p><formula xml:id="formula_9">∂ LTrain(wφ) ∂w φ ∂w φ ∂φ or ∂ LValid.(wφ(λ)) ∂w φ (λ) ∂w φ (λ) ∂λ</formula><p>respectively. This allows our method to use gradient-based hyperparameter optimization. Algorithm 4 builds on Algorithm 3 by using gradient updates on λ as a source of noise. This variant does not have asymptotic guarantees, but performs similarly to Algorithm 3 in practice.</p><p>LValid.(wφ * (λ)) = LValid.(w * (λ)), because w φ * (λ) = w * (λ). Thus, under mild conditions, we will learn a bestresponse in the support of the hyperparameter distribution. If the best-response is differentiable, then there is a neighborhood about each hyperparameter where the bestresponse is approximately linear. If the support of the hyperparameter distribution is this neighborhood, then we can learn the best-response locally with linear regression.</p><p>In practice, there are no guarantees about the network being a universal approximator or the finite-time convergence of optimization. The optimal hypernetwork will depend on the hyperparameter distribution p(λ), not just the support of this distribution. We appeal to experimental results that our method is feasible in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Jointly training parameters and hyperparameters</head><p>Theorem 2.1 holds for any p (λ). In practice, we should choose a p (λ) that puts most of its mass on promising hyperparameter values, because it may not be possible to learn a best-response for all hyperparameters due to limited hypernetwork capacity. Thus, we propose Algorithm 3, which only tries to match a best-response locally. We introduce a "current" hyperparameter λ, which is updated each iteration. We define a conditional hyperparameter distribution, p(λ| λ), which only puts mass close to λ.</p><p>Algorithm 3 combines the two phases of Algorithm 2 into one. Instead of first learning a hypernetwork that can output weights for any hyperparameter then optimizing the hyperparameters, Algorithm 3 only samples hyperparameters near the current guess. This means the hypernetwork just has to be trained to estimate good enough weights for a small set of hyperparameters. There is an extra cost of having to re-train the hypernetwork each time we update λ. The locally-trained hypernetwork can then be used to provide gradients to update the hyperparameters based on validation set performance.</p><p>How simple can we make the hypernetwork, and still obtain useful gradients to optimize hyperparameters? Consider the case in our experiments where the hypernetwork is a linear function of the hyperparameters and the conditional hyperparameter distribution is p(λ| λ) = N ( λ, σI) for some small σ. This hypernetwork learns a tangent hyperplane to a best-response function and only needs to make minor adjustments at each step if the hyperparameter updates are sufficiently small. We can further restrict the capacity of a linear hypernetwork by factorizing its  weights, effectively adding a bottleneck layer with a linear activation and a small number of hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Our work is complementary to the SMASH algorithm of <ref type="bibr" target="#b2">Brock et al. (2017)</ref>, with section 2 discussing our differences.</p><p>Model-free approaches Model-free approaches use only trial-and-error to explore the hyperparameter space. Simple model-free approaches applied to hyperparameter optimization include grid search and random search <ref type="bibr" target="#b0">(Bergstra &amp; Bengio, 2012)</ref>. Hyperband <ref type="bibr" target="#b15">(Li et al., 2016)</ref> combines bandit approaches with modeling the learning procedure.</p><p>Model-based approaches Model-based approaches try to build a surrogate function, which can allow gradientbased optimization or active learning. A common example is Bayesian optimization. Freeze-thaw Bayesian optimization can condition on partially-optimized model performance.</p><p>Optimization-based approaches Another line of related work attempts to directly approximate gradients of the validation loss with respect to hyperparameters. <ref type="bibr" target="#b4">Domke (2012)</ref> proposes to differentiate through unrolled optimization to approximate best-responses in nested optimization and <ref type="bibr">Maclaurin et al. (2015a)</ref> differentiate through entire unrolled learning procedures. DrMAD <ref type="bibr" target="#b8">(Fu et al., 2016)</ref> approximates differentiating through an unrolled learning procedure to relax memory requirements for deep neural networks. HOAG <ref type="bibr" target="#b21">(Pedregosa, 2016)</ref> finds hyperparameter gradients with implicit differentiation by deriving an implicit equation for the gradient with optimality conditions. Franceschi et al. ( <ref type="formula">2017</ref>) study forward and reverse-mode differentiation for constructing hyperparameter gradients. Also, <ref type="bibr" target="#b5">Feng &amp; Simon (2017)</ref> establish conditions where the validation loss of best-responding weights are almost everywhere smooth, allowing gradient-based training of hyperparameters.</p><p>A closely-related procedure to our method is the T 1 -T 2 method of <ref type="bibr" target="#b17">Luketina et al. (2016)</ref>, which also provides an algorithm for stochastic gradient-based optimization of hyperparameters. The convergence of their procedure to local optima of the validation loss depends on approximating the Hessian of the training loss for parameters with the identity matrix. In contrast, the convergence of our method depends on having a suitably powerful hypernetwork.</p><p>Game theory Best-response functions are extensively studied as a solution concept in discrete and continuous multi-agent games (e.g., <ref type="bibr" target="#b9">Fudenberg &amp; Levine (1998)</ref>). Games where learning a best-response can be applied include adversarial training <ref type="bibr" target="#b10">(Goodfellow et al., 2014)</ref>, or Stackelberg competitions (e.g., <ref type="bibr" target="#b3">Brückner &amp; Scheffer (2011)</ref>). For adversarial training, the analog of our method is a discriminator who observes the generator's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In our experiments, we examine the standard example of stochastic gradient-based optimization of neural networks, with a weight regularization penalty. Some gradient-based methods explicitly use the gradient of a loss, while others use the gradient of a learned surrogate loss. Hyper-training learns and substitutes a surrogate best-response function into a real loss. We may contrast our algorithm with methods learning the loss like Bayesian optimization, gradientbased methods only handling hyperparameters that affect the training loss and gradient-based methods which can handle optimization parameters. The best comparison for hyper-training is to gradient-based methods which only handle parameters affecting the training loss because other methods apply to a more general set of problems. In this case, we write the training and validation losses as: In all experiments, Algorithms 2 or 3 are used to optimize weights with a mean squared error on MNIST <ref type="bibr">(Le-Cun et al., 1998)</ref> with LReg as an L 2 weight decay penalty weighted by exp(λ). The elementary model has 7, 850 weights. All hidden units in the hypernetwork have a ReLU activation <ref type="bibr" target="#b20">(Nair &amp; Hinton, 2010)</ref> unless otherwise specified. Autograd <ref type="bibr">(Maclaurin et al., 2015b)</ref> was used to compute all derivatives. For each experiment, the minibatch samples 2 pairs of hyperparameters and up to 1, 000 training data points. We used Adam for training the hypernetwork and hyperparameters, with a step size of 0.0001. We ran all experiments on a CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning a global best-response</head><p>Our first experiment, shown in Figure <ref type="figure" target="#fig_1">2</ref>, demonstrates learning a global approximation to a best-response function using Algorithm 2. To make visualization of the regularization loss easier, we use 10 training data points to exacerbate overfitting. We compare the performance of weights output by the hypernetwork to those trained by standard cross-validation (Algorithm 1). Thus, elementary weights were randomly initialized for each hyperparameter choice and optimized using Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref> for 1, 000 iterations with a step size of 0.0001.</p><p>When training the hypernetwork, hyperparameters were sampled from a broad Gaussian distribution: p (λ) = N (0, 1.5). The hypernetwork has 50 hidden units which results in 400, 450 parameters of the hypernetwork.</p><p>The minimum of the best-response in Figure <ref type="figure" target="#fig_1">2</ref> is close to the real minimum of the validation loss, which shows a hypernetwork can satisfactorily approximate a global bestresponse function in small problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning a local best-response</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the same experiment, but using the Algorithm 3. The fused updates result in finding a best-response approximation whose minimum is the actual minimum faster than the prior experiment. The conditional hyperparameter distribution is given by p(λ| λ) = N ( λ, 0.00001).</p><p>The hypernetwork is a linear model, with only 15, 700 weights. We use the same optimizer as the global bestresponse to update both the hypernetwork and the hyperparameters.</p><p>Again, the minimum of the best-response at the end of training minimizes the validation loss. This experiment shows that using only a locally-trained linear best-response function can give sufficient gradient information to optimize hyperparameters on a small problem. Algorithm 3 is also less computationally expensive than Algorithms 1 or 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GP mean Hyper-training fixed Hyper-training</head><p>Inferred loss 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Inferred Loss 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Inferred Loss 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Inferred Loss 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Third column: Our proposed method, a hypernetwork trained with stochastically sampled hyperparameters. Top row: The distribution of inferred and true losses. The diagonal black line is where predicted loss equals true loss. Bottom row: The distribution of differences between inferred and true losses. The Gaussian process often under-predicts the true loss, while the hypernetwork trained on the same data tends to over-predict the true loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyper-training and unrolled optimization</head><p>To compare hyper-training with other gradient-based hyperparameter optimization methods, we train models with 7, 850 hyperparameters and a separate L 2 weight decay applied to each weight in a 1 layer (linear) model. The conditional hyperparameter distribution and optimizer for the hypernetwork and hyperparameters is the same the prior experiment. We factorize the weights for the model by selecting a hypernetwork with 10 hidden units. The factorized linear hypernetwork has 10 hidden units giving 164, 860 weights. Each hypernetwork iteration is 2 • 10 times as expensive as an iteration on just the model because there is the same number of hyperparameters as model parameters.</p><p>Figure <ref type="figure" target="#fig_6">5</ref>, top, shows that Algorithm 3 converges more quickly than the unrolled reverse-mode optimization introduced in <ref type="bibr">Maclaurin et al. (2015a)</ref> and implemented by <ref type="bibr" target="#b7">Franceschi et al. (2017)</ref>. Hyper-training reaches suboptimal solutions because of limitations on how many hyperparameters can be sampled for each update but overfits validation data less than unrolling. Standard Bayesian optimization cannot be scaled to this many hyperparameters. Thus, this experiment shows Algorithm 3 can efficiently partially optimize thousands of hyperparameters. It may be useful to combine these methods by using a hypernetwork to output initial parameters and then unrolling several steps of optimization to differentiate through.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Optimizing with deeper networks</head><p>To see if we can optimize deeper networks with hypertraining we optimize models with 1, 2, and 3 layers and a separate L 2 weight decay applied to each weight. The conditional hyperparameter distribution and optimizer for the hypernetwork and hyperparameters is the same the prior experiment. We factorize the weights for each model by selecting a hypernetwork with 10 hidden units.</p><p>Figure <ref type="figure" target="#fig_6">5</ref>, bottom, shows that Algorithm 3 can scale to networks with multiple hidden layers and outperform handtuned settings. As we add more layers the difference between validation loss and testing loss decreases, and the model performs better on the validation set. Future work should compare other architectures like recurrent or convolutional networks. Additionally, note that more layers perform with lesser training (not shown), validation, and test losses, instead of lower training loss and higher validation or test loss. This performance indicates that using weight decay on each weight could be a prior for generalization, or that hyper-training enforces another useful prior like the continuity of a best-response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Estimating weights versus estimating loss</head><p>Our approach differs from Bayesian optimization which attempts to directly model the validation loss of optimized weights, where we try to learn to predict optimal weights. In this experiment, we untangle the reason for the better performance of our method: Is it because of a better inductive bias, or because our way can see more hyperparameter settings during optimization?</p><p>First, we constructed a hyper-training set: We optimized 25 sets of weights to completion, given randomly-sampled hyperparameters. We chose 25 samples since that is the regime in which we expect Gaussian process-based approaches to have the most significant advantage. We also constructed a validation set of 10, 215 (optimized weight, hyperparameter) tuples generated in the same manner. We then fit a Gaussian process (GP) regression model with an RBF kernel from sklearn on the validation loss data. A hypernetwork is fit to the same set of hyperparameters and data. Finally, we optimize another hypernetwork using Algorithm 2, for the same amount of time as building the GP training set. The two hypernetworks were linear models and trained with the same optimizer parameters as the 7, 850-dimensional hyperparameter optimization.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the distribution of prediction errors of these three models. We can see that the Gaussian process tends to underestimate loss. The hypernetwork trained with the same small fixed set of examples tends to overestimate loss. We conjecture that this is due to the hypernetwork producing bad weights in regions where it doesn't have enough training data. Because the hypernetwork must provide actual weights to predict the validation loss, poorly-fit regions will overestimate the validation loss. Finally, the hypernetwork trained with Algorithm 2 produces errors tightly centered around 0. The main takeaway from this experiment is a hypernetwork can learn more accurate surrogate functions than a GP for equal compute budgets because it views (noisy) evaluations of more points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we addressed the question of tuning hyperparameters using gradient-based optimization, by replacing the training optimization loop with a differentiable hypernetwork. We gave a theoretical justification that sufficiently large networks will learn the best-response for all hyperparameters viewed in training. We also presented a simpler and more scalable method that jointly optimizes both hyperparameters and hypernetwork weights, allowing our method to work with manageably-sized hypernetworks.</p><p>Experimentally, we showed that hypernetworks could provide a better inductive bias for hyperparameter optimization than Gaussian processes fitting the validation loss empirically.</p><p>There are many directions to extend the proposed methods. For instance, the hypernetwork could be composed with several iterations of optimization, as an easilydifferentiable fine-tuning step. Or, hypernetworks could be incorporated into meta-learning schemes, such as MAML <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>, which finds weights that perform a variety of tasks after unrolling gradient descent.</p><p>We also note that the prospect of optimizing thousands of hyperparameters raises the question of hyperregularization, or regularization of hyperparameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The validation loss of a neural net, estimated by cross-validation (crosses) or by a hypernetwork (line), which outputs 7, 850-dimensional network weights. Crossvalidation requires optimizing from scratch each time. The hypernetwork can be used to evaluate the validation loss cheaply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>* (λ), λ) L(w φ * (λ), λ) λ w * ( λ), w φ * ( λ)P a r a m e te r w H y p e r p a r a m e t e r λ Loss L Valid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 4</head><label>4</label><figDesc>Simplified joint optimization of hypernetwork and hyperparameters initialize φ, λ loop x ∼ Training data, x ∼ Validation data φ -= α∇ φ LTrain(wφ( λ), λ, x) λ -= β∇λ LValid.(wφ( λ), x ) Return λ, w φ ( λ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training and validation losses of a neural network, estimated by cross-validation (crosses) or a linear hypernetwork (lines). The hypernetwork's limited capacity makes it only accurate where the hyperparameter distribution puts mass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Validation and test losses during hyperparameter optimization with a separate L 2 weight decay applied to each weight in the model. Thus, models with more parameters have more hyperparameters. Top: We solve the 7, 850dimensional hyperparameter optimization problem with a linear model and multiple algorithms. Hypernetworkbased optimization converges to a sub-optimal solution faster than unrolled optimization from Maclaurin et al. (2015a). Bottom: Hyper-training is applied different layer configurations in the model.</figDesc><graphic coords="6,55.44,248.68,243.00,243.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparing three approaches to inferring validation loss. First column: A Gaussian process, fit on 25 hyperparameters and the corresponding validation losses.Second column: A hypernetwork, fit on the same 25 hyperparameters and the corresponding optimized weights. Third column: Our proposed method, a hypernetwork trained with stochastically sampled hyperparameters. Top row: The distribution of inferred and true losses. The diagonal black line is where predicted loss equals true loss. Bottom row: The distribution of differences between inferred and true losses. The Gaussian process often under-predicts the true loss, while the hypernetwork trained on the same data tends to over-predict the true loss.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Matthew MacKay</rs>, <rs type="person">Dougal Maclaurin</rs>, <rs type="person">Daniel Flam-Shepard</rs>, <rs type="person">Daniel Roy</rs>, and <rs type="person">Jack Klys</rs> for helpful discussions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><surname>Julien</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Koray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Smash: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritchie</forename><surname>Theodore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stackelberg games for adversarial prediction problems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gradient-based regularization parameter selection for problems with non-smooth penalty functions</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09813</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forward and reverse gradientbased hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><surname>Donini</surname></persName>
		</author>
		<author>
			<persName><surname>Michele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1165" to="1173" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Drmad: distilling reverse-mode automatic differentiation for optimizing hyperparameters of deep neural networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Hongyin</surname></persName>
		</author>
		<author>
			<persName><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Jiashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kian</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><surname>Hsiang</surname></persName>
		</author>
		<author>
			<persName><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><surname>Tat-Seng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00917</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The theory of learning in games</title>
		<author>
			<persName><forename type="first">Drew</forename><surname>Fudenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Léon</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><surname>Giulia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><surname>Hyperband</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06560</idno>
		<title level="m">A novel bandit-based approach to hyperparameter optimization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Practical bayesian optimization</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable gradient-based tuning of continuous regularization hyperparameters</title>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2952" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autograd: Effortless gradients in numpy</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2015 AutoML Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization with approximate gradient</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larochelle</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><surname>Prescott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3896</idno>
		<title level="m">Freeze-thaw bayesian optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
