<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</title>
				<funder ref="#_Ne3ZHN7 #_7Njpj36">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder>
					<orgName type="full">Creative-Pioneering Researchers Program through Seoul National University</orgName>
				</funder>
				<funder ref="#_tcPxRGg">
					<orgName type="full">KIAS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-27">27 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong-Hyun</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingi</forename><surname>Kwon</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaewoong</forename><surname>Choi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Korea Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junghyo</forename><surname>Jo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-27">27 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">BEA97BD8D81BDDE9FBB68FF74C487D89</idno>
					<idno type="arXiv">arXiv:2307.12868v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space x t ∈ X , we analyze them from a geometrical perspective. Our approach involves deriving the local latent basis within X by leveraging the pullback metric associated with their encoding feature maps. Remarkably, our discovered local latent basis enables image editing capabilities by moving x t , the latent space of DMs, along the basis vector at specific timesteps. We further analyze how the geometric structure of DMs evolves over diffusion timesteps and differs across different text conditions. This confirms the known phenomenon of coarse-to-fine generation, as well as reveals novel insights such as the discrepancy between x t across timesteps, the effect of dataset complexity, and the time-varying influence of text prompts. To the best of our knowledge, this paper is the first to present image editing through x-space traversal, editing only once at specific timestep t without any additional training, and providing thorough analyses of the latent structure of DMs. The code to reproduce our experiments can be found at <ref type="url" target="https://github.com/enkeejunior1/Diffusion-Pullback">https://github.com/enkeejunior1/Diffusion-Pullback</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The diffusion models (DMs) are powerful generative models that have demonstrated impressive performance <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref>. DMs have shown remarkable applications, including text-to-image synthesis <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref>, inverse problems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>, and image editing <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Despite their achievements, the research community lacks a comprehensive understanding of the latent space of DMs and its influence on the generated results. So far, the completely diffused images are considered as latent variables but it does not have useful properties for controlling the results. For example, traversing along a direction from a latent produces weird changes in the results. Fortunately, Kwon et al. <ref type="bibr" target="#b25">[26]</ref> consider the intermediate feature space of the diffusion kernel, referred to as H, as a semantic latent space and show its usefulness on controlling generated images. In the similar sense, some works investigate the feature maps of the self-attention or cross-attention operations for controlling the results <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39]</ref>, improving sample quality <ref type="bibr" target="#b7">[8]</ref>, or downstream tasks such as semantic segmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Still, the structure of the space X t where latent variables {x t } live remains unexplored despite its crucial role in understanding DMs. It is especially challenging because 1) the model is trained to estimate the forward noise which does not depend on the input, as opposed to other typical supervisions such as classification or similarity, and 2) there are lots of latent variables over multiple recursive timesteps. In this paper, we aim to analyze X in conjunction with its corresponding representation H, by incorporating a local geometry to X using the concept of a pullback metric in Riemannian geometry.</p><p>First, we discover the local latent basis for X and the corresponding local tangent basis for H. The local basis is obtained by performing singular value decomposition (SVD) of the Jacobian of the mapping from X to H. To validate the discovered local latent basis, we demonstrate that walking along the basis can edit real images in a semantically meaningful way. Furthermore, we can use the discovered local latent basis vector to edit other samples by using parallel transport, when they exhibit comparable local geometric structures. Note that existing editing methods manipulate the self-attention map or cross-attention map over multiple timesteps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39]</ref>. On the other hand, we manipulate only x t once at a specific timestep.</p><p>Second, we investigate how the latent structures differ across different timesteps and samples as follows. The frequency domain of the local latent basis shifts from low-frequency to high-frequency along the generative process. We explicitly confirm it using power spectral density analysis. The difference between local tangent spaces of different samples becomes larger along the generative process. The local tangent spaces at various diffusion timesteps are similar to each other if the model is trained on aligned datasets such as CelebA-HQ or Flowers. However, this homogeneity does not occur on complex datasets such as ImageNet.</p><p>Finally, we examine how the prompts affect the latent structure of text-to-image DMs as follows.</p><p>Similar prompts yield similar latent structures. Specifically, we find a positive correlation between the similarity of prompts and the similarity of local tangent spaces. The influence of text on the local tangent space becomes weaker along the generative process.</p><p>Our work examines the geometry of X and H using Riemannian geometry. We discover the latent structure of X and how it evolves during the generative process and is influenced by prompts. This geometric exploration deepens our understanding of DMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Diffusion Models. Recent advances in DMs make great progress in the field of image synthesis and show state-of-the-art performance <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51]</ref>. An important subject in the diffusion model is the introduction of gradient guidance, including classifier-free guidance, to control the generative process <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>. The work by Song et al. <ref type="bibr" target="#b51">[52]</ref> has facilitated the unification of DMs with score-based models using SDEs, enhancing our understanding of DMs as a reverse diffusion process. However, the latent space is still largely unexplored, and our understanding is limited.</p><p>The study of latent space in GANs. The study of latent spaces has gained significant attention in recent years. In the field of Generative Adversarial Networks (GANs), researchers have proposed various methods to manipulate the latent space to achieve the desired effect in the generated images <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b37">38]</ref>. More recently, several studies <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b9">10]</ref> have examined the geometrical properties of latent space in GANs and utilized these findings for image manipulations. These studies bring the advantage of better understanding the characteristics of the latent space and facilitating the analysis and utilization of GANs. In contrast, the latent space of DMs remains poorly understood, making it difficult to fully utilize their capabilities.</p><p>Image manipulation in DMs. Early works include Choi et al. <ref type="bibr" target="#b10">[11]</ref> and Meng et al. <ref type="bibr" target="#b32">[33]</ref> have attempted to manipulate the resulting images of DMs by replacing latent variables, allowing the generation of desired random images. However, due to the lack of semantics in the latent variables of DMs, current approaches have critical problems with semantic image editing. Alternative approaches have explored the potential of using the feature space within the U-Net for semantic image manipulation. For example, Kwon et al. <ref type="bibr" target="#b25">[26]</ref> have shown that the bottleneck of the U-Net, H, can be used as a semantic latent space. Specifically, they used CLIP <ref type="bibr" target="#b42">[43]</ref> to identify directions within H that facilitate genuine image editing. Baranchuk et al. <ref type="bibr" target="#b5">[6]</ref> and Tumanyan et al. <ref type="bibr" target="#b53">[54]</ref> use the feature map of the U-Net for semantic segmentation and maintaining the structure of generated images. Unlike previous works, our editing method finds the editing direction without supervision, and directly traverses the latent variable along the latent basis.</p><p>Riemannain Geometry. Some studies have applied Riemannian geometry to analyze the latent spaces of deep generative models, such as Variational Autoencoders (VAEs) and GANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57]</ref>. Shao et al. <ref type="bibr" target="#b47">[48]</ref> proposed a pullback metric on the latent space from image space Euclidean metric to analyze the latent space's geometry. This method has been widely used in VAEs and GANs because it only requires a differentiable map from latent space to image space. However, no studies have investigated the geometry of latent space of DMs utilizing the pullback metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discovering the latent basis of DMs</head><p>In this section, we explain how to extract a latent structure of X using differential geometry. First, we introduce a key concept in our method: the pullback metric. Next, by adopting the local Euclidean metric of H and utilizing the pullback metric, we discover the local latent basis of the X . Moreover, although the direction we found is local, we show how it can be applied to other samples via parallel transport. Finally, we introduce x-space guidance for editing data in the X to enhance the quality of image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pullback metric</head><p>We consider a curved manifold, X , where our latent variables x t exist. The differential geometry represents X through patches of tangent spaces, T x , which are vector spaces defined at each point x. Then, all the geometrical properties of X can be obtained from the inner product of ||dx|| 2 = ⟨dx, dx⟩ x in T x . However, we do not have any knowledge of ⟨dx, dx⟩ x . It is definitely not a Euclidean metric. Furthermore, samples of x t at intermediate timesteps of DMs include inevitable noise, which prevents finding semantic directions in T x .</p><p>Fortunately, Kwon et al. <ref type="bibr" target="#b25">[26]</ref> observed that H, defined by the bottleneck layer of the U-Net, exhibits local linear structure. This allows us to adopt the Euclidean metric on H. In differential geometry, when a metric is not available on a space, pullback metric is used. If a smooth map exists between the original metric-unavailable domain and a metric-available codomain, the pullback metric is used to measure the distances in the domain space. Our idea is to use the pullback Euclidean metric on H to define the distances between the samples in X .</p><p>DMs are trained to infer the noise ϵ t from a latent variable x t at each diffusion timestep t. Each x t has a different internal representation h t , the bottleneck representation of the U-Net, at different t's. The differentiable map between X and H is denoted as f : X → H. Hereafter, we refer to x t as x for brevity unless it causes confusion. It is important to note that our method can be applied at any timestep in the denoising process. We consider a linear map, T x → T h , between the domain and codomain tangent spaces. This linear map can be described by the Jacobian J x = ∇ x h which determines how a vector v ∈ T x is mapped into a vector u ∈ T h by u = J x v.</p><p>Using the local linearity of H, we assume the metric, ||dh|| 2 = ⟨dh, dh⟩ h = dh T dh as a usual dot product defined in the Euclidean space. To assign a geometric structure to X , we use the pullback  metric of the corresponding H. In other words, the norm of v ∈ T x is measured by the norm of corresponding codomain tangent vector:</p><formula xml:id="formula_0">||v|| 2 pb ≜ ⟨u, u⟩ h = v T J x T J x v.<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding local latent basis</head><p>Using the pullback metric, we define the local latent vector v ∈ T x that shows a large variability in T h . We find a unit vector v 1 that maximizes ||v|| 2 pb . By maximizing ||v|| 2 pb while remaining orthogonal to v 1 , one can obtain the second unit vector v 2 . This process can be repeated to have</p><formula xml:id="formula_1">n latent directions of {v 1 , v 2 , • • • , v n } in T x .</formula><p>In practice, v i corresponds to the i-th right singular vector from the singular value decomposition (SVD) of J x = U ΛV T , i.e., J x v i = Λ i u i . Since the Jacobian of too many parameters is not tractable, we use a power method <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19]</ref> to approximate the SVD of J x (See Appendix D for the time complexity and Appendix F for the detailed algorithm).</p><p>Henceforth, we refer to T x as a local latent subspace, and {v 1 , v 2 , • • • , v n } as the corresponding local latent basis.</p><formula xml:id="formula_2">T x ≜ span{v 1 , v 2 • • • , v n }, where v i is i-th right singular vector of J x .<label>(2)</label></formula><p>Using the linear transformation between T x and T h via the Jacobian J x , one can also obtain corresponding directions in T h . In practice, u i corresponds to the i-th left singular vector from the SVD of J x . After selecting the top n (e.g., n = 50) directions of large eigenvalues, we can approximate any vector in T h with a finite basis, {u 1 , u 2 , • • • , u n }. When we refer to a local tangent space henceforth, it means the n-dimensional low-rank approximation of the original tangent space.</p><formula xml:id="formula_3">T h ≜ span{u 1 , u 2 • • • , u n }, where u i is the i-th left singular vector of J x .<label>(3)</label></formula><p>The collection of local latent basis vectors, {v 1 , v 2 , • • • , v n }, obtained through our proposed method, can be interpreted as a signal that the model is highly response to for a given x. On the other hand, the basis of the local tangent space, denoted as {u 1 , u 2 • • • , u n }, can be viewed as the corresponding representation associated with the signal.</p><p>In Stable Diffusion, the prompt also influences the Jacobian, which means that the local basis also depends on it. We can utilize any prompt to obtain a local latent basis, and different prompts create distinct geometrical structures. For the sake of brevity, we will omit the word local unless it leads to confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating edited images with x-space guidance</head><p>A naïve approach for manipulating a latent variable x using a latent vector v is through simple addition, specifically x + γv. However, using the naïve approach sometime leads to noisy image generation. To address this issue, instead of directly using the basis for manipulation, we use a basis vector that has passed through the decoder once for manipulation. The x-space guidance is defined as follows</p><formula xml:id="formula_4">xXG = x + γ[ϵ θ (x + v) -ϵ θ (x)]<label>(4)</label></formula><p>where γ is a hyper-parameter controlling the strength of editing and ϵ θ is a diffusion model. Equation <ref type="formula" target="#formula_4">4</ref>is inspired by classifier-free guidance, but the key difference is that it is directly applied in the latent space X . Our x-space guidance provides qualitatively similar results to direct addition, while it shows better fidelity. (See Appendix C for ablation study.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The overall process of image editing</head><p>In this section, we summarize the entire editing process with five steps: 1) The input image is inverted into initial noise x T using DDIM inversion. 2) x T is gradually denoised until t through DDIM generation.</p><p>3) Identify local latent basis {v 1 , • • • , v n } using the pullback metric at t. 4) Manipulate</p><p>x t along the one of basis vectors using the x-space guidance. 5) The DDIM generation is then completed with the modified latent variable xt . Figure <ref type="figure" target="#fig_2">2</ref> illustrates the entire editing process.</p><p>In the context of a text-to-image model, such as Stable Diffusion, it becomes possible to include textual conditions while deriving local basis vectors. Although we do not use any text guidance during DDIM inversion and generation, a local basis with text conditions enables semantic editing that matches the given text. Comprehensive experiments can be found in Section 4.1.</p><p>It is noteworthy that our approach needs no extra training and simplifies image editing by only adjusting the latent variable within a single timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Editing various samples with parallel transport</head><p>Let us consider a scenario where our aim is to edit ten images, changing straight hair to curly hair. Due to the nature of the unsupervised image editing method, it is becomes imperative to manually check the semantic relevance of the latent basis vector in the edited results. Thus, to edit every samples, we have to manually find a straight-to-curly basis vector for individual samples.</p><p>One way to alleviate this tedious task is to apply the curly hair vector obtained from one image to other images. However, the basis vector v ∈ T x obtained at x cannot be used for the other sample x ′ because v / ∈ T x ′ . Thus, in order to apply the direction we obtained to another sample, it is necessary to relocate the extracted direction to a new tangent space. To achieve this, we use parallel transport that moves v i onto the new tangent space T x ′ .</p><p>Parallel transport moves a tangent vector u ∈ T h to u ′ ∈ T h ′ without changing its direction as much as possible while keeping the vector tangent on the manifold <ref type="bibr" target="#b47">[48]</ref>. It is notable that the parallel transport in curved manifold significantly modifies the original vector. Fortunately, H is relatively flat. Therefore, it is beneficial to apply the parallel transport in H. We aims to move v i onto new tangent space T x ′ , using parallel transport in H. First, we convert the latent direction v i ∈ T x to the corresponding direction of u i ∈ T h . Second, we apply the parallel transport</p><formula xml:id="formula_5">u i ∈ T h to u ′ i ∈ T h ′ , where h ′ = f (x ′ ).</formula><p>In the general case, parallel transport involves iterative projection and normalization on the tangent space along the path connecting two points <ref type="bibr" target="#b47">[48]</ref>. However, in our case, we assume that H has Euclidean geometry. Therefore, we move u directly onto T h ′ through projection, without the need for an iterative process. Finally, transform</p><formula xml:id="formula_6">u ′ i into v ′ i ∈ X . Using this parallel transport of v i → v ′</formula><p>i via H, we can apply the local latent basis obtained from x to edit or modify the input x ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Findings and results</head><p>In this section, we analyze the geometric structure of DMs with our method. § 4.1 demonstrates that the latent basis found by our method can be used for image editing. In § 4.2, we investigate how the geometric structure of DMs evolves as the generative process progresses. Lastly, in § 4.3, we examine how the geometric properties of the text-condition model change with a given text.</p><p>The implementation details of our work are provided on Appendix B. The source code for our experiments is included in the supplementary materials and will be publicly available upon publication. ). Notably, when given the "Lion" prompt, it is evident that all the top latent basis vectors align with the direction of the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image editing with the latent basis</head><p>In this subsection, we demonstrate the capability of our discovered latent basis for image editing.</p><p>To extract the latent variables from real images for editing purposes, we use DDIM inversion. In experiments with Stable Diffusion (SD), we do not use guidance, i.e., unconditional sampling, for both DDIM inversion and DDIM sampling processes. This ensures that our editing results solely depend on the latent variable, not on other factors such as prompt conditions. Each column is edited using the latent basis vector obtained from a different text prompt. Importantly, our method employs each prompt only once to derive the local latent basis.</p><p>Figures <ref type="figure" target="#fig_2">2</ref> and <ref type="figure">3</ref> illustrate the example results edited by the latent basis found by our method. The latent basis clearly contains semantics such as age, gender, species, structure, and texture. Note that editing at timestep T yields coarse changes such as age and species. On the other hand, editing at timestep 0.6T leads to fine changes, such as nose color and facial expression. Figure <ref type="figure">4</ref> demonstrates the example results edited by the various latent basis vectors. Interestingly, using the text "lion" as a condition, the entire latent basis captures lion-related attributes. Furthermore, Figure <ref type="figure" target="#fig_4">5</ref> shows that the latent basis aligns with the text not only in terms of object types but also in relation to pose or action. For a qualitative comparison with other state-of-the-art image editing methods, refer to Appendix D. For more examples of editing results, refer to Appendix G. In this subsection, we demonstrate how the latent structure evolves during the generative process and identify three trends. 1) The frequency domain of the latent basis changes from low to high frequency. It agrees with the previous observation that DMs generate samples in coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evolution of latent structures during generative processes</head><p>2) The difference between the tangent spaces of different samples increases over the generative process. It implies finding generally applicable editing direction in latent space becomes harder in later timesteps.</p><p>3) The differences of tangent spaces between timesteps depend on the complexity of the dataset.</p><p>Latent bases gradually evolve from low-to highfrequency structures. Figure <ref type="figure" target="#fig_5">6</ref> is the power spectral density (PSD) of the discovered latent basis over various timesteps. The early timesteps contain a larger portion of low frequency than the later timesteps and the later timesteps contain a larger portion of high frequency.</p><p>This suggests that the model focuses on low-frequency signals at the beginning of the generative process and then shifts its focus to high-frequency signals over time. This result strengthens the common understanding about the coarse-to-fine behavior of DMs over the generative process <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>The discrepancy of tangent spaces from different samples increases along the generative process.</p><p>To investigate the geometry of the tangent basis, we employ a metric on the Grassmannian manifold.</p><p>The Grassmannian manifold is a manifold where each point is a vector space, and the metric defined above represents the distortion across various vector spaces. We use geodesic metric <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b55">56]</ref> to define the discrepancy between two subspaces {T (1) , T (2) }: where θ k denotes the k-th principle angle between T (1)  and T (2) . Intuitively, the concept of geodesic metric can be understood as an angle between two vector spaces.</p><formula xml:id="formula_7">D geo (T (1) , T (2) ) = k θ 2 k ,<label>(5)</label></formula><p>Here, the comparison between two different spaces was conducted for {T h1 , T h2 }. Unlike the X , the H assumes a Euclidean space which makes the computation of geodesic metric that requires an inner product between tangent spaces easier. The relationship between tangent space and latent subspace is covered in more detail in Appendix E.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> demonstrates that the tangent spaces of the different samples are the most similar at t = T and diverge as timestep becomes zero.</p><p>Moreover, the similarity across tangent spaces allows us to effectively transfer the latent basis from one sample to another through parallel transport as shown in Figure <ref type="figure">8</ref>. In T , where the tangent spaces are homogeneous, we consistently obtain semantically aligned editing results. On the other hand, parallel transport at t = 0.6T does not lead to satisfactory editing because the tangent spaces are hardly homogeneous. Thus, we should examine the similarity of local subspaces to ensure consistent editing across samples.</p><p>DMs trained on simpler datasets exhibit more consistent tangent spaces over time. In Figure <ref type="figure">9</ref> (a), we provide a distance matrix of the tangent spaces across different timesteps, measured by the geodesic metric. We observe that the tangent spaces are more similar to each other when a model is trained on CelebA-HQ, compared to ImageNet. To verify this trend, we measure the geodesic distances between tangent spaces of different timesteps and plot the average distances of the same difference in timestep in Figure <ref type="figure">9</ref> (b). As expected, we find that DMs trained on datasets, that are generally considered simpler, have similar local tangent spaces over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of conditioning prompts on the latent structure</head><p>In this subsection, we aim to investigate how prompts influence the generative process from a geometrical perspective. We randomly sampled 50 captions from the MS-COCO dataset <ref type="bibr" target="#b28">[29]</ref> and used them as text conditions.</p><p>Similar text conditions induce similar tangent spaces. In Figure <ref type="figure" target="#fig_8">10</ref> (a), we observe a negative correlation between the CLIP similarity of texts and the distance between tangent spaces. In other words, when provided with similar texts, the tangent spaces are more similar. The linear relationship between the text and the discrepancy of the tangent spaces is particularly strong in the early phase of the generative process as shown by R 2 score in Figure <ref type="figure" target="#fig_8">10 (b)</ref>.</p><p>The generative process depends less on text conditions in later timesteps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section, we provide additional intuitions and implications. It is interesting that our latent basis usually conveys disentangled attributes even though we do not adopt attribute annotation to enforce disentanglement. We suppose that decomposing the Jacobian of the encoder in the U-Nets naturally yields disentanglement to some extent. However, it does not guarantee the perfect disentanglement and some directions are entangled. For example, the editing for beard converts a female subject to a male as shown in Figure <ref type="figure" target="#fig_9">11</ref> (a). This kind of entanglement often occurs in other editing methods due to the dataset bias: female faces seldom have beard.</p><p>While our method has shown effectiveness in Stable Diffusion, more research is needed to fully validate its potential. We have observed that some of the discovered latent vector occasionally leads to abrupt changes during the editing process in Stable Diffusion, as depicted in Figure <ref type="figure" target="#fig_9">11 (b</ref>). This observation highlights the complex geometry of X in achieving seamless editing. Exploring this topic in future research is an interesting area to delve into.</p><p>Our approach is broadly applicable when the feature space in the DM adheres to a Euclidean metric, as demonstrated by H. This characteristic has been observed in the context of U-Net within Kwon et al. <ref type="bibr" target="#b25">[26]</ref>. It would be intriguing to investigate if other architectural designs, especially those similar to transformer structures as introduced in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53]</ref>, also exhibit a Euclidean metric.</p><p>Despite these limitations, our method provides a significant advance in the field of image editing for DMs, and provides a deep understanding of DM through several experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have analyzed the latent space of DMs from a geometrical perspective. We used the pullback metric to identify the latent and tangent bases in X and H. The latent basis found by the pullback metric allows editing images by traversal along the basis. We have observed properties of the bases in two aspects. First, we discovered that 1) the latent bases evolve from low-to high-frequency components;</p><p>2) the discrepancy of tangent spaces from different samples increases along the generative process; and 3) DMs trained on simpler datasets exhibit more consistent tangent spaces over timesteps. Second, we investigated how the latent structure changes based on the text conditions in Stable Diffusion, and discovered that similar prompts make tangent space analogous but its effect becomes weaker over timesteps. We believe that a better understanding of the geometry of DMs will open up new possibilities for adopting DMs in useful applications.</p><p>Table A1: Hyper-parameter settings. model t edit inversion step γ n t boost Stable Diffusion 0.7T 100 1 50 × 0.6T 100 2 50 × Unconditional DMs T 100 0.5 50 0.2T 0.8T 100 1 50 0.2T 0.6T 100 4 50 0.2T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Societal Impacts &amp; Ethics Statements</head><p>Our research endeavors to unravel the geometric structures of the diffusion model and facilitate high-quality image editing within its framework. While our primary application resides within the creative realm, it is important to acknowledge that image manipulation techniques, such as the one proposed in our method, hold the potential for misuse, including the dissemination of misinformation or potential privacy implications. Therefore, the continuous advancement of technologies aimed at thwarting or identifying manipulations rooted in generative models remains of utmost significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation details</head><p>Models and datasets We validate our method and provide analyses on various models using the official code and pre-trained checkpoints. The available combinations of the models and the datasets are: DDPM <ref type="bibr" target="#b21">[22]</ref> on ImageNet <ref type="bibr" target="#b15">[16]</ref>, LSUN-church/bedroom/cat/horse <ref type="bibr" target="#b57">[58]</ref>, and CelebA-HQ <ref type="bibr" target="#b22">[23]</ref>; and DDPM trained with P2 weighting <ref type="bibr" target="#b11">[12]</ref> on FFHQ <ref type="bibr" target="#b23">[24]</ref>, Flowers <ref type="bibr" target="#b57">[58]</ref> and AFHQ <ref type="bibr" target="#b12">[13]</ref>. We also use Stable Diffusion (SD) version 2.1 <ref type="bibr" target="#b45">[46]</ref> for the text-conditional diffusion model.</p><p>For image editing, we use the official codes and pre-trained checkpoints for all baselines and keep the parameters frozen. For analysis, we compare models with the same diffusion scheduling (linear schedule) and resolutions (256 2 ) to ensure a fair comparison, except Stable Diffusion.</p><p>Table <ref type="table">B1</ref> summarizes various hyperparameter settings in our experiments. Specific details not covered in the main text are discussed in the following paragraphs.</p><p>Edit timestep (t edit ) For unconditional DMs, we show the editing results at t edit ∈ {T, 0.8T, 0.6T }, while for Stable Diffusion, we show the editing results at t edit ∈ {0.7T, 0.6T }. Note that our method allows manipulation at any timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inversion step</head><p>We conduct real image editing with DDIM inversion <ref type="bibr" target="#b50">[51]</ref>. We set the number of steps to 100 for obtaining the latent variable x T and all experiments.</p><p>x-space guidance scale (γ) The value of γ determines the magnitude of a single editing step by x-space guidance. Fortunately, through experimentation, we observed that the value of γ does not have a significant impact on image quality unless it is excessively large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-rank approximation (n)</head><p>We employ a low-rank approximation of the tangent space using n = 50 for all settings.</p><p>Quality boosting (t boost ) While DDIM alone already generates high-quality images, Karras et al. <ref type="bibr" target="#b24">[25]</ref> showed that including stochasticity in the process improves image quality and Kwon et al. <ref type="bibr" target="#b25">[26]</ref> suggest similar technique: adding stochasticity at the end of the generative process. We employ this technique in our experiments on every experiment after t = 0.2T , except Stable Diffusion.</p><p>Computing resource For power-method approximation with n = 50, it spends about 3-4 minutes on a single NVIDIA RTX 3090 (24GB). As n specifies the number of bases, it can be as small as a user want to use for image editing. Reducing n provides faster runtime, e.g., 10 seconds for n = 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation study</head><p>In this section, we validate our method with ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random v</head><p>To demonstrate the meaningfulness of the latent basis found by our method, we qualitatively compare its effect to naïve baseline: random directions. The first row in Figure <ref type="figure" target="#fig_10">A1</ref> shows that manipulating the images with a random vector 'v' does not result in semantic editing but rather degrades images. The second row shows the results of projecting the random 'v' onto our obtained latent subspace. The projected results exhibit semantic manipulation such as pose changes without image distortion. It indicates that the found latent subspace captures semantics in the latent space effectively.</p><p>x-space guidance Figure <ref type="figure" target="#fig_2">A2</ref> demonstrates the the effectiveness of x-space guidance compared to a straightforward alternative: simple addition. First, x-space guidance produces higher quality images with similar meaning. Especially Stable Diffusion apparently benefits from x-space guidance regarding smoothness of the editing strength and artifacts. The difference is more significant at t = 0.6T . Note that the meaning of the same directions may slightly differ between the two settings due to non-linearity of the U-Net.</p><p>Currently, we do not have a deeper understanding of the underlying principles of x-space guidance.</p><p>Exploring the reasons behind its ability to improve manipulation quality would be an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparative experiment to other state-of-the-art (SoTA) editing methods</head><p>We conduct qualitative comparisons with text-guided image editing methods. Our SoTA baseline methods include: (i) SDEdit <ref type="bibr" target="#b32">[33]</ref>, (ii) Pix2Pix-zero <ref type="bibr" target="#b38">[39]</ref>, (iii) PnP <ref type="bibr" target="#b53">[54]</ref>, and (iv) Instruct Pix2Pix <ref type="bibr" target="#b6">[7]</ref>. All comparisons were performed using the official code. Please refer to Figure <ref type="figure">A3</ref> for the qualitative results.</p><p>We also compare the time complexity of each method. For a fair comparison, we only identify the first singular vector v 1 , i.e., n = 1, and set the number of DDIM steps to 50. All experiments were conducted on an Nvidia RTX 3090. The runtime for each method is summarized in Table <ref type="table">A2</ref>. The computation cost of our method remains comparable to other approaches, although the Jacobian approximation takes around 2.5 seconds for n = 1. This is because we only need to identify the latent basis vector once at a specific timestep. Furthermore, our approach does not require additional preprocessing steps like generating 100 prompts with GPT and obtaining embedding vectors (as in Pix2Pix-zero), or storing feature vectors, queries, and key values (as in PnP). Our method also does not require finetuning (as in Instruct Pix2Pix). This leads to a significantly reduced total editing process time in comparison to other methods.</p><p>Table A2: Comparisons of the time complexity of state-of-the-art editing methods Image Edit Method Running time Preprocessing Ours 11 sec N/A SDEdit 4 sec N/A Pix2Pix-zero 25 sec 4 min PnP 10 sec 40 sec Instruct Pix2Pix 11 sec N/A  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Discussions</head><p>Why do we measure the geodesic distance of the tangent spaces instead of the latent subspaces?</p><p>The geodesic distance on the Grassmannian manifold between two subspaces is defined as the l 2 -norm of principal angles. To define angles between different vector spaces, an inner product needs to be defined. In our work, we define the inner product in T x using the pullback metric. The issue is that the pullback metric is locally defined for each latent subspace T x (Eq. ( <ref type="formula" target="#formula_0">1</ref>)). Therefore, measuring angles between distant latent subspaces becomes challenging. On the other hand, H follows the assumption of the Euclidean metric. Consequently, even for distant tangent spaces, angles can be easily computed using the dot product. In this regard, we measure the similarity between latent subspaces by exploiting the geodesic distance of their corresponding tangent spaces. Furthermore, when compared to X , H offers the advantage of being a semantic space, making it more suitable for measuring semantic similarity.</p><p>Similar tangent space implies similar latent subspace In Figure <ref type="figure" target="#fig_13">A4</ref>, we calculated the geodesic distance of tangent spaces obtained at different timesteps (or different samples at same the timestep) and the angle between the original latent direction and parallel transported direction between them. It is evident that as the geodesic distance decreases, the amount of distortion during parallel transport also reduces.</p><p>Notice that the similarity between tangent spaces implies consistency of latent basis across timesteps.</p><p>In Figure <ref type="figure" target="#fig_4">A5</ref> (b), we parallel transport the latent vector v i to various tangent spaces and visualize the outcomes. As expected, when the tangent spaces are similar, the transported vector v ′ i retains the original signal. On the other hand, as we move to more distant timesteps, where the tangent space is farther apart, v ′ i deviates from the original signal. denotes the latent vector transported from t a to t b . Transported vector significantly deviates from the original vector, as the tangent space grows further apart according to the distance matrix. For visualization purposes, v i is min-max normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Algorithms</head><p>In this section, for reproducibility purposes, we provide the code for two important algorithms. The code is implemented using PyTorch <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jacobian subspace iteration</head><p>The diffusion model has dimensions that are too large in both X and H, making the computation of the Jacobian infeasible. To overcome this challenge, we attempt the Jacobian subspace iteration algorithm to approximate the singular value of the Jacobian, as proposed in <ref type="bibr" target="#b18">[19]</ref>. For details, please refer to Haas et al. <ref type="bibr" target="#b18">[19]</ref>.  latent basis with given prompt As shown in Figure <ref type="figure" target="#fig_10">A11</ref>, when we condition a specific prompt, such as "Zebra" or "Chimpanzee", the entire latent basis corresponds to the prompt-related attributes. Notably, Changes to "zebra", which are clear, all show similar results, but "chimpanzee" show different results. Nevertheless, it is clear that they are all related to "chimpanzee".</p><p>Real image w/ "Zebra" Real image w/ "Chimpanzee"</p><p>Figure <ref type="figure" target="#fig_10">A11</ref>: More examples of image editing using top-5 latent basis vectors when given the prompt. Notably, Changes to "zebras", which are clear, all show similar results, but "chimpanzees" show different results. Nevertheless, it is clear that it is all related to "chimpanzees".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Image editing using latent basis vectors discovered with various prompts</head><p>We provide additional examples of image editing using latent basis vectors discovered with various prompts. Figure <ref type="figure" target="#fig_10">A12</ref>, A13 show image editing with various pictures and various prompts. For brevity, we denote the prompt "a cat dressed as a witch wearing a wizard hat in a haunted house" by "[ • • • cat • • ]" in Figure <ref type="figure" target="#fig_10">A12</ref>.  In this subsection, we provide a discussion based on the failure cases of our approach. Figure <ref type="figure" target="#fig_10">A14</ref> shows the results of latent basis found at t = T with Stable diffusion. Unlike unconditional models, the directions found at t = T exhibit rapid and drastic unexpected changes. However, landscape photos, which do not contain a main object, exhibited desired editing effects at any timesteps. Moreover, in the case of landscapes, it is conjectured that the latent basis plays a significant role in representing patterns and textures. Analyzing the landscape images generated by Stable diffusion would be an interesting topic.</p><p>Figure <ref type="figure" target="#fig_10">A15</ref> presents examples of failure cases in our image editing using latent basis vectors discovered with various prompts. (a) When using pose or action as a prompt, there are instances where the identity is not preserved. (b) When the shape of the target subject differs significantly from the source image, the results are often unsatisfactory. (c) There are cases where the preservation of the background is not achieved. (d) It is challenging to make significant changes to the entire image.</p><p>Regarding the reasons for these failure cases, we emphasize two factors. First, we manipulate in the X . The result in Figure <ref type="figure" target="#fig_10">A15</ref> (a) implies that X is not a space where disentanglement for identity is achieved effectively. On the other hand, in H, there are results indicating successful preservation of identity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>. Investigating the disentanglement capability of X and any other distinguishing features it may have compared to H would be an interesting future research topic.</p><p>Secondly, we perform manipulation by adding and subtracting the "signal" that the model pays attention to in x t . Here, The signal is captured from the current input x t , which limits the deviation from the original form. Therefore, when there is a substantial difference in shape, such as transforming a giraffe into a tiger, the results may not be satisfactory. Despite these limitations, we have successfully achieved direct manipulation in the latent space x t at a single timestep, which, to our knowledge, is the first of its kind. Through this, we provide insights into the model and contribute to the understanding of the latent space, hopefully benefiting the diffusion community. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conceptual illustration of local geometric structure. (a) The local basis {v 1 , v 2 , • • • } of the local latent subspace T xt within the latent space X is interlinked with the local basis {u 1 , u 2 , • • • } of the local tangent space T ht in the feature space H. (b) The derivation of these local bases is facilitated through the singular value decomposition (SVD) of the Jacobian, which emanates from the U-Net responsible for encoding the feature map f , linking X and H.</figDesc><graphic coords="2,176.57,69.66,176.98,128.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Image editing with the discovered latent basis. (a) Schematic depiction of our image editing procedure. ① An input image is subjected to DDIM inversion, resulting in an initial noisy sample x T . ② The sample x T is progressively denoised until reaching the point t through DDIM generation. ③ Subsequently, the local latent basis {v 1 , • • • , v n } is identified by using the pullback metric. ④ This enables the manipulation of the sample x t along one of the basis vectors using x-space guidance. ⑤ The DDIM generation concludes with the progression from the modified latent variable xt . (b) Examples of edited images using a selected basis vector. The latent basis vector could be conditioned on prompts and it facilitates text-aligned manipulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Examples of image edition using the latent basis. The attributes are manually interpreted since the editing directions are not intentionally supervised. For Stable Diffusion, we used an empty string as a prompt. Each column represents edits made at different diffusion timesteps (0.6T , 0.8T , and T for the unconditional diffusion model; 0.6T and 0.7T for Stable Diffusion).</figDesc><graphic coords="6,122.93,460.96,52.37,52.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of image edition using latent basis vectors discovered with various prompts.Each column is edited using the latent basis vector obtained from a different text prompt. Importantly, our method employs each prompt only once to derive the local latent basis.</figDesc><graphic coords="7,112.41,197.83,57.05,57.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Power Spectral Density (PSD) of latent basis. The PSD at t = T (purple) exhibits a greater proportion of lowfrequency signals, while the PSD at smaller t (beige) reveals a larger proportion of highfrequency signals. The latent vectors v i are min-max normalized for visual clarity.</figDesc><graphic coords="7,359.73,448.98,95.25,89.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Geodesic distance across tangent space of different samples at various diffusion timesteps. Each point represents the average geodesic distance between pairs of 15 samples. It is notable that the similarity of tangent spaces among different samples diminishes as the generative process progresses.</figDesc><graphic coords="8,364.97,187.02,138.84,123.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :Figure 10 :</head><label>8910</label><figDesc>Figure 8: Examples of image edition using parallel transport. The first row demonstrates the results of editing with their respective latent vectors, while the subsequent rows exhibit the results of editing through the parallel transport (P.T) of the latent vectors used in the first row. The latent vector performs effectively when t = T (left and middle), but comparatively less satisfactorily for 0.6T (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10</head><label>10</label><figDesc>(c) illustrates the distances between local tangent spaces for given different prompts with respect to the timesteps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Limitations. (a) Entanglement between attributes due to dataset biases (b) Abrupt changes in Stable Diffusion.Notably, as the diffusion timestep approaches values below 0.7T , the distances between the local tangent spaces start to decrease. It implies that the variation due to walking along the local tangent basis depends less on the text conditions, i.e., the text less influences the generative process, in later timesteps. It is a possible reason why the correlation between the similarity of prompts and the similarity of tangent spaces reduces over timesteps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A1 :</head><label>A1</label><figDesc>Figure A1: Importance the discovered latent directions. Random direction experiments with CelebA-HQ pre-trained model (top) and Stable Diffusion (bottom). Adding random directions instead of latent directions severely distorts the resulting images. When we perform edits along the projection onto the latent subspace T x , the generated image presents a semantically meaningful transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A2 :Figure A3 :</head><label>A2A3</label><figDesc>Figure A2: Importance of the x-space guidance. x-space guidance experiments with AFHQ (top), Flowers pre-trained model (middle), and Stable Diffusion (bottom). x-space guidance helps achieve qualitatively similar editing while preserving the content of the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>(a) Parallel transport across timesteps (b) Parallel transport across samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A4 :</head><label>A4</label><figDesc>Figure A4: Parallel transport between similar tangent spaces creates similar latent directions. The horizontal axis represents the geodesic distance between tangent spaces from (a) different timesteps (b) different samples at t ∈ {T, 0.9T, • • • , 0.1T }. The vertical axis represents the angle between the original latent direction and transported latent direction. Different colors represent various datasets. A positive relationship is observed between tangent space distance and the distortion induced by parallel transport.</figDesc><graphic coords="18,151.51,161.94,138.11,156.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>Figure A5: More examples of tangent spaces across diffusion timesteps. (a) Distance matrix visualization of tangent space measured by geodesic metric across various timesteps. (b) Visualization of the result from parallel transport across timesteps. v ′ ta→t b i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>1 2 3 8 - 15 ''' 16 # 19 #</head><label>28151619</label><figDesc>import torch # &gt;= ver 2.0 def l oc al_ enc od er_ pu llb ac k ( 4 x , t , get_h , n =50 , chunk_size =25 , min_iter =10 , max_iter =100 , con vergen ce_thr eshol d =1 e -4 , x : tensor ; latent variable 9 -t : tensor ; diffusion timestep 10 -get_h : function ; return h given x , t 11 -n ; low -rank approximation dimension 12 -chunk_size ; To avoid OOM error 13 -min_iter ( max_iter ) ; minimum ( maximum ) number of iteration 14 -con verge nce_th reshol d ; to check convergence of power -method set number of chunk to avoid OOM 17 num_chunk = n // chunk_size + 1 18 get dimensions of x space and h space 20 h_shape = get_h (x , t ) . shape 21 c_i , w_i , h_i = x . size (1) , x . size (2) , x . size (3)22c_o , w_o , h_o = h_shape<ref type="bibr" target="#b0">[1]</ref> , h_shape<ref type="bibr" target="#b1">[2]</ref> , h_shape<ref type="bibr" target="#b2">[3]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>25 aListing 1 :import torch 2 3Figure A8 :Figure A10 :</head><label>2512A8A10</label><figDesc>Figure A8: More examples of image editing using the latent basis in Flowers. The editing result using ten v i ' in Flowers. Each column represents edits made at different diffusion timesteps (0.6T , 0.8T , and 1T ). Editing at timestep 1T yields coarse changes. On the other hand, editing at timestep 0.6T leads to fine changes. Please zoom in for the best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><figDesc>• cat• • • ] → [• • • girl• • • ] • cat• • • ] → [• • • dog• • • ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure A12 :Figure A13 :</head><label>A12A13</label><figDesc>Figure A12: More examples of image editing using latent basis vectors discovered with various prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><figDesc>(FigureA15 (b)) When we utilize text conditions, the latent basis aligns with the text information. This leads to not capturing background information, resulting in changes in the background when manipulated. It is also an interesting research topic to capture signals related to the background. (FigureA15 (c)) Since the model focuses on finer features as t approaches 0, if broad changes are desired, manipulation should be performed at t = T . However, manipulation at t = T is unstable. Deep analysis of x t at t = T in Stable diffusion is also an intriguing research topic. (FigureA15 (d))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure A14 :Figure A15 :</head><label>A14A15</label><figDesc>Figure A14: Failure cases of image editing using the latent basis at 1T . The editing result using v i ' in Stable diffusion at 1T .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_0"><p>for i in range ( max_iter ) :</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgement</head><p>This work was supported in part by the <rs type="funder">Creative-Pioneering Researchers Program through Seoul National University</rs>, the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant (Grant No. <rs type="grantNumber">2022R1A2C1006871</rs>) (J. J.), <rs type="funder">KIAS</rs> <rs type="grantName">Individual Grant</rs> [<rs type="grantNumber">AP087501</rs>] via the <rs type="institution">Center for AI and Natural Sciences at Korea Institute for Advanced Study</rs>, and the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant (<rs type="grantNumber">RS-2023-00223062</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ne3ZHN7">
					<idno type="grant-number">2022R1A2C1006871</idno>
				</org>
				<org type="funding" xml:id="_tcPxRGg">
					<idno type="grant-number">AP087501</idno>
					<orgName type="grant-name">Individual Grant</orgName>
				</org>
				<org type="funding" xml:id="_7Njpj36">
					<idno type="grant-number">RS-2023-00223062</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for vi in v_buffer : </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows</title>
		<author>
			<persName><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Latent space oddity: on the curvature of deep generative models</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Arvanitidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Hauberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11379</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Geometrically enriched latent spaces</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Arvanitidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00565</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blended diffusion for text-driven editing of natural images</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18208" to="18218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01324</idno>
		<title level="m">Text-to-image diffusion models with an ensemble of expert denoisers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Label-efficient semantic segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03126</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instructpix2pix: Learning to follow image editing instructions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18392" to="18402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attend-andexcite: Attention-based semantic guidance for text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Vinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13826</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metrics for deep generative models</title>
		<author>
			<persName><forename type="first">Nutan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexej</forename><surname>Klushyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Kurle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Smagt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1540" to="1550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyeon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geonho</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myungjoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06959</idno>
		<title level="m">Do not escape from the manifold: Discovering the local coordinates on the latent space of gans</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghyun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><surname>Ilvr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02938</idno>
		<title level="m">Conditioning method for denoising diffusion probabilistic models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perception prioritized training of diffusion models</title>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaehun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11472" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving diffusion models for inverse problems using manifold constraints</title>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongsu</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dohoon</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00941</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.17115</idno>
		<title level="m">Multiresolution textual inversion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix computations</title>
		<imprint>
			<publisher>JHU press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discovering interpretable directions in the semantic latent space of diffusion models</title>
		<author>
			<persName><forename type="first">René</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Huberman-Spiegelglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Mulayoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11073</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ganspace: Discovering interpretable gan controls</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Härkönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9841" to="9850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Prompt-to-prompt image editing with cross attention control</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01626</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Elucidating the design space of diffusion-based generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diffusion models already have a semantic latent space</title>
		<author>
			<persName><forename type="first">Mingi</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10960</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On explicit curvature regularization in deep generative models</title>
		<author>
			<persName><forename type="first">Yonghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A statistical manifold framework for point cloud data</title>
		<author>
			<persName><forename type="first">Yonghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Park</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v162/lee22d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V 13</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">More control for free! image synthesis with semantic diffusion guidance</title>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chopikyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05744</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Repaint: Inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11461" to="11471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09813</idno>
		<title level="m">Diffusionseg: Adapting diffusion towards unsupervised object discovery</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09794</idno>
		<title level="m">Null-text inversion for editing real images using guided diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Drag your gan: Interactive point-based manipulation on the generative image manifold</title>
		<author>
			<persName><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leimkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimitra</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2023 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03027</idno>
		<title level="m">Zero-shot image-to-image translation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A spectral regularizer for unsupervised disentanglement</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01161</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generating high fidelity data from low-density regions using diffusion models</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firat</forename><surname>Ozgenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Canton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11492" to="11501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The riemannian geometry of deep generative models</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fletcher</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Closed-form factorization of latent semantics in gans</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1532" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Guy</forename><surname>Tevet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sigal</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Shafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14916</idno>
		<title level="m">Human motion diffusion model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Plug-and-play diffusion features for text-driven image-to-image translation</title>
		<author>
			<persName><forename type="first">Narek</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12572</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Open-vocabulary panoptic segmentation with text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalini</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04803</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Schubert varieties and distances between subspaces of different dimensions</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lek-Heng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1176" to="1197" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regularized autoencoders for isometric representation learning</title>
		<author>
			<persName><forename type="first">Sangwoong</forename><surname>Lee Yonghyeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">C</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Latentclr: A contrastive learning approach for unsupervised discovery of interpretable directions</title>
		<author>
			<persName><forename type="first">Enis</forename><surname>Oguz Kaan Yüksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ezgi</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Gülperi Er</surname></persName>
		</author>
		<author>
			<persName><surname>Yanardag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14263" to="14272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Low-rank subspaces in gans</title>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16648" to="16658" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
