The research on characterizing high-order interdependence via entropic conjugation presents a comprehensive framework for understanding complex relationships in multivariate systems. Below are detailed technical explanations and justifications for the researchers' decisions regarding various aspects of their work:

### Definition of High-Order Interdependence
High-order interdependence refers to the relationships among three or more variables that cannot be fully understood by examining pairwise interactions alone. This definition is crucial as it acknowledges that systems often exhibit complex dependencies that require a higher-dimensional perspective. The researchers emphasize the need for metrics that can capture these intricate relationships, which are essential for accurately modeling phenomena in physical, biological, and social systems.

### Choice of Information-Theoretic Metrics
Information-theoretic metrics are chosen because they provide a quantitative framework for measuring dependencies among random variables. These metrics, derived from Shannon's entropy, allow for the assessment of uncertainty and information flow within a system. The researchers focus on these metrics due to their established theoretical foundations and their ability to capture both redundancy and synergy in high-order interactions.

### Adoption of Shannon Entropy as a Basis
Shannon entropy serves as a foundational concept in information theory, quantifying the uncertainty associated with a random variable. The researchers adopt it as a basis because it provides a robust mathematical framework for defining and analyzing interdependencies. By leveraging Shannon entropy, they can derive various metrics that reflect the complexity of interactions among multiple variables.

### Introduction of Entropic Conjugation
Entropic conjugation is introduced as a novel concept to explore the relationships between different information-theoretic metrics. This approach allows for a systematic examination of how metrics can be transformed and related to one another. The researchers argue that entropic conjugation provides a unifying principle that clarifies the nature of existing metrics and highlights gaps in the literature regarding high-order interdependencies.

### Establishing Properties of Labelling-Symmetry and Dependency
The properties of labelling-symmetry and dependency are established to ensure that the metrics are invariant to permutations of the variables and that they accurately reflect interdependencies. Labelling-symmetry guarantees that the analysis is not biased by the naming of variables, while the dependency property ensures that the metrics only capture interactions when variables are not independent. These properties are essential for the validity and applicability of the proposed metrics.

### Decomposition of Metrics into Symmetric and Skew-Symmetric Components
The decomposition of metrics into symmetric and skew-symmetric components is justified as a means to understand the balance between different types of interdependencies. Symmetric components reflect equal contributions from high- and low-order interactions, while skew-symmetric components highlight the differences between them. This decomposition provides a clearer geometric interpretation of the metrics and facilitates the analysis of their properties.

### Selection of Specific Metrics for Analysis (e.g., TC, DTC, TSE)
The selection of specific metrics such as Total Correlation (TC), Dual Total Correlation (DTC), and Tononi-Sporns-Edelman (TSE) complexity is based on their established relevance in the literature and their ability to capture different aspects of interdependence. Each metric offers unique insights into the structure of dependencies, making them suitable for a comprehensive analysis of high-order interdependencies.

### Computational Efficiency Considerations for High-Order Metrics
The researchers emphasize computational efficiency due to the exponential growth of complexity with the number of variables. They focus on metrics that can be computed using a linear number of entropy terms, which is crucial for practical applications in large systems. This consideration ensures that the proposed framework remains feasible for empirical studies.

### Framework for Characterizing High-Order Interdependencies
The framework developed for characterizing high-order interdependencies is grounded in the principles of entropic conjugation and the established properties of the metrics. This framework allows for a systematic exploration of the relationships between different metrics and provides a structured approach to analyzing complex systems. It serves as a foundation for future research and applications in various fields.

### Relationship Between Existing Metrics and Entropic Conjugation
The relationship between existing metrics and entropic conjugation is explored to reveal how different metrics can be unified under a common theoretical framework. This relationship clarifies the connections between seemingly disparate metrics and provides insights into their complementary roles in analyzing high-order interdependencies.

### Identification of Gaps in the Literature for High-Order Metrics
The researchers identify gaps in the literature regarding the characterization of interactions involving more than five variables. This identification is crucial for guiding future research efforts and highlights the need for the development of new metrics and methodologies to address these gaps.

### Use of Partial Information Decomposition (PID) in Analysis
Partial Information Decomposition (PID) is employed to analyze how information about a variable can be attributed to its components. PID allows for a nuanced understanding of redundancy and synergy in high-order interactions, providing a more detailed picture of the interdependencies within a system. This approach complements the information-theoretic metrics and enhances the overall analysis.

### Theoretical Grounding for Symmetry and Skew-Symmetry in Metrics
The theoretical grounding for symmetry and skew-symmetry in metrics is established through the properties of entropic