# Characterising high-order interdependence via entropic conjugation

## Abstract

## 

High-order phenomena play crucial roles in many systems of interest, but their analysis is often highly nontrivial. There is a rich literature providing a number of alternative information-theoretic quantities capturing high-order phenomena, but their interpretation and relationship with each other is not well understood. The lack of principles unifying these quantities obscures the choice of tools for enabling specific type of analyses. Here we show how an entropic conjugation provides a theoretically grounded principle to investigate the space of possible high-order quantities, clarifying the nature of the existent metrics while revealing gaps in the literature. This leads to identify novel notions of symmetry and skew-symmetry as key properties for guaranteeing a balanced account of high-order interdependencies and enabling broadly applicable analyses across physical systems.

## 

Physical and biological systems often exhibit relationships between their parts that cannot be reduced to dependencies in subsets of them [[1]](#b1). The study of these high-order interdependencies has lead to new insights in a wide range of physical systems [[2]](#b2)[[3]](#b3)[[4]](#b4), and also in studies involving genetics [[5,](#b5)[6]](#b6) and neural systems (both biological [[7]](#b7)[[8]](#b8)[[9]](#b9)[[10]](#b10)[[11]](#b11) and artificial [[12]](#b12)[[13]](#b13)[[14]](#b14)), to name a few. Overall, qualitatively different types of interdependence have been found to play complementary roles balancing needs for robustness and flexibility [[15,](#b15)[16]](#b16).

There are different approaches to quantify high-order phenomena [[17]](#b17), among which we focus on informationtheoretic metrics based on Shannon entropy. While there is a rich literature offering such metrics, their interpretation is highly non-trivial -being unclear if these quantities are capturing the same effects or instead provide complementary perspectives. This lack of clarity makes it challenging for researchers to choose the right tools to carry out specific types of analyses, severely hindering the study of high-order phenomena.

Here we address this issue by introducing the notion of entropic conjugation, which establishes a theoretically grounded principle to explore the space of possible highorder quantities. Our results show that the existent highorder metrics have a closer relationship than previously thought, while revealing gaps in the literature for characterising interactions involving more than 5 variables. Moreover, the notions of symmetry and skew-symmetry with respect to conjugation emerge as key guarantees for providing a balanced account of high-order interdependence, enabling analyses that can illuminate the highorder profile of a wide range of physical systems. The proofs of our results can be found in the Appendix.

Measures of multivariate interdependence. Let's consider a system with a state is specified by the random vector X = (X 1 , . . . , X n ) following a joint distribution p X and marginal distributions p Xi . The literature presents various metrics to assess the dependencies between parts of X; here we focus on linear combinations of entropies of the form ϕ(X) = a⊆In λ a H(X a ), with I n = {1, . . . , n}, X a is a vector of variables whose indices are in a ⊆ I n , H is Shannon's entropy, and λ a are scalars. We require these metrics to satisfy two key properties:

(i) Labelling-symmetry: ϕ(X) is invariant to permutations among X 1 , . . . , X n .

(ii) Dependency: ϕ(X) = 0 if the variables are jointly independent (i.e. p X = n k=1 p X k ). Hence, property (i) guarantees that ϕ does not depend on how variables are named and (ii) that it only captures interactions effects between variables [[18]](#b18).

There are several well-known metrics that satisfy these properties. The oldest of these is the interaction information [[19]](#b19), which is defined as

$II(X) := n k=1 (-1) k+1 |a|=k H(X a ).(1)$Other metrics of interdependence are the total correlation (TC) [[20]](#b20) and the dual total correlation (DTC) [[21]](#b21), which are given by TC(X) := n j=1 H(X j ) -H(X) and ( [2](#formula_36))

$DTC(X) := H(X) - n j=1 H(X j |X -j ).(3)$arXiv:2410.10485v1 [cs.IT] 14 Oct 2024

Another such metric, well-known in computational neuroscience, is the Tononi-Sporns-Edelman (TSE) complexity [[22]](#b22), which is defined as

$TSE(X) := ⌊n/2⌋ k=1 n k -1 |a|=k I(X a ; X -a ),(4)$where -a is the set of indices of X that are not in a.

Finally, we also consider the more recently introduced O-information and S-information [[23]](#b23):

$Ω(X) := (n -2)H(X) + n j=1 H(X j ) -H(X -j ) ,(5)$$Σ(X) := n j=1 I(X j ; X -j ).(6)$Of these, TC, DTC, TSE, and Σ are non-negative, while II and Ω can take positive and negative values. We will show these seemingly unrelated metrics can be parsimoniously unified under the concept of entropic conjugation.

Characterising high-order interdependencies. Let us introduce the following average quantities:

$u k (X) := 1 n k+1 k+1 2 i,j∈In i<j |a|=k-1 i,j / ∈a I(X i ; X j |X a ),(7)$with k = 1, . . . , n -1. These quantities satisfy labellingsymmetry and dependency, and capture the interdependencies between k variables -i.e. u j (X) = 0 for j < k if and only if all subsets of k variables or less are statistically independent. Furthermore, it has been shown that all information-theoretic metrics ϕ satisfying labelling-symmetry and dependency can be expressed as

$ϕ(X) = n-1 k=1 c k u k (X)$, where c k ∈ R captures the relevance of (k + 1)-th order dependencies on ϕ [[24]](#b24). Moreover, this decomposition is unique in guaranteeing that ϕ is non-negative if and only if c k ≥ 0 for k = 1, . . . , n-1.

A complementary, more fine-grained way of investigating high-order interdependence is enabled by partial information decomposition (PID), which addresses how information about a variable Y provided by X may be decomposed into the contributions of its different components [[25]](#b25)[[26]](#b26)[[27]](#b27). PID reveals that while pairwise interdependence is quantified by its strength (measured e.g. by the mutual information), higher-order relationships can be of qualitatively different kinds -most notably redundant (multiple variables sharing the same information) or synergistic (a set of variables holding some information that cannot be seen from any subset). Moreover, PID recognises that synergy and redundancy can be mixed in non-trivial ways, and explores this thoroughly via an algebraic construction that leads to the decomposition

$I(X; Y ) = α∈An I α ∂ (X; Y ),(8)$where A n is a collection of elements α that cover all possible combinations of redundancy and synergy (App. B). For example, if n = 2 then A 2 has four elements: α 1 = {{1}{2}} corresponding to the redundancy between X 1 and X 2 , α 2 = {{1, 2}} corresponding to the synergy between them, and α 3 = {{1}} and α 4 = {{2}} corresponding to unique information in one but not the other.

A conjugation of Shannon quantities. We are now ready to introduce the notion of entropy conjugation. Definition 1. The entropic conjugation is defined by

$H(X a ) * :=H(X -a ) -H(X).(9)$The conjugation of a linear combination of entropies

$ϕ(X) = a⊆In λ a H(X a ) is ϕ(X) * := a⊆In λ a H(X a ) * . (10$$)$It can be seen that * is a proper conjugation, as it is linear (by definition) and an involution, as ((H) * ) * = H. Also, a direct calculation shows that entropic conjugation acts on the mutual information as follows:

$I(X a ; X b |X c ) * = I X a ; X b |X -(a∪b∪c) ,(11)$where a, b, c are disjoint subsets of indices. Furthermore, one can show that entropic conjugation exchanges highfor low-order interdependencies, which will be the basis of our analysis of high-order quantities in the next section.

$Proposition 1. u k (X) * = u n-k (X).$A deeper insight on the effect of conjugation can be attained by looking at it via the PID framework. Our next result shows that entropic conjugation is the unique operation that arises from applying the duality principle from order theory [[28]](#b28) to PID, which results in a natural conjugation of atoms † that exchanges redundancies for synergies and vice-versa (for example, if α = {{1, 2}} then α † = {{1}, {2}}). Crucially, this holds for any operationalisation of synergy and redundancy that is consistent with the PID framework (see App. B).

Theorem 1. The natural conjugation of PID atoms † arising from order duality satisfies

$I(X a ; Y |X b ) † = α∈A b a I α † ∂ (X; Y ) = I(X a ; Y |X b ) * , (12$$)$where A b a is a suitable collection of atoms (see Lemma 2). Let's illustrate this result with a simple example. Using the fact that the O-information is equal to redundancy minus synergy [[23]](#b23), Th. 1 implies that Ω(X 1 ; X 2 ; Y )

$* = I {{1},{2}} † ∂ (X; Y ) -I {{1,2}} † ∂ (X; Y ) = I {{1,2}} ∂ (X; Y ) -I {{1},{2}} ∂ (X; Y ) = -Ω(X 1 ; X 2 ; Y ). (13$$)$Symmetric and skew-symmetric metrics. We now use the entropic conjugation to introduce the notions of symmetric and skew-symmetric interdepence quantities.

$Definition 2. A linear combination of entropies ϕ is symmetric if (ϕ) * = ϕ and skew-symmetric if (ϕ) * = -ϕ.$This definition, combined with Prop. 1 and Th. 1, implies that symmetric and skew-symmetric quantities provide balanced accounts of low-and high-order interdependencies (alternatively, redundancies and synergies): symmetric quantities weight these equally, while skewsymmetric weights them equally but with opposite signs. Thus, a practical way to recognise symmetric and skewsymmetric high-order metrics is via their weights in terms of the basis u k , as shown next.

$Lemma 1. If ϕ(X) = k=1 c k u k (X), then • ϕ is symmetric ⇐⇒ c k = c n-k . • ϕ is skew-symmetric ⇐⇒ c k = -c n-k .$With these tools at hand, we now study the existent high-order metrics under the lens of conjugation.

Proposition 2. The mentioned multivariate metrics can be decomposed as follows:

$TC(X) = n-1 k=1 (n -k)u n k (X),(14)$$DTC(X) = n-1 k=1 ku n k (X),(15)$$TSE(X) = n-1 k=1 k(n -k) 2 u n k (X),(16)$$Σ(X) = n n-1 k=1 u n k (X),(17)$$Ω(X) = n-1 k=1 (n -2k)u n k (X),(18)$$II(X) = n-1 k=1 (-1) k+1 n -2 k -1 u n k (X).(19)$Therefore, the following relationships hold:

$TC(X) * = DTC(X),(20)$$Σ(X) * = Σ(X),(21)$TSE(X) * = TSE(X), ( [22](#))

$Ω(X) * = -Ω(X),(23)$$II(X) * = (-1) n II(X). (24$$)$These results show that the S-information and TSE complexity are balanced metrics of overall interdependence strength, while the O-information provides a balanced opposition between high-and low-order interdependencies. In contrast, the interaction information alternates between being symmetric or skew-symmetric in a way that will be better understood in the next subsection. Additionally, this result also shows that the TC and DTC are not balanced metrics, being duals to each other: the TC provides more weight to low-order effects while the DTC to high-order ones.

These results also reveal that this collection of metrics is not as arbitrary as it may seem: when seen from their coefficients c k , they cover the constant (S-information), linear (TC, DTC, and O-information), quadratic (TSE), and binomial (interaction information) cases.

Spanning the possible metrics. We now show that entropic conjugation induces a decomposition of high-order quantities into symmetric and skew-symmetric components -revealing that skew-symmetric quantities are akin to the imaginary part of complex numbers.

Theorem 2. Every information-theoretic metric of interdependence ϕ can be decomposed into unique symmetric and skew-symmetric components as follows:

$ϕ = 1 2 ϕ + ϕ * symmetric + 1 2 ϕ -ϕ * skew-symmetric . (25$$)$Moreover, symmetric and skew-symmetric components are orthogonal under the inner product induced by

$⟨u k , u j ⟩ = δ k j .$This result provides a guide to investigate the geometry of the (n -1)-dimensional space of high-order metrics ϕ satisfying labelling-symmetry and dependency, which we denote by I n .

Corollary 1. If X has n variables, then the (n -1) dimensions of I (X) are divided in the following way:

$dim I n = ⌊n/2⌋ symmetric + ⌊(n -1)/2⌋ skew-symmetric . (26$$)$These results have the following consequences:

-For n = 2 variables, Shannon's mutual information is the only symmetric functional (up to scaling), as there are no skew-symmetric functionals.

-For n = 3 variables, the S-information is the only symmetric functional and the O-information (equivalently, the interaction information) is the only skew-symmetric one (up to scaling). Therefore, if ϕ ∈ I 3 then ϕ(X) = αΣ(X) + βΩ(X).

-For n = 4 variables, the S-information and the interaction information span the subspace of symmetric metrics, while the O-information is the only skew-symmetric one (up to scaling). Therefore, if ϕ ∈ I 4 then ϕ(X) = αΣ(X) + α ′ II(X) + βΩ(X). The variability among u k is captured by two principal components: one of symmetric character which accounts for the overall strength of the interdependence (PC1), and one of skew-symmetric character that accounts for the balance between high-and low-order interdependence (PC2). c) The values of u k projected onto these PCs provide a simple characterisation of these three types of systems in terms of their overall interdependence strength (PC1) and the balance between high-and low-order effects (PC2).

-For n = 5 variables, the space of symmetric metrics is spanned by the S-information and the TSEcomplexity, and the space of skew-symmetric metrics is spanned by the O-information and the interaction information. Therefore, if ϕ ∈ I 5 then ϕ(X) = αΣ(X) + α ′ TSE(X) + βΩ(X) + β ′ II(X).

Larger systems can be analysed in a similar fashion, but the existing metrics do not cover all the dimensions.

Computational tractability. Most u k (and, therefore, most high-order metrics) require estimating a large number of information-theoretic terms, and hence their computation becomes unfeasible when n grows. Our next result characterises the space of possible computationallyefficient symmetric and skew-symmetric. Proposition 3. The S-information and O-information are the only symmetric and skew-symmetric interdependence metrics that can be computed using a linear number of entropy terms.

Note that the TC and DTC also require a linear number of terms, but they are neither symmetric or skewsymmetric -in fact, their decomposition via Th. 2 yields TC = (Σ + Ω)/2 and DTC = (Σ -Ω)/2. Empirical results. To illustrate the applicability of this framework, we investigated the interdependencies exhibited by small spin systems under weak, ferromagnetic (positive), and frustrated (negative) types of interactions (App. C). The latter condition makes it impossible to simultaneously satisfy the tendency of all spins to be different from their neighbours, which is known for inducing high-order interdependencies [[17]](#b17).

To investigate the interdependencies of these systems, we calculated the values of u k according to Eq. ( [7](#formula_5)) and identified the principal axes of variability by via principal component analysis (App. C). Results show that two components explain almost all the variability: a first component of symmetric character similar to the Sinformation, and a second component of skew-symmetric character similar to the O-information (Fig. [1](#fig_0)). In other words, an optimal information-theoretic analysis to characterise the high-order interdependence of these systems reduces to two keys aspects: (i) their strength, and (ii) the balance between high-and low-order components.

## Conclusion.

Here we investigated the space of possible metrics of high-order interdependence taking the form of linear combinations of Shannon entropies. We introduced the notion of entropic conjugation, the effect of which can be understood in two complementary ways: as exchanging how metrics account for high-and low-order interdependencies, or alternatively, how they account for redundancies and synergies. Crucially, while multiple operationalisation of synergy and redundancy exist [[26]](#b26), the properties of entropy conjugation hold for all approaches that are consistent with the PID formalism.

When studying high-oder quantities, non-negative metrics such as the S-information and TSE complexity were found to be invariant (i.e. symmetric) under entropic conjugation, confirming that they provide a balanced account of overall interdependence strength. Similarly, applying entropic conjugation to a signed metric such as the O-information results in a minus sing (i.e. skew-symmetric), guaranteeing that it provides a fair bal-ance of the relative strength of redundancies and synergies. The interaction information was found to be either symmetric or skew-symmetric depending on the number of variables, providing a principled explanation to the observation (first made in Ref. [[25]](#b25)) that interpreting this quantity from a high-order perspective requires nuance.

This framework also let us prove that the well-known high-order metrics cover all possibilities when considering systems of up to n = 5 variables, while the space of possible metrics for capturing interactions involving more variables remains largely unexplored. Additionally, the S-information and O-information were found to be the symmetric and skew-symmetric quantities that are most computationally efficient, and numerical analyses showed their relevance when studying physical systems.

## I α

∂ (X; Y ) quantifies the information about the target variable Y that is accessible via each collection of source variables α 1 , . . . , α l , while not being accessible via subsets of those collections or any other collections that not include them. For example, if n = 2 then I {{1,2}} ∂ (X; Y ) corresponds to the information accessible in (X 1 , X 2 ) but not accessible from either X 1 or X 2 by themselves.

The accessibility relations denoted by PID antichains can be made explicit by 're-representing' them in terms of Boolean functions f : P n → {0, 1}, where P n is the powerset of {1, . . . , n}, taking a set of source indices as an input and returning 0 or 1 depending on whether the associated atom I f ∂ (X; Y ) is or isn't accessible via the set of sources. For example, the atom α = {{1, 2}} corresponds to the Boolean function that gives f (∅) = f ({1}) = f ({2}) = 0 and f ({1, 2}) = 1. Crucially, it has been shown that there is a natural isomorphism between PID antichains and monotonic Boolean functions [[29,](#b29)[30]](#b30), which implies that PID can be re-defined as follows. Definition 3. A PID of the information provided by

$X = (X 1 , . . . , X n ) about Y is a set of quantities I f ∂ (X; Y ) that satisfy for all a ⊆ {1, . . . , n} I(X a ; Y ) = f ∈Bn f (a)=1 I f ∂ (X; Y ),(B1)$where B n is the set of all non-constant monotonic Boolean functions f : P n → {0, 1}.

Note that Eq. (B1) is equivalent to Eq. ( [8](#formula_7)), with the only difference being the way in which PID atoms are labeled (either as antichains or Boolean functions). That said, viewing PID in terms of Boolean functions lets us conveniently handle various expressions, as shown below.

Lemma 2. Given two disjoint sets of source variables a and b, we have

$I(X a ; Y |X b ) = f ∈B b a I f ∂ (X; Y ),(B2)$where

$B b a = {f ∈ B n : f (a ∪ b) = 1, f (b) = 0}.$Note that the set A b a used in Th. 1 corresponds to the same atoms in B b a but represented in antichain form instead of as Boolean functions.

Proof.

## I(X

$a ; Y |X b ) = I(X a , X b ; Y ) -I(X b ; Y ) = f ∈Bn f (a∪b)=1 I f ∂ (X; Y ) - f ∈Bn f (b)=1 I f ∂ (X; Y ), (B3)$from which the desired result follows.

The information atoms have a natural order in terms of their accessibility: an atom I f ∂ (X; Y ) can be said to be 'more accessible' than an atom I g ∂ (X; Y ) if any set via which the latter is accessible is also a set via which the former is accessible. This property is elegantly captured by the Boolean function representation of atoms via the following partial ordering: f ⊑ g if and only if f (a) ≤ g(a) ∀a ∈ P n .

(B4)

Note that this partial ordering gives rise to a lattice of PID atoms denoted by (B n , ⊑), which is isomorphic to the original PID lattice of antichains [[30]](#b30).

Let us now introduce the notion of PID conjugation. For this, let us first note that, according to the ordertheoretic principle of duality [[28]](#b28), every lattice has a dual lattice in which all arrows are reverted. If we think of Boolean functions as bitstrings (with subsets ordered lexicographically), the dual of the PID lattice can be found by simply inverting the digits and reading the bitstring backwards, as shown by our next result. Proposition 4. The mapping f → f † , where f † is the Boolean function satisfying

$∀a ⊆ {1 . . . , n} : f † (a) = 1 ⇔ f (a C ) = 0,(B5)$is an order-reversing involution on (B n , ⊑).

Proof.

$Involution: f † † (a) = 1 if and only if f ((a C ) C ) = f (a) = 1. Hence, f † † = f . Order-reversing: Suppose first that f ⊑ g, meaning that g(a) = 1 → f (a) = 1.$Then, if f † (a) = 1 we must have f (a c ) = 0 and so g(a c ) = 0 and hence g † (a) = 1. Therefore we have g

$† ⊑ f † . Conversely, if g † ⊑ f † , then f † (a) = 1 → g † (a) = 1.$Hence, if g(a) = 1 it follows that g † (a c ) = 0 and thus f † (a c ) = 0 and thus f (a) = 1. Therefore, f ⊑ g.

The effect of † on the PID lattice can be understood as follows. Following Ref. [[31]](#b31), each atom can be expressed as concatenations of meet and join operations corresponding to redundancies and synergies. For example, the atom α = {{1, 2}, {1, 3}} can be constructed as (1 ∨ 2) ∧ (2 ∨ 3), where the join operation (∨) can be thought of as denoting the union between sources (i.e., synergy), and the meet operation (∧) as the intersection between them (i.e., redundancy). Then, the involution introduced in Proposition 4 can be understood as switching meets for joins and vice-versa. For example

${{1, 2}, {1, 3}} † = ((1 ∨ 2) ∧ (2 ∨ 3)) † = (1 ∧ 2) ∨ (1 ∧ 3) (a) = 1 ∨ (2 ∧ 3) = {{1}, {2, 3}},$where equality (a) uses the distributivity between meets and joins [[31]](#b31). This shows that the natural PID involution switches redundancy (i.e. easily accessible information, since it is contained in multiple sources) and synergy (i.e. difficult to access information, since one needs to observe multiple sources). Thus, the more easily an atom can be accessed, the more difficult it is to access its conjugate.

This PID involution leads to a natural conjugation between PID atoms, which we define next. Definition 4. The conjugate of a PID atom is given by

$I f ∂ (X; Y ) † := I f † ∂ (X; Y ),(B6)$where f † is as defined in Prop. 4. Additionally, the conjugate of a linear combination of PID atoms ψ(X; Y ) =

$f ∈Bn c f I f ∂ (X; Y ) is defined as ψ(X; Y ) † := f ∈Bn c f I f † ∂ (X; Y ).(B7)$We now prove Proposition 1, which states that applying PID conjugation on a conditional mutual information I(X a ; Y |X b ) leads to the same outcome as entropic conjugation does -namely, I(X a ; Y |X (a∪b) C ), where the complement is taken within the set of source variables. Please note that the proof uses no properties specific to particular instantiations of synergy or redundancy, and hence the result holds for any operationalisation of these quantities that are consistent with the PID framework.

## Proof of

## Appendix C: Analysis of spin systems

Our experiments considered systems of n spins X = (X 1 , . . . , X n ) ∈ {-1, 1} n following a Boltzmann distribution p X (x) = e -βH(x) /Z with a Hamiltonean of the form

$H(x) = 2 n(n -1) n i=1 n j=i+1 x i x k J i,k ,(C1)$where the coupling coefficients J i,k are i.i.d. sampled from a Gaussian distribution with mean µ and variance σ 2 . The results reported in Fig. [1](#fig_0) corresponds to systems of n = 8 spins with β = 1, σ 2 = 2, and either µ = 5 (ferromagnetic), µ = 0 (weak), or µ = -5 (frustrated). Our analysis pipeline was structured as follows. We first computed the joint distribution of 10 systems of each type, and calculated the value of u k for each of them. The resulting values were then used to perform a principal component analysis. From this, we obtained the loadings of the two first principal components, denoted as ξ k and ν j , respectively. These loadings were then used to construct two high-order metrics, ϕ PC1 (X) := n-1 k=1 ξ k u k (X) and ϕ PC2 (X) := n-1 j=1 ν j u j (X), which corresponds to projecting the value of the u k 's onto the directions given by the principal components. Finally, these two resulting metrics were used to characterise the systems of interest.

![FIG.1. Information-theoretic analysis of the interdependencies observed in systems of n = 8 spins subject to positive (ferromagnetic), negative (frustrated), and weak interactions between them. a) When calculating u k , each type of interactions exhibit distinct profiles of interdependence. b) The variability among u k is captured by two principal components: one of symmetric character which accounts for the overall strength of the interdependence (PC1), and one of skew-symmetric character that accounts for the balance between high-and low-order interdependence (PC2). c) The values of u k projected onto these PCs provide a simple characterisation of these three types of systems in terms of their overall interdependence strength (PC1) and the balance between high-and low-order effects (PC2).]()

![For a, b disjoint subsets of I n , thenI(X a ; Y |X b ) † lemma 2 = (a∪b)=1 & f (b)=0 I f † ∂ (X; Y ) def. f † = f ((a∪b) C )=0 & f (b C )=1 I f ∂ (X; Y ) = f (a∪(a∪b) C )=1 & f ((a∪b) C )=0 I f ∂ (X; Y ) lemma I(X a ; Y |X (a∪b) C ),where the second to last equality follows because a ∪ (a ∪ b) C = b C if a and b are disjoint.]()

