{
  "arxivId": "2407.05872",
  "title": "Scaling Exponents Across Parameterizations and Optimizers",
  "authors": "Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, Jeffrey Pennington",
  "abstract": "Robust and effective scaling of models from small to large width typically\nrequires the precise adjustment of many algorithmic and architectural details,\nsuch as parameterization and optimizer choices. In this work, we propose a new\nperspective on parameterization by investigating a key assumption in prior work\nabout the alignment between parameters and data and derive new theoretical\nresults under weaker assumptions and a broader set of optimizers. Our extensive\nempirical investigation includes tens of thousands of models trained with all\ncombinations of three optimizers, four parameterizations, several alignment\nassumptions, more than a dozen learning rates, and fourteen model sizes up to\n26.8B parameters. We find that the best learning rate scaling prescription\nwould often have been excluded by the assumptions in prior work. Our results\nshow that all parameterizations, not just maximal update parameterization\n(muP), can achieve hyperparameter transfer; moreover, our novel per-layer\nlearning rate prescription for standard parameterization outperforms muP.\nFinally, we demonstrate that an overlooked aspect of parameterization, the\nepsilon parameter in Adam, must be scaled correctly to avoid gradient underflow\nand propose Adam-atan2, a new numerically stable, scale-invariant version of\nAdam that eliminates the epsilon hyperparameter entirely.",
  "url": "https://arxiv.org/abs/2407.05872",
  "issue_number": 0,
  "issue_url": "",
  "created_at": "2025-01-04T15:03:18.848085",
  "state": "open",
  "labels": [
    "paper"
  ],
  "total_reading_time_seconds": 4,
  "last_read": "2025-01-04T15:03:18.849005",
  "last_visited": "2024-12-25T10:23:47.480000+00:00",
  "main_tex_file": null,
  "published_date": "2024-07-08T12:32:51Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}