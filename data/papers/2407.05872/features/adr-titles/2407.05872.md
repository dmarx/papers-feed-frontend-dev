- Decision on the choice of parameterizations for scaling
- Decision on the selection of optimizers for experiments
- Decision to investigate alignment assumptions in prior work
- Decision to derive new theoretical results under weaker assumptions
- Decision to implement per-layer learning rate prescriptions
- Decision to propose Adam-atan2 for epsilon scaling
- Decision to conduct extensive empirical investigations across model sizes
- Decision to utilize hyperparameter transfer across scales
- Decision to define metrics for measuring alignment during training
- Decision to explore the impact of epsilon underflow on performance
- Decision to compare performance across different parameterizations and optimizers
- Decision to focus on stability and non-triviality in parameterization theory
- Decision to analyze the relationship between parameter scaling and update scales
- Decision to document findings on scaling mismatches in learning rates
- Decision to validate theoretical predictions through empirical results
- Decision to emphasize the importance of constant multiplicative factors in learning rates