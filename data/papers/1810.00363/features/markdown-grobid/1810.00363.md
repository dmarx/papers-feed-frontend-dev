# A Kernel Perspective for Regularizing Deep Neural Networks

## Abstract

## 

We propose a new point of view for regularizing deep neural networks by using the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various practical strategies. Specifically, this perspective (i) provides a common umbrella for many existing regularization principles, including spectral norm and gradient penalties, or adversarial training, (ii) leads to new effective regularization penalties, and (iii) suggests hybrid strategies combining lower and upper bounds to get better approximations of the RKHS norm. We experimentally show this approach to be effective when learning on small datasets, or to obtain adversarially robust models.

## Introduction

Learning predictive models for complex tasks often requires large amounts of annotated data. For instance, convolutional neural networks are huge-dimensional and typically involve more parameters than training samples, which raises several challenges: achieving good generalization with small datasets is indeed difficult, which limits the deployment of such deep models to many tasks where labeled data is scarce, e.g., in biology [(Ching et al., 2018)](#b11). Besides, imperceptible adversarial perturbations can significantly degrade the prediction quality [(Szegedy et al., 2013;](#b43)[Biggio & Roli, 2018)](#b8). These issues raise the question of regularization as an essential tool to control the complexity of deep models, as well as their stability to small variations of their inputs.

In this paper, we present a new perspective on regularization of deep networks, by viewing convolutional neural networks (CNNs) as elements of a RKHS following the work of [Bietti & Mairal (2019)](#b7) on deep convolutional kernels. For such kernels, the RKHS contains indeed deep convolutional networks similar to generic ones-up to smooth approximations of rectified linear units. Such a point of view provides a natural regularization function, the RKHS norm, which allows us to control the variations of the predictive model and to limit its complexity for better generalization. Besides, the norm also acts as a Lipschitz constant, which provides a direct control on the stability to adversarial perturbations.

In contrast to traditional kernel methods, the RKHS norm cannot be explicitly computed in our setup. Yet, this norm admits numerous approximations-lower bounds and upper bounds-which lead to many strategies for regularization based on penalties, constraints, or combinations thereof. Depending on the chosen approximation, we recover then many existing principles such as spectral norm regularization [(Cisse et al., 2017;](#b12)[Yoshida & Miyato, 2017;](#)[Miyato et al., 2018a;](#)[Sedghi et al., 2019)](#b37), gradient penalties and double backpropagation [(Drucker & Le Cun, 1991;](#b13)[Simon-Gabriel et al., 2019;](#b39)[Gulrajani et al., 2017;](#b17)[Roth et al., 2017;](#b33)[2018;](#b11)[Arbel et al., 2018)](#b2), adversarial training [(Madry et al., 2018)](#b26), and we also draw links with tangent propagation [(Simard et al., 1998)](#b38). For all these principles, we provide a unified viewpoint and theoretical insights, and we also introduce new variants, which we show are effective in practice when learning with few labeled data, or in the presence of adversarial perturbations.

Moreover, regularization and robustness are tightly linked in our kernel framework. Specifically, some lower bounds on the RKHS norm lead to robust optimization objectives with worst-case 2 perturbations; further, we can extend marginbased generalization bounds in the spirit of [Bartlett et al. (2017)](#b4); [Boucheron et al. (2005)](#b10) to the setting of adversarially robust generalization (see [Schmidt et al., 2018)](#b35), where an adversary can perturb test data. We also discuss connections between recent regularization strategies for training generative adversarial networks and approaches to generative modeling based on kernel two-sample tests (MMD) [(Dziugaite et al., 2015;](#b14)[Li et al., 2017;](#b23)[Bińkowski et al., 2018)](#b9).

## Summary of the contributions.

• We introduce an RKHS perspective for regularizing deep neural networks models which provides a unified view on various practical regularization principles, together with theoretical insight and guarantees; arXiv:1810.00363v4 [stat.ML] 13 May 2019

• By considering lower bounds to the RKHS norm, we obtain new penalties based on adversarial perturbations, adversarial deformations, or gradient norms of prediction functions, which we show to be effective in practice;

• Our RKHS point of view suggests combined strategies based on both upper and lower bounds, which we show often perform empirically best in the context of generalization from small image and biological datasets, by providing a tighter control of the RKHS norm.

Related work. The construction of hierarchical kernels and the study of neural networks in the corresponding RKHS was studied by [Mairal (2016)](#b27); [Zhang et al. (2016;](#b19)[2017)](#); [Bietti & Mairal (2019)](#b7). Some of the regularization strategies we obtain from our kernel perspective are variants of previous approaches to adversarial robustness [(Cisse et al., 2017;](#b12)[Madry et al., 2018;](#b26)[Simon-Gabriel et al., 2019;](#b39)[Roth et al., 2018)](#b34), to improving generalization [(Drucker & Le Cun, 1991;](#b13)[Miyato et al., 2018b;](#)[Sedghi et al., 2019;](#b37)[Simard et al., 1998;](#b38)[Yoshida & Miyato, 2017)](#), and stable training of generative adversarial networks [(Roth et al., 2017;](#b33)[Gulrajani et al., 2017;](#b17)[Arbel et al., 2018;](#b2)[Miyato et al., 2018a)](#). The link between robust optimization and regularization was studied by [Xu et al. (2009a;](#)[b)](#), focusing mainly on linear models with quadratic or hinge losses. The notion of adversarial generalization was considered by [Schmidt et al. (2018)](#b35), who provide lower bounds on a particular data distribution. [Sinha et al. (2018)](#b41) provide generalization guarantees in the different setting of distributional robustness; compared to our bound, they consider expected loss instead of classification error, and their bounds do not highlight the dependence on the model complexity.

## Regularization of Deep Neural Networks

In this section, we recall the kernel perspective on deep networks introduced by [Bietti & Mairal (2019)](#b7), and present upper and lower bounds on the RKHS norm of a given model, leading to various regularization strategies. For simplicity, we first consider real-valued networks and binary classification, before discussing multi-class extensions.

## Relation between deep networks and RKHSs

Kernel methods consist of mapping data living in a set X to a RKHS H associated to a positive definite kernel K through a mapping function Φ : X → H, and then learning simple machine learning models in H. Specifically, when considering a real-valued regression or binary classification problem, classical kernel methods find a prediction function f : X → R living in the RKHS which can be written in linear form, i.e., such that f (x) = f, Φ(x) H for all x in X . While explicit mapping to a possibly infinitedimensional space is of course only an abstract mathematical operation, learning f can be done implicitly by com-puting kernel evaluations and typically by using convex programming [(Schölkopf & Smola, 2001)](#b36).

Moreover, the RKHS norm f H acts as a natural regularization function, which controls the variations of model predictions according to the geometry induced by Φ:

$|f (x) -f (x )| ≤ f H • Φ(x) -Φ(x ) H .$(1)

Unfortunately, our setup does not allow us to use the RKHS norm in a traditional way since evaluating the kernel is intractable. Instead, we propose a different approach that considers explicit parameterized representations of functions contained in the RKHS, given by generic CNNs, and leverage properties of the RKHS and the kernel mapping in order to regularize when learning the network parameters.

Consider indeed a real-valued deep convolutional network f : X → R, where X is simply R d , with rectified linear unit (ReLU) activations and no bias units. By constructing an appropriate multi-layer hierarchical kernel, [Bietti & Mairal (2019)](#b7) show that the corresponding RKHS H contains a CNN with the same architecture and parameters as f , but with activations that are smooth approximations of ReLU.

Although the model predictions might not be strictly equal, we will abuse notation and denote this approximation with smooth ReLU by f as well, with the hope that the regularization procedures derived from the RKHS model will be effective in practice on the original CNN f .

Besides, the mapping Φ(•) is shown to be non-expansive:

$Φ(x) -Φ(x ) H ≤ x -x 2 ,(2)$so that controlling f H provides some robustness to additive 2 -perturbations, by (1). Additionally, with appropriate pooling operations, [Bietti & Mairal (2019)](#b7) show that the kernel mapping is also stable to deformations, meaning that the RKHS norm also controls robustness to translations and other transformations including scaling and rotations, which can be seen as deformations when they are small.

In contrast to standard kernel methods, where the RKHS norm is typically available in closed form, this norm is difficult to compute in our setup, and requires approximations. The following sections present upper and lower bounds on f H , with linear convolutional operations denoted by W k for k = 1, . . . , L, where L is the number of layers. Defining θ := {W k : k = 1, . . . , L}, we then leverage these bounds to approximately solve the following penalized or constrained optimization problems on a training set (x i , y i ), i = 1, . . . , n:

$min θ 1 n n i=1 (y i , f θ (x i )) + λ f θ 2 H or (3) min θ: f θ H ≤C 1 n n i=1 (y i , f θ (x i )).(4)$We also note that while the construction of [Bietti & Mairal (2019)](#b7) considers VGG-like networks [(Simonyan & Zisserman, 2014)](#b40), the regularization algorithms we obtain in practice can be easily adapted to different architectures such as residual networks [(He et al., 2016)](#b19).

## Exploiting lower bounds of the RKHS norm

In this section, we devise regularization algorithms by leveraging lower bounds on f H , obtained by relying on the following variational characterization of Hilbert norms:

$f H = sup u H ≤1 f, u H .$At first sight, this definition is not useful since the set U = {u ∈ H : u H ≤ 1} may be infinite-dimensional and the inner products f, u H cannot be computed in general. Thus, we devise tractable lower bound approximations by considering smaller sets Ū ⊂ U .

Adversarial perturbation penalty. Thanks to the nonexpansiveness of Φ, we can consider the subset Ū ⊂ U defined as

$Ū = {Φ(x + δ) -Φ(x) : x ∈ X , δ 2 ≤ 1}, leading to the bound f H ≥ f 2 δ := sup x∈X , δ 2≤1 f (x + δ) -f (x),(5)$which is reminiscent of adversarial perturbations. Adding a regularization parameter > 0 in front of the norm then corresponds to different sizes of perturbations:

$f H = sup u H ≤ f, u H ≥ sup x∈X , δ 2≤ f (x + δ) -f (x).$(6) Using this lower bound or its square as a penalty in the objective (3) when training a CNN provides a way to regularize. Optimizing over adversarial perturbations has been useful to obtain robust models (e.g., the PGD method of [Madry et al., 2018](#b26)); yet our approach differs in two important ways:

(i) it involves a penalty that is decoupled from the loss term such that in principle, our penalty could be used beyond the supervised empirical risk paradigm. In contrast, PGD optimizes the robust formulation (7) below, which fits training data while considering perturbations on the loss.

(ii) our penalty involves a global maximization problem on the input space X , as opposed to only maximizing on perturbations near training data. In practice, optimizing over X is however difficult and instead, we replace X by random mini-batches of examples, yielding further lower bounds on the RKHS norm. These examples may be labeled or not, in contrast to PGD that perturb labeled examples only. When using such a mini-batch, a gradient of the penalty can be obtained by first finding maximizers x, δ (where x is an element of the mini-batch and δ is a perturbation), and then computing gradients of f θ (x + δ) -f θ (x) with respect to θ by using back-propagation. In practice, we compute the perturbations δ for each example x by using a few steps of projected gradient ascent with constant step-lengths.

Robust optimization yields another lower bound. In some contexts, our penalized approach is related to solving the robust optimization problem

$min θ 1 n n i=1 sup δ 2≤ (y i , f θ (x i + δ)),(7)$which is commonly considered for training adversarially robust classifiers [(Wong & Kolter, 2018;](#b45)[Madry et al., 2018;](#b26)[Raghunathan et al., 2018)](#b32). In particular, [Xu et al. (2009b)](#) show that the penalized and robust objectives are equivalent in the case of the hinge loss with linear predictors, when the data is non-separable. They also show the equivalence for kernel methods when considering the (intractable) full perturbation set U around each point in the RKHS Φ(x i ), that is, predictions f, Φ(x i ) + u H with u in U . Intuitively, when a training example (x i , y i ) is misclassified, we are in the "linear" part of the hinge loss, such that

$sup u H ≤ (y i , f, Φ(x i ) + u H ) = (y i , f (x i )) + f H .$For other losses such as the logistic loss, a regularization effect is still present even for correctly classified examples, though it may be smaller since the loss has a reduced slope for such points. This leads to an adaptive regularization mechanism that may automatically reduce the amount of regularization when the data is easily separable. However, the robust optimization approach might only encourage local stability around training examples, while the global quantity f H may become large in order to better fit the data. We note that a perfect fit of the data with large complexity does not prevent generalization (see, e.g., [Belkin et al., 2018a;](#)[b)](#); yet, such mechanisms are still poorly understood. Nevertheless, it is easy to show that the robust objective (7) lower bounds the penalized objective with penalty f H .

Gradient penalties. Taking Ū = { Φ(x)-Φ(y)

x-y 2 : x, y ∈ X }, which is a subset of U by Eq. (2)-it turns out that this is the same set as for adversarial perturbation penalties, since Φ is homogeneous [(Bietti & Mairal, 2019)](#b7) and X = R d -we obtain a lower bound based on the Lipschitz constant of f :

$f H ≥ sup x,y∈X f (x) -f (y) x -y 2 ≥ ∇f := sup x∈X ∇f (x) 2 ,$(8) where the second inequality becomes an equality when X is convex, and the supremum is taken over points where f is differentiable. Although we are unaware of previous work using this exact lower bound for a generic regularization penalty, we note that variants replacing the supremum over x by an expectation over data have been recently used to stabilize the training of generative adversarial networks [(Gulrajani et al., 2017;](#b17)[Roth et al., 2017)](#b33), and we provide insights in Section 3.2 on the benefits of RKHS regularization in such a setting. Related penalties have been considered in the context of robust optimization, for regularization or robustness, noting that a penalty based on the gradient of the loss function x → (y, f (x)) can give a good approximation of (7) when is small [(Drucker & Le Cun, 1991;](#b13)[Lyu et al., 2015;](#b25)[Roth et al., 2018;](#b34)[Simon-Gabriel et al., 2019)](#b39).

Penalties based on deformation stability. We may also obtain new penalties by considering more exotic sets Ū = {Φ(x) -Φ(x) : x ∈ X , x is a small deformation of x}, where the amount of deformation is dictated by the stability bounds of [Bietti & Mairal (2019)](#b7) in order to ensure that Ū ⊂ U . More precisely, such bounds depend on the maximum displacement and Jacobian norm of the diffeomorphisms considered. These can be easily computed for various parameterized families of transformations, such as translations, scaling or rotations, leading to simple ways to control the regularization strength through the parameters of these transformations. One can also consider infinitesimal deformations from such parameterized transformations, which approximately yields the tangent propagation regularization strategy of [Simard et al. (1998)](#b38). These approaches are detailed in Appendix B. If instead we consider the robust optimization formulation (7), we obtain a form of data augmentation where transformations are optimized instead of sampled, as done by [(Engstrom et al., 2017)](#b15).

## Extensions to multiple classes and beyond

We now extend the regularization strategies based on lower bounds to multi-valued networks, in order to deal with multiple classes. For that purpose, we consider a multi-class penalty f 1 2

$H + . . . + f K 2 H for an R K -valued function f = (f 1 , f 2 , . . . , f K ), and we define f 2 δ := K k=1 f k 2 δ and ∇f 2 := K k=1 ∇f k 2 ,$where f k δ is the adversarial penalty (5), and ∇f k is defined in (8). For deformation stability penalties, we proceed in a similar manner, and for robust optimization formulations ( [7](#formula_6)), the extension is straightforward, given that multiclass losses such as cross-entropy can be directly optimized in an adversarial training or gradient penalty setup.

Finally, we note that while the kernel approach we introduce considers the Euclidian geometry in the input space, it is possible to consider heuristic alternatives for other geometries, such as ∞ perturbations, as discussed in Appendix D.

## Exploiting upper bounds with spectral norms

Instead of lower bounds, one may use instead the following upper bound from [Bietti & Mairal (2019, Proposition 14)](#):

$f H ≤ ω( W 1 , . . . , W L ),(9)$where ω is increasing in all of its arguments, and W k is the spectral norm of the linear operator W k . Here, we simply consider the spectral norm on the filters, given by W := sup x 2≤1 W x 2 . Other generalization bounds relying on similar quantities have been proposed for controlling complexity [(Bartlett et al., 2017;](#b4)[Neyshabur et al., 2018)](#b31), suggesting that using them for regularization is relevant even beyond our kernel perspective, as observed by [Cisse et al. (2017)](#b12); [Sedghi et al. (2019);](#b37)[Yoshida & Miyato (2017)](#).

Extensions to multiple classes are simple to obtain by simply considering spectral norms up to the last layer.

Penalizing the spectral norms. One way to control the upper bound ( [9](#formula_10)) when learning a neural network f θ is to consider a regularization penalty based on spectral norms

$min θ 1 n n i=1 (y i , f θ (x i )) + λ L l=1 W l 2 , (10$$)$where λ is a regularization parameter. To optimize this cost, one can obtain (sub)gradients of the penalty by computing singular vectors associated to the largest singular value of each W l . We consider the method of Yoshida & Miyato (2017), which computes such singular vectors approximately using one or two iterations of the power method, as well as a more costly approach using the full SVD.

Constraining the spectral norms with a continuation approach. In the constrained setting, we want to optimize:

$min θ 1 n n i=1 (y i , f θ (x i )) s.t. W l ≤ τ ; l ∈ 1, . . . , L ,$where τ is a user-defined constraint. This objective may be optimized by projecting each W l in the spectral norm ball of radius τ after each gradient step. Such a projection is achieved by truncating the singular values to be smaller than τ (see Appendix C). We found that the loss was hardly optimized with this approach, and therefore introduce a continuation approach with an exponentially decaying schedule for τ reaching a constant τ 0 after a few epochs, which we found to be important for good empirical performance.

## Combining upper and lower bounds.

One advantage of lower bound penalties is that they are independent of the model parameterization, making them flexible enough to use with more complex architectures. In addition, the connection with robust optimization can provide a useful mechanism for adaptive regularization. However, they do not provide a guaranteed control on the RKHS norm, unlike the upper bound strategies. This is particularly true for robust optimization approaches, which may favor small training loss and local stability over global stability through f H . Nevertheless, we observed that our new approaches based on separate penalties sometimes do help in controlling upper bounds as well (see Section 4).

While these upper bound strategies are useful for limiting model complexity, we found them empirically less effective for robustness (see Section 4.2). However, we observed that combining with lower bound approaches can overcome this weakness, perhaps due to a better control of local stability.

In particular, such combined approaches often provide the best generalization performance in small data scenarios, as well as better guarantees on adversarially robust generalization thanks to a tighter control of the RKHS norm.

## Theoretical Guarantees and Insights

In this section, we study how the kernel perspective allows us to extend standard margin-based generalization bounds to an adversarial setting in order to provide theoretical guarantees on adversarially robust generalization. We then discuss how our kernel approach provides novel interpretations for training generative adversarial networks.

## Guarantees on adversarial generalization

While various methods have been introduced to empirically gain robustness to adversarial perturbations, the ability to generalize with such perturbations, also known as adversarial generalization [(Schmidt et al., 2018)](#b35), still lacks theoretical understanding. Margin-based bounds have been useful to explain the generalization behavior of learning algorithms that can fit the training data well, such as kernel methods, boosting and neural networks [(Koltchinskii & Panchenko, 2002;](#b22)[Boucheron et al., 2005;](#b10)[Bartlett et al., 2017)](#b4). Here, we show how such arguments can be adapted to obtain guarantees on adversarial generalization, i.e., on the expected classification error in the presence of an 2 -bounded adversary, based on the RKHS norm of a learned model. For a binary classification task with labels in Y = {-1, 1} and data distribution D, we would like to bound the expected adversarial error of a classifier f , given for some > 0 by err D (f, ) := P (x,y)∼D (∃ δ 2 ≤ : yf (x + δ) < 0).

(11) Leveraging the fact that f is f H -Lipschitz, we now show how to further bound this quantity using empirical margins, following the usual approach to obtaining margin bounds for kernel methods (e.g., [Boucheron et al., 2005)](#b10). Consider a training dataset (x 1 , y 1 ), . . . ,

$(x n , y n ) ∈ X × Y. Defining L γ n (f ) := 1 n n i=1 1{y i f (x i )$< γ}, we have the following bound, proved in Appendix E:

Proposition 1 (Adversarially robust margin bound). With probability 1 -δ over a dataset {(x i , y i )} i=1,...,n , we have, for all choices of γ > 0 and f ∈ H,

$err D (f, ) ≤ L γ+2 f H n (f ) + Õ f H B γ √ n ,(12)$where B =

$1 n n i=1 K(x i , x i )$and Õ hides a term depending logarithmically on f H , γ, and δ.

When = 0, we obtain the usual margin bound, while > 0 yields a bound on adversarial error err D (f, ), for some neural network f learned from data. Note that other complexity measures based on products of spectral norms may be used instead of f H , as well as multi-class extensions, following [Bartlett et al. (2017)](#b4); [Neyshabur et al. (2018)](#b31). In concurrent work, [Khim & Loh (2018)](#b21); Yin et al. ( [2019](#)) derive similar bounds in the context of fully-connected networks. In contrast to these works, which bound complexity of a modified function class, our bound uses the complexity of the original class and leverages smoothness properties of functions to derive the margin bound.

One can then study the effectiveness of a regularization algorithm by inspecting cumulative distribution (CDF) plots of the normalized margins γi = y i f (x i )/ f H , for different strengths of regularization (an example is given in Figure [2](#fig_0), Section 4.2). According to the bound (12), one can assess expected adversarial error with -bounded perturbations by looking at the part of the plot to the right of γ = 2 . In particular, the value of the CDF at such a value of γ is representative of the bound for large n (since the second term is negligible), while for smaller n, the best bound is obtained for a larger value of γ, which also suggests that the right side of the plots is indicative of performance on small datasets.

When the RKHS norm can be well approximated, our bound provides a certificate on test error in the presence of adversaries. While such an approximation is difficult to obtain in general, the guarantee is most useful when lower and upper bounds of the RKHS norm are controlled together.

## New insights on generative adversarial networks

Generative adversarial networks (GANs) attempt to learn a generator neural network G φ : Z → X , so that the distribution of G φ (z) with z ∼ D z a noise vector resembles a data distribution D x . In this section, we discuss connections between recent regularization techniques for training GANs, and approaches to learning generative models based on a MMD criterion [(Gretton et al., 2012)](#b16), in view of our RKHS framework. Our goal is to provide a new insight on these methods, but not necessarily to provide a new one.

Various recent approaches have relied on regularization strategies on a discriminator network in order to improve the stability of GAN training and the quality of the produced samples. Some of these resemble the approaches presented in Section 2 such as gradient penalties [(Gulrajani et al., 2017;](#b17)[Roth et al., 2017)](#b33) and spectral norm regularization [(Miyato et al., 2018a)](#). We provide an RKHS interpretation of these methods as optimizing an MMD distance with the convolutional kernel introduced in Section 2:

$min φ sup f H ≤1 E x∼Dx [f (x)] -E z∼Dz [f (G φ (z))]. (13)$When learning from an empirical distribution over n samples, the MMD criterion is known to have much better sample complexity than the Wasserstein-1 distance considered by [Arjovsky et al. (2017)](#b3) for high-dimensional data such as images [(Sriperumbudur et al., 2012)](#b42). While the MMD approach has been used for training generative models, it generally relies on a generic kernel function, such as a Gaussian kernel, that appears explicitly in the objective [(Dziugaite et al., 2015;](#b14)[Li et al., 2017;](#b23)[Bińkowski et al., 2018)](#b9). Although using a learned feature extractor can improve this, the Gaussian kernel might be a poor choice when dealing with natural signals such as images, while the hierarchical kernel we consider in our paper is better suited for this type of data, by providing useful invariance and stability properties. Leveraging the variational form of the MMD (13) with this kernel suggests for instance using convolutional networks as the discriminator f , with constraints on the spectral norms in order to ensure f H ≤ C for some C, as done by [Miyato et al. (2018a)](#) through normalization.

## Experiments

We tested the regularization strategies presented in Section 2 in the context of improving generalization on small datasets and training robust models. Our goal is to use common architectures used for large datasets and improve their performance in different settings through regularization. Our Pytorch implementation of the various strategies is available at [https://github.com/albietz/kernel_reg](https://github.com/albietz/kernel_reg).

For the adversarial training strategies, the inner maximization problems are solved using 5 steps of projected gradient ascent with constant step-lengths. In the case of the lower bound penalties f 2 δ and ∇f 2 , we also maximize over examples in the mini-batch, only considering the maximal element when computing gradients with respect to parameters. For the robust optimization problem (7), we use PGD with 2 perturbations, as well as the corresponding 2 (squared) gradient norm penalty on the loss. For the upper bound approaches with spectral norms (SNs), we consider the SN projection strategy with decaying τ , as well as the SN penalty (10), either using power iteration (PI) or a full SVD for computing gradients. We consider the datasets CIFAR10 and MNIST when using a small number of training examples, as well as 102 datasets of biological sequences that suffer from small sample size.

CIFAR10. In this setting, we use 1 000 and 5 000 examples of the CIFAR10 dataset, with or without data augmentation. We consider a VGG network [(Simonyan & Zisserman, 2014)](#b40) with 11 layers, as well as a residual network [(He et al., 2016)](#b19) with 18 layers, which achieve 91% and 93% test accuracy respectively when trained on the full training set with standard data augmentation (horizontal flips + random crops). We do not use batch normalization layers in order to prevent any interaction with spectral norms. Each strategy derived in Section 2 is trained for 500 epochs using SGD with momentum and batch size 128, halving the step-size every 40 epochs. In order to study the potential effectiveness of each method, we assume that a reasonably large validation set is available to select hyper-parameters; thus, we keep 10 000 annotated examples for this purpose. We also show results using a smaller validation set in Appendix A.1.

Table [1](#tab_0) shows the test accuracies on 1 000 examples for upper and lower bound approaches, as well as combined ones. We also include virtual adversarial training (VAT, [Miyato et al., 2018b)](#). We provide extended tables in Appendix A.1 with additional methods, other geometries, results for 5 000 examples, as well as hypothesis tests for comparing pairs of methods and assessing the significance of our findings. Overall, we find that the combined lower bound + SN constraints approaches often yield better results than either method separately. For lower bound approaches alone, we found our f 2 δ and ∇f 2 penalties to often work best, particularly without data augmentation, while robust optimization strategies can be preferable with data augmentation, perhaps thanks to the adaptive regularization effect discussed earlier, which may be helpful in this easier setting. Gradient penalties often outperform adversarial perturbation strategies, possibly because of the closed form gradients which may improve optimization. We also found that adversarial training strategies tend to poorly control SNs compared to gradient penalties, particularly PGD (see also Section 4.2). SN constraints alone can also work well in some cases, particularly for VGG architectures, and often outperform SN penalties. SN penalties can work well nevertheless and provide computational benefits when using the power iteration variant.

Infinite MNIST. In order to assess the effectiveness of lower bound penalties based on deformation stability, we consider the Infinite MNIST dataset [(Loosli et al., 2007)](#b24), which provides an "infinite" number of transformed generated examples for each of the 60 000 MNIST training digits. Here, we use a 5-layer VGG-like network with average pooling after each 3x3 convolution layer, in order to more closely match the architecture assumptions of [Bietti & Mairal (2019)](#b7) for deformation stability. We consider two lower bound penalties that leverage the digit transformations in Infinite MNIST: one based on "adversarial" deformations around each digit, denoted f 2 τ ; and a tangent propagation [(Simard et al., 1998)](#b38) variant, denoted D τ f 2 , which provides an approximation to f 2 τ for small deformations based on gradients along a few tangent vector directions given by deformations (see Appendix B for details). Table [2](#tab_1) shows the obtained test accuracy for subsets of MNIST of size 300 and 1 000. Overall, we find that combining both adversarial penalties f 2 τ and f 2 δ performs best, which suggests that it is helpful to obtain tighter lower approximations of the RKHS norm by considering perturbations of Table [3](#tab_5). Regularization on protein homology detection tasks, with or without data augmentation (DA). Fixed hyperparameters are selected using the first half of the datasets, and we report the average auROC50 score on the second half. See Section A.3 in the appendix for more details and statistical testing.

## Method

No DA DA No weight decay 0.421 0.541 Weight decay 0.432 0.544 SN proj 0.583 0.615 PGD-2 0.488 0.554 grad-2 0.551 0.570 f 2 δ 0.577 0.611 ∇f 2 0.566 0.598 PGD-2 + SN proj 0.615 0.622 grad-2 + SN proj 0.581 0.634 f 2 δ + SN proj 0.631 0.639 ∇f 2 + SN proj 0.576 0.617

different kinds. Explicitly controlling the spectral norms can further improve performance, as does training on deformed digits, which may yield better margins by exploiting the additional knowledge that small deformations preserve labels. Note that data augmentation alone (with some weight decay) does quite poorly in this case, even compared to our lower bound penalties which do not use deformations.

Protein homology detection. Remote homology detection between protein sequences is an important problem to understand protein structure. Given a protein sequence, the goal is to predict whether it belongs to a superfamily of interest. We consider the Structural Classification Of Proteins (SCOP) version 1.67 dataset [(Murzin et al., 1995)](#b30), which we process as described in Appendix A.3 in order to obtain 102 balanced binary classification tasks with 100 protein sequences each, thus resulting in a low-sample regime. Protein sequences were also cut to 400 amino acids.

Sequences are represented with a one-hot encoding strategy-that is, a sequence of length l is represented as a binary matrix in {0, 1} 20×l , where 20 is the number of different amino acids (alphabet size of the sequences). Such a structure can then be processed by convolutional neural networks [(Alipanahi et al., 2015)](#b0). In this paper, we do not try to optimize the structure of the network for the task, since our goal is only to evaluate the effect of regularization strategies. Therefore, we use a simple convolutional network with 3 convolutional layers followed by global max-pooling and a final fully-connected layer (we use filters of size 5, and a max-pooling layer after the second convolutional layer).

Training was done using Adam with a learning rate fixed to 0.01, and a weight decay parameter tuned for each method. Since hyper-parameter selection per dataset is difficult due to the low sample size, we use the same parameters across datasets. This allows us to use the first 51 datasets as a validation set for hyper-parameter tuning, and we report

0.800 0.825 0.850 0.875 0.900 0.925 standard accuracy 0.76 0.78 0.80 0.82 0.84 0.86 adversarial accuracy 2 , test = 0.1 PGD-2 grad-2 |f| 2 | f| 2 PGD-2+ SN proj SN proj SN pen (SVD) clean 0.5 0.6 0.7 0.8 0.9 standard accuracy 0.1 0.2 0.3 0.4 0.5 2 , test = 1.0 Figure 1. Robustness trade-off curves of different regularization methods for VGG11 on CIFAR10. Each plot shows test accuracy vs adversarial test accuracy for 2-bounded, 40-step PGD adversaries with a fixed test. Different points on a curve correspond to training with different regularization strengths. The regularization increases monotonically along a given curve, and the leftmost points correspond to the strongest regularization. For PGD-2 + SN projection, we vary with a fixed τ = 0.8.

average performance with these fixed choices on the remaining 51 datasets. The standard performance measure for this task is the auROC50 score (area under the ROC curve up to 50% false positives). We note that the selection of hyperparameters has a transductive component, since some of the sequences in the test datasets may also appear in the datasets used for validation (possibly with a different label).

The results are shown in Table [3](#tab_5). The procedure used for data augmentation (right column) is described in Appendix A.3. We found that the most effective approach is the adversarial perturbation penalty, together with SN constraints. In particular, we found it to outperform the gradient penalty ∇f 2 , perhaps because in this case gradient penalties are only computed on a discrete set of possible points given by one-hot encodings, while adversarial perturbations may increase stability to wider regions, potentially covering different possible encoded sequences.

## Training adversarially robust models

We consider the same VGG architecture as in Section 4.1, trained on CIFAR10 with data augmentation, with different regularization strategies. Each method is trained for 300 epochs using SGD with momentum and batch size 128, dividing the step-size in half every 30 epochs. This strategy was successful in reaching convergence for all methods.

Figure [1](#) shows the test accuracy of the different methods in the presence of 2 -bounded adversaries, plotted against standard accuracy. We can see that the robust optimization approaches tend to work better in high-accuracy regimes, perhaps because the local stability that they encourage is sufficient on this dataset, while the f 2 δ penalty can be useful in large-perturbation regimes. We find that upper bound approaches alone do not provide robust models, but combining the SN constraint approach with a lower bound strategy (in this case PGD-2 ) helps improve robustness perhaps thanks to a more explicit control of stability. The plots also confirm that gradient penalties on the loss may be preferable for small regularization strengths (they achieve higher accuracy while improving robustness for small test ), while for stronger regularization, the gradient approximation no longer holds and the adversarial training approaches such as PGD (and its combination with SN constraints) are preferred. More experiments confirming these findings are available in Section A.4 of the appendix.

Norm comparison and adversarial generalization. Note that for PGD, in contrast to other methods, we can see that the product of spectral norms (representative of an upper bound on f H ) increases when the lower bound f δ decreases. This suggests that a network learned with PGD with large may have large RKHS norm, possibly because the approach tries to separate -balls around the training examples, which may require a more complex model than simply separating the training examples (see also [Madry et al., 2018)](#b26). This large discrepancy between upper and lower bounds highlights the fact that such models may only be stable locally near training data, though this happens to be enough for robustness on many test examples on CIFAR10.

In contrast, for other methods, and in particular the lower bound penalties f 2 δ and ∇f 2 , the upper and lower bounds appear more tightly controlled, suggesting a more appropriate control of the RKHS norm. This makes our guarantees on adversarial generalization more meaningful, and thus we may look at the empirical distributions of normalized margins γ obtained using f δ for normalization (as an approximation of f H ), shown in Figure [2](#fig_0) (right). The curves suggest that for small γ, and hence small test , smaller values of λ are preferred, while stronger regularization helps for larger γ, yielding lower test error guarantees in the presence of stronger adversaries according to our bounds in Section 3.1. This qualitative behavior is indeed observed in the results of Figure [1](#) on test data for the ∇f 2 penalty. Yin, D., Ramchandran, K., and Bartlett, P. Rademacher complexity for adversarially robust generalization. In Proceedings of the International Conference on Machine Learning (ICML), 2019. Yoshida, Y. and Miyato, T. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017. Zhang, Y., Lee, J. D., and Jordan, M. I. 1 -regularized neural networks are improperly learnable in polynomial time. In Proceedings of the International Conference on Machine Learning (ICML), 2016. Zhang, Y., Liang, P., and Wainwright, M. J. Convexified convolutional neural networks. In Proceedings of the International Conference on Machine Learning (ICML), 2017.

Section A of this supplementary presents extended results from our experiments, along with statistical tests for assessing the significance of our findings. Section B details our lower bound penalties based on deformations and their relationship to tangent propagation. Section C presents our continuation algorithm for optimization with spectral norm constraints. Section D describes heuristic extensions of our lower bound regularization strategies to non-Euclidian geometries. Finally, Section E provides our proof of the margin bound of Proposition 1 for adversarial generalization.

## A. Additional Experiment Results

A.1. CIFAR10

This section provides more extensive results for the experiments on CIFAR10 from Section 4.1. In particular, Table [4](#) shows additional experiments on larger subsets of size 5 000, as well as more methods, including different geometries (see Appendix D). The table also reports results obtained when using a smaller validation set of size 1 000. The full hyper-parameter grid is given in Table [6](#).

In order to assess the statistical significance of our results, we repeated the experiments on 10 new random choices of subsets, using the hyperparameters selected on the original subset from Table [4](#) (except for learning rate, which is selected according to a different validation set for each subset). We then compared pairs of methods using a paired t-test, with p-values shown in Table [5](#). In particular, the results strengthen some of our findings, for instance, that ∇f 2 should be preferred to the gradient penalty on the loss when there is no data augmentation, and that combined upper+lower bound approaches tend to outperform the individual upper or lower bound strategies.

## A.2. Infinite MNIST

We provide more extensive results for the Infinite MNIST dataset in Table [7](#tab_6), in particular showing more regularization strategies, as well as results with or without data augmentation, marked with ( * ). As in the case of CIFAR10, we use SGD with momentum (fixed to 0.9) for 500 epochs, with initial learning rates in [0.005; 0.05; 0.5], and divide the step-size by 2 every 40 epochs. The full hyper-parameter grid is given in Table [9](#tab_8).

As in the case of CIFAR10, we report statistical significance tests in Table [8](#tab_7) comparing pairs of methods based on 10 different random choices of subsets. In particular, the results confirm that weight decay with data augmentation alone tends to give weaker results than separate penalties, and that the combined penalty f 2 τ + f 2 δ , which combines adversarial perturbations of two different types, outperforms each penalty taken by itself on a single type of perturbation, which emphasizes the benefit of considering perturbations of different natures, perhaps thanks to a tighter lower bound approximation of the RKHS norm. We note that grad-2 ( * ) worked well on some subsets, but poorly on others due to training instabilities, possibly because of the selected hyperparameters which are quite large (and thus likely violate the approximation to the robust optimization objective).

## A.3. Protein homology detection

Dataset description. Our protein homology detection experiments consider the Structural Classification Of Proteins (SCOP) version 1.67 dataset [(Murzin et al., 1995)](#b30), filtered and split following the procedures of [(Håndstad et al., 2007)](#b18). Specifically, positive training samples are extracted from one superfamily from which one family is withheld to serve as positive test set, while negative sequences are chosen from outside of the target family's hold and are randomly split into training and test samples in the same ratio as positive samples. This yields 102 superfamily classification tasks, which are generally very class-imbalanced. For each task, we sample 100 class-balanced training samples to use as training set. The positive samples are extended to 50 with Uniref50 using PSI-BLAST [(Altschul et al., 1997)](#b1) if they are fewer.

Data augmentation procedure. We consider in our experiments a discrete way of perturbing training samples to perform data augmentation. Specifically, for a given sequence, a perturbed sequence can be obtained by randomly changing some of the characters. Each character in the sequence is switched to a different one, randomly chosen from the alphabet, with some probability p. We fixed this probability to 0.1 throughout the experiments.

Experimental details and significance tests. In our experiments, we use the Adam optimization algorithm with a learning rate fixed to 0.01 (and β fixed to defaults (0.9, 0.999)), with a batch size of 100 for 300 epochs. The full hyper-parameter grid is given in Table [11](#tab_10). In addition to the average auROC50 scores reported in  comparing pairs of methods in Table [10](#tab_9) in order to verify the significance of our findings. The results confirm that the adversarial perturbation penalty and its combination with spectral norm constraints tends to outperform the other approaches.

## A.4. Robustness

Figure [3](#fig_2) extends Figure [1](#) from Section 4.2 to show more methods, adversary strenghts, and different geometries. For combined (PGD-2 + SN projection) approaches, we can see that stronger constraints (i.e., smaller τ ) tend to reduce standard accuracy, likely because it prevents a good fit of the data, but can provide better robustness to strong adversaries ( test = 1). We can see that using the right metric in PGD indeed helps against an ∞ adversary, nevertheless controlling global stability through the RKHS norm as in the f 2 δ and ∇f 2 penalties can still provide some robustness against such adversaries, even with large test . For gradient penalties, we find that the different geometries behave quite similarly, which may suggest that more appropriate optimization algorithms than SGD could be needed to better accommodate the non-smooth case of  

$2 τ + f 2 δ + SN proj ( * ) grad-2 ( * ) - 1e-02 grad-2 ( * ) f 2 τ + f 2 δ + SN proj ( * ) - - f 2 τ + f 2 δ f 2 δ penalty 1e-07 6e-09 f 2 τ + f 2 δ f 2 τ penalty 2e-06 6e-07 f 2 τ + f 2 δ ( * ) f 2 τ + f 2 δ 2e-03 - f 2 τ + f 2 δ + SN proj ( * ) f 2 τ + f 2 δ 2e-03 2e-04$
## B. Details on Deformation Stability Penalties

This section provides more details on the deformation stability penalties mentioned in Section 2.2, and the practical versions we use in our experiments on the Infinite MNIST dataset [(Loosli et al., 2007)](#b24).

Stability to deformations. We begin by providing some background on deformation stability, recalling that these can provide new lower bound penalties as explained in Section 2.2. Viewing an element x ∈ X as a signal x(u), where u denotes the location (e.g. a two-dimensional vector for images), we denote by x τ a deformed version of x given by x τ (u) = x(u -τ (u)), where τ is a diffeomorphism. The deformation stability bounds of [Bietti & Mairal (2019)](#b7) take the form:

$Φ(x τ ) -Φ(x) H ≤ (C 1 τ ∞ + C 2 ∇τ ∞ ) x ,(14)$where ∇τ (u) is the Jacobian of τ at location u. Here, C 1 controls translation invariance and typically decreases with the total amount of pooling (i.e., translation invariance more or less corresponds to the resolution at the final layer), while C 2 controls stability to deformations (note that ∇τ = 0 for translations) and is typically smaller when using small patches. We note that the bounds assume linear pooling layers with a certain spatial decay, adapted to the resolution of the current layer; our experiments on Infinite MNIST with deformation stability penalties thus use average pooling layers on 2x2 neighborhoods.

Adversarial deformation penalty. We can obtain lower bound penalties by exploiting the above stability bounds in a similar manner to the adversarial perturbation penalty introduced in Section 2.2. In particular, assuming a scalar-valued convolutional network f :

$f 2 τ := sup x∈X ,τ ∈T (f (x τ ) -f (x)) 2 (15)$where T is a collection of diffeomorphisms. When the diffeomorphisms in T have bounded norm τ ∞ and Jacobian norm ∇τ ∞ , and assuming X (or, in practice, the training data) is bounded, the stability bound 14 ensures that the set U T = {Φ(x τ ) -Φ(x) : x ∈ X , τ ∈ T } is included in an RKHS ball with some radius r, so that f τ is a lower bound on r f H .

Tangent gradient penalty. We also consider the following gradient penalty along tangent vectors, which provides an approximation of the above adversarial penalty when considering small, parameterized deformations, and recovers the tangent propagation strategy of [Simard et al. (1998)](#b38):

$D τ f 2 := sup x∈X ∂ α f (x + i α i t x,i ) 2 , (16$$)$where {t x,i } i=1,...,q are tangent vectors at x obtained from a given set of deformations. To see the link with the adversarial deformation penalty 15, consider for simplicity a single deformation, T = {τ 0 }. For small α, we have

x ατ0 ≈ x + αt x , where t x (u) = τ 0 (u) • ∇x(u),

where t x denotes the tangent vector of the deformation manifold {ατ 0 : α} at α = 0 [(Simard et al., 1998)](#b38). Then,

$f (x ατ0 ) -f (x) ≈ α∂ α f (x + αt x ) = α ∇f (x), t x .$In this case, denoting αT = {ατ 0 }, we have

$sup x∈X ,τ ∈αT (f (x τ ) -f (x)) 2 ≈ α 2 sup x∈X |∂ α f (x + αt x )| 2 ,$so that when α is small, the adversarial penalty can be approximated by α D τ f (note that using αT instead of T in the adversarial penalty would also yield a scaling by α, since the stability bounds imply α times smaller perturbations in the RKHS).

Practical implementations on Infinite MNIST. In our experiments on Infinite MNIST, we compute f 2 τ by considering 32 random transformations of each digit in a mini-batch of training examples, and taking the maximum over both the example and the transformation. We do this separately for each class, as for the other lower bound penalties f 2 δ and ∇f 2 . For D τ f 2 , we take {t x,i } i=1,...,q with q = 30 to be tangent vectors given by random diffeomorphisms from Infinite MNIST around each example x.

![Figure 2. (left) Comparison of lower and upper bound quantities ( f δ vs the product of spectral norms). (right) CDF plot of normalized empirical margins for the ∇f 2 penalty with different regularization strengths, normalized by f δ . We consider 1000 fixed training examples when computing f δ .]()

![Figure 2 (left) compares lower and upper bound quantities for different regularization strengths.]()

![Figure 3. Robustness trade-off curves of different regularization methods for VGG11 on CIFAR10 (extended version of Figure 1). The plots show test accuracy vs adversarial test accuracy for 2-bounded (top/bottom) or ∞-bounded (middle), 40-step PGD adversaries with a fixed test. Different points on a curve correspond to training with different regularization strengths. The regularization increases monotonically along a given curve, and the leftmost points correspond to the strongest regularization. The bottom plots consider PGD-2 + SN projection, with different fixed values of the constraint radius τ , for varying in PGD.]()

![Regularization on CIFAR10 with 1 000 examples for VGG-11 and ResNet-18. Each entry shows the test accuracy with/without data augmentation when all hyper-parameters are optimized on a validation set. See also Section A.1 in the appendix for additional results and statistical testing.]()

![Regularization on 300 or 1 000 examples from MNIST, using deformations from Infinite MNIST. ( * ) indicates that random deformations were included as training examples, while f 2 τ and Dτ f 2 use them as part of the regularization penalty. See Section A.2 in the appendix for more results and statistical testing.]()

![we perform paired t-tests for]()

![Test accuracies on subsets of MNIST using deformations from Infinite MNIST. Extended version of Table2. ( * ) indicates that random deformations were included as training examples (i.e., data augmentation), while f 2 τ and Dτ f 2 use them as part of the regularization penalty. As in Table4, we show results obtained using a validation set of size 10 000 (a) and 1 000 (b).]()

![Paired t-tests comparing pairs of methods, on 10 different random choices of subsets of MNIST. Each cell shows the p-value of the corresponding test. We only show p-values smaller than 0.05. Hyperparameters are fixed to the ones obtained for the results in Table2(selected on a different choice of subset), except for the learning rate which is tuned on a separate validation set for each choice of subset.]()

![List of hyper-parameters used for each method on Infinite MNIST. For each method, we additionally consider a learning rate parameter in [0.005; 0.05; 0.5]. For combined penalties, the sets of hyperparameters are listed in the same order as in the first column (e.g., the choices of constraint radius are given last).]()

![Paired t-tests comparing pairs of methods on the 51 test datasets from the set of protein homology detection tasks. Each cell shows the p-value of the corresponding test. We only show p-values smaller than 0.05. We use the same hyperparameters as the ones obtained in the results of Table3.]()

![List of hyper-parameters used for each method on protein homology detection datasets. For combined penalties, the hyperparameters are the cross-products of each individual method.]()

/ ∞ , or perhaps that both algorithms are actually controlling the same notion of complexity on this dataset.

