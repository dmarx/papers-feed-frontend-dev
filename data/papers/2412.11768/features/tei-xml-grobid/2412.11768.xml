<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No More Adam: Learning Rate Scaling at Initialization is All You Need</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-17">17 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer- sity of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichuan</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer- sity of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Collov Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Cai</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Collov Labs</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
							<email>&lt;hongkai.wen@warwick.ac.uk&gt;.</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer- sity of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">No More Adam: Learning Rate Scaling at Initialization is All You Need</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-17">17 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">453264E2ED514961A04F1E4EFE177306</idno>
					<idno type="arXiv">arXiv:2412.11768v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we question the necessity of adaptive gradient methods for training deep neural networks. SGD-SaI is a simple yet effective enhancement to stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning rate Scaling at Initialization (SaI) to distinct parameter groups, guided by their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning rates without relying on adaptive second-order momentum, SGD-SaI helps prevent training imbalances from the very first iteration and cuts the optimizer's memory usage by half compared to AdamW. Despite its simplicity and efficiency, SGD-SaI consistently matches or outperforms AdamW in training a variety of Transformer-based tasks, effectively overcoming a long-standing challenge of using SGD for training Transformers. SGD-SaI excels in ImageNet-1K classification with Vision Transformers(ViT) and GPT-2 pretraining for large language models (LLMs, transformer decoder-only), demonstrating robustness to hyperparameter variations and practicality for diverse applications. We further tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion models, where it consistently outperforms state-of-the-art optimizers. From a memory efficiency perspective, SGD-SaI achieves substantial memory savings for optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters) and 25.15 GB for Llama2-7B compared to AdamW in full-precision training settings. 1   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Stochastic gradient-based optimization methods, such as Stochastic Gradient Descent (SGD), are fundamental to modern machine learning, enabling the successful training of models across a wide range of scientific and engineering applications. However, training objectives and data are often noisy in practice, and gradients may become sparse due to the inherent characteristics of regularization or specific architectural designs. Moreover, architectural differences can introduce imbalances in the learning dynamics across different parameters. To address these challenges, adaptive gradient methods <ref type="bibr" target="#b19">(Ghorbani et al., 2022)</ref> have been developed to handle better non-stationary objectives, noisy data, and sparse gradients. Among these methods, Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref> and AdamW <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2019</ref>) have become indispensable for training Transformer-based models, including Large Language Models (LLMs) <ref type="bibr" target="#b46">(Radford et al., 2019;</ref><ref type="bibr" target="#b51">Team et al., 2023)</ref> and Diffusion Models (DMs) <ref type="bibr" target="#b26">(Ho et al., 2020;</ref><ref type="bibr" target="#b47">Rombach et al., 2022)</ref>. Their popularity stems from their relative robustness and efficiency in optimizing high-dimensional parameter spaces. The core mechanism of Adam's adaptability lies in its second-order momentum term, v, which acts as a local gain <ref type="bibr" target="#b25">(Hinton et al., 2012)</ref>, dynamically adjusting the learning rate for each parameter. This mechanism enables Adam to perform effectively even in noisy or sparse gradients, addressing imbalances in the learning process across different parameters.</p><p>However, this adaptability comes with significant costs when the model size scales up. Specifically, Adam requires storing and updating each parameter's first-order (mean) and second-order (variance) momentum terms. This increases memory usage by at least 3x compared to the parameter size alone. For instance, training a 7-billion parameter model in FP32 using Adam requires approximately 50 GB of memory for the state tensors, a significant challenge even with high-end hardware like NVIDIA A100-80G GPUs. Compared to SGD, the memory demand of Adam can be at least double <ref type="bibr">(Zhang et al., 2024b)</ref>, posing a severe limitation on the scalability of deep learning research.</p><p>Numerous previous works have sought to reduce memory usage by simplifying optimizer states while preserving the adaptive gradient term to address the memory bottleneck while maintaining the effectiveness of adaptive methods. Approaches such as 8-bit Adam <ref type="bibr" target="#b9">(Dettmers et al., 2021)</ref>, Adafactor <ref type="bibr" target="#b48">(Shazeer &amp; Stern, 2018)</ref>, and sign-based methods <ref type="bibr" target="#b2">(Bernstein et al., 2018;</ref><ref type="bibr" target="#b32">Kunstner et al., 2023)</ref> focus on quantizing or sparsifying the optimizer states. Meanwhile, Adam-mini <ref type="bibr">(Zhang et al., 2024b)</ref> introduces parameter block grouping to share adaptive learning rates, leveraging Hessian structure insights <ref type="bibr">(Zhang et al., 2024a)</ref> to reduce memory usage. However, these methods often come with trade-offs. Many risk a performance downgrade compared to AdamW. From an efficiency standpoint, these approaches also introduce additional update complexity. Simplified state tensors still require computations based on full gradients for each parameter at each time step, increasing the overall computational burden. Adam-mini, in particular, necessitates fine-grained parameter partitioning <ref type="bibr">(Zhang et al., 2024b)</ref>, further complicating its implementation. As a result, these limitations lead to longer optimizer step times, ultimately slowing down the training process.</p><p>In this work, we challenge the necessity of adaptive gradient methods for model training and propose a memory-and computation-efficient alternative. We begin by revisiting the foundational motivation behind Adam's use of secondorder momentum. Inspired by the concept of the gradient Signal-to-Noise Ratio (g-SNR) <ref type="bibr" target="#b55">(Xiang et al., 2023)</ref>, which quantifies the relationship between a gradient's norm and variance, we leverage this metric to analyze and measure gradient distribution differences across parameters. Through empirical analysis, we investigate the temporal consistency for g-SNR during training and explain why this value can be determined at first training iterations. Furthermore, we analysed the g-SNR distribution across different ViT parameters and explored the g-SNR value correlated with varying parameters of type and its architecture characteristics.</p><p>Building on this, we argue that g-SNR can be leveraged to adjust learning rate scales, balancing the learning progress based on the distribution of gradients. Incorporating a preconditioned learning rate scale computed during the first training iteration, called Scaled at Initialization(SaI), facilitates stable training progress without incurring the memory and computational overhead associated with adaptive gradient terms. We call our method SGD-SaI, a novel optimization approach that eliminates the need for adaptive gradient methods, treating them as simple yet effective updates compared to SGD. In summary, our contributions are as follows:</p><p>• We challenge the necessity of adaptive gradient methods, specifically identified the existing challenges on Adam-like methods and proposed to use constant g-SNR value to replace the second-order momentum to reduce both the memory and computation cost, called Scaled at Initialization(SaI).</p><p>• We empirically analysed the statistics of g-SNR on parameters during training and identified its characteristics over time and distribution over parameters.</p><p>• We formula our insight into proposed methods, SGD-SaI, solved the long-stand challenge that SGD can not successfully train tasks with transformer architectures and observed outstanding performance in ViT and decoder-only transformer (LLMs).</p><p>• We extend our empirical analysis to other popular and practical task training, such as LoRA training on LLMs and Diffusion Models(DMs) and traditional CNN tasks. We observed consistent improvement compared to existing SOTA optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adaptive Gradient Methods:Stochastic gradient descent (SGD) is an efficient optimization method commonly used in deep learning, but it struggles with tasks that have nonstationary objectives or involve very noisy and/or sparse gradients <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref>, often requiring extensive hyperparameter tuning. To improve upon these limitations, adaptive gradient methods were developed to continuously and dynamically adjust learning rates for individual parameters throughout the training process <ref type="bibr" target="#b15">(Duchi et al., 2011;</ref><ref type="bibr" target="#b21">Graves, 2014;</ref><ref type="bibr" target="#b57">Zeiler, 2012)</ref>, with the Adam optimizer becoming particularly popular. Adam combines features from AdaGrad <ref type="bibr" target="#b54">(Ward et al., 2020)</ref>, which effectively manages sparse gradients, and RMSProp <ref type="bibr" target="#b25">(Hinton et al., 2012)</ref>, which is suitable for online and non-stationary tasks, allowing it to outperform SGD in many cases with less tuning effort. However, Adam has its own challenges, leading to the creation of enhancements such as AdamW <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2019)</ref>, which introduces decoupled weight decay for better generalization, and adaptations <ref type="bibr" target="#b14">(Dozat, 2016)</ref> that incorporate Nesterov momentum for faster convergence. To address early training noise, warm-up phases and Rectified Adam <ref type="bibr" target="#b40">(Liu et al., 2021)</ref> have been proposed. Additionally, Adaptive Weight Decay <ref type="bibr" target="#b18">(Ghiasi et al., 2023)</ref> further improves convergence, while <ref type="bibr" target="#b42">(Mishchenko &amp; Defazio, 2023)</ref> introduced a dynamic component for automatic learning rate adjustments within the Adam framework.</p><p>Adam in Transformer Realm: Transformers <ref type="bibr" target="#b52">(Vaswani, 2017)</ref> have become essential in modern deep learning, particularly in natural language processing. While the Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref> optimizer generally outperforms Stochastic Gradient Descent (SGD) in training Transformer architectures <ref type="bibr" target="#b56">(Xiao et al., 2021)</ref>, it has a significant downside: as model sizes grow, Adam's memory requirements, which are twice that of SGD due to first and second-order momentum storage <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref>, become a concern. To mitigate this overhead, researchers have explored methods like sign-based optimization <ref type="bibr" target="#b2">(Bernstein et al., 2018;</ref><ref type="bibr" target="#b32">Kunstner et al., 2023)</ref> and low-precision quantization <ref type="bibr">(Li et al., 2023a;</ref><ref type="bibr" target="#b9">Dettmers et al., 2021;</ref><ref type="bibr">2022;</ref><ref type="bibr" target="#b8">Dettmers &amp; Zettlemoyer, 2023)</ref>, although these can compromise performance. Studies have shown that Adam's adaptive learning rates based on gradient norm history contribute to its performance advantage <ref type="bibr">(Zhang et al., 2024a)</ref>, whereas SGD lacks this capability. However, finding the right learning rate scale for SGD to match Adam's performance remains unresolved. Adam's insights, rooted in RMSprop <ref type="bibr" target="#b25">(Hinton et al., 2012)</ref>, suggest that a global learning rate should be adjusted according to local gains.</p><p>Researchers have developed block-wise dynamic learning rates that perform comparably to Adam with reduced memory use <ref type="bibr">(Zhang et al., 2024b)</ref>. Similar trends are seen in parameter-efficient fine-tuning, emphasizing the importance of local gains for learning rate adjustments <ref type="bibr" target="#b58">(Zhang &amp; Pilanci, 2024)</ref>. Furthermore, theoretical analyses have raised doubts about the necessity of adaptive gradient methods. While Adam offers practical benefits, research <ref type="bibr" target="#b37">(Li et al., 2024)</ref> indicates that the convergence rates of Adam and SGD are not significantly different.</p><p>Gradient at Initialization: Recent research has highlighted the importance of gradient patterns at initialization, demonstrating a strong correlation between these early signals and a model's eventual performance. Pruning at Initialization (PaI) methods, inspired by the lottery ticket hypothesis <ref type="bibr" target="#b16">(Frankle &amp; Carbin, 2018)</ref>, leverage this principle by iden-tifying high-potential subnetworks before training begins. These techniques typically remove parameters associated with the lowest gradients or the weakest early learning responses <ref type="bibr" target="#b50">(Tanaka et al., 2020;</ref><ref type="bibr" target="#b17">Frankle et al., 2020;</ref><ref type="bibr" target="#b33">Lee et al., 2018)</ref>, emphasizing how initial gradient-based criteria can guide the formation of effective, sparse architectures.</p><p>From a gradient sparsity perspective, PaI methods effectively preserve the essential characteristics of the full network's gradient distribution. The resulting subnetworks maintain similar gradient variance and overall gradient magnitude by masking out parameters tied to minimal gradient or learning response. This careful selection ensures that the pruned models exhibit performance levels on par with their unpruned counterparts despite operating with significantly fewer parameters.</p><p>A similar observation has also been revealed in Zero-Cost NAS studies <ref type="bibr" target="#b0">(Abdelfattah et al., 2021;</ref><ref type="bibr">Li et al., 2023b;</ref><ref type="bibr" target="#b55">Xiang et al., 2023)</ref>, which aim to predict the performance of untrained networks by analyzing gradient patterns, finding that gradient score rankings-such as the gradient sum-correlate more strongly with architectural structures than with data batches or initialization parameters. Research by <ref type="bibr" target="#b5">(Bhardwaj et al., 2021)</ref> highlights that gradient flow patterns are inherently linked to a network's architecture. Additionally, studies <ref type="bibr">(Li et al., 2023b;</ref><ref type="bibr" target="#b55">Xiang et al., 2023)</ref> show that gradient sparsity, measured by mean and variance, is closely related to convergence rates and generalization ability. They emphasize calculating gradient sparsity blockwise due to the diverse distributions of gradients across parameter blocks. Moreover, <ref type="bibr" target="#b34">(Lei et al., 2023)</ref> suggests that a balanced training procedure with low-variance gradients enhances sparse training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Setting</head><p>Notations. A neural network is defined based on a set of trainable parameters in specific architectures. We denote the neural network's parameters as θ ∈ R d , where d is the total number of parameters. The training loss function L(θ) defines the objective to be minimized. The parameter space is partitioned into B blocks based on the definition of network architectures, denoted as</p><formula xml:id="formula_0">θ (i) ∈ R di for i ∈ {1, 2, . . . , B}, where d = B i=1 d i . Each parameter θ (i) j</formula><p>within block i for j ∈ [d i ] is associated with its own gradient g</p><formula xml:id="formula_1">(i) j = ∇ θ (i) j L(θ).</formula><p>Key notations used throughout the paper are as follows:</p><p>We denote t ∈ N as the index for the training step, η &gt; 0 as the global learning rate, λ ≥ 0 as the weight decay coefficient, µ as the momentum coefficient, g</p><formula xml:id="formula_2">(i) t</formula><p>∈ R di as the gradient of the loss w.r.t. θ (i) at step t. [d i ] is the index set {1, 2, 3, . . . , d i } corresponding to the parameters in block i. And O( * ) means the complexity, here we use it to measure the storage.</p><p>Stochastic Gradient-based Optimization: Given the loss function L(θ), the goal of a general optimization process is to update θ in the following form iteratively:</p><formula xml:id="formula_3">θ t+1 = θ t -η t D t ,<label>(1)</label></formula><p>where D t denotes the update direction at step t. The choice of D t defines the specific optimization algorithm. For SGD, the update direction is defined as the negative gradient of the loss with respect to θ t :</p><formula xml:id="formula_4">D t = g t , where g t = ∇L(θ t )<label>(2)</label></formula><p>The first-order momentum term was introduced to SGD to enhance the optimisation process, and it is called SGD with momentum(SGDM) <ref type="bibr" target="#b43">(Nesterov, 1983)</ref>. The momentum m can be defined as:</p><formula xml:id="formula_5">m t = β 1 m t-1 + (1 -β 1 )g t<label>(3)</label></formula><p>The update becomes:</p><formula xml:id="formula_6">D t = m t<label>(4)</label></formula><p>This addition helps accelerate convergence by incorporating information from previous gradients to smooth out the update steps. Specifically, it reduces oscillations in the optimization trajectory, particularly in scenarios with steep or narrow ravines in the loss landscape. By maintaining a running average of past gradients, the momentum term allows SGD to move more consistently in directions that lead to faster convergence, addressing challenges like slow progress on flat regions of the loss surface.</p><p>Adaptive Gradient Methods: Adaptive gradient methods like Adam adopted first-order momentum m t as we mentioned above while introducing the second-order momentum v t , which tracks squared gradients to adjust the learning rate for each parameter, the v t defined as:</p><formula xml:id="formula_7">v t = β 2 v t-1 + (1 -β 2 )g 2 t .</formula><p>(5)</p><p>The update direction for Adam is as follows:</p><formula xml:id="formula_8">D t = α t m t , where α t = 1 √ vt + ϵ . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>vt term is the v t with bias correction. Notably, α t is the local learning rate gain (aka. adaptive learning rate). The key computational challenge is the storage and updating of v t , which requires O(d) additional memory.</p><p>Memory Efficient Adam: As Scaling Law <ref type="bibr" target="#b28">(Kaplan et al., 2020)</ref> introduced, Transformer model sizes in recent days have significantly increased compared to the model size when Adam was introduced. Consequently, the memory overhead of the Adam optimizer has become a significant issue, as it requires at least 3x times of memory compared to parameter size. Several approaches have been proposed to reduce the memory overhead of the second-order momentum v t , including (a) Adafactor <ref type="bibr" target="#b48">(Shazeer &amp; Stern, 2018)</ref> shares the v across dimensions, reducing storage from O(d) to O( √ d). However, Adafactor trades off memory savings for lower update precision. (b) Low-bit optimisers quantize <ref type="bibr" target="#b9">(Dettmers et al., 2021)</ref> the storage of v t to low-precision formats (e.g., 8-bit) to save memory. While effective, quantization introduces additional implementation complexity. (c) Adam-mini <ref type="bibr">(Zhang et al., 2024b)</ref> partitions the block and uses the moving average of the estimated v t for each block, thereby reducing storage from O(d) to O(B). However, Adam-mini not only introduces additional computational costs compared to the Adam update process, but its complex partition policy is also incompatible with the default PyTorch partitioning; for example, in default PyTorch partitioning, the attention QKV considered the same group of parameters, while Adam-mini requires further partitioning them with based on heads or neurons. Furthermore, it adaptively calculates and updates α t over time, highlighting its intensive computational complexity. While these approaches reduce memory usage, they all retain the secondmomentum term v t with a trade-off to either performance or update speed.</p><p>Problem Statements. We aim to eliminate the need for the explicit second-order momentum v t entirely while maintaining effective learning rate adaptation. Instead of using α</p><formula xml:id="formula_10">(i) t = η v(i) t +ϵ</formula><p>, we aim to design a new rescaling factor</p><formula xml:id="formula_11">α (i)</formula><p>t that adapts to the loss landscape without requiring the computation or storage of v (i) t . Given a block-wise parameter θ (i) , we seek a new function F such that:</p><formula xml:id="formula_12">α (i) t = F(g (i) 1 , g (i) 2 , . . . , g (i) t ). (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where F determines the local learning rate gain for each parameter block using the gradient history [g</p><formula xml:id="formula_14">(i) 1 , g (i) 2 , . . . , g<label>(i)</label></formula><p>t ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>Considering the substantial memory overhead introduced by the second-order momentum in the Adam optimizer, this section explores strategies to reduce this cost by revisiting the foundational motivations for adaptive gradient methods.</p><p>In the following subsections, we design a memory-efficient learning rate local gain, termed g-SNR, to replace the second-order momentum. We analyse the distribution of g-SNR across different parameter groups throughout the network. This aligns with the motivation of parallel works <ref type="bibr">(Zhang et al., 2024b</ref>) that focus on partitioning parameter groups for learning rate adjustment. Furthermore, we investigate the behaviour of g-SNR during training, demonstrating how dynamic local gains can be replaced with constant preconditioned values calculated in the initial iterations.</p><p>Finally, we introduce our proposed method, SGD-SaI, detailing its design and implementation. This method builds on the insights derived from g-SNR analysis, offering a memory-efficient alternative to second-order momentum while maintaining competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Memory Efficient Local Gain: g-SNR</head><p>Adam builds upon RMSprop, designed to find a local gain for the learning rate, enabling parameter-specific adjustments within deep neural networks <ref type="bibr" target="#b25">(Hinton et al., 2012;</ref><ref type="bibr" target="#b29">Kingma &amp; Ba, 2014)</ref>. By incorporating second-order momentum, Adam improves upon SGD by better handling problems with non-stationary objectives and tasks characterized by noise or sparse gradients <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref>. This mechanism allows Adam to dynamically rescale gradients, effectively adjusting the learning pace across parameter blocks with distinct gradient patterns. Consequently, Adam outperforms SGD when training architectures with heterogeneity problems in the Hessian matrix, such as Transformers <ref type="bibr">(Zhang et al., 2024a;</ref><ref type="bibr">b)</ref>. Another key insight arises from the warm-up mechanism: even with second-order momentum, Adam still requires a warm-up phase to reduce the learning rate at the beginning of training, aiming to mitigate gradient variance <ref type="bibr" target="#b40">(Liu et al., 2021)</ref>. During this phase, gradients are known to be sparse and noisy. Reducing the learning rate directly during the warm-up phase effectively lowers gradient variance, stabilizing the training process straightforwardly and efficiently.</p><p>Intuitively, adaptive gradient methods dynamically adjust the learning rate for each parameter during training. This mechanism encourages parameters with less learning history to learn more while slowing down the learning pace for parameters progressing too quickly. Essentially, it acts as a compensatory approach to address learning imbalances across parameters after they arise. However, if we can predict and pre-empt these imbalances before they occur, we could potentially eliminate the need for second-order momentum, which relies on learning history to evaluate and correct them.</p><p>Considering the root cause of why learning imbalance occurred across different parameters, we discussed them in two main parts. Firstly, as inherited from the architecture characteristics, the parameter in different layers or with different architectures will receive distinct gradient pattern <ref type="bibr" target="#b50">(Tanaka et al., 2020;</ref><ref type="bibr">Li et al., 2023b;</ref><ref type="bibr" target="#b55">Xiang et al., 2023)</ref>, thus bringing the optimal learning rate for different parameters are distinct and need to be re-adjust with local gain <ref type="bibr" target="#b25">(Hinton et al., 2012)</ref>, Secondly, within the parameter groups, the</p><p>0 100 200 300 400 500 600 700 800 g-SNR 0 20 40 60 80 100 Frequency g-SNR (Mixed) Distribution Across Parameter Blocks norm1 norm2 attn.qkv attn.proj mlp.fc1 mlp.fc2 cls_token pos_embed patch_embed.proj head fc_norm 0 100 200 300 400 500 600 700 800 g-SNR 0 5 10 15 20 25 Frequency g-SNR (Weights Only) Distribution Across Parameter Blocks norm1 norm2 attn.qkv attn.proj mlp.fc1 mlp.fc2 patch_embed.proj head fc_norm 19.6 19.7 19.8 19.9 20.0 20.1 20.2 g-SNR 0 10 20 30 40 50 Frequency g-SNR (Bias Only) Distribution Across Parameter Blocks norm1 norm2 attn.qkv attn.proj mlp.fc1 mlp.fc2 patch_embed.proj head fc_norm <ref type="table">Figure 3</ref>. We observe that the g-SNR varies across different parameter blocks. However, for most weights, the parameter blocks that share the same structure across different transformer layers (blocks) tend to have similar g-SNR values. Additionally, the g-SNR values for the bias parameters are consistently low magnitude. Our method can be viewed as partitioning all parameter blocks based on their structure. gradient can be noisy or sparse based on the objective and data, that will introduce imbalance update to parameters.</p><p>We propose using the gradient signal-to-noise ratio (g-SNR) introduced by <ref type="bibr" target="#b55">(Xiang et al., 2023)</ref> to adjust the learning rate block-wisely, as it measures the norm and variance of gradients of the parameter block, which reflects overall update magnitude and variance of gradient between paramters. Specifically for each block i, the gradient norm (ℓ 2 -norm) and variance are calculated as</p><formula xml:id="formula_15">G (i) norm = di j=1 g (i) j 2 , G (i) var = 1 d i di j=1 g (i) j -ḡ(i) 2 ,</formula><p>where</p><formula xml:id="formula_16">ḡ(i) = 1 di di j=1 g (i)</formula><p>j , and d i is the number of parameters in block i. The gradient signal-to-noise ratio for each block is then given by</p><formula xml:id="formula_17">G (i) snr = G (i) norm G (i) var + ϵ ,</formula><p>where ϵ is a small constant added for numerical stability. To ensure consistent scaling across all blocks, we normalize the g-SNR of each block by the maximum g-SNR among all blocks:</p><formula xml:id="formula_18">G(i) snr = G (i) snr max k G (k) snr .</formula><p>This normalization confines the g-SNR values between 0 and 1, facilitating a fair comparison and adjustment of learning rates across different parameter blocks. Thus, we establish the local gain by replacing v t with the following expression:</p><formula xml:id="formula_19">α (i) t = F(g (i) t ) = G(i) snr where α (i)</formula><p>t represents the local gain at step t guided by the temporal value G(i) snr which determines the update direction D t . This approach reduces the memory overhead of v t from O(d) to O(B).</p><p>Adapting the learning rate according to the normalized gradient signal-to-noise ratio significantly influences gradient variance during training. When the high gradient noise or sparsity in block i occurs, G(i) snr tend to have relatively lower value, the learning rate η is scaled down by a factor α (i) = G(i) snr , resulting in a reduced learning rate η (i) = α (i) η. This adjustment decreases the magnitude of parameter updates for that block:</p><formula xml:id="formula_20">θ (i) t+1 = θ (i) t -η (i) ∇ θ (i) L(θ t ).</formula><p>Lowering the learning rate mitigates the amplification of gradient noise, thereby reducing gradient variance within each training step, leading to smoother convergence and enhanced robustness <ref type="bibr" target="#b39">(Liu et al., 2019)</ref>. If G(i) snr remains low across multiple batches, the continued reduction of η (i) further stabilizes training by preventing large, erratic updates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Different Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Statistics Analysis for g-SNR</head><p>Building on the insights above, we implemented the g-SNR mechanism using PyTorch's Default Partition <ref type="bibr">(Zhang et al., 2024b)</ref>, which computes g-SNR within each parameter block and dynamically re-scales the learning rate accordingly. To assess its effectiveness, we conducted experiments on Vision Transformer (ViT) pre-training tasks using ImageNet-1K, selecting ViT/S-16 for comprehensive tracing and analysis of gradient patterns throughout the training process.</p><p>Our analysis revealed that g-SNR remains relatively stable over time while exhibiting distinct patterns across different parameter classes, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Specifically, we examined transformer blocks from shallow, middle, and deep layers within the network and parameters outside the transformer blocks, such as positional embeddings.</p><p>Given the g-SNR definition we provide in the previous subsection, we analyze its behaviour as follows: As modern initialization schemes (e.g., Xavier <ref type="bibr" target="#b31">(Kumar, 2017)</ref>, Kaiming <ref type="bibr">(He et al., 2015b)</ref>) ensure that at t = 0:</p><formula xml:id="formula_21">G (i) norm (0) and G (i) var (0)</formula><p>are well-controlled. This implies that G (i) snr (0) starts from a stable, architecture-driven ratio. During the training process, parameters are updated and controlled by the step size η is the learning rate. Assuming η is sufficiently small to stabilize training process, we have θ</p><formula xml:id="formula_22">(i) t+1 ≈ θ (i)</formula><p>t . Thus, the change in parameters per iteration is small.Consider the gradient at iteration t + 1:</p><formula xml:id="formula_23">g (i) t+1 = ∇ θ (i) L(θ t+1 ).</formula><p>Consider a first-order Taylor expansion of the gradient around θ (i) (t):</p><formula xml:id="formula_24">g (i) t+1 ≈ g (i) t + J (i) t ∆θ (i) t ,</formula><p>where J (i) t is the Jacobian (or a first-order sensitivity matrix) of g (i) w.r.t. θ (i) , and ∆θ</p><formula xml:id="formula_25">(i) t = θ (i) t+1 -θ (i) t . Since ∥∆θ (i)</formula><p>t ∥ is small, the change in the gradient vector is also small. Hence,</p><formula xml:id="formula_26">g (i) j(t+1) ≈ g (i) j(t) , ∀j.</formula><p>Because each component g (i) j(t+1) differs only slightly from g (i) j(t) , their average and variance remain stable:</p><formula xml:id="formula_27">ḡ(i) t+1 ≈ ḡ(i) t , G (i) var(t+1) ≈ G (i) var(t) .</formula><p>Similarly, for gradient norm,</p><formula xml:id="formula_28">G (i) norm(t+1) = di j=1 g (i) j(t+1) 2 ≈ G (i) norm(t) .</formula><p>Since both</p><formula xml:id="formula_29">G (i) norm(t) and G (i) var(t) remain nearly unchanged, G (i) snr(t+1) = G (i) norm(t+1) G (i) var(t+1) + ϵ ≈ G (i) norm(t) G (i) var(t) + ϵ = G (i) snr(t) .</formula><p>Thus, G snr(t) remains effectively constant over iterations.</p><p>Even though parameters change, the "shape" or statistical profile of the gradient distribution does not drastically alter. The g-SNR measures a dimensionless ratio that characterizes this shape. Minor parameter shifts do not significantly affect this ratio; hence, it remains nearly constant.</p><p>This finding aligns with the observation by <ref type="bibr" target="#b55">(Xiang et al., 2023)</ref> that g-SNR strongly correlates with architecture. Leveraging this insight, we replaced the dynamic calculation of g-SNR with constant values determined during initialization, significantly reducing computational costs during each training step. When calculating g-SNR using PyTorch's Default Partition, we observed that the g-SNR values vary significantly across partitions. By leveraging constant g-SNR values, this approach effectively assigns a pre-conditioned learning rate scale to each partition. (Zhang et al., 2024b) highlights a key limitation of PyTorch's default parameter partitioning: its lack of granularity for optimizers like Adam-mini. While PyTorch groups parameters such as attention QKV together, Adam-mini requires finer partitions, such as by attention heads or neurons, to perform effectively, especially in Transformer-based architectures. This limitation stems from the default partitioning's failure to align with Hessian sub-block structures critical for optimization.</p><p>This observation does not hold true in our case. Our empirical results, shown in Fig. <ref type="figure" target="#fig_3">5</ref> and discussed further in Sec. 5, demonstrate that our method works effectively with PyTorch's Default Partition and does not require any additional fine-grained partitioning strategies. The distribution of g-SNR across different partitions is detailed in Fig. <ref type="figure">3</ref>, where we observe that, for most weights, parameter blocks sharing the same structure across different Transformer layers exhibit similar g-SNR values. Additionally, the g-SNR values for bias parameters remain consistently low, reflecting their uniform magnitude. A notable exception is the norm1 weights from blocks.0, which connect to the input from embedded patches, whereas all other norm1 weights connect to the output of the previous block. This observation highlights that our g-SNR values can effectively identify distinct characteristics among different parameter groups and the network's topological impacts. Moreover, it indicates that gradient sparsity and noise levels vary across parameter groups, confirming the necessity of using a local gain mechanism to balance learning rates across partitions.</p><p>Notably, our approach, compatible with PyTorch's Default Partition, enables simultaneous updates of each coarsegrained parameter block and eliminates the need for dynamic learning rate calculations. This efficiency resulted in a threefold speedup in the optimizer update step compared to Adam-mini when training the GPT2-small model. Moreover, it reduces the implementation complexity associated with the exhaustive Hessian calculations required for fine-grained parameter partitioning <ref type="bibr">(Zhang et al., 2024b)</ref>.</p><p>In summary, instead of relying on second-order momentum to compute gradient history and adjust learning rates to address imbalanced updates after they occur, our g-SNR approach determines the gradient sparsity level at the first iteration of training. This enables assigning appropriate pre-conditioned learning rate scales to different parameter partitions, simplifying the update process, improving memory efficiency, and significantly speeding up optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Proposed Methods Detail: SGD-SaI</head><p>We propose a new method called SGD-SaI that removes adaptive gradient components by rescaling the learning rates of each parameter block using the g-SNR calculated from the initial batch. The algorithm details are presented in Algorithm 6. By leveraging the initial g-SNR, we capture the inherent gradient characteristics of different parameter blocks, allowing for a constant scaling factor that addresses the variations in gradient magnitudes across blocks.</p><p>As our method eliminates the dynamic terms associated with adaptive gradient algorithms, it only introduces a few computations at the first iteration compared to naive Stochastic Gradient Descent with Momentum (SGDM). Specifically, the additional computation involves calculating the g-SNR for each parameter block during the initial batch. After this initial computation, the training proceeds similarly to standard SGDM, making our method computationally efficient and comparable in complexity to traditional SGD.</p><p>To update the g-SNR based on the actual gradient sparsity without affecting the gradient computation, we adopt decoupled weight decay as proposed by Loshchilov and Hutter <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2019)</ref>. Decoupled weight decay applies regularization directly to the parameters rather than incorporating it into the gradient computation. This approach is equivalent to regularization in SGD and allows us to accurately compute the gradient statistics needed for the g-SNR without the weight decay term distorting the gradient values. By doing so, we ensure that the g-SNR reflects the gradients' true sparsity and noise characteristics.</p><p>Our implementation remains extremely straightforward, as we adopt the simplest approach that requires only minimal modifications to the existing SGD optimizer. This simplicity ensures that existing tricks and frameworks that support SGD can seamlessly integrate with and support our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 SGD-SaI</head><p>Require: T (total steps), η (learning rate), θ i (i-th parameter block), L(θ) (loss function), λ (weight decay), µ (momentum), ϵ (small constant), maximize 1: for t ← 1 to T do 2:</p><p>Compute gradient:</p><formula xml:id="formula_30">g i t ← ∇ θ i L(θt-1) 3:</formula><p>if maximize then 4:</p><formula xml:id="formula_31">g i t ←</formula><p>-g i t 5: end if 6: /* Apply momentum */ 7: if t &gt; 1 then 8: m i t ← µm i t-1 + (1 -µ)g i t 9: else 10: m i t ← g i t 11: /* Compute g-SNR */ 12: G i snr ← G i norm G i var + ϵ 13: /* Normalize g-SNR */ 14: Gi snr ← G i snr max k G k snr 15: end if 16: /* Apply weight decay */ 17: θ i t ← θ i t-1 -ληθ i t-1 18: /* Update parameters with scaled learning rate */ 19: θ i t ← θ i t -η Gi snr m i t 20: end for Figure 6</p><p>. Our Algorithm. we introduce a simple parameter-blockwise scaling using the normalized g-SNR to rescale the learning step size. This allows SGD to perform block-wise effective learning, unlocking its potential to work well on networks with block heterogeneity problems <ref type="bibr">(Zhang et al., 2024a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section evaluates our method through several tasks, including pre-training for Large Language Model (LLM) and Vision Transformer (ViT), Parameter-Efficient Fine-Tuning (PEFT) tasks on LLM and Diffusion Model (DM), and traditional Convolutional Neural Network (CNN) tasks.</p><p>The specific tasks are outlined as follows:</p><p>• Large Language Model(Transformer Decode Only)</p><p>We pre-train GPT-2 <ref type="bibr" target="#b46">(Radford et al., 2019)</ref> on Open-WebText <ref type="bibr" target="#b20">(Gokaslan &amp; Cohen, 2019)</ref>. We profile the optimizer state tensors' memory usage and optimizer step time for GPT-2-XL(1.5B) and LLM2-7B.</p><p>• Vision Transformer We investigate the Vision Transformer (ViT/S-16) <ref type="bibr" target="#b13">(Dosovitskiy et al., 2021)</ref> on the ImageNet-1k dataset <ref type="bibr" target="#b7">(Deng et al., 2009)</ref> for image classification tasks. We profile the optimizer state tensors' memory usage and optimizer step time for ViT-H/14.</p><p>• Parameter-Efficient Fine-Tuning (PEFT) LoRA Fur-thermore, we explore Parameter-Efficient Fine-Tuning (PEFT) tasks for GPT-2 LoRA <ref type="bibr" target="#b27">(Hu et al., 2021)</ref> finetuning on the E2E <ref type="bibr" target="#b44">(Novikova et al., 2017)</ref> dataset and Diffusion Model fine-tuning to capture visual concepts.</p><p>For image classification, we report the top-1 validation accuracy, while for Large Language Model (LLM) finetuning tasks, to evaluate the results of the fine-tuning, we report metrics such as BLEU <ref type="bibr" target="#b45">(Papineni et al., 2002)</ref>, NIST <ref type="bibr" target="#b11">(Doddington, 2002)</ref>, MET <ref type="bibr" target="#b1">(Banerjee &amp; Lavie, 2005)</ref>, ROUGE-L <ref type="bibr" target="#b38">(Lin, 2004)</ref>, and CIDEr <ref type="bibr" target="#b53">(Vedantam et al., 2015)</ref>. For all these metrics, higher scores indicate better performance. Additionally, we perform qualitative evaluations for the Diffusion Model (DM) fine-tuning task.</p><p>• Convolutional Neural Networks (CNNs). We study ResNet-18 (11M parameters) on the CIFAR-10 dataset, and architectures from NATS-Bench <ref type="bibr">(Dong et al., 2021)</ref> on CIFAR-10, CIFAR-100, and ImageNet16-120 <ref type="bibr" target="#b30">(Krizhevsky &amp; Hinton, 2009;</ref><ref type="bibr" target="#b6">Chrabaszcz et al., 2017)</ref>. All these are image classification tasks, and we report the top-1 test accuracy as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">LLM Pre-train</head><p>Setups. We pre-train GPT-2-Small (125M) <ref type="bibr" target="#b46">(Radford et al., 2019)</ref> on OpenWebText <ref type="bibr" target="#b20">(Gokaslan &amp; Cohen, 2019)</ref>. We compare SGD-SaI with AdamW <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2019)</ref> and Adam-mini <ref type="bibr">(Zhang et al., 2024b)</ref>. We follow the same settings as described in the previous study <ref type="bibr">(Zhang et al., 2024b)</ref>. We analyse the loss metrics for each optimizer.</p><p>For large-scale LLMs, we provide profiling results focusing on memory usage and wall-clock time during the optimizer step for GPT-2-XL (1.5B parameters) and Llama-2 (7B parameters). Due to resource constraints, these results are limited to the optimizer step time and do not encompass full training runs. We compare SGD-SaI with SGDM, AdamW, Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref>, Adam-mini, and Prodigy <ref type="bibr" target="#b42">(Mishchenko &amp; Defazio, 2023)</ref>. The reported metrics include memory usage of the state tensors and the time costs associated with the optimizer steps. All results were obtained using a single NVIDIA A100 (80GB).</p><p>Results. Figure <ref type="figure">8</ref> compares optimizers (AdamW, Adammini, and SGD-SaI) during pre-training of GPT-2-Small across multiple metrics. While SGD-SaI demonstrates a slightly slower initial convergence speed compared to the Adam family optimizers due to its design, it achieves superior final convergence with a lower training loss (outperforming Adam-mini by 0.13). Similarly, validation loss shows a marginal improvement, with SGD-SaI reducing it by 0.03 compared to Adam-mini.</p><p>Efficiency. For the GPT-2-Small pre-training task. Re- garding update speed, SGD-SaI demonstrates a significant advantage, being three times faster than Adam-mini in parameter updates and outperforming AdamW. Furthermore, the memory efficiency of SGD-SaI is noteworthy-it consumes only half the memory required by AdamW while maintaining performance comparable to Adam-mini, which employs intricate partitioning strategies. Unlike Adam-mini, which requires complex parameter partitioning (eg. users need to manually transform the Pytorch default partitions like the combined QKV block into separate Q, K and V blocks.), SGD-SaI achieves similar or better results using the default PyTorch partitioning, highlighting its simplicity and efficiency. For the untrained models.</p><p>By design, the state tensors for Adam and AdamW are approximately twice the size of the gradient, while Prodigy's state tensors are roughly four times larger. In contrast, SGD-SaI has state tensors of the same size as standard SGDM. This effectively reduces memory usage by up to 75% compared to Prodigy and by 50% compared to Adam(W). The detailed discussion can be found in the Appendix B. As shown in Table 1, SGD-SaI maintains a manageable memory footprint, enabling it to work with large models like Llama-2 (7B) without running into out-of-memory (OOM) errors. In contrast, other optimizers, such as AdamW and Prodigy, exceed available memory limits at this model size, highlighting the scalability challenges posed by memory-intensive optimizers when dealing with long context lengths in LLMs. Adam-Mini requires a partitioning strategy for different parameter groups while adjusting the learning rate adaptively at each time step. This increases memory usage and computational cost as different groups can not update simultaneously. For models larger than 1 billion parameters, the performance gains from Adam-Mini decrease by approximately 45%, while the reduction achieved with SGD-SaI remains around 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ViT Pre-train</head><p>Setups. We pre-train ViT-S/16 <ref type="bibr" target="#b13">(Dosovitskiy et al., 2021)</ref> on the ImageNet1k dataset <ref type="bibr" target="#b7">(Deng et al., 2009)</ref> for the image classification task. We compare SGD-SaI with AdamW <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2019)</ref> as well as popular optimizers including SGDM, Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref>, Adam-mini <ref type="bibr">(Zhang et al., 2024b)</ref> and Prodigy <ref type="bibr" target="#b42">(Mishchenko &amp; Defazio, 2023)</ref>. After conducting a grid search within the same hyperparameter range, we compare the optimiser results. We report the peak and mean of the top-1 validation accuracy to evaluate their generalisation ability and sensitivity to hyperparameter changes. Detailed hyperparameters are in Appendix A.1.</p><p>Due to the intensive computational power requirements for the ViT variants during the grid search, we cannot provide the complete training results. Instead, we follow the same procedure outlined in Section 5.1 and present only the profiling results regarding memory usage and wall-clock time during the optimizer step for ViT-S/16 and ViT-H/14. Additionally, we compare SGD-SaI with SGDM, AdamW, Adam, Adam-mini, and Prodigy. All results were obtained using a single NVIDIA A100 with 80GB of memory.</p><p>Results. We report peak performance under the best hyperparameters, averaging results over three random seeds, and present the mean and standard deviation (see Table <ref type="table">2</ref>). Our simple re-scaling strategy significantly boosts SGDM's performance from 63.80 to 72.92, nearly matching AdamW's 73.04. Meanwhile, the recent SOTA optimizer Prodigy achieves a slightly higher peak at 73.24, though it requires additional one-time memory usage. We will discuss these results further in later sections. Notably, our approach achieves the lowest standard deviation (0.07) across three random seeds, compared to Prodigy's second-lowest at 0.21, highlighting the stability of our method during training.</p><p>In addition, we examine average performance across the hyperparameter search grid using a rest setting that deviates from the best hyperparameters as a tweaked version. Under these conditions, we observe that most previous methods, including AdamW, struggle significantly, leading to dramatic drops in average performance. For example, AdamW, despite being an update over SGD intended to improve robustness to hyperparameters, achieves only 37.21 with a standard deviation of 35.43. In contrast, our method maintains overall performance, achieving an average of 57.55 with a much lower variance (standard deviation of 18.46). Prodigy, a parameter-free optimizer not designed to adjust learning rate and weight decay, fails to converge when these hyperparameters are modified; thus, we exclude it from this part of the comparison for fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimizer</head><p>Peak@top1 (%) Avg@top1 (%) SGDM 63.</p><p>80 ± 0.35 14.33 ± 19.38 Adam 61.56 ± 0.93 20.93 ± 22.05 Adam-Mini 72.29 ± 0.43 36.65 ± 35.39 AdamW 73.04 ± 0.31 37.21 ± 35.43 Prodigy 73.24 ± 0.21 N/A SGD-SaI(Ours) 72.92 ± 0.07 57.55 ± 18.46 Table 2. Comparison of peak and average top-1 validation accuracy on ImageNet-1k for ViT-S/16 trained from scratch. Each optimizer's performance is evaluated over a hyperparameter search space, reporting the highest accuracy (Peak@top1) and the average accuracy (Avg@top1) across all trials. Results are averaged over three seeds, with standard deviations for statistical analysis. Our method achieves significantly higher robustness to hyperparameter variations, maintaining a high average performance (57.55%) and outperforming other optimizers by at least 20%.</p><p>Our method demonstrates superior robustness and effectiveness in training ViT-S/16 models from scratch on ImageNet-1k, outperforming previous optimizers across peak and average performance metrics. While alternative optimizers like AdamW and Prodigy achieve high peak accuracy, their performance drops significantly under hyperparameter variations, highlighting their sensitivity. In contrast, our approach maintains a stable and high average accuracy across diverse hyperparameter settings with minimal standard deviation. It underscores its resilience to hyperparameter tuning and potential for more efficient and reliable model training in real-world applications. This stability makes our method particularly suitable for scenarios where hyperparameter tuning is constrained, offering a consistent and robust solution for training large models.</p><p>Although our method does not utilize adaptive gradient adjustments, it achieves a stable and steady learning pace, ultimately reaching a comparable performance, as shown in Figure <ref type="figure">7</ref>. Empirically, this stability arises from our preconditioned learning rate, which ensures each step is wellcontrolled and converges reliably with sufficient training steps. In contrast, Adam-family optimization methods often achieve faster convergence but are prone to being trapped in suboptimal minima due to their aggressive adaptivity.</p><p>Efficiency. As shown in Table <ref type="table">3</ref>, our method achieves a wall-clock time for optimizer steps comparable to SGDM, while being significantly faster than Adam-mini, Adam(W), and Prodigy. We must note that we present the wall clock time for each optimizer step rather than the total runtime. We did not rely on the grid search results to report the total runtime because all grid search experiments were conducted on a cluster. Due to complex factors, such as the cluster's I/O bottleneck and network congestion, distributed train-ing can be considerably slowed down. We chose to maintain the same settings and device while profiling the LLM. Regarding memory usage, similar trends were observed during the pre-training of GPT-2 Small. For example, the ViT-H/14-0.66B model uses only 2.42 GB of memory with SGD-SaI, compared to 4.86 GB with AdamW and 9.70 GB with Prodigy. Our method reduces memory consumption by 50% compared to Adam(W) and by 75% compared to Prodigy. Further empirical analysis can be found in App. B.</p><p>Model Method State Mem (GB) Wall Time (ms) ViT-S/16(0.0229B) SGDM 0.08 7.9 ± 0.3 AdamW 0.17 45.0 ± 8.0 Adam 0.17 50.0 ± 1.5 Prodigy 0.33 78.0 ± 0.0 Adam-Mini 0.08 84.0 ± 5.0 SGD-SaI(ours) 0.08 12.4 ± 0.2 ViT-H/14(0.66B) SGDM 2.42 40.0 ± 1.0 AdamW 4.86 124.0 ± 4.0 Adam 4.86 127.0 ± 2.0 Prodigy 9.70 260.0 ± 3.0 Adam-Mini 2.54 220.0 ± 20.0 SGD-SaI(ours) 2.42 54.0 ± 13.0</p><p>Table <ref type="table">3</ref>. We maintain the same settings as in Table <ref type="table" target="#tab_4">1</ref>. We ensure a comparable memory footprint to SGDM, while keeping the optimizer step time controlled, resulting in a performance that is 4-6 x faster than Adam-mini.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Parameter Efficient Fine Tuning: PEFT</head><p>We primarily consider PEFT tasks on LLM fine-tuning and the Diffusion Model fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">LLMS PARAMETER EFFICIENT FINE-TUNING</head><p>Setup. We fine-tune the GPT-2 model using the E2E dataset <ref type="bibr" target="#b44">(Novikova et al., 2017)</ref>. The current state-of-the-art (SOTA) methods include scaled-SGD and scaled-AdamW from <ref type="bibr" target="#b58">(Zhang &amp; Pilanci, 2024)</ref>, which adjust the learning rates for parameters A and B with a Riemannian preconditioner. Our primary comparison is between SGD-SaI and these methods, along with Adam-mini. To evaluate the results of the fine-tuning, we report metrics such as BLEU <ref type="bibr" target="#b45">(Papineni et al., 2002)</ref>, NIST <ref type="bibr" target="#b11">(Doddington, 2002)</ref>, MET <ref type="bibr" target="#b1">(Banerjee &amp; Lavie, 2005)</ref>, ROUGE-L <ref type="bibr" target="#b38">(Lin, 2004)</ref>, and CIDEr <ref type="bibr" target="#b53">(Vedantam et al., 2015)</ref>. For all these metrics, higher scores indicate better performance.</p><p>Results. We adopted the same experiment setting to investigate whether our methods suit LoRA Training <ref type="bibr" target="#b27">(Hu et al., 2021)</ref>. We set the default learning rate to 1e -3 and weight decay to 1e -2. Empirically, we observe that SGD-SaI outperforms previous state-of-the-art (SOTA) scaled optimizers and unscaled ones. Table <ref type="table">4</ref> presents surprising results regarding the final scores for LoRA fine-tuning of the GPT-2 medium model with a rank of 4 on the E2E natural language generation tasks. With this simple precondition on SGDM, our method performs significantly better than the previous SOTA strategy using rescaled SGD. Furthermore, our approach exhibits a substantial improvement over AdamW in fine-tuning the GPT-2 architecture, even without meticulous tuning and searching for hyperparameters.</p><p>Method BLEU NIST MET ROUGE-L CIDEr SGD r = 4 66.6 8.54 44.2 68.2 2.32 scaled SGD r = 4 69.2 8.71 46.3 70.9 2.48 AdamW r = 4 68.9 8.69 46.5 71.3 2.51 Adam-Mini r = 4 68.7 8.66 46.3 71.1 2.50 scaled AdamW r = 4 69.6 8.77 46.6 71.8 2.52 SGD-SaI (ours) r = 4 69.9 8.81 46.7 72.1 2.53</p><p>Table <ref type="table">4</ref>. This table presents scores for LoRA fine-tuning of GPT-2 medium model on E2E Natural Language Generation (NLG) challenge with different optimizers. SGD-SaI outperforms all scaled and unscaled optimizers on all evaluation metrics. In particular, our method closes the performance gap between SGD and AdamW and reveals its effectiveness in performing block-wise scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">DMS PARAMETER EFFICIENT FINE-TUNING</head><p>Setup. Using the diffusion model, we extend our experiments to include LoRA fine-tuning on image generation tasks. Specifically, we utilize the ChilloutMix model to address real-world concepts, following the same approach outlined in Mix-of-show <ref type="bibr" target="#b22">(Gu et al., 2024;</ref><ref type="bibr" target="#b58">Zhang &amp; Pilanci, 2024)</ref>. Additionally, we compare our method with the state-of-the-art (SOTA) optimized approach using scaled-AdamW <ref type="bibr" target="#b58">(Zhang &amp; Pilanci, 2024)</ref>. To evaluate the images generated by the diffusion model, we conduct a qualitative assessment to determine which method captures visual concepts more effectively.</p><p>Results. Face generation is a challenging task; the model should understand the visual concept of a specific person's face based on its prompt text. Here, we set the learning rate as default 0.1, a large enough default value. We observed as Fig. <ref type="figure">9</ref>, even without carefully tuning the learning rate, our scaled methods have shown a significantly better ability to capture the visual concept of potter than the previous SOTA scaled approach scaled-AdamW <ref type="bibr" target="#b58">(Zhang &amp; Pilanci, 2024)</ref>. It should be verified that our optimizer has better parameters robustness on training and leads to better convergence in final performance; this should be an essential benefit for the practical use of the optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Convolutional Neural Network(CNN)</head><p>Setup. We follow a similar approach to Section 5.2 for evaluating CNN models. A grid search is performed on ResNet18 <ref type="bibr">(He et al., 2015a)</ref> using the CIFAR-10 dataset, and across various architectures from NATS-Bench <ref type="bibr">(Dong et al., 2021)</ref> on CIFAR-10, CIFAR-100, and ImageNet16-120 <ref type="bibr" target="#b30">(Krizhevsky &amp; Hinton, 2009;</ref><ref type="bibr" target="#b6">Chrabaszcz et al., 2017)</ref>. All tasks involve image classification. We compare SGD-SaI with traditional optimizers (SGD and Adam-family)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Scaled AdamW</p><p>Figure <ref type="figure">9</ref>. Generation results for prompt "a pencil sketch of 〈Vpotter〉" by Mix-of-Show model with scaledAdamW optimizers(up) and our optimizer(down). Our method generates photos that better capture the prompt and align with visual concepts from training samples; At the same time, previous SOTA-scaled AdamW has some significant bad cases that do not follow the prompt, we marked them with a red bounding box. Results. Figure <ref type="figure" target="#fig_3">5</ref> (left graph) presents the performance of ResNet18. Our method achieves a peak accuracy of 95.36%, which not only surpasses that of Adam(W) and SGD but also shows greater stability. In addition, we evaluated a range of search spaces, including datasets such as CIFAR-10, CIFAR-100, and ImageNet16-120, as well as architectures of varying sizes. We conducted a grid search across eleven architectures, testing three learning rates and four weight decay values. The distribution of top-1 accuracies is illustrated in Fig. <ref type="figure" target="#fig_5">10</ref>, which demonstrates the stability of our method across different architectures and hyperparameter settings. Our approach results in models with lower standard deviations and higher mean accuracies, indicating enhanced stability and generalization. These findings highlight the robustness of our method across various CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In summary, our results demonstrate that simply applying selective learning rate scaling at initialization (SGD-SaI) can unlock performance comparable to-if not better than-leading adaptive gradient methods like AdamW, all while retaining the simplicity and efficiency of SGDM. By leveraging g-SNR to guide parameter group scaling, SGD-SaI not only mitigates early training imbalances but also substantially reduces optimizer memory overhead, enabling more resource-efficient model training. Its robustness across a wide range of Transformer-based tasks, including Ima-geNet classification with ViT, GPT-2 pretraining, LoRA fine-tuning, and diffusion modelling, underscores its versatility and practicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitation</head><p>While SGD-SaI demonstrates promising results across various Transformer-based tasks, our study is constrained by limited computational resources, preventing us from conducting large-scale pre-training on more extensive models such as Llama-2-7B. This remains an avenue for future research. However, to address the efficiency challenges of training larger models, we have performed detailed profiling of GPU memory usage and optimizer step speed on these architectures. These preliminary analyses indicate the potential scalability of SGD-SaI, but comprehensive evaluations on larger-scale models are necessary to establish its effectiveness and efficiency in such settings fully. Moreover, our methods ensure a steady and stable update during training, allowing the model to converge better in a given task with sufficient training steps. Thus, we might observe that the convergence speed is relatively lower than Adam's in the early stage of training; as our primary focus is to investigate the effectiveness of the SaI approach, we left the acceleration of convergence speed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Experiments Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details for ViT Experiments</head><p>In this section, we will list list the settings of the experiment regarding to Section 5.2.</p><p>Hyperparameter Settings: We start by following the settings in <ref type="bibr">(Beyer et al., 2022a;</ref><ref type="bibr" target="#b49">Steiner et al., 2022;</ref><ref type="bibr">Beyer et al., 2022b)</ref>; Specifically, we include Nesterov-SGD as a baseline, offering better performance than naive SGD. All optimizers are tested using a grid search within the same hyperparameter ranges: learning rate lr ∈ {0.1, 0.01, 0.001, 0.0001} and weight decay wd ∈ {0.01, 0.001, 0.0001}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details for CNN Experiments</head><p>In this section, we will list the settings of the experiment regarding to Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Datasets:</head><p>We follow the same settings and train some CNN-based architectures proposed in NATS-Benchmark <ref type="bibr">(Dong et al., 2021)</ref>. We test the optimizers on CIFAR-10/CIFAR-100 <ref type="bibr" target="#b30">(Krizhevsky &amp; Hinton, 2009)</ref> and ImageNet16-120 <ref type="bibr" target="#b6">(Chrabaszcz et al., 2017)</ref>. Based on the NATS-Benchmark work, we test different sizes of architectures. Here, we select ten architectures with top-10 validation accuracy and one architecture with bottom-1 validation accuracy in terms of different datasets and training epochs to present.</p><p>Hyperparameter Settings: The optimal learning rate and weight decay are chosen by performing the grid search. The learning rate and weight decay are selected from η ∈ {0.1, 0.01, 0.001} and λ ∈ {0.5, 0.05, 0.005, 0.0005}, respectively. We use the same cosine annealing scheduler on three datasets without learning rate warmup. We use the same data augmentation methods and set the batch size to 256 for all datasets. The experiments are designed to run for full training without early stopping. There is no linear scaling on the initial weight decay either since we are doing the grid search within a feasible range. The seed is only 777 which is the same seed reported by NATS-Benchmark on Size Search Space. The original NATS-Benchmark were produced using SGD with a fixed learning rate 0.1 and weight decay 0.0005 and the default setting of the Nesterov Momentum. For fair comparison, we apply the same grid search policy for SGD, as the baseline with or without Nesterov Momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The performance of various architectures has been represented as histograms showing top-1 accuracy on different datasets. Fig. <ref type="figure" target="#fig_5">10</ref> demonstrates that our method outperforms other optimizers in terms of evaluation accuracies within the same hyperparameter search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Extra Results for ResNet18 on CIFAR10</head><p>As a classic model of the CNNs, we also conduct the grid search on ResNet18 as an extended experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Datasets:</head><p>We follow the similar setting in the Section 5.4. We particularly choose the CIFAR-10 ( <ref type="bibr" target="#b30">Krizhevsky &amp; Hinton, 2009)</ref> as the dataset we test on. We test on the classic ResNet18 model.</p><p>Hyperparameter Settings: Since we are focusing on a single model with one dataset-unlike the NAST-Benchmark CNN experiments discussed in Section 5.4-we are scaling up our search by exploring a wider range of learning rates and weight decays. The learning rates are chosen from the set η ∈ {0.1, 0.01, 0.001, 0.0001} and the weight decays from λ ∈ {0.5, 0.05, 0.005, 0.0005, 0.00005}. We will repeat our grid search three times using three different random seeds. The random seeds used for the experiments are {42, 888, 999}. We opted for a step learning rate scheduler rather than a cosine annealing scheduler to test our method's resilience to different learning rate scheduling policies. The learning rate will decrease by a factor of 10 every 80 epochs, with a total of 200 epochs for training. Our data augmentation methods remain consistent, with a batch size set to 128. These experiments will run for the entire training duration without early stopping, and there will be no linear scaling applied to the initial weight decay, as we are conducting a grid search within a reasonable range. The distribution of accuracies averaged over the three seeds for each hyperparameter combination is depicted in Fig. <ref type="figure" target="#fig_3">5</ref>. The best performance of each optimizer, along with the optimal learning rate and weight decay, is annotated with red numbers on the graph. For simplicity, we are only testing the Stochastic Gradient Descent with Momentum (SGDM) as the baseline. The momentum is set to the default value of 0.9, consistent with both the Adam(W) optimizer and our method to ensure fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The grid search results are shown in the Fig. <ref type="figure" target="#fig_3">5</ref>. Not only does our method converge better (ours 95.36% v.s. SGDM 95.26%), but it also demonstrates greater resilience to changes in hyperparameters. This means the performance is less likely to downgrade compared to SGDM.</p><p>All the experiments in this paper were conducted using various types of GPUs, including NVIDIA GeForce RTX 3090, NVIDIA A100 PCIe 40GB, and NVIDIA A100 80GB. To ensure consistent experimental conditions, each experiment was conducted using only one GPU type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimizer Analysis</head><p>This section provides a supplementary analysis for Section 5. We will detail the optimizers and empirically estimate the lower boundary of the state tensors in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 SGD</head><p>Require: t (step), η (lr), θ i (i-th params), L(θ) (loss function), λ (weight decay), µ (momentum), τ (dampening), maximize 1: repeat 2:</p><p>for t ← 1 do 3:</p><formula xml:id="formula_32">g i t ← ∇L(θ i t-</formula><p>1 ) 4: 5: /* do weight decay */ 6: g i t ← g i t + λθ i t-1 7: 8: /* do momentum */ 9: if µ ̸ = 0 and t &gt; 1 then 10: m i t ← µm i t-1 + (1 -τ )g i t 11: else 12: m i t ← g i t 13: end if 14: 15: if maximize then 16: θ i t ← θ i t-1 + ηm i t 17: else 18: θ i t ← θ i t-1 -ηm i t 19: end if 20: end for 21: until epochs end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Break Down SGD</head><p>Stochastic Gradient Descent (SGD) is faster than adaptive gradient methods primarily due to its simplicity. The key difference between SGD and these adaptive methods is that SGD uses a fixed learning rate, while adaptive methods adjust the learning rate dynamically for each parameter at each step. This adjustment can be done at the element level, as seen in Adam(W), or at the block level, like in Adammini.</p><p>The advantages of SGD can be summarized as follows: a. Runtime efficiency: It offers a fast and efficient iteration time for each optimization step. b. Memory efficiency: When using momentum, SGD requires only one instance of the gradients 2 and incurs no additional memory overhead when momentum is not applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Break Down Adam(W)</head><p>The main difference between Adam and AdamW lies in how they apply weight decay. AdamW applies a direct penalty to the weights themselves, which is known as Decoupled Weight Decay, whereas Adam applies the penalty to the gradients at the outset, utilizing L2 Regularization. Both algorithms enhance adaptability by scaling the learning rate for each parameter individually. In every optimization step, the scaling ratios are recalculated by dividing the first-order moment by the square root of the second-order moment, both of which are maintained and updated in state tensors.</p><p>As illustrated in Alog. 3 (line 17 for Adam) and Alog. 4 (line Algorithm 3 Adam Require: t (step), η (lr), θ i (i-th params), L(θ) (loss function), λ (weight decay), β 1 , β 2 (betas), maximize 1: repeat 2: for t ← 1 do 3: if maximize then 4: g i t ← -∇L(θ i t-1 ) 5: else 6: g i t ← ∇L(θ i t-1 ) 7: end if 8: 9: /* do weight decay */ 10: g i t ← g i t + λθ i t-1 11: 12: /* do momentum */ 13: if t &gt; 1 then 14:</p><formula xml:id="formula_33">m i t ← β 1 m i t-1 + (1 -β 1 )g i t 15: v i t ← β 2 v i t-1 + (1 -β 2 )(g i t ) 2 16:</formula><p>else 17:</p><formula xml:id="formula_34">m i t ← (1 -β 1 )g i t ; v i t ← (1 -β 2 )(g i t ) 2 18: end if 19: mi t ← m i t 1-β t 1 ; vi t ← v i t 1-β t 2 20: 21: θ i t ← θ i t-1 -η vi t +ϵ m i t 22:</formula><p>end for 23: until epochs end 14 for AdamW), the first-order moment m and the secondorder moment v are stored in GPU memory throughout the entire training process.</p><p>Generally, the estimated minimum memory requirement for the state tensors in both Adam and AdamW is approximately twice the size of the gradient tensors. This is because m and v share the same shape as the corresponding gradient tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Break Down Adam-mini</head><p>Adam-mini is a variant of the Adam optimizer. As shown in the Alog. 5, Adam-mini redesigns the adaptive update rules by using the mean of the squared gradients instead of the original squared gradients in most layers except for the embedding layer. This version of Adam reduces the number of learning rates to the number of blocks in each layer while keeping the update rules unchanged in the embedding layers. Therefore, the reduction in memory usage is influenced by the proportion of non-embedding parameters in the model. This limitation not only restricts the potential for memory savings but also incurs additional computational costs due to the extra operations (see Alog. 5 Line 15, 16) needed Algorithm 4 AdamW Require: t (step), η (lr), θ i (i-th params), L(θ) (loss function), λ (weight decay), β 1 , β 2 (betas), maximize 1: repeat</p><p>2: for t ← 1 do 3: if maximize then 4: g i t ← -∇L(θ i t-1 ) 5: else 6: g i t ← ∇L(θ i t-1 ) 7: end if 8: /* do momentum */ 9: if t &gt; 1 then 10: m i t ← β 1 m i t-1 + (1 -β 1 )g i t 11: v i t ← β 2 v i t-1 + (1 -β 2 )(g i t ) 2 12: else 13: m i t ← (1 -β 1 )g i t ; v i t ← (1 -β 2 )(g i t ) 2 14: end if 15: mi t ← m i t 1-β t 1 ; vi t ← v i t 1-β t 2 16: 17: /* do weight decay */ 18: θ i t ← θ i t-1 -ληθ i t-1 19: 20: θ i t ← θ i t-1 -η vi t +ϵ m i t 21:</p><p>end for 22: until epochs end when calculating the new v compared to the original Adam algorithm.</p><p>Regarding the lower boundary of the state tensor memory, as mentioned in <ref type="bibr">(Zhang et al., 2024b)</ref>, Adam-mini can reduce the memory used for the Adam optimizer's v by at least 90%. This results in a memory cost savings of approximately 45% to 50% compared to the original Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Break Down Prodigy</head><p>Prodigy is one of the most popular variants of the Adam optimizer, offering a new approach to calculating the step size. It alleviates the need for extensive learning rate tuning. While most of Adam's update rules remain unchanged, Prodigy introduces a new scaling ratio, denoted as d (for D-Adaption), which adaptively adjusts the learning rate. To update the scaling ratio d for each optimization step, Prodigy requires the maintenance of two additional tensors: the initial weight value x 0 and the denominator s, both of which share the same shape as the gradient.</p><p>As a result, the lower boundary for estimating the memory required by Prodigy's state tensor is approximately four times the size of the gradient by default. The majority of Algorithm 5 Adam-mini Require: t (step), η (lr), θ i (i-th params), L(θ) (loss function), λ (weight decay), β 1 , β 2 (betas), maximize 1: repeat</p><p>2: for t ← 1 do 3: if maximize then 4: g i t ← -∇L(θ i t-1 ) 5: else 6: g i t ← ∇L(θ i t-1 ) 7: end if 8: 9: /* do momentum */ 10: if t &gt; 1 then 11: m i t ← β 1 m i t-1 + (1 -β 1 )g i t 12: if θ i t-1 ∈ embedding layer then 13: v i t ← β 2 v i t-1 + (1 -β 2 )(g i t ) 2 14: else 15: Divide θ i t-1 into Q,K heads if needed. 16: v i t ← β 2 v i t-1 + (1 -β 2 )M ean((g i t ) 2 ) 17: end if 18: else 19: m i t ← (1 -β 1 )g i t ; v i t ← (1 -β 2 )(g i t ) 2 20: end if 21: mi t ← m i t 1-β t 1 ; vi t ← v i t 1-β t 2 22: 23: /* do weight decay */ 24: θ i t ← θ i t-1 -ληθ i t-1 25: 26: θ i t ← θ i t-1 -η vi t +ϵ m i t 27:</p><p>end for 28: until epochs end the memory for the tensor state is occupied by four tensors: m, v, x 0 , s <ref type="bibr">(Algo. 6 Line 11,</ref><ref type="bibr">12,</ref><ref type="bibr">17,</ref><ref type="bibr">18)</ref>. Consequently, Prodigy can be very memory-intensive when applied to large models with billions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Profiling Results on "g-SNR Calculation" Stage</head><p>As discussed in Section B, our method requires calculating the scale ratio during the initial step of the optimization process, which we refer to as the "g-SNR Calculation" stage. This procedure may take extra time because it involves calculating both the gradient norm and its standard deviation. As shown in</p><p>Table 5 and Table 6, although square root operations are generally considered more computationally intensive than standard float addition and multiplication, the time taken for the "g-SNR Calculation" is still relatively small. Since this calculation is performed only once during Algorithm 6 Prodigy Require: t (step), η (lr, default 1 with cosine annealing), θ i (i-th params), L(θ) (loss function), λ (weight decay), β 1 , β 2 (betas), maximize, d 0 &gt; 0 (default 1e -6 ), x 0 1: repeat 2: for t ← 1 do 3: if maximize then 4: g i t ← -∇L(θ i t-1 ) 5: else 6: g i t ← ∇L(θ i t-1 ) 7: end if 8: 9: /* do momentum */ 10: if t &gt; 1 then 11: m i t ← β 1 m i t-1 + (1 -β 1 )d t g i t 12: v i t ← β 2 v i t-1 + (1 -β 2 )d 2 t (g i t ) 2 13: else 14: m i t ← (1 -β 1 )d t g i t ; v i t ← (1 -β 2 )d 2 t (g i t ) 2 15: r t-1 = 0; s t-1 = 0 16: end if 17: r t = √ β 2 r t-1 + (1 -√ β 2 )ηd 2 t ⟨g i t , x 0 -x t ⟩ 18: s t = √ β 2 s t-1 + (1 -√ β 2 )ηd 2 t g i t 19: dt+1 = r k ∥st∥1 20: d t+1 = max(d k , dt+1 ) 21: 22: /* do weight decay */ 23: θ i t ← θ i t-1 -ληθ i t-1 24: 25: θ i t ← θ i t-1 -ηdt √ v i t +dtϵ m i t 26:</p><p>end for 27: until epochs end each training procedure by design, its duration is negligible compared to the overall training iterations. Therefore, our method remains efficient for optimization in the long run.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The chart illustrates how memory usage and optimizer step time (in wall-clock time) increase with larger model sizes. It highlights the substantial memory overhead of storing optimizer states as model sizes grow. SGD-SaI exhibits significantly lower memory usage than AdamW and has the shortest optimization step runtime. This runtime refers to the wall clock time required for the optimizer step function. All statistics were measured on a single NVIDIA A100-80GB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. This graph illustrates the differences in local gain behaviours exhibited by four optimizers throughout the training process. We present two popular adaptive gradient methods: Adam(W) and the memory-efficient Adam-mini. The local gains for these methods are recalculated continuously at each step based on the gradients. In contrast, SGD and SGD-SaI are both non-adaptive methods, meaning their local gains remain fixed throughout the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. We plot the g-SNR distribution over time for three different transformer blocks: shallow (block 0), middle (block 5), and deep (block 11). Additionally, we analyze some distinct types of parameter blocks. Our observations indicate that while the g-SNR values vary across different parameter blocks, they tend to remain relatively constant over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of top-1 test accuracy distributions for CNNs on CIFAR-10 (Left) and ViTs on ImageNet-1k (Right) across different hyperparameter combinations. Each method demonstrates distinct performance trends, including Adam, AdamW, SGD, and SGD-SaI. Adam-Mini is only compared in the ViT case as its modification target on transformer training. SGD-SaI consistently shows enhanced robustness and performance under varying hyperparameter settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure7. This figure displays the training and evaluation loss and accuracy of the ViT on ImageNet1k). Although our method has a slower convergence speed, we can still achieve comparable performance by the end of the training process. Additionally, our approach is designed to have a lower memory footprint and a faster optimization speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. These figures show the accuracy distributions of eleven architectures trained on different optimizers using the same hyperparameter candidates. This row presents the top-1 evaluation accuracy distributions on CIFAR-10, CIFAR-100 and ImageNet16-120. The curves in those histograms are the results of kernel density estimation (KDE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>The efficiency metrics of various models with different optimizers were evaluated using an A100-80GB GPU. The table above summarizes the results, which include the tensor memory usage and wall-clock time (optimization step time measured in milliseconds) for each model-optimizer configuration. For the large language models (LLMs), experiments were conducted with a context length of 1024 and a batch size of 1. All models were profiled in full (FP32) precision.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="2">State Mem (GB) Wall Time (ms)</cell></row><row><cell></cell><cell>SGDM</cell><cell>5.93</cell><cell>41.0 ± 12.0</cell></row><row><cell></cell><cell>AdamW</cell><cell>11.86</cell><cell>138.0 ± 6.0</cell></row><row><cell>GPT2-1.5B</cell><cell>Adam Prodigy</cell><cell>11.86 23.72</cell><cell>145.0 ± 7.0 360.0 ± 45.0</cell></row><row><cell></cell><cell>Adam-Mini</cell><cell>6.52</cell><cell>223.0 ± 2.0</cell></row><row><cell></cell><cell cols="2">SGD-SaI(ours) 5.93</cell><cell>68.0 ± 21.0</cell></row><row><cell></cell><cell>SGDM</cell><cell>25.15</cell><cell>100.0 ± 20.0</cell></row><row><cell></cell><cell>AdamW</cell><cell>49.48</cell><cell>OOM</cell></row><row><cell>Llama2-7B</cell><cell>Adam Prodigy</cell><cell>49.48 98.96</cell><cell>OOM OOM</cell></row><row><cell></cell><cell>Adam-Mini</cell><cell>27.21</cell><cell>421.0 ± 22.0</cell></row><row><cell></cell><cell cols="2">SGD-SaI(ours) 25.15</cell><cell>180.0 ± 30.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The PyTorch implementation is available at https:// github.com/AnonymousAlethiometer/SGD_SaI/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithm Overview</head><p>In this section, we will provide an overview of how SGD-SaI operates. As a non-adaptive gradient method, SGD-SaI calculates the preconditioned scaling factor based on the g-SNR values before applying the first batch of data in the optimization step. These scaling factors vary across different partitions, as they are closely linked to the architectures being utilized. However, once established, they remain constant throughout the entire training process.</p><p>While Adam-mini is memory-efficient, its complex partition rules and repetitive local gain recalculation result in significant computational costs. The improvement in throughput Model Method Iter Times (ms) g-SNR Calc (ms)</p><p>ViT-S/16 SGDM 12.2 ± 2.9 0 SGD-SaI (ours) 13.7 ± 3.8 14.5</p><p>ViT-H/14 SGDM 48.6 ± 7.8 0 SGD-SaI (ours) 65.5 ± 10.0 43.3 GPT2-1.5B SGDM 287.4 ± 0.9 0 SGD-SaI (ours) 340.1 ± 1.1 267.6</p><p>Table <ref type="table">5</ref>. RTX 3090 Profile Results. The results here are based on a single NVIDIA GeForce RTX 3090 GPU. The trials were conducted over 20 iterations, recording the time taken for each optimization step, which is referred to as the "iteration time" column. We compared the time taken for the g-SNR calculation stage and found that it takes an equal amount of time or less than an optimization step. However, since this calculation is only performed once, it is considered tolerable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Method</head><p>Iter Times (ms) g-SNR Calc (ms)</p><p>ViT-S/16 SGDM 7.1 ± 0.0 0 SGD-SaI (ours) 13.0 ± 0.1 13.7</p><p>ViT-H/14 SGDM 51.9 ± 10.7 0 SGD-SaI (ours) 63.1 ± 7.8 106.0 GPT2-1.5B SGDM 353.2 ± 0.9 0 SGD-SaI (ours) 392.8 ± 0.2 353.8</p><p>Table <ref type="table">6</ref>. A100 PCIe 40GB Profile Results. It follows the same setting as Table <ref type="table">5</ref>, expect for the GPU type.</p><p>compared to Adam(W) is primarily due to the ability to reduce memory usage, allowing for larger batch sizes and enabling more data to be processed in parallel. In contrast, we not only reduce the memory footprint but also eliminate the entire adaptive local gain calculation, achieving a significant breakthrough in both memory and computational efficiency.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-cost proxies for lightweight nas</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Compressed optimisation for nonconvex problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Signsgd</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.04434" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Better plain vit baselines for imagenet-1k, 2022a</title>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.01580" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolesnikov</forename></persName>
		</author>
		<ptr target="https://github.com/google-research/big_vision" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marculescu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.00780" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The case for 4-bit precision: k-bit inference scaling laws</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7750" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">8-bit optimizers via block-wise quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2110.02861</idno>
		<ptr target="https://arxiv.org/abs/2110.02861" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Llm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>int8 (</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NATS-Bench: Benchmarking nas algorithms for architecture topology and size</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Musial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3054824</idno>
		<idno>doi:10.1109/TPAMI.2021. 3054824</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incorporating Nesterov Momentum into Adam</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v12/duchi11a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">61</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08576</idno>
		<title level="m">Pruning neural networks at initialization: Why are we missing the mark? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving robustness with adaptive weight decay</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ardekani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.00094" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adaptive gradient methods at the edge of stability</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cardoze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Medapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2207.14484" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Openwebtext corpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1308.0850" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixof-show: Decentralized low-rank adaptation for multiconcept customization of diffusion models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cited on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/˜kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Toronto, Ontario</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 0</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On weight initialization in deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08863</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lavington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13960</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Snip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02340</idno>
		<title level="m">Single-shot network pruning based on connection sensitivity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Balance is essence: Accelerating sparse training via adaptive gradient correction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Mallick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2301.03573" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Memory efficient optimizers with 4-bit states</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2309.01507" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot nas via inverse coefficient of variation on gradients</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName><surname>Zico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convergence of adam under relaxed assumptions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Prodigy: An expeditiously adaptive parameter-free learner</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.06101</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o(1/k 2 )</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
		<ptr target="https://zbmath.org/?q=an:0535.90071" />
	</analytic>
	<monogr>
		<title level="j">Doklady Akademii Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983">1983</date>
			<pubPlace>MathSciNet</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The e2e dataset: New challenges for end-to-end generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09254</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.10752" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><surname>Adafactor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.10270" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6377" to="6389" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adagrad stepsizes: Sharp convergence over nonconvex landscapes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">219</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting network compressibility and topology in zerocost nas</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Conference 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.14881" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">An adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Adadelta</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Riemannian preconditioned lora for fine-tuning foundation models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pilanci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02347</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16788</idno>
		<title level="m">Why transformers need adam: A hessian perspective</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Adam-Mini</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.16793" />
		<title level="m">Use fewer learning rates to gain more, 2024b</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
