- Decision to challenge the necessity of adaptive gradient methods
- Choice of using learning rate Scaling at Initialization (SaI)
- Selection of gradient signal-to-noise ratio (g-SNR) as a guiding metric
- Decision to focus on memory efficiency in optimizer design
- Choice of SGD-SaI as the proposed optimization method
- Decision to empirically analyze g-SNR characteristics during training
- Choice of parameter grouping for learning rate scaling
- Decision to compare SGD-SaI against AdamW and other state-of-the-art optimizers
- Choice of tasks for empirical validation (e.g., ImageNet-1K, LoRA fine-tuning)
- Decision to utilize Transformer architectures for testing
- Choice of full-precision training settings for memory usage analysis
- Decision to document performance improvements over existing optimizers
- Choice of specific hyperparameter settings for experiments
- Decision to analyze the impact of architectural differences on learning dynamics
- Choice to explore robustness across diverse applications and tasks
- Decision to extend analysis to diffusion models and traditional CNN tasks
- Choice of empirical methods for measuring training progress and stability
- Decision to document trade-offs between memory usage and performance
- Choice to investigate the implications of gradient patterns at initialization
- Decision to leverage insights from previous works on adaptive methods and SGD