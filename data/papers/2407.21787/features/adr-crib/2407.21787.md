The decisions made by the researchers in the study "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling" are grounded in a combination of empirical observations, theoretical considerations, and practical constraints. Below is a detailed technical explanation and rationale for each of the specified decisions:

### 1. Decision to Explore Repeated Sampling as a Method for Scaling Inference Compute
The researchers aimed to investigate whether increasing the number of samples generated during inference could enhance the performance of language models. This approach is motivated by the observation that many problems can be solved with multiple attempts, and that the diversity of generated solutions can lead to higher coverage of correct answers. By exploring repeated sampling, the researchers sought to leverage the inherent probabilistic nature of language models to improve their effectiveness in solving complex tasks.

### 2. Choice of Tasks for Evaluation (GSM8K, MATH, MiniF2F-MATH, CodeContests, SWE-bench Lite)
The selected tasks represent a diverse set of challenges that span different domains, including mathematics, coding, and real-world problem-solving. Each task was chosen for its unique characteristics:
- **GSM8K and MATH**: These datasets contain math word problems of varying difficulty, allowing for an assessment of the models' reasoning capabilities.
- **MiniF2F-MATH**: This task involves formalized mathematics problems, which can be verified automatically, providing a clear metric for success.
- **CodeContests**: This dataset includes competitive programming problems that require coding solutions, allowing for evaluation in a programming context.
- **SWE-bench Lite**: This dataset consists of real-world GitHub issues, providing a practical application for the models and testing their ability to handle real coding challenges.

### 3. Selection of Metrics for Success Rate and Coverage
The researchers focused on two key metrics: success rate and coverage. The success rate measures the fraction of problems solved correctly, while coverage quantifies the proportion of problems that can be solved by at least one generated sample. These metrics are critical for understanding the effectiveness of repeated sampling, as they provide insights into both the ability to generate correct solutions and the diversity of those solutions.

### 4. Use of Automatic Verifiers for Certain Tasks
Automatic verifiers were employed for tasks like MiniF2F-MATH, CodeContests, and SWE-bench Lite because they provide a reliable and efficient means of assessing the correctness of generated solutions. This allows the researchers to focus on the relationship between sampling and performance without the added complexity of manual verification, thereby streamlining the evaluation process.

### 5. Implementation of Positive Temperature Sampling for Candidate Generation
Positive temperature sampling was used to introduce randomness into the generation process, allowing for a wider variety of candidate solutions. This approach helps to avoid the pitfalls of deterministic outputs, which may lead to repetitive or suboptimal solutions. By adjusting the temperature, the researchers could control the trade-off between exploration (diversity of samples) and exploitation (quality of samples).

### 6. Decision to Model the Relationship Between Coverage and Number of Samples with an Exponentiated Power Law
The researchers observed that the relationship between coverage and the number of samples often followed a log-linear trend, which can be effectively modeled using an exponentiated power law. This modeling choice allows for a more nuanced understanding of how coverage scales with sampling, suggesting that there are underlying principles governing the performance of language models in inference tasks.

### 7. Choice of Verification Methods (Majority Voting, Reward Models) for Non-Automated Tasks
For tasks without automatic verifiers, the researchers selected majority voting and reward models as verification methods. These approaches were chosen because they provide a systematic way to aggregate the results of multiple samples and identify the most likely correct solution. However, the researchers also noted the limitations of these methods, particularly in terms of their ability to scale with increasing sample sizes.

### 8. Strategy for Balancing Model Size and Sample Count for Cost-Effectiveness
The researchers aimed to find an optimal balance between model size and the number of samples generated to maximize performance while minimizing costs. This strategy is crucial in practical applications, where computational resources and budget constraints are significant factors. By analyzing the performance of different models and sample counts, the researchers could identify configurations that yield the best results for specific tasks.

### 9. Decision to Use Specific Models (DeepSeek-Coder-V2-Instruct, Llama-3) for Experiments
The choice of models was based on their capabilities and relevance to the tasks at hand. DeepSeek-Coder-V2-Instruct was selected for its ability to handle coding tasks effectively, while Llama-3 models were chosen for their strong performance in reasoning and problem-solving. The researchers aimed to leverage the strengths of these models to explore the benefits of repeated sampling.

### 10. Approach to Data Collection and Unbiased Estimation for Coverage Calculation
The researchers employed a systematic approach to data collection, ensuring that the evaluation of coverage was unbiased. They utilized an unbiased estimation formula to calculate coverage metrics, which helps to reduce variance and provides a more