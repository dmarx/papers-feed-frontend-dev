<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</title>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder ref="#_QW7Mgn5">
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder ref="#_MEEfXuZ">
					<orgName type="full">Stanford HAI</orgName>
				</funder>
				<funder ref="#_8rh6ZeN">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Meta</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
					<orgName type="abbreviated">SDSI</orgName>
				</funder>
				<funder>
					<orgName type="full">Clarendon Fund Scholarships</orgName>
				</funder>
				<funder ref="#_9svbwgC #_tmdRVmh">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_bgXc98e #_ZbVhEd5">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">VMWare</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-30">30 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bradley</forename><surname>Brown</surname></persName>
							<email>bradley.brown@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford § Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jordan</forename><surname>Juravsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Ehrlich</surname></persName>
							<email>ryanehrlich@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
							<email>ronald.clark@cs.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford § Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le §</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
							<email>azalia@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-30">30 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">328F14678269209099DADEA08AE21563</idno>
					<idno type="arXiv">arXiv:2407.21787v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -the fraction of problems that are solved by any generated sample -scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability of large language models (LLMs) to solve coding, mathematics, and other reasoning tasks has improved dramatically over the past several years <ref type="bibr" target="#b52">[47,</ref><ref type="bibr" target="#b16">11,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b10">4]</ref>. Scaling the amount of training compute through bigger models, longer pre-training runs, and larger datasets has been a consistent driver of these gains <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b42">37,</ref><ref type="bibr" target="#b33">28]</ref>.</p><p>In contrast, a comparatively limited investment has been made in scaling the amount of computation used during inference. Larger models do require more inference compute than smaller ones, and prompting techniques like chain-of-thought <ref type="bibr" target="#b66">[61]</ref> can increase answer quality at the cost of longer (and therefore more computationally expensive) outputs. However, when interacting with LLMs, users and developers often restrict models to making only one attempt when solving a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x = int(input()) …</head><p>Problem: Input a number from stdin and …</p><p>Step 1: Generate many candidate solutions.</p><p>Step 2: Use a verifier to pick a final answer. The repeated sampling procedure that we follow in this paper. 1) We generate many independent candidate solutions for a given problem by sampling from an LLM with a positive temperature. 2) We use a domain-specific verifier (ex. unit tests for code) to select a final answer from the generated samples.</p><p>In this work, we explore repeated sampling (Figure <ref type="figure">1</ref>) as a simple approach to scaling inference compute in order to improve reasoning performance. Existing work provides encouraging examples that repeated sampling can be beneficial in math, coding, and puzzle-solving settings <ref type="bibr" target="#b65">[60,</ref><ref type="bibr" target="#b53">48,</ref><ref type="bibr" target="#b28">23]</ref>. Notably, AlphaCode <ref type="bibr" target="#b46">[41]</ref>, a state-of-the-art system for competitive programming, finds that performance continues to improve with a million samples per problem. Our goal is to systematically characterize these benefits across a range of tasks, models, and sample budgets.</p><p>The effectiveness of repeated sampling is determined by two key properties:</p><p>1. Coverage: As the number of samples increases, what fraction of problems can we solve using any sample that was generated?</p><p>2. Precision: How often can we identify correct samples from our collection of generations?</p><p>Both properties are needed for achieving strong real-world performance. With unlimited samples, any model that assigns a non-zero probability to every sequence will achieve perfect coverage. However, repeated sampling is only practical if we can improve coverage with a feasible budget. Similarly, generating large sample collections is only useful if the correct samples in a collection can be identified. The difficulty of the precision problem can vary by task. In some settings, existing tools like proof checkers and unit tests can automatically verify every sample. In other cases, like when solving word problems, other methods for verification are needed.</p><p>Exploring coverage first, we find that sampling up to 10,000 times per problem can significantly boost coverage on math and coding tasks (Section 2). When solving CodeContests <ref type="bibr" target="#b46">[41]</ref> programming problems using Gemma-2B <ref type="bibr" target="#b57">[52]</ref>, we increase coverage by over 300x, from 0.02% with one sample to 7.1% with 10,000 samples. Interestingly, the relationship between log(coverage) and the number of samples often follows an approximate power law (Section 3). With Llama-3 <ref type="bibr" target="#b9">[3]</ref> and Gemma models, this leads to coverage growing nearly log-linearly with the number of samples over several orders of magnitude.</p><p>In settings with automatic verification tools, increases in coverage translate directly into improved task performance. When applying repeated sampling to competitive programming and writing Lean proofs, models like Llama-3-8B-Instruct can exceed the single-sample performance of much stronger ones like . This ability to amplify weaker models extends to the challenging SWE-bench Lite dataset of real-life GitHub issues <ref type="bibr" target="#b37">[32]</ref>, where the current single-sample state-of-theart (SOTA), achieved by a mixture of GPT-4o and Claude 3.5 Sonnet, is 43% <ref type="bibr" target="#b8">[1]</ref>. When restricted to a single sample, DeepSeek-Coder-V2-Instruct <ref type="bibr" target="#b25">[20]</ref> solves only 15.9% of issues. By simply increasing the number of samples to 250, we increase the fraction of solved issues to 56%, exceeding the state-of-the-art by 13%.</p><p>In addition to improving model quality, repeated sampling provides a new mechanism for minimizing LLM inference costs (Section 2.3). When holding the total number of inference FLOPs constant, we find that on some datasets (e.g. MATH), coverage is maximized with a smaller model and more samples, while on others (e.g CodeContests) it is better to sample fewer times from a larger model. We also compare API prices between DeepSeek-Coder-V2-Instruct, GPT-4o, and Claude Sonnet 3.5 in the context of solving SWE-bench Lite issues. When keeping the agent framework (Moatless Tools <ref type="bibr" target="#b72">[67]</ref>) constant, sampling five times from the weaker and cheaper DeepSeek model solves more issues than single samples from Claude or GPT while also being over 3x cheaper.</p><p>Finally, we demonstrate that scalable verification is necessary for fully benefiting from repeated sampling. As the number of samples increases, coverage improves through models generating correct solutions to problems they have not previously solved. However, these increasingly rare correct generations are only beneficial if verifiers can "find the needle in the haystack" and identify them from collections of mostly-incorrect samples. In math word problem settings, we find that two common methods for verification (majority voting and reward models) do not possess this ability. When solving MATH <ref type="bibr" target="#b31">[26]</ref> problems with Llama-3-8B-Instruct, coverage increases from 82.9% with 100 samples to 98.44% with 10,000 samples. However, when using majority voting or reward models to select final answers, the biggest performance increase is only from 40.50% to 41.41% over the same sample range. As the number of samples increases, the gap between coverage (i.e. performance with a perfect verifier) and the performance of these methods increases as well (Figure <ref type="figure" target="#fig_8">7</ref>).</p><p>In summary, our primary observations are:</p><p>1. We demonstrate that scaling inference compute through repeated sampling leads to large improvements in coverage across a variety of tasks and models. This makes it possible, and sometimes cost-effective, to amplify weaker models with many samples and outperform single samples from more capable models.</p><p>2. We show that the relationship between coverage and the number of samples can often be modelled using an exponentiated power law, suggesting a form of scaling laws for inference-time compute.</p><p>3. In domains without automatic verifiers, we show that common approaches to verification plateau beyond approximately 100 samples. This leads to a growing gap between the performance achieved with these methods and the coverage upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scaling Repeated Sampling</head><p>We focus on pass-fail tasks where a candidate solution can be scored as right or wrong. The primary metric of interest for these tasks is the success rate: the fraction of problems that we are able to solve. With repeated sampling, we consider a setup where a model can generate many candidate solutions while attempting to solve a problem. The success rate is therefore influenced both by the ability to generate correct samples for many problems (i.e. coverage), as well as the ability to identify these correct samples (i.e. precision).</p><p>The difficulty of the precision problem depends on the availability of tools for sample verification. When proving formal statements in Lean, proof checkers can quickly identify whether a candidate solution is correct. Similarly, unit tests can be used to verify candidate solutions to coding tasks. In these cases, precision is handled automatically, and improving coverage directly translates into higher success rates. In contrast, the tools available for verifying solutions to math word problems from GSM8K and MATH are limited, necessitating additional verification methods that decide on a single final answer from many (often conflicting) samples.</p><p>We consider the following five tasks:</p><p>1. GSM8K: A dataset of grade-school level math word problems <ref type="bibr" target="#b23">[18]</ref>. We evaluate on a random subset of 128 problems from the GSM8K test set.</p><p>2. MATH: Another dataset of math word problems that are generally harder than those from GSM8K <ref type="bibr" target="#b18">[13]</ref>. Similarly, we evaluate on 128 random problems from this dataset's test set.</p><p>3. MiniF2F-MATH: A dataset of mathematics problems that have been formalized into proof checking languages <ref type="bibr" target="#b70">[65]</ref>. We use Lean4 as our language, and evaluate on the 130 test set problems that are formalized from the MATH dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>CodeContests: A dataset of competitive programming problems <ref type="bibr" target="#b46">[41]</ref>. Each problem has a text description, along with a set of input-output test cases (hidden from the model) that can be used to verify the correctness of a candidate solution. We enforce that models write their solutions using Python3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SWE-bench Lite:</head><p>A dataset of real world Github issues, where each problem consists of a description and a snapshot of a code repository <ref type="bibr" target="#b37">[32]</ref>. To solve a problem, models must edit files in the codebase (in the Lite subset of SWE-bench that we use, only a single file needs to be changed). Candidate solutions can be automatically checked using the repository's suite of unit tests.</p><p>Among these tasks, MiniF2F-MATH, CodeContests, and SWE-bench Lite have automatic verifiers (in the form of the Lean4 proof checker, test cases, and unit test suites, respectively). We begin by investigating how repeated sampling improves model coverage. Coverage improvements correspond directly with increased success rates for tasks with automatic verifiers and in the general case provide an upper bound on the success rate. In coding settings, our definition of coverage is equivalent to the commonly-used pass@k metric <ref type="bibr" target="#b20">[15]</ref>, where k denotes the number of samples per problem. We use this metric directly when evaluating on CodeContests and SWE-bench Lite. For MiniF2F the metric is similar, with a "pass" defined according to the Lean4 proof checker. For GSM8K and MATH, coverage corresponds to using an oracle verifier that checks if any sample "passes" by outputting the correct final answer. To reduce the variance when calculating coverage, we adopt the unbiased estimation formula from Chen et al. <ref type="bibr" target="#b20">[15]</ref>. In each experiment, we first generate N samples for each problem index i and calculate the number of correct samples C i . We then calculate the pass@k scores at each k ≤ N of interest according to:</p><formula xml:id="formula_0">pass@k = 1 # of problems # of problems i=1 1 - N -C i k N k<label>(1)</label></formula><p>We use the numerically stable implementation of the above formula suggested in Chen et al. <ref type="bibr" target="#b20">[15]</ref>. Data and code is available at <ref type="url" target="https://scalingintelligence.stanford.edu/pubs/large_language_monkeys/">https://scalingintelligence.stanford.edu/pubs/large_ language_monkeys/</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Repeated Sampling is Effective Across Tasks</head><p>Here, we establish that repeated sampling improves coverage across multiple tasks and a range of sample budgets. We evaluate Llama-3-8B-Instruct and Llama-3-70B-Instruct on CodeContests, MiniF2F, GSM8K, and MATH, generating 10,000 independent samples per problem. For SWEbench Lite, we use DeepSeek-Coder-V2-Instruct <ref type="bibr" target="#b25">[20]</ref>, as the required context length of this task exceeds the limits of the Llama-3 models. As is standard when solving SWE-bench issues, we equip Llama-3-8B-Instruct Llama-3-70B-Instruct Single-Attempt GPT-4o</p><p>Figure <ref type="figure">2</ref>: Across five tasks, we find that coverage (the fraction of problems solved by at least one generated sample) increases as we scale the number of samples. Notably, using repeated sampling, we are able to increase the solve rate of an open-source method from 15.9% to 56% on SWE-bench Lite.</p><p>our LLM with a software framework that provides the model with tools for navigating through and editing codebases. In our work, we use the open-source Moatless Tools library <ref type="bibr" target="#b72">[67]</ref>. Note that solving a SWE-bench issue involves a back-and-forth exchange between the LLM and Moatless Tools. One sample/attempt for this benchmark refers to one entire multi-turn trajectory. To minimize costs, we restrict the number of attempts per issue to 250, with all attempts made independently of one another.</p><p>We report our results in Figure <ref type="figure">2</ref>. We also include the single-attempt performance of GPT-4o on each task, as well the single-attempt state-of-the-art for SWE-bench Lite (CodeStory Aide <ref type="bibr" target="#b8">[1]</ref> which uses a combination of GPT-4o and Claude 3.5 Sonnet). Across all five tasks, we find that coverage smoothly improves as the sample budget increases. When all LLMs are given a single attempt, GPT-4o outperforms the Llama and DeepSeek models at every task. However, as the number of samples increases, all three of the weaker models exceed GPT-4o's single-attempt performance. In the case of SWE-bench Lite, we solve 56% of problems, exceeding the single-attempt SOTA of 43%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Repeated Sampling is Effective Across Model Sizes and Families</head><p>The results from Section 2.1 indicate that repeated sampling improves coverage. However, we only show this trend for three recent, instruction-tuned models with 8B or more parameters. We now show that these trends hold across other model sizes, families, and levels of post-training. We expand our evaluation to include a broader set of models:</p><formula xml:id="formula_1">• Llama 3: Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B-Instruct.</formula><p>• Gemma: Gemma-2B, Gemma-7B <ref type="bibr" target="#b57">[52]</ref>.</p><p>• Pythia: Pythia-70M through Pythia-12B (eight models in total) <ref type="bibr" target="#b14">[9]</ref>.</p><p>We restrict evaluation to the MATH and CodeContests datasets to minimize inference costs, reporting results in Figure <ref type="figure" target="#fig_2">3</ref>. Coverage increases across almost every model we test, with smaller models showing some of the sharpest increases in coverage when repeated sampling is applied. On CodeContests, the coverage of Gemma-2B increases by over 300x, from a pass@1 of 0.02% to a pass@10k of 7.1%. Similarly, when solving MATH problems with Pythia-160M, coverage increases from a pass@1 of 0.27% to a pass@10k of 57%. The exception to this pattern of increasing coverage across models is with the Pythia family evaluated on CodeContests. All Pythia models achieve zero coverage on this dataset, even with a budget of 10,000 samples. We speculate that this due to Pythia being trained on less coding-specific data than Llama and Gemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Repeated Sampling Can Help Balance Performance and Cost</head><p>One takeaway from the results in Sections 2.1 and 2.2 is that repeated sampling makes it possible to amplify a weaker model's capabilities and outperform single samples from stronger models. Here, we demonstrate that this amplification can be more cost-effective than using a stronger, more expensive model, providing practitioners with a new degree of freedom when trying to jointly optimize performance and costs.</p><p>We first consider FLOPs as a cost metric, examining the Llama-3 results from Section 2.1. We re-plot our results from Figure <ref type="figure">2</ref>, now visualizing coverage as a function of total inference FLOPs instead of the sample budget. Since Llama-3 models are dense transformers where the majority of parameters are used in matrix multiplications, we approximate inference FLOPs with the formula: We present our re-scaled results for MiniF2F, CodeContests, MATH, and GSM8K in Figure <ref type="figure">4</ref>. Interestingly, the model that maximizes coverage varies with the compute budget and task. On MiniF2F, GSM8K and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GSM8K (Oracle Verifier)</head><p>Llama-3-8B-Instruct Llama-3-70B-Instruct MATH, Llama-3-8B-Instruct always obtains higher coverage than the larger (and more expensive) 70B model when the FLOP budget is fixed. However for CodeContests, the 70B model is almost always more cost effective. We note that examining FLOPs alone can be a crude cost metric that ignores other aspects of system efficiency <ref type="bibr" target="#b26">[21]</ref>. In particular, repeated sampling can make use of high batch sizes and specialized optimizations that improve system throughput relative to single-attempt inference workloads <ref type="bibr" target="#b39">[34,</ref><ref type="bibr" target="#b11">6,</ref><ref type="bibr" target="#b71">66]</ref>. We discuss this in more detail in Section 5. We also examine the dollar costs of repeated sampling when solving SWE-bench Lite issues using current API pricing. Keeping the agent framework (Moatless Tools) constant, we consider making a single attempt per issue with Claude 3.5 Sonnet and GPT-4o, as well as repeated sampling using DeepSeek-Coder-V2-Instruct. We report the average cost per issue and issue resolution rate with each approach in Table <ref type="table" target="#tab_0">1</ref>. While the DeepSeek model is weaker than the GPT and Claude models, it is also over 10x cheaper. In this case, repeated sampling provides a cheaper alternative to paying a premium for access to strong models while achieving a superior issue solve rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Characterizing the Benefits of Repeated Sampling</head><p>The relationship between an LLM's loss and its training compute has been well-characterized with training scaling laws <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b41">36,</ref><ref type="bibr" target="#b33">28]</ref>. These laws have empirically held over many orders of magnitude and inspire confidence in model developers that large investments in training will pay off. Inspired by training scaling laws, here we aim to better characterize the relationship between coverage and the sample budget (i.e. the amount of inference compute), presenting two interesting observations: Figure <ref type="figure">5</ref>: The relationship between coverage and the number of samples can be modelled with an exponentiated power law for most tasks and models. We highlight that some curves, such as Llama-3-8B-Instruct on MiniF2F-MATH, do not follow this trend closely. We show the mean and standard deviation of the error between the coverage curve and the power law fit across 100 evenly sampled points on the log scale.</p><p>2. For a given task, the coverage curves of different models from the same family resemble S-curves with similar slopes but distinct horizontal offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scaling Laws for Repeated Sampling</head><p>Here, we develop an explicit model for the relationship between coverage and the number of samples. The GPT-4 technical report <ref type="bibr" target="#b51">[46]</ref> finds that the relationship between a model's mean-log-pass-rate on coding problems and its training compute can be modelled well using a power law. We start by adopting the same function class, but now modelling the log of coverage c as a function of the number of samples k:</p><formula xml:id="formula_2">log(c) ≈ ak b (2)</formula><p>where a, b ∈ R are fitted model parameters. In order to directly predict coverage, we exponentiate both sides, ending up with the final model of:</p><formula xml:id="formula_3">c ≈ exp(ak b )<label>(3)</label></formula><p>We provide examples of fitted coverage curves in Figure <ref type="figure">5</ref>, and additional curves in Appendix C.2. While these laws are not as exact as training scaling laws (most strikingly on MiniF2F-MATH), they provide encouraging early evidence that the benefits of inference scaling can be characterized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarities in Coverage Curves Across Models</head><p>Interestingly, when comparing the coverage curves (with a logarithmic x-axis) of different models from the same family on the same task (see Figure <ref type="figure" target="#fig_2">3</ref>), it appears that the traced S-curves have the same slope, but unique horizontal offsets. To investigate this further, we overlay the coverage curves of different models from the same family in Figure <ref type="figure">6</ref>. We do this by picking an anchor coverage value c, and shifting every curve leftward (in log-space) so that each passes through the point <ref type="bibr">(1, c)</ref>. This corresponds to a leftward shift by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pythia MATH (Oracle Verifier)</head><p>Pythia-1.4B Pythia-12B Pythia-160M Pythia-1B Pythia-2.8B Pythia-410M Pythia-6.9B Pythia-70M </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Llama-3 CodeContests</head><p>Llama-3-70B-Instruct Llama-3-8B-Instruct Llama-3-8B Figure <ref type="figure">6</ref>: Overlaying the coverage curves from different models belonging to the same family. We perform this overlay by horizontally shifting every curve (with a logarithmic x-axis) so that all curves pass through the point (1, c). We pick c to be the maximum pass@1 score over all models in the plot. We note that the similarity of the curves post-shifting shows that, within a model family, sampling scaling curves follow a similar shape. log(pass@k -1 (c)), where pass@k -1 (c) denotes the closest natural number k such that pass@k = c. We pick c to be the maximum pass@1 score over all models from the same family. These similarities demonstrate that across models from the same family, the increase in the log-sample-budget (or equivalently, the multiplicative increase in the sample budget) needed to improve coverage from c to c ′ is approximately constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Harnessing Repeated Sampling Requires Precision</head><p>So far, we have focused on measuring model coverage, characterizing the benefits of repeated sampling under the scenario where we can always identify correct model samples. We now turn to the complementary problem of precision: given a collection of model samples, how often can we identify the correct ones? In particular, we are interested in the performance of verifiers as we scale up the number of samples. For some problems, correct solutions are sampled from the model at low probabilities (e.g. 1% or lower, see Figure <ref type="figure" target="#fig_9">8</ref>). As the number of samples increases and rare, correct solutions are generated for more problems, model coverage improves. In order to convert these coverage improvements into higher success rates, verifiers must be able to find the "needle in the haystack" and identify infrequent correct samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Common Verification Methods Don't Always Scale with the Sample Budget</head><p>Of the five tasks we evaluate, only GSM8K and MATH lack tools for automatically verifying solutions. We test three simple and commonly used verification approaches on their ability to identify correct solutions from these datasets:</p><p>1. Majority Vote: We pick the most common final answer <ref type="bibr" target="#b65">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Reward Model + Best-of-N:</head><p>We use a reward model <ref type="bibr" target="#b22">[17]</ref> to score each solution, and pick the answer from the highest-scoring sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reward Model + Majority Vote:</head><p>We calculate a majority vote where each sample is weighted by its reward model score.</p><p>We reuse the collections of 10,000 samples that we generated with Llama-3-8B-Instruct and Llama-3-70B-Instruct in Section 2. We use ArmoRM-Llama3-8B-v0.1 <ref type="bibr" target="#b62">[57]</ref> as a reward model, which scores highly on the reasoning section of the RewardBench leaderboard <ref type="bibr" target="#b44">[39]</ref>. We report our results in Figure <ref type="figure" target="#fig_8">7</ref> as we increase the number of samples. While success rates initially increase with the number of samples for all three methods, they plateau around 100 samples. Meanwhile, coverage continues to increase with the number of samples and eventually exceeds 95%. In the case of majority voting, this success rate saturation is intuitive, since the occurrence of rare, correct solutions does not affect the most common answer that majority voting chooses. Given the poor performance of these verifiers (in particular the reward model), it is reasonable to wonder how "hard" it is to verify a candidate solution. With GSM8K and MATH, only a sample's final answer is used for assessing correctness, with the intermediate chains of thought being discarded. If models generated only non-sensical chains of thought before guessing a correct final answer, verification may not be any easier than solving the problem in the first place. We investigate this question by manually evaluating 105 chains-of-thought from correct Llama-3-8B-Instruct samples to GSM8K problems, reporting our results in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We find that over 90% of the chains-of-thought that we graded are faithful, even among problems where correct answers are generated infrequently. These correct reasoning steps indicate that there is signal for a verifier to exploit when identifying correct samples. Interestingly, during this process we also identified one GSM8K problem that has an incorrect ground truth answer (see Appendix E). This incorrect GSM8K problem is also the only one that Llama-3-70B-Instruct did not generate a "correct" sample for across 10,000 attempts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pass</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Verifiers and Software Tasks: Two Cautionary Tales</head><p>Software development tasks can occupy a middle-ground with respect to available verification tools. On one hand, the ability to execute and test code allows for a higher degree of automatic verification than is possible with unstructured language tasks. However, tools like unit tests take a black-box approach to verifying a piece of code and are not as comprehensive as methods like proof checkers. These imperfections in the verification process can lead to false positives and/or false negatives that are important to consider when applying repeated sampling. Below we provide two examples of software verifier imperfections that we encountered when generating our results from Section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Flaky Tests in SWE-bench Lite</head><p>When producing our results on SWE-bench Lite, we identified that 11.3% of problems have flaky test suites that do not produce consistent results when running them on the same candidate solution. These flaky tests occasionally classify even the dataset's ground-truth issue solutions as incorrect. Additionally, the test suites for some issues can be non-determinstic depending on the candidate solution. For example, two SWE-bench Lite issues involve manipulating Python sets, which are naturally unordered. The gold solutions for these issues explicitly order the items in the set and pass the test suites reliably. However, some model-generated candidate solutions do not impose such an ordering, and therefore pass the tests on some "lucky" runs and not others. In Appendix B, we list all of the problem IDs where we identified flaky tests. We also report our SWE-bench Lite results from Figure <ref type="figure">2</ref> with the problematic issues removed, finding similar results to our evaluations on the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">False Negatives in CodeContests</head><p>Each problem from the CodeContests dataset comes with a set of input-output test cases used to asses the correctness of solutions. These test cases are more comprehensive than those from earlier coding benchmarks like APPS <ref type="bibr" target="#b30">[25]</ref>, cutting down on the frequency of false positive solutions that pass all test cases but do not fully solve the described problem. However, the construction of the CodeContests test suites leads to false negative solutions that are correct but fail the tests.</p><p>For some CodeContests problems, the problem description allows for multiple distinct correct outputs for a given test input. However, the corresponding test cases do not handle these scenarios, instead requiring that one particular correct output is emitted. Additionally, many CodeContests test cases have been programmatically generated by mutating original test cases from the problem. Some mutated inputs violate the problem's input specifications (e.g. a mutated input being zero when the description promises a positive integer). These malformed test cases can lead to inconsistent behaviour between different correct solutions.</p><p>We assess the prevalence of these issues by running each problem's test suite on the list of correct solutions that CodeContests provides. Of the 122 problems in the test set that have Python3 solutions, we find that 35 problems have "correct" solutions that fail the corresponding tests. Since we do not allow models to view all of a problem's test cases (and their peculiarities), applying repeated sampling to these problems contains an element of "rolling the dice" to generate a solution that is not only correct, but emits the particular outputs that pass the tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Limitations</head><p>In this work, we explore repeated sampling as an axis for scaling compute at inference time in order to improve model performance. Across a range of models and tasks, repeated sampling can significantly improve the fraction of problems solved using any generated sample (i.e. coverage). When correct solutions can be identified (either with automatic verification tools or other verification algorithms), repeated sampling can amplify model capabilities during inference. This amplification can make the combination of a weaker model and many samples more performant and cost-effective than using fewer attempts from a stronger, more expensive model.</p><p>Improving Repeated Sampling: In our experiments, we explore only a simple version of repeated sampling where all attempts to a problem are generated independently of one another using the exact same prompt and hyperparameters. We believe that this setup can be refined to improve performance, particularly along the following directions:</p><p>1. Solution Diversity: We currently rely on a positive sampling temperature as the sole mechanism for creating diversity among samples. Combining this token-level sampling with other, higher-level approaches may be able to further increase diversity. For example, AlphaCode conditions different samples with different metadata tags.</p><p>2. Multi-Turn Interactions: Despite automatic verification tools being available when solving Code-Contests and MiniF2F problems, we use only a single-turn setup where models generate a solution without any ability to iterate on it. Providing models with execution feedback from these tools should improve solution quality. We are interested in the tradeoffs associated with multi-turn interactions, since each attempt becomes more expensive, but also may be more likely to succeed.</p><p>3. Learning From Previous Attempts: Currently, our experiments fully isolate attempts from each other. Access to existing samples, particularly if verification tools can provide feedback on them, may be helpful when generating future attempts.</p><p>Repeated Sampling and Inference Systems: Repeated sampling is a distinct LLM inference workload from serving chatbot requests. Production chatbot deployments place an emphasis on low response latencies, and adhering to latency targets can force a lower per-device batch size and reduce hardware utilization. In contrast, when sampling many completions to a single prompt, a larger emphasis can be placed on overall throughput and maximizing hardware utilization. Additionally, repeated sampling can benefit from specialized attention optimizations that exploit overlaps in prompts across sequences <ref type="bibr" target="#b39">[34,</ref><ref type="bibr" target="#b11">6,</ref><ref type="bibr" target="#b71">66]</ref>. Repeated sampling inference can therefore be accomplished at a lower cost than naively making many parallel requests to a chatbot-oriented API. These cost savings can further motivate choosing to sample many times from a cheaper model instead of fewer times from a more expensive one.</p><p>Verifiers: Our results from Section 4 highlight the importance of improving sample verification methods when tools for automatically doing so are unavailable. Equipping models with the ability to assess their own outputs will allow repeated sampling to be scaled to far more tasks. Of particular interest is applying repeated sampling to unstructured tasks like creative writing, which can require a more subjective comparison between different samples than the pass-fail tasks we consider. An alternative direction to developing model-based verifiers is to design converters that can make an unstructured task verifiable, for example by formalizing an informal math statement into a language like Lean so that proof checkers can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Scaling Inference Compute: Methods that perform additional computation during inference have been successful across many areas of deep learning. Across a variety of game environments, state-of-the-art methods leverage inference-time search to examine many possible future game states before deciding on a move <ref type="bibr" target="#b17">[12,</ref><ref type="bibr" target="#b55">50,</ref><ref type="bibr" target="#b15">10]</ref>. Similar tree-based methods can also be effective in combination with LLMs, allowing models to better plan and explore different approaches <ref type="bibr" target="#b68">[63,</ref><ref type="bibr" target="#b13">8,</ref><ref type="bibr" target="#b58">53,</ref><ref type="bibr" target="#b59">54]</ref>. Another axis for increasing LLM inference compute allows models to spend tokens deliberating on a problem before coming to a solution <ref type="bibr" target="#b67">[62,</ref><ref type="bibr" target="#b66">61,</ref><ref type="bibr" target="#b69">64]</ref>. Additionally, multiple models can be ensembled together at inference time to combine their strengths <ref type="bibr" target="#b63">[58,</ref><ref type="bibr" target="#b19">14,</ref><ref type="bibr" target="#b50">45,</ref><ref type="bibr" target="#b61">56,</ref><ref type="bibr" target="#b36">31</ref>]. Yet another approach involves using LLMs to critique and refine their own responses <ref type="bibr" target="#b48">[43,</ref><ref type="bibr" target="#b12">7]</ref>.</p><p>Repeated Sampling: Previous work has demonstrated that repeated sampling can improve LLM capabilities in multiple domains. One of the most effective use cases is coding <ref type="bibr" target="#b53">[48,</ref><ref type="bibr" target="#b20">15,</ref><ref type="bibr" target="#b43">38]</ref>, where performance continues to scale up to a million samples and verification tools (e.g. unit tests) are often available to automatically score every candidate solution. Recently, Greenblatt <ref type="bibr" target="#b28">[23]</ref> shows that repeated sampling is effective when solving puzzles from the ARC challenge <ref type="bibr" target="#b21">[16]</ref>, observing log-linear scaling as the number of samples increases. In chat applications, repeated sampling combined with best-of-N ranking with a reward model can outperform greedily sampling a single response <ref type="bibr" target="#b35">[30]</ref>. In domains without automatic verification tools, existing work shows that using majority voting <ref type="bibr" target="#b65">[60]</ref>, prompting an LLM <ref type="bibr" target="#b24">[19]</ref>, or training a model-based verifier <ref type="bibr" target="#b23">[18,</ref><ref type="bibr" target="#b47">42,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b64">59,</ref><ref type="bibr" target="#b40">35]</ref>, to decide on a final answer can improve performance on reasoning tasks relative to taking a single sample. Nguyen et al. <ref type="bibr" target="#b49">[44]</ref> finds that performing majority voting over answers that exceed a threshold length can outperform voting across all answers. Concurrent with our work, Song et al. <ref type="bibr" target="#b56">[51]</ref> finds that using the best available sample improves LLM performance on chat, math, and code tasks, sweeping up to a max of 128 samples. Additionally, Hassid et al. <ref type="bibr" target="#b29">[24]</ref> find that when solving coding tasks, it can be more effective to draw more samples from a smaller model than draw fewer samples from a larger one.</p><p>Scaling Laws: Characterizing how scaling affects model performance can lead to more informed decisions on how to allocate resources. Scaling laws for LLM training find a power law relationship between loss and the amount of training compute and provide estimates for the optimal model and dataset size given a fixed compute budget <ref type="bibr" target="#b32">[27,</ref><ref type="bibr" target="#b41">36,</ref><ref type="bibr" target="#b33">28]</ref>. Jones <ref type="bibr" target="#b38">[33]</ref> finds scaling laws in the context of the board game Hex, observing that performance scales predictably with model size and the difficulty of the problem. Interestingly, they also show that performance scales with the amount of test-time compute spent while performing tree search. Recently, Shao et al. <ref type="bibr" target="#b54">[49]</ref> observe scaling laws when augmenting LLMs with external retrieval datasets, finding that performance on retrieval tasks scales smoothly with the size of the retrieval corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Example</head><p>Q: Write python code to solve the following coding problem that obeys the constraints and passes the example test cases. The output code needs to read from and write to standard IO. Please wrap your code answer using ```: Petya has equal.. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 MATH</head><p>We report results on 128 randomly selected test-set problems. We sample with a temperature of 0.6 and do not use nucleus sampling. We use the fixed 5 few-shot example from <ref type="bibr" target="#b45">[40]</ref> for each problem. We generate 10, 000 samples per problem. We set 512 as the max token length for the generated solution. To grade solutions, we use the minerva_math functions from LMEval <ref type="bibr" target="#b27">[22]</ref> to extract the model's final answer. We then check correctness if the extracted answer is an exact string match to the ground truth, or if the is_equiv function from minerva_math in LMEval evaluates to true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SWE-bench Lite B.1 Experimental Setup</head><p>For our experiments, we use DeepSeek-Coder-V2-Instruct with the Moatless Tools agent framework (at commit a1017b78e3e69e7d205b1a3faa83a7d19fce3fa6). We use Voyage AI [5] embeddings for retrieval, the default used by Moatless Tools. We make no modifications to the model or framework, using them entirely as off-the-shelf components.</p><p>With this setup, we sample 250 independent completions for each problem using standard temperaturebased sampling. To determine the optimal sampling temperature, we conducted a sweep on a random subset of 50 problems from the test set, testing temperatures of 1.0, 1.4, 1.6, and 1.8. Based on these results, we selected a temperature of 1.6 for our main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Test Suite Flakiness</head><p>During our analysis, we identified 34 problems in SWE-bench Lite whose test suites had flaky tests. Using the SWE-bench testing harness provided by the authors of SWE-bench, we tested each solution repeatedly: for some solutions, sometimes the solution was marked as correct, and other times it was marked as incorrect. In 30 of these 34 cases, we observed flakiness even on the correct solutions provided by the dataset authors. Table <ref type="table" target="#tab_2">3</ref> lists the problem IDs of the 34 instances with flaky tests. An additional instance, astropy astropy-6938, was flaky on some machines and not others. The authors of SWE-bench were able to reproduce the flakiness; however, we were unable to. Our preliminary investigation indicates this specific issue is due to unpinned versions of dependencies in the docker environments that run the unit tests.</p><p>Here, we include results on a subset with the problems in Table <ref type="table" target="#tab_2">3</ref> removed (266 problems). For the full dataset evaluation, on any problem that has flaky tests, we run the test suite 11 times and use majority voting to determine whether a solution passed or failed. For the evaluation on the subset without flaky tests, all baselines we compare against release which problems they correctly solve, so we simply removed the problems with flaky tests and recomputed their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Precision Details</head><p>To calculate the Majority Vote, Reward Model + Best-of-N and Reward Model + Majority Vote metrics, we use the same 128 problem subsets for both MATH and GSM8K datasets introduced in Section 2. Each problem corresponds to 10,000 samples for each model we test. For each verification method, we take 100 random subsets of size k and calculate the success rate using each subset. We report the mean and standard deviation across subsets in Figure <ref type="figure" target="#fig_8">7</ref>. To calculate the Majority Vote answer, we take the plurality answer in each subset (note that two answers are considered equivalent if they are exact string matches or if is_equiv evaluates to true). For the Reward Model + Best-of-N, we take the answer with the highest score assigned by the reward model. For the Reward Model + Majority Vote metric, we sum the reward model score across all the samples with the same final answer, and use the final answer with the highest sum.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Problem 1 (Figure 1 :</head><label>11</label><figDesc>Figure 1: The repeated sampling procedure that we follow in this paper. 1) We generate many independent candidate solutions for a given problem by sampling from an LLM with a positive temperature. 2) We use a domain-specific verifier (ex. unit tests for code) to select a final answer from the generated samples.</figDesc><graphic coords="2,170.04,97.90,51.92,51.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>CodeContestsFigure 3 :</head><label>3</label><figDesc>Figure 3: Scaling inference time compute via repeated sampling leads to consistent coverage gains across a variety of model sizes (70M-70B), families (Llama, Gemma and Pythia) and levels of post-training (Base and Instruct models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>FLOPsPerToken(ContextLen) ≈ 2 * (NumParameters + 2 * NumLayers * TokenDim * ContextLen) TotalInferenceFLOPs ≈ NumPromptTokens t=1 FLOPsPerToken(t) + NumDecodeTokens t=1 FLOPsPerToken(t + NumPromptTokens) * NumCompletions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>, c = exp(ak b )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Comparing coverage (performance with an oracle verifier) to mainstream methods available for picking the correct answer (majority voting, reward model selection and reward model majority voting) as we increase the number of samples. Although near-perfect coverage is achieved, all sample selection methods fail to reach the coverage upper bound and saturate before reaching 100 samples. For every k value, we calculate the metric on 100 subsets of size k then plot the mean and one standard deviation across subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Bar charts showing the fraction of samples (out of 10,000 samples) that are correct, for each problem in the subsets of GSM8K and MATH we evaluate on. There is one bar per problem, and the height of the bar corresponds to the fraction of samples that arrive at the correct answer. Bars are green if self-consistency picked the correct answer and are red otherwise. We highlight that there are many problems with correct solutions, where the correct solutions are sampled infrequently.</figDesc><graphic coords="11,72.00,72.00,468.00,133.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>cnt += 2 * ((n -b) // a) while cnt &lt; 4: cur += 1 cnt += (n // a) ans = min(ans, cur) if b * 2 &lt;= n: cur, cnt = 0, 0 cur = 1 cnt += ((n -2 * b) // a) while cnt &lt; 4: cur += 1 cnt += (n // a) ans = min(ans, cur) print(ans) ``È xample Prompt Q: Write python code to solve the following coding problem that obeys the constraints and passes the example test cases. The output code needs to read from and write to standard IO. Please wrap your code answer using ```: This is the... ... (Rest of question and input/output examples) ... A:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Figure4: Comparing cost, measured in number of inference FLOPs, and coverage for Llama-3-8B-Instruct and Llama-3-70B-Instruct. We see that the ideal model size depends on the task, compute budget, and coverage requirements. Note that Llama-3-70B-Instruct does not achieve 100% coverage on GSM8K due to an incorrectly labelled ground truth answer: see Appendix E.</figDesc><table><row><cell>Model</cell><cell>Cost per attempt (USD)</cell><cell>Number of attempts</cell><cell>Issues solved (%)</cell><cell>Total cost (USD)</cell><cell>Relative total cost</cell></row><row><cell>DeepSeek-Coder-V2-Instruct</cell><cell>0.0072</cell><cell>5</cell><cell>29.62</cell><cell>10.8</cell><cell>1x</cell></row><row><cell>GPT-4o</cell><cell>0.13</cell><cell>1</cell><cell>24.00</cell><cell>39</cell><cell>3.6x</cell></row><row><cell>Claude 3.5 Sonnet</cell><cell>0.17</cell><cell>1</cell><cell>26.70</cell><cell>51</cell><cell>4.7x</cell></row></table><note><p>Comparing API cost (in US dollars) and performance for various models on the SWE-bench Lite dataset using the Moatless Tools agent framework. When sampled more, the open-source DeepSeek-Coder-V2-Instruct model can achieve the same issue solve-rate as closed-source frontier models for under a third of the price.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>@1 # Problems # CoT Graded Correct CoT Incorrect CoT Incorrect Ground TruthHuman evaluation of the validity of the Chain-of-Thought reasoning in Llama-3-8B-Instruct answers to GSM8K problems. 3 chains of thought were graded per problem. Even for difficult questions, where the model only gets ≤ 10% of samples correct, the CoTs almost always follow valid logical steps. For the model generations and human labels, see here.</figDesc><table><row><cell>0-10%</cell><cell>5</cell><cell>15</cell><cell>11</cell><cell>1</cell><cell>1 problem, 3 CoTs</cell></row><row><cell>10-25%</cell><cell>10</cell><cell>30</cell><cell>27</cell><cell>3</cell><cell>0 problems</cell></row><row><cell>25-75%</cell><cell>29</cell><cell>30</cell><cell>28</cell><cell>2</cell><cell>0 problems</cell></row><row><cell>75-100%</cell><cell>84</cell><cell>30</cell><cell>30</cell><cell>0</cell><cell>0 problems</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Instance IDs of problems from SWE-bench Lite that have flaky tests.</figDesc><table><row><cell>Repository</cell><cell>Instance IDs</cell></row><row><cell>django</cell><cell>django django-13315, django django-13447, django django-13590,</cell></row><row><cell></cell><cell>django django-13710, django django-13757, django django-13933,</cell></row><row><cell></cell><cell>django django-13964, django django-14017, django django-14238,</cell></row><row><cell></cell><cell>django django-14382, django django-14608, django django-14672,</cell></row><row><cell></cell><cell>django django-14752, django django-14915, django django-14997,</cell></row><row><cell></cell><cell>django django-14999, django django-15320, django django-15738,</cell></row><row><cell></cell><cell>django django-15790, django django-15814, django django-15819,</cell></row><row><cell></cell><cell>django django-16229, django django-16379, django django-16400,</cell></row><row><cell></cell><cell>django django-17051</cell></row><row><cell>sympy</cell><cell>sympy sympy-13146, sympy sympy-13177, sympy sympy-16988</cell></row><row><cell>requests</cell><cell>psf requests-863, psf requests-2317,</cell></row><row><cell></cell><cell>psf requests-2674, psf requests-3362</cell></row><row><cell cols="2">scikit-learn scikit-learn scikit-learn-13241</cell></row><row><cell>matplotlib</cell><cell>matplotlib matplotlib-23987</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The relationship between coverage and the number of samples can often be modelled with an exponentiated power law.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We thank <rs type="person">Together AI</rs> for partially sponsoring the compute for this project, as well as <rs type="person">Rahul Chalamala</rs> and <rs type="person">Ben Athiwaratkun</rs> for their help managing this infrastructure. We thank <rs type="person">John Yang</rs> for his advice and support when running our SWE-bench experiments. Finally, we are grateful to <rs type="person">Mayee Chen</rs>, <rs type="person">Neel Guha</rs>, <rs type="person">Quinn McIntyre</rs>, <rs type="person">Jon Saad-Falcon</rs>, and <rs type="person">Benjamin Spector</rs> for their helpful discussions and feedback throughout this project.</p><p>We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF2247015</rs> (<rs type="affiliation">Hardware-Aware</rs>), <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. <rs type="grantNumber">W911NF-23-2-0184</rs> (Long-context) and <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="projectName">Interactive Human-AI Teaming</rs>); ONR under Nos. <rs type="grantNumber">N000142312633</rs> (<rs type="affiliation">Deep Signal Processing</rs>); <rs type="funder">Stanford HAI</rs> under No. <rs type="grantNumber">247183</rs>; <rs type="affiliation">NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative (SDSI)</rs>, and members of the Stanford DAWN project: <rs type="funder">Meta</rs>, <rs type="person">Google</rs>, and <rs type="funder">VMWare</rs>. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of <rs type="funder">NIH</rs>, <rs type="institution">ONR</rs>, or the <rs type="institution">U.S. Government</rs>.</p><p>This work was completed with the support of the <rs type="funder">Clarendon Fund Scholarships</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8rh6ZeN">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_9svbwgC">
					<idno type="grant-number">CCF2247015</idno>
				</org>
				<org type="funding" xml:id="_tmdRVmh">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_bgXc98e">
					<idno type="grant-number">W911NF-23-2-0184</idno>
				</org>
				<org type="funded-project" xml:id="_ZbVhEd5">
					<idno type="grant-number">W911NF-21-2-0251</idno>
					<orgName type="project" subtype="full">Interactive Human-AI Teaming</orgName>
				</org>
				<org type="funding" xml:id="_MEEfXuZ">
					<idno type="grant-number">N000142312633</idno>
				</org>
				<org type="funding" xml:id="_QW7Mgn5">
					<idno type="grant-number">247183</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://en.m.wikipedia.org/wiki/Infinite_monkey_theorem">https://en.m.wikipedia.org/wiki/Infinite_monkey_theorem</ref>. Code: <ref type="url" target="https://github.com/ScalingIntelligence/large_language_monkeys">https://github.com/ScalingIntelligence/large_language_monkeys</ref>. Data: <ref type="url" target="https://huggingface.co/datasets/ScalingIntelligence/monkey_business">https://huggingface.co/datasets/ScalingIntelligence/monkey_business</ref>. * Equal Contribution. Work done by BB as a visiting researcher at Stanford.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Sampling Experimental Setup A.1 Lean Formal Proofs</head><p>We report results on the 130 questions in the test set of the lean4 MiniF2F dataset that correspond to formalized MATH problems. This dataset is derived from the fixed version of the original MiniF2F dataset created by Zheng et al. <ref type="bibr" target="#b70">[65]</ref>. We sample with a temperature of 0.5 and do not use nucleus sampling. We generated 10, 000 samples per problem. We use proofs of the following 5 theorems from the validation set as few-shot examples:</p><p>Our prompt consists of:</p><p>1. Few shot examples.</p><p>2. Header imports present in each problem in the HuggingFace dataset cat-searcher/minif2f-lean4</p><p>dataset, an upload of the lean4 MiniF2F dataset.</p><p>3. The theorem definition. In order to avoid leaking information about how to solve the theorem from its name, we replace the name of the theorem with theorem_i. i ∈ {1, 2, 3, 4, 5} for the few-shot examples and i = 6 for the current problem.</p><p>We set 200 as the max token length for the generated solution. To grade solutions, we use the lean-dojo 1.1.2 library with lean version 4.3.0-rc2. We set a timeout of 10 seconds for every tactic step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Example</head><p>Write a lean4 proof to the provided formal statement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CodeContests</head><p>We report results on the 140 test set questions that do not include image tags in the problem description. We sample with a temperature of 0.6 and a top-p value of 0.95 following the experiments in CodeLlama <ref type="bibr" target="#b53">[48]</ref>. We generate 10,000 samples per problem. We use two few-shot examples from the training set that are randomly sampled per-problem. We set 1024 as the max token length for the generated solution. We use the same answer comparison function as <ref type="bibr" target="#b46">[41]</ref> and use the concatenation of public, private, and generated tests to validate correctness of solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem:</head><p>If det A = 2 and det B = 12, then find det(AB). Solution:</p><p>We have that det(AB) = (det A)(det B) = (2)(12) = 24 . Final Answer: The final answer is 24. I hope it is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Prompt</head><p>Problem: What is the domain of the function</p><p>Express your answer as an interval or as a union of intervals. Solution:</p><p>A.4 GSM8K</p><p>We report results on 128 randomly sampled test-set problems. We sample with a temperature of 0.6 and do not use nucleus sampling. We use 5 few-shot examples from the training set that are randomly sampled per-problem. We generate 10, 000 samples per problem. We set 512 as the max token length for the generated solution. To grade solutions, we follow LMEval <ref type="bibr" target="#b27">[22]</ref> and extract answers using a regular expression that extracts the string after the quadruple hashes. Similar to MATH, we then assess correctness by checking if the extracted answer is an exact string match to the ground truth or if is_equiv evaluates to true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Example</head><p>Question: James decides to replace his car. He sold his $20,000 car for 80% of its value and then was able to haggle to buy a $30,000 sticker price car for 90% of its value. How much was he out of pocket? Answer: He sold his car for 20000*.8=$&lt;&lt;20000*.8=16000&gt;&gt;16,000 He bought the new car for 30,000*.9=$&lt;&lt;30000*.9=27000&gt;&gt;27,000 That means he was out of pocket   <ref type="table">3</ref> are excluded. For the graph on the right, all problems are included. We note that the trend is the same with or without the flaky tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Scaling Law Details C.1 Experimental details</head><p>To fit exponentiated power laws to coverage curves, we first sample 40 points spaced evenly along a log scale from 0 to 10, 000 and remove duplicates. We then use SciPy's <ref type="bibr" target="#b60">[55]</ref> curve_fit function to find the a and b parameters from Equation 3 that best fit these points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional results</head><p>In Figure <ref type="figure">10</ref>, we show additional results fitting power laws to coverage curves for an expanded set of datasets and models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E GSM8K incorrect answer</head><p>As discussed in 4.1, we identify that a problem in the GSM8K test set (index 1042 on HuggingFace) has an incorrect ground truth solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Johnny's dad brought him to watch some horse racing and his dad bet money. On the first race, he lost $5. On the second race, he won $1 more than twice the amount he previously lost. On the third race, he lost 1.5 times as much as he won in the second race. How much did he lose on average that day? The mistake is in the second line of the answer: on the third race, Johnny's dad lost $16.5, not $15, meaning he made $11 and lost $16.5 + $5 = $21.5. So, the answer is an average loss of $3.5 per race, not $3 per race (the answer in the dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Llama-3-8B Pythia-70M Llama-3-8B-Instruct Pythia-160M Llama-3-70B-Instruct Pythia-410M</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gemma-2B Pythia-1B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Gemma-7B Pythia-1.4B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pythia-2.8B Pythia-6.9B Pythia-12B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">Llama-3-8B Pythia-70M Llama-3-8B-Instruct Pythia-160M Llama-3-70B-Instruct Pythia-410M</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gemma-2B Pythia-1B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">Gemma-7B Pythia-1.4B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pythia-2.8B Pythia-6.9B Pythia-12B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aide</forename><surname>Dev</surname></persName>
		</author>
		<ptr target="https://aide.dev/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://llama.meta.com/llama3/" />
		<title level="m">Meta llama 3</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="https://www.anthropic.com/news/claude-3-5-sonnet" />
		<title level="m">Claude 3.5 sonnet</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bifurcated attention: Accelerating massively parallel decoding with shared prefixes in llms</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujan</forename><surname>Kumar Gonugondla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Gouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hantian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangfu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parminder</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.08845" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Tran-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ladish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamile</forename><surname>Kamal Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lukosuite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Sellitto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemi</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><forename type="middle">El</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamera</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lanham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Telleen-Lawton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Constitutional ai: Harmlessness from ai feedback</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nyczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v38i16.29720</idno>
		<ptr target="http://dx.doi.org/10.1609/aaai.v38i16.29720" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024-03">March 2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="17682" to="17690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.01373" />
		<title level="m">Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining deep reinforcement learning and search for imperfect-information games</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qucheng</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS &apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS &apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Joseph</forename><surname>Hoane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng-Hsiung</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(01)00129-1</idno>
		<ptr target="https://doi.org/10.1016/S0004-3702" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<idno type="ISSN">0004-3702</idno>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="129" to="130" />
			<date type="published" when="2002-01">jan 2002</date>
		</imprint>
	</monogr>
	<note>Deep blue</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Alphamath almost zero: process supervision without process</title>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minpeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Are more llm calls all you need? towards scaling laws of compound inference systems</title>
		<author>
			<persName><forename type="first">Lingjiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.02419" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.03374" />
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the measure of intelligence</title>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.01547" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from human preferences</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.03741" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Training verifiers to solve math word problems</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Networks of networks: Complexity class principles applied to compound ai systems design</title>
		<author>
			<persName><forename type="first">Jared Quincy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.16831" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.04434" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.12894" />
		<title level="m">The efficiency misnomer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/10256836" />
		<imprint>
			<biblScope unit="page" from="12" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Greenblatt</surname></persName>
		</author>
		<ptr target="https://www.lesswrong.com/posts/Rdwui3wHxCeKb7feK/getting-50-sota-on-arc-agi-with-gpt-4o" />
		<title level="m">Geting 50</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The larger the better? improved llm code-generation via budget reallocation</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hassid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.00725" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Measuring coding challenge competence with apps</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning scaling is predictable, empirically</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Mostofa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.00409" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.15556" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">V-star: Training verifiers for self-taught reasoners</title>
		<author>
			<persName><forename type="first">Arian</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Rewarding chatbots for real-world engagement with millions of users</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Boubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raina</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adian</forename><surname>Liusie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Mudupalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Korshuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fritz</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Assassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christie-Carol</forename><surname>Beauchamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoding</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Rialan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Beauchamp</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.06135" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Llm-blender: Ensembling large language models with pairwise ranking and generative fusion</title>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.02561" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Swe-bench: Can language models resolve real-world github issues?</title>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">E</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.06770" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scaling scaling laws with board games</title>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.03113" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05099</idno>
		<title level="m">Hydragen: High-throughput llm inference with shared prefixes</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mindstar: Enhancing math reasoning in pre-trained llms at inference time</title>
		<author>
			<persName><forename type="first">Jikun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.16265" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.08361" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.04908" />
		<title level="m">Spoc: Search-based pseudocode to code</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rewardbench: Evaluating reward models for language modeling</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Zick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.13787" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Gutman-Solo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.14858" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Sutherland Robson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.abq1158</idno>
		<ptr target="http://dx.doi.org/10.1126/science.abq1158" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">1095-9203</idno>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6624</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Let&apos;s verify step by step</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Hunter Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Cobbe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhisattwa</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.17651" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">When is the consistent prediction likely to be a correct prediction?</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.05778" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Routellm: Learning to route llms with preference data</title>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Kadous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.18665" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.08774" />
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Baptiste Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sten</forename><surname>Gloeckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itai</forename><surname>Sootla</surname></persName>
		</author>
		<author>
			<persName><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Sauvestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artyom</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Grattafiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><surname>Martin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.12950" />
		<title level="m">Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Scaling retrieval-based language models with a trillion-token datastore</title>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><forename type="middle">Wei</forename><surname>Koh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.12854" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.10457" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.08295" />
		<title level="m">Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Toward selfimprovement of llms via imagination, searching, and criticizing</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.12253" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Solving olympiad geometry without human demonstrations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Luong</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06747-5</idno>
		<ptr target="https://doi.org/10.1038/s41586-023-06747-5" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="issue">7995</biblScope>
			<biblScope unit="page" from="476" to="482" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stéfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Jarrod Millman</surname></persName>
		</author>
		<author>
			<persName><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">İlhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antônio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
	</analytic>
	<monogr>
		<title level="m">Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Knowledge fusion of large language models</title>
		<author>
			<persName><forename type="first">Fanqi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.10491" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Interpretable preferences via multi-objective reward modeling and mixture-of-experts</title>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.12845" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Mixture-of-agents enhances large language model capabilities</title>
		<author>
			<persName><forename type="first">Junlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.04692" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.08935" />
		<title level="m">Math-shepherd: Verify and reinforce llms step-by-step without human annotations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.03629" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.10601" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Quiet-star: Language models can teach themselves to think before speaking</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georges</forename><surname>Harik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varuna</forename><surname>Jayasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.09629" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Michael Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00110</idno>
		<title level="m">Minif2f: a cross-system benchmark for formal olympiad-level mathematics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Sglang: Efficient execution of structured language model programs</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangsheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.07104" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><surname>Örwall</surname></persName>
		</author>
		<ptr target="https://github.com/aorwall/moatless-tools/tree/a1017" />
		<title level="m">Moatless tools</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>b78e3e69e7d205b1a3faa83a7d19fce3fa6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
