- **Inference Compute Scaling**: Repeated sampling as a method to scale inference compute, improving reasoning performance across tasks.
  
- **Key Metrics**:
  - **Coverage**: Fraction of problems solved by any generated sample; increases with the number of samples.
  - **Precision**: Ability to identify correct samples from generated candidates.

- **Performance Improvement**: 
  - Example: DeepSeek-Coder-V2-Instruct performance on SWE-bench Lite increases from 15.9% (1 sample) to 56% (250 samples), surpassing the state-of-the-art (43%).

- **Scaling Laws**: Coverage often follows a log-linear relationship with the number of samples, modeled by an exponentiated power law.

- **Task Categories**:
  - **GSM8K**: Grade-school level math problems.
  - **MATH**: More complex math word problems.
  - **MiniF2F-MATH**: Formalized math problems for proof checking.
  - **CodeContests**: Competitive programming problems with hidden test cases.
  - **SWE-bench Lite**: Real-world GitHub issues requiring code modifications.

- **Verification Tools**: 
  - Automatic verifiers (e.g., unit tests, proof checkers) enhance precision and directly correlate coverage improvements to success rates.
  - In absence of automatic verifiers, common methods (majority voting, reward models) plateau in effectiveness.

- **Sample Budget Impact**: 
  - Coverage maximization can depend on model size and sample count; smaller models with more samples can outperform larger models with fewer samples.

- **Experimental Findings**:
  - Coverage increases significantly with repeated sampling; e.g., Gemma-2B coverage on CodeContests increases from 0.02% (1 sample) to 7.1% (10,000 samples).
  - Pythia models show zero coverage on CodeContests despite high sample budgets, indicating training limitations.

- **Cost-Effectiveness**: 
  - Repeated sampling can be more cost-effective than using larger models for certain tasks, maximizing coverage while minimizing inference costs.

- **Mathematical Representation**:
  - Coverage calculation formula:
    \[
    pass@k = 1 - \frac{N - C_i}{N} \text{ for } k \leq N
    \]
  - Where \(C_i\) is the number of correct samples for problem \(i\).

- **Figures and Results**:
  - Figure 1: Repeated sampling procedure flowchart.
  - Figure 2: Coverage improvement across tasks with increasing sample sizes.
  - Figure 3: Coverage trends across different model sizes and families.

- **Conclusion**: Repeated sampling is a viable strategy for enhancing LLM performance, particularly in tasks with automatic verification, while also being mindful of computational costs.