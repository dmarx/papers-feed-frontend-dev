{
  "arxivId": "2411.04330",
  "title": "Scaling Laws for Precision",
  "authors": "Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher R\u00e9, Aditi Raghunathan",
  "abstract": "Low precision training and inference affect both the quality and cost of\nlanguage models, but current scaling laws do not account for this. In this\nwork, we devise \"precision-aware\" scaling laws for both training and inference.\nWe propose that training in lower precision reduces the model's \"effective\nparameter count,\" allowing us to predict the additional loss incurred from\ntraining in low precision and post-train quantization. For inference, we find\nthat the degradation introduced by post-training quantization increases as\nmodels are trained on more data, eventually making additional pretraining data\nactively harmful. For training, our scaling laws allow us to predict the loss\nof a model with different parts in different precisions, and suggest that\ntraining larger models in lower precision may be compute optimal. We unify the\nscaling laws for post and pretraining quantization to arrive at a single\nfunctional form that predicts degradation from training and inference in varied\nprecisions. We fit on over 465 pretraining runs and validate our predictions on\nmodel sizes up to 1.7B parameters trained on up to 26B tokens.",
  "url": "https://arxiv.org/abs/2411.04330",
  "issue_number": 651,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/651",
  "created_at": "2025-01-04T06:53:12.611718",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 11,
  "last_read": "2025-01-04T06:53:12.612468",
  "last_visited": "2024-12-30T20:16:37.184Z",
  "main_tex_file": null,
  "published_date": "2024-11-07T00:10:10Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL"
  ]
}