<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Laws for Precision</title>
				<funder ref="#_x9faxBx">
					<orgName type="full">Google</orgName>
				</funder>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
					<orgName type="abbreviated">SDSI</orgName>
				</funder>
				<funder>
					<orgName type="full">Apple</orgName>
				</funder>
				<funder ref="#_4Ad4wyJ">
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder ref="#_5j9WEbr">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder>
					<orgName type="full">Hertz Fellowship</orgName>
				</funder>
				<funder ref="#_FK56f7t">
					<orgName type="full">Chan Zuckerberg Initiative Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder>
					<orgName type="full">Cisco</orgName>
				</funder>
				<funder ref="#_EXc42gN">
					<orgName type="full">Stanford HAI</orgName>
				</funder>
				<funder ref="#_D7WdcnH #_8Dbsy5x #_TH5HGKD #_EXzEANk">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_MZGjTWu #_wHddeHR">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-30">30 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tanishq</forename><surname>Kumar</surname></persName>
							<email>tkumar@college.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zachary</forename><surname>Ankner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Spector</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>3 MIT 4 Databricks 5 Carnegie</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>3 MIT 4 Databricks 5 Carnegie</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mansheej</forename><surname>Paul</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>3 MIT 4 Databricks 5 Carnegie</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Laws for Precision</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-30">30 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">BA6F9F7CBC6E1A0E6EE8FDF7538A36D2</idno>
					<idno type="arXiv">arXiv:2411.04330v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Post-Train Quantization INT3</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise "precision-aware" scaling laws for both training and inference. We propose that training in lower precision reduces the model's effective parameter count, allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scale has emerged as a central driver of progress in deep learning <ref type="bibr">[Brown et al., 2020]</ref>. Key work on scaling <ref type="bibr" target="#b1">[Kaplan et al., 2020</ref><ref type="bibr">, Hoffmann et al., 2022]</ref> studied tradeoffs between model/dataset size to balance performance and compute. However, the precision in which models are trained and served is an important third factor that contributes to both cost and performance. Deep learning is trending towards lower precision: current frontier models like the Llama-3 series are trained in BF16 <ref type="bibr" target="#b3">[Dubey et al., 2024]</ref>, and there is widespread effort to move the pretraining paradigm to FP8 <ref type="bibr" target="#b4">[Micikevicius et al., 2022]</ref>. The next generation of hardware will support FP4, and advances in weight-only quantization have led to training in binary and ternary at scale <ref type="bibr" target="#b5">[Ma et al., 2024</ref><ref type="bibr">, Wang et al., 2023]</ref>. How far will these paradigms go? Specifically, we ask:</p><p>What are the tradeoffs between precision, parameters, and data? How do they compare for pretraining and inference?</p><p>Studying scaling in precision is challenging because work on scaling laws generally aims to drop fine-grained implementation details in pursuit of universal functional forms while work on quantization generally does the opposite, focuses on the details: how quantization is done, with what type, to what part of the model. In seeking a balance, we consider a variety of plausible functional forms, and choose one that abstracts implementation details of quantization away from loss scaling, Training a fixed model size to various data budgets in BF16 and quantizing weights at the end. We find that degradation due to post-train quantization increases with tokens seen during pretraining, so that eventually additional pretraining data can be harmful. (Right) Our scaling suggests training larger models in lower precision can be compute-optimal according to the cost model in Section 4.3. Weights, activations, attention quantized, all models trained on the same data budget, details in Appendix J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling: Quantized Training</head><p>allowing us to predict loss scaling in many situations of practical interest. This functional form that posits bit precision and parameter count interchangeably contribute to a model's "effective parameter count," N eff , and implementation details like which parts of a model are quantized to what precision, interact with loss scaling only through their effect on this quantity.</p><p>Overall, we study the scaling of the effects of precision on loss as we vary data and parameters, both during and after training. We first study how the degradation induced by post-train quantization scales with parameters and data. We find that the degradation increases with data, so that for a fixed model, training on additional data after a certain point can be actively harmful if the model will be quantized after training. We then shift our focus to quantized training, examining both the quantization-aware-training (weights only) and low-precision training (weights, activations, attention all quantized) settings. Our scaling laws for pretraining suggest that the compute-optimal pretraining precision is in general independent of compute budget. Surprisingly, however, this independence ceases to be true if model size is constrained, in which case the compute-optimal precision grows slowly in compute.</p><p>In all, we pretrain a suite of 465 language models in 3 to 16 bit precisions, as well as post-train quantize each to multiple precisions. For a language model with N parameters, trained on D tokens with training precision P train , and post-train weight precision P post , we ultimately find a unified scaling law that takes the following form: (1)</p><p>where A, B, E, α, β are positive fitted constants, and δ PTQ refers to the loss degradation induced by post-training quantization before inference. Altogether, our results for post-train quantization illustrate how more pretraining FLOPs do not always lead to better models at inferencetime, and our results for low-precision pretraining suggest that both the standard practice of training models in 16-bit, and the race to extremely low (sub 4-bit) pretraining precision, may be suboptimal.</p><p>2 Background, Related Work, and Setup</p><p>Notation. Throughout, D denotes dataset size in tokens and N denotes model size in parameters. P w , P a , P kv refer to the bit precision, in integer-type, of the weights, activations, and key-value cache ("attention")<ref type="foot" target="#foot_0">foot_0</ref> during training, and P post refers to the precision we post-train quantize (PTQ) weights to at the end for model inference. When P or P train is used without reference to a part of the model, all three model parts are tied to the same precision. The inference-time loss degradation induced by post-train quantization will be denoted δ PTQ (N, D, P train , P post ), and it is defined as the change in loss from performing post-training quantization compared to the end of pretraining. We use "high precision" to mean 16-bit or above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Quantization Fundamentals: How, What, When</head><p>The Problem: Compute vs Memory-Bound Workloads. Most deep learning workloads are bottlenecked by either compute, in the form of matrix multiplications, or memory bandwidth, in the form of data movement between different parts of the GPU. Different types of workloads have different bottlenecks: most time is spent doing large matrix multiplications during pretraining, so it is compute-bound; in contrast, small-batch inference is bandwidth-bound by model weights; long-sequence decoding is bandwidth-bound by KV cache, etc. This motivates studying scaling in the training precision of the (weights, activations, KV cache) both in isolation and in combination. Quantization: How. Quantization of an operation typically refers to rounding of values in matrices involved in some computation on the forward or backward pass, depending on what is quantized, and when. Quantization is usually done to integer or floating-point type.</p><p>Quantization: What. Only weights. "Quantization-aware training" Quantizing only weights during training does not offer any compute savings because matrix multiplications are still done in high precision. However, this is commonly done to allow weights to adapt to low precision so they can be served at very low precision at inference-time, thereby alleviating memory bottlenecks <ref type="bibr" target="#b5">[Ma et al., 2024</ref><ref type="bibr">, Wang et al., 2023]</ref>. We will refer to this as "quantization-aware-training" and defer additional discussion to Appendix D.</p><p>Weights, activations, attention. "Low-precision training" Quantizing and activations and attention in addition to weights allows for compute gains because matrix multiplications can be done in low precision (if the hardware supports it) since everything is in the same precision. We will refer to this setting as "low-precision training" to distinguish it from quantization-aware training.</p><p>Quantization: When. Quantization can be done during or after training. In practice, when seeking to reduce inference-time memory costs, one first attempts post-train quantization.</p><p>If that degrades the model too much, quantization-aware-training is used. Post-train quantization is typically only applied to model weights <ref type="bibr" target="#b7">[Frantar et al., 2022</ref><ref type="bibr">, Dettmers et al., 2022</ref><ref type="bibr">, Lin et al., 2023</ref><ref type="bibr">, Xiao et al., 2023]</ref>. To reduce pretraining costs, low-precision-training is needed. We will study scaling laws for post-training quantization in Section 3, for quantized training in Section 4 (examining both quantization-aware training and low precision training) and unify the two in Section 5. The numerical values of all our fitted constants can be found in Appendix K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scaling Laws and Parametric Fits</head><p>Scaling Laws. <ref type="bibr" target="#b2">Hoffmann et al. [2022]</ref> model loss scaling using the functional form L(N, D) = AN -α + BD -β + E where A, B, α, β, E are positive fitted constants, finding that data and parameters should be scaled in roughly equal proportion as more compute becomes available. We will refer to the scaling of <ref type="bibr" target="#b2">[Hoffmann et al., 2022]</ref> as "Chinchilla-optimal" or just "Chinchilla" and note this is often used colloquially as D/N ≈ 20 being pretraining compute-optimal. On the theoretical front, work on scaling laws <ref type="bibr" target="#b11">[Bahri et al., 2024</ref><ref type="bibr" target="#b12">, Bordelon et al., 2024</ref><ref type="bibr">, Lin et al., 2024a]</ref> finds that noise to various parts of model or data affects loss in a predictable way. While previous works have explored the scaling behavior of post-training quantization in terms of total model bits <ref type="bibr" target="#b14">[Dettmers and Zettlemoyer, 2023]</ref> and knowledge capacity <ref type="bibr" target="#b15">[Allen-Zhu and Li, 2024]</ref>, we focus instead on data scaling. We note that in general the exact fitted values of all coefficients and exponents can vary drastically based on small implementation differences: <ref type="bibr" target="#b16">Besiroglu et al. [2024]</ref> find different constants when attempting to replicate <ref type="bibr" target="#b2">[Hoffmann et al., 2022]</ref>, <ref type="bibr" target="#b17">Sardana and Frankle [2023]</ref> fit coefficients A, B of different orders of magnitude. For this reason, we emphasize our contribution is not the numerical values we fit, but the trends and functional forms we identify.</p><p>Overtraining. In practice, accounting for inference costs means training smaller models for substantially longer than Chinchilla-optimal <ref type="bibr" target="#b17">[Sardana and</ref><ref type="bibr">Frankle, 2023, Gadre et al., 2024]</ref>. For instance, Llama-3-8B is trained to D/N ≈ 2000 <ref type="bibr" target="#b3">[Dubey et al., 2024]</ref> and the Gemma-2 series up to D/N &gt; 1000 <ref type="bibr" target="#b19">[Team et al., 2024]</ref>. We refer to such models as "overtrained" in this paper, with the token/parameter ratio D/N being a key quantity throughout. Work on inference-time compute <ref type="bibr" target="#b20">[Snell et al., 2024</ref><ref type="bibr" target="#b21">, Brown et al., 2024]</ref> and on synthetic and multimodal data <ref type="bibr" target="#b22">[Yang et al., 2024</ref><ref type="bibr">, Fan et al., 2024</ref><ref type="bibr" target="#b24">, Bauer et al., 2024]</ref> suggests future models may be even more overtrained. Therefore, modern work on scale must consider ratios much larger than Chinchilla-optimal, and in this work we perform experiments up to D/N ≈ 10 3 and analyze the predictions found by our scaling law for up to D/N ≈ 10 5 . See Appendix B for additional related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Setup</head><p>We train and evaluate a suite of OLMo-style models on the Dolma V1.7 dataset <ref type="bibr" target="#b25">[Groeneveld et al., 2024</ref><ref type="bibr">, Soldaini et al., 2024]</ref>, using a standard Transformer++ implementation; see Appendix A for hyperparameters and ablations. Our experiments consist of a sweep of language model pretraining runs over N ∈ <ref type="bibr">[30,</ref><ref type="bibr">60,</ref><ref type="bibr">110,</ref><ref type="bibr">220]</ref> million parameters (non-embedding) and D ∈ [1. <ref type="bibr">5,</ref><ref type="bibr">3,</ref><ref type="bibr">6,</ref><ref type="bibr">13,</ref><ref type="bibr">26]</ref> billion tokens. Our model sizes are relatively small because we train up to a very high D/N ≈ 10 3 to study data scaling and set off over 20 runs at every (N, D): we sweep 8 values of precision for each of the (weights, activations, attention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scaling Laws for Post-Train Quantization</head><p>The easiest and most common quantization technique is post-train quantizing a model off-the-shelf <ref type="bibr" target="#b27">[Chee et al., 2024</ref><ref type="bibr">, Huang et al., 2024</ref><ref type="bibr">, Dettmers et al., 2022</ref><ref type="bibr">, Lin et al., 2023</ref><ref type="bibr">, Xiao et al., 2023]</ref>. In this section, we consider models trained in BF16 and use GPTQ <ref type="bibr" target="#b7">[Frantar et al., 2022]</ref> to post-train quantize them, replicating our findings with two other methods in Appendix F. We quantify the resulting loss degradation δ PTQ , finding that post-train quantization scales poorly in data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overtrained Models Degrade more when Post-Train Quantized</head><p>We consider different model sizes (columns) trained on various data budgets (x-axis of each plot) and plot in Figure <ref type="figure">2</ref> both the loss after post-train quantization (top row) and the degradation Token/Parameter Ratio</p><p>10 3 10 2 10 1 Degradation, PTQ 100 10 100 10 Figure 2: Loss degradation from PTQ increases with data. Top row is loss after PTQ, bottom row is loss degradation compared to end of training, before PTQ. The top row is thus the gray line in each plot plus the corresponding value in the bottom row. We can see that degradation grows with data, bottom row is fitted with Equation <ref type="formula">2</ref>. For D/N sufficiently large (left), loss can increase in data. Even at lower D/N , where post-quant loss continues to decrease with data, the value of data is reduced compare to the baseline. R 2 = 0.97 over all fitted points (bottom row).</p><p>incurred relative to end of training (bottom row). We find that the degradation δ PTQ increases in training data size across all model sizes, but that for a fixed dataset size larger models incur a smaller degradation. We additionally observe that δ PTQ increases exponentially as we decrease the precision we quantize to. Based on these observations we model δ PTQ as taking the form:</p><formula xml:id="formula_0">δ PTQ (N, D, P post ) = C T D γ D N γ N e -Ppost/γpost (2)</formula><p>where C T , γ D , γ N , γ post are positive fitted constants. As we find the fitted values of γ D and γ N to be similar (see Appendix K for numerical values), we can think of this as an approximate power law in the token/parameter ratio D/N . The intuition for this poor data scaling might be that as models train on more data, they compress more information into their weights, so that perturbations to weights in the form of quantization are more harmful to loss, all else equal. We discuss formal theoretical interpretations in Appendix H. This finding implies that for models that will be post-train quantized, there exists an amount of pretraining data beyond which additional data is actively harmful to performance at inference-time (see top-left, Figure <ref type="figure">2</ref>). This can be defined as the point where additional data increases post-train degradation more than it decreases loss during pretraining. We solve analytically for this critical data size in Appendix E, as well analyze a cost model for workloads where inference-cost is the primary concern. We thus summarize our first scaling finding as follows.</p><p>Finding 1. Overtrained language models are more sensitive to post-training quantization. For models trained in BF16 or above, we can model this loss degradation as</p><formula xml:id="formula_1">δ PTQ (N, D, P post ) = C T D γ D N γ N e -Ppost/γpost</formula><p>where C T , γ D , γ N , γ post are positive fitted constants. This implies that when D/N is sufficiently large, or P post sufficiently small, loss after quantization can increase as models are pretrained for longer, as in Figure <ref type="figure">2</ref>. We will revisit and modify Equation 2 in Section 5 to account for the effects of training in low-precision on δ PTQ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scaling Laws for Quantized Training</head><p>In this section we study pretraining with weights, activations, and KV cache in various precisions. Importantly, only training precision, not test-time precision, is varied in this section; we discuss the interaction between train and test-time precision in Section 5. We sweep the training precisions of the weights, activations, and KV cache P w , P a , P kv ∈ [3, 12] individually, as well as training BF16 baselines. We also pretrain models with arbitrary combinations of P w , P a , P kv to validate our scaling laws. To perform quantization during training, we quantize the forward pass in integer type unless otherwise noted, see Appendix D for implementation details. 4.1 Quantization-Aware-Training: Quantizing Weights During Training has a Consistent and Predictable Effect</p><p>We first examine the trade-off between weight precision P w and parameters N while holding P a = P kv fixed at high precision. We fix D = 13B tokens and perform a grid sweep over combinations of N and P w . We plot the resulting IsoLoss contours where we linearly interpolate the final loss values in Figure <ref type="figure" target="#fig_3">3</ref>. We observe that the bit precision of the weights can be traded off for the number of parameters, i.e., a model with smaller N but larger P w can achieve the same loss as a model with larger N but smaller P w . Additionally, we find that the gains from increasing the bit precision of the weights are large at lower precisions but saturate at higher precisions (typically around 6-7 bits per weight).</p><p>In line with the empirical trends in Figure <ref type="figure" target="#fig_3">3</ref>, we find the best fit for the tradeoff between weight precision and parameters is N eff (N, P w ) = N (1 -e -Pw/γw ), where γ w is a fitted constant measuring the sensitivity of model weights (alternative fits explored in Appendix K). We therefore modify Chinchilla scaling to account for N eff by making the substitution N → N eff (N, P w ), giving the modified form:</p><formula xml:id="formula_2">L(N, D) = A[N (1 -e -Pw/γw )] -α + BD -β + E (3)</formula><p>where we recall that A, B, E, α, β are fitted positive constants in the usual Chinchilla scaling form, and γ w is a fitted constant we introduce. We plot the predictions of our fit compared to observed values in Figure <ref type="figure" target="#fig_4">4</ref> for a range of (N, D).  Quantization-aware training does not change the cost of pretraining. This is because modern GPUs require inputs to a matrix multiplication to have the same precision, i.e. P w = P a = P kv <ref type="bibr" target="#b4">[Micikevicius et al., 2022]</ref>. To understand the interplay between precision and pretraining compute we must now analyze the scaling behavior of P a and P kv as well. Note that in our training experiments, we only quantize on the forward pass to ensure a fair comparison between quantization-aware-training (weights only) and the additional quantization to activations/KV cache, see Appendix D.</p><p>Precision of activations and KV cache affect loss in a similar way. We first verify in Appendix Figure <ref type="figure">20</ref> that varying P a and P kv in isolation give rise to scaling behavior that is best fit by a functional form analogous to the form for P w (Equation 3, Figure <ref type="figure">5</ref>, left).</p><p>We refer to the scaling coefficients computed by varying the precision of just one part of the model at a time as marginally fitted constants, and those found by fitting on runs that include multiple model components in low precision at the same time as jointly fitted constants.</p><p>Constants fitted marginally and jointly make similarly good predictions. We now turn our attention to understanding the interactions between weights, activations, and attention. If the effects of quantizing weights, activations, and attention are independent, then a factorized, multiplicative interaction of the following form is a natural proposal.</p><formula xml:id="formula_3">N eff (P ) = N (1 -e -Pw/γw )(1 -e -Pa/γa )(1 -e -P kv /γ kv ) (4)</formula><p>3.2 3.4 3.6 3.8 4.0 4.2 4.4 Actual 3.0 3.5 4.0 4.5 5.0 Predicted P w Marginal Sweep MSE: 0.0028, R²: 0.9655 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 Actual Joint fit, f(P w , P a , P kv ) MSE: 0.0086, R²: 0.9006 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 Actual Combined Marginals, f(P w )f(P a )f(P kv ) MSE: 0.0089, R²: 0.8973 Figure 5: (Left) Predicted loss based on fitted values with Equation 4. (center) Fitting γ parameters jointly on sweeps with combinations of precisions vs (right) fitting them on "marginal" sweeps where only one model part is in low precision at a time. Outliers are those at extremely low precision whose training runs are sometimes unstable.</p><p>We test whether this independence approximately holds by comparing the predictive power of a model with marginally fitted constants and a model with jointly fitted constants. We show the predictive power of both models in Figure <ref type="figure">5(b,</ref> <ref type="figure">c</ref>), finding that both methods for fitting constants have approximately the same predictive power. These results suggest that the independence assumption is reasonable. We both present further evidence that this "factorized" functional form is a strong fit to the data as well as discuss alternative factorization schemes in Appendix M.</p><p>Finding 2. The effects of quantizing the weights, activations, and KV cache during training are well modeled as independent and multiplicative so that</p><formula xml:id="formula_4">L(N, D, P w , P a , P kv ) = AN -α eff + BD -β + E</formula><p>where N eff (P w , P a , P kv ) = N (1 -e -Pw/γw )(1 -e -Pa/γa )(1 -e -P kv /γ kv ) for which we fit constants γ w , γ a , γ kv that reflect the different sensitivities of weights, activations, and KV cache. If the three precisions are set to the same value P , as in pretraining, this simplifies to N eff (P ) ≈ N (1 -e -P/γ ) 3 where γ is the average of the three parameters.</p><p>We visualize this functional form with our fitted values in Figure <ref type="figure" target="#fig_3">3</ref> (left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implications For Pretraining</head><p>When training in a precision P , meaning P w = P a = P kv = P , compute cost scales linearly in P <ref type="bibr" target="#b29">[Abdelkhalik et al., 2022]</ref> <ref type="foot" target="#foot_1">foot_1</ref> . Hoffmann et al. <ref type="bibr">[2022]</ref> performed all experiments in 16-bit precision and use a cost model of C = 6N D FLOPs. We generalize this to C = 6 16 N DP to account for the linear relation between compute and precision, which reduces to the Chinchilla cost function for P = 16. We now examine three practically relevant variants of the following optimization problem. min</p><formula xml:id="formula_5">N,D,P L(N, D, P ) = A[N (1 -e -P/γ ) 3 ] -α + BD -β + E subject to C = 6 16 N DP<label>(5)</label></formula><p>Since derivations are algebraically involved, we will work up to proportionality and verify proposed solutions numerically. See Appendix E for mathematical details. We note that the implications</p><formula xml:id="formula_6">INT4 (1.76B) INT6 (1.17B) INT8 (880M) INT16 (440M) INT32 (220M)</formula><p>Training Precision (Model Size) of our functional form are true no matter the scale at which future experiments are done, but the numerical values we predict depend on our fitted constants which are fitted on smaller-scale, integer-type experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">If You Must Train in Low Precision, Increase Parameters Before Data</head><p>Minimizing L(N, D) with P fixed, subject to C ∝ N DP . We get with some algebra that at precision P and compute budget C, the optimal allocations N * , D * of parameters and data relative to Chinchilla-optimal N Ch , D Ch will be given by</p><formula xml:id="formula_7">N * (P, C) N Ch (C) ∝ 1 -e -P/γ -3α α+β P -β α+β and D * (P, C) D Ch (C) ∝ 1 -e -P/γ 3α α+β P β α+β (6)</formula><p>which suggests as precision of training decreases at fixed compute, we should increase parameters and decrease data. The interpretation of this is that at very low precisions, our effective parameter count vanishes so that increasing parameter count is compute-optimal since data egregiously outstrips effective parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Compute-Optimal Pretraining Precision is in General Independent of Compute</head><p>Jointly minimizing L(N, D, P ) with C ∝ N DP . This is the setting of pretraining without constraints on N, D, P except for a fixed compute budget. Solving this joint minimization problem gives an implicit equation for P * (C). Denoting u(P ) = [1 -e -P/γ ] -3α , we find (see Appendix E) that this equation takes the form 3α γ u(P ) 3α+1 3α e -P/γ = P -1 u(P ) <ref type="bibr">(7)</ref> which reveals that in general the optimal pretraining precision is independent of compute budget. This suggests that compute-optimal precision should be held fixed to P * while N, D are scaled according to Equation <ref type="formula">6</ref>. We find this P * to be around 7-8 bits when fitting our scaling law on runs with quantization done to integer type. This has two consequences: first, this means the de-facto practice of training models in 16-bit may be suboptimal. Second, the race to low-precision training may have to stop before going below 4-bits, since this would force model sizes to become disproportionately (more than 4x) larger to maintain loss scaling (see Figure <ref type="figure" target="#fig_3">3</ref>, left). We test our predictions in Figure <ref type="figure" target="#fig_5">6</ref> at a larger scale. We train compute-matched models at various parameter count and precision ranging from FP4 to FP32 and 220M to 1.6B parameters. We train in floating-point type since that is standard in pretraining <ref type="bibr" target="#b25">[Groeneveld et al., 2024</ref><ref type="bibr" target="#b30">, Deitke et al., 2024]</ref>, though our scaling laws are fitted on integer type. We plot our predicted trend in Figure <ref type="figure" target="#fig_5">6</ref> (left) and the empirical values in the middle. We find that scaling fits on integer type are a strong fit until 4-bit precision, at which points the difference between the two types becomes more apparent. The matching of qualitative trends throughout, with the optimum being close to the predicted optimum of P * near 7-8 bits suggests that similar scaling laws may exist across types. We initiate a similar analysis for floating-point type in Appendix N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">But Compute-Optimal Pretraining Precision Can Increase in Compute if Model</head><p>Size N is Constrained Minimizing L(D, P ) with N fixed, subject to C ∝ N DP . A common use case in practice is to train a suite of models of various sizes on similar data. The Llama-3 and Gemma-2 series <ref type="bibr" target="#b3">[Dubey et al., 2024</ref><ref type="bibr" target="#b19">, Team et al., 2024]</ref> are examples. In this setting, N is fixed in advance and only D, P are jointly optimized. Surprisingly, our scaling laws predict that models of differing sizes should not necessarily be trained in the same precision, and that compute-optimal precision scales as P * (C) ∝ log C. Since N is held constant and we show in Appendix E that log C ≈ log D in proportion, we can write P * (C) ∝ log(D/N ). The intuition for this is that, for a fixed N , precision acts as a new lever to bring highly overtrained models closer to pretraining optimality<ref type="foot" target="#foot_2">foot_2</ref> by reducing D/N eff .</p><p>Finding 3. When N, D, P are optimized jointly, compute-optimal pretraining precision is independent of compute. 16-bit has many unnecessary bits, and 4-bit requires increasing the model size disproportionately to maintain loss scaling. Our fits imply that 7-8 bits are compute-optimal. In contrast, when N is fixed in advance, such as when training a model family on similar data, P * (C) ∝ log C. This suggests that for models that will be significantly overtrained, higher precision during training may be compute-optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Unified Scaling Law for Precision</head><p>In this section, we combine the two scaling laws presented into a unified functional form that predicts both training/post-training effects, including interactions between the two. We now treat δ PTQ as a function δ PTQ (N, D, P train , P post ) rather than just δ PTQ (N, D, P post ) as we did earlier in Section 3. We find two competing effects at play when predicting δ PTQ , but overall, models trained in lower precision are more robust to post-train quantization in the sense of incurring lower degradation. Two competing effects at play during post-train quantization. Intuitively, training any of P w , P a , P kv in low precision forces the model to learn weights that are robust to "quantization noise," so they degrade less under PTQ. But the reduced N → N eff implies that models trained in low precision will degrade more because δ PTQ increases with N -γ N (Section 3). We call this second effect the "overtraining" effect. In practice, the first "robustification" effect is larger, so that models trained in lower precision overall degrade less when post-train quantized. We confirm using N eff rather than N to predict degradation given various training precisions leads to a substantially stronger fit in Figure <ref type="figure" target="#fig_0">21</ref>(top left, top center), to verify the competing overtraining effect.</p><p>Modifying δ PTQ to account for training precision. We assume training precision is strictly greater than inference precision, and define degradation as identically zero if they are equal. We begin by studying how degradation scales with just weight-precision during training, P w .</p><p>Consider Figure <ref type="figure" target="#fig_6">7</ref>(center). We fix (N, D) and each cell of the heatmap represents the empirical degradation δ PTQ (P w , P post ). We observe that degradation very quickly increases to its exponentially large value from Section 3 if there is any gap between training and inference-time precision. This motivates modifying our initial functional form fitted in Section 3 to</p><formula xml:id="formula_8">δ PTQ (N, D, P w , P post ) = C T e -Ppost/γpost D γ D N γ N eff Overtraining effect [1 -e -Cw(Pw-Ppost) ] Robustification effect (8)</formula><p>where C w is the only new fitted value. Then, we can extend this to include the precision effects of activations/attention in the natural way:</p><formula xml:id="formula_9">δ PTQ (N, D, P w , P a , P kv , P post ) = C T e -Ppost/γpost D γ D N γ N eff x∈{w,a,kv} [1 -e -Cx(Px-Ppost) ]<label>(9)</label></formula><p>We measure the fit to the data of such a functional form in Figure <ref type="figure" target="#fig_6">7</ref>, and find a strong fit with R 2 = 0.90 on over 1000 data points (each of 465 pretraining runs post-train quantized to multiple precisions).</p><p>An interpretable, unified functional form. Now we simplify and interpret the resulting functional form. Consider training with only weights in low precision and take C w = 1 for illustrative purposes so we can simplify Equation <ref type="formula" target="#formula_9">9</ref>. Denote σ 2 tr := e -Pw/γw as "training noise" reflecting the decrease in effective parameter count due to training weights in lower precision. Then, Equation 9 simplifies to δ PTQ (N, D, P train , P post ) = C T (σ 2 PTQ -σ 2 tr )</p><p>Robustification effect</p><formula xml:id="formula_10">• D γ D N γ N eff Overtraining effect (10)</formula><p>which we note is the intuitive modification one might make to the form of the initial post-training quantization degradation we fitted in Section 3, in Finding 3.1, with a small competing effects factor from N eff pushing in the opposite direction. It cleanly reflects the intuition that models are robustified to PTQ noise to the extent they were trained with similar noise.</p><p>Finding 4 (Unified Scaling Laws). Modeling low-precision effects during pretraining as independent and multiplicative noise that accumulates, and including post-training quantization degradation, the predicted loss for a language model with N parameters, trained on D tokens, with training precision P w , P a , P kv to end-time weight-precision P post , can be predicted as</p><formula xml:id="formula_11">L(N, D, P w , P a , P kv , P post ) = AN -α eff + BD -β + E + δ PTQ<label>(11)</label></formula><p>where δ PTQ (N, D, P w , P a , P kv , P post ) is in general as in Equation <ref type="formula" target="#formula_9">9</ref>and N eff (N, P w , P a , P kv ) as in Finding 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Limitations</head><p>We find that the common inference-time technique of post-train quantization can incur large degradation at very high data budgets, demonstrating a striking example of how more pretraining compute does not always imply stronger models at inference-time. Seeking better data scaling, we study quantization-aware and low precision training. We find that parameters and bit precision are well modeled as interchangeably controlling an "effective parameter count" of the model allows us to predict finite-precision loss effects accurately during both training and inference.</p><p>There are limitations to our analysis. First, we use a fixed architecture throughout to examine the effects of precision, parameters, and tokens in a controlled manner. In contrast, low precision training often involves architectural tweaks <ref type="bibr" target="#b5">[Ma et al., 2024</ref><ref type="bibr">, Zhu et al., 2024]</ref> that can close much of the gap from a vanilla full precision model. Second, while compute costs do scale linearly with precision, the gains from halving precision are usually less than 2x due to systems overhead. Third, we only consider loss scaling without downstream model evaluations. We emphasize that the trends we find aim to be suggestive rather than prescriptive, and hope future work can more comprehensively examine these effects at larger model scale. In all, we find that the effects of precision on loss are predictable and consistent, with important and surprising implications. 5 6 7 8 Precision (bits) 4.21 4.22 4.23 4.24 4.25 4.26 4.27 Final Loss Weights 4 5 6 7 8 Precision (bits) 4.25 4.30 4.35 4.40 Final Loss KV Cache 4 5 6 7 8 Precision (bits) 4.18 4.19 4.20 4.21 4.22 4.23</p><p>Final Loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activations</head><p>Figure <ref type="figure">8</ref>: L(P w ), L(P a ), L(P kv ) for ablated hyperparameters, N = 30M, D = 1.5B. We can see the trends persist, where the first few bits reduce final val loss significantly, with diminishing/saturating returns quickly setting in at higher precision. We do not fit constants on these ablated runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Alternative Functional Forms</head><p>There are several plausible functional forms to try a priori. The key junctions are whether a form is 1) additive or multiplicative and 2) interacts with parameters/data or is independent, 3) a power law or exponential. We try a variety of combinations of these three and find the formulation in the main text one of the best fits, notably with the fewest fitted parameters. We emphasize that several fitted forms are likely to be reasonable fits to the data, and an important desiderata for choosing a functional fit is interpretability. Several scaling law papers find multiple fits plausible in terms of predictive power <ref type="bibr">[Muennighoff et al., 2024b</ref><ref type="bibr">, Kaplan et al., 2020]</ref>, and ultimately make a decision based on interpretability.</p><p>We make these fit choices on sweeps of the form L(N, D, P W ) and discuss alternatives to the decomposition/factorization to account for activations and KV cache in Appendix Section M, which assumes an effective parameter count formulation. In this section, a power law refers to a term of the form C w • P -αw where C w , α w are fitted. In general, we find modeling precision effects with power law fits on their own causes the fitted constants A, B to blow up, whereas this does not happen with exponential fits, suggesting the power law does not change sharply enough to match the change in loss induced by precision. We note that while fitting parameters using a double notion of effective parameters and effective data leads to a slightly better fit, it requires more fitted parameters so we stick with the N eff formulation for simplicity and interpretability. When choosing between fits we validate on held-out data and the R 2 values below reflect the fit on the held out data. This is in contrast to our plots in the main text, where we have chosen a functional form and we fit and plot on the same data, as is standard in scaling laws <ref type="bibr">[Muennighoff et al., 2024b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Functional Form</head><p>Val </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Quantization Implementation Details and Types</head><p>Two canonical types for neural network quantization are floating-point (FP) and integer (INT) quantization. Despite their differences in representation, we hypothesize the scaling behavior between floating-point and integer quantization can be described by similar functional forms, where 1(b) provides preliminary evidence for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Integer Quantization and Implementation Details</head><p>In integer quantization, continuous values are mapped to discrete integer values. Typically, this is done by scaling the original values according to a fixed scale factor. Mathematically, for a real number x, the quantized integer value x int is computed as:</p><formula xml:id="formula_12">x int = x s</formula><p>where s is the scaling factor, and ⌊•⌉ denotes rounding to the nearest integer specified by the number of bits. The value can then be dequantized back to an approximate real value by multiplying by s:</p><formula xml:id="formula_13">x dequant = s • x int</formula><p>This process introduces quantization error, defined as the difference between the original value x and the dequantized value x dequant . The goal of quantization is to minimize this error while still reducing the precision. One can think of this as rounding to the nearest point on a uniform lattice. More complicated quantization schemes involve selecting the lattice points in a data or model-dependent manner. Integer quantization, as implemented, uses a fixed-point scaling based on the maximum absolute value of the tensor, and then scales the values within the range</p><formula xml:id="formula_14">[Q n , Q p ],</formula><p>where -1) and Q p = 2 (b-1) -1, with b being the number of bits.</p><formula xml:id="formula_15">Q n = -2 (b</formula><p>Integer quantization first rescales the inputs into the range specified by the number of bits by</p><formula xml:id="formula_16">s = Q p max(|x|)</formula><p>for tensor-based scaling, or</p><formula xml:id="formula_17">s = Q p max(|x|, dim = k)</formula><p>for channel-based scaling. After scaling, the result is rounded to the nearest integer and then clamped to the range [Q n , Q p ]. After matrix multiplication, the result is rescaled back into the original range. We quantize only the forward pass in this work, to ensure fair comparison between quantization-aware-training (weights only) and low-precision training (weights, activations, KV cache). This is because the backward pass is not usually quantized during quantization-awaretraining <ref type="bibr" target="#b5">[Ma et al., 2024]</ref>, so comparing sensitivities of weights (forward only) to activations/KV cache (forward and backward) would not be a principled comparison. In production pretraining in low precision, the matrix multiplications on the backward pass are also quantized, leading to further compute savings. We leave a detailed analysis of how our observations change when accounting for the backward pass to future work. We use integer quantization throughout to fit our scaling laws for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Floating-Point Quantization</head><p>Floating-point quantization is slightly more sophisticated, aiming to make a non-uniform lattice roughly matching the distribution of the weights, which are assumed to be Gaussian. A floatingpoint number is in general represented as:</p><formula xml:id="formula_18">x fp = (-1) s • m • 2 e</formula><p>where s is the sign bit, m is the mantissa, and e is the exponent. In floating-point quantization, both the mantissa and exponent are quantized to reduce the bit width. For exponent-mantissa allocations of bits and details of exponent bias, we follow the guidelines from <ref type="bibr" target="#b4">[Micikevicius et al., 2022]</ref> and quantize weights per channel and activations per-tensor.</p><p>Making a full scaling law for floating-point quantization is more involved than our integer treatment, because the effects of scaling mantissa vs exponent bits are not the same. In contrast, in integer quantization, each additional bit simply causes us to round into a finer-grained lattice after rescaling, thereby reducing quantization error by a predictable amount. In floating-point quantization, altering the exponent affects the dynamic range, while altering the mantissa changes the precision within that range. This flexibility at once makes floating-point quantization more suitable for model training, but harder to analyze. We leave a commensurately detailed analysis of mantissa vs exponent -and more generally floating point -scaling to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Hardware Details</head><p>Weight-only quantization can accelerate inference because software can be written to accommodate moving data between GPU parts (HBM-SRAM) in smaller units (types), so that a given bandwidth can move more data per second. This reduces memory (IO) bottlenecks that often dominate during inference, even with high-batch workloads. However, we emphasize that the type and therefore speed at which the GPU can do matrix multiplications in natively is determined by the hardware provider, so that even when P w = P a = P qkv (including queries), compute savings are only achieved when these correspond with both a bit-width and type that the GPU supports. We aim to study scaling in a fairly hardware-agnostic manner so that our work may be useful in the future, and make no claims about hardware details or optimality. We train all our models with fake (simulated) quantization on NVidia H100 GPUs to remain hardware agnostic, not taking advantage of any true low-precision computation. The only assumption is that when hardware does implement support for integer quantization, it is done in a way that involves some combination of rescaling and rounding, as is standard at the time of writing <ref type="bibr" target="#b14">[Dettmers and Zettlemoyer, 2023</ref><ref type="bibr">, Dettmers et al., 2022</ref><ref type="bibr">, Wu et al., 2020</ref><ref type="bibr" target="#b90">, Jacob et al., 2018</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Derivations E.1 Critical Dataset Size for PTQ</head><p>We seek a D crit that satisfies ∂L(D crit ) ∂D = ∂δ PTQ (D crit ) ∂D</p><p>. Taking both derivatives for the functional forms presented in the main text and equating their opposing effects, we get the equation</p><formula xml:id="formula_19">BD -β-1 crit = γ D C T N -γ N e -Ppost/γpost D γ D -1 crit (12) which implies D crit = βBN γ N e Ppost/γpost γ D C T 1 γ D +β (13)</formula><p>is the predicted point after which pretraining on more data can increase loss of a model that is post-train quantized. Note that this quantity explodes in P , so that a truly unreasonable amount of data is required for longer pretraining to be harmful at commonly used precisions (eg. 8-bit). However, we find that on overtrained models D/N ≫ 10 3 , these overtraining-degradation effects become nontrivial around 5-bits, and dominant below that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Compute-optimality calculations</head><p>We set a constraint C ∝ N DP throughout. Working up to proportionality is essentially rescaling the compute constraint, so it doesn't affect the scaling trends we identify, which is our focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 Fixed Precision Compute Optimal Scaling</head><p>Under fixed precision, the loss takes the form</p><formula xml:id="formula_20">L = u(P )AN -α + BD -β<label>(14)</label></formula><p>where u(P ) = [1 -e -P/γ ] -3α is a fixed constant. The compute optimal scaling when minimizing the loss over N, D gives</p><formula xml:id="formula_21">L = u(P )AN -α + BC -β N β P β<label>(15)</label></formula><p>by replacing D = C N P . Optimizing over N , we see that this is equivalent to the original chinchilla optimization problem but with A → Au(P ) and B → BP β . Performing this optimization, we find</p><formula xml:id="formula_22">N * (P, C) = u(P )Aα BP β β 1 α+β C β α+β , D * (P, C) = u(P )Aα BP β β -1 α+β C α α+β (16)</formula><p>We can relate the above expressions to the original Chinchilla-optimal N, D at full precision</p><formula xml:id="formula_23">N Ch (C), D Ch (C). N * (P, C) N Ch (C) ∝ 1 -e -P/γ -3α α+β P -β α+β and D * (P, C) D Ch (C) ∝ 1 -e -P/γ 3α α+β P β α+β (17) E.2.2 Fixed model size N</formula><p>Now, we investigate the case where model size N is fixed but precision and data are jointly optimized at fixed compute C = N DP . This optimization problem takes the form</p><formula xml:id="formula_24">L = u(P )AN -α + BD -β<label>(18)</label></formula><p>Under fixed compute, we have D = C N P so replacing the second term, we have</p><formula xml:id="formula_25">L = u(P )AN -α + BC -β N β P β (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>where N is a constant. We therefore have a single variable P to minimize the above formula over</p><formula xml:id="formula_27">∂L ∂P = u ′ (P )AN -α + BC -β N β β P β-1 = 0 (20)</formula><p>First, we note that u ′ (P ) has the following form</p><formula xml:id="formula_28">u ′ (P ) = -3α[1 -e -P/γ ] -3α-1 × 1 γ e -P/γ = - 3α γ e -P/γ × u(P ) 3α+1 3α<label>(21)</label></formula><p>We thus desire a solution to the implicit equation 3α γ e -P/γ × u(P )</p><formula xml:id="formula_29">3α+1 3α AN -α = BC -β N β β P β-1<label>(22)</label></formula><p>We now aim to find an approximate asymptotic relationship between P and C as C → ∞. Taking a logarithm of both sides, we find (neglecting additive constants that are independent of C, P )</p><formula xml:id="formula_30">-(3α + 1) ln(1 -e -P/γ ) - 1 γ P ≈ -β ln C<label>(23)</label></formula><p>The correct dominant balance at large C is to take P ⋆ ∼ βγ ln C, as can be verified numerically.</p><p>With the constraint that C = N P D we have that D ⋆ ≈ C N βγ ln C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.3 Minimization over N , D, P with Fixed Compute</head><p>Recall our three-way loss function is given as below. We separate N eff into terms involving (N, P ) explicitly here as it makes the math easier to follow.</p><formula xml:id="formula_31">L(N, D, P ) = AN -α u(P ) + BD -β , u(P ) = [1 -e -P/γ ] -3α<label>(24)</label></formula><p>Under the constraint C ∝ N DP , we can replace D in terms of C, N, P giving the loss expression</p><formula xml:id="formula_32">L = AN -α u(P ) + BN β P β C -β (25) ∂L ∂N = -αAN -α-1 u(P ) + βBN β-1 P β C -β = 0 (26) ∂L ∂P = -3α/γAN -α u(P ) 3α+1 3α e -P/γ + βBN β P β-1 C -β = 0 (27)</formula><p>Multiplying the first equation by N and dividing the second equation by it reveals that the optimal P satisfies a compute-independent implicit equation 3 γ u(P )</p><p>1 3α e -P/γ = P -1 u(P )</p><p>This exercise reveals that the compute optimal strategy when allowed to jointly optimize N, D, P is to choose a fixed precision that satisfies the above equation and then to scale up N, D with the prescription in Appendix I.1.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Inference-time Cost Model</head><p>For many, inference is the primary cost of training and serving models. Here, we present a preliminary analysis of an inference-time cost model. The key tension is that inference cost scales as N P , so that inference costs at a fixed pretraining loss can be reduce by either reducing model size (and overtraining more) or quantizing post-training We will assume here that P = P post refers to the precision weights will be quantized to. In practice, inference costs may depend on the precision of the KV cache and activations to some extent as well, but we assume this for tractability of the following mathematical model, and to get a sense of how overtraining and post-train quantization concerns play out at inference-time. We can phrase this minimization problem in the following way. The system of first-order conditions that results from this constrained optimization problem is not in general tractable analytically, so we solve the above constrained optimization problem for P * (C), N * (C), D * (C) numerically via a simple grid search. We find that N * , D * grow as a power law in C while P * ∝ log C. The clumping in points is an artifact of the numerics of the grid search; the fitted lines represent the loglinear (left) and loglog (middle, right) trends overall.</p><p>It might be surprising that D * is not taken to infinity since it does not appear in the cost function. The reason for this is because if it was, post-train degradation (the third term) would become large. It might also be surprising that D * changes with compute at all. The reason for this is because, once again, of the third term: as we allow more inference-time compute we use more N , and at a larger N we can now tolerate a larger data budget for a given post-train quantization degradation, so being compute-optimal means taking advantage of this and training that larger parameter count on more data.</p><p>The intuition for why P * ∼ log C might be as follows. Consider a situation in which P * is independent of compute: the third term will come to be a bottleneck in loss as compute gets larger because N, D are both being scaled as power laws in compute, and eventually the effect of e -P/γ will become non-negligible in comparison to the first two terms in the loss function. To continue decreasing loss at this point, we must make this term smaller at a rate commensurate with the other terms, which go as a power law in compute. Since precision is inside the exponential, this can be done by taking P ∼ log C. An important thing to note is that since we are ignoring pretraining costs here, the absolute values of predicted D * are much larger than would be realistically possible in 2021, <ref type="bibr" target="#b92">Gilmer et al., 2021]</ref>, so that movement along the top Hessian eigenvector degrades loss by more throughout training. Though sharpness is formally a worst-case sensitivity, we conjecture similar results hold for average case, such as loss degradation induced by isotropic noise. It may be possible that sharpness during language model pretraining does not reach its maximal value for a long time, which is why sensitivity to noise monotonically seems to increase as D/N → ∞ on realistic data budgets. Closely related is the largest eigenvalue of the neural tangent kernel (NTK) which captures the magnitude of the variance of the predictor under parameter noise. This quantity is known to empirically increase during training in a variety of settings, and is closely related to generalization guarantees <ref type="bibr" target="#b93">[Nguyen et al., 2021</ref><ref type="bibr" target="#b94">, Atanasov et al., 2022]</ref>.</p><p>Hierarchical learning strategies become more sensitive throughout training. Our expectation that overtrained language models may degrade more when quantized at inference-time is motivated in part by the following results. The hierarchical nature of learning is by now well understood in some toy settings: in <ref type="bibr" target="#b95">[Abbe et al., 2021]</ref>, it is shown that "staircase" polynomials of increasing degree are learned faster than high-degree monomials since neural networks combine existing features to learn new ones. In <ref type="bibr" target="#b96">[Abbe et al., 2022]</ref> this result was strengthened to show that such hierarchical structure is both necessary and sufficient to learn sparse functions with SGD in two layer neural networks. In this setting, damage to features encoding lower-order polynomials affects all higher-order ones, so that such networks are increasingly sensitive to fixed feature noise throughout learning. Another result of a similar flavor is that of <ref type="bibr" target="#b97">[Barak et al., 2022]</ref>, who explicitly require high-precision gradients for sparse parity to be learned, since sparse parity is learned by the amplification of a small initial signal. If language models learn hierarchically, it is possible that the features that are learned late into overtraining as D/N → ∞ are reliant on base features, so that noise harms the base features and therefore significantly damages higher-order features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Granularity Ablations</head><p>Here, we ablate our choice of quantization granularity (per-tensor vs per-channel) compared to the main text, where we do weights per-channel and activations per-tensor. Per-tensor quantization involves keeping one scalar to rescale all values in a tensor into the quantization codebook range, and per-channel means keeping a scalar per channel dimension; therefore, the latter is strictly more expressive and thus incurs lower quantization loss, than the former, at the cost of slightly more memory usage. Here, we ask: is the increased sensitivity of activations a result of them being inherently more sensitive, or due to the per-tensor design choice.</p><p>These results show that activations are generally more sensitive than weights, since their loss penalty at lower precision goes up faster even when granularity is kept fixed across the two. In fact, quantizing activations per-channel is almost as hard as quantizing weights per-tensor. This is consistent with a broad line of work in quantization finding that activations comprise the central difficulty in quantization <ref type="bibr" target="#b98">[Dettmers and</ref><ref type="bibr">Zettlemoyer, 2023, Ma et al., 2024]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Main Figure Details</head><p>The model on the left is N = 30M parameters, chosen because we could train it to the highest token/parameter ratio given our compute budget. On the right we train a suite of models with N P kept constants on 16B tokens (so that C = 6 16 N DP is matched throughout under our cost model). We plot val loss on Dolma, as throughout the main text, and use floating-point (rather than integer) to make the pretraining claims as realistic as possible.</p><p>4 6 8 10 12 Precision (bits) 3.75 4.00 4.25 4.50 4.75 5.00 Loss Varying Quantization Granularity Weights (per tensor) Weights (per channel) Activations (per tensor) Activations (per channel) Figure 13: Quantization granularity ablation: all combination of (training weight precision, training activation precision) × (per-tensor, per-channel). Dashed lines are per-channel and solid are pertensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Numerical Fits</head><p>Following <ref type="bibr">[Muennighoff et al., 2024b]</ref>, we tie α = β so they do not become very different, though this is not required. Distinct α, β only add expressivity to the model and we have verified the plots look similar without tying. We also only use the full scaling law when specified in the text, since the law is developed piecewise through the text. For instance, Figures <ref type="figure" target="#fig_3">3</ref> and <ref type="figure" target="#fig_4">4</ref> solely fit Chinchilla with a substitution N → N eff (P w ) because at that point P a , P kv have not been introduced. Figures <ref type="figure">5,</ref> <ref type="figure" target="#fig_5">6</ref>, and 7 use our full scaling law, for instance to make predictions. We emphasize our numerical constants are unlikely to be useful because as <ref type="bibr">[Hoffmann et al., 2022, Sardana and</ref><ref type="bibr" target="#b17">Frankle, 2023]</ref> show, fitted constant depend heavily on the architecture and dataset used, which differs from setup to setup. Rather, the trends we identify are the key findings. With that said, our fitted constants are as follows.</p><p>Note that we include biases in our exponent fits, for instance when modelling N eff as a saturating exponential, we find that the different parts of a model cause numerical instability at different values of low precisions, so even if they are the same functional form, they may be translated (left/right shifted versions) of eah other. For instance a fit of the form e x/γx in the main text is really computed with offset e x/γx+n , but including biases everywhere clutters notation and obscures mathematical insight.</p><p>L Are Weights, Activations, and KV Cache Equally Sensitive?</p><p>We find that training runs with P a ≤ 3 or P kv ≤ 3 are not numerically stable, and often diverge, while P w = 3 is still well behaved. In particular, we find activations are more sensitive, though this could be because we quantize activations per-tensor and weights-per channel, rather than activations being inherently more sensitive. Consequently, we do not fit or validate on runs with activations or attention bits equal to 3. We leave a more detailed analysis of fine-grained sensitivity across layers and types of parameters to future work. The Figure below illustrates the empirical sensitivity by plotting L(P ) for the three quantities for various runs (N, D). Consider a model trained with some arbitrary (N, D, P w ). Assuming a Chinchilla function form with N → N eff (P w ), we can write the difference between its loss and the loss of a full precision model as L(N, D, P w ) -L(N, D, ∞) = A[N -α eff -N -α ] as the terms involving B, D, E cancel. Note that N eff (P w = ∞) = N by construction. In practice, we use a BF16 model as the "infinite-precision" model, finding no real difference if we use an FP32 model or even a functional fit estimating P w → ∞ based on our integer quantization loss results. Our goal is to plot what f (P ) looks like where N eff = N • f (P ). Therefore, we can rearrange the above equation as follows</p><formula xml:id="formula_34">f (P ) := N eff N = 1 N L(N, D, P w ) -L(N, D, P w = ∞) A + N -α -1/α<label>(30)</label></formula><p>Then plotting this quantity using our fitted numerical values (See Appendix K) gives us the empirical tradeoff between precision and parameters. We can see that the tradeoff is quickly saturating in P to a value near 1. While the functional form is the same for the three model parts, the fitted constants are different. For instance, runs with P a ≤ 3 or P kv ≤ 3 often diverged, and this was not the case with weight precision. Further, we can see that the KV cache is not sensitive to quantization at higher bit value, but very quickly becomes sensitive around 4-5 bit precision.</p><p>Then as far as the joint functional form for N eff (P w , P a , P kv ) is concerned, we acknowledge that alternative factorizations that do not decompose the model into weights, activations, and KV cache, may have an equally good fit. For instance, decomposing the weights term into a product of layer-wise effects has a reasonable fit though introduces more parameters, and a more coarse-grained version may not decompose the model into parts at all, but only consider tied precisions. We choose this factorized form because QAT considers weights only, and activations and attentions are the two other things that must then be kept in low precision to see compute gains. Since practitioners often care about KV cache on its own, we chose to decompose "activations and attention" as "activations    and KV cache." We emphasize that our main point is not that this factorization is objectively correct, but in observing that such a factorization that assumes approximate independence is possible in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N Floating-Point Experiments</head><p>The key difference between floating point and integer type is that the former allocates some bits to the exponent representation and some to the mantissa, and these bits play different roles, unlike in integer type where every bit plays the same role in making the quantization lattice uniformly more fine-grained. We hypothesize that if exponent and mantissa bits are scaled jointly (ie. increase together as total bit count does), the overall trend will still be predictable with a functional form like ours. To test this, we fit a parametric form like Equation 3 with the constants A, B, E, α = β listed in the table. The overall fit results in values of γ w = 2.8111 and an exponent bias of b = 0.1240, showing the functional form is still a good fit to the data, even for floating point, under reasonably standard bit allocation schemes between mantissa and exponent. On the middle and right, we again re-fit the same parametric form on now specific values of (N, D) and visualize the quality of the resulting predictions.</p><p>We use bit allocations of E2M0, E3M0, E4M1, E3M2, E4M2, E5M2, and E5M6 for 3, 4, 5, 6, 7, 8, 12 bits, respectively, with one sign bit throughout. Since exponent and mantissa bits play in general different roles (ie. the effect of a bit on loss and dynamics depends a lot on whether it comes from the mantissa or exponent in floating point), we expect our functional form does well here because mantissa and exponent allocations both increase jointly as precision rises, so overall the trends are predictable in a similar way. We check directly the role of the two by sweeping ExM3 and E3Mx directly, confirming this intuition. This suggests one route for making fine-grained fits for general arbitrary ExMy combinations is to decompose the effects of mantissa and weights, for instance a form like N eff (P w, m , P w, e , N ). Since this is not needed for standard bit allocation choices as we can see in Figure <ref type="figure" target="#fig_13">16</ref>, we do not delve into this complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O Additional Plots</head><p>0 2 4 6 8 P w (exponent/mantissa) 3.7 3.8 3.9 Loss Loss vs Exponent/Mantissa Weight Bits MxE3 M3Ex Figure 17: Exponent-mantissa bit allocation sweep. We can see the two types of bits have different scaling behavior, but both fit the saturating form where the first few bits reduce loss a lot, with diminishing returns after that. 0 2 4 6 8 10 12 Tokens (billions) 3.50 3.75 4.00 4.25 4.50 4.75 5.00 Val Loss Training-time Effects, P train 0 2 4 6 8 10 12</p><p>Tokens (billions) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-Training Effects, P post</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Schematic of key findings. (Left) Training a fixed model size to various data budgets in BF16 and quantizing weights at the end. We find that degradation due to post-train quantization increases with tokens seen during pretraining, so that eventually additional pretraining data can be harmful. (Right) Our scaling suggests training larger models in lower precision can be compute-optimal according to the cost model in Section 4.3. Weights, activations, attention quantized, all models trained on the same data budget, details in Appendix J.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>+ δ PTQ (N eff , D, P train , P post )Post-Training Effects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (Left) N eff /N from our final scaling law. Our fit of N eff (N, P w ) in this section is the first step towards this (blue). Empirical (center) and predicted (right) IsoLoss contours illustrating the precision-parameter tradeoff. Y-axis is weight precision during quantized training. All runs plotted trained on D = 13B tokens. Predictions from a fitted version of Equation 3, darker lines correspond to lower loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Predicting final validation losses L(N, D, P w ) for various N, D, P w to test our proposed functional form. Points are experimental values, lines are predictions of a single parametric fit of the form in Equation 3. We train only two model sizes at 26B due to compute constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scaling law predictions (left, fitted on integer type) vs empirical values (right, floatingpoint type). Precision of weights, activations, attention fixed to P train . Predictions closely match the empirical trend, but are shifted up by a small amount since floating-point is a more expressive type and will incur lower loss at the same precision. (Right) When N is held fixed, computeoptimal precision increases approximately logarithmically with data. Markers correspond to predicted compute-optimal precision for Llama-3 (8b, 70b, 405b), denoted by (circle, triangle, star) at each IsoFLOP (lines), illustrating how compute-optimal precision increases in data when model size is held fixed.</figDesc><graphic coords="9,398.72,88.13,106.47,65.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Combined plots for predicting degradation. (Left) demonstrates the quality of our fit on all our runs, including all combinations of pre and post-training precisions. (Center, right) illustrate visually that our unified degradation form can predict degradation when training and serving in any precision. Plots (center, right) vary P w only, but fits in (left) include runs where P a , P kv are also jointly varied.</figDesc><graphic coords="11,246.94,96.66,131.56,113.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Numerically minimizing a model of inference-time costs with respect to N, D, P after accounting for post-train-quantization degradation and its relation to overtraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>D, P ) = AN -α + BD -β + C T D γ D N γ N e -P/γ subject to C = N P(29)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Sweeping L(P ) for the three model parts at various N, D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Plotting what N eff looks like empirically. Each black point is a pretraining run, mathematical details of what is plotted here in Appendix E. Blue lines are parametric fits of a saturating exponential.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Fitting an effective parameter form to floating-point precision for weight training. (Left) involves checking quality of fit on 140 training runs in floating point precision for weights during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Illustration of what finite-precision effects during training and inference look like on learning curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Functional Forms with R 2 ,and Number of Fitted Parameters</figDesc><table><row><cell></cell><cell cols="2">R 2 Number of Fitted Parameters</cell></row><row><cell>N eff</cell><cell>0.82</cell><cell>3</cell></row><row><cell>Additive/independent power law</cell><cell>0.71</cell><cell>2</cell></row><row><cell>D eff</cell><cell>0.74</cell><cell>3</cell></row><row><cell>N eff and D eff (tied)</cell><cell>0.79</cell><cell>3</cell></row><row><cell>N eff and D eff (not tied)</cell><cell>0.84</cell><cell>4</cell></row><row><cell>Multiplicative power law, N, P</cell><cell>0.75</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Fitted constants and their values</figDesc><table><row><cell cols="2">Constant Value</cell></row><row><cell>A</cell><cell>4.299e3</cell></row><row><cell>α</cell><cell>0.4965</cell></row><row><cell>B</cell><cell>1.806e4</cell></row><row><cell>E</cell><cell>2.7648</cell></row><row><cell>γ w</cell><cell>2.6745</cell></row><row><cell>n w</cell><cell>0.3037</cell></row><row><cell>γ i</cell><cell>2.2102</cell></row><row><cell>n i</cell><cell>1.4072</cell></row><row><cell>γ kv</cell><cell>0.9578</cell></row><row><cell>n kv</cell><cell>2.4185</cell></row><row><cell>C T</cell><cell>0.0598</cell></row><row><cell>γ D</cell><cell>0.5068</cell></row><row><cell>γ N</cell><cell>0.3439</cell></row><row><cell>γ</cell><cell>0.5907</cell></row><row><cell>b</cell><cell>1.1277</cell></row></table><note><p>M Empirical N eff</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We study KV, rather than QKV, because understanding scaling in the KV cache alone is important for many inference settings. For pretraining claims in Section 4.3, we quantize the entire attention computation, including queries, finding additionally quantizing the query vectors makes a negligible difference to scaling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In practice, the gains are less than linear due to systems overhead.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>An important subtlety here is that since models are overtrained for inference, we want to keep the cost of a forward pass-which is proportional to N P -fixed, not just N . While N P is the same for both a model of N0 parameters in 16-bit and one with 2N0 parameters in 8-bit, the latter has higher N eff with our γ, so will reach a lower pretraining loss on the same data with the same training/inference costs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>Tanishq Kumar thanks <rs type="person">Tim Dettmers</rs>, <rs type="person">Chris De Sa</rs>, <rs type="person">Neil Band</rs> and <rs type="person">Luke Bailey</rs> for helpful comments and discussion, as well as <rs type="person">Ludwig Schmidt</rs> for spotting an early typo. <rs type="person">Blake Bordelon</rs> is supported by a <rs type="grantName">Google PhD Fellowship</rs>. <rs type="person">Cengiz Pehlevan</rs> is supported by <rs type="funder">NSF</rs> grant <rs type="grantNumber">DMS-2134157</rs>, <rs type="funder">NSF</rs> <rs type="programName">CAREER Award IIS-2239780</rs>, and a <rs type="grantName">Sloan Research Fellowship</rs>. This work has been made possible in part by a gift from the <rs type="funder">Chan Zuckerberg Initiative Foundation</rs> to establish the <rs type="projectName">Kempner Institute for the Study of Natural and Artificial Intelligence</rs>. <rs type="person">Aditi Raghunathan</rs> acknowledges support from <rs type="grantNumber">AI2050</rs> program by <rs type="person">Schmidt Sciences</rs> (Grant <rs type="grantNumber">G2264481</rs>), <rs type="funder">Google</rs> <rs type="grantName">Research Scholar</rs>, <rs type="funder">Apple</rs>, <rs type="funder">NSF</rs>, <rs type="funder">Cisco</rs>. We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF2247015</rs> (<rs type="affiliation">Hardware-Aware</rs>), <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. <rs type="grantNumber">W911NF-23-2-0184</rs> (Longcontext) and <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="projectName">Interactive Human-AI Teaming</rs>); ONR under Nos. <rs type="grantNumber">N000142312633</rs> (<rs type="affiliation">Deep Signal Processing</rs>); <rs type="funder">Stanford HAI</rs> under No. <rs type="grantNumber">247183</rs>; <rs type="affiliation">NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative (SDSI)</rs>. <rs type="person">Benjamin F. Spector</rs> is supported by a <rs type="funder">Hertz Fellowship</rs>.</p><p>The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of <rs type="funder">NIH</rs>, <rs type="institution">ONR</rs>, or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_D7WdcnH">
					<idno type="grant-number">DMS-2134157</idno>
					<orgName type="grant-name">Google PhD Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_8Dbsy5x">
					<orgName type="grant-name">Sloan Research Fellowship</orgName>
					<orgName type="program" subtype="full">CAREER Award IIS-2239780</orgName>
				</org>
				<org type="funded-project" xml:id="_FK56f7t">
					<idno type="grant-number">AI2050</idno>
					<orgName type="project" subtype="full">Kempner Institute for the Study of Natural and Artificial Intelligence</orgName>
				</org>
				<org type="funding" xml:id="_x9faxBx">
					<idno type="grant-number">G2264481</idno>
					<orgName type="grant-name">Research Scholar</orgName>
				</org>
				<org type="funding" xml:id="_5j9WEbr">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_TH5HGKD">
					<idno type="grant-number">CCF2247015</idno>
				</org>
				<org type="funding" xml:id="_EXzEANk">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_MZGjTWu">
					<idno type="grant-number">W911NF-23-2-0184</idno>
				</org>
				<org type="funded-project" xml:id="_wHddeHR">
					<idno type="grant-number">W911NF-21-2-0251</idno>
					<orgName type="project" subtype="full">Interactive Human-AI Teaming</orgName>
				</org>
				<org type="funding" xml:id="_EXc42gN">
					<idno type="grant-number">N000142312633</idno>
				</org>
				<org type="funding" xml:id="_4Ad4wyJ">
					<idno type="grant-number">247183</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Hyperparameter Details and Ablations</head><p>We launch over 20 runs for each (N, D) combination to study scaling in precision, trained and validated on the common crawl split of the Dolma dataset <ref type="bibr" target="#b26">[Soldaini et al., 2024]</ref>. We use a standard causal Transformer++ implementation: SwiGLU activations <ref type="bibr" target="#b32">[Shazeer, 2020]</ref>, RoPE embeddings <ref type="bibr" target="#b33">[Su et al., 2021]</ref>, RMSLayerNorm, Adam β values of (0.9, 0.95). We adopt a cosine learning rate schedule with 10% warmup period and peak learning rate of 6e-4 for the smallest model and learning rates scaled with width and depth according to depth-µP for the larger models <ref type="bibr" target="#b34">[Yang et al., 2022</ref><ref type="bibr" target="#b35">, Bordelon et al., 2023]</ref>. We use a sequence length of 1024 and batch size of 256 throughout, with Adam ϵ 1e-15, following <ref type="bibr">[Wortsman et al., 2023a]</ref>. We use weight decay of 0.1, as <ref type="bibr" target="#b37">[Ahmadian et al., 2023]</ref> find some results in the quantization literature may be artifacts of insufficient weight decay. We follow <ref type="bibr" target="#b5">[Ma et al., 2024]</ref> in including a LayerNorm before projections because they find it is important for low precision training to be stable. These are the hyperparameters and settings used for the main scaling law experiments.</p><p>To check robustness, we then ablate these hyperparameter choices, with results in Figure <ref type="figure">8</ref>. In our ablation we use a sequence length of 512 with batch size 128, weight decay of 1e-3, Adam ϵ of 1e-10, a peak learning rate of 1e-4 and a warmup period of duration 3%. We train models with these alternative hyperparameters at various weight, activation, and KV cache precisions. We train and val on C4 <ref type="bibr" target="#b38">[Raffel et al., 2020</ref><ref type="bibr" target="#b39">, Dodge et al., 2021]</ref> instead. Though these ablations are at rather smaller scale due to compute constraints, the loss curves follow the same trends -rapid decrease in final loss with an initial increase in precision from 4 bits, then diminishing returns as we approach higher precision -as in the main text, suggesting the trends are robust to hyperparameter choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Related Work</head><p>Efficient training and inference Low precision has been key to improving the efficiency of training and using LLMs <ref type="bibr" target="#b40">[Micikevicius et al., 2017</ref><ref type="bibr" target="#b41">, Shoeybi et al., 2019</ref><ref type="bibr">, Wortsman et al., 2023b</ref><ref type="bibr" target="#b43">, Zhu et al., 2023]</ref>. Prior works generally study either precision during training <ref type="bibr">[Courbariaux et al.,</ref>   any reasonably training regime, where pretraining costs do matter, even if less than inference costs. But the empirical trends in N * , P * showcase how overtraining with post-train quantization in mind can outperform vanilla overtraining without accounting for its effects on post-train quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Replicating PTQ Scaling with other Quantization Methods</head><p>Here we replicate the finding that post-train degradation due to post-train quantization increases with token/parameter ratio as D γ D /N γ N . We fit the same functional form as in the main text, but get slightly different values of fitted constants, as expect. We replicate on AWQ <ref type="bibr" target="#b9">[Lin et al., 2023]</ref> and round-to-nearest quantization. The former is a modern and sophisticated technique, and the latter a simple and naïve approach to quantization. The fact they, as well as GPTQ in the main text, share the same failure modes suggests that poor post-training quantization data scaling should be the default expectation for any newly proposed PTQ technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PTQ: Learning Rate Schedule Ablation</head><p>Here, we ablate our learning rate and schedule to use warmup with linear decay, as opposed to a cosine schedule, to check it is not an artifact of our choice of learning rate schedule. We do so on our 30M model due to compute constraints, finding the degradation with token/parameter ratio persists, as expected. See Figure <ref type="figure">12</ref>.</p><p>H Why do language models get more sensitive with overtraining?</p><p>This section is speculative. Sharpness. A canonical line of work in optimization demonstrates that model sharpness increases during learning until it hovers at a maximal value (the "edge of stability") <ref type="bibr">[Cohen et al.,</ref>     </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fp8 formats for deep learning</title>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusan</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cornea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Grisenthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kamalu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.05433</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17764</idno>
		<title level="m">The era of 1-bit llms: All large language models are in 1.58 bits</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bitnet: Scaling 1-bit transformers for large language models</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11453</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gptq: Accurate post-training quantization for generative pre-trained transformers</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Saleh Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17323</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">8-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30318" to="30332" />
		</imprint>
	</monogr>
	<note>Gpt3. int8 (</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Awq: Activationaware weight quantization for llm compression and acceleration</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>arxiv. MLSys 2024</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smoothquant: Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickael</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="38087" to="38099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining neural scaling laws</title>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page">2311878121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A dynamical model of neural scaling laws</title>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01092</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Licong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08466</idno>
		<title level="m">Scaling laws in linear regression: Compute, parameters, and data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The case for 4-bit precision: k-bit inference scaling laws</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7750" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 3.3, knowledge capacity scaling laws</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.05405</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Chinchilla scaling: A replication attempt</title>
		<author>
			<persName><forename type="first">Tamay</forename><surname>Besiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ege</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10102</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Beyond chinchilla-optimal: Accounting for inference in language model scaling laws</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00448</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Language models scale reliably with over-training and on downstream tasks</title>
		<author>
			<persName><forename type="first">Yitzhak</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Keh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08540</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gemma 2: Improving open language models at a practical size</title>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Ramé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling llm test-time compute optimally can be more effective than scaling model parameters</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03314</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large language monkeys: Scaling inference compute with repeated sampling</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21787</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Zitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Band</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.07431</idno>
		<title level="m">Synthetic continued pretraining</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling laws of synthetic images for model training... for now</title>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7382" to="7392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Comprehensive exploration of synthetic data generation: A survey</title>
		<author>
			<persName><forename type="first">André</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leppich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kounev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Leznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Chard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02524</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quip: 2-bit quantization of large language models with guarantees</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher M De</forename><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangdong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Magno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><surname>Billm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04291</idno>
		<title level="m">Pushing the limit of post-training quantization for llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Demystifying the nvidia ampere architecture through microbenchmarking and instruction-level analysis</title>
		<author>
			<persName><forename type="first">Hamdy</forename><surname>Abdelkhalik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehia</forename><surname>Arafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandakishore</forename><surname>Santhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Hameed A</forename><surname>Badawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohun</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.17146</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Rui-Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Sifferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sheaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">K</forename><surname>Eshraghian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02528</idno>
		<title level="m">Scalable matmul-free language modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Roformer: enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Noci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Mufan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><surname>Pehlevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16620</idno>
		<title level="m">Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Novak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14322</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Intriguing properties of quantization at scale</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Venkitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Stephen Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34278" to="34294" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Documenting large webtext corpora: A case study on the colossal clean crawled corpus</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08758</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stable and low-precision training for large-scale vision-language models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10271" to="10298" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A survey on model compression for large language models</title>
		<author>
			<persName><forename type="first">Xunyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07633</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7024</idno>
		<title level="m">Training deep neural networks with low precision multiplications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Qlora: Efficient finetuning of quantized llms</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02861</idno>
		<title level="m">8-bit optimizers via block-wise quantization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ultra-low precision 4-bit training of deep neural networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaoutar</forename><forename type="middle">El</forename><surname>Maghraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Viji Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1796" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Llm-qat: Data-free quantization aware training for large language models</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17888</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Awq: Activation-aware weight quantization for on-device llm compression and acceleration</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="87" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Flexgen: High-throughput generative inference of large language models with a single gpu</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="31094" to="31116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Spqr: A sparse-quantized representation for near-lossless llm weight compression</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Svirschevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vage</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kuznedelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Saleh Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Borzunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.03078</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><surname>Tweet</surname></persName>
		</author>
		<ptr target="https://twitter.com/kellerjordan0/status/1837874116533407990" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<idno>10:S0140525X16001837</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<title level="m">A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Crosslingual generalization through multitask finetuning</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01786</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02060</idno>
		<title level="m">Open mixture-of-experts language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Munoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.03988</idno>
		<title level="m">Santacoder: don&apos;t reach for the stars! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161</idno>
		<title level="m">Starcoder: may the source be with you! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Pykhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.19173</idno>
		<title level="m">Starcoder 2 and the stack v2: The next generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Ville</forename><surname>Risto Luukkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jouni</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Eskelinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna-Mari</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Kupari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><surname>Muennighoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05640</idno>
		<title level="m">Aleksandra Piktus, et al. Fingpt: Large generative models for a small language</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viraat</forename><surname>Aryabumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gbemileke</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Onilude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivalika</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Lee</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><surname>Kayid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07827</idno>
		<title level="m">Aya model: An instruction finetuned open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Observational scaling laws and the predictability of language model performance</title>
		<author>
			<persName><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.10938</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Scaling laws and compute-optimal training beyond fixed training durations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bakouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18392</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10551</idno>
		<title level="m">Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Scaling laws for fine-grained mixture of experts</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ludziejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Adamczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Pióro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha</forename><forename type="middle">L</forename><surname>Krutul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Antoniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Ciebiera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krystian</forename><surname>Król</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Odrzygóźdź</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Sankowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07871</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Scaling laws with vocabulary: Larger models deserve larger vocabularies</title>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.13623</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unified scaling laws for routed language models</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><surname>Borgeaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4057" to="4086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Transcending scaling laws with 0.1% extra compute</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdhery</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11399</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">What language model to train if you have one million gpu hours</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.15424</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Cheah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Ferdinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemys</forename><surname>Law Kazienko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.05892</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Scaling laws for generative mixed-modal language models</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="265" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Revisiting neural scaling laws in language and vision</title>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Ibrahim M Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22300" to="22312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Reproducible scaling laws for contrastive language-image learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2818" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07682</idno>
		<title level="m">Emergent abilities of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Scaling laws for downstream task performance of large language models</title>
		<author>
			<persName><forename type="first">Berivan</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Hazimeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Paparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04177</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Datacomp-lm: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11794</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.01492</idno>
		<title level="m">Regmix: Data mixture as regression for language model pre-training</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">A survey on data selection for language models</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16827</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Integer quantization for deep learning inference: Principles and empirical evaluation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Isaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09602</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Gradient descent on neural networks typically occurs at the edge of stability</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cardoze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04369</idno>
		<title level="m">A loss curvature perspective on training instability in deep learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks</title>
		<author>
			<persName><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Mondelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8119" to="8129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabarish</forename><surname>Sainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.12147</idno>
		<title level="m">The onset of variance-limited behavior for networks in the lazy and rich regimes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The staircase property: How hierarchical structure can guide deep learning</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Boix-Adsera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Nagaraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26989" to="27002" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Boix Adsera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4782" to="4887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Hidden progress in deep learning: Sgd learns parities near the computational limit</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21750" to="21764" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">In this work we study both, the precision during training and after, and unify them from a scaling perspective. Other important works include recent popular work on quantization-aware-training [Ma et al., 2024] where weights are quantized to extreme precisions (ternary) on the forward pass during training. This work is consistent with ours in that they can quantize weights so aggressively because weights are less sensitive than activations or KV cache. Further, while we use a fixed architecture throughout to maintain a controlled comparison across precision, they use a nonstandard architecture, learning rate, and weight decay schedule specifically to make training with ternary weights stable. Finally, [Keller, 2018] find similar tradeoffs for floating-point type with weights-only quantization. Large language models and scaling By scaling up the transformer architecture [Vaswani et al., 2017] a variety of large language models have been proposed</title>
		<author>
			<persName><surname>Dettmers</surname></persName>
		</author>
		<editor>Tay et al., 2022a, Krajewski et al., 2024, Tao et al., 2024, Clark et al., 2022, Tay et al., 2022b, Scao et al.</editor>
		<imprint>
			<date type="published" when="2014">2014. 2024. 2021. 2022. 2024. 2022. 2023. 2023. 2023. 2024. 2022. 2023. 2022. 2022</date>
			<publisher>Srivastava et al</publisher>
		</imprint>
	</monogr>
	<note>Sun et al., 2020, Liu et al., 2023] or the effects of changing the precision after training (post-training quantization Deitke et al., 2024 To improve our understanding of these models, various works have investigated their scaling properties Ruan et al., 2024, Allen-Zhu and Li, 2024, Hägele et al., 2024 Many aspects are relevant to scaling including the architecture Peng et al., 2024], the modalities considered Alabdulmohsin et al., 2022, Cherti et al., 2023], the performance metrics Isik et al., 2024], the data composition [Li et al., 2024, Liu et al., 2024, Albalak et al., 2024] and data repetitions [Muennighoff et al., 2024b Our work analyzes one such aspect which is key to better scaling: the numeric precision during and after training</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
