- Decision on the precision levels for training and inference (e.g., BF16, FP8, FP4)
- Choice of quantization methods (e.g., post-training quantization vs. quantization-aware training)
- Determination of effective parameter count (N_eff) formulation
- Selection of functional forms for scaling laws
- Strategy for data scaling in relation to model size
- Approach to handling overtraining in model training
- Decision on the dataset used for training (e.g., Dolma V1.7)
- Choice of model architecture (e.g., Transformer++)
- Methodology for fitting constants in scaling laws
- Criteria for evaluating loss degradation from quantization
- Framework for analyzing compute-optimal precision
- Assumptions regarding the relationship between data size and model performance
- Guidelines for conducting pretraining runs and parameter sweeps
- Decisions on the number of model sizes and data budgets to explore
- Approach to documenting and reporting experimental results
- Considerations for hardware compatibility with low-precision training