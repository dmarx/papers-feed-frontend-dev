- **DIAMOND Overview**: DIffusion As a Model Of eNvironment Dreams, a reinforcement learning agent utilizing a diffusion world model for improved sample efficiency and performance in Atari environments.

- **Key Contribution**: Introduces a diffusion-based approach to world modeling, addressing limitations of discrete latent variable models by preserving visual details crucial for agent performance.

- **Performance Benchmark**: Achieves a mean human normalized score of 1.46 on the Atari 100k benchmark, setting a new state of the art for agents trained entirely within a world model.

- **World Model Definition**: Models the environment as a Partially Observable Markov Decision Process (POMDP) defined by (S, A, O, T, R, O, γ), where:
  - S = set of states
  - A = set of discrete actions
  - O = set of image observations
  - T = transition function
  - R = reward function
  - γ = discount factor

- **Diffusion Process**: Described by the stochastic differential equation (SDE):
  \[
  dx = f(x, τ)dτ + g(τ)dw
  \]
  - **Reverse Process SDE**:
  \[
  dx = [f(x, τ) - g(τ)^2 ∇_x log p_τ(x)]dτ + g(τ)dw
  \]

- **Score Matching Objective**: Utilizes score matching to train a score model from data samples without knowledge of the underlying score function:
  \[
  L(θ) = E \|S_θ(x_τ, τ) - ∇_x log p_{0τ}(x_τ | x_0)\|^2
  \]

- **Conditional Generative Model**: For world modeling, the objective is modified to:
  \[
  L(θ) = E \|D_θ(x_{τ}^{t+1}, τ, x_{0 ≤ t}, a_{≤ t}) - x_{0}^{t+1}\|^2
  \]

- **Training Objective**: The network prediction is defined as:
  \[
  D_θ(x_{τ}^{t+1}, y_{τ}^{t}) = c_{τ}^{skip} x_{τ}^{t+1} + c_{τ}^{out} F_θ(c_{τ}^{in} x_{τ}^{t+1}, y_{τ}^{t})
  \]

- **Noise Schedule**: The perturbation kernel is defined as:
  \[
  p_{0τ}(x_{τ}^{t+1} | x_{0}^{t+1}) = N(x_{τ}^{t+1}; x_{0}^{t+1}, σ^2(τ)I)
  \]

- **U-Net Architecture**: Utilizes a U-Net for the vector field \( F_θ \) to model the diffusion process, incorporating past observations and actions for conditioning.

- **Sampling Method**: Euler's method is effective for generating the next observation from the trained diffusion model, balancing quality and computational cost.

- **Release of Resources**: Code, agents, videos, and playable world models are available at [https://diamond-wm.github.io](https://diamond-wm.github.io).