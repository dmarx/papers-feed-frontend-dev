# Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping

## Abstract

## 

Figure 1. Our method Scaffold-SLAM achieves high-quality photorealistic mapping with quality outperforms state-of-the-art methods (GS-ICP SLAM [1], Photo-SLAM [2], SplaTAM [3], MonoGS [4]) across monocular, stereo, and RGB-D cameras. Top: The results are from TUM RGB-D datasets for RGB-D camera. Bottom: The left three images stemming from Replica datasets for monocular camera and the right three from EuRoC MAV datasets for stereo camera. Non-obvious difference in quality highlighted by insets.

## Introduction

Visual SLAM is a fundamental problem in 3D computer vision, with wide applications in autonomous driving, robotics, virtual reality, and augmented reality. SLAM aims to construct dense or sparse maps to represents the scene. Recently, neural rendering [[5]](#b4) has been integrated into SLAM pipelines, significantly enhancing the scene representation capabilities of the maps. The latest advancement in radiance field rendering is 3D Gaussian Splatting (3DGS) [[6]](#b5), an explicit scene representation that achieves revolutionary improvements in rendering and training speed. Recent SLAM works [[1]](#b0)[[2]](#b1)[[3]](#b2)[[4]](#b3)[[7]](#b6)[[8]](#b7)[[9]](#b8) incorporating 3DGS have demonstrated that explicit representations provide more promising rendering performance compared to implicit representations.

However, current SLAM methods leveraging 3DGS have yet to achieve high-quality rendering across monocular, stereo, and RGB-D cameras simultaneously. Most existing approaches only support RGB-D cameras. For example, SplaTAM [[3]](#b2) jointly optimizes the camera pose and the Gaussians by minimizing the image and depth reconstruction errors, achieving localization and rendering for RGB-D cameras. GS-SLAM [[7]](#b6) derives an analytical formulation for optimizing camera pose tracking and dense mapping with RGB-D re-rendering loss. RTG-SLAM [[8]](#b7) proposes a efficient pipeline to derive a compact Gaussian representation, resulting a real-time RGB-D system. GS-ICP SLAM [[1]](#b0) propose a novel dense RGB-D SLAM approach with a fusion of Generalized Iterative Closest Point (ICP) and 3DGS. CG-SLAM [[9]](#b8) employs an uncertainty-aware 3D Gaussian field to achieve efficient RGB-D SLAM.

There are a few methods supporting monocular, stereo, and RGB-D cameras. MonGS [[4]](#b3) formulates camera tracking for 3DGS using direct optimization against the 3D Gaussians, allowing localization and photorealistic mapping for all three types of cameras. Unfortunately, its rendering quality gap between stereo and monocular cameras is significant. Photo-SLAM [[2]](#b1) introduces a decoupled framework to optimize 3D Gaussians, achieving real-time localization and photorealistic mapping for monocular, stereo, and RGB-D cameras. While it demonstrates strong realtime performance and a minimal gap in rendering quality between monocular and RGB-D cameras, its primary limitation lies in the overall rendering quality. Our work aims to significantly improve the rendering accuracy for monocular, stereo, and RGB-D cameras.

In this paper, we propose Scaffold-SLAM, a novel SLAM system that achieves simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. Our approach shares the same decoupled framework as Photo-SLAM [[2]](#b1), where we utilize a traditional indirect visual SLAM pipeline for localization and geometric mapping. The generated point cloud is used to initialize structured 3D Gaussians. Instead, we introduce two key innovations that enable our method to achieve state-of-the-art photorealistic mapping quality across monocular, stereo, and RGB-D cameras. First, we propose Appearance-from-Motion embedding, which models appearance variations such as exposure and lighting in a learned low-dimensional latent space. We train the embedding to predict the appearance variations across diverse images with the camera pose. Second, we propose a frequency regularization pyramid that constrains the frequencies of rendered image across multiple scales in the frequencies domain. This encourages 3D Gaussians to grow towards complex regions, such as object edges and textures, enabling the model to capture high-frequency details in the scene. Finally, to evaluate the photorealistic mapping quality of our method, we conduct extensive experiments across diverse datasets, including monocular, stereo, and RGB-D cameras. The experimental results show that our approach, Scaffold-SLAM, surpasses state-of-the-art methods in photorealistic mapping quality across all three camera types. The main contributions of this work are as follows: 1. We develop an Appearance-from-Motion embedding to enable our SLAM system to effectively model image appearance variations across diverse images. 2. We propose a frequency regularization pyramid to guide the growth of 3D gaussians toward complex regions to capture finer details in the scene. 3. Extensive evaluations on various datasets demonstrate that our method, Scaffold-SLAM, achieves superior photorealistic mapping quality across monocular, stereo, and RGB-D cameras, while maintaining competitive tracking accuracy. The code will be publicly available.

## Related Work

Visual SLAM. Traditional visual SLAM methods can be broadly classified into two categories: direct methods and indirect methods. Indirect methods rely on extracting and tracking features between consecutive frames to estimate pose and build sparse maps by minimizing the reprojection error. Examples include PTAM [[10]](#b9) and ORB-SLAM3 [[11]](#b10). Direct methods [[12]](#b11)[[13]](#b12)[[14]](#b13), on the other hand, bypass feature extraction and estimate motion and structure by minimizing photometric error, which can build sparse or semi-dense maps. The first dense visual SLAM is Kinect-Fusion [[15]](#b14), which updates the scene using a TSDF representation. Recently, some methods [[16]](#b15)[[17]](#b16)[[18]](#b17)[[19]](#b18)[[20]](#b19) have integrated deep learning into visual SLAM systems. DPVO [[21]](#b20) extends the current state-of-the-art method Droid-SLAM [[19]](#b18) by leveraging the efficiency of sparse block matching, improving computational performance. More recently, Lipson et al. [[20]](#b19) couple optical flow prediction with a pose-solving layer to achieve camera tracking. Our approach favors traditional indirect SLAM for the following insight. Indirect SLAM shares a highly similar pipeline with Structure-from-Motion (SfM), including feature matching, tracking, and bundle adjustment, leading to point cloud with similar intrinsic properties. Since the 3D Gaussians in [[6]](#b5) are initialized by point cloud generated frome SfM, we believe that initializing our Gaussians using point cloud obtained from indirect SLAM is an optimal choice. Implicit Representation based SLAM. The first to introduce radiance field rendering into SLAM systems is neural implicit representations. iMAP [[22]](#b21) pioneers the use of neural implicit representations to achieve tracking and mapping through reconstruction error. Subsequently, many works [[23]](#b22)[[24]](#b23)[[25]](#b24)[[26]](#b25)[[27]](#b26)[[28]](#b27)[[29]](#b28)[[30]](#b29)[[31]](#b30) have explored new representation forms. For instance, Vox-Fusion [[24]](#b23) proposes a voxel-based neural implicit surface representation. ESLAM [[26]](#b25) represents scenes using multi-scale axis-aligned perpendicular feature planes. Point-SLAM [[32]](#b31) adopts a point-based neural implicit representation and achieved far superior rendering quality compared to previous methods. Recently, SNI-SLAM [[28]](#b27) and IBD-SLAM [[30]](#b29) introduce a hierarchical semantic representation and an xyz-map representation, respectively. Some works [[33]](#b32)[[34]](#b33)[[35]](#b34)[[36]](#b35)[[37]](#b36)[[38]](#b37)[[39]](#b38) have investigated other challenges. GO-SLAM [[33]](#b32) integrates Droid-SLAM, while Loopy-SLAM [[34]](#b33) addresses loop closure. However, only Point-SLAM explores novel view synthesis, while others focus on geometric reconstruction.

3D Gaussian Splatting based SLAM. Recently, several works have introduced an explicit representation, 3DGS [[6]](#b5), into visual SLAM systems, achieving both localization and photorealistic mapping. Thanks to the fast training and rendering speed of 3DGS, as well as its excellent rendering quality, these methods have demonstrated superior photorealistic mapping quality and rendering speed compared to various implicit representation approaches [[23-27, 32, 33]](#), including Point-SLAM [[32]](#b31). Most of these methods are RGB-D SLAM systems. For example, SplaTAM [[3]](#b2) and GS-SLAM [[7]](#b6) both optimize camera poses and mapping by minimizing image and depth rendering errors. RTG-SLAM [[8]](#b7) explores the efficiency of Gaussian representations. GS-ICP [[1]](#b0) achieves high-quality photorealistic mapping by fusing 3DGS with Generalized ICP on depth points. CG-SLAM [[9]](#b8) examines the uncertainty in RGB-D sensors. Some methods support monocular, stereo, and RGB-D cameras simultaneously. MonoGS [[4]](#b3) formulates directly estimating camera poses by optimizing 3D Gaussians. However, its rendering quality performs poorly on monocular cameras. Photo-SLAM [[2]](#b1) proposes a real-time, decoupled system for localization and photorealistic mapping. Photo-SLAM excels in impressive real-time performance and resource efficiency, but at the cost of significantly reduced rendering quality. Our proposed Scaffold-SLAM aims to achieve higher-quality photorealistic mapping for monocular, stereo, and RGB-D cameras while maintaining competitive localization accuracy. Similar to Photo-SLAM, our method does not involve reconstructing a dense mesh.

In concurrent works, OG-Mapping [[40]](#b39) is the RGB-D SLAM leveraging scene structure. LoopSplat [[41]](#b40) and GLC-SLAM [[42]](#b41) explore loop closure detection using 3DGS. IG-SLAM [[43]](#b42) specializes in monocular camera. MGSO [[44]](#b43) and GEVO [[45]](#b44) investigate efficiency. Hi-SLAM [[46]](#b45) focus on semantic information.

## Proposed Method

In this section, we present details of our Scaffold-SLAM. The overview of our SLAM system is summarized in Fig. [2](#fig_0).

## Localization and Geometry Mapping

Since traditional indirect SLAM pipelines are highly similar to SfM, the generated point cloud exhibit robust geometric structure. Thus, we follow the traditional indirect SLAM approach, optimizing the camera orientation R ∈ SO(3) and position t ∈ R 3 through motion-only bundle adjustment (BA). The camera poses {R, t} are optimized by minimizing the reprojection error between the matched 3D points P i ∈ R 3 and 2D feature points p i within a sliding window:

${R, t} = i∈X argmin Ri,ti ρ(∥p j -π(R i P j + t i )∥ 2 Σg ) (1)$where X represents the set of all mathches, Σ g denotes the covariance matrix associated with the keypoint's scale, π is the projection function, and ρ is the robust Huber cost function.

We perform a local BA by optimizing a set of covisible keyframes K L alone with the set of points P L observed in those keyframes as follows:

${P i , R l , t l } = argmin P i ,R l ,t l k∈K L ∪K F j∈X k ρ(E(k, j)) (2) E(k, j) = ∥p j -π(R k P j + t k )∥ 2 Σg (3)$where i ∈ P L , l ∈ K L , K F are all other keyframes, and X k is the set of matches between keypoints in keyframe k and points in P L .

Global BA is a special case of local BA, where all keyframes and map points are included in the optimization, except the origin keyframe, which is kept fixed to prevent gauge freedom. After performing local or global BA, we can obtain more accurate poses and map point cloud.

## High-quality Photorealistic Mapping

Following 3DGS, our rendering process is as follows:

$C(R, t) = i∈N c i δ i i-1 j=1 (1 -δ j ) (4$$)$where N is the number of ordered 2D Gaussians overlapping the pixel, δ i = α i • G(R, t, P i , q i , s i ), and G denotes splatting process of 3DGS [[6]](#b5). The parameters of 3D Gaussians include color c, position P, scaling matrix s, rotation matrix q, and opacity α. Inspired by [[47]](#b46), we incrementally construct a sparse grid of anchor points, initialized by voxelizing the point cloud obtained from the geometric mapping. Each anchor point distributes k 3D Gaussians, the color of which is obtained as follows:

${c 0 , . . . , c k-1 } = M c ( fv , δ vc , ⃗ d vc , ℓ (a) R, t )(5)$where δ vc , ⃗ d vc are relative distance and viewing direction of the anchor point, fv is a local context feature, and M c is a individual multi-layer perceptron (MLP). α, q, and s are similarly obtained by individual MLPs, denoted as M α , M q , and M s , respectively. Only the color c incorporates Appearance-from-Motion embedding ℓ (a) R, t .

## Appearance-from-Motion Embedding

To handle photometric variations, appearance embedding is a proven effective technique. It originates from a generative network called Generative Latent Optimization [[48]](#b47). NeRF-W [[49]](#b48) is the first to introduce this technique into neural rendering. By learning a shared appearance representation across all images, it models per-image photometric and environmental variations in a low-dimensional latent space. Each image is assigned a corresponding appearance embedding real-valued vector ℓ (a) i of length n (a) which takes the index i of each image as input. Scaffold-GS [[47]](#b46) also incorporates this technique to enhance the rendering quality. However, in NeRF-W, the embedding ℓ using the left half of the groundtruth image to match the appearance and evaluates metrics on the right half. In the novel view synthesis dataset, there are typically hundreds of views, with approximately 20% allocated to the test-set, making the computational cost of this approach acceptable. However, in SLAM datasets, which contain thousands of images with around 80% being test-set, this cost becomes prohibitive. Scaffold-GS addresses this by randomly selecting a training image index as input for test images. Nevertheless, this approach performs poorly for tasks like SLAM, where the test-set is large. Because the trained views are too sparse relative to the novel views, making it difficult to predict the appearance of novel views.

Fortunately, our focus is on photorealistic mapping of SLAM. It naturally follows that we would use the estimated camera pose corresponding to each training image as the input for appearance embedding, rather than the image index. Our insight is that the camera poses optimized through global bundle adjustment conform to the same maximum a posteriori (MAP) probability distribution. Moreover, each image corresponding to a unique camera pose, which shares analogous properties with the camera indices. Therefore, we can train a network to learn a shared appearance representation across all training camera poses, capturing the underlying probabilistic distribution of these poses. Since the camera poses of test-set also belong to this distribution, the network can predict the appearance for the novel views. We refer to this method as Appearance-from-Motion embedding. We employ a tiny MLP to model the appearance variation of each image based on its camera pose in a lowdimensional latent space, as shown below:

$ℓ (a) R, t = MLP Θa (R, t)(6)$Subsequently, this Appearance-from-Motion embedding ℓ

$(a)$R, t is fed into the color decoder M c , allowing all pixels in an image to share the same appearance representation. Ultimately, the appearance of all images is modeled within a continuous latent space, as illustrated in the Fig. [6](#fig_6). We emphasize once again that our Appearance-from-Motion embedding does not require training on the test-set.

## Frequency regularization Pyramid

To improve the quality of the photorealistic mapping, another challenge is the poor performance of the rendered images in capturing high-frequency details, particularly around object edges and regions with complex textures. Some prior works offer potential solutions: HF-NeuS [[50]](#b49) employs a coarse-to-fine strategy to decompose SDF into base and displacement functions, gradually enhancing highfrequency details. AligNeRF [[51]](#b50) improves upon the perceptual loss proposed by Johnson [[52]](#b51) to enhance highfrequency rendering details. FreGS [[53]](#b52) introduces a strategy combining frequency regularization with frequency annealing. They decompose the high and low frequencies in the frequency domain, allowing the Gaussians to densify more efficiently. We find that this frequency regularization approach can control the growth of anchor points. However, its limitation is that the model can only learn high-frequency details through single-scale features.

To fully leverage multiple levels of detail, we propose a novel frequency regularization technique called the frequency regularization pyramid. To construct images at different scales, we apply bilinear interpolation to smoothly downsample both the ground truth and rendered images.

During training, we supervise the model with the frequency from multi-scale images. Specifically, Let s ∈ S = {s 0 , s 1 , . . . , s n } denotes the scale of an image. For all scales of both the rendered images I s r and ground truth images I s g , we first apply a 2D Fast Fourier Transform (FFT) to obtain the frequency spectra F(I s r )(u, v), F(I s g )(u, v). We then use a high-pass filter H f (u, v) on these spectra to extract the high-frequency F s r,f (u, v), F s g,f (u, v). The loss L vol is computed based on the difference in highfrequency spectra between the rendered and ground truth images across different scales, as shown below:

$L f = λ h s∈S 1 N u,v F s r,h (u, v) -F s g,j (u, v)(7)$$F s i,f (u, v) = H f (u, v) • F(I s i )(u, v), f ∈ {h, l}, i ∈ {r, g}$where N = HW denotes the size of the image. Unlike FreGS, we do not use low-frequency information to supervise our model. This is because low-frequency components are typically used to encourage the model to learn the overall structure of the environment. However, in our case, the geometric mapping process already generates a wellstructured point cloud that is used to initialize the Gaussians. Moreover, we employ an annealing strategy to guide the supervision of the frequency regularization pyramid.

Since the SLAM system incrementally builds the scene and requires time for the environment structure to stabilize, we introduce frequency regularization pyramid only after a certain number of iterations T s . Furthermore, as the scene's structure and details become stable toward the end of optimization, we stop applying frequency supervision after T e iterations.

## Optimization

Finally, the optimization of the learnable parameters in the model, the MLP that predicts parameters of 3D Gaussians, and the MLP for Appearance-from-Motion embedding are achieved by minimizing the L1 loss L 1 , SSIM term, frequency regularization L f , and volume regularization L vol between the rendered images and the ground truth images, denoted as

$L = (1 -λ)L 1 + λ(1 -L SSIM ) + λ vol L vol + λ f L f (8)$where

$L vol = Nng i=1 Prod(s i )(9)$Here, Prod(•) denotes product of the values of a vector and N ng presents the number of 3D Gaussians.

## Experiment

## Experiment Setup

Implementation: Our Scaffold-SLAM is fully implemented using the LibTorch framework with C++ and CUDA, and the localization module is based on ORB-SLAM3 [[11]](#b10). The algorithm is developed under the Photo-SLAM [[2]](#b1) framework, with the removal of its geometry densification module and Gaussian pyramid training strategy, as these components did not enhance our method. Except for the non-open-source GS-SLAM [[7]](#b6), all methods compared in this paper are run on the same machine using their official code. The system is equipped with an NVIDIA RTX 4090 24GB GPU and a Ryzen Threadripper Pro 5995WX CPU. By default, our method runs for 30K iterations.

Baseline: For monocular and stereo camera setups, we compare our method with Photo-SLAM [[2]](#b1), MonoGS [[4]](#b3), and Photo-SLAM-30K. For RGB-D cameras, we additionally include comparisons with RTG-SLAM [[8]](#b7), GS-SLAM [[7]](#b6), SplaTAM [[3]](#b2), and GS-ICP SLAM [[1]](#b0), all of which represent state-of-the-art SLAM approaches based on 3DGS.

For camera pose estimation, we also compare with ORB-SLAM3 [[11]](#b10), DROID-SLAM [[19]](#b18), and GO-SLAM [[33]](#b32). While Photo-SLAM [[2]](#b1) achieves strong real-time performance, its photorealistic mapping typically doesn't reach   30K iterations. Since both Photo-SLAM [[2]](#b1) and our method are decoupled approaches, we introduce Photo-SLAM-30K as a baseline, where the number of iterations is fixed at 30K, to demonstrate that our results are not simply due to increasing the iteration count in Photo-SLAM [[2]](#b1).

Metric: We follow the evaluation protocol of MonoGS [[4]](#b3) to assess both camera pose estimation and novel view synthesis. For camera pose estimation, we report the root mean square error (RMSE) of the absolute trajectory error (ATE) [[54]](#b53) for all frames. For photorealistic mapping, we report standard rendering quality metrics, including PSNR, SSIM, and LPIPS [[55]](#b54). To evaluate the photorealistic mapping quality, we only calculate the average metrics over novel views for all methods. To ensure fairness, no keyframes (training views) are included in the evaluation, and for all RGB-D SLAM methods, no masks are applied to either the rendered or ground truth images during metric calculation. As a result, the reported metrics for Photo-SLAM [[2]](#b1) are slightly lower than those in the original paper, as the original averages both novel and training views. Similarly, the metrics of SplaTAM [[3]](#b2) and GS-ICP SLAM [[1]](#b0) are slightly lower than reported, as the original methods use a mask to exclude outliers, removing corresponding RGB pixels from both the rendered and real images based on anomalies in the depth image.

Datasets: We evaluate on well-known TUM RGB-D [[56]](#b55) and Replica [[57]](#b56) datasets for monocular and RGB-D cameras. For stereo cameras, we use the EuRoC MAV dataset [58].

## Results Analysis

Camera Tracking Accuracy As shown in Table [2](#tab_1), our method demonstrates competitive accuracy in tracking for monocular, stereo, and RGB-D camera compared to stateof-the-art methods. This is attributed to the high precision of the ORB-SLAM3 [[11]](#b10)   dataset and obtain the highest rendering accuracy across all sequences in TUM RGB-D. TUM RGB-D is a more challenging dataset compared to Replica, featuring cluttered scenes with various small objects, as well as significant motion blur in RGB images and many holes in depth maps. GS-ICP SLAM is a state-of-the-art method for RGB-D SLAM and achieved second place in rendering accuracy on Replica. However, it is overly reliant on depth maps, resulting in poor performance on TUM RGB-D. In contrast, our Scaffold-SLAM uses depth maps primarily to enhance localization accuracy and geometric mapping results. The appearance variations and high-frequency details in cluttered scenes are effectively handled by the two key elements of our method: appearance-from-Motion embedding and frequency regularization pyramid. Thus, Scaffold-SLAM shows significant improvement on TUM RGB-D. Table [3](#tab_2) records the quantitative rendering results for monocular camera scenes, where Scaffold-SLAM surpasses other methods. Notably, Scaffold-SLAM still significantly outperforms the comparative methods on TUM RGB-D. Importantly, compared to RGB-D scenes, the rendering accuracy of our method does not decrease substantially, whereas Mono-GS experiences a sharp decline. This indicates that our method does not heavily depend on depth image.

While stereo cameras are closer to human vision, progress in this area has been slow. Scaffold-SLAM takes a small step forward in 3DGS-based stereo SLAM. The quantitative results for photorealistic mapping presented in Table [3](#tab_2), where our method also achieves the highest rendering quality, surpassing the current state-of-the-art method, MonoGS. This confirms that our system possesses broader applicability.

Across all scenes, our method greatly outperforms Photo-SLAM-30K, proving that the rendering quality of our method is not simply achieved by increasing the number of iterations. This also validates the effectiveness of Appearance-from-Motion embedding and frequency regu- Table 5. Ablation Study on replacement of key components. Best results are marked as Best score.  larization pyramid. Efficiency Comparison. We evaluate the rendering speed, tracking speed, and mapping time of our method, as shown in Table [1](#tab_0). Our Scaffold-SLAM achieves real-time rendering and tracking speeds. Although our method shows a slight decrease in real-time performance compared to Photo-SLAM [[2]](#b1) and GS-ICP SLAM [[1]](#b0), it maintains an advantage over Mono-GS [[4]](#b3) and SplaTAM [[3]](#b2) in terms of tracking speed, and mapping time, while offering the highest rendering quality.

## Ablation Studies

We isolate two modules from our algorithm: Appearancefrom-Motion embedding and frequency regularization pyramid, and conducted a series of experiments to assess their impacts. Our method without these two modules is referred to as Base, while our complete method is denoted as Ours.

Quantitative results are presented in Replacement of key components. Next, we evaluate the superior performance of our proposed Appearance-from-Motion embedding and frequency regularization pyramid compared to the appearance embedding from Scaffold-GS [[47]](#b46) and the frequency regularization from FreGS [[53]](#b52).

Based on the Base model, we train two additional models: one incorporating appearance embedding and FRP, denoted as Base+AE+FRP, and another with Appearancefrom-Motion embedding and the frequency regularization, denoted as Base+AfME+FR. As shown in Table [5](#), our full method Ours achieves the highest PSNR score. This demonstrates that, compared to appearance embedding, our Appearance-from-Motion embedding is more effective in predicting appearance variations across a wide range of novel views, thus avoiding additional training on the test set. On the other hand, it also highlights that introducing multiscale frequency constraints helps the model better capture high-frequency details in the scene, leading to superior rendering quality.

## Conclusion

In this paper, we introduce Scaffold-SLAM, a SLAM method that achieves high-quality photorealistic mapping for monocular, stereo, and RGB-D cameras. We explore the limits of decoupled approaches by integrating traditional indirect SLAM with a structured 3D Gaussian representation. Extensive experiments demonstrate that our method surpasses coupled approaches in rendering quality across all camera types. We also highlight two key innovations, Appearance-from-Motion embedding and frequency regularization pyramid, that significantly enhance photorealistic mapping quality. By incorporating Appearance-from-Motion embedding, our method successfully predicts substantial appearance variations from fewer training views. The proposed frequency regularization pyramid effectively supervises the optimization and growth of 3D Gaussians, enabling the modeling of more scene details. Future

![Figure 2. Overview of our method. Our method supports monocular, stereo, and RGB-D cameras. The input image stream is processed by the tracking and geometric mapping modules, generating high-quality point cloud and accurate poses. These point cloud are used to incrementally construct Gaussians. The poses are fed into the Appearance-from-Motion embedding to model lighting and other appearance changes in the environment. Additionally, we introduce the frequency regularization pyramid to supervise the training of Gaussians, allowing for improved modeling of high-frequency details in the scene.]()

![Figure 3. Illustration of the appearance variations modeled by Appearance-from-Motion embedding. All images are rendered from novel views with significant lighting changes.]()

![(a) GSICP [1] (b) SplaTAM [3] (c) MonoGS [4] (d) Ours (e) Ground Truth]()

![Figure 4. We show comparisons of ours to state-of-the-art methods for RGB-D camera. The top scene is rooo0 from Replica datasets, and the bottom is fr3 office from TUM RGB-D datasets. Non-obvious difference in quality highlighted by insets.]()

![Figure 5. We show comparisons of ours to state-of-the-art methods for Monocular and Stereo cameras. The top scene is fr3 office from TUM RGB-D datasets, the mid is rooo0 from Replica datasets, and the bottom is V201 easy from EuRoC MAV datasets. Non-obvious difference in quality highlighted by insets.]()

![Figure 6. Illustration of the appearance variations modeled by Appearance-from-Motion embedding. All images are rendered from novel views with significant lighting changes.]()

![Quantitative evaluation of our method compared to state-of-the-art methods for RGB-D camera on Replica and TUM RGB-D datasets. Best results are marked as Best score , second best score and third best score . GS-SLAM * denotes the result of GS-SLAM is taken from[7], all others are obtained in our experiments. '-' denotes the system does not provide valid results.]()

![Camera tracking result on Replica, TUM RGB-D, and EuRoC MAV datasets for Monocular, stereo, and RGB-D cameras. RMSE of ATE (cm) is reported.]()

![algorithm used for localization. Novel View synthesis The quantitative rendering results for novel views in RGB-D scenes are recorded in Table1, where Scaffold-SLAM significantly outperforms the comparative methods, achieving the highest average rendering quality on both TUM RGB-D and Replica datasets. We achieve the best results in most sequences of the Replica Quantitative evaluation of our method compared to state-of-the-art methods for Monocular (Mono) and Stereo cameras on Replica, TUM RGB-D, and EuRoC MAV datasets. Best results are marked as Best score and second best score .]()

![Ablation Study on the key components (Appearancefrom-Motion embedding and frequency regularization pyramid). Best results are marked as Best score.]()

![Based on the Base model, we train two additional models: one with the Appearance-from-Motion embedding added, denoted as Base+AfME, and another with the frequency regularization pyramid, denoted as Base+FRP. As shown in Table4, our complete method Ours surpasses Base, Base+AfME, and Base+FRP in terms of PSNR scores. Furthermore, both Base+AfME and Base+FRP demonstrate PSNR improvements over the Base model. This confirms that Appearance-from-Motion embedding and frequency regularization pyramid are crucial for improving rendering quality. The qualitative results also reveal that Appearance-from-Motion embedding has learned photometric variations across consecutive views, strongly validating the effectiveness of embedding appearance into the low-dimensional pose space. Additionally, we provide a comparison of rendering results with and without the frequency regularization pyramid, where it is evident that the model achieves more realistic object edge modeling after applying frequency regularization.]()

