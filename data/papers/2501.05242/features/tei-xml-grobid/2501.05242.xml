<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-09">9 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianci</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiang</forename><surname>Liu</surname></persName>
							<email>liuzhiang@mail.nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Biao</forename><surname>Lu</surname></persName>
							<email>lubiao@mail.nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongchun</forename><surname>Fang</surname></persName>
							<email>fangyc@nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-09">9 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">D887A006F87EA9C4D59663D659A71580</idno>
					<idno type="arXiv">arXiv:2501.05242v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1. Our method Scaffold-SLAM achieves high-quality photorealistic mapping with quality outperforms state-of-the-art methods (GS-ICP SLAM [1], Photo-SLAM [2], SplaTAM [3], MonoGS [4]) across monocular, stereo, and RGB-D cameras. Top: The results are from TUM RGB-D datasets for RGB-D camera. Bottom: The left three images stemming from Replica datasets for monocular camera and the right three from EuRoC MAV datasets for stereo camera. Non-obvious difference in quality highlighted by insets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual SLAM is a fundamental problem in 3D computer vision, with wide applications in autonomous driving, robotics, virtual reality, and augmented reality. SLAM aims to construct dense or sparse maps to represents the scene. Recently, neural rendering <ref type="bibr" target="#b4">[5]</ref> has been integrated into SLAM pipelines, significantly enhancing the scene representation capabilities of the maps. The latest advancement in radiance field rendering is 3D Gaussian Splatting (3DGS) <ref type="bibr" target="#b5">[6]</ref>, an explicit scene representation that achieves revolutionary improvements in rendering and training speed. Recent SLAM works <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> incorporating 3DGS have demonstrated that explicit representations provide more promising rendering performance compared to implicit representations.</p><p>However, current SLAM methods leveraging 3DGS have yet to achieve high-quality rendering across monocular, stereo, and RGB-D cameras simultaneously. Most existing approaches only support RGB-D cameras. For example, SplaTAM <ref type="bibr" target="#b2">[3]</ref> jointly optimizes the camera pose and the Gaussians by minimizing the image and depth reconstruction errors, achieving localization and rendering for RGB-D cameras. GS-SLAM <ref type="bibr" target="#b6">[7]</ref> derives an analytical formulation for optimizing camera pose tracking and dense mapping with RGB-D re-rendering loss. RTG-SLAM <ref type="bibr" target="#b7">[8]</ref> proposes a efficient pipeline to derive a compact Gaussian representation, resulting a real-time RGB-D system. GS-ICP SLAM <ref type="bibr" target="#b0">[1]</ref> propose a novel dense RGB-D SLAM approach with a fusion of Generalized Iterative Closest Point (ICP) and 3DGS. CG-SLAM <ref type="bibr" target="#b8">[9]</ref> employs an uncertainty-aware 3D Gaussian field to achieve efficient RGB-D SLAM.</p><p>There are a few methods supporting monocular, stereo, and RGB-D cameras. MonGS <ref type="bibr" target="#b3">[4]</ref> formulates camera tracking for 3DGS using direct optimization against the 3D Gaussians, allowing localization and photorealistic mapping for all three types of cameras. Unfortunately, its rendering quality gap between stereo and monocular cameras is significant. Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> introduces a decoupled framework to optimize 3D Gaussians, achieving real-time localization and photorealistic mapping for monocular, stereo, and RGB-D cameras. While it demonstrates strong realtime performance and a minimal gap in rendering quality between monocular and RGB-D cameras, its primary limitation lies in the overall rendering quality. Our work aims to significantly improve the rendering accuracy for monocular, stereo, and RGB-D cameras.</p><p>In this paper, we propose Scaffold-SLAM, a novel SLAM system that achieves simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. Our approach shares the same decoupled framework as Photo-SLAM <ref type="bibr" target="#b1">[2]</ref>, where we utilize a traditional indirect visual SLAM pipeline for localization and geometric mapping. The generated point cloud is used to initialize structured 3D Gaussians. Instead, we introduce two key innovations that enable our method to achieve state-of-the-art photorealistic mapping quality across monocular, stereo, and RGB-D cameras. First, we propose Appearance-from-Motion embedding, which models appearance variations such as exposure and lighting in a learned low-dimensional latent space. We train the embedding to predict the appearance variations across diverse images with the camera pose. Second, we propose a frequency regularization pyramid that constrains the frequencies of rendered image across multiple scales in the frequencies domain. This encourages 3D Gaussians to grow towards complex regions, such as object edges and textures, enabling the model to capture high-frequency details in the scene. Finally, to evaluate the photorealistic mapping quality of our method, we conduct extensive experiments across diverse datasets, including monocular, stereo, and RGB-D cameras. The experimental results show that our approach, Scaffold-SLAM, surpasses state-of-the-art methods in photorealistic mapping quality across all three camera types. The main contributions of this work are as follows: 1. We develop an Appearance-from-Motion embedding to enable our SLAM system to effectively model image appearance variations across diverse images. 2. We propose a frequency regularization pyramid to guide the growth of 3D gaussians toward complex regions to capture finer details in the scene. 3. Extensive evaluations on various datasets demonstrate that our method, Scaffold-SLAM, achieves superior photorealistic mapping quality across monocular, stereo, and RGB-D cameras, while maintaining competitive tracking accuracy. The code will be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual SLAM. Traditional visual SLAM methods can be broadly classified into two categories: direct methods and indirect methods. Indirect methods rely on extracting and tracking features between consecutive frames to estimate pose and build sparse maps by minimizing the reprojection error. Examples include PTAM <ref type="bibr" target="#b9">[10]</ref> and ORB-SLAM3 <ref type="bibr" target="#b10">[11]</ref>. Direct methods <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, on the other hand, bypass feature extraction and estimate motion and structure by minimizing photometric error, which can build sparse or semi-dense maps. The first dense visual SLAM is Kinect-Fusion <ref type="bibr" target="#b14">[15]</ref>, which updates the scene using a TSDF representation. Recently, some methods <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> have integrated deep learning into visual SLAM systems. DPVO <ref type="bibr" target="#b20">[21]</ref> extends the current state-of-the-art method Droid-SLAM <ref type="bibr" target="#b18">[19]</ref> by leveraging the efficiency of sparse block matching, improving computational performance. More recently, Lipson et al. <ref type="bibr" target="#b19">[20]</ref> couple optical flow prediction with a pose-solving layer to achieve camera tracking. Our approach favors traditional indirect SLAM for the following insight. Indirect SLAM shares a highly similar pipeline with Structure-from-Motion (SfM), including feature matching, tracking, and bundle adjustment, leading to point cloud with similar intrinsic properties. Since the 3D Gaussians in <ref type="bibr" target="#b5">[6]</ref> are initialized by point cloud generated frome SfM, we believe that initializing our Gaussians using point cloud obtained from indirect SLAM is an optimal choice. Implicit Representation based SLAM. The first to introduce radiance field rendering into SLAM systems is neural implicit representations. iMAP <ref type="bibr" target="#b21">[22]</ref> pioneers the use of neural implicit representations to achieve tracking and mapping through reconstruction error. Subsequently, many works <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> have explored new representation forms. For instance, Vox-Fusion <ref type="bibr" target="#b23">[24]</ref> proposes a voxel-based neural implicit surface representation. ESLAM <ref type="bibr" target="#b25">[26]</ref> represents scenes using multi-scale axis-aligned perpendicular feature planes. Point-SLAM <ref type="bibr" target="#b31">[32]</ref> adopts a point-based neural implicit representation and achieved far superior rendering quality compared to previous methods. Recently, SNI-SLAM <ref type="bibr" target="#b27">[28]</ref> and IBD-SLAM <ref type="bibr" target="#b29">[30]</ref> introduce a hierarchical semantic representation and an xyz-map representation, respectively. Some works <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref> have investigated other challenges. GO-SLAM <ref type="bibr" target="#b32">[33]</ref> integrates Droid-SLAM, while Loopy-SLAM <ref type="bibr" target="#b33">[34]</ref> addresses loop closure. However, only Point-SLAM explores novel view synthesis, while others focus on geometric reconstruction.</p><p>3D Gaussian Splatting based SLAM. Recently, several works have introduced an explicit representation, 3DGS <ref type="bibr" target="#b5">[6]</ref>, into visual SLAM systems, achieving both localization and photorealistic mapping. Thanks to the fast training and rendering speed of 3DGS, as well as its excellent rendering quality, these methods have demonstrated superior photorealistic mapping quality and rendering speed compared to various implicit representation approaches <ref type="bibr">[23-27, 32, 33]</ref>, including Point-SLAM <ref type="bibr" target="#b31">[32]</ref>. Most of these methods are RGB-D SLAM systems. For example, SplaTAM <ref type="bibr" target="#b2">[3]</ref> and GS-SLAM <ref type="bibr" target="#b6">[7]</ref> both optimize camera poses and mapping by minimizing image and depth rendering errors. RTG-SLAM <ref type="bibr" target="#b7">[8]</ref> explores the efficiency of Gaussian representations. GS-ICP <ref type="bibr" target="#b0">[1]</ref> achieves high-quality photorealistic mapping by fusing 3DGS with Generalized ICP on depth points. CG-SLAM <ref type="bibr" target="#b8">[9]</ref> examines the uncertainty in RGB-D sensors. Some methods support monocular, stereo, and RGB-D cameras simultaneously. MonoGS <ref type="bibr" target="#b3">[4]</ref> formulates directly estimating camera poses by optimizing 3D Gaussians. However, its rendering quality performs poorly on monocular cameras. Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> proposes a real-time, decoupled system for localization and photorealistic mapping. Photo-SLAM excels in impressive real-time performance and resource efficiency, but at the cost of significantly reduced rendering quality. Our proposed Scaffold-SLAM aims to achieve higher-quality photorealistic mapping for monocular, stereo, and RGB-D cameras while maintaining competitive localization accuracy. Similar to Photo-SLAM, our method does not involve reconstructing a dense mesh.</p><p>In concurrent works, OG-Mapping <ref type="bibr" target="#b39">[40]</ref> is the RGB-D SLAM leveraging scene structure. LoopSplat <ref type="bibr" target="#b40">[41]</ref> and GLC-SLAM <ref type="bibr" target="#b41">[42]</ref> explore loop closure detection using 3DGS. IG-SLAM <ref type="bibr" target="#b42">[43]</ref> specializes in monocular camera. MGSO <ref type="bibr" target="#b43">[44]</ref> and GEVO <ref type="bibr" target="#b44">[45]</ref> investigate efficiency. Hi-SLAM <ref type="bibr" target="#b45">[46]</ref> focus on semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we present details of our Scaffold-SLAM. The overview of our SLAM system is summarized in Fig. <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Localization and Geometry Mapping</head><p>Since traditional indirect SLAM pipelines are highly similar to SfM, the generated point cloud exhibit robust geometric structure. Thus, we follow the traditional indirect SLAM approach, optimizing the camera orientation R ∈ SO(3) and position t ∈ R 3 through motion-only bundle adjustment (BA). The camera poses {R, t} are optimized by minimizing the reprojection error between the matched 3D points P i ∈ R 3 and 2D feature points p i within a sliding window:</p><formula xml:id="formula_0">{R, t} = i∈X argmin Ri,ti ρ(∥p j -π(R i P j + t i )∥ 2 Σg ) (1)</formula><p>where X represents the set of all mathches, Σ g denotes the covariance matrix associated with the keypoint's scale, π is the projection function, and ρ is the robust Huber cost function.</p><p>We perform a local BA by optimizing a set of covisible keyframes K L alone with the set of points P L observed in those keyframes as follows:</p><formula xml:id="formula_1">{P i , R l , t l } = argmin P i ,R l ,t l k∈K L ∪K F j∈X k ρ(E(k, j)) (2) E(k, j) = ∥p j -π(R k P j + t k )∥ 2 Σg (3)</formula><p>where i ∈ P L , l ∈ K L , K F are all other keyframes, and X k is the set of matches between keypoints in keyframe k and points in P L .</p><p>Global BA is a special case of local BA, where all keyframes and map points are included in the optimization, except the origin keyframe, which is kept fixed to prevent gauge freedom. After performing local or global BA, we can obtain more accurate poses and map point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">High-quality Photorealistic Mapping</head><p>Following 3DGS, our rendering process is as follows:</p><formula xml:id="formula_2">C(R, t) = i∈N c i δ i i-1 j=1 (1 -δ j ) (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>where N is the number of ordered 2D Gaussians overlapping the pixel, δ i = α i • G(R, t, P i , q i , s i ), and G denotes splatting process of 3DGS <ref type="bibr" target="#b5">[6]</ref>. The parameters of 3D Gaussians include color c, position P, scaling matrix s, rotation matrix q, and opacity α. Inspired by <ref type="bibr" target="#b46">[47]</ref>, we incrementally construct a sparse grid of anchor points, initialized by voxelizing the point cloud obtained from the geometric mapping. Each anchor point distributes k 3D Gaussians, the color of which is obtained as follows:</p><formula xml:id="formula_4">{c 0 , . . . , c k-1 } = M c ( fv , δ vc , ⃗ d vc , ℓ (a) R, t )<label>(5)</label></formula><p>where δ vc , ⃗ d vc are relative distance and viewing direction of the anchor point, fv is a local context feature, and M c is a individual multi-layer perceptron (MLP). α, q, and s are similarly obtained by individual MLPs, denoted as M α , M q , and M s , respectively. Only the color c incorporates Appearance-from-Motion embedding ℓ (a) R, t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Appearance-from-Motion Embedding</head><p>To handle photometric variations, appearance embedding is a proven effective technique. It originates from a generative network called Generative Latent Optimization <ref type="bibr" target="#b47">[48]</ref>. NeRF-W <ref type="bibr" target="#b48">[49]</ref> is the first to introduce this technique into neural rendering. By learning a shared appearance representation across all images, it models per-image photometric and environmental variations in a low-dimensional latent space. Each image is assigned a corresponding appearance embedding real-valued vector ℓ (a) i of length n (a) which takes the index i of each image as input. Scaffold-GS <ref type="bibr" target="#b46">[47]</ref> also incorporates this technique to enhance the rendering quality. However, in NeRF-W, the embedding ℓ using the left half of the groundtruth image to match the appearance and evaluates metrics on the right half. In the novel view synthesis dataset, there are typically hundreds of views, with approximately 20% allocated to the test-set, making the computational cost of this approach acceptable. However, in SLAM datasets, which contain thousands of images with around 80% being test-set, this cost becomes prohibitive. Scaffold-GS addresses this by randomly selecting a training image index as input for test images. Nevertheless, this approach performs poorly for tasks like SLAM, where the test-set is large. Because the trained views are too sparse relative to the novel views, making it difficult to predict the appearance of novel views.</p><p>Fortunately, our focus is on photorealistic mapping of SLAM. It naturally follows that we would use the estimated camera pose corresponding to each training image as the input for appearance embedding, rather than the image index. Our insight is that the camera poses optimized through global bundle adjustment conform to the same maximum a posteriori (MAP) probability distribution. Moreover, each image corresponding to a unique camera pose, which shares analogous properties with the camera indices. Therefore, we can train a network to learn a shared appearance representation across all training camera poses, capturing the underlying probabilistic distribution of these poses. Since the camera poses of test-set also belong to this distribution, the network can predict the appearance for the novel views. We refer to this method as Appearance-from-Motion embedding. We employ a tiny MLP to model the appearance variation of each image based on its camera pose in a lowdimensional latent space, as shown below:</p><formula xml:id="formula_5">ℓ (a) R, t = MLP Θa (R, t)<label>(6)</label></formula><p>Subsequently, this Appearance-from-Motion embedding ℓ</p><formula xml:id="formula_6">(a)</formula><p>R, t is fed into the color decoder M c , allowing all pixels in an image to share the same appearance representation. Ultimately, the appearance of all images is modeled within a continuous latent space, as illustrated in the Fig. <ref type="figure" target="#fig_6">6</ref>. We emphasize once again that our Appearance-from-Motion embedding does not require training on the test-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Frequency regularization Pyramid</head><p>To improve the quality of the photorealistic mapping, another challenge is the poor performance of the rendered images in capturing high-frequency details, particularly around object edges and regions with complex textures. Some prior works offer potential solutions: HF-NeuS <ref type="bibr" target="#b49">[50]</ref> employs a coarse-to-fine strategy to decompose SDF into base and displacement functions, gradually enhancing highfrequency details. AligNeRF <ref type="bibr" target="#b50">[51]</ref> improves upon the perceptual loss proposed by Johnson <ref type="bibr" target="#b51">[52]</ref> to enhance highfrequency rendering details. FreGS <ref type="bibr" target="#b52">[53]</ref> introduces a strategy combining frequency regularization with frequency annealing. They decompose the high and low frequencies in the frequency domain, allowing the Gaussians to densify more efficiently. We find that this frequency regularization approach can control the growth of anchor points. However, its limitation is that the model can only learn high-frequency details through single-scale features.</p><p>To fully leverage multiple levels of detail, we propose a novel frequency regularization technique called the frequency regularization pyramid. To construct images at different scales, we apply bilinear interpolation to smoothly downsample both the ground truth and rendered images.</p><p>During training, we supervise the model with the frequency from multi-scale images. Specifically, Let s ∈ S = {s 0 , s 1 , . . . , s n } denotes the scale of an image. For all scales of both the rendered images I s r and ground truth images I s g , we first apply a 2D Fast Fourier Transform (FFT) to obtain the frequency spectra F(I s r )(u, v), F(I s g )(u, v). We then use a high-pass filter H f (u, v) on these spectra to extract the high-frequency F s r,f (u, v), F s g,f (u, v). The loss L vol is computed based on the difference in highfrequency spectra between the rendered and ground truth images across different scales, as shown below:</p><formula xml:id="formula_7">L f = λ h s∈S 1 N u,v F s r,h (u, v) -F s g,j (u, v)<label>(7)</label></formula><formula xml:id="formula_8">F s i,f (u, v) = H f (u, v) • F(I s i )(u, v), f ∈ {h, l}, i ∈ {r, g}</formula><p>where N = HW denotes the size of the image. Unlike FreGS, we do not use low-frequency information to supervise our model. This is because low-frequency components are typically used to encourage the model to learn the overall structure of the environment. However, in our case, the geometric mapping process already generates a wellstructured point cloud that is used to initialize the Gaussians. Moreover, we employ an annealing strategy to guide the supervision of the frequency regularization pyramid.</p><p>Since the SLAM system incrementally builds the scene and requires time for the environment structure to stabilize, we introduce frequency regularization pyramid only after a certain number of iterations T s . Furthermore, as the scene's structure and details become stable toward the end of optimization, we stop applying frequency supervision after T e iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Optimization</head><p>Finally, the optimization of the learnable parameters in the model, the MLP that predicts parameters of 3D Gaussians, and the MLP for Appearance-from-Motion embedding are achieved by minimizing the L1 loss L 1 , SSIM term, frequency regularization L f , and volume regularization L vol between the rendered images and the ground truth images, denoted as</p><formula xml:id="formula_9">L = (1 -λ)L 1 + λ(1 -L SSIM ) + λ vol L vol + λ f L f (8)</formula><p>where</p><formula xml:id="formula_10">L vol = Nng i=1 Prod(s i )<label>(9)</label></formula><p>Here, Prod(•) denotes product of the values of a vector and N ng presents the number of 3D Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Implementation: Our Scaffold-SLAM is fully implemented using the LibTorch framework with C++ and CUDA, and the localization module is based on ORB-SLAM3 <ref type="bibr" target="#b10">[11]</ref>. The algorithm is developed under the Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> framework, with the removal of its geometry densification module and Gaussian pyramid training strategy, as these components did not enhance our method. Except for the non-open-source GS-SLAM <ref type="bibr" target="#b6">[7]</ref>, all methods compared in this paper are run on the same machine using their official code. The system is equipped with an NVIDIA RTX 4090 24GB GPU and a Ryzen Threadripper Pro 5995WX CPU. By default, our method runs for 30K iterations.</p><p>Baseline: For monocular and stereo camera setups, we compare our method with Photo-SLAM <ref type="bibr" target="#b1">[2]</ref>, MonoGS <ref type="bibr" target="#b3">[4]</ref>, and Photo-SLAM-30K. For RGB-D cameras, we additionally include comparisons with RTG-SLAM <ref type="bibr" target="#b7">[8]</ref>, GS-SLAM <ref type="bibr" target="#b6">[7]</ref>, SplaTAM <ref type="bibr" target="#b2">[3]</ref>, and GS-ICP SLAM <ref type="bibr" target="#b0">[1]</ref>, all of which represent state-of-the-art SLAM approaches based on 3DGS.</p><p>For camera pose estimation, we also compare with ORB-SLAM3 <ref type="bibr" target="#b10">[11]</ref>, DROID-SLAM <ref type="bibr" target="#b18">[19]</ref>, and GO-SLAM <ref type="bibr" target="#b32">[33]</ref>. While Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> achieves strong real-time performance, its photorealistic mapping typically doesn't reach   30K iterations. Since both Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> and our method are decoupled approaches, we introduce Photo-SLAM-30K as a baseline, where the number of iterations is fixed at 30K, to demonstrate that our results are not simply due to increasing the iteration count in Photo-SLAM <ref type="bibr" target="#b1">[2]</ref>.</p><p>Metric: We follow the evaluation protocol of MonoGS <ref type="bibr" target="#b3">[4]</ref> to assess both camera pose estimation and novel view synthesis. For camera pose estimation, we report the root mean square error (RMSE) of the absolute trajectory error (ATE) <ref type="bibr" target="#b53">[54]</ref> for all frames. For photorealistic mapping, we report standard rendering quality metrics, including PSNR, SSIM, and LPIPS <ref type="bibr" target="#b54">[55]</ref>. To evaluate the photorealistic mapping quality, we only calculate the average metrics over novel views for all methods. To ensure fairness, no keyframes (training views) are included in the evaluation, and for all RGB-D SLAM methods, no masks are applied to either the rendered or ground truth images during metric calculation. As a result, the reported metrics for Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> are slightly lower than those in the original paper, as the original averages both novel and training views. Similarly, the metrics of SplaTAM <ref type="bibr" target="#b2">[3]</ref> and GS-ICP SLAM <ref type="bibr" target="#b0">[1]</ref> are slightly lower than reported, as the original methods use a mask to exclude outliers, removing corresponding RGB pixels from both the rendered and real images based on anomalies in the depth image.</p><p>Datasets: We evaluate on well-known TUM RGB-D <ref type="bibr" target="#b55">[56]</ref> and Replica <ref type="bibr" target="#b56">[57]</ref> datasets for monocular and RGB-D cameras. For stereo cameras, we use the EuRoC MAV dataset [58].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results Analysis</head><p>Camera Tracking Accuracy As shown in Table <ref type="table" target="#tab_1">2</ref>, our method demonstrates competitive accuracy in tracking for monocular, stereo, and RGB-D camera compared to stateof-the-art methods. This is attributed to the high precision of the ORB-SLAM3 <ref type="bibr" target="#b10">[11]</ref>   dataset and obtain the highest rendering accuracy across all sequences in TUM RGB-D. TUM RGB-D is a more challenging dataset compared to Replica, featuring cluttered scenes with various small objects, as well as significant motion blur in RGB images and many holes in depth maps. GS-ICP SLAM is a state-of-the-art method for RGB-D SLAM and achieved second place in rendering accuracy on Replica. However, it is overly reliant on depth maps, resulting in poor performance on TUM RGB-D. In contrast, our Scaffold-SLAM uses depth maps primarily to enhance localization accuracy and geometric mapping results. The appearance variations and high-frequency details in cluttered scenes are effectively handled by the two key elements of our method: appearance-from-Motion embedding and frequency regularization pyramid. Thus, Scaffold-SLAM shows significant improvement on TUM RGB-D. Table <ref type="table" target="#tab_2">3</ref> records the quantitative rendering results for monocular camera scenes, where Scaffold-SLAM surpasses other methods. Notably, Scaffold-SLAM still significantly outperforms the comparative methods on TUM RGB-D. Importantly, compared to RGB-D scenes, the rendering accuracy of our method does not decrease substantially, whereas Mono-GS experiences a sharp decline. This indicates that our method does not heavily depend on depth image.</p><p>While stereo cameras are closer to human vision, progress in this area has been slow. Scaffold-SLAM takes a small step forward in 3DGS-based stereo SLAM. The quantitative results for photorealistic mapping presented in Table <ref type="table" target="#tab_2">3</ref>, where our method also achieves the highest rendering quality, surpassing the current state-of-the-art method, MonoGS. This confirms that our system possesses broader applicability.</p><p>Across all scenes, our method greatly outperforms Photo-SLAM-30K, proving that the rendering quality of our method is not simply achieved by increasing the number of iterations. This also validates the effectiveness of Appearance-from-Motion embedding and frequency regu- Table 5. Ablation Study on replacement of key components. Best results are marked as Best score.  larization pyramid. Efficiency Comparison. We evaluate the rendering speed, tracking speed, and mapping time of our method, as shown in Table <ref type="table" target="#tab_0">1</ref>. Our Scaffold-SLAM achieves real-time rendering and tracking speeds. Although our method shows a slight decrease in real-time performance compared to Photo-SLAM <ref type="bibr" target="#b1">[2]</ref> and GS-ICP SLAM <ref type="bibr" target="#b0">[1]</ref>, it maintains an advantage over Mono-GS <ref type="bibr" target="#b3">[4]</ref> and SplaTAM <ref type="bibr" target="#b2">[3]</ref> in terms of tracking speed, and mapping time, while offering the highest rendering quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We isolate two modules from our algorithm: Appearancefrom-Motion embedding and frequency regularization pyramid, and conducted a series of experiments to assess their impacts. Our method without these two modules is referred to as Base, while our complete method is denoted as Ours.</p><p>Quantitative results are presented in Replacement of key components. Next, we evaluate the superior performance of our proposed Appearance-from-Motion embedding and frequency regularization pyramid compared to the appearance embedding from Scaffold-GS <ref type="bibr" target="#b46">[47]</ref> and the frequency regularization from FreGS <ref type="bibr" target="#b52">[53]</ref>.</p><p>Based on the Base model, we train two additional models: one incorporating appearance embedding and FRP, denoted as Base+AE+FRP, and another with Appearancefrom-Motion embedding and the frequency regularization, denoted as Base+AfME+FR. As shown in Table <ref type="table">5</ref>, our full method Ours achieves the highest PSNR score. This demonstrates that, compared to appearance embedding, our Appearance-from-Motion embedding is more effective in predicting appearance variations across a wide range of novel views, thus avoiding additional training on the test set. On the other hand, it also highlights that introducing multiscale frequency constraints helps the model better capture high-frequency details in the scene, leading to superior rendering quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce Scaffold-SLAM, a SLAM method that achieves high-quality photorealistic mapping for monocular, stereo, and RGB-D cameras. We explore the limits of decoupled approaches by integrating traditional indirect SLAM with a structured 3D Gaussian representation. Extensive experiments demonstrate that our method surpasses coupled approaches in rendering quality across all camera types. We also highlight two key innovations, Appearance-from-Motion embedding and frequency regularization pyramid, that significantly enhance photorealistic mapping quality. By incorporating Appearance-from-Motion embedding, our method successfully predicts substantial appearance variations from fewer training views. The proposed frequency regularization pyramid effectively supervises the optimization and growth of 3D Gaussians, enabling the modeling of more scene details. Future</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of our method. Our method supports monocular, stereo, and RGB-D cameras. The input image stream is processed by the tracking and geometric mapping modules, generating high-quality point cloud and accurate poses. These point cloud are used to incrementally construct Gaussians. The poses are fed into the Appearance-from-Motion embedding to model lighting and other appearance changes in the environment. Additionally, we introduce the frequency regularization pyramid to supervise the training of Gaussians, allowing for improved modeling of high-frequency details in the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of the appearance variations modeled by Appearance-from-Motion embedding. All images are rendered from novel views with significant lighting changes.</figDesc><graphic coords="4,309.68,72.00,116.93,87.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>(a) GSICP [1] (b) SplaTAM [3] (c) MonoGS [4] (d) Ours (e) Ground Truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. We show comparisons of ours to state-of-the-art methods for RGB-D camera. The top scene is rooo0 from Replica datasets, and the bottom is fr3 office from TUM RGB-D datasets. Non-obvious difference in quality highlighted by insets.</figDesc><graphic coords="6,57.50,233.58,94.04,70.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. We show comparisons of ours to state-of-the-art methods for Monocular and Stereo cameras. The top scene is fr3 office from TUM RGB-D datasets, the mid is rooo0 from Replica datasets, and the bottom is V201 easy from EuRoC MAV datasets. Non-obvious difference in quality highlighted by insets.</figDesc><graphic coords="7,56.27,333.71,118.79,75.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Illustration of the appearance variations modeled by Appearance-from-Motion embedding. All images are rendered from novel views with significant lighting changes.</figDesc><graphic coords="8,50.93,279.15,116.93,66.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of our method compared to state-of-the-art methods for RGB-D camera on Replica and TUM RGB-D datasets. Best results are marked as Best score , second best score and third best score . GS-SLAM * denotes the result of GS-SLAM is taken from<ref type="bibr" target="#b6">[7]</ref>, all others are obtained in our experiments. '-' denotes the system does not provide valid results.</figDesc><table><row><cell>Datasets (Camera)</cell><cell></cell><cell>Replica (RGB-D)</cell><cell></cell><cell cols="3">TUM RGB-D (RGB-D)</cell><cell cols="3">Avg. Runtime (RGB-D)</cell></row><row><cell>Method</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>LPIPS ↓</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>LPIPS ↓</cell><cell>Rendring FPS ↑</cell><cell>Tracking FPS ↑</cell><cell>Mapping Time ↓</cell></row><row><cell>MonoGS [4]</cell><cell>36.82</cell><cell>0.964</cell><cell>0.069</cell><cell>24.11</cell><cell>0.800</cell><cell>0.231</cell><cell>706</cell><cell>1.33</cell><cell>37m40s</cell></row><row><cell>Photo-SLAM [2]</cell><cell>35.50</cell><cell>0.949</cell><cell>0.056</cell><cell>21.25</cell><cell>0.741</cell><cell>0.207</cell><cell>1562</cell><cell>30.30</cell><cell>1m20s</cell></row><row><cell>Photo-SLAM-30K</cell><cell>36.94</cell><cell>0.952</cell><cell>0.040</cell><cell>21.73</cell><cell>0.757</cell><cell>0.186</cell><cell>1439</cell><cell>30.87</cell><cell>6m32s</cell></row><row><cell>RTG-SLAM [8]</cell><cell>32.79</cell><cell>0.918</cell><cell>0.164</cell><cell>16.47</cell><cell>0.574</cell><cell>0.461</cell><cell>447</cell><cell>17.24</cell><cell>12m03s</cell></row><row><cell>GS-SLAM  *  [7]</cell><cell>34.27</cell><cell>0.975</cell><cell>0.082</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>387</cell><cell>-</cell><cell>-</cell></row><row><cell>SplaTAM [3]</cell><cell>33.96</cell><cell>0.969</cell><cell>0.099</cell><cell>23.60</cell><cell>0.783</cell><cell>0.164</cell><cell>531</cell><cell>0.15</cell><cell>3h45m</cell></row><row><cell>GS-ICP SLAM [1]</cell><cell>37.14</cell><cell>0.968</cell><cell>0.045</cell><cell>21.25</cell><cell>0.741</cell><cell>0.207</cell><cell>630</cell><cell>30.32</cell><cell>1m32s</cell></row><row><cell>Ours</cell><cell>39.14</cell><cell>0.974</cell><cell>0.023</cell><cell>25.95</cell><cell>0.853</cell><cell>0.160</cell><cell>400</cell><cell>17.18</cell><cell>11m14s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Camera tracking result on Replica, TUM RGB-D, and EuRoC MAV datasets for Monocular, stereo, and RGB-D cameras. RMSE of ATE (cm) is reported.</figDesc><table><row><cell>Camera Type</cell><cell></cell><cell>RGB-D</cell><cell></cell><cell></cell><cell>Monocular</cell><cell></cell><cell>Stereo</cell></row><row><cell>Datasets</cell><cell cols="6">TUM R Replica Avg. TUM R Replica Avg.</cell><cell>EuRoC</cell></row><row><cell cols="2">Method RMSE ORB-SLAM3 [11] 1.269</cell><cell>1.478</cell><cell>1.374</cell><cell>1.218</cell><cell>3.942</cell><cell cols="2">2.580 11.187</cell></row><row><cell cols="3">DRIOD-SLAM [19] 97.986 0.634</cell><cell cols="5">49,31 89.559 0.725 45.142 38.590</cell></row><row><cell>GO-SLAM [33]</cell><cell cols="6">20.236 0.571 10.404 55.820 71.054 20.5315</cell><cell>-</cell></row><row><cell>MonoGS [4]</cell><cell>1.502</cell><cell>0.565</cell><cell>1.033</cell><cell cols="4">4.009 37.054 63.437 49.241</cell></row><row><cell>Photo-SLAM [2]</cell><cell>1.385</cell><cell>0.582</cell><cell>0.984</cell><cell>1.539</cell><cell>0.793</cell><cell cols="2">1.166 11.023</cell></row><row><cell>Photo-SLAM-30K</cell><cell>1.831</cell><cell>0.611</cell><cell>1.221</cell><cell>1.367</cell><cell>0.748</cell><cell cols="2">1.058 10.876</cell></row><row><cell>RTG-SLAM [8]</cell><cell>0.985</cell><cell>0.191</cell><cell>0.581</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GS-SLAM  *  [7]</cell><cell>3.700</cell><cell>0.500</cell><cell>2.100</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SplaTAM [3]</cell><cell>3.259</cell><cell>0.366</cell><cell>1.813</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GS-ICP SLAM [1]</cell><cell>2.921</cell><cell>0.177</cell><cell>1.549</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="2">1.080 0.465</cell><cell>0.768</cell><cell>1.642</cell><cell>0.512</cell><cell>1.077</cell><cell>7.462</cell></row></table><note><p>↓ RMSE ↓ RMSE ↓ RMSE ↓ RMSE ↓ RMSE ↓ RMSE ↓</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>algorithm used for localization. Novel View synthesis The quantitative rendering results for novel views in RGB-D scenes are recorded in Table1, where Scaffold-SLAM significantly outperforms the comparative methods, achieving the highest average rendering quality on both TUM RGB-D and Replica datasets. We achieve the best results in most sequences of the Replica Quantitative evaluation of our method compared to state-of-the-art methods for Monocular (Mono) and Stereo cameras on Replica, TUM RGB-D, and EuRoC MAV datasets. Best results are marked as Best score and second best score .</figDesc><table><row><cell></cell><cell></cell><cell>Replica (Mono)</cell><cell></cell><cell cols="3">TUM RGB-D (Mono)</cell><cell></cell><cell>EuRoC (Stereo)</cell><cell></cell></row><row><cell>method</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>LPIPS ↓</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>LPIPS ↓</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>LPIPS ↓</cell></row><row><cell>MonoGS [4]</cell><cell>28.34</cell><cell>0.878</cell><cell>0.256</cell><cell>21.00</cell><cell>0.705</cell><cell>0.393</cell><cell>22.60</cell><cell>0.789</cell><cell>0.274</cell></row><row><cell>Photo-SLAM [2]</cell><cell>33.60</cell><cell>0.934</cell><cell>0.077</cell><cell>20.17</cell><cell>0.708</cell><cell>0.224</cell><cell>11.90</cell><cell>0.409</cell><cell>0.439</cell></row><row><cell>Photo-SLAM-30K</cell><cell>36.70</cell><cell>0.952</cell><cell>0.046</cell><cell>21.06</cell><cell>0.733</cell><cell>0.186</cell><cell>11.77</cell><cell>0.405</cell><cell>0.430</cell></row><row><cell>Ours</cell><cell>37.71</cell><cell>0.963</cell><cell>0.041</cell><cell>24.52</cell><cell>0.823</cell><cell>0.153</cell><cell>23.64</cell><cell>0.791</cell><cell>0.182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation Study on the key components (Appearancefrom-Motion embedding and frequency regularization pyramid). Best results are marked as Best score.</figDesc><table><row><cell>Camera type</cell><cell></cell><cell>Mono</cell><cell></cell><cell>RGB-D</cell><cell></cell><cell>Stereo</cell></row><row><cell>Datasets</cell><cell cols="5">TUM RGB-D Replica TUM RGB-D Replica</cell><cell>EuRoC</cell></row><row><cell>Method</cell><cell></cell><cell>PSNR ↑</cell><cell>PSNR ↑</cell><cell>PSNR ↑</cell><cell cols="2">PSNR ↑ PSNR ↑</cell></row><row><cell>Base</cell><cell></cell><cell>21.34</cell><cell>36.79</cell><cell>24.20</cell><cell>38.67</cell><cell>22.91</cell></row><row><cell>Base+AfME</cell><cell></cell><cell>24.91</cell><cell>36.41</cell><cell>25.04</cell><cell>38.60</cell><cell>23.52</cell></row><row><cell>Base+FRP</cell><cell></cell><cell>23.69</cell><cell>37.48</cell><cell>24.66</cell><cell>39.12</cell><cell>23.87</cell></row><row><cell>Ours</cell><cell></cell><cell>25.12</cell><cell>37.71</cell><cell>25.95</cell><cell>39.14</cell><cell>23.64</cell></row><row><cell>Camera type</cell><cell></cell><cell cols="2">Mono</cell><cell cols="2">RGB-D</cell><cell>Stereo</cell></row><row><cell>Datasets</cell><cell></cell><cell cols="5">TUM RGB-D Replica TUM RGB-D Replica EuRoC</cell></row><row><cell>Method</cell><cell></cell><cell>PSNR ↑</cell><cell>PSNR ↑</cell><cell>PSNR ↑</cell><cell cols="2">PSNR ↑ PSNR ↑</cell></row><row><cell>Base+AE+FRP</cell><cell></cell><cell>24.53</cell><cell>37.86</cell><cell>25.84</cell><cell>39.12</cell><cell>22.32</cell></row><row><cell cols="2">Base+AfME+FR</cell><cell>25.10</cell><cell>37.65</cell><cell>25.82</cell><cell>39.04</cell><cell>23.58</cell></row><row><cell>Ours</cell><cell></cell><cell>25.12</cell><cell>37.71</cell><cell>25.95</cell><cell>39.14</cell><cell>23.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Based on the Base model, we train two additional models: one with the Appearance-from-Motion embedding added, denoted as Base+AfME, and another with the frequency regularization pyramid, denoted as Base+FRP. As shown in Table4, our complete method Ours surpasses Base, Base+AfME, and Base+FRP in terms of PSNR scores. Furthermore, both Base+AfME and Base+FRP demonstrate PSNR improvements over the Base model. This confirms that Appearance-from-Motion embedding and frequency regularization pyramid are crucial for improving rendering quality. The qualitative results also reveal that Appearance-from-Motion embedding has learned photometric variations across consecutive views, strongly validating the effectiveness of embedding appearance into the low-dimensional pose space. Additionally, we provide a comparison of rendering results with and without the frequency regularization pyramid, where it is evident that the model achieves more realistic object edge modeling after applying frequency regularization.</figDesc><table><row><cell>and 5.</cell></row><row><cell>Key components. We first evaluate the impact of the pro-</cell></row><row><cell>posed Appearance-from-Motion embedding (AfME) and</cell></row><row><cell>frequency regularization pyramid (FRP) on photorealistic</cell></row><row><cell>mapping metrics.</cell></row></table></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>work will focus on enhancing the real-time performance of our method without compromising photorealistic mapping quality.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rgbd gs-icp slam</title>
		<author>
			<persName><forename type="first">Seongbo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiung</forename><surname>Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonwoo</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2024. 1, 2, 3, 5, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photo-slam: Real-time simultaneous localization and photorealistic mapping for monocular stereo and rgb-d cameras</title>
		<author>
			<persName><forename type="first">Huajian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">June 2024. 1, 2, 3, 5, 6, 7, 8</date>
			<biblScope unit="page" from="21584" to="21593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Splatam: Splat track &amp; map 3d gaussians for dense rgb-d slam</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Keetha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Karhade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Murthy</forename><surname>Jatavallabhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">June 2024. 1, 3, 5, 6, 8</date>
			<biblScope unit="page" from="21357" to="21366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaussian splatting slam</title>
		<author>
			<persName><forename type="first">Hidenobu</forename><surname>Matsuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riku</forename><surname>Murai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">June 2024. 1, 2, 3, 5, 6, 7, 8</date>
			<biblScope unit="page" from="18039" to="18048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Kerbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Kopanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leimkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2004">2023. 1, 2, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gs-slam: Dense visual slam with 3d gaussian splatting</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">June 2024. 1, 2, 3, 5, 6</date>
			<biblScope unit="page" from="19595" to="19604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rtg-slam: Real-time 3d reconstruction at scale using gaussian splatting</title>
		<author>
			<persName><forename type="first">Zhexi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingke</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2024 Conference Papers, SIGGRAPH &apos;24. Association for Computing Machinery</title>
		<imprint>
			<date type="published" when="2006">2024. 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cg-slam: Efficient dense rgb-d slam in a consistent uncertainty-aware 3d gaussian field</title>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small ar workspaces</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Elvira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">J</forename><surname>Gómez Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lsdslam: Large-scale direct monocular slam</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct sparse mapping</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Zubizarreta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iker</forename><surname>Aguinaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martinez</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1363" to="1370" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 10th IEEE International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Codeslam -learning a compact, optimisable representation for dense visual slam</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002">June 2018. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scenecode: Monocular dense semantic reconstruction using learned encoded scene representations</title>
		<author>
			<persName><forename type="first">Shuaifeng</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nodeslam: Neural object descriptors for multi-view shape reconstruction</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="949" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006">2021. 2, 5, 6</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16558" to="16569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-session slam with differentiable wide-baseline pose optimization</title>
		<author>
			<persName><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002">June 2024. 2</date>
			<biblScope unit="page" from="19626" to="19635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep patch visual odometry</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="39033" to="39051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Davison. imap: Implicit mapping and positioning in real-time</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002">October 2021. 2</date>
			<biblScope unit="page" from="6229" to="6238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nice-slam: Neural implicit scalable encoding for slam</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjia</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="499" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Orbeez-slam: A realtime monocular visual slam with orb features and nerfrealized mapping</title>
		<author>
			<persName><forename type="first">Chi-Ming</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang-Che</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya-Ching</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Qian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hung</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Fong</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9400" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eslam: Efficient dense slam system based on hybrid representation of signed distance fields</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johari</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Camilla</forename><surname>Carta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franc</forename><surname>¸ois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fleuret</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002">June 2023. 2</date>
			<biblScope unit="page" from="17408" to="17419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coslam: Joint coordinate and sparse parametric encodings for neural real-time slam</title>
		<author>
			<persName><forename type="first">Hengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003">June 2023. 3</date>
			<biblScope unit="page" from="13293" to="13302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Snislam: Semantic neural implicit slam</title>
		<author>
			<persName><forename type="first">Siting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hesheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002">June 2024. 2</date>
			<biblScope unit="page" from="21167" to="21177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ngel-slam: Neural implicit representation-based global consistent low-latency slam system</title>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6952" to="6958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ibd-slam: Learning image-based depth fusion for generalizable slam</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002">June 2024. 2</date>
			<biblScope unit="page" from="10563" to="10573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nicer-slam: Neural implicit scene encoding for rgb slam</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point-slam: Dense neural point cloud-based slam</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Sandström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Go-slam: Global optimization for consistent 3d instant reconstruction</title>
		<author>
			<persName><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006">October 2023. 2, 3, 5, 6</date>
			<biblScope unit="page" from="3727" to="3737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Loopy-slam: Dense neural slam with loop closures</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Liso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Sandström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Yugay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003">June 2024. 3</date>
			<biblScope unit="page" from="20363" to="20373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Plgslam: Progressive neural scene represenation with local to global bundle adjustment</title>
		<author>
			<persName><forename type="first">Tianchen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guole</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024-06">June 2024</date>
			<biblScope unit="page" from="19657" to="19666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cp-slam: Collaborative neural point-based slam system</title>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="39429" to="39442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uncle-slam: Uncertainty learning for dense neural slam</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Sandström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
			<biblScope unit="page" from="4537" to="4548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Qing Cheng, and Norbert Haala. Hi-slam: Monocular real-time dense mapping with hybrid implicit fields</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiecheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1548" to="1555" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nis-slam: Neural implicit semantic rgb-d slam for 3d consistent scene understanding</title>
		<author>
			<persName><forename type="first">Hongjia</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qirui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Og-mapping: Octree-based structured 3d gaussians for online dense mapping</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Sandström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName><surname>Loopsplat</surname></persName>
		</author>
		<title level="m">Loop closure by registering 3d gaussian splats</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Glc-slam: Gaussian splatting slam with efficient loop closure</title>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ig-slam: Instant gaussian slam</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sarikamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aydin Alatan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mgso: Monocular real-time photometric slam with efficient 3d gaussian splatting</title>
		<author>
			<persName><forename type="first">Yan</forename><forename type="middle">Song</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Qasim</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">Srebrnjak</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imad</forename><surname>Elhajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Asmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Gevo: Memory-efficient monocular visual odometry using gaussians</title>
		<author>
			<persName><forename type="first">Dasong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hi-slam: Scaling-up semantics in slam with a hierarchically categorical gaussian splatting</title>
		<author>
			<persName><forename type="first">Boying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaffold-gs: Structured 3d gaussians for view-adaptive rendering</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mulin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Xiangli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024-06">June 2024</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Alexey Dosovitskiy, and Daniel Duck-worth. Nerf in the wild: Neural radiance fields for unconstrained photo collections</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004">June 2021. 4</date>
			<biblScope unit="page" from="7210" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hf-neus: Improved surface reconstruction using high-frequency details</title>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1966">1966-1978, 2022. 5</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Alignerf: High-fidelity neural radiance fields via alignmentaware training</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005">June 2023. 5</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fregs: 3d gaussian splatting with progressive frequency regularization</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-05">June 2024. 5, 8</date>
			<biblScope unit="page" from="21424" to="21433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Grupp</surname></persName>
		</author>
		<ptr target="https://github.com/MichaelGrupp/evo" />
		<title level="m">Python package for the evaluation of odometry and slam</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Budge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">June</forename><surname>Yon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pesqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzo</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>De Nardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><surname>Newcombe</surname></persName>
		</author>
		<title level="m">The replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The euroc micro aerial vehicle datasets</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Burri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janosch</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Gohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sammy</forename><surname>Omari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">W</forename><surname>Achtelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1157" to="1163" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
