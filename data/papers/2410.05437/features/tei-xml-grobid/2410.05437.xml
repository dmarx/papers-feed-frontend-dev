<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ESPACE: Dimensionality Reduction of Activations for Model Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-07">7 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Charbel</forename><surname>Sakr</surname></persName>
							<email>csakr@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ESPACE: Dimensionality Reduction of Activations for Model Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-07">7 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">050A9CB53727EAFAD220B36F0D258216</idno>
					<idno type="arXiv">arXiv:2410.05437v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose ESPACE 1 , an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Capabilities of large language models (LLMs) have recently soared in natural language understanding and generative power. It is appreciated that there exists a correlation between model size and achievable accuracy. Indeed, as LLMs consume trillions of tokens during their training, a large parameter volume is required to capture intricate linguistic features <ref type="bibr" target="#b0">[1]</ref>. This leads to a trade-off in LLMs: larger parameter counts improve accuracy but come with increased serving cost. However, it is also appreciated that the computational requirements of inference may be lower than those of training <ref type="bibr" target="#b1">[2]</ref>. To that end, numerous studies have investigated compression of LLMs to reduce inference cost. The most popular LLM compression techniques are quantization <ref type="bibr" target="#b2">[3]</ref> and pruning <ref type="bibr" target="#b3">[4]</ref>. A less explored, but powerful technique is tensor decomposition, and in our work, we propose a novel, activation-centric way to decompose LLM tensors. Our proposal is to project activations onto a static set of components optimizing fidelity. The projection reduces activation dimensionality and leads to weight compression at inference as a byproduct of matrix multiplication associativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work and motivation for activation-centric tensor decomposition</head><p>Recent research has proposed many quantization and pruning techniques for compressing LLMs. Examples of advances in LLM quantization include SmoothQuant <ref type="bibr" target="#b2">[3]</ref>, AWQ <ref type="bibr" target="#b4">[5]</ref>, and GPTQ <ref type="bibr" target="#b5">[6]</ref>; while notable LLM pruning works include SparseGPT <ref type="bibr" target="#b3">[4]</ref>, LLM-Pruner <ref type="bibr" target="#b6">[7]</ref>, and ReLU-based masking <ref type="bibr" target="#b7">[8]</ref>. These methods are conceptually orthogonal to our proposal for activation projection which can be implemented in low precision or sparse formats. Nevertheless, compression fundamentally introduces noise, and an open problem is to study the impact of combining different methods, e.g., quantization and matrix factorization. This is beyond the scope of our paper, but a good direction for future work.</p><p>Our work is also orthogonal to non-compressive LLM serving acceleration such as continuous batching <ref type="bibr" target="#b8">[9]</ref> or speculative decoding <ref type="bibr" target="#b9">[10]</ref>, and attention optimizations such as PagedAttention <ref type="bibr" target="#b10">[11]</ref>, RadixAttention <ref type="bibr" target="#b11">[12]</ref>, and FlashAttention <ref type="bibr" target="#b12">[13]</ref>. Our study is on matrix multiplication layers involving weights and activations, and hence is mutually exclusive to works improving cross multiplications of activation tensors in attention. In fact, all our experiments use FlashAttention.</p><p>Finally, we turn to tensor decomposition, also known as factorization. Thus far, compression for LLM inference using factorization has been focused on weight decomposition. KnGPT <ref type="bibr" target="#b13">[14]</ref> uses the Kronecker transform to pack a large matrix into two smaller ones. TSVD <ref type="bibr" target="#b14">[15]</ref> performs iterative singular value decomposition (SVD) on weight matrices to produce high rank ternary components. TensorGPT <ref type="bibr" target="#b15">[16]</ref> and HEAT <ref type="bibr" target="#b16">[17]</ref> compress weight matrices into a cascade product of small matrices using the tensor-train algorithm. SVD-LoRa <ref type="bibr" target="#b17">[18]</ref> uses a truncated SVD on weights and finetunes the model using LoRa <ref type="bibr" target="#b18">[19]</ref>. The LoRa adapters are then merged to the main branch using bounds on the rank of sum of low rank matrices. ASVD <ref type="bibr" target="#b19">[20]</ref> performs a truncated SVD on the weights after re-scaling them by a diagonal matrix and their inverse encapsulating activation statistics. This work realizes the importance of activation-awareness but still uses weight-centric compression. SliceGPT <ref type="bibr" target="#b20">[21]</ref> extracts principal components in normalization layers to guide the deletion of rows and columns in weight matrices. The compression is achieved using a factorization made implicit via computational invariance. The statistical method employed by sliceGPT shares similarities with one of our results, but our problem formulation and solution are different. Factorization can also streamline LLM training and finetuning. For instance, LoRa <ref type="bibr" target="#b18">[19]</ref> finetunes pretrained models using residual low rank adapters which are then absorbed into the main branch. Similarly, GaLore <ref type="bibr" target="#b21">[22]</ref> applies a low rank approximation to gradients in back-propagation. These works do not modify inference parameter and operation count, and are hence orthogonal to ours. Our method could be applied in tandem with LoRa or GaLore, but this is beyond the scope of this paper.</p><p>Since factorization increases the number of LLM tensors, achieving high compression rates requires the intermediate dimensions to be much smaller than that of original dot product. This breakage in computation usually necessitates a retraining or finetuning stage to be healed. Unfortunately, this healing process is impeded because factorized LLMs have fewer learnable parameters which decreases expressivity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>To our knowledge, no prior art has explored activation decomposition. Indeed, applying factorization solvers (e.g., SVD) dynamically incurs large inference runtime overheads. Yet, activation decomposition has several desired features which we examine in Section 2 and motivate via the following insights: (a) weights stay uncompressed during retraining, preventing the aforementioned loss of expressivity; (b) large activation tensors contain inherent redundancies making them prime candidates for compression; and (c) since most LLM computation comprises multiplications of weights and activations, decomposing the latter can lead to compressing the former at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>We propose Eigen Static Principal Activation Component Estimation (ESPACE), an LLM compression technique based on activation dimensionality reduction. Our contributions are as follows:</p><p>• We project activation tensors onto a static and pre-calibrated orthonormal matrix. The projection lowers activation dimensionality but keeps weight matrices intact and fully available for training.</p><p>At inference, leveraging matrix multiplication associativity, model compression is achieved through pre-computation of the product of weight and projection matrices. • We theoretically derive optimal constructions for activation dimensionality reduction. Specifically, the projection matrix is calibrated in a manner to minimize activation decomposition mean squared error and forward propagated noise metrics. The solution is based on an eigenvalue decomposition of activation auto-correlation and yields multiple candidate projections for each activaton tensor. • We empirically study compression of models in the GPT3, Llama2, and Nemotron4 families evaluated on the Wikitext-103 dataset for perplexity and the LM evaluation harness for downstream task accuracy. The amelioration in size versus perplexity<ref type="foot" target="#foot_2">foot_2</ref> trade-offs is summarized in Figure <ref type="figure" target="#fig_0">1</ref>. • We show that ESPACE can compress LLMs by ∼50% at the cost of a small accuracy loss, as low as 0.18 increase in perplexity on GPT3-22B. • At lower compression rates, we find encouraging empirical evidence that ESPACE filters out noise and improves accuracy; e.g., ∼20% compressed GPT3-8B lowers its baseline perplexity by 0.38. • As an additional benefit of ESPACE, tangible latency reduction of 35%-to-45% is obtained in matrix multiplication layers. This speed-up translates to up to ∼ 40% faster prefill inference latency metricized by the time to first token and measured on existing hardware. • By comparison to existing works on tensor decomposition, we determine that ESPACE is a first step in pushing the frontier of compression rate versus accuracy retention (see Figure <ref type="figure" target="#fig_4">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dimensionality Reduction &amp; Projections</head><p>In this section, we introduce notation for matrix multiplication, review weight decomposition, and introduce our proposed mechanism of dimensionality reduction via activation projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matrix Multiplication and Weight Decomposition</head><p>We consider general matrix multiplications (GEMMs) described in Figure <ref type="figure" target="#fig_1">2</ref>(a) of the form</p><formula xml:id="formula_0">Y = W T X (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where W is a weight matrix of size K × N and X is an input activation tensor of size K × M so that the output activation tensor Y is of size N × M . Typically, K and N are defined by network topology and layer instance, they are commonly referred to as embedding or hidden size. In contrast, M stacks multiple dimensions in an activation tensors to obtain a 2D matrix view. Generally, these are the sequence and batch dimensions.</p><p>Transformer-based LLMs have four GEMM layers per block: query-key-value (QKV), projection (Proj), fully connected 1 (FC1), and fully connected 2 (FC2) layers. Our study is concerned with these layers, while cross activation multiplication and embedding layers are untouched. For notational simplicity, in this paper, we do not include layer indices in our equations.</p><p>The matrix W in (1) stores layer parameters and dictates the model's inference accuracy. To improve convergence of these parameters, an optimizer state is stored alongside weights during training and tracks historical values of gradients and updates <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. On the other hand, the activation tensor X depends on the input stimulus to the network, and is therefore generated on the fly.</p><p>Thus, at inference, weights are fixed but activations are dynamic. As a consequence, prior work on tensor decomposition has focused on compressing frozen weight matrices. One way of doing so is breaking W T into a low-rank approximation using some form of truncated SVD <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>, which is described in Figure <ref type="figure" target="#fig_1">2</ref>(b). Specifically, (1) is approximated as:</p><formula xml:id="formula_2">Y ≈ UVX<label>(2)</label></formula><p>where U and V are matrices of size N ×L and L×K, respectively, with L being the factorization rank.</p><p>For the decomposition procedure to be useful, two conditions need to be met: (a) L &lt;&lt; min(K, N ) for compression, and (b) the approximation W T ≈ UV should be accurate. However, achieving both conditions simultaneously may be challenging because a very low rank factorization usually leads to significant accuracy drop <ref type="bibr" target="#b21">[22]</ref>. As with other compression techniques, e.g., quantization and pruning, retraining of the compressed model may be employed to recover accuracy. However, the decomposition in (2) introduces two training-related hurdles: (a) the effective number of trainable parameters has decreased significantly which reduces model expressivity, and (b) the breakage of spatial weight structure prevents the retraining procedure from loading the original optimizer state. Retraining a model without its optimizer state is known to introduce significant difficulty in convergence <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Activation Decomposition via Static Projection</head><p>Since weight decomposition poses the above hurdles, we motivate the need for an activation-centric solution. In some measure, activation compression may be more achievable due to the large stack dimension M comprising batches and sequences. Statistically, the Central Limit Theorem claims that stacking data is likely to exhibit redundancies <ref type="bibr" target="#b24">[25]</ref>. In the case of LLMs, such redundancies are further pronounced due to the likelihood of repeated tokens and information in natural language.</p><p>Therefore, activations should be prime candidates for tensor decomposition. Nevertheless, prior arts have not explored activation decomposition due to one fundamental limitation: unlike weights, activations are generated on the fly; meaning that tensors must be compressed during inference, potentially incurring large runtime penalties.</p><p>We propose to apply static dimensionality reduction on the activation tensor X in (1). Concretely, our proposal is to project X onto a pre-computed static orthonormal matrix P of size K × L, where crucially L &lt;&lt; K. Reconstructing X requires a re-expansion using the transpose of the projection matrix, i.e., X ≈ PP T X. While P T P = I L×L , we note that PP T ̸ = I K×K since L &lt;&lt; K. Thus, the proposed activation transformation is noisy, and in Section 3, we derive optimal conditions on the calibration of P to minimize the effects of this noise.</p><p>Our proposal, described in Figure <ref type="figure" target="#fig_1">2(c</ref>) is to approximate the GEMM in (1) using the following:</p><formula xml:id="formula_3">Y = W T X ≈ W T PP T X = W T PP T X = P T W T P T X<label>(3)</label></formula><p>where we used associativity of matrix multiplication to highlight key aspects of our approach.</p><p>During training/finetuning: we view our GEMM as W T PP T X where X has been replaced by its approximation. We emphasize that P is static and does not get updated during training.</p><p>Meanwhile, W T is fully available for adaptation to the activation approximation. The availability of all learnable weights elides losing model expressivity. The structure of W T is also unchanged and can be mapped to the baseline's optimizer state. Thus, the proposed approach does not suffer from the same limitations as weight decomposition techniques. We do note that introducing P induces a small storage overhead at train time. However, when L &lt;&lt; K, and the order of computation is properly compiled, the number of operations per iteration is lower than baseline training; and, though not central to our contribution, we did observe up to 15% reduction in training iteration time for 50% compressed models.</p><p>During inference: we view our GEMM as P T W T P T X where the required matrices are P of size K × L and P T W of size N × L, which is pre-computed before deployment. Thus, per-layer parameter count required for inference has decreased from KN to L(K + N ), which, provided L &lt;&lt; {K, N } presents an opportunity for significant model compression. For instance, if N = K, i.e., W T is square, and L = K /4, then our method yields 50% compression at inference time. This is one of the compression rates we target in Section 4.</p><p>We emphasize that P is not shared across GEMM layers; rather, each GEMM layer decomposed according to (3) has its own pre-calibrated matrix P. Furthermore, by virtue of (3) not introducing dependencies across mini-batches, our method is fully compatible with data parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Eigen Static Principal Activation Component Estimation</head><p>Our proposed activation decomposition induces an approximation error as X ̸ = PP T X. In this section, we first introduce an ergodic estimation of activation auto-correlation. This important statistic is then used for theoretical constructions of P with guarantees on computational accuracy. Multiple results are presented and then combined in our compression studies in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Activation auto-correlation estimation</head><p>Let x be an arbitrary K-dimensional vector in X; we define the activation auto-correlation matrix of size K × K as C X = E xx T where expectation is taken over activation vectors. This matrix is symmetric positive semi-definite having a real eigenvalue decomposition (EVD) C X = VDV T where V is an orthonormal matrix whose columns are eigenvectors, and D is a diagonal matrix containing the corresponding non-negative eigenvalues, assumed to be sorted in decreasing order. The eigenvector corresponding to the i th largest eigenvalue is called i th principal eigenvector.</p><p>This autocorrelation matrix can be empirically estimated using an instance of the activation tensor:</p><formula xml:id="formula_4">X = [x 1 | . . . |x M ] ⇒ XX T = x 1 x T 1 + . . . + x M x T M ⇒ C X = XX T /M<label>(4</label></formula><p>) However, evaluating (4) and its EVD dynamically introduces a prohibitive computational overhead. Thus, we estimate C X in a pre-deployment calibration process. Specifically, during calibration, we sample and forward pass B random input batches, and for each, calculate C (i) X = X (i) X (i) T /M, where superscript i denotes batch index. Then, we average our estimate of the auto-correlation matrix as</p><formula xml:id="formula_5">C X = B i=1 C (i)</formula><p>X /B and use its eigenvalue decomposition for further optimizations. This ergodic approach of estimating activation statistics as part of a calibration process has been employed to great effect in other compression works on quantization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and pruning <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Activation decomposition with minimum mean squared error</head><p>Let us write X = PP T X; for a vector x ∈ X, its counterpart in x ∈ X is given by:</p><formula xml:id="formula_6">x = L i=1 ⟨p i , x⟩p i<label>(5)</label></formula><p>where {p i } L i=1 are the orthonormal column vectors of P, i.e., ⟨p i , p j ⟩ = 1 {i==j} , ∀i, j ∈ 1 . . . L.</p><p>We define the mean squared error (MSE) of the decomposition as</p><formula xml:id="formula_7">E ∥x -x∥ 2<label>(6</label></formula><p>) with the L 2 -norm used throughout this paper. Our first result constructs P minimizing this MSE.</p><p>Theorem 1. For an activation tensor X whose auto-correlation matrix has an eigenvalue decomposition given by C X = VDV T , the projection matrix P minimizing the mean squared error in (6) is given by P = [v 1 | . . . |v L ] where v i is the i th principal eigenvector in V.</p><p>Proof. See Appendix A.1. The result is readily obtained by substituting x in ( <ref type="formula" target="#formula_6">5</ref>) into (6) and minimizing the MSE which involves quadratic forms involving the positive semi-definite C X .</p><p>Theorem 1 shares similarities with the Principal Component Analysis (PCA) algorithm <ref type="bibr" target="#b28">[29]</ref>. PCA extracts low dimensional features having maximum correlation with input data. Unlike PCA, we omit input normalization to elide its computational cost. Still, we term the columns of P in Theorem 1 as Principal Activation Components. Since those are obtained using an EVD on a static estimation of C X , we call our method Eigen Static Principal Activation Component Estimation (ESPACE).</p><p>The MSE in Theorem 1 is a strong indicator of the quality of an approximation technique, e.g., it is often employed in quantization studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. However, empirical data may contain large outliers which can dominate the optimization process; say a few high-magnitude vectors in <ref type="bibr" target="#b5">(6)</ref> masking the contribution of small data on the solution. An alternate metric to the MSE can be employed to prevent such artifacts in averaging: the normalized MSE (NMSE) defined as:</p><formula xml:id="formula_8">E ∥x-x∥ 2 /∥x∥ 2 (7)</formula><p>The solution of Theorem 1 can be slightly modified to minimize the NMSE in <ref type="bibr" target="#b6">(7)</ref>.</p><p>Corollary 2. For an activation tensor X, let ĈX = E ( x /∥x∥) ( x /∥x∥) T be its input-normalized auto-correlation matrix having an eigenvalue decomposition given by ĈX = VDV T , the projection matrix P minimizing the normalized mean squared error in (7) is given by P</p><formula xml:id="formula_9">= [v 1 | . . . |v L ] where v i is the i th principal eigenvector in V.</formula><p>Proof. The proof in Appendix A.2 uses equivalence of NMSE and MSE with L 2 -normalized vectors.</p><p>Corollary 2 applies to the decomposition in (3) with no activation normalization required at compute time. Rather, normalization is done during calibration, where ĈX is estimated instead of C X .</p><p>Both solutions in Theorem 1 and Corollary 2 are options to be employed in ESPACE, where either may be more suitable on a layer-wise basis. Next we present further options for ESPACE based on the optimization of alternate metrics to the MSE and NMSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Activation decomposition with optimized forward propagated accuracy metrics</head><p>While local fidelity metrics, such as the MSE and NMSE above, are good indicators of the quality of an approximation technique, it has been shown that better insights on a neural network's accuracy may be derived via the study of forward propagated noise <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. In this section, we study the effects of the decomposition in (3) on the output of the GEMM, and the output loss of the model. At a given layer, let us write an arbitrary scalar in the GEMM output tensor in (1) as y ∈ Y. Note that y = ⟨w, x⟩ for some weight vector in w ∈ W and activation vector x ∈ X. We also let ỹ be the associated output when the GEMM is approximated by <ref type="bibr" target="#b2">(3)</ref>, which is given by ỹ = ⟨w, x⟩ with x</p><p>given by ( <ref type="formula" target="#formula_6">5</ref>). We define the GEMM Output-referred MSE (GO-MSE) as</p><formula xml:id="formula_10">E (y -ỹ) 2 .</formula><p>Similarly, given an input to the network, we write the output loss function (the vocab cross-entropy) as L. When one arbitrary activation tensor is transformed as per (3), a mismatch in computation is introduced and propagated all the way to the output. We let L be the resulting new value of the loss function. We define the Network Loss-referred MSE (NL-MSE) as</p><formula xml:id="formula_11">E (L -L) 2 .</formula><p>A closed form solution for P in (3) minimizing the GO-MSE and NL-MSE is elusive to us. Therefore, we derive upper bounds on these metrics which we use as a proxy for optimization. Proposition 3. For a GEMM in (1) and its decomposition in (3), the GO-MSE is upper bounded by:</p><formula xml:id="formula_12">E (y -ỹ) 2 ≤ 2E ∥w∥ 2 • ∥x∥ 2 -2E [⟨w, x⟩ • ⟨w, x⟩]<label>(8)</label></formula><p>and the NL-MSE is upper bounded by</p><formula xml:id="formula_13">E (L -L) 2 ≤ 2E ∥∇ x ∥ 2 • ∥x∥ 2 -2E [⟨∇ x , x⟩ • ⟨∇ x , x⟩]<label>(9)</label></formula><p>where a first order Taylor approximation on the loss function is assumed and its gradient with respect to vector x is denoted as ∇ x .</p><p>Proof. The proof in Appendix A.3 first shows ∥x∥ 2 &lt; ∥x∥ 2 and then uses the Cauchy Schwarz inequality to establish both bounds.</p><p>Next, we provide closed form solutions for P in (3) minimizing the bounds in Proposition 3. Theorem 4. For a GEMM in (1) and its decomposition in (3), the projection matrix minimizing the bounds in Proposition 3 is given by P</p><formula xml:id="formula_14">= [v 1 | . . . |v L ]</formula><p>where v i is the i th principal eigenvector in V obtained via eigenvalue decomposition on a matrix C = VDV T defined as:</p><formula xml:id="formula_15">C = E xx T ww T + ww T xx T and C = E xx T ∇ x ∇ T x + ∇ x ∇ T x xx T<label>(10)</label></formula><p>to minimize the upper bounds on GO-MSE in (8) and NL-MSE in (9), respectively.</p><p>Proof. The proof is included in Appendix A.4, where we also include modifications required in calibration. Specifically, C X is reused and left/right multiplied by WW T /N to yield C in <ref type="bibr" target="#b9">(10)</ref> minimizing the bound on GO-MSE. An additional backward pass is needed to properly scale activation vectors and their gradients when calibrating C in <ref type="bibr" target="#b9">(10)</ref> minimizing the bound on NL-MSE.</p><p>Theorem 4 augments Theorem 1 and Corollary 2 with two options for the design of P. Much like Corollary 2, we supplement our new solutions with L 2 -normalization to include</p><formula xml:id="formula_16">Ĉ = E (xx T ww T +ww T xx T ) /∥w∥ 2 •∥x∥ 2 and Ĉ = E (xx T ∇x∇ T x +∇x∇ T x xx T ) /∥∇x∥ 2 •∥x∥ 2</formula><p>as alternate choices for the calibrated matrices C in <ref type="bibr" target="#b9">(10)</ref>. Unlike Corollary 2, L 2 -normalization in these two matrices does not correspond to a notable optimization. Nevertheless, these options are retained in the spirit of suppressing the influence of large data in calibration.</p><p>Thus, overall we have six choices for P. Since each can be obtained as part of a fast and predeployment calibration phase, we may simply select the best one for each layer. In our experiments of Section 4, the best candidate is determined via a per-layer validation over all six choices. A sensitivity study on the impact of each of the six candidates is provided in Appendix B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Compression Studies</head><p>In this section, we report on experimental studies investigating LLM compression using ESPACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We employ three sets of open source LLMs: GPT3 <ref type="bibr" target="#b32">[33]</ref>, Llama2 <ref type="bibr" target="#b33">[34]</ref>, and Nemotron4 <ref type="bibr" target="#b34">[35]</ref>. Specifically, we experiment on GPT3-{1.3B, 8B, 22B}, Llama2-{7B, 13B}, and Nemotron4-15B. Accuracy is evaluated in two ways: perplexity measured on the Wikitext-103 dataset <ref type="bibr" target="#b35">[36]</ref> and zero-shot downstream task accuracy of: BoolQ (BQ) <ref type="bibr" target="#b36">[37]</ref>, Hellaswag (HS) <ref type="bibr" target="#b37">[38]</ref>, PIQA (PQ) <ref type="bibr" target="#b38">[39]</ref>, RACE (RA) <ref type="bibr" target="#b39">[40]</ref>, and WinoGrande (WG) <ref type="bibr" target="#b40">[41]</ref>.</p><p>The Wikitext-103 dataset is split into train, validation, and test sets. We use 512 random sequences from the training set for calibrating projection matrices required by ESPACE. We use the validation set for layer-wise sensitivity studies. The test set is used to report perplexity results in this section.</p><p>Our implementation uses NVIDIA's Megatron LM <ref type="bibr" target="#b32">[33]</ref> and downstream task evaluation invokes Eleuther AI's LM evaluation harness <ref type="bibr" target="#b41">[42]</ref>. For the latter, we report raw accuracy scores, and their average; we do not post process results or apply normalization to the scores.</p><p>When ESPACE is applied, we retrain the models to adapt to the approximation error of activation projection as discussed in Section 2. Retraining simply extends the models' pre-training sessions and uses the 330B-token MTNLG dataset <ref type="bibr" target="#b42">[43]</ref>, which was used to train GPT3 models. All implementation details are included in Appendix B to help reproducibility of our results.</p><p>We metricize model size reduction via inference compression rate. Specifically, for layers decomposed per (3), we count the number of entries in P and P T W ; for other layers, we count those in W T . We also report the latency of executing all network GEMMs in (1) or ( <ref type="formula" target="#formula_3">3</ref>), which we measure using a NVIDIA A100 GPU and a simple, un-optimized implementation (see Appendix B.4). We also report prefill inference latency, metricized via the Time to First Token (TTFT), and measured using the Megatron-LM implementation. In our measurements, we use a batch size of 1 and sequence length of 2048 and 4096 for GPT3 and Llama2/Nemotron4 models, respectively. The reported reductions in total GEMM latency and TTFT constitute evidence that compression improves inference throughput.</p><p>It is beyond the scope of this paper to evaluate the impact of ESPACE on end-to-end token throughput and latency on LLM inference serving systems, since this requires a complex set of optimizations including but not limited to optimization of back-to-back GEMMs into fused kernels, KV caching, continuous batching, as well as thorough performance studies with varying input and output sequence lengths. Thus, we leave an evaluation of token generation throughput and energy savings and improvements to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Validation perplexity studies</head><p>Our experiments start with a calibration phase where we prepare the static projection matrix P for each layer. The dimension L in P is chosen as the lowest power of two such that layer compression is at least 50%. The power-of-two restriction ensures best tensor core utilization, and the resulting compression rate depends on the dimensions of the original layer (K and N ). Exact details of these values for all layers and models are included in Appendix B.2.</p><p>We perform a sensitivity study on the Wikitext-103 validation perplexity when ESPACE is applied out-of-the-box (no retraining) one layer at a time. For each layer, we identify which of our six candidates projection matrices in Section 3 yields lowest validation perplexity. Layers are then sorted according to their impact on perplexity from least to most destructive. We then evaluate the validation perplexity when ESPACE is progressively applied to out-of-the-box to all layers according to this ranking. Fine-grained details of this exploration are included in Appendix C for all models. This exploration yields an interesting finding: as we progressively apply ESPACE to more layers, the perplexity marginally increases until an inflection point after which accuracy degradation accelerates. This inflection occurs at 20% to 40% compression depending on the model. Figure <ref type="figure" target="#fig_2">3</ref> depicts this phenomenon for GPT3-22B, and the same data for other models can be found in Appendix C.2.</p><p>We find that out-of-the-box application of ESPACE works better for larger models; GPT3-22B, the largest model we experimented on, exhibits an inflection in perplexity at 40% compression, which is the highest in our results. This is consistent with many earlier works on general compression of neural networks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Interestingly, a 20% out-of-the-box compressed GPT3-22B is iso-accurate to its uncompressed counterpart (see Figure <ref type="figure" target="#fig_2">3</ref>); without retraining, its test perplexity of 6.61 which is within 1% of the 6.55 baseline.</p><p>After the above validation study is performed, we select two configurations for layers to be compressed using ESPACE: (a) layers corresponding to the inflection point, i.e., 20% to 40% compression, and (b) as many layers needed to achieve a compression of ∼50%. For both configurations, we retrain the compressed models and further evaluate their achievable accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compression of GPT3 models</head><p>Once compression targets and layer configurations are set, we retrain GPT3 models on the MTNLG dataset. Although we use all of the 330B available tokens, we do observe the training loss quickly converging. We leave training hyperparameters unchanged except for one: we disable dropout. Our rationale is that activation projection is one form of deterministic and structured dropout such that additional regularization may not be needed. Results<ref type="foot" target="#foot_3">foot_3</ref> on GPT3 models are included in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We find that ESPACE can compress GPT3 models by ∼50% at the cost of a small accuracy degradation. In the case of GPT3-22B, the perplexity increase is of only 0.18; in general, the gap decreases for larger overall model size. By and large, similar trends are observed for downstream task accuracies and we note that most scores of 50% compressed models fall within 5% of the baseline.</p><p>For lower compression ratios (inflection points at 20% to 40%), ESPACE converges to an accuracy better than that of the baseline. The best improvement occurs for GPT3-8B, where ESPACE produces a 6B model with 0.38 lower perplexity than its 8B baseline. The improvements are observed both in terms of perplexity and downstream task accuracy. While GPT3 models may be over-parameterized, we posit that ESPACE acts a regularizer at moderate compression rates. Specifically, we believe that projection onto principal activation components filters out unnecessary information coming from small eigenvalue components.</p><p>For all models, we observe an encouraging translation of compression to GEMM latency reduction by up to 49% which leads to noticeable speed-up in TTFT by up to 43%..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Compression of Llama2 models and comparison to related works</head><p>For Llama2, we only retrain using 200B MTNLG tokens because we observed quick convergence for GPT3. Llama2 models were trained on an undisclosed dataset of 2T tokens <ref type="bibr" target="#b33">[34]</ref>. Therefore, with 200B tokens, the healing phase of ESPACE constitutes no more than 10% of the original pre-training session. Since Llama2 pre-training details are not openly available, we re-used all hyperparameters from GPT3, which is likely to be sub-optimal. In spite of the two handicaps of dataset disparity and hyperparameter sub-optimality, we obtained promising results as reported in Table <ref type="table" target="#tab_0">1</ref>.</p><p>For Llama2-7B, we first retrained the uncompressed baseline. The purpose of this experiment is twofold: (a) ensure that our hyperparameters at least do not corrupt the model, and (b) verify that the healing process is not just an artifact of processing more tokens. Both hypotheses appeared to be valid: the retrained baseline has nearly identical accuracy compared to the original model.</p><p>Generally, we find that the trends of ESPACE compression for Llama2 are similar to GPT3, albeit slightly less successful. Though the results are still promising, we attribute the slight shortcomings in accuracy to the handicaps above. We find that 50% ESPACE compression on Llama2 leads to ∼0.6 perplexity increase and similar degradation in terms of downstream task accuracy. Notably, compressing the Llama2-13B model to to a 6.3B model yields comparable accuracy to the Llama2-7B baseline which itself is a 6.5B model.</p><p>In addition, for 20% compression, we find that ESPACE matches the accuracy of the baseline for Llama2 models. While not as impressive as the improvements observed with GPT3, ESPACE is able to produce 5B and 10B models matching the 7B and 13B baselines, which does push the pareto frontier of accuracy versus model size in the right direction as shown in Figure <ref type="figure" target="#fig_0">1</ref>.  The Llama2-7B model has been used in related works on tensor decomposition mentioned in Section 1.1; specifically, ASVD <ref type="bibr" target="#b19">[20]</ref>, SVD-LoRa <ref type="bibr" target="#b17">[18]</ref>, and SliceGPT <ref type="bibr" target="#b20">[21]</ref>.</p><p>Both ASVD and sliceGPT have reported perplexity on Wikitext, but SVD-LoRa performed task-specific finetuning on a variety of datasets and averaged perplexities. Therefore, in Figure <ref type="figure" target="#fig_4">4</ref>, we compare our results to these works using perplexity increase over baseline, rather than raw perplexity, for maximum inclusivity.</p><p>SVD-LoRa performed an SVD decomposition on the weights such that the intermediate dimension is half of dotproduct which leads to no compression. On the other hand, ASVD and sliceGPT can only achieve modest compression ratios of up to 25% with some loss in accuracy. Recall that these works apply factorization on weights which is the fundamental difference to ESPACE. As seen in Figure <ref type="figure" target="#fig_4">4</ref>, ESPACE is a step in the right direction towards improving the state-of-the-art in tensor decomposition of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compression of Nemotron4-15B</head><p>Finally, we used ESPACE to compress Nemotron4-15B into 9.54 and 6.25 billion parameters, as reported in Table <ref type="table" target="#tab_0">1</ref>. Retraining consumed 275B tokens which corresponds to ∼ 3% of this model's original training session. Once more, compression with ESPACE leads to minimal degradation in the moderate regime (25%) and yields a small accuracy drop in the aggressive regime (50%).</p><p>Consistently with our findings for the above models, ESPACE reduces GEMM execution time by up to 46%. This, in turn, improves the TTFT by up to 26%. An interesting observation is that, for Llama2 and Nemotron4 models, the TTFT improvement is slightly less pronounced than for GPT3 models. This is simply due to the fact that the latter uses a sequence length of 2048, whereas the former two use 4096. A larger sequence length means more time is spent in attention cross-activation products which amortizes the speed-up in the GEMM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented ESPACE, a novel compression technique realizing tensor decomposition of LLMs in an activation-centric manner. A set of theoretical results were derived to guide the construction of activation projection which is done statically. Experimentally, we have shown promising results where ESPACE is able to ∼50% compress modern LLMs at the cost of a small accuracy degradation. Compared to related works, ESPACE is a first step in pushing the frontier of model size versus accuracy trade-offs. Future work includes combining ESPACE with alternate compression techniques such as quantization and pruning, and evaluating decomposition of activation tensors in attention. As potential extension to our algorithm, the use of matrix sketching and random projections may pave the way for better overall compressibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs of theoretical results</head><p>In this first appendix, we provide proofs for the various theoretical results in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 1</head><p>For a pair of vectors x ∈ X and x ∈ X, and using (5), we have the squared error:</p><formula xml:id="formula_17">∥x -x∥ 2 = ∥x∥ 2 + ∥x∥ 2 -2x T x = ∥x∥ 2 + ∥x∥ 2 -2 L i=1 p i T x p i T x ⇒ ∥x -x∥ 2 = ∥x∥ 2 + ∥x∥ 2 -2 L i=1 p i T x<label>2</label></formula><p>Furthermore, note that</p><formula xml:id="formula_18">∥x∥ 2 = xT x = L i=1 p i T x p i T L i=1 p i T x p i</formula><p>and since {p i } L i=1 are orthonromal, cross product terms vanish and we have:</p><formula xml:id="formula_19">∥x∥ 2 = L i=1 p i T x 2</formula><p>which we plug back into the expression for the squared error:</p><formula xml:id="formula_20">∥x -x∥ 2 = ∥x∥ 2 + L i=1 p i T x 2 -2 L i=1 p i T x 2 = ∥x∥ 2 - L i=1 p i T x<label>2</label></formula><p>Furthermore, note that:</p><formula xml:id="formula_21">p i T x 2 = p i T x p i T x = p i T x x T p i = p i T xx T p i</formula><p>where we used the commutativity of dot product and associativity of matrix multiplication. Thus the squared error is given by:</p><formula xml:id="formula_22">∥x -x∥ 2 = ∥x∥ 2 - L i=1 p i T xx T p i</formula><p>Finally, we take expectation on both sides and obtain a formula for the MSE:</p><formula xml:id="formula_23">E ∥x -x∥ 2 = E ∥x∥ 2 -E L i=1 p i T xx T p i = E ∥x∥ 2 - L i=1 p i T E xx T p i</formula><p>where we used linearity of expectation and the fact that {p i } L i=1 are not random. In this formula for the MSE, E ∥x∥ 2 does not depend on {p i } L i=1 , and therefore, minimizing the MSE is equivalent to maximizing the following expression involving the auto-correlation matrix:</p><formula xml:id="formula_24">L i=1 p i T E xx T p i = L i=1 p i T C X p i</formula><p>where each term in the summation is a quadratic form on the positive semi-definite auto-correlation matrix C X . Since {p i } L i=1 are orthonormal, this is an equivalent form of the Rayleigh quotient <ref type="bibr" target="#b47">[48]</ref> and the solution is to assign {p i } L i=1 as the L principal eigenvectors of C X . This concludes the proof of Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Corollary 2</head><p>The result can be readily obtained as a consequence of the following:</p><formula xml:id="formula_25">E ∥x -x∥ 2 ∥x∥ 2 = E x ∥x∥ - x ∥x∥ 2 = E X   L i=1 ⟨p i , x⟩p i ∥x∥ - x ∥x∥ 2   ⇒ E ∥x -x∥ 2 ∥x∥ 2 = E   L i=1 ⟨p i , x ∥x∥ ⟩p i - x ∥x∥ 2  </formula><p>Therefore, the setup is identical to that of Theorem 1 and we may apply the same solution as Appendix A.1 above. The only difference is that activation vectors are L 2 -normalized which is why ĈX (which is also positive semi-definite) is used in lieu of C X in Corollary 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Proposition 3</head><p>A prelimnary result needed is to show that for any activation vector, we have ∥x∥ 2 &lt; ∥x∥ 2 . We first note that while x = L i=1 ⟨p i , x⟩p i per (5), we also have that x = K i=1 ⟨p i , x⟩p i , where {p i } K i=1 extend the set of orthonormal vectors {p i } L i=1 to be complete, i.e., equivalent to no truncation of columns of the full rank matrix V when constructing P, regardless of the metric being optimized.</p><p>Using orthonormality of projection vectors, similar to the proof of Theorem 1 in Appendix A.1 above, we obtain ∥x∥ 2 = L i=1 p i T x 2 and ∥x∥ 2 = K i=1 p i T x 2 . Therefore:</p><formula xml:id="formula_26">∥x∥ 2 -∥x∥ 2 = K i=L+1 p i T x 2 ≥ 0 ⇒ ∥x∥ 2 &lt; ∥x∥ 2</formula><p>where we used the fact that a sum of non-negative quantities is non-negative.</p><p>Then for a scalar y ∈ Y and its counterpart ỹ ∈ Ỹ, we have:</p><formula xml:id="formula_27">(y -ỹ) 2 = w T x -w T x 2 = w T x 2 + w T x 2 -2 w T xw T x ≤ ∥w∥ 2 • ∥x∥ 2 + ∥w∥ 2 • ∥x∥ 2 -2 w T xw T x ≤ ∥w∥ 2 • ∥x∥ 2 + ∥w∥ 2 • ∥x∥ 2 -2 w T xw T x = 2∥w∥ 2 • ∥x∥ 2 -2 w T xw T x</formula><p>where the first upper bound uses the Cauchy-Schwarz inequality while the second uses ∥x∥ 2 &lt; ∥x∥ 2 which we proved above. Taking expectations on both sides of the inequality yields the upper bound on GO-MSE in <ref type="bibr" target="#b7">(8)</ref>.</p><p>Next, when a first order Taylor approximation on the loss function is assumed, we have the following relation between the unperturbed loss value L and its counterpart L when an activation vector x is projected to x per (5):</p><formula xml:id="formula_28">L = L + ∇ T x (x -x) ⇒ L -L = ∇ T x x -∇ T x x ⇒ L -L 2 = ∇ T x x -∇ T x x 2 = ∇ T x x 2 + ∇ T x x 2 -2 ∇ T x x∇ T x x</formula><p>once more, we use the Cauchy-Schwarz inequality and the fact that ∥x∥ 2 &lt; ∥x∥ 2 to establish:</p><formula xml:id="formula_29">∇ T x x 2 ≤ ∥∇ x ∥ 2 • ∥x∥ 2 &amp; ∇ T x x 2 ≤ ∥∇ x ∥ 2 • ∥x∥ 2 ≤ ∇ x ∥ 2 • ∥x∥ 2</formula><p>which we plug into the difference in network losses above to obtain:</p><formula xml:id="formula_30">L -L 2 ≤ 2∥∇ x ∥ 2 • ∥x∥ 2 -2 ∇ T x x∇ T x x</formula><p>Taking expectations on both sides yields the upper bound on NL-MSE in <ref type="bibr" target="#b8">(9)</ref>. This completes the proof of Proposition 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 4</head><p>In order to minimize the upper bound on the GO-MSE in <ref type="bibr" target="#b7">(8)</ref>, note that it suffices to maximize the quantity 2E [⟨w, x⟩ • ⟨w, x⟩] since 2E ∥w∥ 2 • ∥x∥ 2 does not depend on {p i } L i=1 . We have the following:</p><formula xml:id="formula_31">2⟨w, x⟩ • ⟨w, x⟩ = 2w T xw T L i=1 p T i x p i = 2 L i=1 w T xw T p i p T i x = L i=1 w T xw T p i p T i x + w T xw T p i p T i x</formula><p>But since the dot product is commutative, i.e., a T b = b T a for any two vectors a and b, we may rearrange each of the two identical terms inside the summation as follows:</p><formula xml:id="formula_32">w T xw T p i p T i x = p T i xx T ww T p i &amp; w T xw T p i p T i x = p T i ww T xx T p i</formula><p>Therefore, we obtain:</p><formula xml:id="formula_33">2⟨w, x⟩ • ⟨w, x⟩ = L i=1 p T i xx T ww T p i + p T i ww T xx T p i = L i=1 p T i xx T ww T + ww T xx T p i</formula><p>Taking expectations, we find that the quantity that needs to be maximized in order to minimize the bound on GO-MSE in ( <ref type="formula" target="#formula_12">8</ref>) is:</p><formula xml:id="formula_34">L i=1 p T i E xx T ww T + ww T xx T p i</formula><p>Similar to the proof of Theorem 1 in Appendix A.1, this is yet again a sum of a quadratic form over the orthonormal set of vectors {p i } L i=1 and the solution is therefore to assign these vectors as the L principal vectors of C = E xx T ww T + ww T xx T as per <ref type="bibr" target="#b9">(10)</ref>.</p><p>Note that the derivation above decomposed the dot products to obtain a quadratic form on the matrix xx T ww T + ww T xx T because its symmetry is required for real eigenvalue decomposition. Also note that if positive definiteness is not achieved, we sort absolute values of eigenvalues. Finally, observe that this solution requires no overhead on the calibration process. Indeed, assuming weights and activations are independent, we note that</p><formula xml:id="formula_35">E xx T ww T + ww T xx T = C X C W + C W C X</formula><p>where C W = E ww T is the weight auto-correlation matrix, which can simply be calibrated as</p><formula xml:id="formula_36">C W = WW T /N.</formula><p>Thus we only require left and right scaling of the calibrated auto-correlation matrix.</p><p>Similarly, to minimize the upper bound on NL-MSE in <ref type="bibr" target="#b8">(9)</ref>, it suffices to maximize</p><formula xml:id="formula_37">2E [⟨∇ x , x⟩ • ⟨∇ x , x⟩].</formula><p>Using the exact same derivation as the above, replacing w by ∇ x , we obtain that the quantity to be maximized is</p><formula xml:id="formula_38">L i=1 p T i E xx T ∇ x ∇ T x + ∇ x ∇ T x xx T p i which is done by assigning {p i } L i=1 as the L principal vectors of C = E xx T ∇ x ∇ T x + ∇ x ∇ T</formula><p>x xx T as per <ref type="bibr" target="#b9">(10)</ref>. Calibrating this matrix does require an extra step, where we perform a backward pass to estimate activation gradients. For ease of implementation, in our results, we make an approximation on the per-sequence independence of activation vectors and their gradients. This greatly reduces the memory requirements of the calibration process. And for each sample sequence in the calibration set, we compute </p><formula xml:id="formula_39">C (i) = X (i) X (i) T G (i) G (i) T +G (i) G (i) T X (i) X (i) T /M 2 where G (i)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional experimental results</head><p>In this appendix, we include additional experimental results that were not included in the main paper. These results are not essential to the description of our work nor its conclusion, and the main paper integrally contains all essential information related to our contribution. The additional results listed in this appendix are for the benefit of readers interested in going further and learning about fine-grained details behind the main results of Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Sensitivity studies on the construction of projection matrix</head><p>In Section 3, we presented theoretical results leading to six choices for the construction of projection matrix P. These constructions were done in a manner to optimize one of the following fidelity metrics: MSE, NMSE, GO-MSE, GO-MSE with L 2 -normalization, NL-MSE, and NL-MSE with L 2 -normalization. Here we show the impact of various choices of P constructions on the validation perplexity, at a per-layer granularity. Specifically, we apply the ESPACE projection out-of-the-box one layer at a time, for each of the six candidates, and evaluate the resulting validation perplexity. In this way, we are able to determine which of the six candidate choices of P works best at each layer.</p><p>The results of this sensitivity analysis are included in Figures <ref type="figure" target="#fig_7">5</ref> and <ref type="figure">6</ref> for GPT3 and Llama2 models, respectively. It is shown that the best choice of projection matrix P depends on layer instance, and there is no clear pattern to find out which solution works best a priori. This result justifies the need to optimize several proxy metric for accuracy, not just the MSE of activation approximation. Particularly, most solutions do appear to be related to GO-MSE and NL-MSE, as well as thei L 2 -normalized variants. Therefore, these results provide supporting evidence on the importance of the results in Proposition 3 and Theorem A.4. This also validates the choice of using bounds on GO-MSE and NL-MSE for optimization since closed form solution for the unbounded metrics are elusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Progressive application of ESPACE to the layers of a network</head><p>Once the best projection matrix P is identified for each layer, we plot the corresponding validation perplexity for out-of-the-box application of ESPACE at the corresponding layer using the corresponding choice of P. These results are shown in Figure <ref type="figure">7</ref>, where several observations are made. First, larger models have more resilience to out-of-the-box application of ESPACE; this observation was made in Section 4. Second, it appears that FC1 layers are the most sensitive ones, followed by FC2, and QKV/Proj layers are generally robust to the application of ESPACE. Finally, we observe that in some instances, some layers close to the input and output (i.e., on either ends of the model) appear to be most sensitive to the application of ESPACE. This behavior was observed in other works on compression, such as shortGPT <ref type="bibr" target="#b50">[51]</ref>.</p><p>As mentioned in Section 4, layers are then sorted according to their impact on perplexity from least to most destructive. ESPACE is then progressively applied to out-of-the-box to all layers according to this ranking. In Figure <ref type="figure" target="#fig_2">3</ref> in the main text, we had shown the results corresponding to this applciation for GPT3-22B. In Figure <ref type="figure" target="#fig_6">8</ref>, we show similar results for the other four networks we experimented on, i.e., GPT3-{1.3B, 8B} and Llama2-{7B, 13B}. Similar to the findings on GPT3-22B, we do observe an inflection point after which accuracy degradation accelerates. This inflection occurs around 20% for GPT3-{1.3B, 8B} and Llama2-{7B, 13B}. With retraining, the healing process recovers accuracy for all models, as detailed in Section 4.</p><p>We note the following:</p><p>• For GPT3-1.3B, we exclude some layers from the application of ESPACE. These are the layers for which out-of-the-box application of ESPACE leads to a validation perplexity increase of more than 2% compared to the baseline. These layers can be found in Figure <ref type="figure">7</ref> and correspond to several FC1 and FC2 layers close to either ends of the model. For this reason, GPT3-1.3B is compressed to 47% instead of 50% in Section 4 and Table <ref type="table" target="#tab_0">1</ref>. • For GPT3-22B, we apply ESPACE to all layers since validation perplexity increase is very small in Figure <ref type="figure">7</ref>. For this reason, the overall compression for GPT3-22B is slightly over 50%; it is 55% in Section 4 and Table <ref type="table" target="#tab_0">1</ref>.</p><p>NeurIPS Paper Checklist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Claims</head><p>Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p><p>Answer: [Yes]</p><p>Justification: We accurately report the paper's contribution in the abstract and introduction; this is why we chose to have a bulleted list in a standalone subsection in Section 1.2. For clarity, we listed all contributions in the paper, in the order in which they appear: our proposal, followed by theoretical results, and summaries of experimental results.</p><p>Guidelines:</p><p>• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Limitations</head><p>Question: Does the paper discuss the limitations of the work performed by the authors?</p><p>Answer: [Yes]</p><p>Justification: In our discussion on related works in Section 1.1, we have listed possible combinations of applying ESPACE alongside other methods (e.g., quantization or parameter efficient tuning), and have mentioned that this was not the scope of our paper. Furthermore, in the experimental Section 4, we have discussed the limitations of only metricizing compression via model size and weight times activation GEMM latency reductions. We argued that a more nuanced study is needed on the inference cost implications as Transformers comprise other GEMMs, and the regime (context pre-fill versus auto-regressive phase) needs to be taken into account. We have mentioned that a detailed study on inference cost with ESPACE is part of future work.</p><p>Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.</p><p>• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: We provide detailed proofs in Appendix A for all results in the theoretical Section 3. Furthermore, following each Theorem, Proposition, and Corollary in Section 3, we also provide a teaser to the full proof in the main text, for the benefit of interested readers, where we try to provide the main intuition before referring to full proofs in Appendix A.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: All details, including fine-grained configurations, software toolkit employed, and training recipes including hyperaparameters are included in Appendix B. These are also briefly mentioned in the main paper in Section 4, with references to Appendix B for the readers interested in more details needed for full reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guidelines:</head><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Open access to data and code</head><p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p><p>Answer: <ref type="bibr">[No]</ref> Justification: As was mentioned in the previous question, we have provided all implementation details required to reproduce our results, including hyperparameters and a description of software implementation in Appendix B. We note that our implementation is based on the open-source Megatron-LM <ref type="bibr" target="#b32">[33]</ref> and that additions made to this software toolkit needed to reproduce our results are included in Appendix B, where we also include an invitation to the reader to reach out to us (after blind reviewing is over) with any questions regarding reproducibility. As such, we believe the description of the work in the paper is sufficient for reproducibility; yet, we are happy to consider open sourcing our code in the future.</p><p>Guidelines:</p><p>• The answer NA means that paper does not include experiments requiring code.</p><p>• Please see the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https://nips.cc/ public/guides/CodeSubmissionPolicy</ref>) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so 'No' is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https: //nips.cc/public/guides/CodeSubmissionPolicy</ref>) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p><p>• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setting/Details</head><p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p><p>Answer: [Yes] Justification: Yes, and this was already mentioned in our responses to the two questions above. We provide all fine-grained details of our implementation in Appendix B, which is referred to in the main text.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Statistical Significance</head><p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: While we do not report error bars, we do evaluate our method on a variety of downstream tasks, for the specific purpose of making conclusions with statistical significance; rather than relying on one number here and there. We also note that this approach of evaluating on several tasks is also widely adopted in the community for works on compression of LLMs.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments. • The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: These are reported in Section 4 and Appendix B. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <ref type="url" target="https://neurips.cc/public/EthicsGuidelines">https://neurips.cc/public/EthicsGuidelines</ref>? Answer: [Yes]</p><p>Justification: Yes, we are only working on a method to compress LLMs using tensor decomposition of activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guidelines:</head><p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p><p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Broader Impacts</head><p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p><p>Answer: <ref type="bibr">[NA]</ref> Justification: As per the guideline below, our work falls under the umbrella of optimization to the implementation of LLMs. Similar to the example provided in the guideline, we believe that there is no need to point out societal implications of making LLMs run faster.</p><p>Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: [NA]</head><p>Justification: There are no such risks associated with our work, and as such the paper does not describe safeguards.</p><p>Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have done our best to provide credit to any prior work upon which we have built ours. This is mostly the case for Megatron-LM <ref type="bibr" target="#b32">[33]</ref>, which we used for our experiments, and cited extensively throughout Section 4 and Appendix B.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not release new assets.</p><p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.</p><p>• At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Perplexity 2 versus model size for GPT3 and Llama2 models and comparison to compressed models using ESPACE.</figDesc><graphic coords="2,302.22,389.75,79.56,50.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Decompositions in GEMMs: (a) baseline multiplication of weight matrix and activation tensor, (b) truncated SVD on the weight matrix, and (c) proposed approach of inserting a static matrix to project activations. With ESPACE, all weights are available for training, while inference compression is achieved via per-computation of P T W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Validation perplexity for GPT3-22B when ESPACE is progressively applied to its GEMM layers. The order of layer selection is based on a layer-wise sensitivity analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison to related works compressing Llama2-7B using matrix factorization techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Sensitivity studies on the choice of projection construction for (a) Llama2-7B, (b) Llama2-13B.For each layer, we apply ESPACE out-of-the-box using the six various candidates for the projection matrix P constructed in Section 3. The black line corresponds to the baseline perplexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Progressive out-of-the-box application of ESPACE on GPT3{1.3B, 8B} and Llama2-{7B, 13B}. The plot for GPT3-22B was provided in the main text in Figure 3. The progressive application of ESPACE is based on the ranking of layers from least to most destructive based on validation perplexity sensistivity in Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )</head><label>a</label><figDesc>If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>GEMM latency, time to first token, Wikitext-103 perplexity (WK-103 PPL), and downstream task accuracy of GPT3, Llama2, and Nemotron4 models compressed with ESPACE 3 .</figDesc><table><row><cell>Method</cell><cell># of</cell><cell>Total GEMM</cell><cell>TTFT impl. in</cell><cell>WK-103</cell><cell></cell><cell cols="4">Downstream Task Accuracy ↑</cell></row><row><cell>(Compression)</cell><cell>Weights</cell><cell>Latency</cell><cell>Megatron-LM</cell><cell>PPL ↓</cell><cell>BQ</cell><cell>HS</cell><cell>PQ</cell><cell>RA</cell><cell cols="2">WG Avg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GPT3-1.3B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>1.21 × 10 9</cell><cell>24.2ms</cell><cell>39.8ms</cell><cell>9.94</cell><cell cols="5">64.3 43.5 74.2 37.6 58.1</cell><cell>55.5</cell></row><row><cell>ESPACE (20%)</cell><cell>9.71 × 10 8</cell><cell>20.6ms (-15%)</cell><cell>36.1ms (-9%)</cell><cell>9.53</cell><cell cols="5">60.6 45.1 73.0 36.4 62.9</cell><cell>55.6</cell></row><row><cell>ESPACE (47%)</cell><cell>6.42 × 10 8</cell><cell cols="2">15.9ms (-34%) 31.7ms (-20%)</cell><cell>11.07</cell><cell cols="5">62.3 39.9 71.6 34.5 58.7</cell><cell>53.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GPT3-8B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>8.05 × 10 9</cell><cell>136ms</cell><cell>186ms</cell><cell>7.38</cell><cell cols="5">69.0 54.2 78.1 41.4 67.8</cell><cell>62.1</cell></row><row><cell>ESPACE (21%)</cell><cell>6.33 × 10 9</cell><cell>110ms (-19%)</cell><cell>155ms (-16%)</cell><cell>7.00</cell><cell cols="5">70.3 55.3 78.9 40.7 69.3</cell><cell>62.9</cell></row><row><cell>ESPACE (50%)</cell><cell>4.08 × 10 9</cell><cell>76.8ms (-44%)</cell><cell>122ms (-35%)</cell><cell>7.66</cell><cell cols="5">66.5 52.3 77.6 38.9 66.9</cell><cell>60.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GPT3-22B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>2.17 × 10 10</cell><cell>354ms</cell><cell>457ms</cell><cell>6.55</cell><cell cols="5">76.4 57.2 79.3 40.7 70.5</cell><cell>64.8</cell></row><row><cell cols="2">ESPACE (40%) 1.30 × 10 10</cell><cell>229ms (-35%)</cell><cell>313ms (-32%)</cell><cell>6.29</cell><cell cols="5">76.6 57.3 79.5 40.2 70.2</cell><cell>64.8</cell></row><row><cell>ESPACE (55%)</cell><cell>9.74 × 10 9</cell><cell>181ms (-49%)</cell><cell>261ms(-43%)</cell><cell>6.73</cell><cell cols="5">72.2 55.8 79.3 40.1 69.7</cell><cell>63.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Llama2-7B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>6.48 × 10 9</cell><cell>210ms</cell><cell>368ms</cell><cell>5.06</cell><cell cols="5">79.2 57.1 78.1 44.0 69.5</cell><cell>65.6</cell></row><row><cell>Retrained (0%)</cell><cell>6.48 × 10 9</cell><cell>210ms</cell><cell>368ms</cell><cell>5.06</cell><cell cols="5">78.2 57.9 78.0 43.7 70.6</cell><cell>65.7</cell></row><row><cell>ESPACE (21%)</cell><cell>5.11 × 10 9</cell><cell>169ms (-19%)</cell><cell>322ms (-12%)</cell><cell>5.07</cell><cell cols="5">77.1 57.1 78.7 42.7 69.2</cell><cell>65.0</cell></row><row><cell>ESPACE (50%)</cell><cell>3.24 × 10 9</cell><cell>113ms (-46%)</cell><cell>266ms (-28%)</cell><cell>5.67</cell><cell cols="3">72.2 52.0 76.5</cell><cell>38</cell><cell>63.5</cell><cell>60.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Llama2-13B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>1.27 × 10 10</cell><cell>406ms</cell><cell>643ms</cell><cell>4.61</cell><cell cols="5">82.4 60.2 79.5 46.8 71.9</cell><cell>68.2</cell></row><row><cell cols="2">ESPACE (20%) 1.01 × 10 10</cell><cell>336ms (-17%)</cell><cell>562ms (-13%)</cell><cell>4.59</cell><cell cols="5">78.3 60.5 79.5 43.0 72.8</cell><cell>66.8</cell></row><row><cell>ESPACE (50%)</cell><cell>6.34 × 10 9</cell><cell>259ms (-36%)</cell><cell>447ms (-31%)</cell><cell>5.13</cell><cell cols="5">75.7 56.2 78.0 41.5 69.1</cell><cell>64.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Nemotron4-15B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>1.25 × 10 10</cell><cell>414ms</cell><cell>741ms</cell><cell>6.06</cell><cell cols="5">78.3 62.1 81.1 47.0 75.2</cell><cell>68.7</cell></row><row><cell>ESPACE (25%)</cell><cell>9.54 × 10 9</cell><cell>324ms (-22%)</cell><cell>655ms (-12%)</cell><cell>6.28</cell><cell cols="6">78.9 59.9 80.0 46.4 72.8 67.6</cell></row><row><cell>ESPACE (50%)</cell><cell>6.25 × 10 9</cell><cell>223ms (-46%)</cell><cell>545ms (-26%)</cell><cell>6.93</cell><cell cols="5">77.9 57.0 77.8 42.4 69.9</cell><cell>65.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use the french pronunciation "espace", which means "space".38th Conference on Neural Information Processing Systems (NeurIPS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2024).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Perplexity depends on tokenizer so that comparisons across LLM families (GPT3 vs Llama2) are not useful.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Boldface indicates best result per task. Italics indicates ESPACE results within 5% of the baseline</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>B i=1 C (i)/B. This completes the proof of Theorem 4.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation details</head><p>In this appendix, we discuss all details behind our implementation in Section 4. These details include per-model specific application of ESPACE as well as retraining recipes. We strive to provide excessive details such that independent reproducibility of our results is seamless. We also encourage readers to reach out to us (after blind reviewing) for any questions on implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Software implementation</head><p>As was mentioned in Section 4, our implementation is built on top of Megatron-LM <ref type="bibr" target="#b32">[33]</ref> which itself is based on the Pytorch framework. We use Pytorch for all extra introductions needed by ESPACE except for eigenvalue decomposition. Our experiments were carried out in a cluster of A100 GPUs and use BF16 precision. Specifically, during the calibration process, we use Pytorch to track the required auto-correlation matrices; this simply done by averaging repeated instantiations of XX T as described in Section 3.</p><p>Once the calibration of auto-correlation matrices is over, we use DLPack to transfer them from the Pytorch framework to RAPIDS framework. We then use the CUPY library in RAPIDS to perform fast (a few milliseconds per auto-correlation matrix) eigenvalue decomposition on GPUs. After truncating eigenvectors, we send back the projection matrix to Pytorch using DLPack.</p><p>Once the projection matrix P is calibrated and inference/training is to be done using ESPACE, we simply insert a projection operation within the Megatron implementation to perform the operations in (3) as appropriate. The projection matrices are inserted as Pytorch buffers, rather than parameters, since they do not get updated during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ESPACE configurations</head><p>In Section 4, we mentioned that ESPACE was applied at each layer such that the number of components L satisfies two constraints: (a) be a power of two for best tensor core utilization, and (b) yield a compression of at least 50% at that layer. The exact values of L for each model and layer type are included in Table <ref type="table">2</ref>. Note that the only exception corresponds to QKV layers in Llama2-13B and Nemotron4-15B, where we use a value of L = 2048 which corresponds to a compression of ∼ 45% instead of &gt;50% at least. This is only because this amount of compression is already significant that we didn't feel the need to push for L = 1024, which would have lead to a compression of &gt; 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Retraining hyperparameters</head><p>By and large, we use the exact same recipe that was used to pretrain the open source GPT3 models <ref type="bibr" target="#b32">[33]</ref>. As mentioned in Section 4, the only modification to hyperparameters is disabling dropout and weight decay, and identical hyperparameters are used for both sets of experiments on GPT3 and Llama2 families. The only arbitrary choices we had to make was on the selection of learning rate schedule and global batch size. We use a cosine decay for all runs, and remaining choicesa are as follows:</p><p>• For GPT3-1.3B, the initial learning rate is set to 1.0 × 10 -4 , the final learning rate is set to 1.0 × 10 -5 , and the global batch size is set to 512. • For GPT3-8B, the initial learning rate is set to 5.0×10 -5 , the final learning rate is set to 5.0×10 -6 , and the global batch size is set to 512. • For GPT3-22B, the initial learning rate is set to 5.0 × 10 -5 , the final learning rate is set to 5.0 × 10 -6 , and the global batch size is set to 1024. • For Llama2-7B and Llama2-13B, training is done in two stages (each of 100B tokens). In the first stage, the initial learning rate is set to 5.0 × 10 -4 , and the final learning rate is set to 5.0 × 10 -5 .</p><p>In the second stage, the initial learning rate is set to 5.0 × 10 -5 , and the final learning rate is set to 5.0 × 10 -6 . For both stages, the global batch size is set to 256. • For Nemotron4-15B, the initial learning rate is set to 1.0 × 10 -5 , the final learning rate is set to 0, and the global batch size is set to 512.</p><p>We did not perform hyperparameter tuning, the above was purely arbitrary, but based on the following intuition:  • For GPT3 models, we use a smaller learning rate for larger models, and start with a learning rate 10× smaller than it's pre-training value. We use identical batch sizes as pre-training. • For Llama2 models, as pre-training hyperparameters are undisclosed, we use our best guess of what could work well. The two stage training approach is inspired by a recent work on 1.58-bit LLMs <ref type="bibr" target="#b48">[49]</ref>, while the choice of a batch size of 256 is inspired by ChipNemo <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 GEMM latency measurements</head><p>Here we describe the methodology employed to measure GEMM latency as reported in Table <ref type="table">1</ref>. We assume a batch size of 1, such that the M dimension of tensor X equals the sequence length (2048 and 4096 for GPT3 and Llama2 models, respectively). We also assume a single-GPU implementation throughout, such that any tensor parallelism is first folded into single GEMM per-layer. Similar to our accuracy experiments, we use BF16 precision for all latency measurements.</p><p>For each GEMM layer implementing either (1) or (3), we measure its latency individually. In Table <ref type="table">1</ref>, we report aggregated measurements depending on the model configuration. Specifically, we measure latency of computing QKV, Proj, FC1, and FC2 GEMMs with dimensions listed in Table <ref type="table">2</ref>, and then add all results together for each transformer block of the corresponding model.</p><p>We measure individual GEMM latencies in Pytorch. Specifically, for each configuration, we sample 1000 set of matrices of appropriate dimension and compute the appropriate GEMM. We synchronize before and after the computation occurs, and record times after synchronization. The elapsed times are averaged and then aggregated. Since the measurements were taken with native PyTorch code, we note that the implementation is un-optimized. Further improvements could be possible in future work from removing PyTorch overheads, implementing custom fused kernels, or other optimizations. Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To repeat or not to repeat: Insights from scaling llm under token-crisis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding llms: A comprehensive overview from training to inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02038</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smoothquant: Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="38087" to="38099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparsegpt: Massive language models can be accurately pruned in one-shot</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10323" to="10337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Awq: Activation-aware weight quantization for llm compression and acceleration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00978</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gptq: Accurate post-training quantization for generative pre-trained transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17323</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Llm-pruner: On the structural pruning of large language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="21702" to="21720" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Relu strikes back: Exploiting activation sparsity in large language models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Del Mundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Samei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04564</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orca: A distributed serving system for {Transformer-Based} generative models</title>
		<author>
			<persName><forename type="first">G.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="521" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speculative decoding with big little decoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Symposium on Operating Systems Principles</title>
		<meeting>the 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="611" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficiently programming large language models using sglang</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.07104</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Kronecker decomposition for gpt compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edalati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tahaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Nia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08152</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ternary singular value decomposition as a better parameterized form in linear mapping</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07641</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.00526</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Heat: Hardwareefficient automatic tensor decomposition for transformer compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.16749</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Low-rank pruning of llama2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Badri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<ptr target="https://mobiusml.github.io/low-rank-llama2/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Asvd: Activation-aware singular value decomposition for compressing large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.05821</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Croci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G D</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.15024</idno>
		<title level="m">Slicegpt: Compress large language models by deleting rows and columns</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Galore: Memory-efficient llm training by gradient low-rank projection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03507</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cited on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Information theory and the central limit theorem</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal clipping and magnitude-aware differentiation for improved quantization-aware training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="19123" to="19138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Integer quantization for deep learning inference: Principles and empirical evaluation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09602</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient transformer inference with statically structured sparse attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Genc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 60th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley interdisciplinary reviews: computational statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="433" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Analytical guarantees on numerical precision of deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3007" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An analytical method to determine minimum per-layer precision of deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1090" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Per-tensor fixed-point quantization of the back-propagation algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jhunjhunwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dattagupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16819</idno>
		<title level="m">Nemotron-4 15b technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">Sept. 2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keisuke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yejin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korthikanti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11990</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Profit: A novel training method for sub-4-bit mobilenet models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="430" to="446" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 16</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Overcoming oscillations in quantization-aware training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fournarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16318" to="16330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pruning vs quantization: Which is better?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuzmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Behboodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Everybody prune now: Structured pruning of llms with only forward passes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolawole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Kagey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05406</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The era of 1-bit llms: All large language models are in 1.58 bits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17764</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Chipnemo: Domain-adapted llms for chip design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-D</forename><surname>Ene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bayraktaroglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.00176</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Shortgpt: Layers in large language models are more redundant than you expect</title>
		<author>
			<persName><forename type="first">X</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03853</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
