{
  "arxivId": "2410.05437",
  "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
  "authors": "Charbel Sakr, Brucek Khailany",
  "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality\nreduction of activations. Unlike prior works on weight-centric tensor\ndecomposition, ESPACE projects activations onto a pre-calibrated set of\nprincipal components. The activation-centrality of the approach enables\nretraining LLMs with no loss of expressivity; while at inference, weight\ndecomposition is obtained as a byproduct of matrix multiplication\nassociativity. Theoretical results on the construction of projection matrices\nwith optimal computational accuracy are provided. Experimentally, we find\nESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small\naccuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At\nlower compression rates of 20% to 40%, ESPACE drives GPT3 models to\noutperforming their baseline, by up to a 0.38 decrease in perplexity for\nGPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency\non existing hardware. Comparison with related works on compressing Llama2-7B\nvia matrix factorization shows that ESPACE is a first step in advancing the\nstate-of-the-art in tensor decomposition compression of LLMs.",
  "url": "https://arxiv.org/abs/2410.05437",
  "issue_number": 915,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/915",
  "created_at": "2025-01-10T20:44:15.235532",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 23,
  "last_read": "2025-01-10T20:44:15.236395",
  "last_visited": "2025-01-10T20:41:20.568Z",
  "main_tex_file": null,
  "published_date": "2024-10-07T18:59:22Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}