<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title>
				<funder>
					<orgName type="full">AntGroup Knowledge Graph Team</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-26">26 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
							<email>leywar.liang@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
							<email>mengshu.sms@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengke</forename><surname>Gui</surname></persName>
							<email>zhengke.gzk@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongshu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peilong</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhouyu</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongpu</forename><surname>Bo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaidong</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zaoyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
							<email>jun.zhoujun@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group Knowledge Graph Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-26">26 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">247C745FC09646C432BBF79496E8ADCF</idno>
					<idno type="arXiv">arXiv:2409.13731v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects:</p><p>(1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-the-art methods, achieving a relative improvement of 19.6% on hotpotQA and 33.5% on 2wiki in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&amp;A tasks of Ant Group, including E-Government Q&amp;A and E-Health Q&amp;A, achieving significant improvement in professionalism compared to RAG methods. Furthermore, we will soon natively support KAG on the opensource KG engine OpenSPG, allowing developers to more easily build rigorous knowledge decision-making or convenient information retrieval services. This will facilitate the localized development of KAG, enabling developers to build domain knowledge services with higher accuracy and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the rapidly advancing Retrieval-Augmented Generation (RAG) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> technology has been instrumental in equipping Large Language Models (LLMs) with the capability to acquire domain-specific knowledge. This is achieved by leveraging external retrieval systems, thereby significantly reducing the occurrence of answer hallucinations and allows for the efficient construction of applications in specific domains. In order to enhance the performance of the RAG system in multi-hop and cross-paragraph tasks, knowledge graph, renowned for strong reasoning capabilities, have been introduced into the RAG technical framework, including GraphRAG <ref type="bibr" target="#b5">[6]</ref>, DALK <ref type="bibr" target="#b6">[7]</ref>, SUGRE <ref type="bibr" target="#b7">[8]</ref>, ToG 2.0 <ref type="bibr" target="#b8">[9]</ref>, GRAG <ref type="bibr" target="#b9">[10]</ref>, GNN-RAG <ref type="bibr" target="#b10">[11]</ref> and HippoRAG <ref type="bibr" target="#b11">[12]</ref>.</p><p>Although RAG and its optimization have solved most of the hallucination problems caused by a lack of domain-specific knowledge and real-time updated information, the generated text still lacks coherence and logic, rendering it incapable of producing correct and valuable answers, particularly in specialized domains such as law, medicine, and science where analytical reasoning is crucial. This shortcoming can be attributed to three primary reasons. Firstly, real-world business processes typically necessitate inferential reasoning based on the specific relationships between pieces of knowledge to gather information pertinent to answering a question. RAG, however, commonly relies on the similarity of text or vectors for retrieving reference information, which may lead to incomplete and repeated search results. secondly, real-world processes often involve logical or numerical reasoning, such as determining whether a set of data increases or decreases in a time series, and the next token prediction mechanism used by language models is still somewhat weak in handling such problems.</p><p>In contrast, the technical methodologies of knowledge graphs can be employed to address these issues. Firstly, KG organize information using explicit semantics; the fundamental knowledge units are SPO triples, comprising entities and the relationships between them <ref type="bibr" target="#b12">[13]</ref>. Entities possess clear entity types, as well as relationships. Entities with the same meaning but expressed differently can be unified through entity normalization, thereby reducing redundancy and enhancing the interconnectedness of knowledge <ref type="bibr" target="#b13">[14]</ref>. During retrieval, the use of query syntax (such as SPARQL <ref type="bibr" target="#b14">[15]</ref> and SQL <ref type="bibr" target="#b15">[16]</ref>) enables the explicit specification of entity types, mitigating noisy from same named or similar entities, and allows for inferential knowledge retrieval by specifying relationships based on query requirements, as opposed to aimlessly expanding into similar yet crucial neighboring content. Meanwhile, since the query results from knowledge graphs have explicit semantics, they can be used as variables with specific meanings. This enables further utilization of the LLM's planning and function calling capabilities <ref type="bibr" target="#b16">[17]</ref>, where the retrieval results are substituted as variables into function parameters to complete deterministic inferences such as numerical computations and set operations.</p><p>phases, allowing fragmented knowledge generated through automation to be aligned and connected through domain knowledge. In the offline indexing phase, it can improve the standardization and connectivity of knowledge, and in the online Q&amp;A phase, it can serve as a bridge between user questions and indexing accurately.</p><p>• We proposed a model for KAG. To support the capabilities required for the operation of the KAG framework, such as index construction, retrieval, question understanding, semantic reasoning, and summarization, we enhance the three specific abilities of general LLMs: Natural Language Understanding (NLU), Natural Language Inference (NLI), and Natural Language Generation (NLG) to achieve better performance in each functional module.</p><p>We evaluated the effectiveness of the system on three complex Q&amp;A datasets: 2WikiMultiHopQA <ref type="bibr" target="#b17">[18]</ref>, MuSiQue <ref type="bibr" target="#b18">[19]</ref> and HotpotQA <ref type="bibr" target="#b19">[20]</ref>. The evaluation focused on both end-to-end Q&amp;A performance and retrieval effectiveness. Experimental results showed that compared to HippoRAG <ref type="bibr" target="#b11">[12]</ref>, KAG achieved significant improvements across all three tasks, with F1 scores increasing by 19.6%, 12.2% and 12.5% respectively. Furthermore, retrieval metrics also showed notable enhancements.</p><p>KAG is applied in two professional Q&amp;A scenarios within Ant Group: E-Government and E-Health.</p><p>In the E-Government scenario, it answers users' questions about administrative processes based on a given repository of documents. For E-Health, it responds to inquiries related to diseases, symptoms, treatments, utilizing the provided medical resources. Practical application results indicate that KAG achieves significantly higher accuracy than traditional RAG methods, thereby enhancing the credibility of Q&amp;A applications in professional fields. We will soon natively support KAG on the open source KG engine OpenSPG, allowing developers to more easily build rigorous knowledge decision-making or convenient information retrieval services.</p><p>In summary, we propose a knowledge-augmented technical framework, KAG, targeting professional question-answering scenarios and validate the effectiveness of this framework based on complex question-answering tasks. We present two industry application cases based on Ant Group's business scenarios and have open-sourced the code to assist developers in building local applications using KAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we will first introduce the overall framework of KAG, and then discuss five key enhancements in sections 2.1 to 2.5. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the KAG framework consists of three parts: KAG-Builder, KAG-Solver, and KAG-Model. The KAG-Builder is designed for building offline indexes, in this module, we proposed a LLM Friendly Knowledge Representation framework and mutual-indexing between knowledge structure and text chunk. In the module KAG-Solver we introduced a Logical-form-guided hybrid reasoning solver that integrates LLM reasoning, knowledge reasoning, and mathematical logic reasoning. Additionally, knowledge alignment by semantic reasoning is used to enhance the accuracy of knowledge representation and retrieval in both KAG-Builder and KAG-Solver. The KAG-Model optimizes the capabilities needed by each module based on a general language model, thereby improving the performance of all modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LLM Friendly Knowledge Representation</head><p>In order to define a more friendly knowledge semantic representation for LLMs, we upgrade SPG from three aspects: deep text-context awareness, dynamic properties and knowledge stratification, and name it LLMFriSPG.</p><formula xml:id="formula_0">M = {T , ρ, C , L }</formula><p>where, M represents all types defined in LLMFriSPG, T represents all EntityType(e.g., Person in Figure <ref type="figure" target="#fig_1">2</ref>), EventType classes and all pre-defined properties that are compatible with LPG syntax declarations. C represents all ConceptType classes, concepts and concept relations, it is worth noting that the root node of each concept tree is a ConceptType class that is compatible with LPG syntax(e.g., TaxoOfPerson in Figure <ref type="figure" target="#fig_1">2</ref>.), each concept node has a unique ConceptType class. ρ represents the inductive relations from instances to conecepts. L represents all executable rules defined on logical relations and logical concepts. For ∀t ∈ T :  </p><formula xml:id="formula_1">p t = {p c t , p t f , p b t }</formula><p>As is show in Figure <ref type="figure" target="#fig_1">2</ref>, where, p t represents all properties and relations of type t, and p c t represents the domain experts pre-defined part, p f t represents the part added in an ad-hoc manner, p b t represents the system built-in properties, such as supporting_chunks, descripiton, summary and be-longTo. For any instance e i , denote typeo f (e i ) as t k , and supporting_chunks represents the set of all text chunks containing instance e i , the user defines the chunk generation strategy and the maximum length of the chunk in KAG builder phase, description represents the general descriptive information specific to class t k . It is worth noting that the meaning of description added to the type t k and the instance e i is different, when description is attached to t k , it signifies the global description for that type. Conversely, when it is associated with an instance e i , it represents the general descriptive information for e i consistent with the orignal document context, description can effectively assist LLM in understanding the precise meaning of a specific instance or type, and can be used in tasks such as information extraction, entity linking, and summary generation. summary represents the summary of e i or r j in the original document context. belongTo represents the inductive semantics from instance to concept. Each EntityType or EventType can be associated with a ConceptType through belongTo. It is worth noting that, 1) T and C have different functions. The statement t adopts the object-oriented principle to better match the representation of the LPG <ref type="bibr" target="#b20">[21]</ref>, and C is managed by a text-based concept tree. This article will not introduce the SPG semantics in detail.</p><p>2) p c t and p f t can be instantiated separately. That is, they share the same class declaration, but in the instance storage space, pre-defined static properties and realtime-added dynamic properties can coexist, and we also support instantiating only one of them. This approach can better balance the application scenarios of professional decision-making and information retrieval. General information retrieval scenarios mainly instantiate dynamic properties, while professional decision-making application scenarios mainly instantiate static properties. Users can strike a balance between ease of use and professionalism based on business scenario requirements. 3) p c t and p f t share the same conceptual terminology. Concepts are general common sense knowledge that is independent of specific documents or instances. Different instances are linked to the same concept node to achieve the purpose of classifying the instances. We can achieve semantic alignment between LLM and instances through concept graphs, and concepts can also be used as navigation for knowledge retrieval. the details are shown in section 2.4 and 2.3. In order to more accurately define the hierarchical representation of information and knowledge, as shown in 3, we denote KG cs as knowledge layer, which represents the domain knowledge that complies with the domain schema constraints and has been summarized, integrated, and evaluated. denote KG f r as graph information layer, which represents the graph data such as entities and relations obtained through information extraction. denote RC as raw chunks layer, which represents the original document chunks after semantic segmentation. the KG cs layer fully complies with the SPG semantic specification and supports knowledge construction and logical rule definition with strict schema constraints, SPG requires that domain knowledge must have pre-defined schema constraints. It has high knowledge accuracy and logical rigor. However, due to its heavy reliance on manual annotation, the labor cost of construction is relatively high and the information completeness is insufficient. KG f r shares the same EntityTypes, Eventtypes and Conceptual system with KG cs , and provides effective information supplement for KG cs . Meanwhile, the supporting_chunks, summary, and description edges built between KG f r and RC form an inverted index based on graph structure, making RC an effective original-text-context supplement for KG f r and with high information completeness. As is show in the right part of figure <ref type="figure" target="#fig_2">3</ref>, in a specific domain application, R(KG cs ), R(KG f r ), and R(RC) respectively represent their knowledge coverage in solving the target domain problems. If the application has higher requirements for knowledge accuracy and logic rigorousness, it is necessary to build more domain structured knowledge and consume more expert manpower to increase the coverage of R(KG cs ). On the contrary, if the application has higher requirements for retrieval efficiency and a certain degree of information loss or error tolerance, it is necessary to increase the coverage of R(KG f r ) to fully utilize KAG's automated knowledge construction capabilities and reduce expert manpower consumption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mutual Indexing</head><p>As illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, KAG-Builder consists of three coherent processes: structured information acquisition, knowledge semantic alignment and graph storage writer. The main goals of this module include: 1) building a mutual-indexing between the graph structure and the text chunk to add more descriptive context to the graph structure, 2) using the concept semantic graph to align different knowledge granularities to reduce noise and increase graph connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Semantic Chunking</head><p>According to the document's structural hierarchy and the inherent logical connections between paragraphs, a semantic chunking process is implemented based on system-built-in prompts. This semantic chunking produces chunks that adhere to both length constraints (specifically for LLM's context window size constraints) and semantic coherence, ensuring that the content within each chunk is thematically cohesive. We defined Chunk EntityType in RC, which includes fields such as id, summary, and mainText. Each chunk obtained after semantic segmentation will be written into an instance of Chunk, where id is a composite field consisting of articleID, paraCode, idInPara concatenated by the connector # in order to ensure that consecutive chunks are adjacent in the id space. articleID represents the globally unique article ID, paraCode represents the paragraph code in the article, and idInPara is the sequential code of each chunk in the paragraph. Consequently, an adjacency in the content corresponds to a sequential adjacency in their identifiers. Furthermore, a reciprocal relation is established and maintained between the original document and its segmented chunks, facilitating navigation and contextual understanding across different granularities of the document's content. This structured approach to segmentation not only optimizes compatibility with large-scale language models but also preserves and enhances the document's inherent semantic structure and association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Information Extraction with More Descriptive Context</head><p>Given a dataset, we use fine-tuning-free LLM(such as GPT-3.5, DeepSeek, QWen, etc,.) or our fine-tuned model Hum to extract entities, events, concepts and relations to construct KG f r , subsequently, construct the mutual-indexing structure between KG f r and RC, enabling cross-document links through entities and relations. This process includes three steps. First, it extracts the entity set E = {e 1 , e 2 , e 3 , ...} chunk by chunk, second, extracts the event set EV = {ev 1 , ev 2 , ev 3 , ...} associated to all entities and iteratively extracts the relation set R = {r 1 , r 2 , r 3 , ...} between all entities in E, finally, completes all hypernym relations between the instance and its spgClass. To provide more convenience for the subsequent Knowledge Alignment phase, and overcome the problem of low discrimination of knowledge phrases such as Wikidata <ref type="bibr" target="#b21">[22]</ref> and ConceptNet <ref type="bibr" target="#b22">[23]</ref>, in the entity extraction phase, we use LLMs to generate built-in properties description, summary, semanticType, spgClass, descripitonOfSemanticType by default for each instance e at one time, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, we store them in the e instance storage according to the structure of e.description,e.summary, &lt;e, belongTo, semanticType&gt; and &lt;e, hasClass, spgClass&gt;. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Domain Knowledge Injection And Constraints</head><p>When openIE is applied to professional domains, irrelevant noise will be introduced. Previous researches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref> have shown that noisy and irrelevant corpora can significantly undermine the performance of LLMs. It is a challenge to align the granularity of extracted information and domain knowledge. The domain knowledge alignment capabilities in KAG include: 1) Domain term and concept injection. We use an iterative extraction approach, First, we store domain concepts and terms with description in KG storage. Second, we extract all instances in the document through openIE, then we perform vector retrieval to obtain all possible concept and term sets E d . Finally, we add E d to the extraction prompt and perform another extraction to obtain a set E a d that is mostly aligned with the domain knowledge. 2) Schema-constraint Extraction. In the vertical professional domains, the data structure between multiple documents in each data source such as drug instructions, physical examination reports, government affairs, online order data, structured data tables, etc. has strong consistency, and is more suitable for information extraction with schema-constraint, structured Extraction also makes it easier to do knowledge management and quality improvement. For detailed information about knowledge construction based on Schema-constraint, please refer to the SPG <ref type="foot" target="#foot_2">1</ref> and OneKE <ref type="bibr" target="#b24">[25]</ref>. This article will not introduce it in detail. It is worth noting that, as shown in figure <ref type="figure" target="#fig_1">2</ref>, for the same entity type, such as Person, we can pre-define properties and relations such as name, gender, placeOfBirth, (Person, hasFather, Person), (Person, hasFriend, Person), and can also extract tripples directly such as (Jay Chou, spgClass, Person), (Jay Chou, constellation, Capricorn), (Jay Chou, record company, Universal Music Group) through openIE. 3) Pre-defined Knowledge Structures By Document Type. Professional documents such as drug instructions, government affairs documents, and legal definitions generally have a relatively standardized document structure. Each type of document can be defined as an entity type, and different paragraphs are different properties of the entity. Taking government affairs as an example, we can pre-define the GovernmentAffair EntityType and properites such as administrative divisions, service procedures, required materials, service locations, and target groups. The divided chunks are the values of different properties. If the user asks "What materials are needed to apply for housing provident fund in Xihu District?", you can directly take out the chunk corresponding to property required materials to answer the question, avoiding the possible hallucinations caused by LLM re-generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Mutual indexing between text chunk vectors and knowledge structures</head><p>KAG's mutual-indexing is a knowledge management and storage mechanism that conforms to the LLMFriSPG semantic representation. As is described in section 2.1, it includes four core data structures: 1) Shared Schemas are coarse-grained-types pre-defined as SPG Classes at project level, it includes EntityTypes, ConceptTypes, and EventTypes, they serve as a high-level categorization such as Person, Organization, GEOLocation, Date, Creature, Work, Event. 2) Instance Graph include all event and entity instances in KG cs and KG f r . that is, instances constructed through openIE with schema-free or structured extraction with schema-constraint are both stored as instances in KG storage. 3) Text Chunks are special entity node that conforms to the definition of the Chunk EntityType. 4) Concept Graph is the core component for knowledge alignment. it consists of a series of concepts and concept relations, concept nodes are also fine-grained-types of instances. Through relation prediction, instance nodes can be linked to concept nodes to obtain their finegrained semantic types. , and two storage structures: 1) KG Store. Store KG data structures in LPG databases, such as TuGraph, Neo4J. 2) Vector Store. Store text and vectors in a vector storage engine, such as ElasticSearch, Milvus, or the vector storage embedded in the LPG engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Logical Form Solver</head><p>In the process of solving complex problems, three key steps are involved: planning, reasoning and retrieval. Disassembling question is a planning process to determine the next problem to be tackled. Reasoning includes retrieving information based on the disassembled question, inferring the answer to the question according to the retrieved results, or re-disassembling the sub-question when the retrieved content cannot answer the question. Retrieval is to find the content that can be used as reference for the original question or the disassembled sub-question. Since interactions between Algorithm 1 Logical Form Solver</p><p>1: memory ← [] 2: query cur ← query 3: for round ∈ (0, n) do 4: l f list ← LFPlanner(query cur ) 5: history ← [] 6:</p><formula xml:id="formula_2">for l f ∈ l f list do 7: l f subquery , l f f unc ← l f 8:</formula><p>retrievals sub , answer sub ← Reasoner(l f subquery , l f f unc ) 9:</p><p>history.append([l f subquery , retrievals sub , answer sub ]) if not Judge(query, memory) then 13:</p><p>query cur ← SupplyQuery(query, memory)</p><formula xml:id="formula_3">14:</formula><p>end if 15: end for 16: answer ← Generator(query, memory) 17: return answer different modules in traditional RAG are based on vector representations of natural language, inaccuracies often arise. Inspired by the logical forms commonly used in KGQA, we designed an executable language with reasoning and retrieval capabilities. This language breaks down a question into multiple logical expressions, each of which may include functions for retrieval or logical operations. The mutual indexing described in Section 2.2 makes this process possible. Meanwhile, we designed a multi-turn solving mechanism based on reflection and global memory, inspired by ReSP <ref type="bibr" target="#b25">[26]</ref>. The KAG solving process, as referenced in Figure <ref type="figure">6</ref> and Algorithm 17, first decomposes the current question query cur into a list of subquestions l f list represented in logical form, and performs hybrid reasoning to solve them. If an exact answer can be obtained through multi-hop reasoning over structured knowledge, it returns the answer directly. Otherwise, it reflects on the solution results: storing the answers and retrieval results corresponding to l f list in global memory and determining whether the question is resolved. If not, it generates supplementary questions and proceeds to the next iteration. Section 2.3.1, 2.3.2 and 2.3.3 introduce logical form function for planning, logical form for reasoning and logical form for retrieval respectively. In general, the proposed logical form language has the following three advantages:</p><p>• The use of symbolic language enhances the rigor and interpretability of problem decomposition and reasoning.</p><p>• Make full use of LLMFriSPG hierarchical representation to retrieve facts and texts knowledge guided by the symbolic graph structure</p><p>• Integrate the problem decomposition and retrieval processes to reduce the system complexity.</p><p>Figure <ref type="figure">6</ref>: An Example of logical form execution. In this figure, the construction process of KG on the left is shown in Figure <ref type="figure" target="#fig_4">5</ref>, and the overall reasoning and iteration process is on the right. First, a logical form decomposition is performed based on the user's overall question, and then logical-form-guided reasoning is used for retrieval and reasoning. Finally, Generation determines whether the user's question is satisfied. If not, a new question is supplied to enter a new logical form decomposition and reasoning process. If it is determined to be satisfied, Generation directly outputs the answer.</p><p>Table <ref type="table">13</ref> illustrates a multi-round scenario consistent with pseudocode 17. Although first round the exact number of plague occurrences couldn't be determined, but we can extracted information indicates: "Venice, the birthplace of Antonio Vivaldi, experienced the devastating Black Death, also known as the Great Plague. This pandemic caused by Yersinia pestis led to 75 to 200 million deaths in Eurasia, peaking in Europe from 1347 to 1351. The plague brought significant upheavals in Europe. Although specific occurrence records in Venice aren't detailed, it's clear the city was impacted during the mid-14th century.". As is shown in Table <ref type="table">13</ref>,After two iterations, the answer determined is: 22 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Logical Form Planning</head><p>Logical Functions are defined as Table <ref type="table">1</ref>, with each function representing an execution action. Complex problems are decomposed by planning a combination of these expressions, enabling reasoning about intricate issues.</p><formula xml:id="formula_4">Function Name Function Declaration Retrieval Retrieval(s = s i : type[name], p = p i : edge, o = o i : type[name], s.prop = value, p.prop = value, o.prop = value) Sort Sort(A, direction = min|max, limit = n) Math math i = Math(expr),</formula><p>expr is in LaTeX syntax and can be used to perform operations on sets. e.g. count: ∥A∥, sum:</p><formula xml:id="formula_5">∑ A Deduce Deduce(le f t = A, right = B, op = entailment|greater|less|equal) Output Out put(A, B, ...)</formula><p>Table <ref type="table">1</ref>: Functions of logical form.</p><p>Retrieval. According to the the knowledge or information retrieved from SPO, s, p, o should not repeatedly appear multiple times in the same expression. Constraints can be applied to the s, p, o for querying. For multi-hop queries, multiple retrievals are required. When the current variable refers to a previously mentioned variable, the variable name must be consistent with the referenced variable name, and only the variable name needs to be provided. The knowledge type and name are only specified during the first reference.</p><p>Sort. Sort the retrieved results. A is the variable name for the retrieved subject-predicateobject(SPO) (s i , o i , or s.prop, p.prop, o.prop). direction specifies the sorting direction, where direction = min means sorting in ascending order and direction = max means sorting in descending order. limit = n indicates outputting the topN results.</p><p>Math. Perform mathematical calculations. expr is in LaTeX syntax and can be used to perform calculations on the retrieved results (sets) or constants. math i represents the result of the calculation and can be used as a variable name for reference in subsequent actions.</p><p>Deduce. Deduce the retrieval or calculation results to answer the question. A, B can be the variable names from the retrieved SPO or constants. The operator op = entailment|greater|less|equal represents A entails B, A is greater than B, A is less than B, and A is equal to B, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Logical Form for Reasoning</head><p>When the query statement represented by natural language is applied to the search, the logic is often fuzzy, such as "find a picture containing vegetables or fruits" and "find a picture containing vegetables and fruits". Whether text search or vector search is used, the similarity between the two queries is very high, but the corresponding answers are quite different. The same is true for problems involving logical reasoning processes such as and or not, and intersection differences. To this end, we use logical form to express the question, so that it can express explicit semantic relations. Similar to IRCOT, we decompose complex original problem and plan out various execution actions such as multi-step retrieval, numerical reasoning, logical reasoning, and semantic deduce. Each sub-problem is expressed using logical form functions, and dependencies between sub-questions are established through variable references. The inference resolution process for each sub-question is illustrated as Algorithm 9. In this process, the GraphRetrieval module performs KG structure retrieval according to the logical form clause to obtain structured graph results. Another key module, HybridRetrieval, combining natural language expressed sub-problems and logical functions for comprehensive retrieval of documents and sub-graph information. To understand how logical functions can be utilized to reason about complex problems, refer to the following examples as Table <ref type="table" target="#tab_4">14</ref>.</p><p>Output. Directly output A, B, ... as the answers. Both A and B are variable names that reference the previously retrieved or calculated Algorithm 2 Logical Form Reasoner Require: Each sub-query resulting from the decomposition of a question based on the logical form, along with their respective logical function, are denoted as l f subquery and l f f unc Ensure: The retrievals and answer of each sub-query, are denoted as retri sub and answer sub retri kg ← GraphRetrieval(l f subquery , l f f unc ) 2: if retri kg ̸ = None and retri kg &gt; threshold then retri sub ← retri kg 4: else retri doc ← HybridRetrieval(l f subquery , retri kg ) 6:</p><p>retri sub ← retri kg , retri doc end if 8: answer sub ← Generator(l f subquery , retri sub ) return retri sub , answer sub</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Logical Form for Retrieval</head><p>In naive RAG, retrieval is achieved by calculating the similarity (e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation capability of embedding models plays a key role. This mainly includes a sparse encoder (BM25) and a dense retriever (BERT architecture pre-training language models). Sparse and dense embedding approaches capture different relevance features and can benefit from each other by leveraging complementary relevance information.</p><p>The existing method of combining the two is generally to combine the scores of the two search methods in an ensemble, but in practice different search methods may be suitable for different questions, especially in questions requiring multi-hop reasoning. When query involves proper nouns, people, places, times, numbers, and coordinates, the representation ability of the pre-trained presentation model is limited, and more accurate text indexes are needed. For queries that are closer to the expression of a paragraph of text, such as scenes, behaviors, and abstract concepts, the two may be coupled in some questions.</p><p>In the design of logical form, it is feasible to effectively combine two retrieval methods. When keyword information is needed as explicit filtering criteria, conditions for selection can be specified within the retrieval function to achieve structured retrieval.</p><p>For example, for the query "What documents are required to apply for a disability certificate at West Lake, Hangzhou?", the retrieval function could be represented as: "Retrieval(s=s1:Event[applying for a disability certificate], p=p1:support_chunks, o=o1:Chunk, s.location=West Lake, Hangzhou)". This approach leverages the establishment of different indices (sparse or dense) to facilitate precise searches or fuzzy searches as needed.</p><p>Furthermore, when structured knowledge in the form of SPO cannot be retrieved using logical functions, alternative approaches can be employed. These include semi-structured retrieval, which involves using logical functions to search through chunks of information, and unstructured retrieval. The latter encompasses methods such as Retrieval-Augmented Generation (RAG), where sub-problems expressed in natural language are used to retrieve relevant chunks of text. This highlights the adaptability of the system to leverage different retrieval strategies based on the availability and nature of the information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Knowledge Alignment</head><p>Constructing KG index through information-extraction and retrieving based on vector-similarity has three significant defects in knowledge alignment:</p><p>• Misaligned semantic relations between knowledge. Specific semantic relations, such as contains, causes and isA, are often required between the correct answer and the query, while the similarity relied upon in the retrieval process is a weak semantic measure that lacks properties and direction, which may lead to imprecise retrieval of content.</p><p>• Misaligned knowledge granularity. The problems of knowledge granularity difference, noise, and irrelevance brought by openIE pose great challenges to knowledge management. Due to the diversity of language expressions, there are numerous synonymous or similar nodes, resulting in low connectivity between knowledge elements, making the retrieval recall incomplete.</p><p>• Misaligned with the domain knowledge structure. There is a lack of organized, systematic knowledge within specific domains. Knowledge that should be interrelated appears in a fragmented state, leading to a lack of professionalism in the retrieved content.</p><p>To solve these problems, we propose a solution that leverages concept graphs to enhance offline indexing and online retrieval through semantic reasoning. This involves tasks such as knowledge instance standardization, instance-to-concept linking, semantic relation completion, and domain knowledge injection. As described in section 2.2.2, we added descriptive text information to each instance, concept or relation in the extraction phase to enhance its interpretability and contextual relevance. Meanwhile, as described in section 2.2.3, KAG supports the injection of domain concepts and terminology knowledge to reduce the noise problem caused by the mismatch of knowledge granularity in vertical domains. The goal of concept reasoning is to make full use of vector retrieval and concept reasoning to complete concept relations based on the aforementioned knowledge structure to enhance the accuracy and connectivity of the domain KG. Refer to the definition of SPG concept semantics <ref type="foot" target="#foot_3">2</ref> , as is shown in Fire causes smoke.</p><p>Table <ref type="table" target="#tab_1">2</ref>: Commonly used semantic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Enhance Indexing</head><p>The process of enhancing indexing through semantic reasoning, as shown in Figure <ref type="figure" target="#fig_4">5</ref> , specifically implemented as predicting semantic relations or related knowledge elements among index items using LLM, encompassing four strategies:</p><p>• Disambiguation and fusion of knowledge instances. Taking entity instance e cur as an example, first, the one-hop relations and description information of e cur are used to predict synonymous relations to obtain the synonym instance set E syn of e cur . Then, the fused target entity e tar is determined from E syn . Finally, the entity fusion rules are used to copy the properties and relations of the remaining instances in E syn to e tar , and the names of these instances are added to the synonyms of e tar , the remaining instances will also be deleted immediately.</p><p>• Predict relations between instances and concepts. For each knowledge instance (such as event, entity), predict its corresponding concept and add the derived triple &lt; e i , belongTo, c j &gt; to the knowledge index. As is shown in Figure <ref type="figure" target="#fig_4">5</ref>, &lt;Chamber, belongTo, Legislative Body&gt; means that the Chamber belongs to Legislative Body in classification.</p><p>• Complete concepts and relations between concepts. During the extraction process, we use concept reasoning to complete all hypernym and isA relations between semanticType and spgClass. As is shown in Figure <ref type="figure" target="#fig_4">5</ref> and Table <ref type="table" target="#tab_1">2</ref>, we can obtain the semanticType of Chamber is Legislative Body, and its spgClass is Organization in the extraction phase. Through semantic completion, we can get &lt;Legislative Body, isA, Government Agency&gt;, &lt;Government Agency, isA, Organization&gt;. Through semantic completion, the triple information of KG f r space is more complete and the connectivity of nodes is stronger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Enhance Retrieval</head><p>In the retrieval phase, we utilize semantic relation reasoning to search the KG index based on the phrases and types in the logical form. For the types, mentions or relations in the logical form, we employ the method of combining semantic relation reasoning with similarity retrieval to replace the traditional similarity retrieval method. This retrieval method makes the retrieval path professional and logical, so as to obtain the correct answer. First, the hybrid reasoning performs precise type matching and entity linking. If the type matching fails, then, semantic reasoning is performed. As shown in Figure <ref type="figure">6</ref>, if the type Political Party fails to match, semantic reasoning is used to predict that Political Party contains Political Faction, and reasoning or path calculation is performed starting from Political Faction.</p><p>Take another example. If the user query q 1 is "Which public places can cataract patients go for leisure?" and the document content d 2 is "The museum is equipped with facilities to provide barrier-free visiting experience services such as touch, voice interpretation, and fully automatic guided tours for the visually impaired.", It is almost impossible to retrieve d 2 based on the vector similarity with q 1 . However, it is easier to retrieve d 2 through the semantic relation of &lt;cataract patient, isA, visually impaired&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">KAG-Model</head><p>KAG includes two main computational processes: offline index building and online query and answer generation. In the era of small language models, these two tasks were typically handled by two separate pipelines, each containing multiple task-specific NLP models. This results in high complexity for the application system, increased setup costs, and inevitable cascading losses due to error propagation between modules. In contrast, large language models, as a capability complex, can potentially integrate these pipelines into a unified, simultaneous end-to-end reasoning process.</p><p>As shown in Figure <ref type="figure">7</ref>, the processes of indexing and QA each consist of similar steps. Both of the two pipelines can be abstracted as classify, mention detection, mention relation detection, semantic alignment, embedding, and chunk, instance, or query-focused summary. Among these, classify, mention detection, and mention relation detection can be categorized as NLU, while semantic alignment and embedding can be grouped under NLI. Finally, the chunk, instance or query-focused summary can be classified under NLG. Thus, we can conclude that the three fundamental capabilities of natural language processing that a RAG system relies on are NLU, NLI, and NLG.</p><p>We focused on exploring methods to optimize these three capabilities, which are introduced in subsections 2.5.1, 2.5.2, and 2.5.3 respectively. Additionally, to reduce the cascade loss caused by linking models into a pipeline, we further explored methods to integrate multiple inference processes into a single inference. Subsection 2.5.4 will discuss how to equip the model with retrieval capabilities to achieve better performance and efficiency through one-pass inference.</p><p>Figure <ref type="figure">7</ref>: The model capabilities required for KAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Natural Language Understanding</head><p>NLU is one of the most common foundational tasks in natural language processing, including text classification, named entity recognition, relation Extraction, subject and object extraction, trigger detection, event argument extraction, event extraction, and machine reading comprehension. We have collected over 30 public datasets to enhance understanding capabilities. Experiments found that simply transforming the original datasets into instruction datasets can achieve comparable results to specialized models on trained tasks, but this approach does not improve the model's NLU capabilities on unseen domains. Therefore, we conducted large-scale instruction reconstruction, designing various instruction synthesis strategies to create an NLU instruction dataset with over 20,000 diverse instructions. By utilizing this dataset for supervised fine-tuning on a given base model, the model has demonstrated enhanced NLU capabilities in downstream tasks. The instruction reconstruction strategy mainly consists of the following three types.</p><p>• Label bucketing: <ref type="bibr" target="#b24">[25]</ref>This strategy focuses on label-guided tasks, where the aim is to extract text based on labels or map text to specified labels, including classification, NER, RE, and EE. When labels in a dataset collectively co-occur in the training set, the model may learn this pattern and overfit to the dataset, failing to independently understand the meaning of each label. Therefore, during the instruction synthesis process, we adopt a polling strategy that designates only one label from each training sample as part of a bucket. Additionally, since some labels have similar semantics and can be confused, we group easily confused labels into a single bucket, allowing the model to learn the semantic differences between the two labels more effectively.</p><p>• Flexible and Diverse Input and Output Formats: The LLM employs an instructionfollowing approach for inference, and a highly consistent input-output format may cause the model to overfit to specific tasks, resulting in a lack of generalization for unseen formats. Therefore, we have flexibly processed the input and output formats. The output is handled as five different formatting instructions, as well as two types of natural language instructions. Additionally, the output format can dynamically be specified as markdown, JSON, natural language, or any format indicated in the examples.</p><p>• Instructoin with Task Guildline: Traditional NLP training often employs a "sea of questions" approach, incorporating a wide variety of data in the training set. This allows the model to understand task requirements during the learning process, such as whether to include job titles when extracting personal names. For the training of LLMs, we aim for the model to perform tasks like a professional annotator by comprehending the task description. Therefore, for the collected NLU tasks, we summarize the task descriptions using a process of self-reflection within the LLM. This creates training data that includes task descriptions within the instructions. Additionally, to enhance task diversity, we implement heuristic strategies to rephrase the task descriptions and answers. This enables the model to understand the differences between task descriptions more accurately and to complete tasks according to the instructions.</p><p>We fine-tuned six foundational models: qwen2, llama2, baichuan2, llama3, mistral, phi3, and used six understanding benchmarks recorded on OpenCompass for performance validation. The table <ref type="table">3</ref> shows that the KAG-Model has a significant improvement in NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Natural Language Inference</head><p>The NLI task is used to infer the semantic relations between given phrases. Typical NLI tasks include entity linking, entity disambiguation, taxonomy expansion, hypernym discovery, and text entailment. In the context of knowledge base Q&amp;A, due to the diversity and ambiguity of natural language expressions, as well as the subtle and different types of semantic connections between phrases, it often requires further alignment or retrieval of related information through NLI tasks based on NLU. As described in section 2.4, we categorize the key semantic relation in knowledge base applications into six types. Among these, relations such as isA, isPartOf and contains exhibit directional and distance-based partial order relations. During the reasoning process, it is crucial to accurately determine these semantic relations to advance towards the target answer. In traditional approaches, separate training of representation pre-training models and KG completion(KGC) models is often employed to reason about semantic relations. However, these KGC models tend to focus</p><p>Models C3 WSC XSum Lambda Lcsts Race Average GPT4 95.10 74.00 20.10 65.50 12.30 92.35 59.89 Qwen2 92.27 66.35 18.68 62.39 13.07 88.37 56.86 KAG Qwen2 92.88 70.19 31.33 66.16 18.53 88.17 61.21 Llama2 81.70 50.96 23.29 63.26 15.99 55.64 48.47 KAG Llama2 82.36 63.46 24.51 65.22 17.51 68.48 53.59 Baichuan2 84.44 66.35 20.81 62.43 16.54 76.85 54.57 KAG Baichuan2 84.11 66.35 21.51 62.64 17.27 77.18 54.84 Llama3 86.63 65.38 25.84 36.72 0.09 83.76 49.74 KAG Llama3 83.40 62.50 26.72 54.07 18.45 81.16 54.38 Mistral 67.29 30.77 21.16 59.98 0.78 73.46 42.24 KAG Mistral 47.29 39.42 21.54 69.09 17.14 72.42 44.48 Phi3 68.60 42.31 0.60 71.74 3.47 73.18 43.32 KAG Phi3 85.21 25.94 0.36 71.24 15.49 74.00 45.37</p><p>Table 3: Enhancement of natural language understanding capabilities in different LLMs by KAG. The experimental results are based on the open-compass framework and tested using the "gen" mode. The evaluation metrics for C3, WSC, Lambda, and Race are ACC. XSum and Lcsts are measured using ROUGE-1. Race includes Race-middle and Race-high, and their average is taken.</p><p>on learning graph structures and do not fully utilize the essential textual semantic information for semantic graph reasoning. LLMs possess richer intrinsic knowledge, and can leverage both semantic and structural information to achieve more precise reasoning outcomes. To this end, we have collected a high-quality conceptual knowledge base and ontologies from various domains, creating a conceptual knowledge set that includes 8,000 concepts and their semantic relations. Based on this knowledge set, we constructed a training dataset that includes six different types of conceptual reasoning instructions to enhance the semantic reasoning capabilities of a given base model, thereby providing semantic reasoning support for KAG.</p><p>Semantic reasoning is one of the core ability required in KAG process, we use NLI tasks and general reasoning Q&amp;A tasks to evaluate the ability of our model, the results are as shown in Table <ref type="table" target="#tab_4">4</ref> and Table <ref type="table">5</ref>. The evaluation results indicates that our KAG-Model demonstrates a significant improvement in tasks related with semantic reasoning: First, Table <ref type="table">5</ref> shows that on the Hypernym Discovery task(which is consistent in form with the reasoning required in semantic enhanced indexing and retrieval.), our fine-tuned KAG-llama model outperforms Llama3 and ChatGPT-3.5 significantly. In addition, the better performance of our model on CMNLI, OCNLI and SIQA compared with Llama3 in Table <ref type="table" target="#tab_4">4</ref> shows that our model has good capabilities in general logical reasoning. Table <ref type="table">5</ref>: Hypernym Discovery performance comparison on SemEval2018-Task9 dataset, measured in MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Natural Language Generation</head><p>Models that have not undergone domain adaptation training often exhibit significant differences from the target text in domain logic and writing style. Moreover, acquiring sufficient amounts of annotated data in specialized domains frequently poses a challenge. Therefore, we have established two efficient fine-tuning methods for specific domain scenarios, allowing the generation process to better align with scene expectations: namely, K-Lora and AKGF.</p><p>Pre-learning with K-LoRA. First of all, we think that using knowledge to generate answers is the reverse process of extracting knowledge from text. Therefore, by inverting the previously described extraction process, we can create a 'triples-to-text' generation task. With extensive fine-tuning on a multitude of instances, the model can be trained to recognize the information format infused by the KG. Additionally, as the target text is domain-specific, the model can acquire the unique linguistic style of that domain. Furthermore, considering efficiency, we continue to utilize LoRA-based SFT. We refer to the LoRA obtained in this step as K-LoRA.</p><p>Alignment with KG Feedback. The model may still exhibit hallucinations in its responses due to issues such as overfitting. Inspired by the RLHF(Reinforcement Learning with Human Feedback) approach <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, we hope that the KG can serve as an automated evaluator, providing feedback on knowledge correctness of the current response, thereby guiding the model towards further optimization. First, we generate a variety of responses for each query by employing diverse input formats or random seeds. Subsequently, we incorporate the KG to score and rank these responses. The scoring process compare generated answer with knowledge in KG to ascertain their correctness. The reward is determined by the number of correctly matched knowledge triples. The formula for calculating the reward is represented by Formula 1.</p><formula xml:id="formula_6">reward = log(rspo + α × re) (1)</formula><p>where α is a hyperparameter, rspo represents the number of SPO matches, and re represents the number of entity matches.</p><p>We select two biomedical question-answering datasets, CMedQA <ref type="bibr" target="#b28">[29]</ref> and BioASQ <ref type="bibr" target="#b29">[30]</ref>, for evaluating our model. CMedQA is a comprehensive dataset of Chinese medical questions and answers, while BioASQ is an English biomedical dataset. We randomly choose 1,000 instances from each for testing. For CMedQA, we employ the answer texts from the non-selected Q&amp;A pairs as corpora to construct a KG in a weakly supervised manner. Similarly, with BioASQ, we use all the provided reference passages as the domain-specific corpora. Experimental results, as shown in Table <ref type="table" target="#tab_5">6</ref>, demonstrate significant enhancement in generation performance. For more details on the specific implementation process, please refer to our paper <ref type="bibr" target="#b30">[31]</ref> Model We consider continual pre-training as a basic method of domain knowledge infusion, on par with other retrieval-based methods. Consequently, we do not report on the outcomes of hybrid approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Onepass Inference</head><p>Most retrieval enhanced systems operate in a series of presentation models, retrievers, and generation models, resulting in high system complexity, construction costs, and the inevitable concatenation loss caused by error transfer between modules. We introduces an efficient one-pass unified generation and retrieval (OneGen) model to enable an arbitrary LLM to generate and retrieve in one single forward pass. Inspired by the latest success in LLM for text embedding, we expand the original vocabulary by adding special tokens (i.e. retrieval tokens), and allocate the retrieval task to retrieval tokens generated in an autoregressive manner. During training, retrieval tokens only participate in representation fine-tuning through contrastive learning, whereas other output tokens are trained using language model objectives. At inference time, we use retrieval tokens for efficient retrieving on demand. Unlike the previous pipeline approach where at least two models are needed for retrieval and generation, OneGen unified them in one model, thus eliminating the need for a separate retriever and greatly reducing system complexity.</p><p>As shown in experiment results in Table <ref type="table" target="#tab_6">7</ref>, we draw the following conclusions: (1) OneGen demonstrates efficacy in R → G task, and joint training of retrieval and generation yields performance gains on the RAG task. The Self-RAG endows LLMs with self-assessment and adaptive retrieval, while OneGen adds self-retrieval. Our method outperforms the original Self-RAG across all datasets, especially achieving improvements of 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets. To evaluate the effectiveness of the KAG for knowledge-intensive question-answering task, we perform experiments on 3 widely-used multi-hop QA datasets, including HotpotQA <ref type="bibr" target="#b19">[20]</ref>, 2WikiMultiHopQA <ref type="bibr" target="#b17">[18]</ref>, and MuSiQue <ref type="bibr" target="#b18">[19]</ref>. For a fair comparison, we follow IRCoT <ref type="bibr" target="#b32">[33]</ref> and HippoRAG <ref type="bibr" target="#b11">[12]</ref> utilizing 1,000 questions from each validation set and using the retrieval corpus related to selected questions.</p><p>Evaluation Metric. When evaluating QA performance, we use two metrics: Exact Match (EM) and F1 scores. For assessing retrieval performance, we calculate the hit rates based on the Top 2/5 retrieval results, represented as Recall@2 and Recall@5.</p><p>Comparison Methods. We evaluate our approach against several robust and commonly utilized retrieval RAG methods. NativeRAG using ColBERTv2 <ref type="bibr" target="#b33">[34]</ref> as retriever and directly generates answers based on all retrieved documents <ref type="bibr" target="#b34">[35]</ref>. HippoRAG is a RAG framework inspired by human long-term memory that enables LLMs to continuously integrate knowledge across external documents. In this paper, we also use ColBERTv2 <ref type="bibr" target="#b33">[34]</ref> as its retriever <ref type="bibr" target="#b11">[12]</ref>. IRCoT interleaves chain-ofthought (CoT) generation and knowledge retrieval steps in order to guide the retrieval by CoT and vice-versa. This interleaving allows retrieving more relevant information for later reasoning steps. It is a key technology for implementing multi-step retrieval in the existing RAG framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overall Results</head><p>The end-to-end Q&amp;A performance is shown in Table <ref type="table">8</ref>. Within the RAG frameworks leveraging ChatGPT-3.5 as backbone model, HippoRAG demonstrates superior performance compared to Na-tiveRAG. HippoRAG employs a human long-term memory strategy that facilitates the continuous integration of knowledge from external documents into LLMs, thereby significantly enhancing Q&amp;A capabilities. However, given the substantial economic costs associated with utilizing ChatGPT-3.5, we opted to use the DeepSeek-V2 API as a viable alternative. On average, the performance of the IR-CoT + HippoRAG configuration utilizing the DeepSeek-V2 API slightly surpasses that of ChatGPT-3.5. Our constructed framework KAG shows significant performance improvement compared to IRCoT + HippoRAG, with EM increases of 11.5%, 19.8%, and 10.5% on HotpotQA, 2WikiMul-tiHopQA, and MuSiQue respectively, and F1 improvements of 12.5%, 19.1%, and 12.2%. These advancements in end-to-end performance can largely be attributed to the development of more effective indexing, knowledge alignment and hybrid solving libraries within our framework. We evaluate the effectiveness of the single-step retriever and multi-step retriever, with the retrieval performance shown in Table <ref type="table">9</ref>. From the experimental results, it is evident that the multi-step retriever generally outperforms the single-step retriever. Analysis reveals that the content retrieved by the single-step retriever exhibits very high similarity, resulting in an inability to use the single-step retrieval outcomes to derive answers for certain data that require reasoning. The multi-step retriever alleviates this issue. Our proposed KAG framework directly utilizes the multi-step retriever and significantly enhances retrieval performance through strategies such as mutual-indexing, logical form solving, and knowledge alignment.</p><p>Framework Model HotpotQA 2WikiMultiHopQA MuSiQue EM F1 EM F1 EM F1 NativeRAG [35, 34] ChatGPT-3.5 43.4 57.7 33.4 43.3 15.5 26.4 HippoRAG [12, 34] ChatGPT-3.5 41.8 55.0 46.6 59.2 19.2 29.8 IRCoT+NativeRAG ChatGPT-3.5 45.5 58.4 35.4 45.1 19.1 30.5 IRCoT+HippoRAG ChatGPT-3.5 45.7 59.2 47.7 62.7 21.9 33.3 IRCoT+HippoRAG DeepSeek-V2 51.0 63.7 48.0 57.1 26.2 36.5 KAG w/ LFS re f 3 DeepSeek-V2 59.8 74.0 66.3 76.1 35.4 48.2 KAG w/ LFSH re f 3 DeepSeek-V2 62.5 76.2 67.8 76.2 36.7 48.7</p><p>Table 8: The end-to-end generation performance of different RAG models on three multi-hop Q&amp;A datasets. The values in bold and underline are the best and second best indicators respectively.</p><p>Retriever HotpotQA 2Wiki MuSiQue Recall@2 Recall@5 Recall@2 Recall@5 Recall@2 Recall@5 Single-step BM25 <ref type="bibr" target="#b35">[36]</ref> 55. <ref type="bibr" target="#b3">4</ref> 72.2 51.8 61.9 32.3 41.2 Contriever [37] 57.2 75.5 46.6 57.5 34.8 46.6 GTR [38] 59.4 73.3 60.2 67.9 37.4 49.1 RAPTOR [39] 58.1 71.2 46.3 53.8 35.7 45.3 Proposition [40] 58.7 71.1 56.4 63.1 37.6 49.3 NativeRAG [35, 34] 64.7 79.3 59.2 68.2 37.9 49.2 HippoRAG [12, 34] 60.5 77.7 70.7 89.1 40.9 51.9 Multi-step IRCoT + BM25 65.6 79.0 61.2 75.6 34.2 44.7 IRCoT + Contriever 65.9 81.6 51.6 63.8 39.1 52.2 IRCoT + NativeRAG 67.9 82.0 64.1 74.4 41.7 53.7 IRCoT + HippoRAG 67.0 83.0 75.8 93.9 45.3 57.6 KAG 72.8 88.8 65.4 91.9 48.5 65.7</p><p>Table <ref type="table">9</ref>: The performance of different retrieval models on three multi-hop Q&amp;A datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>The objective of this experiment is to deeply investigate the impact of the knowledge alignment and logic form solver on the final results. We conduct ablation studies for each module by substituting different methods and analyzing the changes in outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Knowledge Graph Indexing Ablation</head><p>In the graph indexing phase, we propose the following two substitution methods:</p><p>1) Mutual Indexing Method. As a baseline method of KAG, according to the introduction in Sections 2.1 and 2.2, we use information extraction methods (such as OpenIE) to extract phrases and triples in document chunks, and form the mutual-indexing between graph structure and text chunks according to the hierarchical representation of LLMFriSPG, and then write them into KG storage. We denote this method as M_Indexing.</p><p>2) Knowledge Alignment Enhancement. This method uses knowledge alignment to enhance the KG mutual-indexing and the logical form-guided reasoning &amp; retrieval. According to the introduction in Section 2.4, it mainly completes tasks such as the classification of instances and concepts, the prediction of hypernyms/hyponyms of concepts, the completion of the semantic relationships between concepts, the disambiguation and fusion of entities, etc., which enhances the semantic distinction of knowledge and the connectivity between instances, laying a solid foundation for subsequent reasoning and retrieval guided by logical forms. We denote this method as K_Alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Reasoning and Retrieval Ablation</head><p>Multi-round Reflection. We adopted the multi-round reflection mechanism from ReSP <ref type="bibr" target="#b25">[26]</ref> to assess whether the Logical Form Solver has fully answered the question. If not, supplementary questions are generated for iterative solving until the information in global memory is sufficient.</p><p>We analyzed the impact of the maximum iteration count n on the results, denoted as re f n . If n = 1, it means that the reflection mechanism is not enabled. In the reasoning and retrieval phase, we design the following three substitution methods:</p><p>1) Chunks Retriever.</p><p>We define KAG's baseline retrieval strategy with reference to HippoRAG's <ref type="bibr" target="#b11">[12]</ref> retrieval capabilities, with the goal of recalling the top_k chunks that support answering the current question. The Chunk score is calculated by weighting the vector similarity and the personalized pagerank score. We denote this method as ChunkRetri, we denote ChunkRetri with n-round reflections as CR re f n .</p><p>2) Logical Form Solver (Enable Graph Retrieval). Next, we employ a Logical Form Solver for reasoning. This method uses pre-defined logical forms to parse and answer questions. First, it explores the reasoning ability of the KG structure in KG cs and KG f r spaces, focusing on accuracy and rigor in reasoning. Then, it uses supporting_chunks in RC to supplement retrieval when the previous step of reasoning has no results. We denote this method as LFS re f n . The parameter n is maximum number of iteration parameter.</p><p>3) Logical Form Solver (Enable Hybrid Retrieval). In order to make full use of the mutualindexing structure between KG f r and RC to further explore the role of KG structure in enhancing chunk retrieval, we modify the LFS re f n by disabling the Graph Retrieval functionality for direct reasoning. Instead, all answers are generated using the Hybrid Retrieval method. This approach enables us to evaluate the contribution of graph retrieval to the performance of reasoning. We denote this method as LFSH re f n .</p><p>Through the design of this ablation study, we aim to comprehensively and deeply understand the impact of different graph indexing and reasoning methods on the final outcomes, providing strong support for subsequent optimization and improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Experimental Results and Discussion</head><p>Graph Index Reasoning HotpotQA 2Wiki MuSiQue EM F1 EM F1 EM F1 M_Indexing CR re f 3 52.4 65.4 48.2 56.0 24.6 36.6 K_Alignment CR re f 3 54.7 69.5 62.7 72.5 29.6 41.1 LFS re f 1 59.1 73.4 65.2 74.4 31.3 43.4 LFS re f 3 59.8 74.0 66.3 76.1 35.4 48.2 LFSH re f 1 61.5 76.0 66.0 75.0 33.5 44.3 LFSH re f 3 62.5 76.2 67.8 76.2 36.7 48.7</p><p>Table <ref type="table">10</ref>: The end-to-end generation performance of different model methods on three multi-hop Q&amp;A datasets. The backbone model is DeepSeek-V2 API. As is described in Algorithm 17, re f 3 represents a maximum of 3 rounds of reflection, and re f 1 represents a maximum of 1 round, which means that no reflection is introduced.</p><p>The analysis of the experimental outcomes can be approached from the following two perspectives:</p><p>1) Knowledge Graph Indexing. As is shown in Table <ref type="table">11</ref>, after incorporation Knowledge Alignment into the KG mutual-indexing, the top-5 recall rates of CR re f 3 improved by 9.2%, 28.4%, and 9.5% respectively, with an average improvement of 15.7%. As shown in Figure <ref type="figure">9</ref>, after enhancing knowledge alignment, the relation density is significantly increased, and the frequency-outdegree graph is shifted to the right as a whole</p><p>• The 1-hop graph exhibits a notable rightward shift, indicating that the addition of semantic structuring has increased the number of neighbors for each node, thereby enhancing the graph's density.</p><p>• The 2-hop and 3-hop graphs display an uneven distribution, with sparse regions on the left and denser regions on the right. When comparing before and after K_Alignment, it is evident that the vertices in each dataset have shifted rightward, with the left side becoming more sparse. This suggests that nodes with fewer multi-hop neighbors have gained new neighbors, leading to this observed pattern.</p><p>This signifies that the newly added semantic relations effectively enhance graph connectivity, thereby improving document recall rates.</p><p>2) Graph Inference Analysis. In terms of recall, LFSH re f 3 achieves improvements over CR re f 3 under the same graph index, with increases in top-5 recall rates by 15%, 32.2%, and 12.7%, averaging an improvement of 19.9%. This enhancement can be attributed to two main factors:</p><p>• LFSH re f 3 decomposes queries into multiple executable steps, with each sub-query retrieving chunks individually. As shown in the time analysis in Figure <ref type="figure">8</ref>, both LFSH re f 3 and LFS re f 3 consume more than twice the time of LFSH re f 3 , indicating that increased computational time is a trade-off for improved recall rates.</p><p>• LFSH re f 3 not only retrieves chunks but also integrates SPO triples from execution into chunk computation. Compared to LFSH re f 3 , it retrieves additional query-related relationships.</p><p>Due to the subgraph-based query answering in LFS re f 3 , it cannot be compared directly in recall rate analysis but can be examined using the F1 metric. In comparison to LFSH re f 3 , LFS re f 3 answered questions based on the retrieved subgraphs with proportions of 33%, 34%, and 18%,respectively. LFS re f 3 shows a decrease in the F1 metric by 2.2%, 0.1%, and 0.5%, while the computation time reduces by 12%, 22%, and 18%.</p><p>The analysis of the cases with decreased performance reveals that errors or incomplete SPOs during the construction phase lead to incorrect sub-query answers, resulting in wrong final answers. This will be detailed in the case study. The reduction in computation time is primarily due to the more efficient retrieval of SPOs compared to document chunks.</p><p>In industrial applications, computation time is a crucial metric. Although LFS re f n may introduce some errors, these can be improved through graph correction and completion. It is noteworthy that in the current experiments, the slight decrease in metrics has been traded off for reduced computation time, which we consider a feasible direction for industrial implementation.</p><p>For analyze the impact of the maximum number of iterations parameter n on the results, LFS re f 1 compared to LFS re f 3 , the F1 scores decreased by 0.6%, 1.6%, and 4.8%, respectively. Based on the experiments of LFS re f 3 , the proportions for an iteration count of 1 were analyzed to be 97.2%, 94.8%, and 87.9%; LFSH re f 1 compared to LFSH re f 3 , the F1 scores decreased by 0.2%, 1.2%, and 4.4%, respectively. Based on the experiments of LFSH re f 3 , the proportions for an iteration count of 1 were analyzed to be 98.3%, 95.2%, and 84.1%; showing a positive correlation with the F1 score reduction. Table <ref type="table">13</ref> provides a detailed analysis of the effect of iteration rounds on the solution of the final answer. Increasing the maximum number of iterations parameter facilitates the re-planning of existing information when LFS re f n is unable to complete the solution, thereby addressing some unsolvable case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KAG for E-Goverment</head><p>We used the KAG framework and combined it with the Alipay E-government service scenario to build a Q&amp;A application that supports answering users' questions about service methods, required materials, service conditions, and service locations. To build the e-government Q&amp;A application, we first collected 11,000 documents about government services, and based on the methods described in section 2, implemented functional modules such as index building, logical-form-guided reasoning and solving, semantic enhancement, and conditional summary generation.</p><p>During the offline index construction phase, the semantic chunking strategy is used to segment government service documents to obtain specific matters and their properties such as the administrative region, service process, required materials, service location, target audience, and the corresponding chunks.</p><p>In the reasoning and solving phase, a logical function is generated based on the given user question and graph index structure, and the logical form is executed according to the steps of the logical function. First, the index item of the administrative area where the user is located is accurately located. Then, the item name, group of people, etc. are used for search. Finally, the corresponding chunk is found through the required materials or service process. specifically inquired by the user.</p><p>In the semantic enhancement phase, we added two semantic relations, synonymy and hypernymy, between items. A synonymous relation refers to items in two different regions with different names but the same meaning, such as renewal of social security card and application for lost social security card; a co-hypernymy relation refers to two items belonging to different subcategories under the same major category of items, such as applying for housing provident fund loan for construction of new housing and applying for housing provident fund loan for construction and renovation of new housing, the two items have a common hypernymy applying for housing provident fund loan.</p><p>We compared the effects of the two technical solutions, NaiveRAG and KAG, as shown in the table below. It is evident that KAG shows significant improvements in both completeness and accuracy compared to NaiveRAG.</p><p>Methods SampleNum Precision Recall NaiveRAG 492 66.5 52.6 KAG 492 91.6 71.8</p><p>Table 12: Ablation Experiments of KAG in E-Goverment Q&amp;A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KAG for E-Health</head><p>We have developed a medical Q&amp;A application based on the Alipay Health Manager scenario, which supports answering user's questions regarding popular science about disease, symptom, vaccine, operation, examination and laboratory test, also interpretation of medical indicators, medical recommendation, medical insurance policy inquires, hospital inquires, and doctor information inquires. We have sorted out authoritative medical document materials through a team of medical experts, and produced more than 1.8 million entities and more than 400,000 term sets, with a total of more than 5 million relations. Based on this high-quality KG, we have also produced more than 700 DSL<ref type="foot" target="#foot_4">foot_4</ref> rules for indicator calculations to answer the questions of indicator interpretation.</p><p>During the knowledge construction phase, a strongly constrained schema is used to achieve precise structural definition of entities such as diseases, symptoms, medications, and medical examinations. This approach facilitates accurate answers to questions and generates accurate knowledge, while also ensuring the rigor of relations between entities. In the reasoning phase, the logical form is generated based on the user's query, and then translated to DSL form for the query on KG. The query result is returned in the form of triples as the answer. The logical form not only indicates how to query the KG, but also contains the key structural information in the user's query (such as city, gender, age, indicator value, etc.). When parsing the logical form for query in graph, the DSL rules which produced by medical expert will also be triggered, and the conclusion will be returned in the form of triples. For example, if a user asks about "blood pressure 160", it will trigger the rules as:</p><p>, which strictly follows the defination of L in LLMFriSPG, and the conclusion that the person may have hypertension will be obtained.</p><p>In the semantic enhancement phase, we utilize the term set to express the two semantic relations of synonymy and hypernym of concepts. The hypernym supports the expression of multiple hypernyms. During knowledge construction and user Q&amp;A phase, entities are aligned with medical terms. For example, in the concept of surgery type, the hypernym of deciduous tooth extraction and anterior tooth extraction is tooth extraction. When the user only asks questions about tooth extraction, all its hyponyms can be retrieved based on the term, and then the related entity information can be retrieved for answering. With the support of KAG, we achieved a recall rate of 60.67% and a precision rate of 81.32% on the evaluation set which sampling online Q&amp;A queries. In the end-to-end scenario, the accuracy of medical insurance policy inquires (Beijing, Shanghai, Hangzhou) reached 77.2%, and the accuracy rate of popular science intentions has exceeded 94%, and the accuracy rate of interpreting indicator intentions has exceeded 93%.</p><p>5 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DIKW Pyramid</head><p>Following the DIKW pyramid theories <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, after data is processed and contextualised, it becomes information, and by integrating information with experience, understanding, and expertise, we gain knowledge. We usually use information extraction technology to obtain information from the original text <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, and obtain knowledge from the information through linking, fusion, analysis, and learning technology <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46]</ref>. Information and knowledge are a single entity having different forms. There are no unified language to represent data, information and knowledge, RDF/OWL <ref type="bibr" target="#b48">[49]</ref> only provides binary representation in the form of triples, and LPG <ref type="bibr" target="#b20">[21]</ref> lacks support for knowledge semantics and classification. SPG<ref type="foot" target="#foot_5">foot_5</ref>  <ref type="bibr" target="#b49">[50]</ref> supports knowledge hierarchy and classification representation, but lacks text context support that is friendly to large language models. Our proposed LLMFriSPG supports hierarchical representation from data to information to knowledge, and also provides reverse context-enhanced mutual-indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Vector Similarity-based RAG</head><p>The external knowledge base use the traditional search engine provides an effective method for updating the knowledge of LLMs, it retrievals supporting documents by calculating the text or vector similarity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> between the query and document, and then answers questions using the in-context learning method of LLMs. In addition, this method faces great challenges in understanding longdistance knowledge associations between documents. Simple vector-based retrieval is not suitable for multi-step reasoning or tracking logical links between different information fragments. To address these challenges, researchers have explored methods such as fine-grained document segmentation, CoT <ref type="bibr" target="#b32">[33]</ref>, and interactive retrieval <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>. Despite these optimizations, traditional query-chunks similarity methods still has difficulty in accurately focusing on the relations between key knowledge in complex questions, resulting in low information density and ineffective association of remote knowledge. We will illustrate the logical-form-guided solving method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Information Retrieval-based GraphRAG</head><p>This type of methods use information extraction techniques to build entity and relation associations between different documents, which can better perceive the global information of all documents. Typical tasks in the knowledge construction phase include: graph information extraction and knowledge construction&amp;enhancement. Methods like GraphRAG <ref type="bibr" target="#b50">[51]</ref>, ToG 2.0 <ref type="bibr" target="#b8">[9]</ref>, HippoRAG <ref type="bibr" target="#b11">[12]</ref> use OpenIE to extract graph-structure information like entities and relations, some of them exploit multihop associations between entities to improve the effectiveness of cross-document retrieval <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, methods like DALK <ref type="bibr" target="#b6">[7]</ref> use PubTator Central(PTC) annotation to reduce the noise problem of ope-nIE, some of them utilize entity disambiguation technology to enhance the consistency of graph information <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52]</ref>. GraphRAG <ref type="bibr" target="#b50">[51]</ref> generates element-level and community-level summaries when building offline indexes, and it uses a QFS <ref type="bibr" target="#b52">[53]</ref> method to first calculate the partial response of each summary to the query and then calculate the final response. This inherent characteristic of GraphRAG's hierarchical summarization makes it difficult to solve questions such as multi-hop Q&amp;A and incremental updates of documents. KGs constructed by openIE contains a lot of noise or irrelevant information <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. According to the DIKW pyramid hierarchy, these methods only extract the information graph structure and make limited attempts to disambiguate entities in the transformation of information into knowledge,but they do not address issues such as semantic directionality and logical sensitivity. This paper will introduce a method in KAG to enhance information-to-knowledge conversion based on domain concept semantic graph alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">KG-based Question and Answering</head><p>Reasoning based on traditional KGs has good explainability and transparency, but is limited by the scale of the domain KG, the comprehensiveness of knowledge, the detailed knowledge coverage, and the timeliness of updates <ref type="bibr" target="#b56">[57]</ref>. n this paper, we introduce HybridReasoning to alleviate issues such as knowledge sparsity, inconsistent entity granularity, and high graph construction costs. The approach leverages KG retrieval and reasoning to enhance generation, rather than completely replacing RAG.</p><p>To achieve KG-enhanced generation, it is necessary to address KG-based knowledge retrieval and reasoning. One approach is knowledge edge retrieval (IR) <ref type="bibr" target="#b57">[58]</ref>, which narrows down the scope by locating the most relevant entities, relations, or triples based on the question. Another approach is semantic parsing (SP) <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, which converts the question from unstructured natural language descriptions into executable database query languages (such as SQL, SPARQL <ref type="bibr" target="#b60">[61]</ref>, DSL<ref type="foot" target="#foot_6">foot_6</ref> , etc.), or first generates structured logical forms (such as S-expressions <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>) and then converts them into query languages.</p><p>Although conversational QA over large-scale knowledge bases can be achieved without explicit semantic parsing (e.g., HRED-KVM <ref type="bibr" target="#b63">[64]</ref>), most work focuses on exploring context-aware semantic parsers <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Some papers use sequence-to-sequence models to directly generate query languages <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>. These methods are developed for a specific query language, and sometimes even for a specific dataset, lacking generality for supporting different types of structured data. Others use step-by-step query graph generation and search strategies for semantic parsing <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref>. This method is prone to uncontrollable issues generated by LLM, making queries difficult and having poor interpretability. Methods like ChatKBQA <ref type="bibr" target="#b62">[63]</ref>, CBR-KBQA <ref type="bibr" target="#b70">[71]</ref> completely generate S-expressions and provide various enhancements for the semantic parsing process. However, the structure of S-expressions is relatively complex, and integrating multi-hop questions makes it difficult for LLMs to understand and inconvenient for integrating KBQA and RAG for comprehensive retrieval. To address these issues, we propose a multi-step decomposed logical form to express the multi-hop retrieval and reasoning process, breaking down complex queries into multiple sub-queries and providing corresponding logical expressions, thereby achieving integrated retrieval of SPO and chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Bidirectional-enhancement of LLMs and KGs</head><p>LLM and KG are two typical neural and symbolic knowledge utilization methods. Since the pretrained language model such as BERT <ref type="bibr" target="#b71">[72]</ref>, well-performed language models are used to help improve the tasks of KGs. The LLMs with strong generalization capability are especially believed to be helpful in the life-cycle of KGs. There are a lot of works conducted to explore the potential of LLMs for in-KG and out-of-KG tasks. For example, using LLMs to generate triples to complete triples is proved to be much cheaper than the traditional human-centric KG construction process, with acceptable accuracy for the popular entities <ref type="bibr" target="#b72">[73]</ref>. In the past decade, methods for in-KG tasks are designed by learning from KG structures, such as structure embedding-based methods. The text information such as names and descriptions of entities is not fully utilized due to the limited text understanding capability of natural language processing methods until LLMs provide a way. Some works using LLMs for text semantic understanding and reasoning of entities and relations in KG completion <ref type="bibr" target="#b73">[74]</ref>, rule learning <ref type="bibr" target="#b74">[75]</ref>, complex logic querying <ref type="bibr" target="#b75">[76]</ref>, etc. On the other way, KGs are also widely used to improve the performance of LLMs. For example, using KGs as external resources to provide accurate factual information, mitigating hallucination of LLMs during answer generation <ref type="bibr" target="#b8">[9]</ref>, generating complex logical questions answering planning data to fine-tune the LLMs, improving LLMs planning capability and finally improving its logical reasoning capability <ref type="bibr" target="#b76">[77]</ref>, using KGs to uncover associated knowledge that has changed due to editing for better knowledge editing of LLMs <ref type="bibr" target="#b77">[78]</ref>, etc. The bidirectional-enhancement of LLMs and KGs is widely explored and partially achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>In this article, we have proven the adaptability of the KAG framework in Q&amp;A scenarios in vertical and open domains. However, the currently developed version of OpenSPG-KAG 0.5 still has major limitations that need to be continuously overcome, such as:</p><p>Implementing our framework requires multiple LLM calls during the construction and solving phases. A substantial number of intermediate tokens required to be generated during the planning stage to facilitate the breakdown of sub-problems and symbolic representation, this leads to computational and economic overhead, as illustrated in Table <ref type="table" target="#tab_4">14</ref>, where the problem decomposition not only outputs sub-problems but also logical functions, resulting in approximately twice as many generated tokens compared to merely decomposing the sub-problems. Meanwhile, currently, all model invocations within the KAG framework, including entity recognition, relation extraction, relation recall, and standardization, rely on large models. This multitude of models significantly increases the overall runtime. In future domain-specific implementations, tasks like relation recall, entity recognition, and standardization could be substituted with smaller, domain-specific models to enhance operational efficiency.</p><p>The ability to decompose and plan for complex problems requires a high level of capability. Currently, this is implemented using LLMs, but planning for complex issues remains a significant challenge. For instance, when the task is to compare who is older, the problem should be decomposed into comparing who was born earlier</p><p>. Directly asking for age is not appropriate, as they are deceased, and "what is the age" refers to the age at death, which doesn't indicate who is older. Decomposing and planning complex problems necessitates ensuring the model's accuracy, stability, and solvability in problem decomposition and planning. The current version of the KAG framework does not yet address optimizations in these areas. We will further explore how pre-training, SFT, and COT strategies can improve the model's adaptability to logical forms and its planning and reasoning capabilities. Question: Which film has the director who is older, God'S Gift To Women or Aldri Annet Enn Bråk? Q1: Which director directed the film God'S Gift To Women? A1: Michael Curtiz Q2: Which director directed the film Aldri Annet Enn Bråk? A2: Edith Carlmar Q3: What is the age of the director of God'S Gift To Women? A3: 74 years old. Michael Curtiz (December 24, 1886 to April 11, 1962)... Q4: What is the age of the director of Aldri Annet Enn Bråk? A4: 91 years old. Edith Carlmar (Edith Mary Johanne Mathiesen) (15 November 1911 to 17 May 2003) ... Q5: Compare the ages of the two directors to determine which one is older. A5: Edith Carlmar is older. Actually, Michael Curtiz was born earlier.</p><p>OpenIE significantly lowers the threshold for building KGs, but it also obviously increases the technical challenges of knowledge alignment. Although the experiments in this article have shown that the accuracy and connectivity of extracted knowledge can be improved through knowledge alignment. However, there are still more technical challenges waiting to be overcome, such as optimizing the accuracy of multiple-knowledge(such as events, rules, pipeline, etc.) extraction and the consistency of multiple rounds of extraction. In addition, schema-constraint knowledge extraction based on the experience of domain experts is also a key way to obtain rigorous domain knowledge, although the labor cost is high. These two methods should be applied collaboratively to better balance the requirements of vertical scenarios for the rigor of complex decision-making and the convenience of information retrieval. For instance, when extracting team members from multiple texts and asked about the total number of team members, a comprehensive extraction is crucial for providing an accurate answer based on the structured search results. Incorrect extractions also impair response accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In order to build professional knowledge services in vertical domains, fully activate the capabilities and advantages of symbolic KGs and parameterized LLMs, and at the same time significantly reduce the construction cost of domain KGs, we proposed the KAG framework and try to accelerated its application in professional domains. In this article, we introduce in detail the knowledge accuracy, information completeness and logical rigorous are the key characteristics that professional knowledge services must have. At the same time, we also introduce innovations such as LLMs friendly knowledge representation, mutual-indexing of knowledge structure and text chunks, knowledge alignment by semantic reasoning, logic-form-guided hybrid reasoning&amp;solving and KAG model. Compared with the current most competitive SOTA method, KAG has achieved significant improvements on public data sets such as HotpotQA, 2wiki, musique. We have also conducted case verifications in E-goverment Q&amp;A and E-Health Q&amp;A scenarios of Alipay, further proving the adaptability of the KAG framework in professional domains.</p><p>In the future, there is still more work to be explored to continuously reduce the cost of KG construction and improve the interpretability and transparency of reasoning, such as multiple knowledge extraction, knowledge alignment based on OneGraph, domain knowledge injection, large-scale instruction synthesis, illusion suppression of knowledge logic constraints, etc.</p><p>This study does not encompass the enhancement of models for decomposing and planning complex problems, which remains a significant area for future research. In future work, KAG can be employed as a reward model to provide feedback and assess the model's accuracy, stability, and solvability through the execution of planning results, thereby enhancing the capabilities of planning models.</p><p>We will also work in depth with the community organization OpenKG to continue to tackle key technical issues in the collaboration between LLMs and KGs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The KAG Framework. The left side shows KAG-Builder, while the right side displays KAG-Solver. The gray area at the bottom of the image represents KAG-Model.</figDesc><graphic coords="4,127.80,72.00,356.38,150.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LLMFriSPG:A knowledge representation framework that is friendly to LLMs. Instances and concepts are separated to achieve more effective alignment with LLMs through concepts. In this study, entity instances and event instances are collectively referred to as instances unless otherwise specified. SPG properties are divided into knowledge and information areas, also called static and dynamic area, which are compatible with decision-making expertise with strong schema constraints and document retrieval index knowledge with open information representation. The red dotted line represents the fusion and mining process from information to knowledge. The enhanced document chunk representation provides traceable and interpretable text context for LLMs.</figDesc><graphic coords="4,147.60,265.46,316.80,163.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical representation of knowledge and information.</figDesc><graphic coords="5,108.00,263.24,396.00,177.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The Pipeline of KAG Builder for domain unstructured documents. From left to right, first, phrases and triples are obtained through information extraction, then disambiguation and fusion are completed through semantic alignment, and finally, the constructed KG is written into the storage.</figDesc><graphic coords="6,108.00,72.00,396.00,135.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An Example of KAG-Builder pipeline</figDesc><graphic coords="7,127.80,72.00,356.40,165.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="9,127.80,110.04,356.40,187.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="13,127.80,502.55,356.40,181.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="20,147.60,489.54,316.80,185.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>we have summarized six semantic relations commonly required for retrieval and reasoning. Additional semantic relations can be added based on the specific requirements of the actual scenario.</figDesc><table><row><cell>Formal Expression</cell><cell>Description</cell><cell>Example</cell></row><row><cell></cell><cell>A synonym relation means that a word or phrase</cell><cell></cell></row><row><cell>&lt;var1, synonym, var2&gt;</cell><cell>var2 that has the same or nearly the same meaning as another word or phrase var1 in the same language</cell><cell>Fast is a synonym of quick.</cell></row><row><cell></cell><cell>and given context.</cell><cell></cell></row><row><cell></cell><cell>An isA relation means that a hypernym var2 that is</cell><cell></cell></row><row><cell>&lt;var1, isA, var2&gt;</cell><cell>more generic or abstract than a given word or phrase var1 and encompasses a broader category that the</cell><cell>Car isA Vehicle.</cell></row><row><cell></cell><cell>given word belongs to.</cell><cell></cell></row><row><cell></cell><cell>An isPartOf relation means that something var1 is a</cell><cell></cell></row><row><cell>&lt;var1, isPartOf, var2&gt;</cell><cell>component or constituent of something var2 larger. This relation shows that an item is a part of a</cell><cell>Wheel isPartOf car.</cell></row><row><cell></cell><cell>bigger whole.</cell><cell></cell></row><row><cell></cell><cell>A contains relation means that something var1</cell><cell></cell></row><row><cell>&lt;var1, contains, var2&gt;</cell><cell>includes or holds var2, something else within it. This indicates that one item has the other as a subset</cell><cell>Library contains books.</cell></row><row><cell></cell><cell>or component.</cell><cell></cell></row><row><cell>&lt;var1, belongTo, var2&gt;</cell><cell>An belongTo relation means that something var1 is an instance of concept var2.</cell><cell>Chamber belongTo Legislative Body.</cell></row><row><cell>&lt;var1, causes, var2&gt;</cell><cell></cell><cell></cell></row></table><note><p>A causes relation means that one event or action var1 brings about another var2. This indicates a causal relation where one thing directly results in the occurrence of another.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Enhancement of natural language Inference capabilities in different LLMs by KAG. The evaluation metrics for CMNLI, OCNLI, SIQA are measured with accuracy.</figDesc><table><row><cell>Models</cell><cell>CMNLI</cell><cell>OCNLI</cell><cell>SIQA</cell></row><row><cell>Llama3</cell><cell>35.14</cell><cell>32.1</cell><cell>44.27</cell></row><row><cell>KAG-Llama3</cell><cell>49.52</cell><cell>44.31</cell><cell>65.81</cell></row><row><cell></cell><cell>1A.English</cell><cell>2A.Medical</cell><cell>2B.Music</cell></row><row><cell>ChatGPT-3.5</cell><cell>30.04</cell><cell>26.12</cell><cell>28.47</cell></row><row><cell>Llama3-8B</cell><cell>23.47</cell><cell>24.26</cell><cell>18.73</cell></row><row><cell>KAG-Llama3</cell><cell>38.26</cell><cell>55.14</cell><cell>30.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on CMedQA &amp; BioASQ. "CP" indicates "continual pre-trained".</figDesc><table><row><cell></cell><cell>CMedQA</cell><cell></cell><cell>BioASQ</cell><cell></cell></row><row><cell></cell><cell>Rouge-L</cell><cell>BLEU</cell><cell>Rouge-L</cell><cell>BLEU</cell></row><row><cell>ChatGPT-3.5 0-shot</cell><cell>14.20</cell><cell>1.78</cell><cell>21.14</cell><cell>5.93</cell></row><row><cell>ChatGPT-3.5 2-shot</cell><cell>14.66</cell><cell>2.53</cell><cell>21.42</cell><cell>6.11</cell></row><row><cell>Llama2</cell><cell>14.02</cell><cell>2.86</cell><cell>23.47</cell><cell>7.11</cell></row><row><cell>KAG Llama2</cell><cell>15.44</cell><cell>3.46</cell><cell>24.21</cell><cell>7.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>3.1pt on Pub dataset and 2.8pt on ARC dataset, validating the benefits of joint training. (2) OneGen is highly efficient in training, with instruction-finetuned LLMs showing strong retrieval capabilities with minimal additional tuning. It requires less and lower-quality retrieval data, achieving comparable performance with just 60K noisy samples and incomplete documents, without synthetic data. For more details on the specific implementation process, please refer to paper<ref type="bibr" target="#b31">[32]</ref> In RAG for Multi-Hop QA settings, performance comparison across different datasets using different LLMs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Generation Performance</cell><cell cols="2">Retrieval Performance</cell></row><row><cell></cell><cell></cell><cell cols="2">HotpotQA</cell><cell cols="2">2WikiMultiHopQA</cell><cell>HotpotQA</cell><cell>2WikiMultiHopQA</cell></row><row><cell>BackBone</cell><cell>Retriever</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>Recall@1</cell><cell>Recall@1</cell></row><row><cell>Llama2-7B</cell><cell>Contriever self</cell><cell>52.83 54.82</cell><cell>65.64 67.93</cell><cell>70.02 75.02</cell><cell>74.35 78.86</cell><cell>73.76 75.90</cell><cell>68.75 69.79</cell></row><row><cell>Llama3.1-7B</cell><cell>Contriever self</cell><cell>53.72 55.38</cell><cell>66.46 68.35</cell><cell>70.92 75.88</cell><cell>75.29 79.60</cell><cell>69.79 72.55</cell><cell>66.80 68.98</cell></row><row><cell>Qwen2-1.5B</cell><cell>Contriever self</cell><cell>48.55 48.75</cell><cell>61.02 60.98</cell><cell>68.32 73.84</cell><cell>72.66 77.44</cell><cell>72.41 72.70</cell><cell>67.70 69.27</cell></row><row><cell>Qwen2-7B</cell><cell>Contriever self</cell><cell>53.32 55.12</cell><cell>66.22 67.60</cell><cell>70.80 76.17</cell><cell>74.86 79.82</cell><cell>74.15 75.68</cell><cell>69.01 69.96</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>, *: These authors contributed equally to this work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>, †: Corresponding author.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>Official site of SPG: https://spg.openkg.cn/en-US</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Semantic Classification of Concept: https://openspg.yuque.com/ndx6g9/ps5q6b/fe5p4nh1zhk6p1d8</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>DSL: https://openspg.yuque.com/ndx6g9/ooil9x/sdtg4q3bw4ka5wmz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>Official site of SPG: https://spg.openkg.cn/en-US</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>DSL: https://openspg.yuque.com/ndx6g9/ooil9x/sdtg4q3bw4ka5wmz</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>This work was completed by the <rs type="funder">AntGroup Knowledge Graph Team</rs>, in addition to the authors in the list, other contributors include <rs type="person">Yuxiao He</rs>, <rs type="person">Deng Zhao</rs>, <rs type="person">Xiaodong Yan</rs>, <rs type="person">Dong Han</rs>, <rs type="person">Fanzhuang Meng</rs>, <rs type="person">Yang Lv</rs>, <rs type="person">Zhiying Yin</rs>, etc, thank you all for your continuous innovation attempts and hard work. This work also received strong support from <rs type="person">Professor Huajun Chen</rs>, <rs type="person">Researcher Wen Zhang</rs> of <rs type="affiliation">Zhejiang University</rs>, and <rs type="person">Professor Wenguang Chen</rs> of <rs type="institution">AntGroup Technology Research Institute</rs>, thank you all.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Index</head><p>Reasoning HotpotQA 2Wiki MuSiQue R@2 R@5 R@2 R@5 R@2 R@5 M_Indexing CR re f  The recall performance of different methods across three datasets is presented. The answers to some sub-questions in the LFS re f n method use KG reasoning without recalling supporting chunks, which is not comparable to other methods in terms of recall rate. BackBone model is DeepSeek-V2 API.</p><p>Figure <ref type="figure">8</ref>: Each of the three test datasets comprises 1000 test problems, with 20 tasks processed concurrently and maximum number of iterations n is 3. CR re f 3 method exhibits the fastest execution, whereas LFSH re f 3 method is the slowest. Specifically, CR re f 3 method outperforms LFSH re f 3 method by 149%, 101%, and 134% across the three datasets. In comparison, on the same dataset, the LFS re f 3 method outperforms the LFSH re f 3 method by 13%, 22%, and 18%, respectively, with F1 relative losses of 2.6%, 0.1%, and 1.0%, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Answer</head><p>Question: How many times did the plague occur in the birth place of Concerto in C Major Op 3 6's composer?</p><p>Step1: What specific records or historical accounts detail the number of plague occurrences in Venice during the mid-14th century? A: 22 times Table <ref type="table">13</ref>: An example of using logical-from to guide question planning, reasoning, retrieval, and answer generation, and using multiple rounds of reflection to rephrase questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Example of Logical form Reasoner</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for large language models: A survey</title>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxiang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Com-putational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">Houda</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023. 2023</date>
			<biblScope unit="page" from="9248" to="9274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmarking large language models in retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="17754" to="17762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on rag meeting llms: Towards retrieval-augmented large language models</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbo</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6491" to="6501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXivarXiv:2311.09210</idno>
		<title level="m">Chain-of-note: Enhancing robustness in retrieval-augmented language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">From local to global: A graph rag approach to query-focused summarization</title>
		<author>
			<persName><forename type="first">Darren</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ha</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Mody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Truitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Larson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dalk: Dynamic co-augmentation of llms and kg to answer alzheimer&apos;s disease questions with scientific literature</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">Young</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkwon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Chacko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duy</forename><surname>Duong-Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04819</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Knowledge graphaugmented language models for knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Myung</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaren</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10805</idno>
		<title level="m">Think-on-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Grag: Graph retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">Yuntong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gnn-rag: Graph neural retrieval for large language model reasoning</title>
		<author>
			<persName><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hipporag: Neurobiologically inspired long-term memory for large language models</title>
		<author>
			<persName><forename type="first">Jiménez</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14831</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge graph refinement: A survey of approaches and evaluation methods</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="489" to="508" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Commonsense knowledge graph completion via contrastive pretraining and node clustering</title>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangqing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Integrating multi-head convolutional encoders with cross-attention for improved sparql query translation</title>
		<author>
			<persName><forename type="first">Yi-Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jui-Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan-Ho</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Knowledge base question answering: A semantic parsing perspective</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps</title>
		<author>
			<persName><forename type="first">Xanh</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh-Khoa</forename><surname>Duong Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
		<editor>
			<persName><forename type="first">Donia</forename><surname>Scott</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Núria</forename><surname>Bel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</editor>
		<meeting>the 28th International Conference on Computational Linguistics, COLING 2020<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">December 8-13, 2020. 2020</date>
			<biblScope unit="page" from="6609" to="6625" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multihop questions via single-hop question composition</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><surname>Musique</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="539" to="554" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple yet strong pipeline for hotpotqa</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="8839" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A schema-first formalism for labeled property graph databases: Enabling structured data loading and analytics</title>
		<author>
			<persName><forename type="first">Chandan</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roopak</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ieee/acm international conference on big data computing, applications and technologies</title>
		<meeting>the 6th ieee/acm international conference on big data computing, applications and technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conceptnet-a practical commonsense reasoning tool-kit</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT technology journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How easily do irrelevant inputs skew the responses of large language models</title>
		<author>
			<persName><forename type="first">Siye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.03302</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Honghao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14710</idno>
		<title level="m">Iepile: Unearthing large-scale schema-based information extraction corpus</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Retrieve, summarize, plan: Advancing multi-hop question answering with an iterative approach</title>
		<author>
			<persName><forename type="first">Zhouyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.13101</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Daniel M Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Chinese medical question answer matching based on interactive sentence representation learning</title>
		<author>
			<persName><forename type="first">Xiongtao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungang</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011.13573, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overview of BioASQ 2022: The tenth BioASQ challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirini</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="337" to="361" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient knowledge infusion via KG-LLM alignment</title>
		<author>
			<persName><forename type="first">Zhouyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="page" from="2986" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Onegen: Efficient one-pass unified generation and retrieval for llms</title>
		<author>
			<persName><forename type="first">Jintian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023. 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10014" to="10037" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Colbertv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<editor>
			<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iván</forename><surname>Vladimir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Meza</forename><surname>Ruíz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">July 10-15, 2022. 2022</date>
			<biblScope unit="page" from="3715" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<title level="s">Special Issue of the SIGIR Forum</title>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<meeting>the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM/Springer</publisher>
			<date type="published" when="1994-07">July 1994. 1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised dense information retrieval with contrastive learning</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2022">2022, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large dual encoders are generalizable retrievers</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hernández Ábrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">December 7-11, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9844" to="9855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">RAPTOR: recursive abstractive processing for tree-organized retrieval</title>
		<author>
			<persName><forename type="first">Parth</forename><surname>Sarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubh</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">May 7-11, 2024. OpenReview.net, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dense X retrieval: What retrieval granularity should we use?</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/2312.06648</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From data to wisdom</title>
		<author>
			<persName><surname>Russell L Ackoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied systems analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data, information, knowledge, wisdom (dikw): A semiotic theoretical and empirical exploration of the hierarchy and its quality dimension</title>
		<author>
			<persName><forename type="first">Sasa</forename><surname>Baskarada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Koronios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australasian Journal of Information Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding the difference between information management and knowledge management</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Claudio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terra</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Terezinha</forename><surname>Angeloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KM Advantage</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The data, information, knowledge, wisdom chain: the metaphorical link. Intergovernmental Oceanographic Commission</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Information extraction</title>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Databases</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="377" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From information to knowledge: harvesting entities and relationships from web sources</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</title>
		<meeting>the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="65" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Information extraction: Past, present and future. Multisource, multilingual information extraction and summarization</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Piskorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Yangarber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="23" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Knowledge-based systems for development. Advanced Knowledge Based Systems: Model</title>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Priti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Sajja</surname></persName>
		</author>
		<author>
			<persName><surname>Akerkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications &amp; Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Semantic web for the working ontologist: effective modeling in RDFS and OWL</title>
		<author>
			<persName><forename type="first">Dean</forename><surname>Allemang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hendler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Chen Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Kgfabric</surname></persName>
		</author>
		<title level="m">A scalable knowledge graph warehouse for enterprise data interconnection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">From local to global: A graph rag approach to query-focused summarization</title>
		<author>
			<persName><forename type="first">Darren</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ha</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Mody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Truitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Larson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16130</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction</title>
		<author>
			<persName><forename type="first">Bhaskarjit</forename><surname>Sarmah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benika</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhagash</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04948</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluation of question-focused summarization systems</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Task-Focused Summarization and Question Answering</title>
		<meeting>the Workshop on Task-Focused Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aser: A large-scale eventuality knowledge graph</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cane</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Ki</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the web conference 2020</title>
		<meeting>the web conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="201" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Codekgc: Code language model for generative knowledge graph construction</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinuo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Discos: Bridging the gap between discourse knowledge and commonsense knowledge</title>
		<author>
			<persName><forename type="first">Tianqing</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2648" to="2659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Knowledge graph and knowledge reasoning: A systematic review</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan-Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian-Shu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Science and Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">100159</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Knowledge retrieval (kr)</title>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/WIC/ACM International Conference on Web Intelligence (WI&apos;07)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="729" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dialog-to-action: Conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="2946" to="2955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantics and complexity of sparql</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2006</title>
		<editor>
			<persName><forename type="first">Isabel</forename><surname>Cruz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dean</forename><surname>Allemang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chris</forename><surname>Preist</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Schwabe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Mika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mike</forename><surname>Uschold</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lora</forename><forename type="middle">M</forename><surname>Aroyo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Three levels of generalization for question answering on knowledge bases</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Beyond I.I.D</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3477" to="3488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Chatkbqa: A generatethen-retrieve framework for knowledge base question answering with fine-tuned large language models</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meina</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><surname>Tuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2024</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Conversational question answering over knowledge graphs with transformer and graph attention networks</title>
		<author>
			<persName><forename type="first">Endri</forename><surname>Kacupaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Plepi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Maleshkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<editor>
			<persName><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</editor>
		<meeting>the 16th Conference of the European Chapter<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">April 19 -23, 2021. 2021</date>
			<biblScope unit="page" from="850" to="862" />
		</imprint>
	</monogr>
	<note>EACL 2021</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Modeling transitions of focal entities for conversational knowledge base question answering</title>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
	<note>Long Papers), Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Leveraging abstract meaning representation for knowledge base question answering</title>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021. 2021</date>
			<biblScope unit="page" from="3884" to="3894" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A universal questionanswering platform for knowledge graphs</title>
		<author>
			<persName><forename type="first">Reham</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishika</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><surname>Kalnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Essam</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Management of Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Beamqa: Multi-hop knowledge graph question answering with sequence-to-sequence prediction and beam search</title>
		<author>
			<persName><forename type="first">Farah</forename><surname>Atif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ola</forename><forename type="middle">El</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djellel</forename><surname>Difallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Structgpt: A general framework for large language model to reason over structured data</title>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023</title>
		<editor>
			<persName><forename type="first">Houda</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023. 2023</date>
			<biblScope unit="page" from="9237" to="9251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Don&apos;t generate, discriminate: A proposal for grounding language models to real-world environments</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4928" to="4949" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Case-based reasoning for natural language queries over knowledge bases</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Yoon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazaros</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="9594" to="9611" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Evaluating the knowledge base completion potential of GPT</title>
		<author>
			<persName><forename type="first">Blerta</forename><surname>Veseli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Christoph</forename><surname>Kalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">Houda</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023. 2023</date>
			<biblScope unit="page" from="6432" to="6443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Making large language models perform better in knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Chatrule: Mining logical rules with large language models for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<idno>CoRR, abs/2309.01538</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Complex logical reasoning over knowledge graphs using large language models</title>
		<author>
			<persName><forename type="first">Nurendra</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
		<idno>CoRR, abs/2305.01157</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Learning to plan for retrieval-augmented large language models from knowledge graphs</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2406.14282</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Step2: Determine the Foundation Years of Each Identified Team. Logical Form: Retrieval(s=o1, p=p2:foundationYear, o=o2:Year) Step3: Which team was founded last? Logical Form: Sort(set=o1, orderby=o2, direction=max, limit=1) question: What is the sum of 30 + 6 and the age of the founder of Tesla in 2027 ? Step1: What is the sum of 30 + 6 ? Logical Form: math1 = Math(30+6) Step2: Who is the founder of Tesla? Logical Form: Retrieval(s=s2:Company[Tesla], p=p2:founder, o=o2) Step</title>
		<author>
			<persName><forename type="first">Mengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2402.13593</idno>
	</analytic>
	<monogr>
		<title level="m">Step5: What is the sum of math1 and math4? Logical Form: math5 = Math(math1+math4) Logical Reasoning question: Find a picture containing vegetables or fruits. Step1: Find pictures containing vegetables. Logical Form: Retrieval(s=s1:Image, p=p2:contains, o=o1:Vegetables) Step2: Find pictures containing fruits</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Knowledge graph enhanced large language model editing Numerical Reasoning question: Which sports team for which Cristiano Ronaldo played in 2011 was founded last ? Step1: Identify the Sports Teams Cristiano Ronaldo Played for in 2011 which year was the founder of Tesla born? Logical Form: Retrieval(s=o2, p=p3:yearOfBirth, o=o3) Step4: How old will the founder of Tesla be in the year 2027? Logical Form: math4 = Math(2027-o3 Logical Form: Retrieval(s=s1:Player[Cristiano Ronaldo], p=p1:playedFor, o=o1:SportsTeam, p.PlayedForInYear=2011) Retrieval(s=s2:Image, p=p2:contains, o=o2:Fruits) Step3: Output s1, s2. Logical Form: Output(s1, s2) question: Find a picture containing vegetables and fruits. Step1: Find pictures containing vegetables. Logical Form: Retrieval(s=s1:Image, p=p2:contains, o=o1:Vegetables) Step2: Find pictures containing fruits. Logical Form: Retrieval(s=s1, p=p2:contains, o=o2:Fruits) Step3: Output s1. Logical Form: Output(s1) Semantic Deduce question: Do I need to present the original ID Step1: What documents are required to apply for a passport? Logical Form: Retrieval(s=s1:Event[apply for a passport], p=p1:support_chunks, o=o1:Chunk) Step2: Does this set of documents include the original identity card? Logical Form: Deduce(left=o1, right=the original identity card, op=entailment) Table 14: The cases of reasoning with logical form</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
