<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models</title>
				<funder ref="#_Tfptaan #_sXxAwK4">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_NKtuGf4">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-03">3 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
							<email>shawnshan@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jenna</forename><surname>Cryan</surname></persName>
							<email>jennacryan@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Wenger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
							<email>htzheng@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
							<email>ranahanocka@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-03">3 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">FD607E0C2D03F4EB5214904E91B8E633</idno>
					<idno type="arXiv">arXiv:2302.04222v5[cs.CR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after "fine-tuning" on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply "style cloaks" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (&gt;92%) and against adaptive countermeasures (&gt;85%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is not an exaggeration to say that the arrival of text-toimage generator models has transformed, perhaps upended, the art industry. By sending simple text prompts like "A picture of a corgi on the moon" to diffusion models such as StableDiffusion or MidJourney, anyone can generate incredibly detailed, high resolution artwork that previously required many hours of work by professional artists. AI-art such as those in Figure <ref type="figure" target="#fig_0">1</ref> have won awards at established art conventions <ref type="bibr" target="#b68">[69]</ref>, served as cover images for magazines <ref type="bibr" target="#b46">[47]</ref>, and used to illustrate children's books <ref type="bibr" target="#b60">[61]</ref> and video games <ref type="bibr" target="#b76">[77]</ref>. More powerful models continue to arrive <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b82">83]</ref>, catalyzed by VC funding <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b97">98]</ref>, technical research breakthroughs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>, and powered at their core by continuous training on a large volume of human-made art scraped from online art repositories such as ArtStation, Pinterest and DeviantArt.</p><p>Only months after their arrival, these models are rapidly growing in users and platforms. In September 2022, Mid-Journey reported over 2.7 million users and 275K AI art images generated each day <ref type="bibr" target="#b30">[31]</ref>. Beyond simple prompts, many have taken the open sourced StableDiffusion model, and "fine-tuned" it on additional samples from specific artists, allowing them to generate AI art that mimics the specific artistic styles of that artist <ref type="bibr" target="#b31">[32]</ref>. In fact, entire platforms have sprung up where home users are posting and sharing their own customized diffusion models that specialize on mimicking specific artists, likeness of celebrities, and NSFW themes <ref type="bibr" target="#b13">[14]</ref>.</p><p>Beyond open questions of copyrights <ref type="bibr" target="#b5">[6]</ref>, ethics <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b59">60]</ref>, and consent <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>, it is clear that these AI models have had significant negative impacts on independent artists. For the estimated hundreds of thousands of independent artists across the globe, most work on commissions, and attract customers by advertising and promoting samples of their artwork online. First, professional artists undergo years of training to develop their individual artistic styles. A model that mimics this style profits from that training without compensating the artist, effectively ending their ability to earn a living. Second, as synthetic art mimicry continues to grow for popular artists, they displace original art in search results, further disrupting the artist's ability to advertise and promote work to potential customers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b74">75]</ref>. Finally, these mimicry attacks are demoralizing art students training to be future artists. Art students see their future careers replaced by AI models even if they can successfully find and develop their own artistic styles <ref type="bibr" target="#b54">[55]</ref>.</p><p>Today, all of these consequences have indeed occurred in the span of a few months. Art students are quitting the field; AI models that mimic specific artists are uploaded and shared for free; and professional artists are losing their livelihoods to models mimicking their unique styles. Artists are fighting back via lawsuits <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref>, online boycotts and petitions <ref type="bibr" target="#b21">[22]</ref>, but legal and regulatory action can take years, and are difficult to enforce internationally. Thus most artists are faced a choice to 1) do nothing, or 2) stop sharing samples of their art online to avoid training models, and in doing so cripple their main way to advertise and promote their work to customers.</p><p>In this paper, we present the design, implementation and evaluation of a technical alternative to protect artists against style mimicry by text-to-image diffusion models. We present Glaze, a system that allows an artist to apply carefully computed perturbations to their art, such that diffusion models will learn significantly altered versions of their style, and be ineffective in future attempts at style mimicry. We worked closely with members of the professional artist community to develop Glaze, and conduct multiple user studies with 1,156 participants from the artist community to evaluate its efficacy, usability, and robustness against a variety of active countermeasures.</p><p>Intuitively, Glaze works by taking a piece of artwork, and computing a minimal perturbation (a "style cloak") which, when applied, shifts the artwork's representation in the generator model's feature space towards a chosen target art style. Training on multiple cloaked images teaches the generator model to shift the artistic style it associates with the artist, leading to mimicry art that fails to match the artist's true style.</p><p>Our work makes several key contributions: • We engage with top professional artists and the broader community, and conduct user studies to understand their views and concerns towards AI art and the impact on their careers and community. • We propose Glaze, a system that protects artists from style mimicry by adding minimal perturbations to their artwork to mislead AI models to generate art different from the targeted artist. 92% of surveyed artists find the perturbations small enough not to disrupt the value of their art. • Surveyed artists find that Glaze successfully disrupts style mimicry by AI models on protected artwork. 93% of artists rate the protection is successful under a variety of settings, including tests against real-world mimicry platforms. • In challenging scenarios where an artist has already posted significant artworks online, we show Glaze protection remains high. <ref type="bibr" target="#b86">87</ref>.2% of surveyed artists rate the protection as successful when an artist is only able to cloak 1/4 of their online art (75% of art is uncloaked). • We evaluate Glaze and show that it is robust (protection success &gt; 85%) to a variety of adaptive countermeasures. • We discuss Glaze deployment and post-deployment experiences, including countermeasures in the wild.</p><p>Ethics. Our user study was reviewed and approved by our institutional review board (IRB). All art samples used in experiments were used with explicit consent by their respective artists. All user study participants were compensated for their time, although many refused payment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: AI Art and Style Mimicry</head><p>In this section, we provide critical context in the form of basic background on current AI art models and style mimicry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-to-Image Generation</head><p>Since Text-to-image generation was first proposed in 2015 <ref type="bibr" target="#b49">[50]</ref>, a stream of research has proposed newer model architectures and training methods enabling generation of higher-quality images <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b106">107]</ref>. The high level design of recent models used for AI art generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref> is shown in Figure <ref type="figure" target="#fig_3">3</ref>. During training, the model takes in an image x and uses a feature extractor Φ to extract its features, producing Φ(x). Simultaneously, a conditional image generator G takes in a corresponding text caption (s) and outputs a predicted feature vector G(s). Then the parameters of G are optimized so the text feature vector G(s) matches the image feature vector Φ(x). At generation time, a user gives G a text prompt s 0 , and G outputs an image feature vector G(s 0 ). A decoder D then decodes G(s 0 ) to produce the final generated image.</p><p>Compared to earlier models based on generative adversarial networks (GANs) or variational autoencoders (VAE) <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b106">107]</ref>, more recent models <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref> leveraging diffusion models produce significantly higher quality images. Feature extractor (Φ) is used to reduce the dimensionality of the input image to facilitate the generation process. The extractor Φ and decoder D are often a pair of variational autoencoder (VAE) <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b66">67]</ref>, i.e., extractor (encoder) extracts image features and decoder map features back to images. Training Data Sources. The training datasets of these models typically contain image/ALT text pairs scraped from the Internet. They are extremely large, e.g. LAION <ref type="bibr" target="#b77">[78]</ref> contains 5 billion images collected from 3 billion webpages.</p><p>These datasets are subject to minimal curation and governance. Data collectors typically only filter out data with extremely short or incorrect text captions (based on an automated text/image alignment metric <ref type="bibr" target="#b77">[78]</ref>). Since copyrighted images are not filtered <ref type="bibr" target="#b77">[78]</ref>, these datasets are rife with private, sensitive content, including copyrighted artworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Style Mimicry</head><p>In a style mimicry attack, a bad actor uses an AI art model to create art in a particular artist's style without their consent. More than 67% of art pieces showcased on a popular AI-artsharing website leverage style mimicry <ref type="bibr" target="#b52">[53]</ref>. Style mimicry techniques. Today, a "mimic" can easily copy the style of a victim artist with only an open-source textto-image model and a few samples of artwork from the artist. A naive mimicry attack directly queries a generic text-toimage model using the name of the victim artist. For example, the prompt "a painting in the style of Greg Rutkowski" would cause the model to generate images in the style of Polish  artist Greg Rutkowski. This is because many of Rutkowski's artworks appear in training datasets of these generic models labeled with his name.</p><p>Naive mimicry can succeed when the artist is well-known and has a significant amount of art online, but fail on other artists. In more recent mimicry attacks, a mimic fine-tunes a generic text-to-image model on samples of a target artist's work (as few as 20 unique pieces) downloaded from online sources. This calibrates the model to the victim artist's style, identifying important features related to the victim style and associating these regions in the feature space with the victim artist's name <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b69">70]</ref>. This enables style mimicry with impressive accuracy. The entire fine-tuning process takes less than 20 minutes on a low-end consumer GPU 1 . Real-work mimicry incidents. The first well-known incident of mimicry was when a Reddit user stole American artist Hollie Mengert's style and open-sourced the style-specific model on Reddit <ref type="bibr" target="#b2">[3]</ref>. Figure <ref type="figure" target="#fig_2">2</ref> has a side-by-side comparison of Hollie's original artwork and plagiarized artwork generated via style mimicry. Later, famous cartoonist Sarah Andersen reported that AI art models can mimic her cartoon drawings <ref type="bibr" target="#b1">[2]</ref>, and other similar incidents abound <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b99">100]</ref>.</p><p>Several companies <ref type="bibr" target="#b76">[77]</ref> have even hosted style mimicry as a service, allowing users to upload a few art pieces painted by victim artists and producing new art in the victim styles. CivitAI <ref type="bibr" target="#b13">[14]</ref> built a large online marketplace where people share their customized stable diffusion models, fine-tuned on certain artwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collaborating with Artists</head><p>Next, we explain our collaborative relationship with professional artists, and its significant impact on our key evaluation metrics in this paper. We also summarize key results from our first user study on views of AI art and mimicry by members of the artist community. Artists have spoken out against style mimicry in numerous venues, focusing particularly on how it violates their intellectual property rights and threatens their livelihoods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93]</ref>. Others have taken direct action. The Concept Art Association raised over $200K to fight AI art, and filed a class action lawsuit in the US against AI art companies <ref type="bibr" target="#b34">[35]</ref>. In November 2022, artists organized a large protest against ArtStation <ref type="bibr" target="#b91">[92]</ref>, the large digital art sharing platform that allowed users to post AI artwork without identification. Anti-AI images flooded the site for several weeks, until Art-Station banned the protest images <ref type="bibr" target="#b22">[23]</ref>. Recently, the Writers Guild of America (WGA) went on strike demanding contractual changes to ban generative AI <ref type="bibr" target="#b11">[12]</ref>.</p><p>Members of the professional art community reached out to us in Sept 2022. We joined online town halls and meetings alongside hundreds of professionals, including Emmy winners and artists at major film studios. After learning more, we began an active collaboration with multiple professional artists, including award-winning artist Karla Ortiz, who leads efforts defending artists and is lead plaintiff in the class action suit. The artists helped this project in multiple ways, by 1) sharing experiences about specific ways AI-art has impacted them and their colleagues; 2) sharing domain knowledge about what is acceptable to artists in terms of perturbations on their art; and 3) helping to widely disseminate our user study to members of their professional organizations, including the Concept Art Association and the Animation Guild (TAG839).</p><p>Evaluation via Direct Feedback from Artists. Our goal is to help artists disrupt AI models trying to mimic their artistic style, without adversely impacting their own artwork. Because "success" in this context is highly subjective ("Did this AI-art successfully mimic Karla's painting style?"), we believe the only reliable evaluation metric is direct feedback by professional artists themselves. Therefore, wherever possible, the evaluation of Glaze is done via detailed user studies engaging members of the professional artist community, augmented by an empirical score we develop based on genre prediction using CLIP models.</p><p>Survey # of artists Content Survey 1 1156 1) Broad views of AI art and style mimicry( §3.1) 2) Glaze's usability, i.e. acceptable levels of cloaking ( §6.3) 3) Glaze performance in disrupting style mimicry ( §6.3) Survey 2 (Extension to Survey 1) 151 1) Additional performance tests ( §6.3) 2) Robustness to advanced scenarios ( §6.4) and countermeasures ( §7) 3) Additional system evaluation (Appendix A)</p><p>Table <ref type="table">1</ref>. Information on our user studies: the number of artist participants and where we report the results of the studies. We sent Survey 2 to some specific participants from survey 1 who volunteered to participate in a followup study.</p><p>We deployed two user studies during the course of this project (see Table <ref type="table">1</ref>). Both are IRB-approved by our institution. Both draw participants from professional artists informed via their social circles and professional networks. The first (Survey 1, §3.1, §6.3), asked participants about their broad views of AI style mimicry, and then presented them with a number of inputs and outputs of our tool, and asked them to give ratings corresponding to key metrics we wanted to evaluate. We select a subset of participants from the first study to participate in a longer and more in-depth study (Survey 2) where they were asked to evaluate the performance of Glaze in additional settings ( §6.3, §6.4, §7, and Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Artists' Opinions on Style Mimicry</head><p>While we expected artists to view style mimicry negatively, we wanted to better understand how much individual artists understood this topic and how many perceived it as a threat. Here we describe results from Survey 1 to gather perceptions of the potential impact of AI art on existing artists. Survey Design. Our survey consisted of both multiple choice and free response questions to understand how well people understand the concept of AI art, and how well the models successfully imitate the style of artists. Additionally, we asked artists about the extent to which they anticipate the emergence of AI art to impact their artistic activities, such as posting their art online and their job security. A handful of professional artists helped disseminate our survey to their respective artist community groups. Overall, we collected responses from 1,207 participants, consisting primarily of professional artists (both full-time (46%) and part-time/freelancer (50%)) and some non-artist members of the art community who felt invested in the impact of AI art (4%). Of the participants who consider themselves artists, their experience varied: &lt;1 year (13%), 1-5 years (49%), 5-10 years (19%), 10+ years (19%). Participants' primary art style varied widely, including: animation, concept art, abstract, anime, game art, digital 2D/3D, illustration, character artwork, storyboarding, traditional painting/drawing, graphic design, and others. Key Results. Our study found that 91% of the artists have read about AI art extensively, and either know of or worry about their art being used to train the models. Artists expect AI mimicry to have a significant impact on artist community: 97% artists state it will decrease some artists' job security; 88% artists state it will discourage new students from studying art; and 70% artists state it will diminish creativity. "Junior positions will become extinct," as stated by one participant.</p><p>Many artists (&gt; 89% artists) have already or plan to take actions because of AI mimicry. Over 95% of artists post their artwork online. Out of these artists, 53% of them anticipate reducing or removing their online artwork, if they haven't already. Out of these artists, 55% of them believe reducing their online presence will significantly impact their careers. One participant stated "AI art has unmotivated myself from uploading more art and made me think about all the years I spent learning art." 78% of artists anticipate AI mimicry would impact their job security, and this percentage increases to 94% for the job security of newer artists. Further, 24% of artists believe AI art has already impacted their job security, and an additional 53% expect to be affected within the next 3 years. Over 51% of artists expressed interest in proactive measures, such as personally joining class action lawsuits against AI companies.</p><p>Professional artists thought AI mimicry was very successful at mimicking the style of specific artists. We showed the artists examples of original artwork from 23 artists, and the artwork generated by a model attempting to mimic their styles (detailed mimicry setup in §6). 77% of artists found the AI model successfully or very successfully mimic the styles of victim artists, with one stating "it's shocking how well AI can mimic the original artwork." Additionally, 19% of participants thought the AI mimicry is somewhat successful, leaving only &lt; 5% of artists rating the mimicry as unsuccessful. Several artists also pointed out that, as artists, upon close inspection they could spot differences between the AI art and originals, but were skeptical the general public would notice them.</p><p>A significant concern of most participants, surprisingly, is not just the existence of AI art, but rather scraping of existing artworks without permission or compensation. As one participant stated: "If artists are paid to have their pieces be used and asked permission, and if people had to pay to use that AI software with those pieces in it, I would have no problem." However, without consent to use their artwork to train the models, "it's incredibly disrespectful to the artist to have their work 'eaten' by a machine [after] many years to grow our skills and develop our styles."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminaries</head><p>We propose Glaze, a tool that protects artists against AI style mimicry. An artist uses Glaze to add small digital perturbations ("cloak") to images of their own art before sharing online (Figure <ref type="figure" target="#fig_6">5</ref>). A text-to-image model that trains on cloaked images of artwork will learn an incorrect representation of the artist's style in feature space i.e., the model's internal understanding of artistic styles. When asked to generate art pieces in victim's style, the model will fail to mimic the style of the victim, and instead output art pieces in a recognizably different style.  Here, we first introduce the threat model, then discuss existing alternatives to the AI style mimicry problem. We present the intuition behind Glaze and detailed design in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI company</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Threat Model</head><p>Here we state assumptions for both the artists protecting their own art and the users training models to replicate their artistic style. We refer to these AI art model trainers as "mimics." Artists. Artists want to share and promote their artwork online without allowing mimics to train models that replicate their art styles. Sharing art online enables artists to sell their work and attract commissioned work, fueling their livelihoods ( §3). Artists protect themselves by adding imperceptible perturbations to their artwork before sharing them as shown in Figure <ref type="figure" target="#fig_6">5</ref>. The goal of the Glaze cloak is to disrupt the style mimicry process, while only introducing minimal perturbation on images of the artwork.</p><p>We assume the artists have access to moderate computing resources (e.g., a laptop) and add perturbation to images of their artwork locally before posting online. We also assume artists have access to some public feature extractor (e.g., open-source models such as Stable Diffusion). We begin with assumption that artists use the same feature extractor as mimics (large majority of mimics use the open-source Stable Diffusion model). We later relax this assumption. Mimics. The mimic's goal is to train a text-to-image model that generates high-quality art pieces of any subject in the victim's style. A mimic could be a well-funded AI company, e.g., Stability AI or OpenAI, or an individual interested in the style of victim artist. We assume the mimic has:</p><p>• access to the weights of generic text-to-image models welltrained on large datasets; • access to art pieces from the target artist; • significant computational power.</p><p>We assume the attack scenario where the mimic fine-tunes its model on images of the artist's artwork (as shown in Figure <ref type="figure" target="#fig_5">4</ref>). This is stronger than the naive mimic attack without fine tuning. Finally, we assume the mimic is aware of our protection tool and can deploy adaptive countermeasures ( §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Potential Alternatives and Challenges</head><p>A number of related prior works target protection against invasive and unauthorized facial recognition models. They proposed "image cloaking" as a tool to prevent a user's images from being used to train a facial recognition model of them <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b94">95]</ref>. They share a similar high level approach, by using optimized perturbations that cause cloaked images to have drastically different feature representations from original user images. It is possible to adapt existing cloaking-based systems to protect artists against AI style mimicry. Protection system would compute a cloak on each artwork in order to perturb its feature space representation to be different from its unperturbed representation. This can succeed if the cloak significantly shifts the artwork's feature representation, making resulting models generate dramatically different artwork.</p><p>We found that in practice, however, existing solutions are unable to introduce large-enough feature space shifts to achieve the desired protection. This is due to the properties of feature spaces in text-to-image models. Face recognition models classify identities, so their feature spaces mainly represent identity-related information. On the other hand, text-to-image models reconstruct original images from extracted features, so their feature spaces retain more information about the original image (objects, locations, color, style, etc.). Thus, producing the same shift in feature representation in a textto-image model is much harder (requires more perturbation budget) than in a classification model. This observation is validated by prior work showing that adversarial perturbations are much less effective at attacking generative models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b84">85]</ref>. Specifically, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b84">85]</ref> found that adversarial attack methods that are effective at attacking classifiers are significantly less effective at attacking autoencoders. We empirically confirm that existing cloaking methods cannot prevent AI mimicry ( §A.1 in Appendix). We show that Fawkes <ref type="bibr" target="#b80">[81]</ref>  and LowKey <ref type="bibr" target="#b12">[13]</ref> perform poorly in this setting, even when artists add highly visible cloaks to their artwork.</p><p>For generative models, concurrent work <ref type="bibr" target="#b75">[76]</ref> proposes Pho-toGuard, a method to cloak images to prevent unauthorized image edits (inpainting) on cloaked images. Similar to existing cloaking systems, PhotoGuard tries to indiscriminately minimize all information contained in an image (i.e., the norm of the feature vector) to prevent models from editing the image. Thus, it is also not effective at mimicry prevention.</p><p>Design Challenges. The main reason that existing cloaking methods fail to prevent AI mimicry is because they indiscriminately shift all features in an image, wasting the cloak perturbation budget on shifting unnecessary features (e.g., object shape, location, etc.). Protecting artist's style requires only shifting features related to the artistic style of victim. This can be achieved if a text-to-image model learns to draw objects similar to those drawn by the victim artist as long as the model cannot mimic the artist's unique style. Thus, optimal protection from mimicry requires concentrating the cloak on style-specific features.</p><p>Unfortunately, identifying and separating out these stylespecific features is difficult. Even assuming the existence of interpretability methods that perfectly explain the feature space of a text-to-image model, there is no clear way to mathematically define and calculate "artistic styles." In all likelihood, any definition would change across different styles. For example, "impressionist" likely correlates more strongly with color features, whereas "cubism" correlates with shape features. Even across multiple art pieces in the same style, the style may manifest differently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLAZE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Disrupting Style Mimicry with Glaze</head><p>In this section, we introduce Glaze, its design intuition followed by the detailed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Intuition</head><p>Our key intuition is to identify and isolate style-specific features of an artist's original artwork, i.e., the set of image features that correspond to artistic style. Then Glaze computes cloaks while focusing the perturbation budget on these stylespecific features to maximize impact on stylistic features. As discussed, identifying and calculating style-specific features in model's feature space is difficult due to the poor interpretability of model features and how art style manifests differently across artworks. We overcome these two challenges by designing a style-dependent and artwork-dependent method that operates at image space. Given an artwork, we leverage "style transfer," an end-to-end computer vision technique, to modify and isolate its style components. "Style transfer" transforms an image into a new image with a differ-ent style (e.g., from impressionist style to cubist style) while keeping other aspects of the image similar (e.g., subject matter and location).</p><p>We leverage style transfer in our protection technique as follows. Given an original artwork from the victim artist, we apply style-transfer to produce a similar piece of art with a different style, e.g., in style of "an oil painting by Van Gogh" (Figure <ref type="figure">6 a</ref>). The new version has similar content to the original, but its style mirrors that of Van Gogh. We show more style-transfer examples with different target styles in Figure <ref type="figure">7</ref>. Now, we can use the style-transferred artwork as projection target to guide the perturbation computation. This perturbs the original artwork's style-specific features towards that of the style-transferred version. We do this by optimizing a cloak that, when added to the original artwork, makes its feature representation similar to the style-transferred image. Since the content is identical between the pair of images, cloak optimization will focus its perturbation budget on style features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Computing Style Cloaks</head><p>Using this approach, we compute style cloaks to disrupt style mimicry as follows. Given an artwork (x), we use an existing feature extractor to compute the style-transferred version of x into target style T : Ω(x, T ). We then compute a style cloak δ x , such that δ x moves x's style-specific feature representation to match that of Ω(x, T ) while minimizing visual impact. The cloak generation optimization is: min</p><formula xml:id="formula_0">δ x Dist (Φ(x + δ x ), Φ(Ω(x, T ))) ,<label>(1)</label></formula><formula xml:id="formula_1">subject to |δ x | &lt; p,</formula><p>where Φ is a generic image feature extractor commonly used in text-to-image generation tasks, Dist(.) computes the distance of two feature representations, |δ x | measures the perceptual perturbation caused by cloaking, and p is the perceptual perturbation budget. As discussed in §5.1, the use of the style-transferred image Ω(x, T ) guides the cloak optimization in Eq (1) to focus on changing style-specific image features. To maximize cloak efficacy, the target style T should be dissimilar from artist's original style in the feature space. We discuss our heuristic for selecting target styles in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed System Design</head><p>Now we present the detailed design of Glaze. Given a victim artist V , Glaze takes as input the set of V 's artwork to be shared online X V , an image feature extractor Φ, a styletransfer model Ω, and perturbation budget p. Note that in many cases, a single model (e.g. Stable Diffusion) provides both Φ and Ω.</p><p>Step 1: Choose Target Style. The selected target style T should be sufficiently different from V 's style in model feature space to maximize chances of disrupting style mimicry. For example, Fauvism and Impressionism are distinct art styles that often look visually similar to the untrained eye. Image of an impressionist painting style cloaked to Fauvism might not produce a visually discernible effect on model-generated paintings. Note that an artist can maximize their ability to avoid mimicry if they consistently style cloak all their artwork towards the same target T .</p><p>For a new user, Glaze uses the following algorithm to randomly select T from a set of candidate styles reasonably different from V 's style. The algorithm first inspects a public dataset of artists, each with a specific style (e.g., Monet, Van Gogh, Picasso). For each candidate target artist/style, it selects a few images in that style and calculates their feature space centroid using Φ. It also computes V 's centroid in Φ using V 's artwork. Then, it locates the set of candidate styles whose centroid distance to V 's centroid is between the 50 to 75 percentile of all candidates. Finally, it randomly selects T from the candidate set.</p><p>Step 2: Style transfer. Glaze then leverages a pre-trained style-transfer model Ω <ref type="bibr" target="#b66">[67]</ref> to generate the style-transferred artwork for optimization. Given each art piece x ∈ X V and target style T , it style transfers x to target style T to produce style-transferred image Ω(x, T ).</p><p>Step 3: Compute cloak perturbation. Then, Glaze computes the cloak perturbation, δ x for x, following the optimization defined by eq. ( <ref type="formula" target="#formula_0">1</ref>), subject to |δ x | &lt; p. Our implementation uses LPIPS (Learned Perceptual Image Patch Similarity) <ref type="bibr" target="#b105">[106]</ref> to bound the perturbation. Different from the L p distance used in previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b70">71]</ref>, LPIPS has gained popularity as a measure of user-perceived image distortion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b67">68]</ref>. Bounding cloak generation with this metric ensures that cloaked versions of images are visually similar to the originals. We apply the penalty method <ref type="bibr" target="#b55">[56]</ref> to solve the optimization in eq.( <ref type="formula" target="#formula_0">1</ref>) as follows:</p><formula xml:id="formula_2">min δ x ||Φ(Ω(x, T )), Φ(x + δ x )|| 2 2 + α • max(LPIPS(δ x ) -p, 0)<label>(2</label></formula><p>) where α controls the impact of the input perturbation. L 2 distance is used to calculate feature space distance. Upload artwork online. Finally, the artist posts the cloaked artwork online. For artists already with a large online presence, they can cloak and re-upload artwork on their online portfolio. While updating online images is not always possible, Glaze can be effective even when the mimic's model has significant amount of uncloaked art ( §6.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">On the Efficacy of Style Cloaks</head><p>Glaze's style cloaks work by shifting feature representation of artwork in the generator model. But how much shift do we need in order to have a noticeable impact on mimicked art?</p><p>Two reasons suggest that even small shifts in style will have a meaningful impact in disrupting style mimicry. First, generative models used for style mimicry have continuous output spaces, i.e., any shift in image feature representation results in changes in the generated image. Because generative models are trained to interpolate their continuous feature spaces <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b95">96]</ref>, any shift in the model's representation of art style results in a new style, a "blend" between the artist and the chosen target style. Second, mimicked artwork must achieve reasonable quality and similarity in style to the artist to be useful. Small shifts in the style space often produce incoherent blends of conflicting styles that are enough to disrupt style mimicry, e.g., thick oil brushstrokes of Van Gogh's style mixed into a realism portrait.</p><p>These two factors contribute to Glaze's success in more challenging scenarios ( §6.4), and its robustness against countermeasures (e.g. adversarial training) that succeed against cloaking tools for facial recognition ( §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we evaluate Glaze's efficacy in protecting artists from style mimicry. We first describe the datasets, models, and experimental configurations used in our tests. Then we present the results of Glaze's protection in a variety of settings. Due to Glaze's highly visual nature, we evaluate its performance using both direct visual assessment by human artists in a user study, and automated metrics (see §6.2 for details). Summary of results. Over 93% of artists surveyed believe Glaze effectively protects artists' styles from AI style mimicry attacks. Protection efficacy remains high in challenging settings, like when the mimic has access to unprotected artwork. Glaze also achieves high protection performance against a real-world mimicry-as-a-service platform. Of our 1156 artist participants, over 92% found the perturbations introduced by cloaking small enough not to disrupt the value of their art, and over 88% would like to use Glaze to protect their own artwork from mimicry attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setup</head><p>Mimicry dataset.</p><p>We evaluate Glaze's performance in protecting the styles of the following two groups of artists:</p><p>• Current artists: 4 professional artists let us use their artwork in our experiments. These artists have different styles and backgrounds (e.g., full-time/freelancers, watercolor painters/digital artists, well-known/independent). Each provided us with between 26 to 34 private original art pieces for our experiments. We use perceptual hashing <ref type="bibr" target="#b37">[38]</ref> to verify that none of these are included in existing public datasets used to train generic text-to-image models (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b77">78]</ref>). • Historical artists: We also evaluate Glaze's protection on 195 historical artists (e.g., van Gogh, Monet) from the WikiArt dataset <ref type="bibr" target="#b72">[73]</ref>. The WikiArt dataset contains 42,129 art pieces from 195 artists. Each art piece is labeled with its genre (e.g., impressionism, cubism). We randomly sampled 30 art pieces from each artist to use in style mimicry attacks. Generic text-to-image models found online have been trained on some artwork from these artists. Using this art simulates a more challenging scenario in which a famous artist attempts to disrupt a model that already understands their style. Mimicry attack setup. We recreate the strongest-possible mimicry attack scenario, based on techniques used in realworld mimicry incidents <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b99">100]</ref>, that works as follows.</p><p>First, we take art pieces from the victim artist V and generate a text caption for each piece using an image captioning model <ref type="bibr" target="#b48">[49]</ref>. The pretrained image captioning model generates a short sentence to describe the image. We found that this model can correctly caption protected images (examples in Figure <ref type="figure" target="#fig_0">18</ref>), likely because Glaze focuses on perturbing style features while the captioning models focus on image content. Then, we append the artist's name to each caption, e.g., "mountain range by Vincent van Gogh". Finally, we fine-tune a pre-trained generic text-to-image model (details below) on the caption/image pairs. We use 80% of the art pieces from the victim artists to finetune models that mimic each artist's style, reserving the rest for testing. We fine-tune for 3000 optimization steps, which we find achieves the best mimicry performance (Figure <ref type="figure" target="#fig_0">19</ref> in Appendix). We then use the fine-tuned, style-specific model to generate mimicked artwork in style of each victim artist. We query the model using the generated captions (which include V 's name) from the held-out test artwork set. We generate 5 pieces of mimicked art for each text caption using different random seeds and compare these to the real victim art pieces with this caption. Additional details on training and generation parameters, as well as its sensitivity to random seed selection and the number of training art pieces are in Appendix A.2. Text-to-image models. We use two state-of-the-art, public, generic text-to-image models in our experiments: • Stable Diffusion (SD): Stable Diffusion is a popular and highperforming open-source text-to-image model <ref type="bibr" target="#b82">[83]</ref>,trained on 11.5 million images from the LAION dataset <ref type="bibr" target="#b77">[78]</ref>. SD training takes over 277 GPU months (on A100 GPU) and costs around $600K <ref type="bibr" target="#b82">[83]</ref>. SD uses diffusion methods to generate images and achieves state-of-the-art performance on several benchmarks <ref type="bibr" target="#b66">[67]</ref>. Viewed as one of the best open-source models, SD has powered many recent developments in textto-image generation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b76">77]</ref>. We use SD version 2.1 in the paper <ref type="bibr" target="#b82">[83]</ref>, the most up-to-date version as of December 2022.</p><formula xml:id="formula_3">• DALL•E-mega (DALL•E-m): DALL•E-m-mega,</formula><p>an updated version of the more well-known DALL•E-m-mini, is an opensource model based on OpenAI's DALL•E-m 1 [65]. The model leverages a VAE for image generation and is trained on 17 million images from three different datasets [10, 82, 87]. Training takes 2 months on 256 TPUs [16]. While DALL•E-m performs worse than diffusion-based models like SD, we use it to evaluate how Glaze generalizes to different model architectures.</p><p>Glaze configuration. We generate cloaks for each of victim V 's art pieces following the methodology of §5.3. First, we use the target selection algorithm to select a target style T . We choose from a set of 1119 candidate target styles, collected by querying the WikiArt dataset with artist and genre names, e.g., "Impressionism painting by Monet"<ref type="foot" target="#foot_0">foot_0</ref> . We then style transfer each victim art piece into the target style leveraging the style transfer functionality of stable diffusion model (stable diffusion model has both text-to-image and style transfer functionality). A style transfer model takes in an original image and a target prompt as input. Leveraging a similar diffusion process, the model modifies the original image to a style similar to that described in the target prompt. More information on style transfer can be found in <ref type="bibr" target="#b71">[72]</ref>. Finally, we optimize a cloak for each art piece using Eq. 2 by running the Adam optimizer for 500 steps. We benchmark Glaze's runtime on artwork with resolution ranging from 512 to 6000 pixels, using SD's feature extractor (ViT model with 83 million parameters). It takes an average of 1.2 mins on Titan RTX GPU and 7.3 mins on a single Intel i7 CPU to generate a cloak for a single piece of art.</p><p>In our initial experiments, we assume Glaze generates cloaks using the same image feature extractor as the mimic (e.g. SD's or DALL•E-m's feature extractor). We relax this assumption and evaluate Glaze's performance when artists and mimics use different feature extractors in §6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics</head><p>We evaluate our protection performance using both visual assessment and feedback from human artists, and a scalable metric. Here, we describe the setup of our evaluation study and define the exact metrics used for evaluation. Artist-rated protection success rate (Artist-rated PSR):</p><p>The user studies ask artists to rate the performance of Glaze. We generate a dataset of mimicry attacks on 13 victim artists (the 4 current artists and 9 randomly chosen historical artists) across 23 protection scenarios (including ones in §7). For each participant, we randomly select a set of mimicry attacks out of these 13 × 23 settings and ask them to evaluate protection success. For each mimicry attempt, we show participants 4 mimicked art pieces and 4 original art pieces from the victim artist. Using original art pieces as an indicator of the human artist's style, we ask participants to consider the mimicked art, and rate the success of Glaze's protection on a 5-level Likert scale (ranging from "not successful at all" to "very successful"). Each mimicry attempt is evaluated by at least 10 participants. We define artist-rated PSR as the percent of participants who rated Glaze's protection as "successful" or</p><p>Generic model Artist dataset w/o Glaze w/ Glaze (p=0.05) Artist-rated PSR CLIP-based genre shift Artist-rated PSR CLIP-based genre shift SD Current 4.6 ± 0.3% 2.4 ± 0.2% 94.3 ± 0.8% 96.4 + 0.5% Historical 4.2 ± 0.2% 1.3 ± 0.2% 93.3 + 0.6% 96.0 + 0.3% DALL•E-m Current 31.9 ± 3.5% 6.4 ± 0.8% 97.4 ± 0.2% 97.4 + 0.3% Historical 29.8 ± 2.4% 5.8 ± 0.6% 96.8 ± 0.3% 97.1 + 0.2% Table <ref type="table">2</ref>. Glaze has a high protection success rate, as measured by artists and CLIP, against style mimicry attacks. We compare protection success when artists do not use Glaze vs. when they do (with perturbation budget 0.05).</p><p>"very successful." Our user studies primarily focus on artists, as they would be most affected by this technology. We found though, that not all current artists despise AI art, and some view it as a new avenue for a different form of artistry. CLIP-based genre shift: We define a new metric based on CLIP <ref type="bibr" target="#b61">[62]</ref>, using the intuition that Glaze succeeds if the mimicked art has been impacted enough by Glaze to be classified into a different art genre from the artist's original artwork. We leverage CLIP model's ability to classify art images into art genres. Given a set of mimicked art targeting an artist V , we define CLIP-based genre shift rate as the percentage of mimicked art whose top 3 predicted genres do not contain V 's original genre. A higher genre shift rate means more mimicked art belongs to a different genre from the victim artist, and thus means more successful protection.</p><p>To calculate the genre shift we use a set of 27 historical genres from WikiArt dataset and 13 digital art genres <ref type="bibr" target="#b32">[33]</ref> as the candidate output labels. In Appendix A.3, we show that a pre-trained CLIP model is able to achieve high genre classification performance. We report the average CLIP-based genre shift for all 199 victim artists across all mimicked artworks.</p><p>We use CLIP-based genre shift as a supplemental metric to evaluate Glaze because it is only able to detect style changes at the granularity of art genres. However, mimicry attacks also fail when Glaze causes the mimicked artwork quality to be very low, something that CLIP cannot measure. Measuring the quality of generated image has been a challenging and ongoing research problem in computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Glaze's Protection Performance</head><p>Style mimicry success when Glaze is not used. Mimicry attacks are very successful when the mimic has access to a victim's original (unmodified) artwork. Examples of mimicked artwork can be found in Figure <ref type="figure">8</ref>. The leftmost two columns of Figure <ref type="figure">8</ref> show a victim artist's original artwork, while the third column depicts mimicked artwork generated by a style-specific model trained on victim's original artwork when Glaze is not used. In our user study, over &gt; 95% of respondents rated the attack as successful. Table <ref type="table">2</ref>, row 1, gives the artist-rated and CLIP-based genre shift for mimicry attacks on unprotected art.</p><p>SD models produce stronger mimicry attacks than DALL•Em models, according to our user study (see Table <ref type="table">2</ref>). This is Perturbation budget Artist-rated PSR CLIP-based genre shift 0 (no cloak) 4.6 ± 1.4% 2.4 ± 0.8% 0.05 93.3 ± 0.6% 96.0 ± 0.3% 0.1 95.9 ± 0.4% 98.2 ± 0.1% 0.2 96.1 ± 0.3% 98.5 ± 0.1%   <ref type="table">2</ref>). Glaze's protection performance is slightly higher for current artists than for historical artists. This is likely because the historical artists' images are present in the training datasets of our generic models (SD, DALL•Em), highlighting the additional challenge of protecting well-known artists whose style was already learned by the generic models. How large of perturbations will artists tolerate? Increasing the Glaze perturbation budget enhances protection performance. We observe that both artist-rated and CLIP-based genre shift increase with perturbation budget (see Figure <ref type="figure">9</ref>, Table 3, and Figure <ref type="figure" target="#fig_2">20</ref>). Given this tradeoff between protection success and Glaze protection visibility on original artwork, we evaluate how perturbation size impacts artists' willingness to use Glaze.</p><p>We find that artists are willing to add fairly large Glaze perturbations to their artwork in exchange for protection against mimicry. To measure this, we show 3 randomly chosen pairs of original/cloaked artwork to each of the 1,156 artists in our first study. For each art pair, we ask the artist whether they would be willing to post the cloaked artwork (instead of the original, unmodified version) on their personal website. More than 92% of artists select "willing" or "very willing" when p = 0.05. This number only slightly increases to 94.3% when p = 0.03. Figure <ref type="figure" target="#fig_9">10</ref> details artists' preferences as perturbation budget increases. (see Figure <ref type="figure" target="#fig_10">11</ref> for examples of cloaked artwork with increasing p). Based on these results, we use perturbation budget p = 0.05 for all our experiments, since most artists are willing to tolerate this perturbation size. Surprisingly, over 32.8% artists are willing to use cloaks with p = 0.2, which is clearly visible to human eye (see Figure <ref type="figure" target="#fig_10">11</ref>). While we are surprised by this high perturbation tolerance, in our follow-up free response artists noted that they would be willing to tolerate large perturbations because of the devastating consequence if their styles are stolen. One participant stated that "I am willing to sacrifice a bit image quality for protection." Many artists (&gt; 80%) also noted that they have already used traditional, more visually disruptive techniques to protect their artwork online when posting online, i.e., adding watermark or reducing image resolution. One participant stated that "I already use low to medium resolution images only for online posting, thus this would not impact my quality control too much."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Glaze's Protection Robustness</head><p>Next, we test Glaze's efficacy in more challenging scenarios. First, we measure performance when the mimic uses a different feature extractor for mimicry than the one used by the artist to generate the cloak. Second, we measure what happens when the mimic has uncloaked artwork samples from the victim. Due to the poor mimicry performance of DALL•E-m, we focus our evaluation using SD as the generic model. Artist/mimic use different feature extractors. In the real world, it is possible that the mimic will use a different model (and thus a different image feature extractor) for style mimicry than the one used by the victim artist to cloak their artwork. While the feature extractors may still be similar because of the well-known transferability property between large models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b100">101]</ref>, their differences could reduce the efficacy of cloaking. We test this scenario using three feature extractors-Φ-A, Φ-B, and Φ-C. Φ-A and Φ-B have different model architectures (autoencoder-KL <ref type="bibr" target="#b66">[67]</ref> vs. VQ-VAE <ref type="bibr" target="#b64">[65]</ref>) Artist-rated PSR CLIP-based genre shift Current 6.2 ± 0.5% 3.8 ± 0.3% 92.5 ± 0.5% 94.2 + 0.3% Historical 7.2 ± 0.6% 3.3 ± 0.4% 92.1 + 0.3% 93.9 + 0.4% Table <ref type="table">4</ref>. Performance of Glaze against real-world mimicry service (scenario.gg). Mimicry service achieves high mimicry success when no protection is used. When Glaze is used, the mimicry service has low performance. but are both trained on the ImageNet dataset <ref type="bibr" target="#b18">[19]</ref>. Φ-A and Φ-C have different model architectures (autoencoder-KL vs VQ-VAE) and training datasets (ImageNet vs. CelebA <ref type="bibr" target="#b47">[48]</ref>).</p><p>In our experiments, the victim artist uses one feature extractor (either Φ-B or Φ-C) to optimize cloaked artwork, and the mimic trains their style-specific models with SD models using Φ-A. Despite the difference in victim/mimic extractors, Glaze's protection remains highly successful (left half of Figure <ref type="figure" target="#fig_2">12</ref>)-the style of mimicked artwork remains distinct from artist's true style. Artist-rated and CLIP-based genre shift measurements confirm this observation. Artist-rated PSR is &gt; 90.2%, while CLIP-based genre shift is &gt; 94.0%. The PSR is slightly higher when the two feature extractors only differ in architectures (Φ-B to Φ-A) than when they differ in both architecture and training data (Φ-C to Φ-A).</p><p>Mimic has access to uncloaked artwork. Another challenging scenario is when the mimic gains access to some uncloaked artwork from victim artists. This is a realistic scenario for many prominent artists with a large online presence. As expected, Glaze's protection performance decreases when the mimic has access to more uncloaked artwork (right side of Figure <ref type="figure" target="#fig_2">12</ref>). As the ratio of uncloaked/cloaked art in the mimic's dataset increases, the mimicked artwork becomes more similar to artist's original style. Yet, Glaze is still reasonably effective (87.2% artist-rated PSR) even when artists can only cloak 25% of their artwork. This validates our hypothesis in §5.4 that cloaking will have a noticeable effect as long as the mimic has some cloaked training data.</p><p>A mimic with access to a large amount of uncloaked artwork is still an issue for Glaze. Fortunately, in our user study, we found that 1) many artists constantly create and share new artwork online, which can be cloaked to offset the percentage of uncloaked artwork, and 2) many artists change their artistic style over time. In our user study, we asked artists to estimate the number of unique art pieces they currently have online (M) and the estimated number of art pieces they anticipate uploading each subsequent year (Y ). Among artists with an existing online presence, over 40% have Y /M &gt; 25%, meaning that one year from now, &gt; 20% of their total online artwork would be cloaked (if they start using Glaze immediately). More than 81% of artists also stated that their art style has changed over their career, and half of these said that theft of their old, outdated styles is less concerning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Real-World Performance</head><p>Next, we test Glaze against a real-world style mimicry-asa-service system, scenario.gg <ref type="bibr" target="#b76">[77]</ref>. Scenario.gg is a web service that allows users to upload a set of images in a specific style. The service then trains a model to mimic the style and returns an API endpoint that allows the user to generate mimicked images in the trained style. The type of model or mimicry method used by the service is unknown.</p><p>Glaze remains effective against scenario.gg. We ask scenario.gg to mimic the style from a set of cloaked or uncloaked artwork from 4 current artists and 19 historical artists. Table <ref type="table">4</ref> shows that when no protection is used, scenario.gg can successfully mimic the victim style (&lt; 7.2% protection success). The mimicry success of scenario.gg is lower than our mimicry technique, likely because scenario.gg trains the model for fewer iterations due to computational constraints. When we use Glaze to cloak the artwork and upload the cloaked artwork, scenario.gg fails to mimic the victim style (&gt; 92.1% artist-rated PSR and &gt; 93.9% CLIPbased genre shift rate) as shown in Table <ref type="table">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Countermeasures</head><p>We consider potential countermeasures a mimic could employ to reduce the effectiveness of Glaze. We consider the strongest adversarial setting, in which the mimic has white-box access to our protection system, i.e., access to the feature extractor used and protection algorithm. In our experiments, we assume the mimic uses the SD model as the generic model and test the efficacy of each countermeasure on the 13 victim artists from §6.2. Here, we focus on artist-rated PSR metric, because many countermeasures trade off image quality for mimicry efficacy, and CLIP-based metric does not consider image quality. Image transformation. A popular approach to mitigate the impact of small image perturbations, like those introduced by Glaze, is to transform training images before using them for model training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. In our setting, the mimic could augment the cloaked artwork before fine-tuning their model on them to potentially reduce cloak efficacy. We first test Glaze's resistance to two popular image transformations, adding Gaussian noise and image compression. We also consider a stronger version of this countermeasure that then tries to correct the image quality degradation introduced by the transformations.</p><p>Transforming cloaked artwork does not defeat Glaze's protection. Figure <ref type="figure" target="#fig_3">13</ref> shows that as the magnitude of Gaussian noise (σ) increases, the quality of mimicked artwork decreases as fast as or faster than cloak effectiveness. This is because models trained on noisy images learn to generate noisy images. We observe a similar outcome when mimic uses JPEG compression (Figure <ref type="figure" target="#fig_13">14</ref>), where image resolution and quality degrade due to heavy compression. Artists-rated PSR decreases slightly but remains above &gt; 87.4% across both types of data transformations. Artists consider Glaze's protection to be successful when mimicked artwork is of poor quality.</p><p>The mimic can take this countermeasure one step further by reversing the quality degradation introduced by the noising/compression process. Specifically, a mimic can run image denoising or image upscaling tools on the mimicked artwork (e.g., ones shown in Figure <ref type="figure" target="#fig_3">13</ref> and <ref type="figure" target="#fig_13">14</ref>) to increase their quality. We found this approach improves generated image quality but still does not allow for successful mimicry. For denoising, we ran a state-of-the-art CNN-based image denoiser <ref type="bibr" target="#b103">[104]</ref> that is specifically trained to remove "additive Gaussian noise" (the same type of noise added to cloaked artwork). The last column of Figure <ref type="figure" target="#fig_3">13</ref> shows the denoised image (using the noisy mimicked image when σ = 0.2 as the input). While the process removes significant amounts of noise, the denoised artwork still has many artifacts, especially around complex areas of the artwork (e.g., human face). We observe similar results for image upscaling, where we use a diffusion-based image upscaler <ref type="bibr" target="#b82">[83]</ref> to improve the quality of compressed images (Figure <ref type="figure" target="#fig_13">14</ref>). Overall, our artist-rated protection success rate remains &gt; 85.3% against this improved countermeasure. Radiya et al. <ref type="bibr" target="#b63">[64]</ref> robust training. Radiya et al. <ref type="bibr" target="#b63">[64]</ref> design a robust training method to defeat cloaking tools like Fawkes <ref type="bibr" target="#b80">[81]</ref> and Lowkey <ref type="bibr" target="#b12">[13]</ref> in the face recognition setting. At a high level, this method augments the attacker's training dataset with some cloaked images generated by the cloaking tool and the correct output labels. Training on such data makes the model more robust against cloak perturbations on unseen cloaked images at inference time, and thus, can potentially circumvent the protection.</p><p>We test if this robust training approach can defeat Glaze. We assume the mimic first robustly trains the feature extractors in their generic models using cloaked artwork generated by Glaze, and then trains the generator model to generate images from the robust feature space. Finally, the mimic uses the robust generic model for style mimicry as in §6. We discuss the detailed robust training setup in Appendix A. <ref type="bibr" target="#b3">4</ref>.</p><p>Glaze performance remains high, even if the mimic robustly trains the generic model for many iterations before using it for style mimicry (see Figure <ref type="figure" target="#fig_14">15</ref>). As the model becomes more robust, the mimicked artwork is less impacted by cloaking (less influenced of the target style). However, robust training greatly degrades mimicked image quality, preventing successful mimicry. Overall, the artist-rated PSR remains &gt; 88.7%. To mitigate robust training's impact on image quality, we explore an alternative robust training method, where we robustly train a new feature extractor designed to remove cloak's impact while operating in the original feature space (thus no need to change the image generator). We found this robust training approach is also ineffective (details in §A.4).</p><p>As discussed in §5.4, Glaze remains reasonably effective against Radiya et al. because 1) the continuous output space of the generative model, and 2) high quality requirement of art generation. Robust training reduces cloaking's effectiveness but cannot completely remove its impact. In the classification case (facial recognition), this reduced effectiveness only manifests in small changes in classification confidence (compared to no cloaking) and often does not change the discrete classification outcome. However, in the context of generator models, the continuous output space means that even less-effective cloaks still directly affect the mimicked artwork. Combined with the high quality requirement, the reduced protection effect is enough to disrupt style mimicry, as shown in Figure <ref type="figure" target="#fig_14">15</ref>. Additional robust training simply degrades generation quality, rather than reducing cloaking efficacy. Outlier Detection. Another countermeasure could involve leveraging outlier detection to identify and remove protected images <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91]</ref>. We test Glaze's robustness to a stateof-the-art outlier detection method that leverages contrastive training <ref type="bibr" target="#b90">[91]</ref>. Contrastively trained models project data into a well-separated feature space, which the mimic could leverage.</p><p>We assume the mimic has a ground truth set (20) of original artworks from a given artist. The mimic first projects these art pieces into the feature space of a model trained with contrastive loss on ImageNet dataset <ref type="bibr" target="#b90">[91]</ref>. The mimic then trains a one-class SVM outlier detector <ref type="bibr" target="#b43">[44]</ref> using these ground truth features. Now, given a new artwork from the same artists, the mimic detects whether the artwork is an outlier using the detector. Detection results on 4 current artists ( §6) show that outlier detection has limited effectiveness against Glaze (&lt; 65% precision and &lt; 53% recall at detecting Glaze pro-tected images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations and Releasing Glaze</head><p>We conclude with a discussion of the limitations of the current system, then describe our experiences during and after the Glaze release. Limitations. First, protection from Glaze relies on artists cloaking a portion of their art in the mimic model's training dataset. This is challenging for established artists because 1) their styles have matured over the years and are more stable, and 2) many of their art pieces have already been downloaded from art repositories like ArtStation and DeviantArt. These artists' styles can be mimicked using only older artworks collected before the release of Glaze. While artists can prevent mimics from training on newer artwork, they need to rely on opt-out and removal options at art repositories to stop style mimicry.</p><p>Second, a system like Glaze that protects artists faces an inherent challenge of being future-proof. Any technique we use to cloak artworks today might be overcome by a future countermeasure, possibly rendering previously protected art vulnerable. While we are under no illusion that Glaze will remain future-proof in the long run, we believe it is an important and necessary first step towards artist-centric protection tools to resist invasive AI mimicry. We hope that Glaze and followup projects will provide some protection to artists while longer term (legal, regulatory) efforts take hold. Releasing Glaze and managing expectations. We released Glaze as a free application on Mac and Windows in March 2023. We have repeatedly communicated Glaze's limitations to users, both on our website and in communications to artists via our download page, on Twitter, in emails to artists, etc. In these communications, we clearly state that Glaze is not a permanent solution against AI mimicry and could potentially be defeated by future attacks.</p><p>As of June 2023, Glaze has been downloaded &gt; 740K times by artists around the world. Reception on social media and emails to our lab have been extremely enthusiastic and positive. Artists have helped design Glaze's user interface, made how-to videos on YouTube, and managed ad campaigns on Instagram to spur adoption in the community. Based on numerous requests on social media and via emails, we plan to test and deploy a web service in Summer 2023 to expand Glaze access to artists who lack compute and GPUs.</p><p>One excellent outcome from the Glaze release has been the technical discussions it has spurred with a variety of stakeholders. We began and are continuing collaborative efforts to advocate for artists rights, with art-centric social networks, advocate groups in the US (CAA) and the EU (EGAIR), government representatives, and companies who want to protect the IP of their images/characters. Real-world countermeasures. Finally, we want to describe our experiences deploying Glaze in an adversarial setting. In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glazed art</head><p>Plagiarized art by PEZ the 3 months since initial release, multiple groups have sought to attack or bypass Glaze protection. While several attempts had minimal impact, we describe the two most serious attempts here and evaluate their effectiveness. The first attack <ref type="bibr" target="#b50">[51]</ref> leverages a newer style mimicry method <ref type="bibr" target="#b93">[94]</ref>, reverse engineering with PEZ. PEZ is able to perform high-quality style mimicry using a single original image from the original artist. Initial tests showed Glaze is robust against PEZ style mimicry (Figure <ref type="figure" target="#fig_0">16</ref>). Glaze remains effective likely because Glaze directly modifies the feature representation of the art, and is thus effective against stronger mimicry attempts.</p><p>A second category of attacks tries to perform pixel-level image smoothing to remove cloaks added by Glaze <ref type="bibr" target="#b104">[105]</ref>. This applies bilateral filters on Glazed images repeatedly, seeking to remove all added perturbations. We evaluate this attack on Glazed artwork in §6 and fine-tuning a model on the smoothed images. Figure <ref type="figure" target="#fig_0">17</ref> shows Glaze remains effective against pixel smoothing. This result is consistent with prior work showing that image smoothing cannot prevent adversarial perturbations <ref type="bibr" target="#b101">[102]</ref>.</p><p>Finally, while we have not yet observed any successful attacks against Glaze, we are continuously exploring design improvements to further enhance robustness against potential future countermeasures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Adapting Existing Cloaking Systems</head><p>Here, we consider whether prior image cloaking systems can be adapted to provide protection against art style mimicry. Our results show adapting existing cloaking systems has limited effectiveness for our goals. Adapting existing cloaking systems.</p><p>Fawkes <ref type="bibr" target="#b80">[81]</ref> generates a cloak on user face images by optimizing the feature space difference between the cloaked image and a target image. The target image is simply a face image of a different person. We adapt Fawkes to anti-mimicry protection by switching the feature extractor from facial recognition to the same one we use for Glaze. For the target image used, we assume Fawkes randomly picks an artwork from a different artist. Fawkes uses DSSIM to bound the input perturbation. For a fair comparison, we change Fawkes perturbation from DSSIM to LPIPS, ones used by Glaze.</p><p>The general design of Lowkey <ref type="bibr" target="#b12">[13]</ref> is similar to Fawkes, except Lowkey does not optimize cloak images towards a target in feature space but simply optimizes cloaked images to be different from the original one. We directly apply LowKey for anti-mimicry protection: Lowkey maximizes the cloaked artwork to have a different feature representation from the original artwork.</p><p>Photoguard <ref type="bibr" target="#b75">[76]</ref> works by minimizing the norm of the image feature vector. It is equivalent to Fawkes when Fawkes selects the zero feature vector as the target for optimization. For anti-mimicry, we adapted Photoguard to minimize the norm of feature representation of the cloaked artwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance comparison.</head><p>Figure <ref type="figure" target="#fig_2">22</ref> show Fawkes, Lowkey, and Photoguard have limited effectiveness at protection against mimicry. Out of the three existing systems, Fawkes achieves the best performance with 41.0% artist-rated protection success rate. While we can see small artifacts introduced by Fawkes and Lowkey, they are not sufficient to prevent mimicry. In our tests, we use the same LPIPS perturbation level and the same feature extractor for optimization for all cloaking systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional Information on Style Mimicry</head><p>Impact of fine-tuning on mimicry success.</p><p>Figure <ref type="figure" target="#fig_2">21</ref> compares the mimicry performance when a mimic attack fine-tunes on the victim artist's artwork or directly using a generic model. For artists who are not household names (e.g. iconic artists like Van Gogh), fine-tuning significantly improves mimicry performance. We generate images using text captions containing the artist's name, e.g., "a river by Nathan Fowkes." Details on training parameters. For stable diffusion, we follow the same training parameters as the original paper <ref type="bibr" target="#b66">[67]</ref>. We use 5 • 10 -6 learning rate and batch size of 32. For a generation, we follow the default setting using the PNDM sampler and 50 sampling steps. For DALL•E-m, we also follow the same training setup as <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 2 • 10 -5 and batch size 32. To generate images, we use the default setting with a condition scale equal to 10. Impact of selecting random seed.</p><p>For diffusion-based models (e.g., SD), artwork generation is controlled by a random seed (random noise input at the beginning). Different random seeds lead to very different images, and thus it is common practice for mimics to generate a set of artwork using different seed and select the best artwork. A relevant question is, can a mimic use sheer randomness to generate a plagiarized artwork that succeeds despite Glaze protection.</p><p>We investigate the impact of random seed selection on mimicry success in the presence of Glaze. Given a stylespecific model and a given text prompt, the mimic generates 100 plagiarized artworks using different random seeds. Similar to how we calculate CLIP-based genre shift, we then use the CLIP model to identify any artwork that belongs to the same genre as the target artist's style. The results show that 4.3% of the time, the mimic is able to find at least 1 out of the 100 plagiarized artwork that passed CLIP filtering. While the filtered artwork does belong to the same genre as the artist, we found they tend to have lower image quality. We verify this observation in our user study, and &gt; 94.1% human artists rated the protection remains successful (i.e. these art pieces failed to mimick the art style). We believe the reason that some plagiarized artwork still shares the same genre as victim style after protection, is that text-to-image models today are still imperfect and often output poor-quality images in rare cases with some random seed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Sample AI-generated art pieces from the Midjourney community showcase [53, 69].</figDesc><graphic coords="1,412.35,264.62,61.47,61.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Real-world incident of AI plagiarizing the style of artist Hollie Mengert [3]. Left: original artwork by Hollie Mengert. Right: plagiarized artwork generated by a model trained to mimic Hollie's style.</figDesc><graphic coords="3,58.76,72.00,107.73,107.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1Figure 3 .</head><label>3</label><figDesc>Figure 3. High level model architecture of text-to-image models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. High level overview of the mimicry attack scenario. The mimic scrapes copyrighted artwork from the victim artist and uses these to fine-tune a pre-trained, generic text-to-image model. The generic model is trained and open-sourced by an AI company. The mimic then uses the fine-tuned model to generate artwork in the style of the victim artist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Overview of Glaze, a system that protects victim artists from AI style mimicry by cloaking their online artwork. (Top) An artist V applies the cloaking algorithm (uses a feature extractor Φ and a target style T ) to generate cloaked versions of V 's art pieces. Each cloak is a small perturbation unnoticeable to human eye. (Bottom) A mimic scrapes the cloaked art pieces from online and uses them to fine-tune a model to mimic V 's style. When prompted to generate artwork in the style of V , mimic's model will generate artwork in the target style T , rather than V 's true style.</figDesc><graphic coords="6,317.88,319.69,55.99,56.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. High level overview of how Glaze perturbs the style-specific features of the artwork. a) Glaze style transfers the original artwork to a different style, which changes its style but leaves other features unaltered. b) Glaze optimizes a cloak that makes the artwork's features representation match that of the style-transferred art, while constraining the amount of visible changes to the artwork.Original artwork"Oil painting"Style transferred artwork with target:"Abstract painting" "Cubism painting"</figDesc><graphic coords="6,440.55,319.69,56.02,56.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Example Glaze protection results for three artists. Columns 1-2: artist's original artwork; column 3: mimicked artwork when artist does not use protection; column 4: style-transferred artwork (original artwork in column 1 is the source) used for cloak optimization and the name of target style; column 5-6: mimicked artwork when artist uses cloaking protection with perturbation budget p = 0.05 or p = 0.1 respectively. All mimicry examples here use SD-based models.</figDesc><graphic coords="10,350.41,207.78,55.71,55.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Artists' willingness to post cloaked artwork in place of the original decreases as perturbation budget of the cloaks increases.</figDesc><graphic coords="10,135.82,207.78,55.71,55.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Original artwork and cloaked artwork computed using three different cloak perturbation budgets.</figDesc><graphic coords="11,64.76,138.59,53.30,53.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Glaze remains successful under two challenging scenarios. Left: when artist and mimic use different feature extractors. Right: when artists can only cloak a portion of their artwork in mimic's dataset. Bottom of the figure shows artist-rated PSR and CLIP-based genre shift for the corresponding setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Glaze's protection performance remains high as mimic adds JPEG compression to the cloaked artwork. Even when the mimic also upscales the mimicked images (last column), Glaze's protection persists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Glaze's protection performance remains high against robust training countermeasure proposed by Radiya et al. . The protection performance first decreases then increases as mimic robustly trains the model with an increasing number of steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 .Figure 17 .</head><label>1617</label><figDesc>Figure 16. Glazed image and generated image from PEZ mimicry method. The original image is Musa Victoriosa, a new painting created by Karla Ortiz to be the first artwork to be released publicly under Glaze protection.</figDesc><graphic coords="14,472.24,288.37,70.40,70.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance of our system (artist-rated protection success rate and CLIP-based genre shift rate) increases as the perturbation budget increases. (SD model, averaged over all victim artists).</figDesc><table><row><cell cols="5">Willingness to post cloaked artwork</cell></row><row><cell>p=0.03</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p=0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p=0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p=0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80% 100%</cell></row><row><cell cols="3">very unwilling</cell><cell cols="2">somewhat unwilling</cell></row><row><cell cols="2">neutral</cell><cell></cell><cell cols="2">somewhat willing</cell></row><row><cell cols="2">very willing</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>One artist may paint in multiple styles, resulting in multiple candidate target styles from a single artist.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank our anonymous reviewers and shepherd for their insightful feedback. We also thank <rs type="person">Karla Ortiz</rs>, <rs type="person">Lyndsey Gallant</rs>, <rs type="person">Nathan Fowkes</rs>, <rs type="person">Kim Van Deun</rs>, <rs type="person">Jon Lam</rs>, <rs type="person">Eveline Fröhlich</rs>, <rs type="person">Edit Ballai</rs>, <rs type="person">Kat Loveland</rs> and many other artists, without whom this project would not be possible. This work is supported in part by <rs type="funder">NSF</rs> grants <rs type="grantNumber">CNS-2241303</rs>, <rs type="grantNumber">CNS-1949650</rs>, and the <rs type="funder">DARPA</rs> <rs type="programName">GARD program</rs>. Opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Tfptaan">
					<idno type="grant-number">CNS-2241303</idno>
				</org>
				<org type="funding" xml:id="_sXxAwK4">
					<idno type="grant-number">CNS-1949650</idno>
				</org>
				<org type="funding" xml:id="_NKtuGf4">
					<orgName type="program" subtype="full">GARD program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 CLIP-based metric</head><p>We test CLIP's performance in classifying artwork into the correct art genre. We take 27 historical genres from WikiArt and 13 digital art genres <ref type="bibr" target="#b32">[33]</ref> as the candidate labels. We collect a test dataset consisting of 1000 artwork from WikiArt dataset, each containing the ground truth labels from the Wikiart dataset. Then we collect 100 artwork for each of the 13 digital art genres by searching the name of the genre on ArtStation, one of the largest digital art-sharing platforms. We evaluate CLIP performance using top-3 accuracy as many art genres are similar to each other (e.g., impressionism vs fauvism). CLIP achieves 96.4% top-3 accuracy on artwork from WikiArt and 94.2% for artwork from ArtStation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Countermeasures</head><p>Details on robust training. Here, we give details on the robust training method we used. We follow prior work <ref type="bibr" target="#b73">[74]</ref> on robust training of autoencoder models. The mimic first uses Glaze to generate a large number of cloaked artwork using artwork from WikiArt dataset. Given the feature extractor Φ used by mimic's text-to-image model, the mimic trains Φ to minimize the following loss function:</p><p>where x cloaked and x org is a pair of cloaked and original artworks. This optimization effectively forces Φ to extract the same feature representation for cloaked and original artwork. To prevent the extractor from collapsing (e.g., output zero vectors for all inputs), we regularized the training with the standard VAE reconstruction loss and train the decoder D at the same time. Given the high discrepancy between features of cloaked and original artwork, this training process significantly modifies the internals of Φ as well as the feature space. Thus, the mimic needs to fine-tune the decoder D and generator G on the new robust feature space. We assume the mimic trains Φ for K steps on K different pairs of cloaked/original artwork, and then fine-tunes D and G until convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original artwork</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mimicked art when GLAZE not used</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mimicked art when GLAZE is used</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">AI Render -Stable Diffusion in Blender</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Render</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Alt-Right Manipulated My Comic</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Then A.I. Claimed It. The New York Times</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Invasive Diffusion: How one unwilling illustrator found herself turned into an AI model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ediffi: Text-to-image diffusion models with an ensemble of expert denoisers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01324</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AI-created images lose U.S. copyrights in test for new technology</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brittain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reuters</title>
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISec</title>
		<meeting>of AISec</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE S&amp;P</title>
		<meeting>of IEEE S&amp;P</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face-off: Adversarial face obfuscation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PETS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="369" to="390" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Re-imagen: Retrieval-augmented text-to-image generator</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14491</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hundreds of animation guild members join WGA writers on picket line for first time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
			<publisher>ABC7 News</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lowkey: Leveraging adversarial attacks to protect social media users from facial recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cherepanova</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Civitai</surname></persName>
		</author>
		<ptr target="https://civitai.com" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">When AI can make art -what does it mean for creativity? The Guardian</title>
		<author>
			<persName><forename type="first">L</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-11">Nov 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Dayma</surname></persName>
		</author>
		<author>
			<persName><surname>Mega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training Journal</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Dayma</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">Dalle mini, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security</title>
		<meeting>of USENIX Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meet the three artists behind a landmark lawsuit against AI art generators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dixit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BuzzFeedNews</title>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AI art and the problem of consent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dryhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArtReview</title>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Artists stage mass protest against AI-generated artwork on artstation. Ars Technica</title>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Does ArtStation become PromptStation? Data Economy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eliaçik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-01">Jan. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Foggysight: A scheme for facial lookup privacy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturmfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PETS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Curtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00410</idno>
		<title level="m">Detecting adversarial samples from artifacts</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What does the rise of AI mean for the future of art? Sydney Morning Herald</title>
		<author>
			<persName><forename type="first">E</forename><surname>Flux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">AI art &amp; the ethical concerns of artists. Beautiful Bizarre</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An image is worth one word: Personalizing text-toimage generation using textual inversion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01618</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial attacks on variational autoencoders</title>
		<author>
			<persName><forename type="first">Gondim-Ribeiro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04646</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Midjourney founder admits to using a &apos;hundred million&apos; images without consent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Growcoot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-12">Dec 2022</date>
			<publisher>PetaPixel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mind-Boggling Midjourney Statistics in 2022. Tokenized</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heidorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">This artist is dominating AI-generated art. and he&apos;s not happy about it</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<date type="published" when="2022-09">Sept 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Digital Illustration Styles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Illustrators</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Midjourney v4: an incredible new version of the AI image generator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ivanenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mezha</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Class Action Filed Against Stability AI, Midjourney, and DeviantArt for DMCA Violations</title>
		<author>
			<persName><forename type="first">Joseph Saveri Law Firm</forename><surname>Llp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Right of Publicity Violations, Unlawful Competition, Breach of TOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagic: Text-based real image editing with diffusion models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient near-duplicate detection and sub-image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MM</title>
		<meeting>of MM</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial examples for generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPW</title>
		<meeting>of SPW</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<title level="m">Adversarial examples in the physical world</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06026</idno>
		<title level="m">The role of imagenet classes in fréchet inception distance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Perceptual adversarial robustness: Defense against unseen threat models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12655</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A New Stable Diffusion Plug</title>
		<author>
			<persName><forename type="first">G</forename><surname>Levine</surname></persName>
		</author>
		<editor>For GIMP &amp; Krita. 80.lv</editor>
		<imprint>
			<date type="published" when="2022-09">Sept. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving one-class SVM for anomaly detection</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CAT</title>
		<meeting>of IEEE CAT</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scaling language-image pre-training via masking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00794</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The world&apos;s smartest artificial intelligence just made its first magazine cover. Cosmopolitan</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">VC-GPT: Visual conditioned GPT for end-to-end generative vision-and-language pretraining</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12723</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Generating images from captions with attention</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02793</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Marx</surname></persName>
		</author>
		<ptr target="https://twitter.com/DigThatData/status/1636386617392009224" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">On distillation of guided diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03142</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Community Showcase</title>
		<author>
			<persName><surname>Midjourney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Is Lensa AI Stealing From Human Art? An Expert Explains The Controversy</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ScienceAlert</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">AI is causing student artists to rethink their creative career plans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">KQED Arts</title>
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Numerical optimization, series in operations research and financial engineering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">NovelAI changelog</title>
		<author>
			<persName><surname>Novelai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: From phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><surname>Art</surname></persName>
		</author>
		<title level="m">Generator Cupixel Rakes in 5M From Craft Store JOANN. Built in Boston</title>
		<imprint>
			<date type="published" when="2022-08">August 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">AI creating &apos;art&apos; is an ethical and copyright nightmare</title>
		<author>
			<persName><forename type="first">L</forename><surname>Plunkett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-08">August 2022</date>
			<publisher>Kotaku</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">He Used AI to Publish a Children&apos;s Book in a Weekend</title>
		<author>
			<persName><forename type="first">N</forename><surname>Popli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artists Are Not Happy About It. Time</title>
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Data poisoning won&apos;t save you from facial recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Radiya-Dixit</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14851</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Augmented lagrangian adversarial attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generated Picture Won an Art Prize. Artists Aren&apos;t Happy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Roose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2022-09">Sept. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno>arxiv:2208.12242</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05122</idno>
		<title level="m">Adversarial manipulation of deep representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Palette: Image-to-image diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Large-scale classification of fine-art paintings: Learning the right metric on the right feature</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00855</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Arae: Adversarially robust training of autoencoders improves novelty detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="726" to="736" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">AI is coming for commercial art jobs. can it be stopped? Forbes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salkowitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-09">Sept 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Raising the cost of malicious AI-powered image editing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khaddaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06588</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">AI-generated game assets</title>
		<author>
			<persName><surname>Scenario</surname></persName>
		</author>
		<author>
			<persName><surname>Gg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08402</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Post-breach recovery: Protection against white-box adversarial examples for leaked dnn models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM CCS</title>
		<meeting>of ACM CCS</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Gotta catch&apos;em all: Using honeypots to catch adversarial attacks on neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM CCS</title>
		<meeting>of ACM CCS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fawkes: Protecting privacy against unauthorized deep learning models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security</title>
		<meeting>of USENIX Security</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Stable Diffusion v2.1 and DreamStudio Updates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Stability</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">When does machine learning fail? generalized transferability for evasion and poisoning attacks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security</title>
		<meeting>of USENIX Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00155</idno>
		<title level="m">Adversarial images for variational autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">DF-GAN: A simple and effective baseline for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Image Apps Like Lensa AI Are Sweeping the Internet, and Stealing From Artists</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yahoo News</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Deep feature interpolation for image content changes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Neural cleanse: Identifying and mitigating backdoor attacks in neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE S&amp;P</title>
		<meeting>of IEEE S&amp;P</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Understanding the behaviour of contrastive loss</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">ArtStation is hiding images protesting AI art on the platform. The Verge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weatherbed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">People Have Raised Serious Concerns About The AI Art App That&apos;s All Over Your Instagram Feed</title>
		<author>
			<persName><forename type="first">K</forename><surname>Weekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BuzzFeed News</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03668</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Antifacial recognition technology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Sok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE S&amp;P</title>
		<meeting>of IEEE S&amp;P</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Sampling generative networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04468</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Scenario lands $6M for its AI platform that generates game art assets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wiggers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">TechCrunch</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Stability AI, the startup behind Stable Diffusion, raises 101M</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wiggers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">TechCrunch</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Why Artists are Fed Up with AI Art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Fayden Art</title>
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Smooth adversarial examples</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Adversecleaner</surname></persName>
		</author>
		<ptr target="https://github.com/lllyasviel/AdverseCleaner" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Et Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
