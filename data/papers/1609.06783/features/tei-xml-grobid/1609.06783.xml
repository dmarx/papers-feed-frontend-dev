<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes</title>
				<funder>
					<orgName type="full">Department of Communications</orgName>
				</funder>
				<funder ref="#_HFmG3TQ">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-09-22">22 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wai</forename><surname>Kar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University Data61/NICTA</orgName>
								<address>
									<country>Australia Wray Buntine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University Data61/NICTA</orgName>
								<address>
									<country>Australia Wray Buntine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
							<email>cchangyou@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lan</forename><surname>Du</surname></persName>
							<email>lan.du@monash.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Duke University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Lijoi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonietta</forename><surname>Mira</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-22">22 Sep 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">9D5C29C38FFA9AA8DAAF0ADCA85761FC</idno>
					<idno type="DOI">10.1016/j.ijar.2016.07.007</idno>
					<idno type="arXiv">arXiv:1609.06783v1[stat.ML]</idno>
					<note type="submission">Submitted 31 Jan 2016;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-28T01:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian nonparametric methods</term>
					<term>Markov chain Monte Carlo</term>
					<term>topic models</term>
					<term>hierarchical Pitman-Yor processes</term>
					<term>Twitter network modelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We live in the information age. With the Internet, information can be obtained easily and almost instantly. This has changed the dynamic of information acquisition, for example, we can now (1) attain knowledge by visiting digital libraries, (2) be aware of the world by reading news online, (3) seek opinions from social media, and (4) engage in political debates via web forums. As technology advances, more information is created, to a point where it is infeasible for a person to digest all the available content. To illustrate, in the context of a healthcare database (PubMed), the number of entries has seen a growth rate of approximately 3,000 new entries per day in the ten-year period from 2003 to 2013 <ref type="bibr" target="#b76">(Suominen et al., 2014)</ref>. This motivates the use of machines to automatically organise, filter, summarise, and analyse the available data for the users. To this end, researchers have developed various methods, which can be broadly categorised into computer vision <ref type="bibr" target="#b53">(Low, 1991;</ref><ref type="bibr" target="#b56">Mai, 2010)</ref>, speech recognition <ref type="bibr" target="#b70">(Rabiner and Juang, 1993;</ref><ref type="bibr" target="#b35">Jelinek, 1997)</ref>, and natural language processing (NLP, <ref type="bibr" target="#b58">Manning and Schütze, 1999;</ref><ref type="bibr" target="#b37">Jurafsky and Martin, 2000)</ref>. This article focuses on text analysis within NLP.</p><p>In text analytics, researchers seek to accomplish various goals, including sentiment analysis or opinion mining <ref type="bibr">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b49">Liu, 2012)</ref>, information retrieval <ref type="bibr" target="#b57">(Manning et al., 2008)</ref>, text summarisation <ref type="bibr" target="#b51">(Lloret and Palomar, 2012)</ref>, and topic modelling <ref type="bibr" target="#b4">(Blei, 2012)</ref>. To illustrate, sentiment analysis can be used to extract digestible summaries or reviews on products and services, which can be valuable to consumers. On the other hand, topic models attempt to discover abstract topics that are present in a collection of text documents.</p><p>Topic models were inspired by latent semantic indexing (LSI, <ref type="bibr" target="#b43">Landauer et al., 2007</ref>) and its probabilistic variant, probabilistic latent semantic indexing (pLSI), also known as the probabilistic latent semantic analysis <ref type="bibr">(pLSA, Hofmann, 1999)</ref>. Pioneered by <ref type="bibr">Blei et al. (2003)</ref>, latent Dirichlet allocation (LDA) is a fully Bayesian extension of pLSI, and can be considered the simplest Bayesian topic model. The LDA is then extended to many different types of topic models. Some of them are designed for specific applications <ref type="bibr" target="#b87">(Wei and Croft, 2006;</ref><ref type="bibr" target="#b62">Mei et al., 2007)</ref>, some of them model the structure in the text <ref type="bibr" target="#b7">(Blei and Lafferty, 2006;</ref><ref type="bibr" target="#b16">Du, 2012)</ref>, while some incorporate extra information in their modelling <ref type="bibr" target="#b71">(Ramage et al., 2009;</ref><ref type="bibr" target="#b36">Jin et al., 2011)</ref>.</p><p>On the other hand, due to the well known correspondence between the Gamma-Poisson family of distributions and the Dirichlet-multinomial family, Gamma-Poisson factor models <ref type="bibr" target="#b13">(Canny, 2004)</ref> and their nonparametric extensions, and other Poisson-based variants of non-negative matrix factorisation (NMF) form a methodological continuum with topic models. These NMF methods are often applied to text, however, we do not consider these methods here. This article will concentrate on topic models that take into account additional information. This information can be auxiliary data (or metadata) that accompany the text, such as keywords (or tags), dates, authors, and sources; or external resources like word lexicons. For example, on Twitter, a popular social media platform, its messages, known as tweets, are often associated with several metadata like location, time published, and the user who has written the tweet. This information is often utilised, for instance, <ref type="bibr" target="#b41">Kinsella et al. (2011)</ref> model tweets with location data, while <ref type="bibr" target="#b86">Wang et al. (2011b)</ref> use hashtags for sentiment classification on tweets. On the other hand, many topic models have been designed to perform bibliographic analysis by using auxiliary information. Most notable of these is the author-topic model <ref type="bibr">(ATM, Rosen-Zvi et al., 2004)</ref>, which, as its name suggests, incorporates authorship information. In addition to authorship, the Citation Author Topic model <ref type="bibr" target="#b81">(Tu et al., 2010)</ref> and the Author Cite Topic Model <ref type="bibr" target="#b39">(Kataria et al., 2011</ref>) make use of citations to model research publications. There are also topic models that employ external resources to improve modelling. For instance, <ref type="bibr" target="#b30">He (2012)</ref> and <ref type="bibr" target="#b46">Lim and Buntine (2014)</ref> incorporate a sentiment lexicon as prior information for a weakly supervised sentiment analysis.</p><p>Independent to the use of auxiliary data, recent advances in nonparametric Bayesian methods have produced topic models that utilise nonparametric Bayesian priors. The simplest examples replace Dirichlet distributions by the Dirichlet process (DP, <ref type="bibr" target="#b22">Ferguson, 1973)</ref>. The simplest is hierarchical Dirichlet process LDA (HDP-LDA) proposed by <ref type="bibr" target="#b79">Teh et al. (2006)</ref> that replaces just the document by topic matrix in LDA. One can further extend topic models by using the Pitman-Yor process (PYP, <ref type="bibr" target="#b34">Ishwaran and James, 2001</ref>) that gen-eralises the DP, by replacing the second Dirichlet distribution which generates the topic by word matrix in LDA. This includes the work of <ref type="bibr" target="#b74">Sato and Nakagawa (2010)</ref>, <ref type="bibr">Du et al. (2012b)</ref>, <ref type="bibr" target="#b48">Lindsey et al. (2012)</ref>, among others. Like the HDP, the PYPs can be stacked to form hierarchical Pitman-Yor processes (HPYP), which are used in more complex models. Another fully nonparametric extension to topic modelling uses the Indian buffet process <ref type="bibr" target="#b1">(Archambeau et al., 2015)</ref> to sparsify both the document by topic matrix and the topic by word matrix in LDA.</p><p>Advantages of employing nonparametric Bayesian methods with topic models is the ability to estimate the topic and word priors and to infer the number of clusters 1 from the data. Using the PYP also allows the modelling of the power-law property exhibited by natural languages <ref type="bibr" target="#b25">(Goldwater et al., 2005)</ref>. These touted advantages have been shown to yield significant improvements in performance <ref type="bibr" target="#b12">(Buntine and Mishra, 2014)</ref>. However, we note the best known approach for learning with hierarchical Dirichlet (or <ref type="bibr">Pitman-Yor)</ref> processes is to use the Chinese restaurant franchise <ref type="bibr" target="#b78">(Teh and Jordan, 2010)</ref>. Because this requires dynamic memory allocation to implement the hierarchy, there has been extensive research in attempting to efficiently implement just the HDP-LDA extension to LDA mostly based around variational methods <ref type="bibr" target="#b80">(Teh et al., 2008;</ref><ref type="bibr">Wang et al., 2011a;</ref><ref type="bibr" target="#b10">Bryant and Sudderth, 2012;</ref><ref type="bibr" target="#b73">Sato et al., 2012;</ref><ref type="bibr" target="#b32">Hoffman et al., 2013)</ref>. Variational methods have rarely been applied to more complex topic models, as we consider here, and unfortunately Bayesian nonparametric methods are gaining a reputation of being difficult to use. A newer collapsed and blocked Gibbs sampler <ref type="bibr" target="#b14">(Chen et al., 2011)</ref> has been shown to generally outperform the variational methods as well as the original Chinese restaurant franchise in both computational time and space and in some standard performance metrics <ref type="bibr" target="#b12">(Buntine and Mishra, 2014)</ref>. Moreover, the technique does appear suitable for more complex topic models, as we consider here.</p><p>This article, 2 extending the algorithm of <ref type="bibr" target="#b14">Chen et al. (2011)</ref>, shows how to develop fully nonparametric and relatively efficient Bayesian topic models that incorporate auxiliary information, with a goal to produce more accurate models that work well in tackling several applications. As a by-product, we wish to encourage the use of state-of-the-art Bayesian techniques, and also to incorporate auxiliary information, in modelling.</p><p>The remainder of this article is as follows. We first provide a brief background on the Pitman-Yor process in Section 2. Then, in Section 3, we detail our modelling framework by illustrating it on a simple topic model. We continue through to the inference procedure on the topic model in Section 4. Finally, in Section 5, we present an application on modelling social network data, utilising the proposed framework. Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background on Pitman-Yor Process</head><p>We provide a brief, informal review of the Pitman-Yor process (PYP, <ref type="bibr" target="#b34">Ishwaran and James, 2001)</ref> in this section. We assume the readers are familiar with basic probability distributions (see <ref type="bibr" target="#b82">Walck, 2007)</ref> and the Dirichlet process <ref type="bibr">(DP, Ferguson, 1973)</ref>. In addition, we refer the readers to <ref type="bibr" target="#b31">Hjort et al. (2010)</ref> for a tutorial on Bayesian nonparametric modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pitman-Yor Process</head><p>The Pitman-Yor process (PYP, <ref type="bibr" target="#b34">Ishwaran and James, 2001)</ref> is also known as the twoparameter Poisson-Dirichlet process. The PYP is a two-parameter generalisation of the DP, now with an extra parameter α named the discount parameter in addition to the concentration parameter β. Similar to DP, a sample from a PYP corresponds to a discrete distribution (known as the output distribution) with the same support as its base distribution H. The underlying distribution of the PYP is the Poisson-Dirichlet distribution (PDD), which was introduced by <ref type="bibr" target="#b68">Pitman and Yor (1997)</ref>.</p><p>The PDD is defined by its construction process. For 0 ≤ α &lt; 1 and β &gt; -α, let V k be distributed independently as follows:</p><formula xml:id="formula_0">(V k | α, β) ∼ Beta(1 -α, β + kα) , for k = 1, 2, 3, . . . ,<label>(1)</label></formula><p>and define (p 1 , p 2 , p 3 , . . . ) as</p><formula xml:id="formula_1">p 1 = V 1 ,<label>(2)</label></formula><formula xml:id="formula_2">p k = V k k-1 i=1 (1 -V i ) , for k ≥ 2 . (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>If we let p = (p 1 , p2 , p3 , . . . ) be a sorted version of (p 1 , p 2 , p 3 , . . . ) in descending order, then p is Poisson-Dirichlet distributed with parameter α and β:</p><formula xml:id="formula_4">p ∼ PDD(α, β) .<label>(4)</label></formula><p>Note that the unsorted version (p 1 , p 2 , p 3 , . . . ) follows a GEM(α, β) distribution, which is named after Griffiths, Engen and McCloskey <ref type="bibr" target="#b67">(Pitman, 2006)</ref>.</p><p>With the PDD defined, we can then define the PYP formally. Let H be a distribution over a measurable space (X , B), for 0 ≤ α &lt; 1 and β &gt; -α, suppose that p = (p 1 , p 2 , p 3 , . . . ) follows a PDD (or GEM) with parameters α and β, then PYP is given by the formula</p><formula xml:id="formula_5">p(x | α, β, H) = ∞ k=1 p k δ X k (x) , for k = 1, 2, 3, . . . ,<label>(5)</label></formula><p>where X k are independent samples drawn from the base measure H and δ X k (x) represents probability point mass concentrated at X k (i.e., it is an indicator function that is equal to 1 when x = X k and 0 otherwise):</p><formula xml:id="formula_6">δ x (y) = 1 if x = y 0 otherwise . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>This construction, Equation (1), is named the stick-breaking process. The PYP can also be constructed using an analogue to Chinese restaurant process (which explicitly draws a sequence of samples from the base distribution). A more extensive review on the PYP is given by <ref type="bibr" target="#b11">Buntine and Hutter (2012)</ref>.</p><p>A PYP is often more suitable than a DP in modelling since it exhibits a power-law behaviour (when α = 0), which is observed in natural languages <ref type="bibr" target="#b25">(Goldwater et al., 2005;</ref><ref type="bibr" target="#b78">Teh and Jordan, 2010)</ref>. The PYP has also been employed in genomics <ref type="bibr" target="#b21">(Favaro et al., 2009)</ref> and economics <ref type="bibr" target="#b0">(Aoki, 2008)</ref>. Note that when the discount parameter α is 0, the PYP simply reduces to a DP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pitman-Yor Process with a Mixture Base</head><p>Note that the base measure H of a PYP is not necessarily restricted to a single probability distribution. H can also be a mixture distribution such as</p><formula xml:id="formula_8">H = ρ 1 H 1 + ρ 2 H 2 + • • • + ρ n H n ,<label>(7)</label></formula><p>where n i=1 ρ i = 1 and {H 1 , . . . H n } is a set of distributions over the same measurable space (X , B) as H.</p><p>With this specification of H, the PYP is also named the compound Poisson-Dirichlet process in <ref type="bibr" target="#b16">Du (2012)</ref>, or the doubly hierarchical Pitman-Yor process in <ref type="bibr" target="#b87">Wood and Teh (2009)</ref>. A special case of this is the DP equivalent, which is also known as the DP with mixed random measures in <ref type="bibr" target="#b40">Kim et al. (2012)</ref>. Note that we have assumed constant values for the ρ i , though of course we can go fully Bayesian and assign a prior distribution for each of them, a natural prior would be the Dirichlet distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Remark on Bayesian Inference</head><p>Performing exact Bayesian inference on nonparametric models is often intractable due to the difficulty in deriving the closed-form posterior distributions. This motivates the use of Markov chain Monte Carlo (MCMC) methods (see <ref type="bibr" target="#b23">Gelman et al., 2013)</ref> for approximate inference. Most notable of the MCMC methods are the Metropolis-Hastings (MH) algorithms <ref type="bibr" target="#b63">(Metropolis et al., 1953;</ref><ref type="bibr" target="#b29">Hastings, 1970)</ref> and Gibbs samplers <ref type="bibr" target="#b24">(Geman and Geman, 1984)</ref>. These algorithms serve as a building block for more advanced samplers, such as the MH algorithms with delayed rejection <ref type="bibr" target="#b64">(Mira, 2001)</ref>. Generalisations of the MCMC method include the reversible jump MCMC <ref type="bibr" target="#b27">(Green, 1995)</ref> and its delayed rejection variant <ref type="bibr" target="#b28">(Green and Mira, 2001)</ref> can also be employed for Bayesian inference, however, they are out of the scope in this article.</p><p>Instead of sampling one parameter at a time, one can develop an algorithm that updates more parameters in each iteration, a so-called blocked Gibbs sampler <ref type="bibr" target="#b50">(Liu, 1994)</ref>. Also, in practice we are usually only interested in a certain subset of the parameters; in such cases we can sometimes derive more efficient collapsed Gibbs samplers <ref type="bibr" target="#b50">(Liu, 1994)</ref> by integrating out the nuisance parameters. In the remainder of this article, we will employ a combination of the blocked and collapsed Gibbs samplers for Bayesian inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Modelling Framework with Hierarchical Pitman-Yor Process</head><p>In this section, we discuss the basic design of our nonparametric Bayesian topic models using thierarchical Pitman-Yor processes (HPYP). In particular, we will introduce a simple topic model that will be extended later. We discuss the general inference algorithm for the topic model and hyperparameter optimisation.</p><p>Development of topic models is fundamentally motivated by their applications. Depending on the application, a specific topic model that is most suitable for the task should be designed and used. However, despite the ease of designing the model, the majority of time is spent on implementing, assessing, and redesigning it. This calls for a better designing cycle/routine that is more efficient, that is, spending less time in implementation and more time in model design and development. is usually referred as topic side, while the right hand side (with γ and φ) is called the vocabulary side. The word node denoted by w dn is observed. The notations are defined in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We can achieve this by a higher level implementation of the algorithms for topic modelling. This has been made possible in other statistical domains by BUGS (Bayesian inference using Gibbs sampling, <ref type="bibr" target="#b55">Lunn et al., 2000)</ref> or JAGS (just another Gibbs sampler, <ref type="bibr" target="#b69">Plummer, 2003)</ref>, albeit with standard probability distributions. Theoretically, BUGS and JAGS will work on LDA; however, in practice, running Gibbs sampling for LDA with BUGS and JAGS is very slow. This is because their Gibbs samplers are uncollapsed and not optimised. Furthermore, they cannot be used in a model with stochastic processes, like the Gaussian process (GP) and DP.</p><p>Below, we present a framework that allows us to implement HPYP topic models efficiently. This framework allows us to test variants of our proposed topic models without significant reimplementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Pitman-Yor Process Topic Model</head><p>The HPYP topic model is a simple network of PYP nodes since all distributions on the probability vectors are modelled by the PYP. For simplicity, we assume a topic model with three PYP layers, although in practice there is no limit to the number of PYP layers. We present the graphical model of our generic topic model in Figure <ref type="figure" target="#fig_0">1</ref>. This model is a variant of those presented in <ref type="bibr" target="#b12">Buntine and Mishra (2014)</ref>, and is presented here as a starting model for illustrating our methods and for subsequent extensions.</p><p>At the root level, we have µ and γ distributed as PYPs:</p><formula xml:id="formula_9">µ ∼ PYP(α µ , β µ , H µ ) ,<label>(8)</label></formula><formula xml:id="formula_10">γ ∼ PYP(α γ , β γ , H γ ) .<label>(9)</label></formula><p>The variable µ is the root node for the topics in a topic model while γ is the root node for the words. To allow arbitrary number of topics to be learned, we let the base distribution for µ, H µ , to be a continuous distribution or a discrete distribution with infinite samples.</p><p>We usually choose a discrete uniform distribution for γ based on the word vocabulary size of the text corpus. This decision is technical in nature, as we are able to assign a tiny probability to words not observed in the training set, which eases the evaluation process. Thus</p><formula xml:id="formula_11">H γ = {• • • , 1 |V| , • • • }</formula><p>where |V| is the set of all word vocabulary of the text corpus. We now consider the topic side of the HPYP topic model. Here we have ν, which is the child node of µ. It follows a PYP given ν, which acts as its base distribution:</p><formula xml:id="formula_12">ν ∼ PYP(α ν , β ν , µ) . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>For each document d in a text corpus of size D, we have a document-topic distribution θ d , which is a topic distribution specific to a document. Each of them tells us about the topic composition of a document.</p><formula xml:id="formula_14">θ d ∼ PYP(α θ d , β θ d , ν) , for d = 1, . . . , D .<label>(11)</label></formula><p>While for the vocabulary side, for each topic k learned by the model, we have a topicword distribution φ k which tells us about the words associated with each topic. The topicword distribution φ k is PYP distributed given the parent node γ, as follows:</p><formula xml:id="formula_15">φ k ∼ PYP(α φ k , β φ k , γ) , for k = 1, . . . , K .<label>(12)</label></formula><p>Here, K is the number of topics in the topic model. For every word w dn in a document d which is indexed by n (from 1 to N d , the number of words in document d), we have a latent topic z dn (also known as topic assignment) which indicates the topic the word represents. z dn and w dn are categorical variables generated from θ d and φ k respectively:</p><formula xml:id="formula_16">z dn | θ d ∼ Discrete(θ d ) ,<label>(13)</label></formula><formula xml:id="formula_17">w dn | z dn , φ ∼ Discrete(φ z d ) , for n = 1, . . . , N d . (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>The above α and β are the discount and concentration parameters of the PYPs (see Section 2.1), note that they are called the hyperparameters in the model. We present a list of variables used in this section in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Representation and Posterior Likelihood</head><p>In a Bayesian setting, posterior inference requires us to analyse the posterior distribution of the model variables given the observed data. For instance, the joint posterior distribution for the HPYP topic model is</p><formula xml:id="formula_19">p(µ, ν, γ, θ, φ, Z | W, Ξ) .<label>(15)</label></formula><p>Here, we use bold face capital letters to represent the set of all relevant variables. For instance, W captures all words in the corpus. Additionally, we denote Ξ as the set of all hyperparameters and constants in the model. Note that deriving the posterior distribution analytically is almost impossible due to its complex nature. This leaves us with approximate Bayesian inference techniques as mentioned in Section 2.3. However, even with these techniques, performing posterior inference </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All topics</head><p>Collection of all topics z dn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All words</head><p>Collection of all words w dn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ξ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All hyperparameters</head><p>Collection of all hyperparameters and constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All customer counts Collection of all customers counts</head><formula xml:id="formula_20">c N k . T All table counts Collection of all table counts t N k .</formula><p>with the posterior distribution is difficult due to the coupling of the probability vectors from the PYPs.</p><p>The key to an efficient inference procedure with the PYPs is to marginalise out the PYPs in the model and record various associated counts instead, which yields a collapsed sampler. To achieve this, we adopt a Chinese Restaurant Process (CRP) metaphor <ref type="bibr" target="#b78">(Teh and Jordan, 2010;</ref><ref type="bibr" target="#b5">Blei et al., 2010)</ref> to represent the variables in the topic model. With this metaphor, all data in the model (e.g., topics and words) are the customers; while the PYP nodes are the restaurants the customers visit. In each restaurant, each customer is to be seated at only one table, though each table can have any number of customers. Each table in a restaurant serves a dish, the dish corresponds to the categorical label a data point may The dishes are the symbols in the middle of the rectangles, here they are denoted by the sunny symbol and the cloudy symbol. In this illustration, we know the number of customers corresponds to each table, for example, the green table is occupied by three customers. Also, since Restaurant 1 is the parent of Restaurant 2, the tables in Restaurant 2 are treated as the customers for Restaurant 1.</p><p>have (e.g., the topic label or word). Note that there can be more than one table serving the same dish. In a HPYP topic model, the tables in a restaurant N are treated as the customers for the parent restaurant P (in the graphical model, P points to N ), and they share the same dish. This means that the data is passed up recursively until the root node.</p><p>For illustration, we present a simple example in Figure <ref type="figure" target="#fig_1">2</ref>, showing the seating arrangement of the customers from two restaurants. Naïvely recording the seating arrangement (table and dish) of each customer brings about computational inefficiency during inference. Instead, we adopt the table multiplicity (or table counts) representation of <ref type="bibr" target="#b14">Chen et al. (2011)</ref> which requires no dynamic memory, thus consuming only a factor of memory at no loss of inference efficiency. Under this representation, we store only the customer counts and table counts associated with each restaurant. The customer count c N k denotes the number of customers who are having dish k in restaurant N . The corresponding symbol without subscript, c N , denotes the collection of customer counts in restaurant N , that is, Here the setting is the same as Figure <ref type="figure" target="#fig_1">2</ref> but the seating arrangement of the customers are "forgotten" and only the table and customer counts are recorded. Thus, we only know that there are three sunny tables in Restaurant 2, with a total of nine customers.</p><formula xml:id="formula_21">c N = (• • • , c N k , • • • ).</formula><p>k c N k . Similar to the customer count, the table count t N k denotes the number of nonempty tables serving dish k in restaurant N . The corresponding t N and T N are defined similarly. For instance, from the example in Figure <ref type="figure" target="#fig_1">2</ref>, we have c 2 sun = 9 and t 2 sun = 3, the corresponding illustration of the table multiplicity representation is presented in Figure <ref type="figure" target="#fig_2">3</ref>. We refer the readers to <ref type="bibr" target="#b14">Chen et al. (2011)</ref> for a detailed derivation of the posterior likelihood of a restaurant.</p><p>For the posterior likelihood of the HPYP topic model, we marginalise out the probability vector associated with the PYPs and represent them with the customer counts and table counts, following <ref type="bibr">Chen et al. (2011, Theorem 1)</ref>. We present the modularised version of the full posterior of the HPYP topic model, which allows the posterior to be computed very quickly. The full posterior consists of the modularised likelihood associated with each PYP in the model, defined as</p><formula xml:id="formula_22">f (N ) = β N α N T N β N C N K k=1 S c N k t N k , α N c N k t N k -1 , for N ∼ PYP α N , β N , P .<label>(16)</label></formula><p>Here, S x y, α are generalised Stirling numbers <ref type="bibr">(Buntine and Hutter, 2012, Theorem 17)</ref>. Both (x) T and (x|y) T denote Pochhammer symbols with rising factorials <ref type="bibr">(Oldham et al., 2009, Section 18</ref>):</p><formula xml:id="formula_23">(x) T = x • (x + 1) • • • x + (T -1) , (<label>17</label></formula><formula xml:id="formula_24">) (x|y) T = x • (x + y) • • • x + (T -1)y . (<label>18</label></formula><formula xml:id="formula_25">)</formula><p>With the CRP representation, the full posterior of the HPYP topic model can now be written -in terms of f (•) given in Equation ( <ref type="formula" target="#formula_22">16</ref>) -as</p><formula xml:id="formula_26">p(Z, T, C | W, Ξ) ∝ p(Z, W, T, C | Ξ) ∝ f (µ)f (ν) D d=1 f (θ d ) K k=1 f (φ k ) f (γ) |V| v=1 1 |V| t γ v . (<label>19</label></formula><formula xml:id="formula_27">)</formula><p>This result is a generalisation of Chen et al. (2011, Theorem 1) to account for discrete base distribution -the last term in Equation ( <ref type="formula" target="#formula_26">19</ref>) corresponds to the base distribution of γ, and v indexes each unique word in vocabulary set V. The bold face T and C denote the collection of all table counts and customer counts, respectively. Note that the topic assignments Z are implicitly captured by the customer counts:</p><formula xml:id="formula_28">c θ d k = N d n=1 I(z dn = k) ,<label>(20)</label></formula><p>where I(•) is the indicator function, which evaluates to 1 when the statement inside the function is true, and 0 otherwise. We would like to point out that even though the probability vectors of the PYPs are integrated out and not explicitly stored, they can easily be reconstructed. This is discussed in Section 4.4. We move on to Bayesian inference in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Posterior Inference for the HPYP Topic Model</head><p>We focus on the MCMC method for Bayesian inference on the HPYP topic model. The MCMC method on topic models follows these simple procedures -decrementing counts contributed by a word, sample a new topic for the word, and update the model by accepting or rejecting the proposed sample. Here, we describe the collapsed blocked Gibbs sampler for the HPYP topic model. Note the PYPs are marginalised out so we only deal with the counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decrementing the Counts Associated with a Word</head><p>The first step in a Gibbs sampler is to remove a word and corresponding latent topic, then decrement the associated customer counts and table counts. To give an example from Figure <ref type="figure" target="#fig_1">2</ref>, if we remove the red customer from Restaurant 2, we would decrement the customer count c 2 sun by 1. Additionally, we also decrement the table count t 2 sun by 1 because the red customer is the only customer on its table. This in turn decrements the customer count c 1 sun by 1. However, this requires us to keep track of the customers' seating arrangement which leads to increased memory requirements and poorer performance due to inadequate mixing <ref type="bibr" target="#b14">(Chen et al., 2011)</ref>.</p><p>To overcome the above issue, we follow the concept of table indicator <ref type="bibr" target="#b14">(Chen et al., 2011)</ref> and introduce a new auxiliary Bernoulli indicator variable u N k , which indicates whether removing the customer also removes the table by which the customer is seated. Note that our Bernoulli indicator is different to that of <ref type="bibr" target="#b14">Chen et al. (2011)</ref> which indicates the restaurant a customer contributes to. The Bernoulli indicator is sampled as needed in the decrementing procedure and it is not stored, this means that we simply "forget" the seating arrangements and re-sample them later when needed, thus we do not need to store the seating arrangement. The Bernoulli indicator of a restaurant N depends solely on the customer counts and the table counts:</p><formula xml:id="formula_29">p u N k = t N k /c N k if u N k = 1 1 -t N k /c N k if u N k = 0 .<label>(21)</label></formula><p>In the context of the HPYP topic model described in Section 3.1, we formally present how we decrement the counts associated with the word w dn and latent topic z dn from document d and position n. First, on the vocabulary side (see Figure <ref type="figure" target="#fig_0">1</ref>), we decrement the customer count c φz dn w dn associated with φ z dn by 1. Then sample a Bernoulli indicator u φz dn w dn according to Equation (21). If u φz dn w dn = 1, we decrement the table count t φz dn</p><p>w dn and also the customer count c γ w dn by one. In this case, we would sample a Bernoulli indicator u γ w dn for γ, and decrement t γ w dn if u γ w dn = 1. We do not decrement the respective customer count if the Bernoulli indicator is 0. Second, we would need to decrement the counts associated with the latent topic z dn . The procedure is similar, we decrement c θ d z dn by 1 and sample the Bernoulli indicator u θ d z dn . Note that whenever we decrement a customer count, we sample the corresponding Bernoulli indicator. We repeat this procedure recursively until the Bernoulli indicator is 0 or until the procedure hits the root node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling a New Topic for a Word</head><p>After decrementing the variables associated with a word w dn , we use a blocked Gibbs sampler to sample a new topic z dn for the word and the corresponding customer counts and table counts. The conditional posterior used in sampling can be computed quickly when the full posterior is represented in a modularised form. To illustrate, the conditional posterior for z dn and its associated customer counts and table counts is</p><formula xml:id="formula_30">p(z dn , T, C | Z -dn , W, T -dn , C -dn , Ξ) = p(Z, T, C | W, Ξ) p(Z -dn , T -dn , C -dn | W, Ξ) ,<label>(22)</label></formula><p>which is further broken down by substituting the posterior likelihood defined in Equation (19), giving the following ratios of the modularised likelihoods:</p><formula xml:id="formula_31">f (µ) f (µ -dn ) f (ν) f (ν -dn ) f (θ d ) f (θ -dn d ) f (φ z dn ) f (φ -dn z dn ) f (γ) f (γ -dn ) 1 |V| t γ w dn -(t γ w dn ) -dn . (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>The superscript -dn indicates that the variables associated with the word w dn are removed from the respective sets, that is, the customer counts and table counts are after the decrementing procedure. Since we are only sample the topic assignment z dn associated with one Table 2: All possible proposals of the blocked Gibbs sampler for the variables associated with w dn . To illustrate, one sample would be z dn = 1, t N z dn does not increment (stays the same), and c N z dn increments by 1, for all N in {µ, ν, θ d , φ z dn , γ}. We note that the proposals can include states that are invalid, but this is not an issue since those states have zero posterior probability and thus will not be sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable Possibilities Variable</head><p>Possibilities Variable Possibilities</p><formula xml:id="formula_33">z dn {1, . . . , K} t N z dn {t N z dn , t N z dn + 1} c N z dn {c N z dn , c N z dn + 1}</formula><p>word, the customer counts and table counts can only increment by at most 1, see Table <ref type="table">2</ref> for a list of all possible proposals. This allows the ratios of the modularised likelihoods, which consists of ratios of Pochhammer symbol and ratio of Stirling numbers</p><formula xml:id="formula_34">f (N ) f (N -dn ) = (β N ) (C N ) -dn (β N ) C N (β N |α N ) T N (β N |α N ) (T N ) -dn K k=1 S c N k t N k , α N S (c N k ) -dn (t N k ) -dn , α N ,<label>(24)</label></formula><p>to simplify further. For instance, the ratios of Pochhammer symbols can be reduced to constants, as follows:</p><formula xml:id="formula_35">(x) T +1 (x) T = x + T , (x|y) T +1 (x|y) T = x + yT . (<label>25</label></formula><formula xml:id="formula_36">)</formula><p>The ratio of Stirling numbers, such as S y+1 x+1, α /S y x, α , can be computed quickly via caching <ref type="bibr" target="#b11">(Buntine and Hutter, 2012)</ref>. Technical details on implementing the Stirling numbers cache can be found in <ref type="bibr" target="#b45">Lim (2016)</ref>.</p><p>With the conditional posterior defined, we proceed to the sampling process. Our first step involves finding all possible changes to the topic z dn , customer counts, and the table counts (hereafter known as 'state') associated with adding the removed word w dn back into the topic model. Since only one word is added into the model, the customer counts and the table counts can only increase by at most 1, constraining the possible states to a reasonably small number. Furthermore, the customer counts of a parent node will only be incremented when the table counts of its child node increases. Note that it is possible for the added customer to generate a new dish (topic) for the model. This requires the customer to increment the table count of a new dish in the root node µ by 1 (from 0).</p><p>Next, we compute the conditional posterior (Equation ( <ref type="formula" target="#formula_30">22</ref>)) for all possible states. The conditional posterior (up to a proportional constant) can be computed quickly by breaking down the posterior and calculating the relevant parts. We then normalise them to sample one of the states to be the proposed next state. Note that the proposed state will always be accepted, which is an artifact of Gibbs sampler.</p><p>Finally, given the proposal, we update the HPYP model by incrementing the relevant customer counts and table counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimising the Hyperparameters</head><p>Choosing the right hyperparameters for the priors is important for topic models. <ref type="bibr">Wallach et al. (2009a)</ref> show that an optimised hyperparameter increases the robustness of the topic models and improves their model fitting. The hyperparameters of the HPYP topic models are the discount parameters and concentration parameters of the PYPs. Here, we propose a procedure to optimise the concentration parameters, but leave the discount parameters fixed due to their coupling with the Stirling numbers cache.</p><p>The concentration parameters β of all the PYPs are optimised using an auxiliary variable sampler similar to <ref type="bibr" target="#b77">Teh (2006)</ref>. Being Bayesian, we assume the concentration parameter β N of a PYP node N has the following hyperprior:</p><formula xml:id="formula_37">β N ∼ Gamma(τ 0 , τ 1 ) , for N ∼ PYP α N , β N , P ,<label>(26)</label></formula><p>where τ 0 is the shape parameter and τ 1 is the rate parameter. The gamma prior is chosen due to its conjugacy which gives a gamma posterior for β N .</p><p>To optimise β N , we first sample the auxiliary variables ω and ζ i given the current value of α N and β N , as follows:</p><formula xml:id="formula_38">ω | β N ∼ Beta C N , β N , (<label>27</label></formula><formula xml:id="formula_39">)</formula><formula xml:id="formula_40">ζ i | α N , β N ∼ Bernoulli β N β N + iα N , for i = 0, 1, . . . , T N -1 .<label>(28)</label></formula><p>With these, we can then sample a new β N from its conditional posterior</p><formula xml:id="formula_41">β N ω, ζ ∼ Gamma   τ 0 + T N -1 i=0 ζ i , τ 1 -log(1 -ω)   . (<label>29</label></formula><formula xml:id="formula_42">)</formula><p>The collapsed Gibbs sampler is summarised by Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Estimating the Probability Vectors of the PYPs</head><p>Recall that the aim of topic modelling is to analyse the posterior of the model parameters, such as one in Equation ( <ref type="formula" target="#formula_19">15</ref>). Although we have marginalised out the PYPs in the above Gibbs sampler, the PYPs can be reconstructed from the associated customer counts and table counts. Recovering the full posterior distribution of the PYPs is a complicated task. So, instead, we will analyse the PYPs via the expected value of their conditional marginal posterior distribution, or simply, their posterior mean,</p><formula xml:id="formula_43">E[N | Z, W, T, C, Ξ] , for N ∈ {µ, ν, γ, θ d , φ k } . (<label>30</label></formula><formula xml:id="formula_44">)</formula><p>The posterior mean of a PYP corresponds to the probability of sampling a new customer for the PYP. To illustrate, we consider the posterior of the topic distribution θ d . We let zdn to be a unknown future latent topic in addition to the known Z. With this, we can write the posterior mean of θ dk as</p><formula xml:id="formula_45">E[θ dk | Z, W, T, C, Ξ] = E[p(z dn = k | θ d , Z, W, T, C, Ξ) | Z, W, T, C, Ξ] = E[p(z dn = k | Z, T, C) | Z, W, T, C, Ξ] .<label>(31</label></formula><p>) Algorithm 1 Collapsed Gibbs Sampler for the HPYP Topic Model 1. Initialise the HPYP topic model by assigning random topic to the latent topic z dn associated to each word w dn . Then update all the relevant customer counts C and table counts T by using Equation (20) and setting the table counts to be about half of the customer counts. 2. For each word w dn in each document d, do the following: (a) Decrement the counts associated with w dn (see Section 4.1). (b) Block sample a new topic for z dn and corresponding customer counts C and table counts T (see Section 4.2). (c) Update (increment counts) the topic model based on the sample. 3. Update the hyperparameter β N for each PYP nodes N (see Section 4.3). 4. Repeat Steps 2 -3 until the model converges or when a fix number of iterations is reached.</p><p>by replacing θ dk with the posterior predictive distribution of zdn and note that zdn can be sampled using the CRP, as follows:</p><formula xml:id="formula_46">p(z dn = k | Z, T, C) = (α θ d T θ d + β θ d )ν k + c θ d k -α θ d T θ d k β θ d + C θ d .<label>(32)</label></formula><p>Thus, the posterior mean of θ d is given as</p><formula xml:id="formula_47">E[θ dk | Z, W, T, C, Ξ] = (α θ d T θ d + β θ d )E[ν k | Z, W, T, C, Ξ] + c θ d k -α θ d T θ d k β θ d + C θ d ,<label>(33)</label></formula><p>which is written in term of the posterior mean of its parent PYP, ν. The posterior means of the other PYPs such as ν can be derived by taking a similar approach. Generally, the posterior mean corresponds to a PYP N (with parent PYP P) is as follows:</p><formula xml:id="formula_48">E[N k | Z, W, T, C, Ξ] = (α N T N + β N )E[P k | Z, W, T, C, Ξ] + c N k -α N T N k β N + C N ,<label>(34)</label></formula><p>By applying Equation ( <ref type="formula" target="#formula_48">34</ref>) recursively, we obtain the posterior mean for all the PYPs in the model. We note that the dimension of the topic distributions (µ, ν, θ) is K + 1, where K is the number of observed topics. This accounts for the generation of a new topic associated with the new customer, though the probability of generating a new topic is usually much smaller. In practice, we may instead ignore the extra dimension during the evaluation of a topic model since it does not provide useful interpretation. One way to do this is to simply discard the extra dimension of all the probability vectors after computing the posterior mean. Another approach would be to normalise the posterior mean of the root node µ after discarding the extra dimension, before computing the posterior mean of others PYPs. Note that for a considerably large corpus, the difference in the above approaches would be too small to notice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluations on Topic Models</head><p>Generally, there are two ways to evaluate a topic model. The first is to evaluate the topic model based on the task it performs, for instance, the ability to make predictions. The second approach is the statistical evaluation of the topic model on modelling the data, which is also known as the goodness-of-fit test. In this section, we will present some commonly used evaluation metrics that are applicable to all topic models, but we first discuss the procedure for estimating variables associated with the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Predictive Inference on the Test Documents</head><p>Test documents, which are used for evaluations, are set aside from learning documents. As such, the document-topic distributions θ associated with the test documents are unknown and hence need to be estimated. One estimate for θ is its posterior mean given the variables learned from the Gibbs sampler:</p><formula xml:id="formula_49">θd = E[θ d | Z, W, T, C, Ξ] ,<label>(35)</label></formula><p>obtainable by applying Equation ( <ref type="formula" target="#formula_48">34</ref>). Note that since the latent topics Z corresponding to the test set are not sampled, the customer counts and table counts associated with θ d are 0, thus θd is equal to ν, the posterior mean of ν. However, this is not a good estimate for the topic distribution of the test documents since they will be identical for all the test documents. To overcome this issue, we will instead use some of the words in the test documents to obtain a better estimate for θ. This method is known as document completion <ref type="bibr" target="#b84">(Wallach et al., 2009b)</ref>, as we use part of the text to estimate θ, and use the rest for evaluation. Getting a better estimate for θ requires us to first sample some of the latent topics zdn in the test documents. The proper way to do this is by running an algorithm akin to the collapsed Gibbs sampler, but this would be excruciatingly slow due to the need to re-sample the customer counts and table counts for all the parent PYPs. Instead, we assume that the variables learned from the Gibbs sampler are fixed and sample the zdn from their conditional posterior sequentially, given the previous latent topics:</p><formula xml:id="formula_50">p(z dn = k | wdn , θ d , φ, zd1 , . . . , zd,n-1 ) ∝ θ dk φ kw dn .<label>(36)</label></formula><p>Whenever a latent topic zdn is sampled, we increment the customer count c θ d zdn for the test document. For simplicity, we set the table count t θ d zdn to be half the corresponding customer counts c θ d zdn , this avoids the expensive operation of sampling the table counts. Additionally, θ d is re-estimated using Equation ( <ref type="formula" target="#formula_49">35</ref>) before sampling the next latent topic. We note that the estimated variables are unbiased.</p><p>The final θ d becomes an estimate for the topic distribution of the test document d. The above procedure is repeated R times to give R samples of θ </p><p>This Monte Carlo estimate can then be used for computing the evaluation metrics. Note that when estimating θ, we have ignored the possibility of generating a new topic, that is, the latent topics z are constrained to the existing topics, as previously discussed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Goodness-of-fit Test</head><p>Measures of goodness-of-fit usually involves computing the discrepancy of the observed values and the predicted values under the model. However, the observed variables in a topic model are the words in the corpus, which are not quantifiable since they are discrete labels. Thus evaluations on topic models are usually based on the model likelihoods instead.</p><p>A popular metric commonly used to evaluate the goodness-of-fit of a topic model is perplexity, which is negatively related to the likelihood of the observed words W given the model, this is defined as</p><formula xml:id="formula_52">perplexity(W | θ, φ) = exp - D d=1 N d n=1 log p(w dn | θ d , φ) D d=1 N d ,<label>(38)</label></formula><p>where p(w dn | θ d , φ) is the likelihood of sampling the word w dn given the document-topic distribution θ d and the topic-word distributions φ. Computing p(w dn | θ d , φ) requires us to marginalise out z dn from their joint distribution, as follows:</p><formula xml:id="formula_53">p(w dn | θ d , φ) = k p(w dn , z dn = k | θ d , φ) = k p(w dn | z dn = k, φ k ) p(z dn = k | θ d ) = k φ kw dn θ dk .<label>(39)</label></formula><p>Although perplexity can be computed on the whole corpus, in practice we compute the perplexity on test documents. This is to measure if the topic model generalises well to unseen data. A good topic model would be able to predict the words in the test set better, thereby assigning a higher probability p(w dn | θ d , φ) in generating the words. Since perplexity is negatively related to the likelihood, a lower perplexity is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Document Clustering</head><p>We can also evaluate the clustering ability of the topic models. Note that topic models assign a topic to each word in a document, essentially performing a soft clustering <ref type="bibr" target="#b20">(Erosheva and Fienberg, 2005)</ref> for the documents in which the membership is given by the document-topic distribution θ. To evaluate the clustering of the documents, we convert the soft clustering to hard clustering by choosing a topic that best represents the documents, hereafter called the dominant topic. The dominant topic of a document d corresponds to the topic that has the highest proportion in the topic distribution, that is,</p><formula xml:id="formula_54">Dominant Topic(θ d ) = arg max k θ dk . (<label>40</label></formula><formula xml:id="formula_55">)</formula><p>Two commonly used evaluation measures for clustering are purity and normalised mutual information (NMI, <ref type="bibr" target="#b57">Manning et al., 2008)</ref>. The purity is a simple clustering measure which can be interpreted as the proportion of documents correctly clustered, while NMI is an information theoretic measures used for clustering comparison. If we denote the ground truth classes as S = {s 1 , . . . , s J } and the obtained clusters as R = {r 1 , . . . , r K }, where each s j and r k represents a collection (set) of documents, then the purity and NMI can be computed as</p><formula xml:id="formula_56">purity(S, R) = 1 D K k=1 max j |r k ∩ s j | , NMI(S, R) = 2 MI(S; R) E(S) + E(R) , (<label>41</label></formula><formula xml:id="formula_57">)</formula><p>where MI(S; R) denotes the mutual information between two sets and E(•) denotes the entropy. They are defined as follows:</p><formula xml:id="formula_58">MI(S; R) = K k=1 J j=1 |r k ∩ s j | D log 2 D |r k ∩ s j | |r k ||s j | , E(R) = - K k=1 |r k | D log 2 |r k | D . (<label>42</label></formula><formula xml:id="formula_59">)</formula><p>Note that the higher the purity or NMI, the better the clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Application: Modelling Social Network on Twitter</head><p>This section looks at how we can employ the framework discussed above for an application of tweet modelling, using auxiliary information that is available on Twitter. We propose the Twitter-Network topic model (TNTM) to jointly model the text and the social network in a fully Bayesian nonparametric way, in particular, by incorporating the authors, hashtags, the "follower" network, and the text content in modelling. The TNTM employs a HPYP for text modelling and a Gaussian process (GP) random function model for social network modelling. We show that the TNTM significantly outperforms several existing nonparametric models due to its flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Motivation</head><p>Emergence of web services such as blogs, microblogs and social networking websites allows people to contribute information freely and publicly. This user-generated information is generally more personal, informal, and often contains personal opinions. In aggregate, it can be useful for reputation analysis of entities and products <ref type="bibr" target="#b2">(Aula, 2010)</ref>, natural disaster detection <ref type="bibr" target="#b38">(Karimi et al., 2013)</ref>, obtaining first-hand news <ref type="bibr" target="#b9">(Broersma and Graham, 2012)</ref>, or even demographic analysis <ref type="bibr" target="#b15">(Correa et al., 2010)</ref>. We focus on Twitter, an accessible source of information that allows users to freely voice their opinions and thoughts in short text known as tweets.</p><p>Although LDA <ref type="bibr">(Blei et al., 2003</ref>) is a popular model for text modelling, a direct application on tweets often yields poor result as tweets are short and often noisy <ref type="bibr" target="#b89">(Zhao et al., 2011;</ref><ref type="bibr" target="#b3">Baldwin et al., 2013)</ref>, that is, tweets are unstructured and often contain grammatical and spelling errors, as well as informal words such as user-defined abbreviations due to the 140 characters limit. LDA fails on short tweets since it is heavily dependent on word co-occurrence. Also notable is that the text in tweets may contain special tokens known as hashtags; they are used as keywords and allow users to link their tweets with other tweets tagged with the same hashtag. Nevertheless, hashtags are informal since they have no standards. Hashtags can be used as both inline words or categorical labels. When used as labels, hashtags are often noisy, since users can create new hashtags easily and use any existing hashtags in any way they like. 3 Hence instead of being hard labels, hashtags are best treated as special words which can be the themes of the tweets. These properties of tweets make them challenging for topic models, and ad hoc alternatives are used instead. For instance, <ref type="bibr" target="#b59">Maynard et al. (2012)</ref> advocate the use of shallow method for tweets, and <ref type="bibr" target="#b61">Mehrotra et al. (2013)</ref> utilise a tweet-pooling approach to group short tweets into a larger document. In other text analysis applications, tweets are often 'cleansed' by NLP methods such as lexical normalisation <ref type="bibr" target="#b3">(Baldwin et al., 2013)</ref>. However, the use of normalisation is also criticised <ref type="bibr" target="#b19">(Eisenstein, 2013)</ref>, as normalisation can change the meaning of text.</p><p>In the following, we propose a novel method for better modelling of microblogs by leveraging the auxiliary information that accompanies tweets. This information, complementing word co-occurrence, also opens the door to more applications, such as user recommendation and hashtag suggestion. Our major contributions include (1) a fully Bayesian nonparametric model named the Twitter-Network topic model (TNTM) that models tweets well, and (2) a combination of both the HPYP and the GP to jointly model text, hashtags, authors and the followers network. Despite the seeming complexity of the TNTM model, its implementation is made relatively straightforward using the flexible framework developed in Section 3. Indeed, a number of other variants were rapidly implemented with this framework as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Twitter-Network Topic Model</head><p>The TNTM makes use of the accompanying hashtags, authors, and followers network to model tweets better. The TNTM is composed of two main components: a HPYP topic model for the text and hashtags, and a GP based random function network model for the followers network. The authorship information serves to connect the two together. The HPYP topic model is illustrated by region b in Figure <ref type="figure" target="#fig_4">4</ref> while the network model is captured by region a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">HPYP Topic Model</head><p>The HPYP topic model described in Section 3 is extended as follows. For the word distributions, we first generate a parent word distribution prior γ for all topics:</p><formula xml:id="formula_60">γ ∼ PYP(α γ , β γ , H γ ) ,<label>(43)</label></formula><p>where H γ is a discrete uniform distribution over the complete word vocabulary V. 4 Then, we sample the hashtag distribution ψ k and word distribution ψ k for each topic k, with γ as 3. For example, hashtag hijacking, where a well defined hashtag is used in an "inappropriate" way. The most notable example would be on the hashtag #McDStories, though it was initially created to promote happy stories on McDonald's, the hashtag was hijacked with negative stories on McDonald's. 4. The complete word vocabulary contains words and hashtags seen in the corpus. The author-topic distributions ν serve to link the two together. Each tweet is modelled with a hierarchy of document-topic distributions denoted by η, θ , and θ, where each is attuned to the whole tweet, the hashtags, and the words, in that order. With their own topic assignments z and z, the hashtags y and the words w are separately modelled. They are generated from the topic-hashtag distributions ψ and the topic-word distributions ψ respectively. The variables µ 0 , µ 1 and γ are priors for the respective PYPs. The connections between the authors are denoted by x, modelled by random function F.</p><formula xml:id="formula_61">ν i µ 0 µ 1 η d a d θ ′ d θ d z ′ dm z dn y dm w dn ψ k γ ψ ′ k A D K M d N d F x ij E a ⃝ b ⃝</formula><p>the base distribution:</p><formula xml:id="formula_62">ψ k | γ ∼ PYP(α ψ k , β ψ k , γ) ,<label>(44)</label></formula><formula xml:id="formula_63">ψ k | γ ∼ PYP(α ψ k , β ψ k , γ) , for k = 1, . . . , K .<label>(45)</label></formula><p>Note that the tokens of the hashtags are shared with the words, that is, the hashtag #happy shares the same token as the word happy, and are thus treated as the same word. This treatment is important since some hashtags are used as words instead of labels. 5 Additionally, this also allows any words to be hashtags, which will be useful for hashtag recommendation.</p><p>For the topic distributions, we generate a global topic distribution µ 0 , which serves as a prior, from a GEM distribution. Then generate the author-topic distribution ν i for each 5. For instance, as illustrated by the following tweet: i want to get into #photography. can someone recommend a good beginner #camera please? i dont know where to start</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Random Function Network Model</head><p>The network modelling is connected to the HPYP topic model via the author-topic distributions ν, where we treat ν as inputs to the GP in the network model. The GP, represented by F, determines the link between two authors (x ij ), which indicates the existence of the social links between author i and author j. For each pair of authors, we sample their connections with the following random function network model:</p><formula xml:id="formula_64">Q ij | ν ∼ F(ν i , ν j ) ,<label>(58)</label></formula><formula xml:id="formula_65">x ij | Q ij ∼ Bernoulli s(Q ij ) , for i = 1, . . . , A; j = 1, . . . , A ,<label>(59)</label></formula><p>where s(•) is the sigmoid function:</p><formula xml:id="formula_66">s(t) = 1 1 + e -t .<label>(60)</label></formula><p>By marginalising out F, we can write Q ∼ GP(ς, κ), where Q is a vectorised collection of Q ij . 6 ς denotes the mean vector and κ is the covariance matrix of the GP:</p><formula xml:id="formula_67">ς ij = Sim(ν i , ν j ) ,<label>(61)</label></formula><formula xml:id="formula_68">κ ij,i j = s 2 2 exp - Sim(ν i , ν j ) -Sim(ν i , ν j ) 2 2l 2 + σ 2 I(ij = i j ) ,<label>(62)</label></formula><p>where s, l and σ are the hyperparameters associated to the kernel. Sim(•, •) is a similarity function that has a range between 0 and 1, here chosen to be cosine similarity due to its ease of computation and popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Relationships with Other Models</head><p>The TNTM is related to many existing models after removing certain components of the model. When hashtags and the network components are removed, the TNTM is reduced to a nonparametric variant of the author topic model (ATM). Oppositely, if authorship information is discarded, the TNTM resembles the correspondence LDA <ref type="bibr">(Blei and Jordan, 2003)</ref>, although it differs in that it allows hashtags and words to be generated from a common vocabulary.</p><p>In contrast to existing parametric models, the network model in the TNTM provides possibly the most flexible way of network modelling via a nonparametric Bayesian prior (GP), following <ref type="bibr" target="#b52">Lloyd et al. (2012)</ref>. Different to <ref type="bibr" target="#b52">Lloyd et al. (2012)</ref>, we propose a new kernel function that fits our purpose better and achieves significant improvement over the original kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Representation and Model Likelihood</head><p>As with previous sections, we represent the TNTM using the CRP representation discussed in Section 3.2. However, since the PYP variables in the TNTM can have multiple parents, we extend the representation following <ref type="bibr">Du et al. (2012a)</ref>. The distinction is that we store 6. Q = (Q11, Q12, . . . , QAA) T , note that ς and κ follow the same indexing.</p><p>multiple tables counts for each PYP, to illustrate, t N →P k represents the number of tables in PYP N serving dish k that are contributed to the customer counts in PYP P, c P k . Similarly, the total table counts that contribute to P is denoted as</p><formula xml:id="formula_69">T N →P = k t N →P k . Note the number of tables in PYP N is t N k = P t N →P k</formula><p>, while the total number of tables is T N = P T N →P . We refer the readers to <ref type="bibr">Lim et al. (2013, Appendix B)</ref> for a detailed discussion. We use bold face capital letters to denote the set of all relevant lower case variables, for example, we denote W • = {W, Y} as the set of all words and hashtags; Z • = {Z, Z } as the set of all topic assignments for the words and the hashtags; T as the set of all table counts and C as the set of all customer counts; and we introduce Ξ as the set of all hyperparameters. By marginalising out the latent variables, we write down the model likelihood corresponding to the HPYP topic model in terms of the counts:</p><formula xml:id="formula_70">p(Z • , T, C | W • , Ξ) ∝ p(Z • , W • , T, C | Ξ) ∝ f (µ 0 )f (µ 1 ) A i=1 f (ν i ) K k=1 f (ψ k )f (ψ k ) f (γ) × D d=1 f (η d )f (θ d )f (θ d )g ρ θ d g ρ θ d |V| v=1 1 |V| t γ v ,<label>(63)</label></formula><p>where f (N ) is the modularised likelihood corresponding to node N , as defined by Equation ( <ref type="formula" target="#formula_22">16</ref>), and g(ρ) is the likelihood corresponding to the probability ρ that controls which parent node to send a customer to, defined as</p><formula xml:id="formula_71">g(ρ N ) = B λ N 0 + T N →P 0 , λ N 1 + T N →P 1 ,<label>(64)</label></formula><p>for N ∼ PYP α N , β N , ρ N P 0 + (1-ρ N )P 1 . Note that B(a, b) denotes the Beta function that normalises a Dirichlet distribution, defined as follows:</p><formula xml:id="formula_72">B(a, b) = Γ(a) Γ(b) Γ(a + b) .<label>(65)</label></formula><p>For the random function network model, the conditional posterior can be derived as</p><formula xml:id="formula_73">p(Q | X, ν, Ξ) ∝ p(X, Q | ν, Ξ) ∝ A i=1 A j=1 s(Q ij ) x ij 1 -s(Q ij ) 1-x ij × |κ| -1 2 exp - 1 2 (Q -ς) T κ -1 (Q -ς) .<label>(66)</label></formula><p>The full posterior likelihood is thus the product of the topic model posterior (Equation ( <ref type="formula" target="#formula_70">63</ref>)) and the network posterior (Equation ( <ref type="formula" target="#formula_73">66</ref>)):</p><formula xml:id="formula_74">p(Q, Z • , T, C | X, W • , Ξ) = p(Z • , T, C | W • , Ξ) p(Q | X, ν, Ξ) . (<label>67</label></formula><formula xml:id="formula_75">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performing Posterior Inference on the TNTM</head><p>In the TNTM, combining a GP with a HPYP makes its posterior inference non-trivial. Hence, we employ approximate inference by alternatively performing MCMC sampling on the HPYP topic model and the network model, conditioned on each other. For the HPYP topic model, we employ the flexible framework discussed in Section 3 to perform collapsed blocked Gibbs sampling. For the network model, we derive a Metropolis-Hastings (MH) algorithm based on the elliptical slice sampler <ref type="bibr" target="#b65">(Murray et al., 2010)</ref>. In addition, the authortopic distributions ν connecting the HPYP and the GP are sampled with an MH scheme since their posteriors do not follow a standard form. We note that the PYPs in this section can have multiple parents, so we extend the framework in Section 3 to allow for this.</p><p>The collapsed Gibbs sampling for the HPYP topic model in TNTM is similar to the procedure in Section 4, although there are two main differences. The first difference is that we need to sample the topics for both words and hashtags, each with a different conditional posterior compared to that of Section 4. While the second is due to the PYPs in TNTM can have multiple parents, thus an alternative to decrementing the counts is required. A detailed discussion on performing posterior inference and hyperparameter sampling is presented in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Twitter Data</head><p>For evaluation of the TNTM, we construct a tweet corpus from the Twitter 7 dataset (Yang  and Leskovec, 2011), 7 This corpus is queried using the hashtags #sport, #music, #finance, #politics, #science and #tech, chosen for diversity. We remove the non-English tweets with langid.py <ref type="bibr" target="#b54">(Lui and Baldwin, 2012)</ref>. We obtain the data on the followers network from <ref type="bibr" target="#b42">Kwak et al. (2010)</ref>. 8 However, note that this followers network data is not complete and does not contain information for all authors. Thus we filter out the authors that are not part of the followers network data from the tweet corpus. Additionally, we also remove authors who have written less than fifty tweets from the corpus. We name this corpus T6 since it is queried with six hashtags. It is consists of 240,517 tweets with 150 authors after filtering.</p><p>Besides the T6 corpus, we also use the tweet datasets described in <ref type="bibr" target="#b61">Mehrotra et al. (2013)</ref>. The datasets contains three corpora, each of them is queried with exactly ten query terms. The first corpus, named the Generic Dataset, are queried with generic terms. The second is named the Specific Dataset, which is composed of tweets on specific named entities. Lastly, the Events Dataset is associated with certain events. The datasets are mainly used for comparing the performance of the TNTM against the tweet pooling techniques in <ref type="bibr" target="#b61">Mehrotra et al. (2013)</ref>. We present a summary of the tweet corpora in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Experiments and Results</head><p>We consider several tasks to evaluate the TNTM. The first task involves comparing the TNTM with existing baselines on performing topic modelling on tweets. We also compare the TNTM with the random function network model on modelling the followers network. Next, we evaluate the TNTM with ablation studies, in which we perform comparison with 7. <ref type="url" target="http://snap.stanford.edu/data/twitter7.html">http://snap.stanford.edu/data/twitter7.html</ref> 8. <ref type="url" target="http://an.kaist.ac.kr/traces/WWW2010.html">http://an.kaist.ac.kr/traces/WWW2010.html</ref>  the TNTM itself but with each component taken away. Additionally, we evaluate the clustering performance of the TNTM, we compare the TNTM against the state-of-the-art tweets-pooling LDA method in <ref type="bibr" target="#b61">Mehrotra et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Experiment Settings</head><p>In all the following experiments, we vary the discount parameters α for the topic distributions µ 0 , µ 1 , ν i , η m , θ m , and θ m , we set α to 0.7 for the word distributions ψ, φ and γ to induce power-law behaviour <ref type="bibr" target="#b26">(Goldwater et al., 2011)</ref>. We initialise the concentration parameters β to 0.5, noting that they are learned automatically during inference, we set their hyperprior to Gamma(0.1, 0.1) for a vague prior. We fix the hyperparameters λ, s, l and σ to 1, as we find that their values have no significant impact on the model performance. 9</p><p>In the following evaluations, we run the full inference algorithm for 2,000 iterations for the models to converge. We note that the MH algorithm only starts after 1,000 iterations. We repeat each experiment five times to reduce the estimation error for the evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Goodness-of-fit Test</head><p>We compare the TNTM with the HDP-LDA and a nonparametric author-topic model (ATM) on fitting the text data (words and hashtags). Their performances are measured using perplexity on the test set (see Section 4.5.2). The perplexity for the TNTM, accounting for both words and hashtags, is</p><formula xml:id="formula_76">Perplexity(W • ) = exp - log p W • | ν, µ 1 , ψ, ψ D d=1 N d + M d ,<label>(68)</label></formula><p>where the likelihood p W</p><formula xml:id="formula_77">• | ν, µ 1 , ψ, ψ is broken into p W • | ν, µ 1 , ψ, ψ = D d=1 M d m=1 p(y dm | ν, µ 1 , ψ ) N d n=1 p(w dn | y d , ν, µ 1 , ψ) .<label>(69)</label></formula><p>9. We vary these hyperparameters over the range of 0.01 to 10 during testing. on perplexity is from modelling the hashtags, which suggests that the hashtag information is the most important for modelling tweets. Second to the hashtags, the authorship information is very important as well. Even though modelling the power-law behaviour is not that important for perplexity, we see that the improvement on the network log likelihood is best achieved by modelling the power-law. This is because the flexibility enables us to learn the author-topic distributions better, and thus allowing the TNTM to fit the network data better. This also suggests that the authors in the corpus tend to focus on a specific topic rather than having a wide interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.4">Document Clustering and Topic Coherence</head><p>Mehrotra <ref type="bibr">et al. (2013)</ref> shows that running LDA on pooled tweets rather than unpooled tweets gives significant improvement on clustering. In particular, they find that grouping tweets based on the hashtags provides most improvement. Here, we show that instead of resorting to such an ad hoc method, the TNTM can achieve a significantly better result on clustering. The clustering evaluations are measured with purity and normalised mutual information (NMI, see <ref type="bibr" target="#b57">Manning et al., 2008)</ref> described in 4.5.3. Since ground truth labels are unknown, we use the respective query terms as the ground truth for evaluations. Note that tweets that satisfy multiple labels are removed. Given the learned model, we assign a tweet to a cluster based on its dominant topic. We perform the evaluations on the Generic, Specific and Events datasets for comparison purpose. We note the lack of network information in these datasets, and thus we employ only the HPYP part of the TNTM. Additionally, since the purity can trivially be improved by increasing the number of clusters, we limit the maximum number of topics to twenty for a fair comparison. We present the results in Table <ref type="table" target="#tab_4">6</ref>. We can see that the TNTM outperforms the pooling method in all aspects except on the Specific dataset, where it achieves the same purity as the best pooling scheme. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.5">Automatic Topic Labelling</head><p>Traditionally, researchers assign a topic for each topic-word distribution manually by inspection. More recently, there have been attempts to label topics automatically in topic modelling. For instance, <ref type="bibr" target="#b44">Lau et al. (2011)</ref> use Wikipedia to extract labels for topics, and <ref type="bibr" target="#b60">Mehdad et al. (2013)</ref> use the entailment relations to select relevant phrases for topics. Here, we show that we can use hashtags to obtain good topic labels. In Table <ref type="table" target="#tab_5">7</ref>, we display the top words from the topic-word distribution ψ k for each topic k. Instead of manually assigning the topic labels, we display the top three hashtags from the topic-hashtag distribution ψ k . As we can see from Table <ref type="table" target="#tab_5">7</ref>, the hashtags appear suitable as topic labels. In fact, by empirically evaluating the suitability of the hashtags in representing the topics, we consistently find that, over 90 % of the hashtags are good candidates for the topic labels. Moreover, inspecting the topics show that the major hashtags coincide with the query terms used in constructing the T6 dataset, which is to be expected. This verifies that the TNTM is working properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this article, we proposed a topic modelling framework utilising PYPs, for which their realisation is a probability distribution or another stochastic process of the same type. In particular, for the purpose of performing inference, we described the CRP representation for the PYPs. This allows us to propose a single framework, discussed in Section 3, to implement these topic models, where we modularise the PYPs (and other variables) into blocks that can be combined to form different models. Doing so enables significant time to be saved on implementation of the topic models.</p><p>We presented a general HPYP topic model, that can be seen as a generalisation to the HDP-LDA <ref type="bibr" target="#b78">(Teh and Jordan, 2010)</ref>. The HPYP topic model is represented using a Chinese Restaurant Process (CRP) metaphor <ref type="bibr" target="#b78">(Teh and Jordan, 2010;</ref><ref type="bibr" target="#b5">Blei et al., 2010;</ref><ref type="bibr" target="#b14">Chen et al., 2011)</ref>, and we discussed how the posterior likelihood of the HPYP topic model can be modularised. We then detailed the learning algorithm for the topic model in the modularised form.</p><p>We applied our HPYP topic model framework on Twitter data and proposed the Twitter-Network Topic model (TNTM). The TNTM models the authors, text, hashtags, and the authors-follower network in an integrated manner. In addition to HPYP, the TNTM employs the Gaussian process (GP) for the network modelling. The main suggested use of the TNTM is for content discovery on social networks. Through experiments, we show that jointly modelling of the text content and the network leads to better model fitting as compared to modelling them separately. Results on the qualitative analysis show that the learned topics and the authors' topics are sound. Our experiments suggest that incorporating more auxiliary information leads to better fitting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Future Research</head><p>For future work on TNTM, it would be interesting to apply TNTM to other types of data, such as blogs and news feeds. We could also use TNTM for other applications. such as hashtag recommendation and content suggestion for new Twitter users. Moreover, we could extend TNTM to incorporate more auxiliary information: for instance, we can model the location of tweets and the embedded multimedia contents such as URL, images and videos. Another interesting source of information would be the path of retweeted content.</p><p>Another interesting area of research is the combination of different kinds of topic models for a better analysis. This allows us to transfer learned knowledge from one topic model to another. The work on combining LDA has already been looked at by <ref type="bibr" target="#b75">Schnober and Gurevych (2015)</ref>, however, combining other kinds of topic models, such as nonparametric ones, is unexplored.</p><p>(z dn = k) and c θ d k for hashtag y dm (z dm = k). Decrementing the customer count may or may not decrement the respective table count. However, if the table count is decremented, then we would decrement the customer count of the parent PYP. This is relatively straight forward in Section 4.1 since the PYPs have only one parent. Here, when a PYP N has multiple parents, we would sample for one of its parent PYPs and decrement the table count corresponding to the parent PYP. Although not the same, the rationale of this procedure follows Section 4.1.</p><p>We explain in more details below. When the customer count c N k is decremented, we introduce an auxiliary variable u N k that indicates which parent of N to remove a table from, or none at all. The sample space for u N k is the P parent nodes P 1 , . . . , P P of N , plus ∅. When u N k is equal to P i , we decrement the table count t N →P i k and subsequently decrement the customer count c P i k in node P i . If u N k equals to ∅, we do not decrement any table count. The process is repeated recursively as long as a customer count is decremented, that is, we stop when u N k = ∅. The value of u N k is sampled as follows:</p><formula xml:id="formula_78">p u N k = t N →P i k /c N k if u N k = P i 1 -P i p u N k = P i if u N k = ∅ .<label>(70)</label></formula><p>To illustrate, when a word w dn (with topic z dn ) is removed, we decrement c</p><p>θ d z dn , that is, c θ d z dn becomes c θ d z dn -1. We then determine if this word contributes to any table in node θ d by sampling u θ d z dn from Equation (70). If u θ d z dn = ∅, we do not decrement any table count and proceed with the next step in Gibbs sampling; otherwise, u θ d z dn can either be θ d or η d , in these cases, we would decrement t θ d →u θ d z dn z dn and c u θ d z dn</p><p>z dn , and continue the process recursively. We present the decrementing process in Algorithm 2. To remove a word w dn during inference, we would need to decrement the counts contributed by w dn (and z dn ). For the topic side, we decrement the counts associated with node N = θ d with group k = z dn using Algorithm 2. While for the vocabulary side, we decrement the counts associated with the node N = ψ z dn with group k = w dn . The effect of the word on the other PYP variables are implicitly considered through recursion.</p><p>Note that the procedure to decrementing a hashtag y dm is similar, in this case, we decrement the counts for N = θ d with k = z dm (topic side), then decrement the counts for N = ψ z dm with k = y dm (vocabulary side).</p><p>A.2 Sampling a New Topic for a Word or a Hashtag</p><p>After decrementing, we sample a new topic for the word or the hashtag. The sampling process follows the procedure discussed in Section 4.2, but with different conditional posteriors (for both the word and the hashtag). The full conditional posterior probability for the collapsed blocked Gibbs sampling can be derived easily. For instance, the conditional posterior for sampling the topic z dn of word w dn is</p><formula xml:id="formula_79">p(z dn , T, C | Z •-dn , W • , T -dn , C -dn , Ξ) = p(Z • , T, C | W • , Ξ) p(Z • -dn , T -dn , C -dn | W • , Ξ)<label>(71)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical model of the HPYP topic model. It is an extension to LDA by allowing the probability vectors to be modelled by PYPs instead of the Dirichlet distributions. The area on the left of the graphical model (consists of µ, ν and θ)is usually referred as topic side, while the right hand side (with γ and φ) is called the vocabulary side. The word node denoted by w dn is observed. The notations are defined in Table1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the Chinese restaurant process representation. The customers are represented by the circles while the tables are represented by the rectangles.The dishes are the symbols in the middle of the rectangles, here they are denoted by the sunny symbol and the cloudy symbol. In this illustration, we know the number of customers corresponds to each table, for example, the green table is occupied by three customers. Also, since Restaurant 1 is the parent of Restaurant 2, the tables in Restaurant 2 are treated as the customers for Restaurant 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of the Chinese restaurant with the table counts representation.Here the setting is the same as Figure2but the seating arrangement of the customers are "forgotten" and only the table and customer counts are recorded. Thus, we only know that there are three sunny tables in Restaurant 2, with a total of nine customers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graphical model for the Twitter-Network Topic Model (TNTM) composed of a HPYP topic model (region b ) and a GP based random function network model (region a ).The author-topic distributions ν serve to link the two together. Each tweet is modelled with a hierarchy of document-topic distributions denoted by η, θ , and θ, where each is attuned to the whole tweet, the hashtags, and the words, in that order. With their own topic assignments z and z, the hashtags y and the words w are separately modelled. They are generated from the topic-hashtag distributions ψ and the topic-word distributions ψ respectively. The variables µ 0 , µ 1 and γ are priors for the respective PYPs. The connections between the authors are denoted by x, modelled by random function F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>List of variables for the HPYP topic model used in this section.</figDesc><table><row><cell>Variable</cell><cell>Name</cell><cell>Description</cell></row><row><cell>z dn</cell><cell>Topic</cell><cell>Topical label for word w dn .</cell></row><row><cell>w dn</cell><cell>Word</cell><cell>Observed word or phrase at position n in document d.</cell></row><row><cell>φ k</cell><cell>Topic-word distribution</cell><cell>Probability distribution in generating words for topic k.</cell></row><row><cell>θ d</cell><cell>Document-topic distribution</cell><cell>Probability distribution in generating topics for document d.</cell></row><row><cell>γ</cell><cell>Global word distribution</cell><cell>Word prior for φ k .</cell></row></table><note><p>ν Global topic distribution Topic prior for θ d . µ Global topic distribution Topic prior for ν. α N Discount Discount parameter for PYP N . β N Concentration Concentration parameter for PYP N . H N Base distribution Base distribution for PYP N . c N k Customer count Number of customers having dish k in restaurant N . t N k Table count Number of tables serving dish k in restaurant N .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of the datasets used in this section, showing the number of tweets (D), authors (A), unique word tokens (|V|), and the average number of words and hashtags in each tweet. The T6 dataset is queried with six different hashtags and thus has a higher number of hashtags per tweet. We note that there is a typo on the number of tweets for the Events Dataset in<ref type="bibr" target="#b61">Mehrotra et al. (2013)</ref>, the correct number is 107,128.</figDesc><table><row><cell cols="6">Dataset Tweets Authors Vocabulary Words/Tweet Hashtags/Tweet</cell></row><row><cell>T6</cell><cell>240 517</cell><cell>150</cell><cell>5 343</cell><cell>6.35</cell><cell>1.34</cell></row><row><cell cols="3">Generic 359 478 213 488</cell><cell>14 581</cell><cell>6.84</cell><cell>0.10</cell></row><row><cell cols="3">Specific 214 580 116 685</cell><cell>15 751</cell><cell>6.31</cell><cell>0.25</cell></row><row><cell>Events</cell><cell>107 128</cell><cell>67 388</cell><cell>12 765</cell><cell>5.84</cell><cell>0.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Clustering evaluations of the TNTM against the LDA with different pooling schemes. Note that higher purity and NMI indicate better performance. The results for the different pooling methods are obtained from Table4in<ref type="bibr" target="#b61">Mehrotra et al. (2013)</ref>. The TNTM achieves better performance on the purity and the NMI for all datasets except for the Specific dataset, where it obtains the same purity score as the best pooling method.</figDesc><table><row><cell>Method/Model</cell><cell></cell><cell>Purity</cell><cell></cell><cell></cell><cell>NMI</cell><cell></cell></row><row><cell cols="7">Data Generic Specific Events Generic Specific Events</cell></row><row><cell>No pooling</cell><cell>0.49</cell><cell>0.64</cell><cell>0.69</cell><cell>0.28</cell><cell>0.22</cell><cell>0.39</cell></row><row><cell>Author</cell><cell>0.54</cell><cell>0.62</cell><cell>0.60</cell><cell>0.24</cell><cell>0.17</cell><cell>0.41</cell></row><row><cell>Hourly</cell><cell>0.45</cell><cell>0.61</cell><cell>0.61</cell><cell>0.07</cell><cell>0.09</cell><cell>0.32</cell></row><row><cell>Burstwise</cell><cell>0.42</cell><cell>0.60</cell><cell>0.64</cell><cell>0.18</cell><cell>0.16</cell><cell>0.33</cell></row><row><cell>Hashtag</cell><cell>0.54</cell><cell>0.68</cell><cell>0.71</cell><cell>0.28</cell><cell>0.23</cell><cell>0.42</cell></row><row><cell>TNTM</cell><cell>0.66</cell><cell>0.68</cell><cell>0.79</cell><cell>0.43</cell><cell>0.31</cell><cell>0.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Topical analysis on the T6 dataset with the TNTM, which displays the top three hashtags and the top n words on six topics. Instead of manually assigning a topic label to the topics, we find that the top hashtags can serve as the topic labels.</figDesc><table><row><cell>Topic</cell><cell>Top Hashtags</cell><cell>Top Words</cell></row><row><cell cols="2">Topic 1 finance, money, economy</cell><cell>finance, money, bank, marketwatch, stocks, china, group, shares, sales</cell></row><row><cell cols="2">Topic 2 politics, iranelection, tcot</cell><cell>politics, iran, iranelection, tcot, tlot, topprog, obama, musiceanewsfeed</cell></row><row><cell>Topic 3</cell><cell>music, folk, pop</cell><cell>music, folk, monster, head, pop, free, indie, album, gratuit, dernier</cell></row><row><cell cols="2">Topic 4 sports, women, asheville</cell><cell>sports, women, football, win, game, top, world, asheville, vols, team</cell></row><row><cell>Topic 5</cell><cell>tech, news, jobs</cell><cell>tech, news, jquery, jobs, hiring, gizmos, google, reuters</cell></row><row><cell>Topic 6</cell><cell>science, news, biology</cell><cell>science, news, source, study, scientists, cancer, researchers, brain, biology, health</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c 2016 Lim, Buntine, Chen and Du. http://dx.doi.org/10.1016/j.ijar.2016.07.007</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors like to thank <rs type="person">Shamin Kinathil</rs>, the editors, and the anonymous reviewers for their valuable feedback and comments. NICTA is funded by the <rs type="funder">Australian Government</rs> through the <rs type="funder">Department of Communications</rs> and the <rs type="funder">Australian Research Council</rs> through the <rs type="programName">ICT Centre of Excellence Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HFmG3TQ">
					<orgName type="program" subtype="full">ICT Centre of Excellence Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>author i, and a miscellaneous topic distribution µ 1 to capture topics that deviate from the authors' usual topics:</p><p>For each tweet d, given the author-topic distribution ν and the observed author a d , we sample the document-topic distribution η d , as follows:</p><p>Next, we generate the topic distributions for the observed hashtags (θ d ) and the observed words (θ d ), following the technique used in the adaptive topic model <ref type="bibr">(Du et al., 2012a)</ref>. We explicitly model the influence of hashtags to words, by generating the words conditioned on the hashtags. The intuition comes from hashtags being the themes of a tweet, and they drive the content of the tweet. Specifically, we sample the mixing proportions ρ θ d , which control the contribution of η d and µ 1 for the base distribution of θ d , and then generate θ d given ρ θ d :</p><p>We set θ d and η d as the parent distributions of θ d . This flexible configuration allows us to investigate the relationship between θ d , θ d and η d , that is, we can examine if θ d is directly determined by η d , or through the θ d . The mixing proportions ρ θ d and the topic distribution θ d is generated similarly:</p><p>The hashtags and words are then generated in a similar fashion to LDA. For the m-th hashtag in tweet d, we sample a topic z dm and the hashtag y dm by</p><p>where M d is the number of seen hashtags in tweet d. While for the n-th word in tweet d, we sample a topic z dn and the word w dn by</p><p>where N d is the number of observed words in tweet d. We note that all above α, β and λ are the hyperparameters of the model. We show the importance of the above modelling with ablation studies in Section 5.6. Although the HPYP topic model may seem complex, it is a simple network of PYP nodes since all distributions on the probability vectors are modelled by the PYP. We also compare the TNTM against the original random function network model in terms of the log likelihood of the network data, given by log p(X | ν). We present the comparison of the perplexity and the network log likelihood in Table <ref type="table">4</ref>. We note that for the network log likelihood, the less negative the better. From the result, we can see that the TNTM achieves a much lower perplexity compared to the HDP-LDA and the nonparametric ATM. Also, the nonparametric ATM is significantly better than the HDP-LDA. This clearly shows that using more auxiliary information gives a better model fitting. Additionally, we can also see that jointly modelling the text and network data leads to a better modelling on the followers network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Ablation Test</head><p>Next, we perform an extensive ablation study with the TNTM. The components that are tested in this study are (1) authorship, (2) hashtags, (3) PYP µ 1 , (4) connection between PYP θ d and θ d , and (5) power-law behaviour on the PYPs. We compare the full TNTM against variations in which each component is ablated. Table <ref type="table">5</ref> presents the test set perplexity and the network log likelihood of these models, it shows significant improvements of the TNTM over the ablated models. From this, we see that the greatest improvement which can then be easily decomposed into simpler form (see discussion in Section 4.2) using Equation ( <ref type="formula">63</ref>). Here, the superscript -dn indicates the word w dn and the topic z dn are removed from the respective sets. Similarly, the conditional posterior probability for sampling the topic z dm of hashtag y dm can be derived as</p><p>where the superscript -dm signals the removal of the hashtag y dm and the topic z dm . As in Section 4.2, we compute the posterior for all possible changes to T and C corresponding to the new topic (for z dn or z dm ). We then sample the next state using a Gibbs sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Estimating the Probability Vectors of the PYPs with Multiple Parents</head><p>Following Section 4.4, we estimate the various probability distributions of the PYPs by their posterior means. For a PYP N with a single PYP parent P 1 , as discussed in Section 4.4, we can estimate its probability vector N = ( N1 , . . . , NK ) as</p><p>which lets one analyse the probability vectors in a topic model using recursion. Unlike the above, the posterior mean is slightly more complicated for a PYP N that has multiple PYP parents P 1 , . . . , P P . Formally, we define the PYP N as</p><p>where the mixing proportion ρ N = (ρ N 1 , . . . , ρ N P ) follows a Dirichlet distribution with parameter λ N = (λ N 1 , . . . , λ N P ):</p><p>Before we can estimate the probability vector, we first estimate the mixing proportion with its posterior mean given the customer counts and table counts:</p><p>Then, we can estimate the probability vector N = ( N1 , . . . , NK ) by</p><p>where ĤN = ( ĤN 1 , . . . , ĤN K ) is the expected base distribution:</p><p>With these formulations, all the topic distributions and the word distributions in the TNTM can be reconstructed from the customer counts and table counts. For instance, the author-topic distribution ν i of each author i can be determined recursively by first estimating the topic distribution µ 0 . The word distributions for each topic are similarly estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 MH Algorithm for the Random Function Network Model</head><p>Here, we discuss how we learn the topic distributions µ 0 and ν from the random function network model. We configure the MH algorithm to start after running one thousand iterations of the collapsed blocked Gibbs sampler, this is to we can quickly initialise the TNTM with the HPYP topic model before running the full algorithm. In addition, this allows us to demonstrate the improvement to the TNTM due to the random function network model.</p><p>To facilitate the MH algorithm, we have to represent the topic distributions µ 0 and ν explicitly as probability vectors, that is, we do not store the customer counts and table counts for µ 0 and ν after starting the MH algorithm. In the MH algorithm, we propose new samples for µ 0 and ν, and then accept or reject the samples. The details for the MH algorithm is as follow.</p><p>In each iteration of the MH algorithm, we use the Dirichlet distributions as proposal distributions for µ 0 and ν:</p><p>These proposed µ 0 and ν are sampled given the their previous values, and we note that the first µ 0 and ν are computed using the technique discussed in A.3. These proposed samples are subsequently used to sample Q new . We first compute the quantities ς new and κ new using the proposed µ new 0 and ν new with Equation (61) and Equation ( <ref type="formula">62</ref>). Then we sample Q new given ς new and κ new using the elliptical slice sampler (see <ref type="bibr" target="#b65">Murray et al., 2010)</ref>: 5. Accept or reject the samples with acceptance probability from Equation ( <ref type="formula">82</ref>).</p><p>Finally, we compute the acceptance probability A = min(A, 1), where</p><p>and we define f * (µ 0 | ν, T) and f * (ν | T) as</p><p>The f * (•) corresponds to the topic model posterior of the variables µ 0 and ν after we represent them as probability vectors explicitly. Note that we treat the acceptance probability A as 1 when the expression in Equation ( <ref type="formula">82</ref>) evaluates to more than 1. We then accept the proposed samples with probability A, if the sample are not accepted, we keep the respective old values. This completes one iteration of the MH scheme. We summarise the MH algorithm in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Hyperparameter Sampling</head><p>We sample the hyperparameters β using an auxiliary variable sampler while leaving α fixed. We note that the auxiliary variable sampler for PYPs that have multiple parents are identical to that of PYPs with single parent, since the sampler only used the total customer counts C N and the total table counts T N for a PYP N . We refer the readers to Section 4.3 for details.</p><p>We would like to point out that hyperparameter sampling is performed for all PYPs in TNTM for the first one thousand iterations. After that, as µ 0 and ν are represented as probability vectors explicitly, we only sample the hyperparameters for the other PYPs Algorithm 4 Full inference algorithm for the TNTM.</p><p>1. Initialise the HPYP topic model by assigning random topic to the latent topic z dn associated with each word w dn , and to the latent topic z dm associated with each hashtag y dm . Then update all the relevant customer counts C and table counts T.</p><p>2. For each word w dn in each document d, perform the following:</p><p>(a) Decrement the counts associated with w dn (see A.1).</p><p>(b) Blocked sample a new topic for z dn and corresponding customer counts C and table counts T (with Equation ( <ref type="formula">71</ref>)).</p><p>(c) Update (increment counts) the topic model based on the sample.</p><p>3. For each hashtag y dm in each document d, perform the following:</p><p>(a) Decrement the counts associated with y dm (see A.1).</p><p>(b) Blocked sample a new topic for z dn and corresponding customer counts C and table counts T (with Equation ( <ref type="formula">72</ref>)).</p><p>(c) Update (increment counts) the topic model based on the sample.</p><p>4. Sample the hyperparameter β N for each PYP N (see A.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Repeat</head><p>Steps 2 -4 for 1,000 iterations.</p><p>6. Alternatingly perform the MH algorithm (Algorithm 3) and the collapsed blocked Gibbs sampler conditioned on µ 0 and ν.</p><p>7. Sample the hyperparameter β N for each PYP N except for µ 0 and ν.</p><p>8. Repeat Steps 6 -7 until the model converges or when a fix number of iterations is reached.</p><p>(except µ 0 and ν). We note that sampling the concentration parameters allows the topic distributions of each author to vary, that is, some authors have few very specific topics and some other authors can have a wider range of topics. For simplicity, we fix the kernel hyperparameters s, l and σ to 1. Additionally, we also make the priors for the mixing proportions uninformative by setting the λ to 1. We summarise the full inference algorithm for the TNTM in Algorithm 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thermodynamic limits of macroeconomic or financial models: One-and two-parameter Poisson-Dirichlet models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics and Control</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="84" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent IBP compound Dirichlet allocation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="333" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social media, reputation risk and ambient publicity management</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strategy &amp; Leadership</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How noisy social media text, how diffrnt [sic] social media sources?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The nested Chinese Restaurant Process and Bayesian nonparametric inference of topic hierarchies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling annotated data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR 2003</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR 2003<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML 2006</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML 2006<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social media as beat</title>
		<author>
			<persName><forename type="first">M</forename><surname>Broersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Practice</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="419" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Truly nonparametric online variational inference for hierarchical Dirichlet processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Rostrevor, Northern Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2699" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Bayesian view of the Poisson-Dirichlet process</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1007.0296v2</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experiments with non-parametric topic models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GaP: a factor model for discrete data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR 2014</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR 2014<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sampling table configurations for the hierarchical Poisson-Dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases -Volume Part I, ECML 2011</title>
		<meeting>the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases -Volume Part I, ECML 2011<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="296" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Who interacts on the Web?: The intersection of users&apos; personality and social media use</title>
		<author>
			<persName><forename type="first">T</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Hinsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>De Zúñiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="253" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Non-parametric Bayesian methods for structured topic models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Canberra, Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Australian National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modelling sequential text with an adaptive topic model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="535" to="545" />
		</imprint>
	</monogr>
	<note>EMNLP-CoNLL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="503" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What to do about bad language on the internet</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the ACL: Human Language Technologies, NAACL-HLT 2013</title>
		<meeting>the 2013 Conference of the North American Chapter of the ACL: Human Language Technologies, NAACL-HLT 2013<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="359" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Erosheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<title level="m">Bayesian Mixed Membership Models for Soft Clustering and Classification</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="11" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian non-parametric inference for species variety with a two-parameter Poisson-Dirichlet process prior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lijoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Prünster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="993" to="1008" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some nonparametric problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis, Third Edition. Chapman &amp; Hall/CRC Texts in Statistical Science</title>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpolating between types and tokens by estimating power-law generators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18, NIPS 2005</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Producing power-law distributions and damping word frequencies with two-stage language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2335" to="2382" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reversible jump Markov chain Monte Carlo computation and Bayesian model determination</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="711" to="732" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Delayed rejection in reversible jump Metropolis-Hastings</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1053" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monte Carlo sampling methods using Markov chains and their applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Incorporating sentiment prior knowledge for weakly supervised sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Hjort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
		<title level="m">Bayesian Nonparametrics</title>
		<meeting><address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 1999</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 1999<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gibbs sampling methods for stick-breaking priors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishwaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">453</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Statistical Methods for Speech Recognition</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transferring topical knowledge from auxiliary long texts for short text clustering</title>
		<author>
			<persName><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM 2011</title>
		<meeting>the 20th ACM International Conference on Information and Knowledge Management, CIKM 2011<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="775" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech &amp; Language Processing</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Classifying microblogs for disasters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Australasian Document Computing Symposium</title>
		<meeting>the 18th Australasian Document Computing Symposium<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context sensitive topic models for author influence in document networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence -Volume Three</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence -Volume Three<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="2274" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dirichlet process with mixed random measures: A nonparametric topic model for labeled data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning, ICML 2012</title>
		<meeting>the 29th International Conference on Machine Learning, ICML 2012<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="727" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">I&apos;m eating a sandwich in Glasgow&quot;: Modeling locations with tweets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kinsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Search and Mining User-generated Contents, SMUC 2011</title>
		<meeting>the 3rd International Workshop on Search and Mining User-generated Contents, SMUC 2011<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What is Twitter, a social network or a news media?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW 2010</title>
		<meeting>the 19th International Conference on World Wide Web, WWW 2010<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Handbook of Latent Semantic Analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kintsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Lawrence Erlbaum</publisher>
			<pubPlace>Mahwah, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic labelling of topic models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the ACL: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1536" to="1545" />
		</imprint>
	</monogr>
	<note>ACL-HLT 2011</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Nonparametric Bayesian Topic Modelling with Auxiliary Data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Canberra, Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Australian National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Twitter Opinion Topic Model: Extracting product opinions from tweets by leveraging hashtags and sentiment lexicon</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1319" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Twitter-Network Topic Model: A full Bayesian treatment for social network and text modeling</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems: Topic Models Workshop, NIPS Workshop 2013</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A phrase-discovering topic model using hierarchical Pitman-Yor processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Stipicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="214" to="222" />
		</imprint>
	</monogr>
	<note>EMNLP-CoNLL</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">427</biblScope>
			<biblScope unit="page" from="958" to="966" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Text summarisation in progress: A literature review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random function priors for exchangeable arrays with applications to graphs and relational data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Orbanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25, NIPS 2012</title>
		<meeting><address><addrLine>Rostrevor, Northern Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="998" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Low</surname></persName>
		</author>
		<title level="m">Introductory Computer Vision and Image Processing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">langid.py: An off-the-shelf language identification tool</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations, ACL 2012</title>
		<meeting>the ACL 2012 System Demonstrations, ACL 2012<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">WinBUGS -a Bayesian modelling framework: Concepts, structure, and extensibility</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="337" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Introduction to image processing and computer vision</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Mai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Hanoi, Vietnam</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute of Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Challenges in developing opinion mining tools for social media</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of @NLP can u tag #user generated content, LREC Workshop 2012</title>
		<meeting>@NLP can u tag #user generated content, LREC Workshop 2012<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Towards topic labeling with phrase entailment and aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the ACL: Human Language Technologies, NAACL-HLT 2013</title>
		<meeting>the 2013 Conference of the North American Chapter of the ACL: Human Language Technologies, NAACL-HLT 2013<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving LDA topic models for microblogs via tweet pooling and automatic labeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
	<note>SIGIR 2013</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Topic sentiment mixture: Modeling facets and opinions in weblogs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW 2007</title>
		<meeting>the 16th International Conference on World Wide Web, WWW 2007<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fast computing machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On Metropolis-Hastings algorithms with delayed rejection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metron -International Journal of Statistics</title>
		<imprint>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="231" to="241" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>LIX</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Elliptical slice sampling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010<address><addrLine>Brookline, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Microtome Publishing</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An Atlas of Functions: With Equator, the Atlas Function Calculator</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Oldham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spanier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2009. 2008</date>
			<publisher>Springer Science and Business Media</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Opinion mining and sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pitman</surname></persName>
		</author>
		<title level="m">Combinatorial Stochastic Processes</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="855" to="900" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Distributed Statistical Computing</title>
		<meeting>the 3rd International Workshop on Distributed Statistical Computing<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI 2004</title>
		<meeting>the 20th Conference on Uncertainty in Artificial Intelligence, UAI 2004<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Practical collapsed variational Bayes inference for hierarchical Dirichlet process</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Topic models with power-law using Pitman-Yor process</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="673" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Combining topic models for corpus exploration: Applying LDA for complex corpus research tasks in a digital humanities project</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schnober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Workshop on Topic Models: Post-Processing and Applications, TM 2015</title>
		<meeting>the 2015 Workshop on Topic Models: Post-Processing and Applications, TM 2015<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Twitter for health -building a social media search engine to better understand and curate laypersons&apos; personal experiences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Mining of Web-based Medical Content</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>De Gruyter</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="133" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A Bayesian interpretation of interpolated Kneser-Ney</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno>TRA2/06</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian nonparametric models with applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Nonparametrics, chapter 5</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hierarchical Dirichlet Processes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Collapsed variational inference for HDP</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<meeting><address><addrLine>Rostrevor, Northern Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1481" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Citation author topic model in expert search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING 2010</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, COLING 2010<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1265" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Handbook on statistical distributions for experimentalists</title>
		<author>
			<persName><forename type="first">C</forename><surname>Walck</surname></persName>
		</author>
		<idno>SUF-PFY/96-01</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>University of Stockholm, Sweden</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009b. 2009</date>
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Online variational inference for the hierarchical Dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Brookline, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Microtome Publishing</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="752" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Topic sentiment analysis in Twitter: A graph-based hashtag sentiment classification approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM 2011</title>
		<meeting>the 20th ACM International Conference on Information and Knowledge Management, CIKM 2011<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011b</date>
			<biblScope unit="page" from="1031" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><surname>Acm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2006</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2006<address><addrLine>New York, NY, USA; Brookline, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Microtome Publishing</publisher>
			<date type="published" when="2006">2006. 2009. 2009</date>
			<biblScope unit="page" from="607" to="614" />
		</imprint>
	</monogr>
	<note>LDA-based document models for ad-hoc retrieval Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Patterns of temporal variation in online media</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM 2011</title>
		<meeting>the Fourth ACM International Conference on Web Search and Data Mining, WSDM 2011<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Comparing Twitter and traditional media using topic models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR 2011</title>
		<meeting>the 33rd European Conference on Advances in Information Retrieval, ECIR 2011<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
