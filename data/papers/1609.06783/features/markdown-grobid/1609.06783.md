# Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes

## Abstract

## 

The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.

## Introduction

We live in the information age. With the Internet, information can be obtained easily and almost instantly. This has changed the dynamic of information acquisition, for example, we can now (1) attain knowledge by visiting digital libraries, (2) be aware of the world by reading news online, (3) seek opinions from social media, and (4) engage in political debates via web forums. As technology advances, more information is created, to a point where it is infeasible for a person to digest all the available content. To illustrate, in the context of a healthcare database (PubMed), the number of entries has seen a growth rate of approximately 3,000 new entries per day in the ten-year period from 2003 to 2013 [(Suominen et al., 2014)](#b76). This motivates the use of machines to automatically organise, filter, summarise, and analyse the available data for the users. To this end, researchers have developed various methods, which can be broadly categorised into computer vision [(Low, 1991;](#b53)[Mai, 2010)](#b56), speech recognition [(Rabiner and Juang, 1993;](#b70)[Jelinek, 1997)](#b35), and natural language processing (NLP, [Manning and Schütze, 1999;](#b58)[Jurafsky and Martin, 2000)](#b37). This article focuses on text analysis within NLP.

In text analytics, researchers seek to accomplish various goals, including sentiment analysis or opinion mining [(Pang and Lee, 2008;](#)[Liu, 2012)](#b49), information retrieval [(Manning et al., 2008)](#b57), text summarisation [(Lloret and Palomar, 2012)](#b51), and topic modelling [(Blei, 2012)](#b4). To illustrate, sentiment analysis can be used to extract digestible summaries or reviews on products and services, which can be valuable to consumers. On the other hand, topic models attempt to discover abstract topics that are present in a collection of text documents.

Topic models were inspired by latent semantic indexing (LSI, [Landauer et al., 2007](#b43)) and its probabilistic variant, probabilistic latent semantic indexing (pLSI), also known as the probabilistic latent semantic analysis [(pLSA, Hofmann, 1999)](#). Pioneered by [Blei et al. (2003)](#), latent Dirichlet allocation (LDA) is a fully Bayesian extension of pLSI, and can be considered the simplest Bayesian topic model. The LDA is then extended to many different types of topic models. Some of them are designed for specific applications [(Wei and Croft, 2006;](#b87)[Mei et al., 2007)](#b62), some of them model the structure in the text [(Blei and Lafferty, 2006;](#b7)[Du, 2012)](#b16), while some incorporate extra information in their modelling [(Ramage et al., 2009;](#b71)[Jin et al., 2011)](#b36).

On the other hand, due to the well known correspondence between the Gamma-Poisson family of distributions and the Dirichlet-multinomial family, Gamma-Poisson factor models [(Canny, 2004)](#b13) and their nonparametric extensions, and other Poisson-based variants of non-negative matrix factorisation (NMF) form a methodological continuum with topic models. These NMF methods are often applied to text, however, we do not consider these methods here. This article will concentrate on topic models that take into account additional information. This information can be auxiliary data (or metadata) that accompany the text, such as keywords (or tags), dates, authors, and sources; or external resources like word lexicons. For example, on Twitter, a popular social media platform, its messages, known as tweets, are often associated with several metadata like location, time published, and the user who has written the tweet. This information is often utilised, for instance, [Kinsella et al. (2011)](#b41) model tweets with location data, while [Wang et al. (2011b)](#b86) use hashtags for sentiment classification on tweets. On the other hand, many topic models have been designed to perform bibliographic analysis by using auxiliary information. Most notable of these is the author-topic model [(ATM, Rosen-Zvi et al., 2004)](#), which, as its name suggests, incorporates authorship information. In addition to authorship, the Citation Author Topic model [(Tu et al., 2010)](#b81) and the Author Cite Topic Model [(Kataria et al., 2011](#b39)) make use of citations to model research publications. There are also topic models that employ external resources to improve modelling. For instance, [He (2012)](#b30) and [Lim and Buntine (2014)](#b46) incorporate a sentiment lexicon as prior information for a weakly supervised sentiment analysis.

Independent to the use of auxiliary data, recent advances in nonparametric Bayesian methods have produced topic models that utilise nonparametric Bayesian priors. The simplest examples replace Dirichlet distributions by the Dirichlet process (DP, [Ferguson, 1973)](#b22). The simplest is hierarchical Dirichlet process LDA (HDP-LDA) proposed by [Teh et al. (2006)](#b79) that replaces just the document by topic matrix in LDA. One can further extend topic models by using the Pitman-Yor process (PYP, [Ishwaran and James, 2001](#b34)) that gen-eralises the DP, by replacing the second Dirichlet distribution which generates the topic by word matrix in LDA. This includes the work of [Sato and Nakagawa (2010)](#b74), [Du et al. (2012b)](#), [Lindsey et al. (2012)](#b48), among others. Like the HDP, the PYPs can be stacked to form hierarchical Pitman-Yor processes (HPYP), which are used in more complex models. Another fully nonparametric extension to topic modelling uses the Indian buffet process [(Archambeau et al., 2015)](#b1) to sparsify both the document by topic matrix and the topic by word matrix in LDA.

Advantages of employing nonparametric Bayesian methods with topic models is the ability to estimate the topic and word priors and to infer the number of clusters 1 from the data. Using the PYP also allows the modelling of the power-law property exhibited by natural languages [(Goldwater et al., 2005)](#b25). These touted advantages have been shown to yield significant improvements in performance [(Buntine and Mishra, 2014)](#b12). However, we note the best known approach for learning with hierarchical Dirichlet (or [Pitman-Yor)](#) processes is to use the Chinese restaurant franchise [(Teh and Jordan, 2010)](#b78). Because this requires dynamic memory allocation to implement the hierarchy, there has been extensive research in attempting to efficiently implement just the HDP-LDA extension to LDA mostly based around variational methods [(Teh et al., 2008;](#b80)[Wang et al., 2011a;](#)[Bryant and Sudderth, 2012;](#b10)[Sato et al., 2012;](#b73)[Hoffman et al., 2013)](#b32). Variational methods have rarely been applied to more complex topic models, as we consider here, and unfortunately Bayesian nonparametric methods are gaining a reputation of being difficult to use. A newer collapsed and blocked Gibbs sampler [(Chen et al., 2011)](#b14) has been shown to generally outperform the variational methods as well as the original Chinese restaurant franchise in both computational time and space and in some standard performance metrics [(Buntine and Mishra, 2014)](#b12). Moreover, the technique does appear suitable for more complex topic models, as we consider here.

This article, 2 extending the algorithm of [Chen et al. (2011)](#b14), shows how to develop fully nonparametric and relatively efficient Bayesian topic models that incorporate auxiliary information, with a goal to produce more accurate models that work well in tackling several applications. As a by-product, we wish to encourage the use of state-of-the-art Bayesian techniques, and also to incorporate auxiliary information, in modelling.

The remainder of this article is as follows. We first provide a brief background on the Pitman-Yor process in Section 2. Then, in Section 3, we detail our modelling framework by illustrating it on a simple topic model. We continue through to the inference procedure on the topic model in Section 4. Finally, in Section 5, we present an application on modelling social network data, utilising the proposed framework. Section 6 concludes.

## Background on Pitman-Yor Process

We provide a brief, informal review of the Pitman-Yor process (PYP, [Ishwaran and James, 2001)](#b34) in this section. We assume the readers are familiar with basic probability distributions (see [Walck, 2007)](#b82) and the Dirichlet process [(DP, Ferguson, 1973)](#). In addition, we refer the readers to [Hjort et al. (2010)](#b31) for a tutorial on Bayesian nonparametric modelling.

## Pitman-Yor Process

The Pitman-Yor process (PYP, [Ishwaran and James, 2001)](#b34) is also known as the twoparameter Poisson-Dirichlet process. The PYP is a two-parameter generalisation of the DP, now with an extra parameter α named the discount parameter in addition to the concentration parameter β. Similar to DP, a sample from a PYP corresponds to a discrete distribution (known as the output distribution) with the same support as its base distribution H. The underlying distribution of the PYP is the Poisson-Dirichlet distribution (PDD), which was introduced by [Pitman and Yor (1997)](#b68).

The PDD is defined by its construction process. For 0 ≤ α < 1 and β > -α, let V k be distributed independently as follows:

$(V k | α, β) ∼ Beta(1 -α, β + kα) , for k = 1, 2, 3, . . . ,(1)$and define (p 1 , p 2 , p 3 , . . . ) as

$p 1 = V 1 ,(2)$$p k = V k k-1 i=1 (1 -V i ) , for k ≥ 2 . (3$$)$If we let p = (p 1 , p2 , p3 , . . . ) be a sorted version of (p 1 , p 2 , p 3 , . . . ) in descending order, then p is Poisson-Dirichlet distributed with parameter α and β:

$p ∼ PDD(α, β) .(4)$Note that the unsorted version (p 1 , p 2 , p 3 , . . . ) follows a GEM(α, β) distribution, which is named after Griffiths, Engen and McCloskey [(Pitman, 2006)](#b67).

With the PDD defined, we can then define the PYP formally. Let H be a distribution over a measurable space (X , B), for 0 ≤ α < 1 and β > -α, suppose that p = (p 1 , p 2 , p 3 , . . . ) follows a PDD (or GEM) with parameters α and β, then PYP is given by the formula

$p(x | α, β, H) = ∞ k=1 p k δ X k (x) , for k = 1, 2, 3, . . . ,(5)$where X k are independent samples drawn from the base measure H and δ X k (x) represents probability point mass concentrated at X k (i.e., it is an indicator function that is equal to 1 when x = X k and 0 otherwise):

$δ x (y) = 1 if x = y 0 otherwise . (6$$)$This construction, Equation (1), is named the stick-breaking process. The PYP can also be constructed using an analogue to Chinese restaurant process (which explicitly draws a sequence of samples from the base distribution). A more extensive review on the PYP is given by [Buntine and Hutter (2012)](#b11).

A PYP is often more suitable than a DP in modelling since it exhibits a power-law behaviour (when α = 0), which is observed in natural languages [(Goldwater et al., 2005;](#b25)[Teh and Jordan, 2010)](#b78). The PYP has also been employed in genomics [(Favaro et al., 2009)](#b21) and economics [(Aoki, 2008)](#b0). Note that when the discount parameter α is 0, the PYP simply reduces to a DP.

## Pitman-Yor Process with a Mixture Base

Note that the base measure H of a PYP is not necessarily restricted to a single probability distribution. H can also be a mixture distribution such as

$H = ρ 1 H 1 + ρ 2 H 2 + • • • + ρ n H n ,(7)$where n i=1 ρ i = 1 and {H 1 , . . . H n } is a set of distributions over the same measurable space (X , B) as H.

With this specification of H, the PYP is also named the compound Poisson-Dirichlet process in [Du (2012)](#b16), or the doubly hierarchical Pitman-Yor process in [Wood and Teh (2009)](#b87). A special case of this is the DP equivalent, which is also known as the DP with mixed random measures in [Kim et al. (2012)](#b40). Note that we have assumed constant values for the ρ i , though of course we can go fully Bayesian and assign a prior distribution for each of them, a natural prior would be the Dirichlet distribution.

## Remark on Bayesian Inference

Performing exact Bayesian inference on nonparametric models is often intractable due to the difficulty in deriving the closed-form posterior distributions. This motivates the use of Markov chain Monte Carlo (MCMC) methods (see [Gelman et al., 2013)](#b23) for approximate inference. Most notable of the MCMC methods are the Metropolis-Hastings (MH) algorithms [(Metropolis et al., 1953;](#b63)[Hastings, 1970)](#b29) and Gibbs samplers [(Geman and Geman, 1984)](#b24). These algorithms serve as a building block for more advanced samplers, such as the MH algorithms with delayed rejection [(Mira, 2001)](#b64). Generalisations of the MCMC method include the reversible jump MCMC [(Green, 1995)](#b27) and its delayed rejection variant [(Green and Mira, 2001)](#b28) can also be employed for Bayesian inference, however, they are out of the scope in this article.

Instead of sampling one parameter at a time, one can develop an algorithm that updates more parameters in each iteration, a so-called blocked Gibbs sampler [(Liu, 1994)](#b50). Also, in practice we are usually only interested in a certain subset of the parameters; in such cases we can sometimes derive more efficient collapsed Gibbs samplers [(Liu, 1994)](#b50) by integrating out the nuisance parameters. In the remainder of this article, we will employ a combination of the blocked and collapsed Gibbs samplers for Bayesian inference.

## Modelling Framework with Hierarchical Pitman-Yor Process

In this section, we discuss the basic design of our nonparametric Bayesian topic models using thierarchical Pitman-Yor processes (HPYP). In particular, we will introduce a simple topic model that will be extended later. We discuss the general inference algorithm for the topic model and hyperparameter optimisation.

Development of topic models is fundamentally motivated by their applications. Depending on the application, a specific topic model that is most suitable for the task should be designed and used. However, despite the ease of designing the model, the majority of time is spent on implementing, assessing, and redesigning it. This calls for a better designing cycle/routine that is more efficient, that is, spending less time in implementation and more time in model design and development. is usually referred as topic side, while the right hand side (with γ and φ) is called the vocabulary side. The word node denoted by w dn is observed. The notations are defined in Table [1](#tab_0).

We can achieve this by a higher level implementation of the algorithms for topic modelling. This has been made possible in other statistical domains by BUGS (Bayesian inference using Gibbs sampling, [Lunn et al., 2000)](#b55) or JAGS (just another Gibbs sampler, [Plummer, 2003)](#b69), albeit with standard probability distributions. Theoretically, BUGS and JAGS will work on LDA; however, in practice, running Gibbs sampling for LDA with BUGS and JAGS is very slow. This is because their Gibbs samplers are uncollapsed and not optimised. Furthermore, they cannot be used in a model with stochastic processes, like the Gaussian process (GP) and DP.

Below, we present a framework that allows us to implement HPYP topic models efficiently. This framework allows us to test variants of our proposed topic models without significant reimplementation.

## Hierarchical Pitman-Yor Process Topic Model

The HPYP topic model is a simple network of PYP nodes since all distributions on the probability vectors are modelled by the PYP. For simplicity, we assume a topic model with three PYP layers, although in practice there is no limit to the number of PYP layers. We present the graphical model of our generic topic model in Figure [1](#fig_0). This model is a variant of those presented in [Buntine and Mishra (2014)](#b12), and is presented here as a starting model for illustrating our methods and for subsequent extensions.

At the root level, we have µ and γ distributed as PYPs:

$µ ∼ PYP(α µ , β µ , H µ ) ,(8)$$γ ∼ PYP(α γ , β γ , H γ ) .(9)$The variable µ is the root node for the topics in a topic model while γ is the root node for the words. To allow arbitrary number of topics to be learned, we let the base distribution for µ, H µ , to be a continuous distribution or a discrete distribution with infinite samples.

We usually choose a discrete uniform distribution for γ based on the word vocabulary size of the text corpus. This decision is technical in nature, as we are able to assign a tiny probability to words not observed in the training set, which eases the evaluation process. Thus

$H γ = {• • • , 1 |V| , • • • }$where |V| is the set of all word vocabulary of the text corpus. We now consider the topic side of the HPYP topic model. Here we have ν, which is the child node of µ. It follows a PYP given ν, which acts as its base distribution:

$ν ∼ PYP(α ν , β ν , µ) . (10$$)$For each document d in a text corpus of size D, we have a document-topic distribution θ d , which is a topic distribution specific to a document. Each of them tells us about the topic composition of a document.

$θ d ∼ PYP(α θ d , β θ d , ν) , for d = 1, . . . , D .(11)$While for the vocabulary side, for each topic k learned by the model, we have a topicword distribution φ k which tells us about the words associated with each topic. The topicword distribution φ k is PYP distributed given the parent node γ, as follows:

$φ k ∼ PYP(α φ k , β φ k , γ) , for k = 1, . . . , K .(12)$Here, K is the number of topics in the topic model. For every word w dn in a document d which is indexed by n (from 1 to N d , the number of words in document d), we have a latent topic z dn (also known as topic assignment) which indicates the topic the word represents. z dn and w dn are categorical variables generated from θ d and φ k respectively:

$z dn | θ d ∼ Discrete(θ d ) ,(13)$$w dn | z dn , φ ∼ Discrete(φ z d ) , for n = 1, . . . , N d . (14$$)$The above α and β are the discount and concentration parameters of the PYPs (see Section 2.1), note that they are called the hyperparameters in the model. We present a list of variables used in this section in Table [1](#tab_0).

## Model Representation and Posterior Likelihood

In a Bayesian setting, posterior inference requires us to analyse the posterior distribution of the model variables given the observed data. For instance, the joint posterior distribution for the HPYP topic model is

$p(µ, ν, γ, θ, φ, Z | W, Ξ) .(15)$Here, we use bold face capital letters to represent the set of all relevant variables. For instance, W captures all words in the corpus. Additionally, we denote Ξ as the set of all hyperparameters and constants in the model. Note that deriving the posterior distribution analytically is almost impossible due to its complex nature. This leaves us with approximate Bayesian inference techniques as mentioned in Section 2.3. However, even with these techniques, performing posterior inference 

## Z

## All topics

Collection of all topics z dn .

## W

## All words

Collection of all words w dn .

## Ξ

## All hyperparameters

Collection of all hyperparameters and constants.

## C

## All customer counts Collection of all customers counts

$c N k . T All table counts Collection of all table counts t N k .$with the posterior distribution is difficult due to the coupling of the probability vectors from the PYPs.

The key to an efficient inference procedure with the PYPs is to marginalise out the PYPs in the model and record various associated counts instead, which yields a collapsed sampler. To achieve this, we adopt a Chinese Restaurant Process (CRP) metaphor [(Teh and Jordan, 2010;](#b78)[Blei et al., 2010)](#b5) to represent the variables in the topic model. With this metaphor, all data in the model (e.g., topics and words) are the customers; while the PYP nodes are the restaurants the customers visit. In each restaurant, each customer is to be seated at only one table, though each table can have any number of customers. Each table in a restaurant serves a dish, the dish corresponds to the categorical label a data point may The dishes are the symbols in the middle of the rectangles, here they are denoted by the sunny symbol and the cloudy symbol. In this illustration, we know the number of customers corresponds to each table, for example, the green table is occupied by three customers. Also, since Restaurant 1 is the parent of Restaurant 2, the tables in Restaurant 2 are treated as the customers for Restaurant 1.

have (e.g., the topic label or word). Note that there can be more than one table serving the same dish. In a HPYP topic model, the tables in a restaurant N are treated as the customers for the parent restaurant P (in the graphical model, P points to N ), and they share the same dish. This means that the data is passed up recursively until the root node.

For illustration, we present a simple example in Figure [2](#fig_1), showing the seating arrangement of the customers from two restaurants. Naïvely recording the seating arrangement (table and dish) of each customer brings about computational inefficiency during inference. Instead, we adopt the table multiplicity (or table counts) representation of [Chen et al. (2011)](#b14) which requires no dynamic memory, thus consuming only a factor of memory at no loss of inference efficiency. Under this representation, we store only the customer counts and table counts associated with each restaurant. The customer count c N k denotes the number of customers who are having dish k in restaurant N . The corresponding symbol without subscript, c N , denotes the collection of customer counts in restaurant N , that is, Here the setting is the same as Figure [2](#fig_1) but the seating arrangement of the customers are "forgotten" and only the table and customer counts are recorded. Thus, we only know that there are three sunny tables in Restaurant 2, with a total of nine customers.

$c N = (• • • , c N k , • • • ).$k c N k . Similar to the customer count, the table count t N k denotes the number of nonempty tables serving dish k in restaurant N . The corresponding t N and T N are defined similarly. For instance, from the example in Figure [2](#fig_1), we have c 2 sun = 9 and t 2 sun = 3, the corresponding illustration of the table multiplicity representation is presented in Figure [3](#fig_2). We refer the readers to [Chen et al. (2011)](#b14) for a detailed derivation of the posterior likelihood of a restaurant.

For the posterior likelihood of the HPYP topic model, we marginalise out the probability vector associated with the PYPs and represent them with the customer counts and table counts, following [Chen et al. (2011, Theorem 1)](#). We present the modularised version of the full posterior of the HPYP topic model, which allows the posterior to be computed very quickly. The full posterior consists of the modularised likelihood associated with each PYP in the model, defined as

$f (N ) = β N α N T N β N C N K k=1 S c N k t N k , α N c N k t N k -1 , for N ∼ PYP α N , β N , P .(16)$Here, S x y, α are generalised Stirling numbers [(Buntine and Hutter, 2012, Theorem 17)](#). Both (x) T and (x|y) T denote Pochhammer symbols with rising factorials [(Oldham et al., 2009, Section 18](#)):

$(x) T = x • (x + 1) • • • x + (T -1) , (17$$) (x|y) T = x • (x + y) • • • x + (T -1)y . (18$$)$With the CRP representation, the full posterior of the HPYP topic model can now be written -in terms of f (•) given in Equation ( [16](#formula_22)) -as

$p(Z, T, C | W, Ξ) ∝ p(Z, W, T, C | Ξ) ∝ f (µ)f (ν) D d=1 f (θ d ) K k=1 f (φ k ) f (γ) |V| v=1 1 |V| t γ v . (19$$)$This result is a generalisation of Chen et al. (2011, Theorem 1) to account for discrete base distribution -the last term in Equation ( [19](#formula_26)) corresponds to the base distribution of γ, and v indexes each unique word in vocabulary set V. The bold face T and C denote the collection of all table counts and customer counts, respectively. Note that the topic assignments Z are implicitly captured by the customer counts:

$c θ d k = N d n=1 I(z dn = k) ,(20)$where I(•) is the indicator function, which evaluates to 1 when the statement inside the function is true, and 0 otherwise. We would like to point out that even though the probability vectors of the PYPs are integrated out and not explicitly stored, they can easily be reconstructed. This is discussed in Section 4.4. We move on to Bayesian inference in the next section.

## Posterior Inference for the HPYP Topic Model

We focus on the MCMC method for Bayesian inference on the HPYP topic model. The MCMC method on topic models follows these simple procedures -decrementing counts contributed by a word, sample a new topic for the word, and update the model by accepting or rejecting the proposed sample. Here, we describe the collapsed blocked Gibbs sampler for the HPYP topic model. Note the PYPs are marginalised out so we only deal with the counts.

## Decrementing the Counts Associated with a Word

The first step in a Gibbs sampler is to remove a word and corresponding latent topic, then decrement the associated customer counts and table counts. To give an example from Figure [2](#fig_1), if we remove the red customer from Restaurant 2, we would decrement the customer count c 2 sun by 1. Additionally, we also decrement the table count t 2 sun by 1 because the red customer is the only customer on its table. This in turn decrements the customer count c 1 sun by 1. However, this requires us to keep track of the customers' seating arrangement which leads to increased memory requirements and poorer performance due to inadequate mixing [(Chen et al., 2011)](#b14).

To overcome the above issue, we follow the concept of table indicator [(Chen et al., 2011)](#b14) and introduce a new auxiliary Bernoulli indicator variable u N k , which indicates whether removing the customer also removes the table by which the customer is seated. Note that our Bernoulli indicator is different to that of [Chen et al. (2011)](#b14) which indicates the restaurant a customer contributes to. The Bernoulli indicator is sampled as needed in the decrementing procedure and it is not stored, this means that we simply "forget" the seating arrangements and re-sample them later when needed, thus we do not need to store the seating arrangement. The Bernoulli indicator of a restaurant N depends solely on the customer counts and the table counts:

$p u N k = t N k /c N k if u N k = 1 1 -t N k /c N k if u N k = 0 .(21)$In the context of the HPYP topic model described in Section 3.1, we formally present how we decrement the counts associated with the word w dn and latent topic z dn from document d and position n. First, on the vocabulary side (see Figure [1](#fig_0)), we decrement the customer count c φz dn w dn associated with φ z dn by 1. Then sample a Bernoulli indicator u φz dn w dn according to Equation (21). If u φz dn w dn = 1, we decrement the table count t φz dn

w dn and also the customer count c γ w dn by one. In this case, we would sample a Bernoulli indicator u γ w dn for γ, and decrement t γ w dn if u γ w dn = 1. We do not decrement the respective customer count if the Bernoulli indicator is 0. Second, we would need to decrement the counts associated with the latent topic z dn . The procedure is similar, we decrement c θ d z dn by 1 and sample the Bernoulli indicator u θ d z dn . Note that whenever we decrement a customer count, we sample the corresponding Bernoulli indicator. We repeat this procedure recursively until the Bernoulli indicator is 0 or until the procedure hits the root node.

## Sampling a New Topic for a Word

After decrementing the variables associated with a word w dn , we use a blocked Gibbs sampler to sample a new topic z dn for the word and the corresponding customer counts and table counts. The conditional posterior used in sampling can be computed quickly when the full posterior is represented in a modularised form. To illustrate, the conditional posterior for z dn and its associated customer counts and table counts is

$p(z dn , T, C | Z -dn , W, T -dn , C -dn , Ξ) = p(Z, T, C | W, Ξ) p(Z -dn , T -dn , C -dn | W, Ξ) ,(22)$which is further broken down by substituting the posterior likelihood defined in Equation (19), giving the following ratios of the modularised likelihoods:

$f (µ) f (µ -dn ) f (ν) f (ν -dn ) f (θ d ) f (θ -dn d ) f (φ z dn ) f (φ -dn z dn ) f (γ) f (γ -dn ) 1 |V| t γ w dn -(t γ w dn ) -dn . (23$$)$The superscript -dn indicates that the variables associated with the word w dn are removed from the respective sets, that is, the customer counts and table counts are after the decrementing procedure. Since we are only sample the topic assignment z dn associated with one Table 2: All possible proposals of the blocked Gibbs sampler for the variables associated with w dn . To illustrate, one sample would be z dn = 1, t N z dn does not increment (stays the same), and c N z dn increments by 1, for all N in {µ, ν, θ d , φ z dn , γ}. We note that the proposals can include states that are invalid, but this is not an issue since those states have zero posterior probability and thus will not be sampled.

## Variable Possibilities Variable

Possibilities Variable Possibilities

$z dn {1, . . . , K} t N z dn {t N z dn , t N z dn + 1} c N z dn {c N z dn , c N z dn + 1}$word, the customer counts and table counts can only increment by at most 1, see Table [2](#) for a list of all possible proposals. This allows the ratios of the modularised likelihoods, which consists of ratios of Pochhammer symbol and ratio of Stirling numbers

$f (N ) f (N -dn ) = (β N ) (C N ) -dn (β N ) C N (β N |α N ) T N (β N |α N ) (T N ) -dn K k=1 S c N k t N k , α N S (c N k ) -dn (t N k ) -dn , α N ,(24)$to simplify further. For instance, the ratios of Pochhammer symbols can be reduced to constants, as follows:

$(x) T +1 (x) T = x + T , (x|y) T +1 (x|y) T = x + yT . (25$$)$The ratio of Stirling numbers, such as S y+1 x+1, α /S y x, α , can be computed quickly via caching [(Buntine and Hutter, 2012)](#b11). Technical details on implementing the Stirling numbers cache can be found in [Lim (2016)](#b45).

With the conditional posterior defined, we proceed to the sampling process. Our first step involves finding all possible changes to the topic z dn , customer counts, and the table counts (hereafter known as 'state') associated with adding the removed word w dn back into the topic model. Since only one word is added into the model, the customer counts and the table counts can only increase by at most 1, constraining the possible states to a reasonably small number. Furthermore, the customer counts of a parent node will only be incremented when the table counts of its child node increases. Note that it is possible for the added customer to generate a new dish (topic) for the model. This requires the customer to increment the table count of a new dish in the root node µ by 1 (from 0).

Next, we compute the conditional posterior (Equation ( [22](#formula_30))) for all possible states. The conditional posterior (up to a proportional constant) can be computed quickly by breaking down the posterior and calculating the relevant parts. We then normalise them to sample one of the states to be the proposed next state. Note that the proposed state will always be accepted, which is an artifact of Gibbs sampler.

Finally, given the proposal, we update the HPYP model by incrementing the relevant customer counts and table counts.

## Optimising the Hyperparameters

Choosing the right hyperparameters for the priors is important for topic models. [Wallach et al. (2009a)](#) show that an optimised hyperparameter increases the robustness of the topic models and improves their model fitting. The hyperparameters of the HPYP topic models are the discount parameters and concentration parameters of the PYPs. Here, we propose a procedure to optimise the concentration parameters, but leave the discount parameters fixed due to their coupling with the Stirling numbers cache.

The concentration parameters β of all the PYPs are optimised using an auxiliary variable sampler similar to [Teh (2006)](#b77). Being Bayesian, we assume the concentration parameter β N of a PYP node N has the following hyperprior:

$β N ∼ Gamma(τ 0 , τ 1 ) , for N ∼ PYP α N , β N , P ,(26)$where τ 0 is the shape parameter and τ 1 is the rate parameter. The gamma prior is chosen due to its conjugacy which gives a gamma posterior for β N .

To optimise β N , we first sample the auxiliary variables ω and ζ i given the current value of α N and β N , as follows:

$ω | β N ∼ Beta C N , β N , (27$$)$$ζ i | α N , β N ∼ Bernoulli β N β N + iα N , for i = 0, 1, . . . , T N -1 .(28)$With these, we can then sample a new β N from its conditional posterior

$β N ω, ζ ∼ Gamma   τ 0 + T N -1 i=0 ζ i , τ 1 -log(1 -ω)   . (29$$)$The collapsed Gibbs sampler is summarised by Algorithm 1.

## Estimating the Probability Vectors of the PYPs

Recall that the aim of topic modelling is to analyse the posterior of the model parameters, such as one in Equation ( [15](#formula_19)). Although we have marginalised out the PYPs in the above Gibbs sampler, the PYPs can be reconstructed from the associated customer counts and table counts. Recovering the full posterior distribution of the PYPs is a complicated task. So, instead, we will analyse the PYPs via the expected value of their conditional marginal posterior distribution, or simply, their posterior mean,

$E[N | Z, W, T, C, Ξ] , for N ∈ {µ, ν, γ, θ d , φ k } . (30$$)$The posterior mean of a PYP corresponds to the probability of sampling a new customer for the PYP. To illustrate, we consider the posterior of the topic distribution θ d . We let zdn to be a unknown future latent topic in addition to the known Z. With this, we can write the posterior mean of θ dk as

$E[θ dk | Z, W, T, C, Ξ] = E[p(z dn = k | θ d , Z, W, T, C, Ξ) | Z, W, T, C, Ξ] = E[p(z dn = k | Z, T, C) | Z, W, T, C, Ξ] .(31$) Algorithm 1 Collapsed Gibbs Sampler for the HPYP Topic Model 1. Initialise the HPYP topic model by assigning random topic to the latent topic z dn associated to each word w dn . Then update all the relevant customer counts C and table counts T by using Equation (20) and setting the table counts to be about half of the customer counts. 2. For each word w dn in each document d, do the following: (a) Decrement the counts associated with w dn (see Section 4.1). (b) Block sample a new topic for z dn and corresponding customer counts C and table counts T (see Section 4.2). (c) Update (increment counts) the topic model based on the sample. 3. Update the hyperparameter β N for each PYP nodes N (see Section 4.3). 4. Repeat Steps 2 -3 until the model converges or when a fix number of iterations is reached.

by replacing θ dk with the posterior predictive distribution of zdn and note that zdn can be sampled using the CRP, as follows:

$p(z dn = k | Z, T, C) = (α θ d T θ d + β θ d )ν k + c θ d k -α θ d T θ d k β θ d + C θ d .(32)$Thus, the posterior mean of θ d is given as

$E[θ dk | Z, W, T, C, Ξ] = (α θ d T θ d + β θ d )E[ν k | Z, W, T, C, Ξ] + c θ d k -α θ d T θ d k β θ d + C θ d ,(33)$which is written in term of the posterior mean of its parent PYP, ν. The posterior means of the other PYPs such as ν can be derived by taking a similar approach. Generally, the posterior mean corresponds to a PYP N (with parent PYP P) is as follows:

$E[N k | Z, W, T, C, Ξ] = (α N T N + β N )E[P k | Z, W, T, C, Ξ] + c N k -α N T N k β N + C N ,(34)$By applying Equation ( [34](#formula_48)) recursively, we obtain the posterior mean for all the PYPs in the model. We note that the dimension of the topic distributions (µ, ν, θ) is K + 1, where K is the number of observed topics. This accounts for the generation of a new topic associated with the new customer, though the probability of generating a new topic is usually much smaller. In practice, we may instead ignore the extra dimension during the evaluation of a topic model since it does not provide useful interpretation. One way to do this is to simply discard the extra dimension of all the probability vectors after computing the posterior mean. Another approach would be to normalise the posterior mean of the root node µ after discarding the extra dimension, before computing the posterior mean of others PYPs. Note that for a considerably large corpus, the difference in the above approaches would be too small to notice.

## Evaluations on Topic Models

Generally, there are two ways to evaluate a topic model. The first is to evaluate the topic model based on the task it performs, for instance, the ability to make predictions. The second approach is the statistical evaluation of the topic model on modelling the data, which is also known as the goodness-of-fit test. In this section, we will present some commonly used evaluation metrics that are applicable to all topic models, but we first discuss the procedure for estimating variables associated with the test set.

## Predictive Inference on the Test Documents

Test documents, which are used for evaluations, are set aside from learning documents. As such, the document-topic distributions θ associated with the test documents are unknown and hence need to be estimated. One estimate for θ is its posterior mean given the variables learned from the Gibbs sampler:

$θd = E[θ d | Z, W, T, C, Ξ] ,(35)$obtainable by applying Equation ( [34](#formula_48)). Note that since the latent topics Z corresponding to the test set are not sampled, the customer counts and table counts associated with θ d are 0, thus θd is equal to ν, the posterior mean of ν. However, this is not a good estimate for the topic distribution of the test documents since they will be identical for all the test documents. To overcome this issue, we will instead use some of the words in the test documents to obtain a better estimate for θ. This method is known as document completion [(Wallach et al., 2009b)](#b84), as we use part of the text to estimate θ, and use the rest for evaluation. Getting a better estimate for θ requires us to first sample some of the latent topics zdn in the test documents. The proper way to do this is by running an algorithm akin to the collapsed Gibbs sampler, but this would be excruciatingly slow due to the need to re-sample the customer counts and table counts for all the parent PYPs. Instead, we assume that the variables learned from the Gibbs sampler are fixed and sample the zdn from their conditional posterior sequentially, given the previous latent topics:

$p(z dn = k | wdn , θ d , φ, zd1 , . . . , zd,n-1 ) ∝ θ dk φ kw dn .(36)$Whenever a latent topic zdn is sampled, we increment the customer count c θ d zdn for the test document. For simplicity, we set the table count t θ d zdn to be half the corresponding customer counts c θ d zdn , this avoids the expensive operation of sampling the table counts. Additionally, θ d is re-estimated using Equation ( [35](#formula_49)) before sampling the next latent topic. We note that the estimated variables are unbiased.

The final θ d becomes an estimate for the topic distribution of the test document d. The above procedure is repeated R times to give R samples of θ 

This Monte Carlo estimate can then be used for computing the evaluation metrics. Note that when estimating θ, we have ignored the possibility of generating a new topic, that is, the latent topics z are constrained to the existing topics, as previously discussed in Section 4.4.

## Goodness-of-fit Test

Measures of goodness-of-fit usually involves computing the discrepancy of the observed values and the predicted values under the model. However, the observed variables in a topic model are the words in the corpus, which are not quantifiable since they are discrete labels. Thus evaluations on topic models are usually based on the model likelihoods instead.

A popular metric commonly used to evaluate the goodness-of-fit of a topic model is perplexity, which is negatively related to the likelihood of the observed words W given the model, this is defined as

$perplexity(W | θ, φ) = exp - D d=1 N d n=1 log p(w dn | θ d , φ) D d=1 N d ,(38)$where p(w dn | θ d , φ) is the likelihood of sampling the word w dn given the document-topic distribution θ d and the topic-word distributions φ. Computing p(w dn | θ d , φ) requires us to marginalise out z dn from their joint distribution, as follows:

$p(w dn | θ d , φ) = k p(w dn , z dn = k | θ d , φ) = k p(w dn | z dn = k, φ k ) p(z dn = k | θ d ) = k φ kw dn θ dk .(39)$Although perplexity can be computed on the whole corpus, in practice we compute the perplexity on test documents. This is to measure if the topic model generalises well to unseen data. A good topic model would be able to predict the words in the test set better, thereby assigning a higher probability p(w dn | θ d , φ) in generating the words. Since perplexity is negatively related to the likelihood, a lower perplexity is better.

## Document Clustering

We can also evaluate the clustering ability of the topic models. Note that topic models assign a topic to each word in a document, essentially performing a soft clustering [(Erosheva and Fienberg, 2005)](#b20) for the documents in which the membership is given by the document-topic distribution θ. To evaluate the clustering of the documents, we convert the soft clustering to hard clustering by choosing a topic that best represents the documents, hereafter called the dominant topic. The dominant topic of a document d corresponds to the topic that has the highest proportion in the topic distribution, that is,

$Dominant Topic(θ d ) = arg max k θ dk . (40$$)$Two commonly used evaluation measures for clustering are purity and normalised mutual information (NMI, [Manning et al., 2008)](#b57). The purity is a simple clustering measure which can be interpreted as the proportion of documents correctly clustered, while NMI is an information theoretic measures used for clustering comparison. If we denote the ground truth classes as S = {s 1 , . . . , s J } and the obtained clusters as R = {r 1 , . . . , r K }, where each s j and r k represents a collection (set) of documents, then the purity and NMI can be computed as

$purity(S, R) = 1 D K k=1 max j |r k ∩ s j | , NMI(S, R) = 2 MI(S; R) E(S) + E(R) , (41$$)$where MI(S; R) denotes the mutual information between two sets and E(•) denotes the entropy. They are defined as follows:

$MI(S; R) = K k=1 J j=1 |r k ∩ s j | D log 2 D |r k ∩ s j | |r k ||s j | , E(R) = - K k=1 |r k | D log 2 |r k | D . (42$$)$Note that the higher the purity or NMI, the better the clustering.

## Application: Modelling Social Network on Twitter

This section looks at how we can employ the framework discussed above for an application of tweet modelling, using auxiliary information that is available on Twitter. We propose the Twitter-Network topic model (TNTM) to jointly model the text and the social network in a fully Bayesian nonparametric way, in particular, by incorporating the authors, hashtags, the "follower" network, and the text content in modelling. The TNTM employs a HPYP for text modelling and a Gaussian process (GP) random function model for social network modelling. We show that the TNTM significantly outperforms several existing nonparametric models due to its flexibility.

## Motivation

Emergence of web services such as blogs, microblogs and social networking websites allows people to contribute information freely and publicly. This user-generated information is generally more personal, informal, and often contains personal opinions. In aggregate, it can be useful for reputation analysis of entities and products [(Aula, 2010)](#b2), natural disaster detection [(Karimi et al., 2013)](#b38), obtaining first-hand news [(Broersma and Graham, 2012)](#b9), or even demographic analysis [(Correa et al., 2010)](#b15). We focus on Twitter, an accessible source of information that allows users to freely voice their opinions and thoughts in short text known as tweets.

Although LDA [(Blei et al., 2003](#)) is a popular model for text modelling, a direct application on tweets often yields poor result as tweets are short and often noisy [(Zhao et al., 2011;](#b89)[Baldwin et al., 2013)](#b3), that is, tweets are unstructured and often contain grammatical and spelling errors, as well as informal words such as user-defined abbreviations due to the 140 characters limit. LDA fails on short tweets since it is heavily dependent on word co-occurrence. Also notable is that the text in tweets may contain special tokens known as hashtags; they are used as keywords and allow users to link their tweets with other tweets tagged with the same hashtag. Nevertheless, hashtags are informal since they have no standards. Hashtags can be used as both inline words or categorical labels. When used as labels, hashtags are often noisy, since users can create new hashtags easily and use any existing hashtags in any way they like. 3 Hence instead of being hard labels, hashtags are best treated as special words which can be the themes of the tweets. These properties of tweets make them challenging for topic models, and ad hoc alternatives are used instead. For instance, [Maynard et al. (2012)](#b59) advocate the use of shallow method for tweets, and [Mehrotra et al. (2013)](#b61) utilise a tweet-pooling approach to group short tweets into a larger document. In other text analysis applications, tweets are often 'cleansed' by NLP methods such as lexical normalisation [(Baldwin et al., 2013)](#b3). However, the use of normalisation is also criticised [(Eisenstein, 2013)](#b19), as normalisation can change the meaning of text.

In the following, we propose a novel method for better modelling of microblogs by leveraging the auxiliary information that accompanies tweets. This information, complementing word co-occurrence, also opens the door to more applications, such as user recommendation and hashtag suggestion. Our major contributions include (1) a fully Bayesian nonparametric model named the Twitter-Network topic model (TNTM) that models tweets well, and (2) a combination of both the HPYP and the GP to jointly model text, hashtags, authors and the followers network. Despite the seeming complexity of the TNTM model, its implementation is made relatively straightforward using the flexible framework developed in Section 3. Indeed, a number of other variants were rapidly implemented with this framework as well.

## The Twitter-Network Topic Model

The TNTM makes use of the accompanying hashtags, authors, and followers network to model tweets better. The TNTM is composed of two main components: a HPYP topic model for the text and hashtags, and a GP based random function network model for the followers network. The authorship information serves to connect the two together. The HPYP topic model is illustrated by region b in Figure [4](#fig_4) while the network model is captured by region a .

## HPYP Topic Model

The HPYP topic model described in Section 3 is extended as follows. For the word distributions, we first generate a parent word distribution prior γ for all topics:

$γ ∼ PYP(α γ , β γ , H γ ) ,(43)$where H γ is a discrete uniform distribution over the complete word vocabulary V. 4 Then, we sample the hashtag distribution ψ k and word distribution ψ k for each topic k, with γ as 3. For example, hashtag hijacking, where a well defined hashtag is used in an "inappropriate" way. The most notable example would be on the hashtag #McDStories, though it was initially created to promote happy stories on McDonald's, the hashtag was hijacked with negative stories on McDonald's. 4. The complete word vocabulary contains words and hashtags seen in the corpus. The author-topic distributions ν serve to link the two together. Each tweet is modelled with a hierarchy of document-topic distributions denoted by η, θ , and θ, where each is attuned to the whole tweet, the hashtags, and the words, in that order. With their own topic assignments z and z, the hashtags y and the words w are separately modelled. They are generated from the topic-hashtag distributions ψ and the topic-word distributions ψ respectively. The variables µ 0 , µ 1 and γ are priors for the respective PYPs. The connections between the authors are denoted by x, modelled by random function F.

$ν i µ 0 µ 1 η d a d θ ′ d θ d z ′ dm z dn y dm w dn ψ k γ ψ ′ k A D K M d N d F x ij E a ⃝ b ⃝$the base distribution:

$ψ k | γ ∼ PYP(α ψ k , β ψ k , γ) ,(44)$$ψ k | γ ∼ PYP(α ψ k , β ψ k , γ) , for k = 1, . . . , K .(45)$Note that the tokens of the hashtags are shared with the words, that is, the hashtag #happy shares the same token as the word happy, and are thus treated as the same word. This treatment is important since some hashtags are used as words instead of labels. 5 Additionally, this also allows any words to be hashtags, which will be useful for hashtag recommendation.

For the topic distributions, we generate a global topic distribution µ 0 , which serves as a prior, from a GEM distribution. Then generate the author-topic distribution ν i for each 5. For instance, as illustrated by the following tweet: i want to get into #photography. can someone recommend a good beginner #camera please? i dont know where to start

## Random Function Network Model

The network modelling is connected to the HPYP topic model via the author-topic distributions ν, where we treat ν as inputs to the GP in the network model. The GP, represented by F, determines the link between two authors (x ij ), which indicates the existence of the social links between author i and author j. For each pair of authors, we sample their connections with the following random function network model:

$Q ij | ν ∼ F(ν i , ν j ) ,(58)$$x ij | Q ij ∼ Bernoulli s(Q ij ) , for i = 1, . . . , A; j = 1, . . . , A ,(59)$where s(•) is the sigmoid function:

$s(t) = 1 1 + e -t .(60)$By marginalising out F, we can write Q ∼ GP(ς, κ), where Q is a vectorised collection of Q ij . 6 ς denotes the mean vector and κ is the covariance matrix of the GP:

$ς ij = Sim(ν i , ν j ) ,(61)$$κ ij,i j = s 2 2 exp - Sim(ν i , ν j ) -Sim(ν i , ν j ) 2 2l 2 + σ 2 I(ij = i j ) ,(62)$where s, l and σ are the hyperparameters associated to the kernel. Sim(•, •) is a similarity function that has a range between 0 and 1, here chosen to be cosine similarity due to its ease of computation and popularity.

## Relationships with Other Models

The TNTM is related to many existing models after removing certain components of the model. When hashtags and the network components are removed, the TNTM is reduced to a nonparametric variant of the author topic model (ATM). Oppositely, if authorship information is discarded, the TNTM resembles the correspondence LDA [(Blei and Jordan, 2003)](#), although it differs in that it allows hashtags and words to be generated from a common vocabulary.

In contrast to existing parametric models, the network model in the TNTM provides possibly the most flexible way of network modelling via a nonparametric Bayesian prior (GP), following [Lloyd et al. (2012)](#b52). Different to [Lloyd et al. (2012)](#b52), we propose a new kernel function that fits our purpose better and achieves significant improvement over the original kernel.

## Representation and Model Likelihood

As with previous sections, we represent the TNTM using the CRP representation discussed in Section 3.2. However, since the PYP variables in the TNTM can have multiple parents, we extend the representation following [Du et al. (2012a)](#). The distinction is that we store 6. Q = (Q11, Q12, . . . , QAA) T , note that ς and κ follow the same indexing.

multiple tables counts for each PYP, to illustrate, t N →P k represents the number of tables in PYP N serving dish k that are contributed to the customer counts in PYP P, c P k . Similarly, the total table counts that contribute to P is denoted as

$T N →P = k t N →P k . Note the number of tables in PYP N is t N k = P t N →P k$, while the total number of tables is T N = P T N →P . We refer the readers to [Lim et al. (2013, Appendix B)](#) for a detailed discussion. We use bold face capital letters to denote the set of all relevant lower case variables, for example, we denote W • = {W, Y} as the set of all words and hashtags; Z • = {Z, Z } as the set of all topic assignments for the words and the hashtags; T as the set of all table counts and C as the set of all customer counts; and we introduce Ξ as the set of all hyperparameters. By marginalising out the latent variables, we write down the model likelihood corresponding to the HPYP topic model in terms of the counts:

$p(Z • , T, C | W • , Ξ) ∝ p(Z • , W • , T, C | Ξ) ∝ f (µ 0 )f (µ 1 ) A i=1 f (ν i ) K k=1 f (ψ k )f (ψ k ) f (γ) × D d=1 f (η d )f (θ d )f (θ d )g ρ θ d g ρ θ d |V| v=1 1 |V| t γ v ,(63)$where f (N ) is the modularised likelihood corresponding to node N , as defined by Equation ( [16](#formula_22)), and g(ρ) is the likelihood corresponding to the probability ρ that controls which parent node to send a customer to, defined as

$g(ρ N ) = B λ N 0 + T N →P 0 , λ N 1 + T N →P 1 ,(64)$for N ∼ PYP α N , β N , ρ N P 0 + (1-ρ N )P 1 . Note that B(a, b) denotes the Beta function that normalises a Dirichlet distribution, defined as follows:

$B(a, b) = Γ(a) Γ(b) Γ(a + b) .(65)$For the random function network model, the conditional posterior can be derived as

$p(Q | X, ν, Ξ) ∝ p(X, Q | ν, Ξ) ∝ A i=1 A j=1 s(Q ij ) x ij 1 -s(Q ij ) 1-x ij × |κ| -1 2 exp - 1 2 (Q -ς) T κ -1 (Q -ς) .(66)$The full posterior likelihood is thus the product of the topic model posterior (Equation ( [63](#formula_70))) and the network posterior (Equation ( [66](#formula_73))):

$p(Q, Z • , T, C | X, W • , Ξ) = p(Z • , T, C | W • , Ξ) p(Q | X, ν, Ξ) . (67$$)$
## Performing Posterior Inference on the TNTM

In the TNTM, combining a GP with a HPYP makes its posterior inference non-trivial. Hence, we employ approximate inference by alternatively performing MCMC sampling on the HPYP topic model and the network model, conditioned on each other. For the HPYP topic model, we employ the flexible framework discussed in Section 3 to perform collapsed blocked Gibbs sampling. For the network model, we derive a Metropolis-Hastings (MH) algorithm based on the elliptical slice sampler [(Murray et al., 2010)](#b65). In addition, the authortopic distributions ν connecting the HPYP and the GP are sampled with an MH scheme since their posteriors do not follow a standard form. We note that the PYPs in this section can have multiple parents, so we extend the framework in Section 3 to allow for this.

The collapsed Gibbs sampling for the HPYP topic model in TNTM is similar to the procedure in Section 4, although there are two main differences. The first difference is that we need to sample the topics for both words and hashtags, each with a different conditional posterior compared to that of Section 4. While the second is due to the PYPs in TNTM can have multiple parents, thus an alternative to decrementing the counts is required. A detailed discussion on performing posterior inference and hyperparameter sampling is presented in the appendix.

## Twitter Data

For evaluation of the TNTM, we construct a tweet corpus from the Twitter 7 dataset (Yang  and Leskovec, 2011), 7 This corpus is queried using the hashtags #sport, #music, #finance, #politics, #science and #tech, chosen for diversity. We remove the non-English tweets with langid.py [(Lui and Baldwin, 2012)](#b54). We obtain the data on the followers network from [Kwak et al. (2010)](#b42). 8 However, note that this followers network data is not complete and does not contain information for all authors. Thus we filter out the authors that are not part of the followers network data from the tweet corpus. Additionally, we also remove authors who have written less than fifty tweets from the corpus. We name this corpus T6 since it is queried with six hashtags. It is consists of 240,517 tweets with 150 authors after filtering.

Besides the T6 corpus, we also use the tweet datasets described in [Mehrotra et al. (2013)](#b61). The datasets contains three corpora, each of them is queried with exactly ten query terms. The first corpus, named the Generic Dataset, are queried with generic terms. The second is named the Specific Dataset, which is composed of tweets on specific named entities. Lastly, the Events Dataset is associated with certain events. The datasets are mainly used for comparing the performance of the TNTM against the tweet pooling techniques in [Mehrotra et al. (2013)](#b61). We present a summary of the tweet corpora in Table [3](#tab_3).

## Experiments and Results

We consider several tasks to evaluate the TNTM. The first task involves comparing the TNTM with existing baselines on performing topic modelling on tweets. We also compare the TNTM with the random function network model on modelling the followers network. Next, we evaluate the TNTM with ablation studies, in which we perform comparison with 7. [http://snap.stanford.edu/data/twitter7.html](http://snap.stanford.edu/data/twitter7.html) 8. [http://an.kaist.ac.kr/traces/WWW2010.html](http://an.kaist.ac.kr/traces/WWW2010.html)  the TNTM itself but with each component taken away. Additionally, we evaluate the clustering performance of the TNTM, we compare the TNTM against the state-of-the-art tweets-pooling LDA method in [Mehrotra et al. (2013)](#b61).

## Experiment Settings

In all the following experiments, we vary the discount parameters α for the topic distributions µ 0 , µ 1 , ν i , η m , θ m , and θ m , we set α to 0.7 for the word distributions ψ, φ and γ to induce power-law behaviour [(Goldwater et al., 2011)](#b26). We initialise the concentration parameters β to 0.5, noting that they are learned automatically during inference, we set their hyperprior to Gamma(0.1, 0.1) for a vague prior. We fix the hyperparameters λ, s, l and σ to 1, as we find that their values have no significant impact on the model performance. 9

In the following evaluations, we run the full inference algorithm for 2,000 iterations for the models to converge. We note that the MH algorithm only starts after 1,000 iterations. We repeat each experiment five times to reduce the estimation error for the evaluations.

## Goodness-of-fit Test

We compare the TNTM with the HDP-LDA and a nonparametric author-topic model (ATM) on fitting the text data (words and hashtags). Their performances are measured using perplexity on the test set (see Section 4.5.2). The perplexity for the TNTM, accounting for both words and hashtags, is

$Perplexity(W • ) = exp - log p W • | ν, µ 1 , ψ, ψ D d=1 N d + M d ,(68)$where the likelihood p W

$• | ν, µ 1 , ψ, ψ is broken into p W • | ν, µ 1 , ψ, ψ = D d=1 M d m=1 p(y dm | ν, µ 1 , ψ ) N d n=1 p(w dn | y d , ν, µ 1 , ψ) .(69)$9. We vary these hyperparameters over the range of 0.01 to 10 during testing. on perplexity is from modelling the hashtags, which suggests that the hashtag information is the most important for modelling tweets. Second to the hashtags, the authorship information is very important as well. Even though modelling the power-law behaviour is not that important for perplexity, we see that the improvement on the network log likelihood is best achieved by modelling the power-law. This is because the flexibility enables us to learn the author-topic distributions better, and thus allowing the TNTM to fit the network data better. This also suggests that the authors in the corpus tend to focus on a specific topic rather than having a wide interest.

## Document Clustering and Topic Coherence

Mehrotra [et al. (2013)](#) shows that running LDA on pooled tweets rather than unpooled tweets gives significant improvement on clustering. In particular, they find that grouping tweets based on the hashtags provides most improvement. Here, we show that instead of resorting to such an ad hoc method, the TNTM can achieve a significantly better result on clustering. The clustering evaluations are measured with purity and normalised mutual information (NMI, see [Manning et al., 2008)](#b57) described in 4.5.3. Since ground truth labels are unknown, we use the respective query terms as the ground truth for evaluations. Note that tweets that satisfy multiple labels are removed. Given the learned model, we assign a tweet to a cluster based on its dominant topic. We perform the evaluations on the Generic, Specific and Events datasets for comparison purpose. We note the lack of network information in these datasets, and thus we employ only the HPYP part of the TNTM. Additionally, since the purity can trivially be improved by increasing the number of clusters, we limit the maximum number of topics to twenty for a fair comparison. We present the results in Table [6](#tab_4). We can see that the TNTM outperforms the pooling method in all aspects except on the Specific dataset, where it achieves the same purity as the best pooling scheme. 

## Automatic Topic Labelling

Traditionally, researchers assign a topic for each topic-word distribution manually by inspection. More recently, there have been attempts to label topics automatically in topic modelling. For instance, [Lau et al. (2011)](#b44) use Wikipedia to extract labels for topics, and [Mehdad et al. (2013)](#b60) use the entailment relations to select relevant phrases for topics. Here, we show that we can use hashtags to obtain good topic labels. In Table [7](#tab_5), we display the top words from the topic-word distribution ψ k for each topic k. Instead of manually assigning the topic labels, we display the top three hashtags from the topic-hashtag distribution ψ k . As we can see from Table [7](#tab_5), the hashtags appear suitable as topic labels. In fact, by empirically evaluating the suitability of the hashtags in representing the topics, we consistently find that, over 90 % of the hashtags are good candidates for the topic labels. Moreover, inspecting the topics show that the major hashtags coincide with the query terms used in constructing the T6 dataset, which is to be expected. This verifies that the TNTM is working properly.

## Conclusion

In this article, we proposed a topic modelling framework utilising PYPs, for which their realisation is a probability distribution or another stochastic process of the same type. In particular, for the purpose of performing inference, we described the CRP representation for the PYPs. This allows us to propose a single framework, discussed in Section 3, to implement these topic models, where we modularise the PYPs (and other variables) into blocks that can be combined to form different models. Doing so enables significant time to be saved on implementation of the topic models.

We presented a general HPYP topic model, that can be seen as a generalisation to the HDP-LDA [(Teh and Jordan, 2010)](#b78). The HPYP topic model is represented using a Chinese Restaurant Process (CRP) metaphor [(Teh and Jordan, 2010;](#b78)[Blei et al., 2010;](#b5)[Chen et al., 2011)](#b14), and we discussed how the posterior likelihood of the HPYP topic model can be modularised. We then detailed the learning algorithm for the topic model in the modularised form.

We applied our HPYP topic model framework on Twitter data and proposed the Twitter-Network Topic model (TNTM). The TNTM models the authors, text, hashtags, and the authors-follower network in an integrated manner. In addition to HPYP, the TNTM employs the Gaussian process (GP) for the network modelling. The main suggested use of the TNTM is for content discovery on social networks. Through experiments, we show that jointly modelling of the text content and the network leads to better model fitting as compared to modelling them separately. Results on the qualitative analysis show that the learned topics and the authors' topics are sound. Our experiments suggest that incorporating more auxiliary information leads to better fitting models.

## Future Research

For future work on TNTM, it would be interesting to apply TNTM to other types of data, such as blogs and news feeds. We could also use TNTM for other applications. such as hashtag recommendation and content suggestion for new Twitter users. Moreover, we could extend TNTM to incorporate more auxiliary information: for instance, we can model the location of tweets and the embedded multimedia contents such as URL, images and videos. Another interesting source of information would be the path of retweeted content.

Another interesting area of research is the combination of different kinds of topic models for a better analysis. This allows us to transfer learned knowledge from one topic model to another. The work on combining LDA has already been looked at by [Schnober and Gurevych (2015)](#b75), however, combining other kinds of topic models, such as nonparametric ones, is unexplored.

(z dn = k) and c θ d k for hashtag y dm (z dm = k). Decrementing the customer count may or may not decrement the respective table count. However, if the table count is decremented, then we would decrement the customer count of the parent PYP. This is relatively straight forward in Section 4.1 since the PYPs have only one parent. Here, when a PYP N has multiple parents, we would sample for one of its parent PYPs and decrement the table count corresponding to the parent PYP. Although not the same, the rationale of this procedure follows Section 4.1.

We explain in more details below. When the customer count c N k is decremented, we introduce an auxiliary variable u N k that indicates which parent of N to remove a table from, or none at all. The sample space for u N k is the P parent nodes P 1 , . . . , P P of N , plus ∅. When u N k is equal to P i , we decrement the table count t N →P i k and subsequently decrement the customer count c P i k in node P i . If u N k equals to ∅, we do not decrement any table count. The process is repeated recursively as long as a customer count is decremented, that is, we stop when u N k = ∅. The value of u N k is sampled as follows:

$p u N k = t N →P i k /c N k if u N k = P i 1 -P i p u N k = P i if u N k = ∅ .(70)$To illustrate, when a word w dn (with topic z dn ) is removed, we decrement c

θ d z dn , that is, c θ d z dn becomes c θ d z dn -1. We then determine if this word contributes to any table in node θ d by sampling u θ d z dn from Equation (70). If u θ d z dn = ∅, we do not decrement any table count and proceed with the next step in Gibbs sampling; otherwise, u θ d z dn can either be θ d or η d , in these cases, we would decrement t θ d →u θ d z dn z dn and c u θ d z dn

z dn , and continue the process recursively. We present the decrementing process in Algorithm 2. To remove a word w dn during inference, we would need to decrement the counts contributed by w dn (and z dn ). For the topic side, we decrement the counts associated with node N = θ d with group k = z dn using Algorithm 2. While for the vocabulary side, we decrement the counts associated with the node N = ψ z dn with group k = w dn . The effect of the word on the other PYP variables are implicitly considered through recursion.

Note that the procedure to decrementing a hashtag y dm is similar, in this case, we decrement the counts for N = θ d with k = z dm (topic side), then decrement the counts for N = ψ z dm with k = y dm (vocabulary side).

A.2 Sampling a New Topic for a Word or a Hashtag

After decrementing, we sample a new topic for the word or the hashtag. The sampling process follows the procedure discussed in Section 4.2, but with different conditional posteriors (for both the word and the hashtag). The full conditional posterior probability for the collapsed blocked Gibbs sampling can be derived easily. For instance, the conditional posterior for sampling the topic z dn of word w dn is

$p(z dn , T, C | Z •-dn , W • , T -dn , C -dn , Ξ) = p(Z • , T, C | W • , Ξ) p(Z • -dn , T -dn , C -dn | W • , Ξ)(71)$![Figure 1: Graphical model of the HPYP topic model. It is an extension to LDA by allowing the probability vectors to be modelled by PYPs instead of the Dirichlet distributions. The area on the left of the graphical model (consists of µ, ν and θ)is usually referred as topic side, while the right hand side (with γ and φ) is called the vocabulary side. The word node denoted by w dn is observed. The notations are defined in Table1.]()

![Figure 2: An illustration of the Chinese restaurant process representation. The customers are represented by the circles while the tables are represented by the rectangles.The dishes are the symbols in the middle of the rectangles, here they are denoted by the sunny symbol and the cloudy symbol. In this illustration, we know the number of customers corresponds to each table, for example, the green table is occupied by three customers. Also, since Restaurant 1 is the parent of Restaurant 2, the tables in Restaurant 2 are treated as the customers for Restaurant 1.]()

![Figure 3: An illustration of the Chinese restaurant with the table counts representation.Here the setting is the same as Figure2but the seating arrangement of the customers are "forgotten" and only the table and customer counts are recorded. Thus, we only know that there are three sunny tables in Restaurant 2, with a total of nine customers.]()

![Figure 4: Graphical model for the Twitter-Network Topic Model (TNTM) composed of a HPYP topic model (region b ) and a GP based random function network model (region a ).The author-topic distributions ν serve to link the two together. Each tweet is modelled with a hierarchy of document-topic distributions denoted by η, θ , and θ, where each is attuned to the whole tweet, the hashtags, and the words, in that order. With their own topic assignments z and z, the hashtags y and the words w are separately modelled. They are generated from the topic-hashtag distributions ψ and the topic-word distributions ψ respectively. The variables µ 0 , µ 1 and γ are priors for the respective PYPs. The connections between the authors are denoted by x, modelled by random function F.]()

![List of variables for the HPYP topic model used in this section.]()

![Summary of the datasets used in this section, showing the number of tweets (D), authors (A), unique word tokens (|V|), and the average number of words and hashtags in each tweet. The T6 dataset is queried with six different hashtags and thus has a higher number of hashtags per tweet. We note that there is a typo on the number of tweets for the Events Dataset inMehrotra et al. (2013), the correct number is 107,128.]()

![Clustering evaluations of the TNTM against the LDA with different pooling schemes. Note that higher purity and NMI indicate better performance. The results for the different pooling methods are obtained from Table4inMehrotra et al. (2013). The TNTM achieves better performance on the purity and the NMI for all datasets except for the Specific dataset, where it obtains the same purity score as the best pooling method.]()

![Topical analysis on the T6 dataset with the TNTM, which displays the top three hashtags and the top n words on six topics. Instead of manually assigning a topic label to the topics, we find that the top hashtags can serve as the topic labels.]()

c 2016 Lim, Buntine, Chen and Du. http://dx.doi.org/10.1016/j.ijar.2016.07.007

