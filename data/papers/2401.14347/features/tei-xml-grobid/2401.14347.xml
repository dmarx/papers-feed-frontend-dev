<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolving higher-order synergies reveals a trade-off between stability and information integration capacity in complex systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-30">January 30, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Varley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departmenet of Computer Science</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<settlement>Burlington</settlement>
									<region>VT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vermont Complex Systems Center</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<settlement>Burlington</settlement>
									<region>VT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Bongard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departmenet of Computer Science</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<settlement>Burlington</settlement>
									<region>VT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vermont Complex Systems Center</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<settlement>Burlington</settlement>
									<region>VT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolving higher-order synergies reveals a trade-off between stability and information integration capacity in complex systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-30">January 30, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">92AA5F44BA74F7FC6D61E82D3725F5DC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has recently been an explosion of interest in how "higher-order" structures emerge in complex systems comprised of many interacting elements (often called "synergistic" information). This "emergent" organization has been found in a variety of natural and artificial systems, although at present the field lacks a unified understanding of what the consequences of higher-order synergies and redundancies are for systems under study. Typical research treat the presence (or absence) of synergistic information as a dependent variable and report changes in the level of synergy in response to some change in the system. Here, we attempt to flip the script: rather than treating higher-order information as a dependent variable, we use evolutionary optimization to evolve boolean networks with significant higher-order redundancies, synergies, or statistical complexity. We then analyse these evolved populations of networks using established tools for characterizing discrete dynamics: the number of attractors, average transient length, and Derrida coefficient. We also assess the capacity of the systems to integrate information. We find that high-synergy systems are unstable and chaotic, but with a high capacity to integrate information. In contrast, evolved redundant systems are extremely stable, but have negligible capacity to integrate information. Finally, the complex systems that balance integration and segregation (known as Tononi-Sporns-Edelman complexity) show features of both chaosticity and stability, with a greater capacity to integrate information than the redundant systems while being more stable than the random and synergistic systems. We conclude that there may be a fundamental trade-off between the robustness of a systems dynamics and its capacity to integrate information (which inherently requires flexibility and sensitivity), and that certain kinds of complexity naturally balance this trade-off.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past two decades, information theory has emerged as something of a lingua franca for the study of complex systems, as it provides a natural, mathematical framework for exploring the relationships between "parts" and "wholes" in multivariate systems <ref type="bibr" target="#b0">[1]</ref>. In particular, information theory can provide insights into "emergent" or "higher-order" interactions, where information is encoded in the interaction between large numbers of different elements <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and, crucially, not accessible from a reduced subset (challenging the historic focus on scientific reductionism). Newly developed formal tools like the partial information decomposition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, the integrated information decomposition <ref type="bibr" target="#b6">[7]</ref>, and heuristics like the O-information 1 arXiv:2401.14347v2 [cs.IT] 26 Jan 2024 <ref type="bibr" target="#b7">[8]</ref> have empowered scientists to start looking for higher-order information structures in a variety of complex systems, often with great success. Higher-order synergies appear to be ubiquitous in natural and artificial systems, having been found in climate data <ref type="bibr" target="#b8">[9]</ref>, sociological data <ref type="bibr" target="#b9">[10]</ref>, artificial neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, cortical neuronal networks <ref type="bibr" target="#b13">[14]</ref>, and global brain dynamics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref>. Furthermore, alterations in redundancy and synergy seem to reflect meaningful differences between systems. For example, synergy in on-going brain dynamics has been found to decrease when consciousness is lost <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and change with age <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. At the cellular level, the synergies instantiated by individual neurons changes depending on task performance <ref type="bibr" target="#b20">[21]</ref> or drug administration <ref type="bibr" target="#b21">[22]</ref>, and in artificial neural networks, the distribution of redundancies and synergies changes over the course of the learning process <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Despite these results, it remains unclear exactly what the significance of these alterations imply in the general case. Mediano et al., recently proposed that synergistic information integration can be understood as a general measure of "computational complexity" in complex systems <ref type="bibr" target="#b6">[7]</ref>, although "complexity" is arguably as slippery a term as "synergy" <ref type="bibr" target="#b22">[23]</ref>. Broadly, the relationship between higher-order information and other well-understood features of dynamical systems is relatively under-explored.</p><p>Generally, when scientists study higher-order information in a system, the presence (or absence) of higherorder information is the dependent variable. How does synergy change when consciousness is lost? <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>? How does the synergy between social identities differ by demographic category <ref type="bibr" target="#b9">[10]</ref>? How do neural networks encode redundant and synergistic dependencies across training <ref type="bibr" target="#b11">[12]</ref>? In all of these studies, the change in redundancy or synergy is thought to be informative about some essential feature of the system's emergent properties, however synergy itself remains very abstract.</p><p>To try and resolve this ambiguity, in this study, we attempt to flip the script. Instead of treating higherorder information as the dependent variable that changes between conditions, here we "force" particular kinds of higher-order information into simple boolean networks, and then characterize how the presence of higher-order redundancies, synergies, and complexities alters their dynamics. We choose boolean networks for several reasons. The first is that, as naturally discrete models, they are very amenable to simple informationtheoretic analysis (indeed, elementary cellular automata have been a common test-bed for exploring emergent information dynamics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>). The second is that there is a long history of using boolean networks as toy models in complex systems. Seminal work by Kauffman showed how the structural properties of random boolean networks can inform on their dynamics <ref type="bibr" target="#b26">[27]</ref>, and we take direct inspiration from this work, although instead of altering structural properties of the network (degree, density, etc), we instead alter the computational properties of the individual nodes to inject redundancy or synergy into the logic. Finally, boolean networks are a very popular model in systems biology, where they are frequently used to model genetic and metabolic regulatory networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>By exploring the link between the global information structure of a system and its dynamics, we hope to address two outstanding questions in the field of complex systems: the first is understanding what it means when we observe redundancy and synergy in natural and artificial systems ("what have we really learned upon discovering that a system is high in synergy"?), and the second to understand how systems that require particular attributes (canalization, computational capacity, etc) might self-organize their internal dynamics in a way that supports those particular properties. Resolving both of these questions will help scientists in many fields both better understand the structure and function of complex systems, as well as potentially helping with the design of novel systems with desirable computational or dynamic properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Boolean Networks</head><p>A boolean network is a directed graph: G = {V, E} composed of vertices V and directed edges E that connect two nodes V i → V j . Each node V i ∈ V can be in one of two states: {0, 1}, and comes equipped with a function that maps the states of all the "parent" nodes to an updated state of the target node:</p><formula xml:id="formula_0">f : {0, 1} k → {0, 1},</formula><p>where k is the in-degree of the target node. Being naturally discrete models, boolean networks have some useful properties for the purposes of this analysis: for a network with N nodes, there will be only 2 N possible configurations the whole system can adopt (meaning a finite support set), and from any given initial condition, the system will always settle into an attractor after a finite period of time (meaning that the entire state-space can be brute-forced).</p><p>We used twelve-node boolean networks arranged into a ring lattice topology. Each node received four directed inputs from its four nearest neighbours, as well a self-loop ensuring that a given node's immediate past also factored into the computation of the next step (as is the case in the elementary cellular automata).</p><p>A size of twelve was selected as it optimized the trade-off between the runtime required to compute the O-information (which grows with system size), and the richness of the state-space that could be plausibly explored. Unlike the standard elementary cellular automata (which have been frequently used in the past to explore higher-order statistics in discrete systems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>), in our systems each node is allowed to implement its own unique function, vastly increasing the number of possible networks that can be evolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information-Theoretic Measures</head><p>We focused on three typical forms of higher-order information-sharing: redundancy (information duplicated over multiple elements simultaneously), synergy (information that is only present in the joint-state of all the variables and no simpler combination of sources), and "complexity" (the balance between independence and integration <ref type="bibr" target="#b34">[35]</ref>). To estimate the redundancy and synergy, we used the O-information <ref type="bibr" target="#b7">[8]</ref>. First introduced by James and Crutchfield as the "enigmatic information" <ref type="bibr" target="#b30">[31]</ref>, and then further refined by Rosas et al., the O-information provides a heuristic measure of whether a given multidimensional probability distribution (X) is dominated by redundancy (in which case Ω(X) &gt; 0 bit), or if it is dominated by synergy (in which case, Ω(X) &lt; 0 bit). Although the O-information can be written in several equivalent ways, here we prefer the form derived by Varley et al., <ref type="bibr" target="#b14">[15]</ref> as it only requires defining one additional function:</p><formula xml:id="formula_1">Ω(X) = (2 -N )T C(X) + N i=1 T C(X -i ) (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Where N is the number of elements, X -i is the set of all X j ∈ X excluding X i , and T C(X) is the total correlation of X:</p><formula xml:id="formula_3">T C(X) = N i=1 H(X i ) -H(X)<label>(2)</label></formula><p>The total correlation (and by extension, the O-information) is zero if X is comprised of independent elements (i.e. H(X) is maximal). If we understand the total correlation in terms of deviation from independence, the O-information can be interpreted as a measure of whether X's deviation from independence is in the "whole" or the lower-order "parts." If there is structure in the total correlation of the whole that is not Figure <ref type="figure">1</ref>: Intervention distribution. The state-transition structure of a boolean network defines a transition probability matrix (TPM), which gives the probability of a given output (the columns), conditional on a given input (the rows). Here we see the TPM for a three-element system X, and how the intervention distribution is computed. At time t, all global states are equally likely, corresponding to a maximum-entropy distribution. After one timestep, to time t + 1, the distribution of possible states is no longer uniform: the is the distribution of states after performing an intervention on X [36] -it is this distribution that is fed into the calculations for O-information and Tononi-Sporns-Edelman complexity.</p><p>accessible when considering the parts, then (2</p><formula xml:id="formula_4">-N )T C(X) &gt; N i=1 T C(X -i ).</formula><p>The reverse is true if most of the deviation from independence is in lower-order collections of elements.</p><p>To quantify the complexity, we used the Tononi-Sporns-Edelman (TSE) complexity <ref type="bibr" target="#b34">[35]</ref>. The TSE complexity was introduced by Tononi, Sporns, and Edelman in the context of theoretical neuroanatomy, and attempts to quantify the degree to which a system balances integration and segregation. A system is said to have a high complexity if, on average, each element is largely independent of every other element, but the whole system strongly deviates from independence. Like the O-information, the TSE complexity can be written out in terms of the total correlation:</p><formula xml:id="formula_5">T SE(X) = N -1 i=1 i N T C(X) -E[T C(X γ )] |γ|=i<label>(3)</label></formula><p>Where E[T C(X γ )] is the expected value of the total correlation of all subsets of X with γ elements.</p><p>Since the number of subsets of size γ grows with the binomial coefficient 12 γ , it is not practical to sample all possible subsets for all scales of i. Consequently, following <ref type="bibr" target="#b14">[15]</ref>, we took a subsampling approach: if the number of possible subsets of X was greater than 75, that was the number of randomly selected subsets sampled for each scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Intervention Distributions</head><p>Computing the information theoretic measures requires defining a probability distribution on X, which in this case is a distribution on the 2 12 possible states a given boolean network could adopt. Here, we used a modified intervention distribution: initially, all possibles states X = x were set to be equally likely (a maximum entropy distribution). Then, the system was allowed to update one timestep, producing a new distribution. Intuitively, this can be understood using a random walker model: if we imagine the statetransition structure of the whole system as a directed network with 2 12 nodes, we can "place" a single random walker on each node. Updating the system is like allowing the walkers to take a single step. The new distribution of walkers on nodes defines our intervention distribution: some nodes will now have more than one walker, some will have none, etc.</p><p>The use of the maximum entropy distribution can be understood in causal terms <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>: the updated distribution reflects the expected states of the system if it was intervened upon in a manner analogous to the do-calculus <ref type="bibr" target="#b35">[36]</ref>. This causal flavour separates the intervention distribution from other plausible choices, such as the stationary distribution <ref type="bibr" target="#b1">[2]</ref>, which in determinsitic boolean networks, incorporates a very small subset of states. In the context of two-element systems, the use of this interventional distribution with the mutual information is known as the "effective information" <ref type="bibr" target="#b36">[37]</ref>, and so we can think about the application to the O-information or the TSE-complexity as the "effective" O-information or "effective" complexity as well. For a visual explanation of the intervention distribution, see Figure <ref type="figure">1</ref> 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.4 Evolutionary Optimization</head><p>To evolve networks for redundancy, synergy, and complexity we implemented a naive evolutionary optimization. Initially, a population of 500 (if the measure was effective O-information) or 200 (if the measure was effective TSE complexity) random boolean networks were initialized. For each generation, the effective O-information or effective TSE complexity of each member of the population was computed, and the bottom half of the networks were removed. The remaining networks were then "mated" to produce offspring, with pairs selected randomly, with the probability of being selected proportional to the rank of that network within the population.</p><p>The process of producing a "child" network from two "parent" networks is combining the functions associated with each node. Since all networks have the same topology, we matched the nodes in each parent and constructed a new function with 50% of the input-output mappings coming from the first parent and the remaining input-output mappings coming from the second. We also built in an inherent mutation rate of 0.001, which is proportion of times a single input-output mapping is flipped. Each population was allowed to evolve for 750 generations, although the vast majority converged before 500 generations (for visualization, see Figure <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Characterizing Boolean Network Dynamics</head><p>To explore how controlling higher-order information influences dynamics, we analysed the final, evolved systems with a suit of well-established tools used for exploring automata models. The first is the number of attractors that the system can settle in. Since deterministic boolean networks will always fall into either a point-attractor or a limit cycle, the number of attractors is an efficient heuristic for how canalized the network is (for more on canalization in boolean networks, see <ref type="bibr" target="#b37">[38]</ref>). We also computed the transient time from every state to its inevitable attractor. Transient times are typically used to distinguish between boolean networks in an ordered, sub-critical phase (where transients are short) and a chaotic, super-critical phase (where transients are long) <ref type="bibr" target="#b38">[39]</ref>. The final measure was the Derrida coefficient <ref type="bibr" target="#b39">[40]</ref>, which quantifies how robust the system is to perturbation. We followed the method detailed by Manicka et al., <ref type="bibr" target="#b40">[41]</ref>. Briefly, we selected 2000 possible states of the network, and randomly perturbed each state by flipping m bits resulting in a state and a near-copy. Each state (original and copy) was then allowed to update one step and the Hamming distance between them was computed. We selected m from the range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and computed the Derrida coefficient as the slope of the linear regression of the average Hamming distance against m. A Derrida coefficient greater than one indicates a super-critical, chaotic system where small perturbations lead to large differences in future trajectories. A value less than one indicates a stable, sub-critical system where small perturbations have small effects. A Derrida coefficient of one indicates a critical system "on the edge of chaos" <ref type="bibr" target="#b41">[42]</ref>. These three measures provide a set of tools by which the relative stability or chaosticity of a given boolean network can be assessed and correlated with the presence, or absence, of higher-order information structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Information Integration</head><p>To quantify the extent to which these systems are capable of integrating information (sometimes used as a proxy measure for non-trivial "computation" <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>), we used a variation of the Φ value from integrated information theory <ref type="bibr" target="#b42">[43]</ref> following the outline detailed by Mediano et al., in their recent paper <ref type="bibr" target="#b6">[7]</ref>. Briefly, for a given boolean network X, we computed the difference between the effective information in the entire system and the sum of the effective informations after bi-partitioning X into non-overlapping subsets. This measure is typically referred to as Φ W M S :</p><formula xml:id="formula_6">Φ W M S (X) = I(X(t); X(t + 1)) -I(X α (t); X α (t + 1)) + I(X β (t); X β (t + 1))<label>(4)</label></formula><p>Where α and β define the two, non-overlapping partitions of X. Ideally, the partition (α, β) is constructed to minimize the loss of effective information when partitioning the system (the so-called "minimum information bipartition" <ref type="bibr" target="#b36">[37]</ref>), however this is known to be an intractable problem for even modestly sized systems, as it required brute-forcing all possible bipartitions of X. Here, we use a heuristic estimator from spectral graph theory: the algebraic connectivity and the Fiedler vector <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>We began by constructing an effective connectivity graph of the system, defining a matrix M such that M ij = I(X i (t); X j (t + 1)) + I(X j (t); X i (t + 1)). The result is a symmetric, low-dimensional representation of the dynamics of X, where each edge represents the total predictive information following from X i to X j and vice versa. For a dyadic, undirected graph, the, Fiedler vector is the eigenvector of the graph Laplacian associated with the smallest non-zero eigenvalue. From the Fiedler vector, it is possible to extract a bipartitioning of the graph that bisects the graph while approximately minimizing the total mass of the severed edges. This bisection defines the partition (α, β) used in computing Φ W M S . Spectral methods have been previously used to great effect in approximated the integrated information in large systems <ref type="bibr" target="#b45">[46]</ref>, and this method was chosen for its computational efficiency and conceptual simplicity (in contrast to other, more involved optimizations such as Queyranne's algorithm that are sometimes used for this problem <ref type="bibr" target="#b46">[47]</ref>). To ensure that the effective connectivity graphs were connected, a small amount of noise was added to each edge, on the order of 10 -6 bit. The computation of the Fiedler vector was done using the Networkx package.</p><p>The W M S in Φ W M S refers to the idea that it is a "whole-minus-sum": it is the difference in the predictive power of the whole system and the sum of the predictive power of the component parts. The higher the value, the more integrated information there is in the system. Unlike many information-theoretic quantities Φ W M S is not strictly non-negative, and the interpretation of Φ W M S &lt; 0 was a standing question in the field for years. Recently, however, Mediano et al., showed that negative values occur when dynamic redundancy shared by X α and X β overwhelms the integrated information <ref type="bibr" target="#b47">[48]</ref>. This prompted Mediano et al., to propose a corrected measure of integrated information: by adding back in the redundancy it is possible to ensure a non-negative value. Several different temporal redundancy functions have been proposed <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, here we use the minimum mutual information redundancy measure following <ref type="bibr" target="#b6">[7]</ref>: Φ R (X) := Φ W M S (X) + min γ,δ∈{α,β} I(X γ (t); X δ (t + 1))</p><p>(5)</p><p>We selected the top one hundred most redundant (highest O-information), most synergistic (lowest Oinformation), and most complex (highest TSE complexity) evolved boolean networks, and for each one we computed the minimum information bipartition and corrected integrated information Φ R (X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The first result is verifying that the evolutionary optimization worked and successfully evolved a population with non-trivial higher-order information structures. Figure <ref type="figure">2</ref> shows this: for positive O-information, negative O-information, and high TSE complexity, we successfully evolved populations that significantly deviated from the random initial conditions. The average O-information for random systems was -2.396 ± 0.198 bit, while for the high-redundancy systems it grew to 3.246 ± 1.063 bit and for the high-synergy systems it fell to -7.02 ± 0.872 bit. Similarly, the random systems had a low TSE-complexity 4.107 ± 0.285 bit, but after evolving for high complexity, the systems achieved an average 13.919 ± 0.609 bit. The evolution for redundancy showed a curiously biphasic pattern: many of the populations transiently saturated around 0 bit, before accelerating above zero in a second phase of growth.</p><p>Collectively, these show that the evolutionary optimization works to construct systems that have desired higher-order information-structures, and also that there is an apparent link between "randomness" and synergy: randomly initialized systems already have O-informations significantly lower than zero, and larger random systems have lower O-informations. This link will be extensively explored in subsequent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Dynamics: Stability and Chaos</head><p>When characterizing the different classes of systems, we found markedly different dynamics. The random systems had the highest average number of unique attractors (5.285 ± 2.715), followed by very closely by the synergistic systems (4.56±2.156): this small difference was nevertheless significant (U = 16893.5, p = 0.003).</p><p>The high-complexity systems came in third, with an average number of attractors of 3.91 ± 2.607, itself a significant decrease from the synergistic systems (U = 15494.5, p &lt; 4×10 -5 ). Finally, the redundant systems had the fewest average number distinct attractors (2.13±1.457), significantly lower than the high-complexity systems: U = 10314.0, p &lt; 3.77 × 10 -18 . These results suggest that the redundancy has a "canalizing" effect <ref type="bibr" target="#b37">[38]</ref>: given that all networks have the same number of possible states (2 12 ), the low number of attractors in the redundant system indicates that more initial conditions can lead to the same final state than in the random or synergistic cases.</p><p>When comparing the joint entropy of the systems after maximum-entropy perturbation (see Section 2.3), we found that the random systems had the highest joint entropy (10.606 ± 0.126 bit), which was significantly greater than the next-highest class: the high-synergy systems (10.061 ± 0.295, U = 1469.0, p &lt; 4.08 × 10 -58 ). The high-complexity systems had the next-lowest joint entropy (6.984 ± 0.488 bit), Figure <ref type="figure">2</ref>: Evolutionary optimization of redundancy, synergy, and complexity. Top row: Presented above are the evolutionary trajectories for all populations evolving for redundancy (left), synergy (middle), and TSE complexity (right). From random initial conditions, it is clear that the evolutionary optimization is able to discover many configurations that significantly deviate from the random initial conditions. This figure establishes the colour schema that will be used for the paper. Red for redundancy, blue for synergy, and gold for TSE complexity. Bottom row: For each of the three classes of system (high redundancy, high synergy, and high complexity), we selected fittest boolean networks and ran them to get a visual sense of their different properties. Note how the highly-redundant system almost immediately falls into a stable attractor, while the high-synergy system has a long transient time and overall visually noisier patterns. While these individual trajectories are one of many, they are representative of broader trends. For the redundant system, the average tansient time required to hit an attractor is merely 2.75 steps. In contrast, for the synergistic system, the average transient time is 26.91 steps, and for the TSE-maximizing system, the average transient time is 7.7 steps. significantly lower than the high-synergy systems (U = 0.0, p &lt; 2.42 × 10 -67 ). Finally, the high-redunancy systems had the lowest joint entropy (4.523 ± 0.818 bit), significantly lower than the high-complexity systems (U = 803.0, p &lt; 3.26 × 10 -62 )). These results are consistent with the notion that redundancy is canalizing while synergy (and randomness) maintain "flatter" configuration landscapes, although unlike the number of attractors (which describes the limit behaviour of the systems), these show a collapse in joint entropy a mere single timestep after perturbation. This suggests dynamics that are not only canalizing in the limit, but rapidly canalizing in the short term as well.</p><p>We can see a similar result when we consider the time it takes the systems to reach their various attractor states (the transient times). The high-synergy systems had the longest transients (43.965 ± 16.405 steps), which was just barely significantly greater than the random systems (38.96 ± 16.816 steps, U = 16300.0, p = 0.0007). The high-complexity systems had radically shorter transient times (6.671 ± 2.634 steps) compared to the random systems (U = 50.0, p &lt; 5.112 × 10 -67 ). Finally, the high-redundancy systems had very short transient times (U = 2146.5, p &lt; 4.27 × 10 -54 ).</p><p>The final measure of dynamic stability and chaosticity we measured was the Derrida coefficient <ref type="bibr" target="#b39">[40]</ref>, which quantifies the extent to which small perturbations propagate through time. A Derrida coefficient greater than one typically indicates chaos (i.e. small perturbations grow), while a Derrida coefficient less than one indicates stability (i.e. perturbations die out). We found that the synergistic systems had the highest average Derrida coefficients (2.155 ± 0.108), indicating highly sensitive, chaotic dynamics. The random systems had the next-highest coefficients (1.972 ± 0.053), a significant decrease from the synergistic systems (U = 3106.5,</p><formula xml:id="formula_7">p &lt; 1.185×10 -48</formula><p>). The high-complexity systems were closer to the critical point (1.428±0.103), significantly lower than the random systems (U = 0.0, p &lt; 2.414 × 10 -67 . Interestingly, the lowest coefficients were very near the critical boundary of one: the redundancy-dominated systems had average Derrida coefficients of 1.027 ± 0.156 (a significant decrease from the high-complexity systems, U = 1091.5, p &lt; 2.02 × 10 -60 ).</p><p>Collectively, these results point to a consistent picture of how higher-order information can influence the dynamical properties of a boolean network: the presence of synergy seems to be destabilizing, producing signatures of chaos like sensitivity to perturbation, long transience, and lower canalization. In contrast, redundancy had a clear canalizing effect, although the Derrida coefficients also placed the redundant systems at the critical boundary between subcritical and supercritical dynamics. The significance of this is currently unclear. The high-complexity systems, which use TSE-complexity to balance integration and segregation seemed to split the difference: generally more flexible than the highly-redundant systems, but much more stable than the random or synergistic systems. For visualization of all results, see Figure <ref type="figure" target="#fig_0">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Higher-Order Interactions Influence Information Integration Capacity</head><p>To assess how different higher-order information structures influenced the capacity of the system to integrated information, we selected the one hundred most redundant (highest effective O-information), most synergistic (lowest effective O-information), and most complex (highest effective TSE complexity) evolved boolean networks and computed the Φ R measure of integrated information. We also generated one hundred random networks to compare the evolved systems to.</p><p>We found that the random networks had the highest value of Φ R (4.46 bit ± 0.35). The second highest set was the synergistic set, with an average Φ R value of 3.672 bit ± 1.45. The Mann-Whitney U test found significant differences between these groups (U = 6476.0, p = 0.0003). After the synergisitc systems, the family with the next-highest average Φ R value were the high TSE complexity boolean networks (1.08 bit ± 0.537), which were significantly lower than the synergistic family (U = 1080.0, p &lt; 10 -21 ). Finally, the redundancy-dominated networks had the lowest capacity to integrate information, with an average Φ R of 0.162 bit ± 0.31. This was significantly lower than the TSE complexity group (U = 9308.5, p &lt; 10 -25 ).</p><p>For visualization see Figure <ref type="figure" target="#fig_0">3</ref>. These results are consistent with the previous dynamic results: the highly synergistic systems resemble the random systems, while the highly redundant system is profoundly different and the high-complexity systems split the difference, appearing between the two extremes. It also suggests that redundant systems are not particularly effective at integrating information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>After analysing a variety of signatures of dynamical complexity, we can begin to qualitatively describe the impacts of evolving for redundancy, synergy, and complexity. Broadly speaking, synergistic boolean networks resemble random boolean networks: both classes of network are highly sensitive to perturbation and have long transient times before reaching an attractor state (both indicators of chaotic dynamics). They also maintain high entropy dynamics and are capable of integrating information (as evidenced by the high values of Φ R ). In contrast, highly redundant systems are typically stable, with fewer unique attractors, short transient times, and a general robustness to perturbation (suggesting sub-critical dynamics). They do not, however, integrate much information: their "computational" or "information processing" complexity <ref type="bibr" target="#b6">[7]</ref> is very low. Finally, the systems that were evolved to have high TSE complexity <ref type="bibr" target="#b34">[35]</ref> seemed to sit between the redundant and the synergistic systems. They were more sensitive to perturbations and had slightly longer transients than the purely redundant systems, but also showed a greater capacity to integrate information.</p><p>These result suggest that there may be a fundamental trade-off between the capacity to integrate information, and the stability of the system. Systems that are too stable (such as those evolved for high O-information) cannot support information integration, while those systems that can integrate significant information are chaotic and unstable. This trade-off is reminiscent of the well-known compromise between redundancy and efficiency in the economics of supply chains: a supply chain with lots of redundancy can more effectively absorb shocks than a just-in-time chain that is more efficient but buckles under unexpected perturbations <ref type="bibr" target="#b49">[50]</ref>. Here, however, rather than a financial consideration, the consideration is whether the system is capable of "integrating" information from multiple streams into a unified whole.</p><p>One possible implication of this trade-off is that, if one wanted to build a complex system that was both capable of integrating large amounts of information but was stable enough to be predictable (such as an animal nervous system), one possible avenue might be to expand the number of redundant elements enough to stabilize the information-integration capacity. This might partially explain the apparent upward pressure on the size of animal brains over the course of evolution, particularly the recent expansion of brain region associated with information integration <ref type="bibr" target="#b15">[16]</ref>: the capacity to engage in complex information-processing may require a "substrate" of redundant components to stabilize the more complex integrative processes. This is, however, a significant leap from a small population of boolean networks and requires considerably more research before any strong claims can be made.</p><p>Maintaining a population of high-redundancy elements comes with its own energetic costs, and so a structure that innately balances the trade-off between stability and computational capacity may be still desirable. Our results suggest that a system with a high TSE complexity (which balances integration and segregation) <ref type="bibr" target="#b34">[35]</ref> may partially accomplish this. Across all measures, the systems evolved to maximize complexity (rather than high or low O-information) typically split the difference between the extremes of synergy and redundancy: being more stable than the high-synergy systems, but more flexible (and with greater computational capacity) than the purely redundant systems. These results are consonant with early findings by Sporns et al., who found that evolving for TSE complexity in simple systems produced topologies highly reminiscent of those seen in biological nervous systems <ref type="bibr" target="#b50">[51]</ref>. Future work further exploring the dynamical properties of the TSE-complexity, and the evolutionary contexts in which it emerges, may be highly informative about the general properties of evolved information-processing systems like the brain. Finally, the apparently similar behaviour between synergy and randomness is an intriguing result. It is known that Ω(X) = 0 bit ⇐⇒ X i ⊥X j for all X i and X j in X <ref type="bibr" target="#b7">[8]</ref>. The fact that the random systems had O-information values significantly less than zero strongly suggests a link between random systems and synergistic ones. This is not the first paper to suggest as such: Orio, Mediano, and Rosas recently showed that adding small amounts of stochastic noise to elementary cellular automata can transiently increase the synergy present in the system (as measured with the O-information as well) <ref type="bibr" target="#b25">[26]</ref>, and Varley et al., argued that synergistic entropy (in the context of the partial entropy decomposition <ref type="bibr" target="#b4">[5]</ref>) corresponds to irreducible randomness <ref type="bibr" target="#b51">[52]</ref>. The nature of this think remains mysterious, as randomness is typically associated with independence (and by extension, minimal O-information). Where this "structure" comes from, then, is a question of significant interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have shown how evolving small complex systems (in this case, twelve node boolean networks) for the presence of different higher-order information structures (redundancy, synergy, complexity) can influence the kinds of dynamics and computational capabilities the systems display. We found that evolving for redundancy produced systems that were robust to perturbation, highly canalized, and had low capacity to integrate information. Conversely, systems evolved for synergy resembled random systems in many respects: chaotic, sensitive to perturbation, but with a high capacity to integrate information. Finally, systems evolved to be TSE-complex (balancing integration and segregation) combined aspects of both extremes. We propose that there is a fundamental trade-off between stability and computational capacity, and that complex systems combining local segregation with global integration may naturally balance the two extremes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dynamical differences between random, synergistic, redundant, and complex systems. Top right: The number of unique attractors for each network, for each class. Top left: The joint entropy of the global state intervention distribution after one timestep. Middle left: The Derrida coefficient for each network, for each class. Middle right: The average length of the transients. Bottom: The integrated information capacity Φ R for each network for each class.</figDesc><graphic coords="9,218.30,444.29,175.39,183.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,72.00,72.00,468.01,184.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,72.00,257.05,468.00,321.69" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information Theory for Complex Systems Scientists</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Varley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12482</idno>
		<ptr target="http://arxiv.org/abs/2304.12482" />
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
	<note>physics, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence as the conversion of information: a unifying theory</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hoel</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsta.2021.0150</idno>
		<ptr target="https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0150.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page">20210150</biblScope>
			<date type="published" when="2022">2227. July 2022</date>
			<publisher>Royal Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greater than the parts: a review of the information decomposition approach to causal emergence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">I</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><forename type="middle">J</forename><surname>Luppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">B</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Carhart-Harris</surname></persName>
		</author>
		<author>
			<persName><surname>Bor</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsta.2021.0246</idno>
		<ptr target="https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0246.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page">20210246</biblScope>
			<date type="published" when="2022">2227. July 2022</date>
			<publisher>Royal Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nonnegative Decomposition of Multivariate Information</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><forename type="middle">D</forename><surname>Beer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1004.2515</idno>
		<idno>arXiv: 1004.2515</idno>
		<ptr target="http://arxiv.org/abs/1004.2515" />
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
	<note>math-ph, physics:physics, q-bio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Partial Entropy Decomposition: Decomposing multivariate entropy and mutual information via pointwise common surprisal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><surname>Ince</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01591</idno>
		<idno>arXiv: 1702.01591</idno>
		<ptr target="http://arxiv.org/abs/1702.01591" />
		<imprint>
			<date type="published" when="2017-02">February 2017</date>
		</imprint>
	</monogr>
	<note>cs, math, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generalized Decomposition of Multivariate Information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Varley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.08003</idno>
		<ptr target="http://arxiv.org/abs/2309.08003" />
		<imprint>
			<date type="published" when="2023-09">September 2023</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integrated information as a common signature of dynamical and informationprocessing complexity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">B</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName><surname>Barrett</surname></persName>
		</author>
		<idno type="DOI">10.1063/5.0063384</idno>
		<ptr target="https://aip.scitation.org/doi/10.1063/5.0063384.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<idno type="ISSN">1054-1500</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13115</biblScope>
			<date type="published" when="2022-01">January 2022</date>
			<publisher>American Institute of Physics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantifying Highorder Interdependencies via Multivariate Extensions of the Mutual Information</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gastpar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.100.032305</idno>
		<idno type="arXiv">arXiv:1902.11239</idno>
		<ptr target="http://arxiv.org/abs/1902.11239" />
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<idno type="ISSN">2470-0045, 2470-0053</idno>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal information partitioning: Characterizing synergy, uniqueness, and redundancy in interacting environmental variables</title>
		<author>
			<persName><forename type="first">Allison</forename><forename type="middle">E</forename><surname>Goodwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1002/2016WR020216</idno>
		<ptr target="https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR020216" />
	</analytic>
	<monogr>
		<title level="j">Water Resources Research</title>
		<idno type="ISSN">1944-7973</idno>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5920" to="5942" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Untangling Synergistic Effects of Intersecting Social Identities with Partial Information Decomposition</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kaminski</surname></persName>
		</author>
		<idno type="DOI">10.3390/e24101387</idno>
		<idno>Number: 10 Publisher</idno>
		<ptr target="https://www.mdpi.com/1099-4300/24/10/1387" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<idno type="ISSN">1099-4300</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1387</biblScope>
			<date type="published" when="2022-10">October 2022</date>
			<publisher>Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Partial Information Decomposition of Generative Neural Network Models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tycho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><surname>Shanahan</surname></persName>
		</author>
		<idno type="DOI">10.3390/e19090474</idno>
		<ptr target="https://www.mdpi.com/1099-4300/19/9/474" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">474</biblScope>
			<date type="published" when="2017-09">September 2017</date>
			<publisher>Number: 9 Publisher: Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Measure of the Complexity of Neural Representations based on Partial Information Decomposition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Alexander Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Christian</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viola</forename><surname>Priesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Makkeh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=R8TU3pfzFr" />
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<idno type="ISSN">2835-8856</idno>
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Synergistic information supports modality integration and flexible learning in neural networks solving multiple tasks</title>
		<author>
			<persName><forename type="first">Alexandra</forename><forename type="middle">M</forename><surname>Proca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">I</forename><surname>Luppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Crosby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02996</idno>
		<ptr target="http://arxiv.org/abs/2210.02996" />
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
		</imprint>
	</monogr>
	<note>cs, q-bio</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revealing the Dynamics of Neural Information Processing with Multivariate Information Decomposition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ehren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vibin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><forename type="middle">P</forename><surname>Parakkattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Sherrill</surname></persName>
		</author>
		<author>
			<persName><surname>Beggs</surname></persName>
		</author>
		<idno type="DOI">10.3390/e24070930</idno>
		<ptr target="https://www.mdpi.com/1099-4300/24/7/930.Number:7Publisher" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<idno type="ISSN">1099-4300</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">930</biblScope>
			<date type="published" when="2022-07">July 2022</date>
			<publisher>Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Faskowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Sporns</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42003-023-04843-w</idno>
		<idno>42003-023-04843-w. Number: 1 Publisher</idno>
		<ptr target="https://www.nature.com/articles/s" />
	</analytic>
	<monogr>
		<title level="j">Communications Biology</title>
		<idno type="ISSN">2399-3642</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2023-04">April 2023</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A synergistic core for human brain evolution and cognition</title>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">I</forename><surname>Luppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negin</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">D</forename><surname>Fryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">A</forename><surname>Stamatakis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-022-01070-0</idno>
		<ptr target="https://www.nature.com/articles/s41593-022-01070-0.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<idno type="ISSN">1546-1726</idno>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022-05">May 2022</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Synergistic Workspace for Human Consciousness Revealed by Integrated Information Decomposition</title>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">I</forename><surname>Luppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Allanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Pickard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">L</forename><surname>Carhart-Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Finoia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorina</forename><surname>Naci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">A</forename><surname>Stamatakis</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.11.25.398081v3</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2020.11.25.398081v3" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
	<note>Pages: 2020.11.25.398081 Section: New Results</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reduced emergent character of neural dynamics in patients with a disrupted connectome</title>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">I</forename><surname>Luppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Allanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Pickard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Finoia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Peattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">A</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName><surname>Stamatakis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2023.119926</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1053811923000745" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<idno type="ISSN">1053-8119</idno>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page">119926</biblScope>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-Order Interdependencies in the Aging Brain</title>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Gatica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Cofré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricio</forename><surname>Orio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibai</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">P</forename><surname>Swinnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><forename type="middle">M</forename><surname>Cortes</surname></persName>
		</author>
		<idno type="DOI">10.1089/brain.2020.0982</idno>
		<ptr target="https://liebertpub.com/doi/10.1089/brain.2020.0982" />
	</analytic>
	<monogr>
		<title level="j">Brain Connectivity</title>
		<idno type="ISSN">2158-0014</idno>
		<imprint>
			<date type="published" when="2021-04">April 2021</date>
			<publisher>Mary Ann Liebert, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-order functional redundancy in ageing explained via alterations in the connectome in a whole-brain model</title>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Gatica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibai</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">P</forename><surname>Swinnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricio</forename><surname>Orio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Cofré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><forename type="middle">M</forename><surname>Cortes</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010431</idno>
		<ptr target="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010431" />
	</analytic>
	<monogr>
		<title level="j">Publisher: Public Library of Science</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1010431</biblScope>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
	<note>PLOS Computational Biology</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Information-processing dynamics in neural networks of macaque cerebral cortex reflect cognitive state and behavior</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansjörg</forename><surname>Schaffelhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Scherberger</surname></persName>
		</author>
		<author>
			<persName><surname>Dann</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2207677120</idno>
		<ptr target="https://www.pnas.org/doi/10.1073/pnas.2207677120" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2207677120</biblScope>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
	<note>Publisher: Proceedings of the National Academy of Sciences</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The serotonergic psychedelic N,Ndipropyltryptamine alters information-processing dynamics in cortical neural circuits</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Havert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abolfazl</forename><surname>Fosque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naruepon</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Weerawongphrom</surname></persName>
		</author>
		<author>
			<persName><surname>Naganobori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Lily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><surname>Beggs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.20582</idno>
		<ptr target="http://arxiv.org/abs/2310.20582" />
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
		</imprint>
	</monogr>
	<note>nlin, q-bio</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measures of statistical complexity: Why?</title>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">P</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><surname>Crutchfield</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0375-9601(97)00855-4</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0375960197008554" />
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<idno type="ISSN">0375-9601</idno>
		<imprint>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Local Information Dynamics of Distributed Computation in Complex Systems</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-32952-4</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-642-32952-4" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Partial information decomposition as a spatiotemporal filter</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Flecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Beggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><forename type="middle">D</forename><surname>Beer</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.3638449</idno>
		<ptr target="https://aip.scitation.org/doi/full/10.1063/1.3638449.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<idno type="ISSN">1054-1500</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37104</biblScope>
			<date type="published" when="2011-09">September 2011</date>
			<publisher>American Institute of Physics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dynamical noise can enhance highorder statistical structure in complex systems</title>
		<author>
			<persName><forename type="first">Patricio</forename><surname>Orio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13454</idno>
		<ptr target="http://arxiv.org/abs/2305.13454" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>nlin</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emergent properties in random complex automata</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><surname>Kauffman</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-2789(84)90257-4</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/0167278984902574" />
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<idno type="ISSN">0167-2789</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984-01">January 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Boolean modeling of biological regulatory networks: A methodology tutorial</title>
		<author>
			<persName><forename type="first">Assieh</forename><surname>Saadatpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ymeth.2012.10.012</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1046202312002770" />
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<idno type="ISSN">1046-2023</idno>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boolean modeling: a logic-based dynamic approach for understanding signaling and regulatory networks and for making useful predictions</title>
		<author>
			<persName><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juilee</forename><surname>Thakar</surname></persName>
		</author>
		<idno type="DOI">10.1002/wsbm.1273</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/wsbm.1273" />
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Systems Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An Information-Theoretic Approach to Self-Organisation: Emergence of Complex Interdependencies in Coupled Dynamical Systems</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Ugarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.3390/e20100793</idno>
		<ptr target="https://www.mdpi.com/1099-4300/20/10/793" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anatomy of a bit: Information in a time series observation</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">P</forename><surname>Crutchfield</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.3637494</idno>
		<ptr target="https://aip.scitation.org/doi/abs/10.1063/1.3637494.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<idno type="ISSN">1054-1500</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37109</biblScope>
			<date type="published" when="2011-09">September 2011</date>
			<publisher>American Institute of Physics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A measure of statistical complexity based on predictive information with application to finite spin systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><surname>Plumbley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physleta.2011.10.066</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0375960111013880" />
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<idno type="ISSN">0375-9601</idno>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="281" />
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Genuine high-order interactions in brain networks and neurodegeneration</title>
		<author>
			<persName><forename type="first">Rubén</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sol</forename><surname>Fittipaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernando</forename><surname>Santamaria-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Cruzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustina</forename><surname>Birba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Moguilner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enzo</forename><surname>Tagliazucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Ibanez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.nbd.2022.105918</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0969996122003102" />
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Disease</title>
		<idno type="ISSN">0969-9961</idno>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page">105918</biblScope>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Emergence of High-Order Functional Hubs in the Human Brain</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Prejaas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Tewarie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Baudot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danillo</forename><surname>Luchicchi</surname></persName>
		</author>
		<author>
			<persName><surname>Barros De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">P</forename><surname>Girier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommy</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><surname>Broeders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Eduarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Centeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Cofre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Carone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelis</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjan</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Hillebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serafim</forename><surname>Desroches</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menno</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Schoonheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Douw</surname></persName>
		</author>
		<author>
			<persName><surname>Quax</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.02.10.528083v1</idno>
		<idno>02.10.528083v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2023" />
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
	<note>Pages: 2023.02.10.528083 Section: New Results</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A measure for brain complexity: relating functional segregation and integration in the nervous system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Edelman</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.91.11.5033</idno>
		<ptr target="https://www.pnas.org/content/91/11/5033" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quantifying causal emergence shows that macro can beat micro</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">P</forename><surname>Hoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larissa</forename><surname>Albantakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Tononi</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1314922110</idno>
		<ptr target="https://www.pnas.org/content/110/49/19790" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring information integration</title>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Sporns</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2202-4-31</idno>
		<ptr target="https://doi.org/10.1186/1471-2202-4-31" />
	</analytic>
	<monogr>
		<title level="j">BMC Neuroscience</title>
		<idno type="ISSN">1471-2202</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2003-12">December 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Canalization and Control in Automata Networks: Body Segmentation in Drosophila melanogaster</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Pita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0055946</idno>
		<ptr target="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0055946" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<idno type="ISSN">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">55946</biblScope>
			<date type="published" when="2013-03">March 2013</date>
			<publisher>Number: 3 Publisher: Public Library of Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effective Connectivity and Bias Entropy Improve Prediction of Dynamical Regime in Automata Networks</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Xavier Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">C</forename><surname>Rozum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<idno type="DOI">10.3390/e25020374</idno>
		<ptr target="https://www.mdpi.com/1099-4300/25/2/374" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<idno type="ISSN">1099-4300</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">374</biblScope>
			<date type="published" when="2023-02">February 2023</date>
			<publisher>Number: 2 Publisher: Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Phase Transitions in Two-Dimensional Kauffman Cellular Automata</title>
		<author>
			<persName><forename type="first">B</forename><surname>Derrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stauffer</surname></persName>
		</author>
		<idno type="DOI">10.1209/0295-5075/2/10/001</idno>
		<ptr target="https://dx.doi.org/10.1209/0295-5075/2/10/001" />
	</analytic>
	<monogr>
		<title level="j">Europhysics Letters</title>
		<idno type="ISSN">0295-5075</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">739</biblScope>
			<date type="published" when="1986-11">November 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effective connectivity determines the critical dynamics of biochemical networks</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Manicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Marques-Pita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsif.2021.0659</idno>
		<ptr target="https://royalsocietypublishing.org/doi/full/10.1098/rsif.2021.0659.Publisher" />
	</analytic>
	<monogr>
		<title level="j">Journal of The Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">186</biblScope>
			<biblScope unit="page">20210659</biblScope>
			<date type="published" when="2022-01">January 2022</date>
			<publisher>Royal Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Computation at the edge of chaos: Phase transitions and emergent computation</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Langton</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-2789(90)90064-V</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/016727899090064V" />
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<idno type="ISSN">0167-2789</idno>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="37" />
			<date type="published" when="1990-06">June 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Integrated information in discrete dynamical systems: motivation and theoretical framework</title>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Tononi</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1000091</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1000091</biblScope>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Algebraic Connectivity of Graphs</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslovak Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Handbook of Graph Theory</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Yellen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-12">December 2003</date>
			<publisher>Google-Books-ID: mKkIGIea BkC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Information integration in large brain networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Toker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Friedrich</forename><forename type="middle">T</forename><surname>Sommer</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006807</idno>
		<ptr target="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006807" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Algorithms for Searching the Minimum Information Partition in Integrated Information Theory</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Kitazono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masafumi</forename><surname>Oizumi</surname></persName>
		</author>
		<idno type="DOI">10.3390/e20030173</idno>
		<ptr target="https://www.mdpi.com/1099-4300/20/3/173" />
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<idno type="ISSN">1099-4300</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2018-03">March 2018</date>
			<publisher>Number: 3 Publisher: Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards an extended taxonomy of information dynamics via Integrated Information Decomposition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">E</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">I</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">L</forename><surname>Luppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Carhart-Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">B</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><surname>Barrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13186</idno>
		<idno>arXiv: 2109.13186</idno>
		<ptr target="http://arxiv.org/abs/2109.13186" />
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
		</imprint>
	</monogr>
	<note>physics, q-bio</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Decomposing past and future: Integrated information decomposition based on shared probability mass exclusions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Varley</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0282950</idno>
		<ptr target="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0282950" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<idno type="ISSN">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">282950</biblScope>
			<date type="published" when="2023-03">March 2023</date>
			<publisher>Publisher: Public Library of Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Trade-offs in Supply Chain System Risk Mitigation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">R</forename><surname>Swenseth</surname></persName>
		</author>
		<idno type="DOI">10.1002/sres.2299</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/sres.2299" />
	</analytic>
	<monogr>
		<title level="j">Systems Research and Behavioral Science</title>
		<idno type="ISSN">1099-1743</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="565" to="579" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Theoretical neuroanatomy and the connectivity of the cerebral cortex</title>
		<author>
			<persName><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><surname>Edelman</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0166-4328(02)00157-2</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0166432802001572" />
	</analytic>
	<monogr>
		<title level="j">Behavioural Brain Research</title>
		<idno type="ISSN">0166-4328</idno>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="74" />
			<date type="published" when="2002-09">September 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Partial entropy decomposition reveals higher-order information structures in human brain activity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Puxeddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faskowitz</forename><surname>Grazia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName><surname>Sporns</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2300888120</idno>
		<ptr target="https://www.pnas.org/doi/10.1073/pnas.2300888120" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page">2300888120</biblScope>
			<date type="published" when="2023-07">July 2023</date>
		</imprint>
	</monogr>
	<note>Publisher: Proceedings of the National Academy of Sciences</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
