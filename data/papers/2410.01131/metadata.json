{
  "arxivId": "2410.01131",
  "title": "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere",
  "authors": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg",
  "abstract": "We propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output\npredictions. These displacements are defined by the MLP and attention blocks,\nwhose vector components also reside on the same hypersphere. Experiments show\nthat nGPT learns much faster, reducing the number of training steps required to\nachieve the same accuracy by a factor of 4 to 20, depending on the sequence\nlength.",
  "url": "https://arxiv.org/abs/2410.01131",
  "issue_number": 857,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/857",
  "created_at": "2025-01-04T15:03:15.852952",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 122,
  "last_read": "2025-01-07T23:26:00.695853",
  "last_visited": "2025-01-07T23:23:22.376Z",
  "main_tex_file": null,
  "published_date": "2024-10-01T23:50:09Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI"
  ]
}