- **Normalized Transformer (nGPT) Overview**: A novel architecture where all vectors (embeddings, MLP, attention matrices, hidden states) are unit norm normalized, allowing operations on the hypersphere.

- **Key Contributions**:
  - **Hypersphere Optimization**: All network parameters are constrained to a unit norm hypersphere, enabling cosine similarity interpretations for matrix-vector multiplications.
  - **Variable-Metric Optimization**: Multi-step optimization per layer using eigen learning rates, enhancing convergence.

- **Faster Convergence**: nGPT reduces training steps by a factor of 4 to 20 for achieving the same accuracy compared to traditional Transformers.

- **Token Embeddings**:
  - Input and output embeddings are normalized after each training step to improve similarity estimation.
  - Logits computed as \( z_i = E_{\text{output}} h_i \) and normalized via softmax to produce probabilities \( P(y_i | x_1, \ldots, x_{i-1}) \).

- **Layer Structure**:
  - Alternating self-attention and MLP blocks with normalization applied after each transformation.
  - Update equations for attention and MLP blocks:
    - \( h \leftarrow \text{Norm}(h + \alpha_A(h_A - h)) \)
    - \( h \leftarrow \text{Norm}(h + \alpha_M(h_M - h)) \)

- **Self-Attention Mechanism**:
  - Queries, keys, and values are derived from normalized hidden states.
  - Attention scores computed as:
    \[
    \text{Attention}(q, k, v) = \text{softmax}\left(\frac{qk^T}{\sqrt{d_k}} + M\right)v
    \]
  - Normalization of \( q \) and \( k \) ensures bounded dot products.

- **MLP Block**:
  - Input hidden state normalized before passing through linear projections.
  - Gated activation function defined as:
    \[
    \text{SwiGLU}(u, \nu) = u \cdot \text{SiLU}(\nu)
    \]
  - Scaling factors introduced for controlling impact.

- **Effective Learning Rates in Adam**:
  - Core update rule:
    \[
    \theta \leftarrow \theta - \frac{\alpha m}{\sqrt{v} + \epsilon}
    \]
  - Effective step-size controlled by learning rate \( \alpha \).

- **SLERP and LERP**:
  - Spherical Linear Interpolation (SLERP) for geodesic paths on the hypersphere:
    \[
    SLERP(a, b; \alpha) = \frac{\sin((1 - \alpha)\theta)}{\sin(\theta)} a + \frac{\sin(\alpha\theta)}{\sin(\theta)} b
    \]
  - Approximation with Linear Interpolation (LERP):
    \[
    a \leftarrow a + \alpha(b - a)
    \]

- **Normalization Function**: 
  - \( \text{Norm}(x) \) normalizes any vector \( x \) to unit norm without introducing scaling factors.

- **Training Dynamics**: 
  - All matrices and embeddings normalized after each batch pass, ensuring stability and performance improvements.