- Decision to explore theoretical connections between SSMs and Transformers
- Choice of structured semiseparable matrices as a foundational concept
- Adoption of the state space duality (SSD) framework
- Design of the Mamba-2 architecture
- Implementation of efficient algorithms for computing SSMs
- Use of tensor contractions for linear attention proofs
- Introduction of structured masked attention (SMA)
- Decision to leverage existing Transformer optimizations for SSMs
- Choice of multi-input SSM (MIS) architecture
- Implementation of grouped-value attention (GVA) head structure
- Adoption of tensor parallelism for Mamba-2
- Design considerations for sequence parallelism in SSMs
- Decision to allow variable sequence lengths without padding
- Choice of empirical validation methods for Mamba-2 performance
- Decision to open-source model code and pre-trained checkpoints