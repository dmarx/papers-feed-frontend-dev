<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-31">31 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country>University</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-31">31 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">9CD67B9E10AE00C7A8ED9634A1A9643F</idno>
					<idno type="arXiv">arXiv:2405.21060v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8× faster, while continuing to be competitive with Transformers on language modeling. * Alphabetical by last name.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers, in particular decoder-only models (e.g. GPT <ref type="bibr" target="#b18">(Brown et al. 2020</ref>), Llama <ref type="bibr" target="#b100">(Touvron, Lavril, et al. 2023</ref>)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues <ref type="bibr" target="#b96">(Tay et al. 2022)</ref>, such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and Ré 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba <ref type="bibr" target="#b40">(Gu and Dao 2023</ref>)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective.</p><p>Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework <ref type="bibr">(Katharopoulos et al. 2020</ref>), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between "dual forms" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention.<ref type="foot" target="#foot_0">foot_0</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structured Matrices</head><p>Sec. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mamba-2</head><p>Sec. 7 State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:</p><p>• We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices.</p><p>• We significantly improve the theory of linear attention <ref type="bibr">(Katharopoulos et al. 2020)</ref>. We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4).</p><p>• We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM.</p><p>Beyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models.</p><p>Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is 2 -8× faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes (8× the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2K and 6× faster at sequence length 16K.</p><p>Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures.</p><p>We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g. in the style of Megatron <ref type="bibr" target="#b92">(Shoeybi et al. 2019</ref>)). The main ideas include introducing grouped-value attention (GVA) head structure, and moving all data-dependent projections to occur in parallel at the beginning of the block.</p><p>The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the Mamba-2 architecture. We investigate Chinchilla scaling laws for Mamba-2 in the same setting as Mamba, finding that it Pareto dominates Mamba and Transformer++ in both perplexity and wall-clock time. We additionally train a family of Mamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Transformers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on the Pile outperforms Mamba-2.8B, Pythia-2.8B and even Pythia-6.9B trained on the same dataset.</p><p>Systems Optimizations. The SSD framework connects SSMs and Transformers, allowing us to leverage a rich body of work on systems optimizations developed for Transformers (Section 8).</p><p>• For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models by splitting each layer across GPUs on the same node. We design Mamba-2 to be TP-friendly, reducing the number of synchronization point per block by half.</p><p>• For very long sequences whose activations do not fit on one device, sequence parallelism has been developed for the attention blocks. We describe how to train SSMs in general and Mamba-2 in particular with sequence parallelism, by passing the recurrent states between devices.</p><p>• For finetuning with examples of different lengths, for best efficiency, Transformer requires sophisticated techniques to remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained with variable sequence lengths efficiently, requiring no padding tokens.</p><p>Section 9 empirically validates Mamba-2 on language modeling, training efficiency, and a difficult multi-query associative recall task <ref type="bibr" target="#b5">(Arora, Eyuboglu, Zhang, et al. 2024)</ref>. Finally, in Section 10, we provide an extended related work and discuss potential research directions opened up by our framework.</p><p>Model code and pre-trained checkpoints are open-sourced at <ref type="url" target="https://github.com/state-spaces/mamba">https://github.com/state-spaces/mamba</ref>.</p><p>2 Background and Overview</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structured State Space Models</head><p>Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional sequence 𝑥 ∈ R T ↦ → 𝑦 ∈ R T through an implicit latent state ℎ ∈ R (T,N) .</p><p>A general discrete form of structured SSMs takes the form of equation <ref type="bibr" target="#b0">(1)</ref>.</p><p>ℎ 𝑡 = 𝐴ℎ 𝑡 -1 + 𝐵𝑥 𝑡 (1a)</p><formula xml:id="formula_0">𝑦 𝑡 = 𝐶 ⊤ ℎ 𝑡<label>(1b)</label></formula><p>ℎ 𝑡 = 𝐴 𝑡 ℎ 𝑡 -1 + 𝐵 𝑡 𝑥 𝑡 (2a)</p><formula xml:id="formula_1">𝑦 𝑡 = 𝐶 ⊤ 𝑡 ℎ 𝑡<label>(2b)</label></formula><p>where 𝐴 ∈ R (N,N) , 𝐵 ∈ R (N,1) , 𝐶 ∈ R (N,1) . Structured SSMs are so named because the 𝐴 matrix controlling the temporal dynamics must be structured in order to compute this sequence-to-sequence transformation efficiently enough to be used in deep neural networks. The original structures introduced were diagonal plus low-rank (DPLR) (Gu, Goel, and Ré 2022) and diagonal <ref type="bibr">(Gu, Gupta, et al. 2022</ref>; Gupta, Gu, and Berant 2022; J. T. Smith, Warrington, and Linderman 2023), which remains the most popular structure.</p><p>In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and convolutional models <ref type="bibr" target="#b44">(Gu, Johnson, Goel, et al. 2021</ref>). We provide a brief overview below, and refer to prior work for more context and details <ref type="bibr" target="#b39">(Gu 2023</ref>; Gu and Dao 2023).</p><p>Continuous-time Models. The original structured SSMs originated as continuous-time maps on functions 𝑥 (𝑡) ∈ R ↦ → 𝑦 (𝑡) ∈ R, rather than operating directly on sequences. In the continuous-time perspective, in equation (1a) the matrices (𝐴, 𝐵) are not directly learned but generated from underlying parameters ( Å, B), along with a parameterized step size Δ.</p><p>The "continuous parameters" (Δ, Å, B) are converted to "discrete parameters" (𝐴, 𝐵) through fixed formulas 𝐴 = 𝑓 𝐴 (Δ, Å) and 𝐵 = 𝑓 𝐵 (Δ, B), where the pair (𝑓 𝐴 , 𝑓 𝐵 ) is called a discretization rule.</p><p>Remark 1. While our main models adopt the same parameterization and discretization step as prior work (see <ref type="bibr" target="#b40">Gu and Dao (2023)</ref> for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on structured SSMs referred to the continuous parameters ( Å, B) and discrete parameters (𝐴, 𝐵) as (𝐴, 𝐵) and ( Ā, B) instead; we have changed notation to simplify the presentation and focus directly on the discrete parameters, which govern the main SSM recurrence.</p><p>Recurrent Models. Equations ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>) take the form of a recurrence which is linear in its input 𝑥. Structured SSMs can therefore be viewed as types of recurrent neural networks (RNNs), where the linearity endows them with additional properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplification, SSMs are still fully expressive as sequence transformations (in the sense of universal approximation) <ref type="bibr" target="#b54">(Kaul 2020;</ref><ref type="bibr" target="#b68">Orvieto et al. 2023</ref>; Shida Wang and Xue 2023).</p><p>Convolutional Models. When the SSM's dynamics are constant through time as in equation ( <ref type="formula" target="#formula_0">1</ref>), the model is called linear time-invariant (LTI). In this case, they are equivalent to convolutions. Thus, SSMs can also be viewed as types of CNNs, but where (i) the convolution kernels are implicitly parameterized through the SSM parameters (𝐴, 𝐵, 𝐶) and (ii) the convolution kernels are generally global instead of local. Conversely, through classical signal processing theory all sufficiently well-behaved convolutions can be represented as SSMs.</p><p>Commonly, previous LTI SSMs would use the convolutional mode for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode <ref type="bibr" target="#b0">(1)</ref> for efficient autoregressive inference (where the inputs are seen one step at a time).</p><p>Selective State Space Models. The form <ref type="bibr" target="#b1">(2)</ref> where the parameters (𝐴, 𝐵, 𝐶) can also vary in time was introduced in Mamba as the selective SSM. Compared to the standard LTI formulation <ref type="bibr" target="#b0">(1)</ref>, this model can selectively choose to focus on or ignore inputs at every timestep. It was shown to perform much better than LTI SSMs on information-dense data such as language, especially as its state size N increases allowing for more information capacity. However, it can only be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be efficient. Even so, it is still less efficient than hardware-friendly models such as CNNs and Transformers because it does not leverage matrix multiplication units, which modern accelerators such as GPUs and TPUs are specialized for.</p><p>While time-invariant SSMs are closely related to continuous, recurrent, and convolutional sequence models, they are not directly related to attention. In this paper, we show a deeper relationship between selective SSMs and attention, and use it to significantly improve the training speed of SSMs while simultaneously allowing for much larger state sizes N.</p><p>Structured SSMs as Sequence Transformations.</p><p>Definition 2.1. We use the term sequence transformation to refer to a parameterized map on sequences 𝑌 = 𝑓 𝜃 (𝑋 ) where 𝑋, 𝑌 ∈ R (T,P) and 𝜃 is an arbitrary collection of parameters. T represents the sequence or time axis; subscripts index into the first dimension, e.g. 𝑋 𝑡 , 𝑌 𝑡 ∈ R P .</p><p>Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or ( <ref type="formula" target="#formula_1">2</ref>) is a sequence transformation with P = 1; it can be generalized to P &gt; 1 by simply broadcasting across this dimension (in other words, viewing the input as P independent sequences and applying the SSM to each). One can think of P as a head dimension, which we will elaborate on in Section 7.</p><p>Definition 2.2. We define the SSM operator SSM(𝐴, 𝐵, 𝐶) = SSM(𝐴 0:𝑇 , 𝐵 0:𝑇 , 𝐶 0:𝑇 ) as the sequence transformation 𝑋 ∈ R (T,P) ↦ → 𝑌 ∈ R (T,P) defined by equation (2).</p><p>In SSMs, the N dimension is a free parameter called the state size or state dimension. We also call it the state expansion factor, because it expands the size of the input/output by a factor of 𝑁 , with implications for the computational efficiency of these models.</p><p>Finally, we remark that many types of sequence transformations, such as attention, can be represented as a single matrix multiplication across the sequence dimension.</p><p>Definition 2.3. We call a sequence transformation 𝑌 = 𝑓 𝜃 (𝑋 ) a matrix transformation if it can be written in the form 𝑌 = 𝑀 𝜃 𝑋 where 𝑀 is a matrix depending on the parameters 𝜃 . We identify the sequence transformation with the matrix 𝑀, and often drop the dependence on 𝜃 when clear from context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention</head><p>Attention broadly refers to a type of computation that assigns scores to every pair of positions in a sequence, allowing each element to "attend" to the rest. By far the most common and important variant of attention is softmax self-attention, which can be defined as</p><formula xml:id="formula_2">𝑌 = softmax(𝑄𝐾 ⊤ ) • 𝑉</formula><p>for 𝑄, 𝐾, 𝑉 ∈ R (T,P) . The mechanism of pairwise comparisons (induced by materializing 𝑄𝐾 ⊤ ) leads to the characteristic quadratic training cost of attention.</p><p>Many variants of attention have been proposed, but all share the underlying core of these attention scores, with various approximations (</p><p>Tay et al. 2022). The most important variant for this work is linear attention (Katharopoulos et al. 2020). Roughly speaking, this family of methods drops the softmax by folding it into a kernel feature map, and uses associativity of matrix multiplication to rewrite (𝑄𝐾 ⊤ ) • 𝑉 = 𝑄 • (𝐾 ⊤ 𝑉 ). Moreover, in the important case of causal (autoregressive) attention, they show that when the causal mask is incorporated into the left-hand side as (𝐿 • 𝑄𝐾 ⊤ ) • 𝑉 , where 𝐿 is the lower-triangular 1's matrix, then the right-hand side can be expanded as a recurrence. Several recent and concurrent works such as RetNet (Y. Sun et al. 2023) and GateLoop <ref type="bibr" target="#b53">(Katsch 2023</ref>) strengthen this to more general forms of 𝐿 (Section 10). In this work, our formulation of structured masked attention will strongly generalize these ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Structured Matrices</head><p>General matrices 𝑀 ∈ R (T,T) require T 2 parameters to represent and 𝑂 (T 2 ) time to perform basic operations such as matrix-vector multiplication. Structured matrices are those that (i) can be represented in subquadratic (ideally linear) parameters through a compressed representation, and</p><p>(ii) have fast algorithms (most importantly matrix multiplication) by operating directly on this compressed representation.</p><p>Perhaps the most canonical families of structured matrices are sparse and low-rank matrices. However, there exist many other families, such as Toeplitz, Cauchy, Vandermonde, and butterfly matrices, which have all been used in machine learning for efficient models <ref type="bibr">(Dao, Gu, et</ref> al. 2019; D. Fu et al. 2024; Gu, Gupta, et al. 2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overview: Structured State Space Duality</head><p>While this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically.</p><p>Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM <ref type="bibr" target="#b1">(2)</ref>. The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:</p><p>• The structure on 𝐴 is further simplified from diagonal to scalar times identity structure. Each 𝐴 𝑡 can also be identified with just a scalar in this case.</p><p>• We use a larger head dimension P, compared to P = 1 used in Mamba. Typically P = {64, 128} is chosen which is similar to conventions for modern Transformers.</p><p>Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators.</p><p>Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as</p><formula xml:id="formula_3">(𝐿 • 𝑄𝐾 ⊤ ) • 𝑉 𝐿 𝑖 𝑗 = 𝑎 𝑖 × • • • × 𝑎 𝑗+1 𝑖 ≥ 𝑗 0 𝑖 &lt; 𝑗</formula><p>where 𝑎 𝑖 are input-dependent scalars bounded in [0, 1].</p><p>Compared to standard softmax attention, there are two main differences</p><p>• The softmax is dropped.</p><p>• The attention matrix is multiplied elementwise-wise by an additional mask matrix 𝐿.</p><p>Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the "attention sink" phenomenon <ref type="bibr" target="#b28">(Darcet et al. 2024;</ref><ref type="bibr" target="#b106">Xiao et al. 2024</ref>). More importantly, the mask matrix 𝐿 can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time.</p><p>More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4.</p><p>Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form 𝑌 = 𝑀𝑋 for a matrix 𝑀 𝜃 ∈ R (T,T) that depends on 𝜃 = (𝐴, 𝐵, 𝐶).</p><p>In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix 𝑀, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in 𝑀.</p><p>Going beyond these, any algorithm for multiplication by 𝑀 can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of 𝑀, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs <ref type="bibr" target="#b40">(Gu and Dao 2023)</ref>; Listing 1 provides a complete implementation in a few lines of code.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> provides a simple roadmap of the relationships between the concepts presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Notation</head><p>Throughout this paper, we prefer using precise notation that can be mapped to code.</p><p>Matrices and Vectors. We generally use lower case to denote vectors (i.e. tensors with a single axis) and upper case to denote matrices (i.e. tensors with more than one axes). We do not bold matrices in this work. Sometimes, if a matrix is tied or repeated along one axis (and hence can also be viewed as a vector), we may use either upper or lower case for it. <ref type="foot" target="#foot_1">2</ref>• denotes scalar or matrix multiplication while • denotes Hadamard (elementwise) multiplication.</p><p>Indexing. We use Python-style indexing, e.g. 𝑖 : 𝑗 refers to the range (𝑖, 𝑖 + 1, . . . , 𝑗 -1) when 𝑖 &lt; 𝑗 and (𝑖, 𝑖 -1, . . . , 𝑗 + 1) when 𝑖 &gt; 𝑗. For example, for any symbol 𝑣 we let 𝑣 𝑗:𝑖 for 𝑗 ≥ 𝑖 denote the sequence (𝑣 𝑗 , . . . , 𝑣 𝑖+1 ).</p><p>[𝑖] is equivalent to 0 : 𝑖 = (0, . . . , 𝑖 -1). For shorthand, we also let 𝑣 × 𝑗:𝑖 denote the product</p><formula xml:id="formula_4">𝑣 𝑗 × • • • × 𝑣 𝑖+1 . 3</formula><p>Dimensions. To distinguish from matrices and tensors, we often use capital letters in typewriter fonts (e.g. D, N, T) to denote dimensions and tensor shapes. Instead of the traditional notation 𝑀 ∈ R 𝑇 ×𝑇 we frequently use 𝑀 ∈ R (T,T) to reflect tensor shapes in code.</p><p>Tensor Contractions. We will heavily rely on tensor contraction or einsum notation both for clarity and as a central tool in stating and proving our results. We assume the reader to be familiar with this notation, which is commonly used in modern tensor libraries such as numpy. For example, we can use contract(MN, NK → MK) to denote the matrix-matrix multiplication operator, and in our notation contract(MN, NK → MK) (𝑋, 𝑌 ) (which is equivalent to 𝑋 • 𝑌 ) can be translated to code as numpy.einsum( ′ mn, nk → mk ′ , X, Y).</p><p>A large glossary of notation is included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">State Space Models are Structured Matrices</head><p>This section explores different perspectives of the state space model as a sequence transformation, and outlines properties and algorithms of such maps. The main results of this section are about the equivalence between state space models and a family of structured matrices called semiseparable matrices, which imply new efficiency results (Theorems 3.5 and 3.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Matrix Transformation Form of State Space Models</head><p>Recall that our definition of an SSM is defined as a parameterized map defined through <ref type="bibr" target="#b1">(2)</ref>. Our theoretical framework starts by simply writing this transformation as a matrix multiplication mapping the vectors</p><formula xml:id="formula_5">𝑥 ∈ R T ↦ → 𝑦 ∈ R T .</formula><p>By definition, ℎ 0 = 𝐵 0 𝑥 0 . By induction,</p><formula xml:id="formula_6">ℎ 𝑡 = 𝐴 𝑡 . . . 𝐴 1 𝐵 0 𝑥 0 + 𝐴 𝑡 . . . 𝐴 2 𝐵 1 𝑥 1 + • • • + 𝐴 𝑡 𝐴 𝑡 -1 𝐵 𝑡 -2 𝑥 𝑡 -2 + 𝐴 𝑡 𝐵 𝑡 -1 𝑥 𝑡 -1 + 𝐵 𝑡 𝑥 𝑡 = 𝑡 ∑︁ 𝑠=0 𝐴 × 𝑡 :𝑠 𝐵 𝑠 𝑥 𝑠 .</formula><p>Multiplying by 𝐶 𝑡 to produce 𝑦 𝑡 and vectorizing the equation over 𝑡 ∈ [T], we derive the matrix transformation form of SSMs.</p><formula xml:id="formula_7">𝑦 𝑡 = 𝑡 ∑︁ 𝑠=0 𝐶 ⊤ 𝑡 𝐴 × 𝑡 :𝑠 𝐵 𝑠 𝑥 𝑠 𝑦 = SSM(𝐴, 𝐵, 𝐶) (𝑥) = 𝑀𝑥 𝑀 𝑗𝑖 𝐶 ⊤ 𝑗 𝐴 𝑗 • • • 𝐴 𝑖+1 𝐵 𝑖<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semiseparable Matrices</head><p>𝑀 in equation ( <ref type="formula" target="#formula_7">3</ref>) is a particular representation of a class of matrices known as semiseparable matrices. Semiseparable matrices are a fundamental matrix structure. We first define these matrices and their properties.</p><p>Definition 3.1. A (lower triangular) matrix 𝑀 is N-semiseparable if every submatrix contained in the lower triangular portion (i.e. on or below the diagonal) has rank at most N. We call N the order or rank of the semiseparable matrix.</p><p>Definition 3.1, and other forms of related "separable" structure (e.g. quasiseparable matrices and other definitions of semiseparable matrices) are sometimes called structured rank matrices (or rank-structured matrices) because they are characterized by rank conditions on their submatrices. Semiseparable matrices have many structured representations including the hierarchical semiseparable (HSS), sequential semiseparable (SSS), and Bruhat forms (Pernet and Storjohann 2018). We will primarily use the SSS form. 3.2.1 The Sequentially Semiseparable (SSS) Representation Definition 3.2. A lower triangular matrix 𝑀 ∈ R (T,T) has a N-sequentially semiseparable (SSS) representation if it can be written in the form</p><formula xml:id="formula_8">𝑀 𝑗𝑖 = 𝐶 ⊤ 𝑗 𝐴 𝑗 • • • 𝐴 𝑖+1 𝐵 𝑖<label>(4)</label></formula><p>for vectors 𝐵 0 , . . . , 𝐵 T-1 , 𝐶 0 , . . . , 𝐶 T-1 ∈ R N and matrices 𝐴 0 , . . . , 𝐴 T-1 ∈ R (N,N) .</p><p>We define the operator SSS so that 𝑀 = SSS(𝐴 0:T , 𝐵 0:T , 𝐶 0:T ).</p><p>A fundamental result of semiseparable matrices is that they are exactly equivalent to matrices with SSS representations. One direction can be deduced with a simple constructive proof.</p><p>Lemma 3.3. An N-SSS matrix 𝑀 with representation (4) is N-semiseparable.</p><p>Proof. Consider any off-diagonal block 𝑀 𝑗:𝑗 ′ ,𝑖 ′ :𝑖 where 𝑗 ′ &gt; 𝑗 ≥ 𝑖 &gt; 𝑖 ′ . This has an explicit rank-N factorization as</p><formula xml:id="formula_9">       𝐶 ⊤ 𝑗 𝐴 × 𝑗:𝑖 ′ 𝐵 𝑖 ′ . . . 𝐶 ⊤ 𝑗 𝐴 × 𝑗:𝑖 -1 𝐵 𝑖 -1 . . . . . . 𝐶 ⊤ 𝑗 ′ -1 𝐴 × 𝑗 ′ -1:𝑖 ′ 𝐵 𝑖 ′ . . . 𝐶 ⊤ 𝑗 ′ -1 𝐴 × 𝑗 ′ -1:𝑖 -1 𝐵 𝑖 -1        =        𝐶 ⊤ 𝑗 𝐴 × 𝑗:𝑗 . . . 𝐶 ⊤ 𝑗 ′ -1 𝐴 × 𝑗 ′ -1:𝑗        𝐴 × 𝑗:𝑖 -1 𝐴 × 𝑖 -1:𝑖 ′ 𝐵 𝑖 ′ • • • 𝐴 × 𝑖 -1:𝑖 -1 𝐵 𝑖 -1 .<label>(5)</label></formula><p>□ Equation ( <ref type="formula" target="#formula_9">5</ref>) will be used extensively in deriving our fast algorithms for sequence models. The other direction is wellestablished in the literature on semiseparable matrices.</p><p>Proposition 3.4. Every N-semiseparable matrix has a N-SSS representation.</p><p>Furthermore, note that although Definition 3.2 involves 𝑂 (N 2 T) parameters for the representation (in particular to store the 𝐴 matrices), it can actually be compressed down to 𝑂 (NT) parameters, which is asymptotically tight (Pernet, Signargout, and Villard 2023). Therefore in the rest of this paper we will conflate the structured matrix class (Definition 3.1) and a particular representation of it (Definition 3.2); we will always use this representation instead of other candidates. In turn we will use N-SS to refer to an N-semiseparable matrix in SSS form.</p><p>Semiseparable matrices are a fundamental matrix structure and have many important properties. They are deeply related to recurrences at large, and can be defined by multiple characterizations (e.g. Definitions 3.1 and 3.2) which reveal different connections and efficient algorithms for them. We mention some of their other properties in Appendix C.1.</p><p>Remark 2. The notion of semiseparability is very broad and many similar but subtlely different definitions appear in the literature; our definitions may differ slightly from other conventions. First, because we are primarily concerned with causal or autoregressive settings in this paper, we have restricted the definition of semiseparability to the triangular case; Definition 3.1 more formally might be called (N, 0)-semiseparability by some authors. Some authors may also instead refer to it as a form of quasiseparability (Eidelman and Gohberg 1999; Pernet 2016). See <ref type="bibr" target="#b102">Vandebril et al. (2005)</ref> for a brief survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">1-Semiseparable Matrices: the Scalar SSM Recurrence</head><p>We will single out the special case of 1-SS matrices. Note that in this case, the 𝐶 𝑗 and 𝐵 𝑖 are scalars, and can be factored out of the SSS representation (4) (we also use lower-case to emphasize that the parameters are scalars in this case)</p><formula xml:id="formula_10">SSS(𝑎, 𝑏, 𝑐) = diag(𝑐) • 𝑀 • diag(𝑏)</formula><p>where 𝑀 𝑗𝑖 = 𝑎 × 𝑗:𝑖 .</p><p>Since diagonal matrices are easy to handle (e.g. multiplication by a diagonal matrix is the same as elementwise scalar multiplication), we can ignore these terms. Thus our basic representation of a 1-SS matrix is 𝑀 𝑗𝑖 = 𝑎 𝑗:𝑖 or</p><formula xml:id="formula_11">𝑀 = 1SS(𝑎 0:𝑇 )            1 𝑎 1 1 𝑎 2 𝑎 1 𝑎 2 1 . . . . . . . . . . . . 𝑎 𝑇 -1 . . . 𝑎 1 𝑎 𝑇 -1 . . . 𝑎 2 . . . 𝑎 𝑇 -1 1            . (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>The importance of 1-SS matrices lies in their equivalence to the minimal form of a scalar recurrence -the case of a degenerate SSM with state dimension N = 1 and no (𝐵, 𝐶) projections. Note that multiplication 𝑦 = 𝑀𝑥 can be computed by the recurrence ) As sequence transformations, state space models can be represented as a matrix transformation 𝑀 ∈ R (T,T) acting on the sequence dimension T, sharing the same matrix for each channel in a head (Left). This matrix is a semiseparable matrix (Right), which is a rank-structured matrix where every submatrix contained on-and-below the diagonal (Blue) has rank at most N, equal to the SSM's state dimension.</p><formula xml:id="formula_13">𝑦 𝑡 = 𝑎 𝑡 :0 𝑥 0 + • • • + 𝑎 𝑡 :𝑡 𝑥 𝑡 = 𝑎 𝑡 (𝑎 𝑡 -1:0 𝑥 0 + • • • + 𝑎 𝑡 -1:𝑡 -1 𝑥 𝑡 -1 ) + 𝑎 𝑡 :𝑡 𝑥 𝑡 = 𝑎 𝑡 𝑦 𝑡 -1 + 𝑥 𝑡 .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>We thus also refer to matrix multiplication by 1-SS matrices as the scalar SSM recurrence or the cumprodsum (cumulative product sum; a generalization of cumulative product and cumulative sum) operator. As the fundamental form of recurrence, multiplication by 1-SS matrices is important as a building block for our main algorithms.</p><p>We emphasize that one of the central themes of this paper is that many algorithms on sequence models can be reduced to structured matrix multiplication algorithms. 1-SS matrices exemplify this connection: there are many fast algorithms for computing the primitive scalar recurrence or cumprodsum operator, and all of them turn out to be equivalent to different structured factorization of 1-SS matrices. We dedicate Appendix B to these algorithms for 1-SS matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">State Space Models are Semiseparable Matrices</head><p>Recall that our definition of an SSM is defined as a parameterized map defined through Definition 2.1. The connection between SSMs and semiseparable matrices follows from simply writing this transformation as a matrix multiplication mapping the vectors 𝑥 ↦ → 𝑦 ∈ R T .</p><p>Equation <ref type="bibr" target="#b2">(3)</ref> directly establishes the link between state space models and the sequentially semiseparable representation, which in turn are equivalent to semiseparable matrices in general (Lemma 3.3 and Proposition 3.4).</p><p>Theorem 3.5. The state space model transformation 𝑦 = SSM(𝐴, 𝐵, 𝐶) (𝑥) with state size N is identical to matrix multiplication by an N-SS matrix in sequentially semiseparable representation 𝑦 = SSS(𝐴, 𝐵, 𝐶) • 𝑥.</p><p>In other words the sequence transformation operator SSM (Definition 2.2) coincides with the matrix construction operator SSS (Definition 3.2), and we use them interchangeably (or sometimes SS as shorthand). Furthermore-by a twist of fate-structured state space models and sequentially semiseparable matrices have the same acronyms, underscoring their equivalence! Conveniently we can use any of these acronyms SSM (state space model or semiseparable matrix), SSS (structured state space or sequentially semiseparable), or SS (state space or semiseparable) interchangeably to unambiguously refer to either concept. However, we will generally use the convention that SSM refers to state space model, SS refers to semiseparable, and SSS refers to sequentially semiseparable.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the sequence transformation perspective of state space models as semiseparable matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computing State Space Models through Structured Matrix Algorithms</head><p>The reason Theorem 3.5 is important is that it will allow us to reduce the problem of efficient computation of SSMs (and other sequence models) into efficient algorithms for structured matrix multiplication. We briefly provide an overview and defer our main new algorithm to Section 6, after showing the equivalence of SSMs to other sequence models in Sections 4 and 5.</p><p>As previously defined, semiseparable matrices (i.e. rank-structured matrices) are a classical type of structured matrix:</p><p>(i) They have compressed representations such as the SSS form which has only 𝑂 (T) instead of 𝑂 (T 2 ) parameters.</p><p>(ii) They have fast algorithms operating directly on the compressed representation.</p><p>Furthermore, the parameterization and matrix multiplication cost can be tight in the semiseparable order.</p><p>Proposition 3.6 <ref type="bibr" target="#b75">(Pernet, Signargout, and Villard (2023)</ref>). An N-SS matrix of size T can be represented in 𝑂 (NT) parameters and has matrix-vector multiplication in time and space 𝑂 (NT).</p><p>For example, 1-SS matrices illustrate the essence of this connection. The matrix 𝑀 = 1SS(𝑎) is defined by exactly T -1 parameters 𝑎 0:T-1 = 𝑎 1 , . . . , 𝑎 T-1 , and can be computed in 𝑂 (T) time by following the scalar recurrence (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">The Linear (Recurrent) Mode</head><p>Proposition 3.6 can be easily seen in the case of diagonal structured SSMs (S4D <ref type="bibr">(Gu, Gupta, et al. 2022</ref>)), simply by leveraging the state space model formulation (2) and unrolling the recurrence. We provide the formal tensor-contraction algorithm in <ref type="bibr" target="#b7">(8)</ref>, where the dimension S is equal to T<ref type="foot" target="#foot_3">foot_3</ref> .</p><formula xml:id="formula_14">𝑍 = contract(SP, SN → SPN) (𝑋, 𝐵) (S, P, N)<label>(8a)</label></formula><formula xml:id="formula_15">𝐻 = contract(TSN, SPN → TPN) (𝐿, 𝑍 ) (T, P, N)<label>(8b)</label></formula><formula xml:id="formula_16">𝑌 = contract(TN, TPN → TP) (𝐶, 𝐻 ) (T, P)<label>(8c)</label></formula><p>Here, 𝐿 ∈ R (T,T) is defined as 1SS(𝐴), or in other words 𝐿 0:T,0:T = 1SS(𝐴 0:T ) for 𝑖 ∈ [N]. This algorithm involves three steps corresponding to (2):</p><p>(i) expanding the input 𝑋 by the input matrix 𝐵 (8a), (ii) unrolling independent scalar SSM recurrences (8b), and</p><p>(iii) contracting the hidden state 𝐻 by the output matrix 𝐶 (8c).</p><p>Note that we have used the equivalence between scalar SSMs and 1-SS matrices in step (8b).</p><p>Remark 3. We note that (8) is a special case of the Mamba (S6) model. however, a naive implementation is slow because of the expanded tensors 𝑍 and 𝐻 of size (T, P, N); Gu and Dao (2023) introduced a hardware-aware implementation to avoid materializing these tensors.</p><p>Surprisingly, Theorem 3.5 and Proposition 3.6 immediately imply that all SSMs have the same asymptotic efficiency as algorithm <ref type="bibr" target="#b7">(8)</ref>.</p><p>Theorem 3.7. Any state space model (Definition 2.2) of state size N on sequence length T can be computed in time 𝑂 (TN) (not accounting for potential preprocessing).</p><p>We note that this result is new to the structured SSM literature. In particular, given dense unstructured 𝐴 𝑡 matrices, the total representation alone seems to be of size 𝑂 (TN 2 ). Thus Theorem 3.7 states the non-trivial result that with a preprocessing step, even an unstructured SSM can be computed optimally efficiently, with upper bound matching the lower bound 𝑂 (TN) given by the size of 𝐵 and 𝐶.</p><p>Remark 4. Theorem 3.7 is perhaps not too surprising in light of the fact that almost all dense matrices over R (N,N) are diagonalizable over C, leading to the result that almost all dense real SSMs are equivalent to a diagonal complex SSM. This fact underlies the reason why diagonal SSMs are the most popular form of structured SSM <ref type="bibr">(Gu, Gupta, et al. 2022</ref>; Gupta, Gu, and Berant 2022; J. T. Smith, Warrington, and Linderman 2023). However, Theorem 3.7 implies the much stronger result for all real SSMs (not just the diagonalizable ones), as well as dense SSMs over other fields (including C itself).</p><p>In practice, efficiently computable SSMs still require additional structure on 𝐴, particularly to avoid the expensive preprocessing step (which both has order N extra FLOPs and involves hardware-inefficient operations such as singular value decompositions). These structures are the focus of past work on structured SSMs (e.g. S4(D) and Mamba) as well as our new algorithms. In particular, when slightly stronger structure is imposed on 𝐴, we will design very hardware-efficient algorithms through block decompositions of the SSM matrix 𝑀 = SSS(𝐴, 𝐵, 𝐶) in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">The Quadratic (Naive) Mode</head><p>We note that there is another way to compute an SSM exposed by our new matrix point of view. A naive computation of the matrix SSM representation (3) involves simply materializing the sequence transformation matrix 𝑀 = SSS(𝐴, 𝐵, 𝐶). This is a (T, T) matrix, and therefore this naive algorithm will scale quadratically in sequence length. However, when the sequence length T is short, this can actually be more efficient than the linear algorithm due to constant factors and the hardware-friendliness of the computation pattern (e.g. leveraging matrix-matrix multiplications). In fact, for a particular case of structured SSMs, this looks very similar to a quadratic attention computation (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Summary</head><p>Many sequence models are explicitly motivated or defined as matrix sequence transformations -most notably Transformers, where the matrix mixer is the attention matrix. On the other hand, RNNs and SSMs have not previously been described in this way. By providing an explicit matrix transformation form of state space models, we reveal new ways of understanding and using them. From a computational perspective, any method of computing the forward pass of a state space model can be viewed as a matrix multiplication algorithm on semiseparable matrices. The semiseparable matrix perspective provides one lens into state space duality (SSD), where the dual modes respectively refer to a linear-time semiseparable matrix multiplication algorithm and quadratic-time naive matrix multiplication.</p><p>Moreover, leveraging the rich structure of semiseparable matrices can lead to even better algorithms and more insights (e.g. Section 6 and Appendix B). In Appendix C.1, we describe some additional properties of semiseparable matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structured Masked Attention: Generalizing Linear Attention with Structured Matrices</head><p>In this section we revisit the linear attention framework from first principles. The main results in this section are a simple tensor-contraction-based proof of linear attention (Proposition 4.1), and our generalized abstraction of structured masked attention in Definition 4.2. We note that this section derives the main duality results from a different direction than state space models and can be read completely independently of Section 3.</p><p>• Section 4.1 sets up our framework for variants of attention, with a particular focus on kernel attention and masked kernel attention.</p><p>• Section 4.2 provides our first main attention result, a simple proof of linear attention through the lens of tensor contractions.</p><p>• Section 4.3 defines structured masked attention, our generalization of prior attention variants through structured matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Attention Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Attention</head><p>The basic form of (single-head) attention is a map on three sequences of vectors (𝑄, 𝐾, 𝑉 ) ↦ → 𝑌 .</p><formula xml:id="formula_17">𝑄 = input (T, N) 𝐾 = input (S, N) 𝑉 = input (S, P) 𝐺 = 𝑄𝐾 ⊤ (T, S) 𝑀 = 𝑓 (𝐺) (T, S) 𝑌 = 𝐺𝑉 (T, P)<label>(9)</label></formula><p>We use "shape annotations" to indicate the dimensions of tensors, e.g. 𝑄 ∈ R (T,N) . In this general form, S and T represent source and target sequence lengths, N represents the feature dimension, and P represents the head dimension.</p><p>The most common variant of softmax attention uses a softmax activation 𝑓 = softmax to normalize the rows of the 𝐺 matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Self-Attention</head><p>Our treatment is motivated by the most important case of self-attention, where (i) the source and target sequences are the same (i.e. S = T),</p><p>(ii) usually the feature and head dimensions are the same (i.e. N = P),</p><p>(iii) and 𝑄, 𝐾, 𝑉 are generated by linear projections on the same input vector</p><formula xml:id="formula_18">(𝑄 = 𝑊 𝑄 • 𝑋, 𝐾 = 𝑊 𝐾 • 𝑋, 𝑉 = 𝑊 𝑉 • 𝑋 ).</formula><p>However, our presentation abstracts away these choices and begins from the 𝑄, 𝐾, 𝑉 matrices.</p><p>Remark 5. Our focus is on the self-attention case with equal head and feature dimensions (i.e. S = T and N = P), which should be used as the running example. We define the general formulation of attention not only so that our framework captures variants such as cross-attention, but also because separating the notation for dimensions (e.g. S and T) makes the contraction notation proofs of our main results in this section more clear.</p><p>Remark 6. While attention is usually framed as an operation on these three inputs 𝑄, 𝐾, 𝑉 which are viewed symmetrically, the input and output dimensions in (9) indicate otherwise. In particular, the feature dimension N is not present in the output; therefore in the case when S = T (e.g. self-attention), we view 𝑉 as the main input, so that (9) defines a proper sequence transformation 𝑉 ↦ → 𝑌 (Definition 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Kernel Attention</head><p>The step where the softmax function is applied to the Gram matrix 𝐺 can be decomposed into two parts:</p><p>1. Exponentiating the 𝐺 matrix.</p><p>2. Normalizing the 𝐺 matrix on the S axis.</p><p>We can ignore the normalization term for now, as it amounts to simply passing in 𝑉 = 1 and dividing (we revisit this in Section 7.3). The exponentiation term can be viewed as a kernel transformation: there is an (infinite-dimensional) feature map 𝜑 such that exp(𝑄𝐾 ⊤ ) = 𝜑 (𝑄)𝜑 (𝐾) ⊤ . By abstracting away the feature map into the definition of 𝑄 and 𝐾 itself (i.e. define 𝑄, 𝐾 as the post-transformed versions), we can ignore the softmax transformation, and assume that 𝑄, 𝐾 are arbitrarily generated by kernel feature maps and potentially N ≠ P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many instantiations of kernel attention have been proposed, including:</head><p>• The original Linear Attention <ref type="bibr">(Katharopoulos et al. 2020</ref>) defines the kernel feature map as an arbitrary pointwise activation function, such as 𝑥 ↦ → 1 + elu(𝑥).</p><p>• Random Feature Attention (RFA) (H. <ref type="bibr" target="#b73">Peng et al. 2021</ref>) chooses the kernel feature map to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels <ref type="bibr" target="#b86">(Rahimi and Recht 2007)</ref>. This involves random projections (i.e. multiplying 𝑄 and 𝐾 by a random projection 𝑊 and applying the activation 𝑥 ↦ → (cos(𝑥), sin(𝑥)).</p><p>• Performer <ref type="bibr" target="#b19">(Choromanski et al. 2021)</ref> proposes the fast attention via positive orthogonal random features (FAVOR+).</p><p>The positive random features (PRF) part chooses the kernel feature map to be a random projection followed by the feature map 𝑥 ↦ → 2 -1/2 (exp(𝑥), exp(-𝑥)). This choice is motivated so that the kernel elements are positive-valued and provably approximates the softmax attention. [It also proposes choosing the random projections in orthogonal directions, which we do not consider.]</p><p>• cosFormer <ref type="bibr">(Qin, Weixuan Sun, et al. 2022</ref>) augment RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality. This effectively passes 𝑄 𝑡 , 𝐾 𝑡 through the feature map 𝑥 ↦ → (𝑥 cos(𝜋𝑡/2𝑇 ), sin(𝜋𝑡/2𝑇 )).</p><p>• Linear Randomized Attention (Zheng, C. Wang, and Kong 2022) generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exptransformed numerator).</p><p>Other related attention variants include Linformer (Sinong Wang et al. 2020) and Nyströformer <ref type="bibr" target="#b107">(Xiong et al. 2021)</ref>, which both use low-rank approximations of the attention matrix 𝑀 (and are thus compatible with equation ( <ref type="formula" target="#formula_17">9</ref>)), through random projections (Johnson-Lindenstrauss) and kernel approximation (the Nyström method) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Masked (Kernel) Attention</head><p>Let 𝐿 be a mask of shape (T, S). Most commonly, in the autoregressive self-attention case when S = T, 𝐿 may be a lowertriangular matrix of 1's representing a causal mask. Besides enforcing causality, many other types of masks can be applied -in particular various sparsity patterns such as banded, dilated, or block diagonal -which are motivated by reducing the complexity of dense attention.</p><p>Masked attention is usually written in matrix notation as</p><formula xml:id="formula_19">𝑦 = (𝐿 • (𝑄𝐾 ⊤ )) • 𝑉 .<label>(10)</label></formula><p>More precisely, with shape annotations and breaking this down into the precise sequence of computations:</p><formula xml:id="formula_20">𝐺 = 𝑄𝐾 ⊤ (T, S) 𝑀 = 𝐺 • 𝐿 (T, S) 𝑌 = 𝑀𝑉 (T, P)<label>(11)</label></formula><p>Our improved derivation of attention variants in this section starts by noticing that this formula can be written as a single contraction:</p><formula xml:id="formula_21">𝑌 = contract(TN, SN, SP, TS → TP) (𝑄, 𝐾, 𝑉 , 𝐿)<label>(12)</label></formula><p>and the algorithm in ( <ref type="formula" target="#formula_20">11</ref>) can be reframed as computing ( <ref type="formula" target="#formula_21">12</ref>) by a particular ordering of pairwise contractions</p><formula xml:id="formula_22">𝐺 = contract(TN, SN → TS) (𝑄, 𝐾) (T, S)<label>(13a)</label></formula><formula xml:id="formula_23">𝑀 = contract(TS, TS → TS) (𝐺, 𝐿) (T, S)<label>(13b)</label></formula><formula xml:id="formula_24">𝑌 = contract(TS, SP → TP) (𝑀, 𝑉 ) (T, P) (13c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Linear Attention</head><p>Linear attention, and many other variants of efficient attention, is often motivated by changing the order of matrix associativity in the core attention computation (𝑄𝐾 ⊤ )𝑉 = 𝑄 (𝐾 ⊤ 𝑉 ). However when the mask is added, the derivation is somewhat less straightforward (for example, the original paper <ref type="bibr">(Katharopoulos et al. 2020</ref>) and variants (Y. <ref type="bibr" target="#b95">Sun et al. 2023</ref>) state the formula without proof).</p><p>Roughly, the linear attention method claims that the following formula is equivalent to <ref type="bibr" target="#b10">(10)</ref>, which must be verified by expanding the sum and tracking indices carefully. </p><formula xml:id="formula_25">𝑌 = 𝑄 • cumsum(𝐾 ⊤ 𝑉 )<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">A Tensor Contraction Proof of Linear Attention</head><p>We present a simple and rigorous derivation of linear attention that will also immediately reveal how to generalize it. The main idea is to perform the contraction <ref type="bibr" target="#b12">(12)</ref> in an alternate order. We avoid ambiguous matrix notation and work directly with contraction notation:</p><formula xml:id="formula_26">𝑍 = contract(SP, SN → SPN) (𝑉 , 𝐾) (S, P, N) (15a) 𝐻 = contract(TS, SPN → TPN) (𝐿, 𝑍 ) (T, P, N) (15b) 𝑌 = contract(TN, TPN → TP) (𝑄, 𝐻 ) (T, P)<label>(15c)</label></formula><p>Intuitively, we interpret this contraction order as follows.</p><p>The first step (15a) performs an "expansion" into more features, by a factor of the feature dimension N. The third step (15c) contracts the expanded feature dimension away. If 𝐾 is viewed as the input (Remark 6), then 𝑉 and 𝑄 perform the expansion and contraction, respectively.</p><p>The second step is the most critical, and explains the linear part of linear attention. First notice that (15b) is just a direct matrix multiplication by 𝐿 (since the (P, N) axes can be flattened). Also note that this is the only term that involves both T and S axes, hence should have Ω(TS) complexity (i.e. quadratic in sequence length). However, when the mask 𝐿 is the standard causal attention mask (lower triangular 1's), matrix-vector multiplication by 𝐿 is identical to a feature-wise cumulative sum</p><formula xml:id="formula_27">𝑦 =        1 . . . . . . 1 . . . 1        𝑥 ⇐⇒ 𝑦 0 = 𝑥 0 𝑦 𝑡 = 𝑦 𝑡 -1 + 𝑥 𝑡 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Structured Masked Attention</head><p>With the tensor contraction perspective of masked attention <ref type="bibr" target="#b15">(15)</ref>, we can immediately see that the crux of the original linear attention is the fact that matrix-vector multiplication by the causal mask is equivalent to the cumulative sum operator.</p><p>However, we observe that there is no reason the attention mask has to be all 1's. All that is necessary for linear attention to be fast is for 𝐿 to be a structured matrix, which by definition are those that have fast matrix multiplication (Section 2.3).</p><p>In particular, we can use any mask matrix 𝐿 that has sub-quadratic (ideally linear) matrix-vector multiplication, which would have the same complexity as standard linear attention by speeding up the bottleneck equation (15b). The SMA quadratic mode algorithm is the sequence of pairwise contractions defined by <ref type="bibr" target="#b13">(13)</ref>, which corresponds to the standard (masked) attention computation.</p><p>The SMA linear mode algorithm is the sequence of pairwise contractions defined by <ref type="bibr" target="#b15">(15)</ref>, where step (15b) is optimized through the subquadratic structured matrix multiplication.</p><p>We can instantiate structured masked attention to any given class of matrix structure. Some examples include (Figure <ref type="figure" target="#fig_4">3</ref>):</p><p>• Linear attention uses a causal mask.</p><p>• • The decay mask could be generalized to a Toeplitz matrix 𝐿 𝑖 𝑗 = 𝛼 𝑖 -𝑗 for some learnable (or input-dependent) set of parameters 𝛼 ∈ R T . This can be interpreted as a form of relative positional encoding, reminiscent of other methods such as AliBi (Press, N. Smith, and Lewis 2022) but multiplicative instead of additive.</p><p>• Another variant could use a Fourier matrix 𝐿 𝑖 𝑗 = 𝜔 𝑖 𝑗/T to encode positional structure a different way.</p><p>In Section 5, we consider semiseparable SMA, which defines our main SSD model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Summary: The Dual Forms of Masked Attention</head><p>Standard (masked kernel) attention is often conflated between a function and an algorithm. Separating this distinction presents a clear way to understand different variants of attention.</p><p>• We view masked attention as a particular function <ref type="bibr" target="#b12">(12)</ref>.</p><p>• The standard quadratic attention computation (13) can be viewed as an algorithm to compute the function.</p><p>• Linear attention (15) is an alternate algorithm to compute the same function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moreover, in this case</head><p>• The masked attention function is simply a particular contraction on four terms.</p><p>• The quadratic and linear attention algorithms are simply two different orders to perform the contractions.</p><p>It is known that contraction orderings can make large differences in computation complexity, leading to the quadratic vs. linear split. Just as state space models are a transformation that can be computed in multiple ways, with dual quadratic vs. linear forms (Section 3.4), linear attention has a similar duality that results from two contraction orders. In fact, these turn out to be different perspectives on the same underlying duality, which we make explicit in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">State Space Duality</head><p>In Sections 3 and 4, we defined structured state space models and structured attention, discussed their properties, and showed that they both have a quadratic algorithm and a linear algorithm. This section connects them together. Our main result is showing that a particular case of structured state space models coincides with a particular case of structured attention, and that the linear-time SSM algorithm and quadratic-time kernel attention algorithm are dual forms of each other.</p><p>• Section 5.1 specializes state space models to scalar structure, where the naive quadratic computation can be seen as an instance of kernel attention.</p><p>• Section 5.2 specializes structured masked attention to semiseparable SMA, which characterizes masked attention with efficient autoregression.</p><p>• Section 5.3 summarizes the connection between structured masked attention and structured state space models, termed structured state space duality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scalar-Identity Structured State Space Models</head><p>In Section 3 we showed that state space models are equivalent to semiseparable matrix transformations, resulting in both a linear recurrent form and quadratic naive form.</p><p>Recall that SSMs are defined by 𝑦 = SSM(𝐴, 𝐵, 𝐶) (𝑥), and the matrix form of SSMs uses the SSS (sequentially semiseparable) representation 𝑀 = SSS(𝐴, 𝐵, 𝐶) where 𝑀 𝑗𝑖 = 𝐶 ⊤ 𝑗 𝐴 𝑗:𝑖 𝐵 𝑖 (equation ( <ref type="formula" target="#formula_7">3</ref>)). Now let us consider the case where 𝐴 𝑗 is simply a scalar; in other words, an instantiation of a structured SSM where the 𝐴 matrices are extremely structured: 𝐴 = 𝑎𝐼 for scalar 𝑎 and identity matrix 𝐼 . Then we can rearrange</p><formula xml:id="formula_28">𝑀 𝑗𝑖 = 𝐴 𝑗:𝑖 • (𝐶 ⊤ 𝑗 𝐵 𝑖 ).</formula><p>And this can be vectorized into</p><formula xml:id="formula_29">𝐿 1SS(𝑎) 𝑀 = 𝐿 • (𝐶𝐵 ⊤ )</formula><p>where 𝐵, 𝐶 ∈ R (T,N) .</p><p>Using this formulation, the full output 𝑌 = 𝑀𝑋 is computed precisely as</p><formula xml:id="formula_30">𝐺 = contract(TN, SN → TS) (𝐶, 𝐵) (T, S) 𝑀 = contract(TS, TS → TS) (𝐺, 𝐿) (T, S) 𝑌 = contract(TS, SP → TP) (𝑀, 𝑋 ) (T, P)<label>(16)</label></formula><p>where S = T. But this is exactly the same as original definition of masked kernel attention definition (13)! Therefore, as alluded to in Section 3.4, naively computing the scalar structured SSM-by materializing the semiseparable matrix 𝑀 and performing quadratic matrix-vector multiplication-is exactly the same as quadratic masked kernel attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">1-Semiseparable Structured Masked Attention</head><p>Structured masked attention allows for the use of any structured mask 𝐿. When 𝐿 is the causal mask, it is standard linear attention. Note that the causal mask is 𝐿 = SS(1 𝑇 ), i.e. the 1-SS mask is generated by 𝑎 𝑡 = 1 in definition <ref type="bibr" target="#b5">(6)</ref>. This motivates generalizing 𝐿 to the class of 1-semiseparable masks, or 1-semiseparable structured masked attention (1-SS SMA), where the cumsum in linear attention's recurrence is replaced by a more general recurrence -the scalar SSM scan, i.e. 1-semiseparable matrix multiplication (Section 3.2.2).</p><p>Finally, the most important reason we consider 1-semiseparable SMA is because the linear form for computing it is a special case of diagonal state space model. The linear form of SMA is algorithm <ref type="bibr" target="#b15">(15)</ref>, where the bottleneck step (15b) can be viewed as matrix multiplication by the 1-SS mask. In Section 3, we also wrote out the computation for a diagonal SSM <ref type="bibr" target="#b7">(8)</ref>, where the bottleneck step (8b) is a scalar SSM recurrence which is equivalent to 1-SS multiplication. The only difference is that (8b) has an extra N dimension in 𝐿, because the matrix 𝐴 is a diagonal matrix of size N. This N dimension would disappear if all diagonal entries of 𝐴 are the same, which results in Corollary 5.1.</p><p>Corollary 5.1. 1-SS SMA (masked attention with 1-semiseparable structured matrices 𝐿) (15) is a special case of a diagonal SSM <ref type="bibr" target="#b7">(8)</ref> where the diagonal matrix is a scalar multiple of the identity.</p><p>While Corollary 5.1 says that 1-SS SMA has an efficient recurrent form, we can also show a converse result that characterizes which instances of SMA has efficient autoregression.</p><p>Theorem 5.2. For any instantiation of structured masked attention (Definition 4.2) that is an autoregressive process with bounded order, the structured mask 𝐿 must be a semiseparable matrix.</p><p>In other words, efficient autoregressive attention is general semiseparable SMA. Theorem 5.2 is proved in Appendix C.2.</p><p>Remark 7. While 1-semiseparable SMA is a special case of a state space model, general semiseparable SMA is strictly more expressive than 1-SS SMA, and cannot be described by a standard SSM. However, the semiseparable multiplication by 𝐿 and the linear form of SMA (equation (15a)) each involve an expansion and contraction step, and can be absorbed into a similar instance of 1-SS SMA with a single (larger) expansion.</p><p>In summary, 1-semiseparable structured attention is the most important case of SMA, because it is:</p><p>• a natural generalization of linear attention with an input-dependent recurrence.</p><p>• the simplest case of general semiseparable attention, which is equivalent to efficient autoregressive attention.</p><p>• a special case of a diagonal state space model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Structured State-Space Duality (SSD)</head><p>To summarize our results:</p><p>• Structured state-space models (Section 3) are a model usually defined through a linear-time recurrence. However, by expanding the matrix formulation characterizing its linear sequence-to-sequence transformation, one can derive a quadratic form.</p><p>• Attention variants (Section 4) are a model defined through quadratic-time pairwise interactions. However, by viewing it as a four-way tensor contraction and reducing in a different order, one can derive a linear form.</p><p>• A natural special case of each one -more precisely, state space models with scalar-identity structure on the 𝐴 matrices, and structured masked attention with 1-semiseparable structure on its 𝐿 mask -are duals of each other with the exact same linear and quadratic forms.</p><p>Figure <ref type="figure">4</ref> summarizes the duality between these two representations.</p><p>An extended related work and discussion (Section 10) describes the relationship between SSD and general SSMs / attention in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A Hardware-Efficient Algorithm for SSD Models</head><p>The benefits of developing the theoretical SSD framework between SSMs, attention, and structured matrices lies in using the connections to improve the models and algorithms. In this section, we show how various algorithms for computing SSD models efficiently can be derived from various algorithms for computing structured matrix multiplication.</p><p>Our main computational result is an algorithm for computing SSD models that combines both the linear (recurrent) mode and quadratic (attention) mode. This algorithm is as computation efficient as SSMs (linear scaling in sequence length) and as hardware-friendly as attention (primarily uses matrix multiplications).</p><p>Theorem 6.1. Consider an SSD model with state expansion factor N and head dimension P = N. There exists an algorithm for computing the model on any input 𝑋 ∈ R (T,P) which only requires 𝑂 (TN 2 ) training FLOPs, 𝑂 (TN) inference FLOPs, 𝑂 (N 2 ) inference memory, and whose work is dominated by matrix multiplications. Structured State Space Model Structured Masked Attention 𝐶 (contraction matrix) 𝑄 (queries) 𝐵 (expansion matrix) 𝐾 (keys) 𝑋 (input sequence) 𝑉 (values) 𝐴 𝑗:𝑖 (state matrix) 𝐿 𝑗𝑖 (mask) N (state expansion dim.) N (kernel feature dim.) 𝐻 (hidden states (8b)) SMA linear dual (15) = 𝐿 • 𝑋 𝐵 (linear mode) SSM quadratic dual (16) 𝐺 (Gram matrix (13a)) = 𝑄 • 𝐾 ⊤ (quadratic mode) Structured State Space Model (SSM) Diagonal State Space Model Scalar-Identity SSM Structured Masked Attention (SMA) Semiseparable SMA 1-Semiseparable SMA S4D S5 S6 Linear Attention Efficient Autoregressive Attention RetNet TransNormer GateLoop Structured State Space Duality (SSD) DSS S4 Figure 4: (Structured State Space Duality.) State space duality describes the close relationship between state space models and masked attention. (Left) General SSMs and SMA both possess linear and quadratic forms, with direct analogs in notation. (Right) SSMs and SMA intersect at a large class of state space dual models (SSD) which capture many sequence models as special cases. Note that all of these bounds are tight, because a state space model with state expansion N operating on a head size of N has total state size N 2 (yielding the lower bounds for training and inference FLOPs of 𝑂 (TN 2 ) and 𝑂 (N 2 ) respectively). Furthermore the input 𝑋 itself has TN elements, yielding the memory lower bound.</p><p>The main idea behind Theorem 6.1 is once again viewing the problem of computing a state space model as a semiseparable matrix multiplication, but leveraging its structure in a new way. Instead of computing the whole matrix in either recurrent or attention mode, we perform a block decomposition of the matrix. The diagonal blocks can be computed using the dual attention mode, which can be efficiently done with matrix multiplications, while the off-diagonal blocks can be factored by the rank-structure of semiseparable matrices and reduced to a smaller recurrence. We highlight that Listing 1 provides a self-contained implementation of the SSD algorithm. Compared to the general selective SSM of Gu and Dao (2023), this implementation is much simpler, and relatively efficient even in native PyTorch without requiring special low-level kernels.</p><p>To begin, we partition the matrix 𝑀 into a T Q × T Q grid of submatrices of size Q × Q, for some block size Q. Note that the off-diagonal blocks are low-rank by the defining property of semiseparable matrices (Definition 3.1). <ref type="foot" target="#foot_4">5</ref>(Block Decomposition)</p><formula xml:id="formula_31">𝑀 =          𝑀 (0,0) 𝑀 (1,0) 𝑀 (1,1) . . . . . . . . . 𝑀 (T/Q-1,0) 𝑀 (T/Q-1,1) . . . 𝑀 (T/Q-1,T/Q-1)          (Diagonal Block) 𝑀 ( 𝑗,𝑗 ) = SSM(𝐴 𝑗Q:( 𝑗+1)Q , 𝐵 𝑗Q:( 𝑗+1)Q , 𝐶 𝑗Q:( 𝑗+1)Q ) (Low-Rank Block) 𝑀 ( 𝑗,𝑖 ) =        𝐶 ⊤ 𝑗Q 𝐴 𝑗Q:𝑗Q-1 . . . 𝐶 ⊤ ( 𝑗+1)Q-1 𝐴 ( 𝑗+1)Q-1:𝑗Q-1        𝐴 𝑗Q-1:(𝑖+1)Q-1        𝐵 ⊤ 𝑖Q 𝐴 (𝑖+1)Q-1:𝑖Q . . . 𝐵 ⊤ (𝑖+1)Q-1 𝐴 (𝑖+1)Q-1:(𝑖+1)Q-1        ⊤</formula><p>This is easiest illustrated through an example, e.g. for T = 9 and decomposing into chunks of length Q = 3. The shaded cells are low-rank factorizations of the off-diagonal blocks of the semiseparable matrix.</p><formula xml:id="formula_32">𝑀 =                       𝐶 ⊤ 0 𝐴 0:0 𝐵 0 𝐶 ⊤ 1 𝐴 1:0 𝐵 0 𝐶 ⊤ 1 𝐴 1:1 𝐵 1 𝐶 ⊤ 2 𝐴 2:0 𝐵 0 𝐶 ⊤ 2 𝐴 2:1 𝐵 1 𝐶 ⊤ 2 𝐴 2:2 𝐵 2 𝐶 ⊤ 3 𝐴 3:0 𝐵 0 𝐶 ⊤ 3 𝐴 3:1 𝐵 1 𝐶 ⊤ 3 𝐴 3:2 𝐵 2 𝐶 ⊤ 3 𝐴 3:3 𝐵 3 𝐶 ⊤ 4 𝐴 4:0 𝐵 0 𝐶 ⊤ 4 𝐴 4:</formula><p>1 𝐵 1 𝐶 ⊤ 4 𝐴 4:2 𝐵 2 𝐶 ⊤ 4 𝐴 4:3 𝐵 3 𝐶 ⊤ 4 𝐴 4:4 𝐵 4 𝐶 ⊤ 5 𝐴 5:0 𝐵 0 𝐶 ⊤ 5 𝐴 5:1 𝐵 1 𝐶 ⊤ 5 𝐴 5:2 𝐵 2 𝐶 ⊤ 5 𝐴 5:3 𝐵 3 𝐶 ⊤ 5 𝐴 5:4 𝐵 4 𝐶 ⊤ 5 𝐴 5:5 𝐵 5 𝐶 ⊤ 6 𝐴 6:0 𝐵 0 𝐶 ⊤ 6 𝐴 6:1 𝐵 1 𝐶 ⊤ 6 𝐴 6:2 𝐵 2 𝐶 ⊤ 6 𝐴 6:3 𝐵 3 𝐶 ⊤ 6 𝐴 6:4 𝐵 4 𝐶 ⊤ 6 𝐴 6:5 𝐵 5 𝐶 ⊤ 6 𝐴 6:6 𝐵 6 𝐶 ⊤ 7 𝐴 7:0 𝐵 0 𝐶 ⊤ 7 𝐴 7:1 𝐵 1 𝐶 ⊤ 7 𝐴 7:2 𝐵 2 𝐶 ⊤ 7 𝐴 7:3 𝐵 3 𝐶 ⊤ 7 𝐴 7:4 𝐵 4 𝐶 ⊤ 7 𝐴 7:5 𝐵 5 𝐶 ⊤ 7 𝐴 7:6 𝐵 6 𝐶 ⊤ 7 𝐴 7:7 𝐵 𝐶 ⊤ 8 𝐴 8:0 𝐵 0 𝐶 ⊤ 8 𝐴 8:1 𝐵 1 𝐶 ⊤ 8 𝐴 8:2 𝐵 2 𝐶 ⊤ 8 𝐴 8:3 𝐵 3 𝐶 ⊤ 8 𝐴 8:4 𝐵 4 𝐶 ⊤ 8 𝐴 8:5 𝐵 5 𝐶 ⊤ 8 𝐴 8:6 𝐵 6 𝐶 ⊤ 8 𝐴 8:7 𝐵 𝐶 ⊤ 8 𝐴 8:8 𝐵 8</p><formula xml:id="formula_33">                      =                       𝐶 ⊤ 0 𝐴 0:0 𝐵 0 𝐶 ⊤ 1 𝐴 1:0 𝐵 0 𝐶 ⊤ 1 𝐴 1:1 𝐵 1 𝐶 ⊤ 2 𝐴 2:0 𝐵 0 𝐶 ⊤ 2 𝐴 2:1 𝐵 1 𝐶 ⊤ 2 𝐴 2:2 𝐵 2 𝐶 ⊤ 3 𝐴 3:3 𝐵 3 𝐶 ⊤</formula><p>4 𝐴 4:3 𝐵 3 𝐶 ⊤ 4 𝐴 4:4 𝐵 4 𝐶 ⊤ 5 𝐴 5:3 𝐵 3 𝐶 ⊤ 5 𝐴 5:4 𝐵 4 𝐶 ⊤ 5 𝐴 5:5 𝐵 5 𝐶 ⊤ 6 𝐴 6:6 𝐵 6 𝐶 ⊤ 7 𝐴 7:6 𝐵 6 𝐶 ⊤ 7 𝐴 7:7 𝐵 𝐶 ⊤ 8 𝐴 8:6 𝐵 6 𝐶 ⊤ 8 𝐴 8:7 𝐵 𝐶 ⊤ 8 𝐴 8:8 𝐵 8</p><formula xml:id="formula_34">      𝐶 ⊤ 3 𝐴 3:2 𝐶 ⊤ 4 𝐴 4:2 𝐶 ⊤ 5 𝐴 5:2       𝐴 2:2       𝐵 ⊤ 0 𝐴 2:0 𝐵 ⊤ 1 𝐴 2:1 𝐵 ⊤ 2 𝐴 2:2       ⊤       𝐶 ⊤ 6 𝐴 6:5 𝐶 ⊤ 7 𝐴 7:5 𝐶 ⊤ 8 𝐴 8:5       𝐴 5:2       𝐵 ⊤ 0 𝐴 2:0 𝐵 ⊤ 1 𝐴 2:1 𝐵 ⊤ 2 𝐴 2:2       ⊤       𝐶 ⊤ 6 𝐴 6:5 𝐶 ⊤ 7 𝐴 7:5 𝐶 ⊤ 8 𝐴 8:5       𝐴 5:5       𝐵 ⊤ 3 𝐴 5:3 𝐵 ⊤ 4 𝐴 5:4 𝐵 ⊤ 5 𝐴 5:5       ⊤                      </formula><p>From here we can reduce the problem into these two parts. These can also be interpreted as dividing the output of a "chunk" 𝑦 𝑗Q:( 𝑗+1)Q into two components: the effect of inputs within the chunk 𝑥 𝑗Q:( 𝑗+1)Q , and the effect of inputs before the chunk 𝑥 0:𝑗Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Diagonal Blocks</head><p>The diagonal blocks are easy to handle, because they are simply self-similar problems of a smaller size. The 𝑗-th block represents computing the answer SSM(𝐴 𝑅 , 𝐵 𝑅 , 𝐶 𝑅 ) (𝑥 𝑅 ) for the range 𝑅 = 𝑗Q :</p><formula xml:id="formula_35">( 𝑗 + 1)Q = ( 𝑗Q, 𝑗Q + 1, . . . , 𝑗Q + Q -1).</formula><p>The key is that this block can be computed using any desired method. In particular, for small chunk lengths Q, this problem is computed more efficiently using the dual quadratic SMA form. Additionally, the chunks can be computed in parallel.</p><p>These subproblems can be interpreted as: what is the output per chunk supposing that the initial state (to the chunk) is 0.</p><p>In other words for chunk 𝑗, this computes the correct outputs taking into account only the chunk inputs 𝑥 𝑗Q:( 𝑗+1)Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Low-Rank Blocks</head><p>The low-rank factorizations consist of 3 terms, and there are correspondingly three pieces of the computation. In this factorization, we will use the terminology</p><p>• The terms like</p><formula xml:id="formula_36">      𝐵 ⊤ 0 𝐴 2:0 𝐵 ⊤ 1 𝐴 2:1 𝐵 ⊤ 2 𝐴 2:2       ⊤</formula><p>are called the right factors or 𝐵-block-factors.</p><p>• The terms like 𝐴 5:2 are called the center factors or 𝐴-block-factors.</p><p>• The terms like</p><formula xml:id="formula_37">      𝐶 ⊤</formula><p>6 𝐴 6:5 𝐶 ⊤ 7 𝐴 7:5 𝐶 ⊤ 8 𝐴 8:5       are called the left factors or 𝐶-block-factors. 𝑋 Inputs 𝑌 Outputs 𝐻 States Diagonal Block: Input → Output Low-Rank Block: Input → State Low-Rank Block: State → Output Low-Rank Block: State → State Semiseparable Matrix Block Decomposition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑀</head><p>Figure <ref type="figure">5</ref>: (SSD Algorithm.) By using the matrix transformation viewpoint of state space models to write them as semiseparable matrices (Section 3), we develop a more hardware-efficient computation of the SSD model through a blockdecomposition matrix multiplication algorithm. The matrix multiplication also has an interpretation as a state space model, where blocks represent chunking the input and output sequence. Diagonal blocks represent intra-chunk computations and the off-diagonal blocks represent inter-chunk computations, factored through the SSM's hidden state.</p><p>Right Factors. This step computes the multiplication by the right 𝐵-block-factors of the low-rank factorization. Note that for each chunk, this is a (N, Q) by (Q, P) matrix multiplication, where N is the state dimension and 𝑃 is the head dimension. The result is a (N, P) tensor for each chunk, which has the same dimensionality as the expanded hidden state ℎ.</p><p>This can be interpreted as: what is the final state per chunk supposing that the initial state (to the chunk) is 0. In other words this computes ℎ 𝑗Q+Q-1 assuming that 𝑥 0:𝑗Q = 0.</p><p>Center Factors. This step computes the effect of the center 𝐴-block-factors terms in the low-rank factorization. In the previous step, the final states per chunk have total shape (T/Q, N, P). This is now multiplied by a 1-SS matrix generated by</p><formula xml:id="formula_38">𝐴 × 2Q-1:Q-1 , 𝐴 × 3Q-1:2Q-1 , . . . , 𝐴 × T-1:T-Q-1 .</formula><p>This step can be computed by any algorithm for computing 1-SS multiplication (also known as the scalar SSM scan or cumprodsum operator).</p><p>This can be interpreted as: what is the actual final state per chunk taking into account all previous inputs; in other words, this computes the true hidden state ℎ 𝑗Q taking into account all of 𝑥 0:( 𝑗+1)Q .</p><p>Left Factors. This step computes the multiplication by the left 𝐶-block-factors of the low-rank factorization. For each chunk, this can be represented by a matrix multiplication contract(QN, NP → QP).</p><p>This can be interpreted as: what is the output per chunk taking into account the correct initial state ℎ 𝑗Q-1 , and supposing the inputs 𝑥 𝑗Q:( 𝑗+1)Q are 0. In other words for chunk 𝑗, this computes the correct outputs taking into account only the prior inputs 𝑥 0:𝑗Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Computational Cost</head><p>We define the notation BMM(B, M, N, K) to define a batched matrix multiplication contract(MK, KN → MN) with batch dimension B. From this notation we can infer three aspects of the efficiency:</p><p>• Computation cost: total of 𝑂 (BMNK) FLOPs.</p><p>• Memory cost: total of 𝑂 (B(MK + KN + MN)) space. Center Blocks. The cost of the quadratic SMA computation consists of three steps (equation ( <ref type="formula" target="#formula_30">16</ref>)):</p><p>• Computing the kernel matrix 𝐶 ⊤ 𝐵, which has cost BMM(T/Q, Q, Q, N).</p><p>• Multiplying by the mask matrix, which is an elementwise operation on tensors of shape (T/Q, Q, Q).</p><p>• Multiplying by the 𝑋 values, which has cost BMM(T/Q, Q, P, N) Low-Rank Blocks: Right Factors. This step is a single matrix multiplication with cost BMM(T/Q, N, P, Q).</p><p>Low-Rank Blocks: Center Factors. This step is a scalar SSM scan (or 1-SS multiplication) of length T/Q on (N, P) independent channels. The work of this scan is TNP/Q, which is negligible compared to the other factors.</p><p>Note that because of the blocking which reduces the length of the sequence from T to T/Q, this scan has Q times smaller cost than a pure SSM scan (e.g. the selective scan of Mamba). Thus we observe that on most problem lengths, other algorithms (Appendix B) may be more efficient or much easier to implement without a significant slowdown. For example, a naive implementation of this via 1-SS matrix multiplication has cost BMM(1, T/Q, NP, T/Q), which is much easier to implement and can be more efficient than a naive recurrence/scan implementation.</p><p>Low-Rank Blocks: Left Factors. This step is a single matrix multiplication with cost BMM(T/Q, Q, P, N).</p><p>Total Cost. If we set N = P = Q (in other words the state dimension, head dimension, and chunk length are equal), then all BMM terms above become BMM(T/N, N, N, N). The computational chacteristics of this are:</p><p>• Total FLOP count of 𝑂 (TN 2 ).</p><p>• Total memory of 𝑂 (TN).</p><p>• The work consists primarily of matrix multiplications on matrices of shape (N, N).</p><p>Notice that the memory consumption is tight; the inputs and outputs 𝑥, 𝑦 have shape (T, P) = (T, N). Meanwhile the flop count reflects an extra factor of N, which is cost incurred by the autoregressive state size and is common to all models.</p><p>Aside from the matmuls, there is a scalar SSM scan on NP = N 2 features and sequence length T/Q. This has cost 𝑂 (T/QN 2 ) FLOPs and 𝑂 (log(T/Q)) depth. Although it does not use matrix multiplications, it is still parallelizable and the total work done is negligible compared to the other steps; this has a negligible cost in our GPU implementation.</p><p>Comparison to Pure SSM and Attention Models. Quadratic attention is also very hardware efficient by only leveraging matrix multiplications, but has T 2 𝑁 total FLOPs. Its slower computation speed at both training and inference can directly be seen as a consequence of having a larger state size -standard attention has a state size scaling with sequence length T because it caches its history and does not compress its state.</p><p>Linear SSMs have TNP = TN 2 total FLOPs, which is the same as SSD. However, a naive implementation requires a state expansion (15a) that materializes extra memory, and a scalar operation (15b) that does not leverage matrix multiplications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention SSM SSD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State size</head><formula xml:id="formula_39">T N N Training FLOPs T 2 N TN 2 TN 2 Inference FLOPs TN N 2 N 2 (Naive) memory T 2 TN 2 TN Matrix multiplication ✓ ✓</formula><p>We note that many other matrix decompositions are possible (for example, see Appendix B for a compendium of algorithms for 1-SS multiplication through different structured matrix decompositions) which may lead to more algorithms for SSDs that could be better for other specialized settings. Even more broadly, we note that semiseparable matrices have a rich literature and many more representations besides the SSS form that we use (Definition 3.2), and even more efficient algorithms may be possible. developed for Transformers. We discuss several design choices, resulting in the Mamba-2 architecture. These axes of variation are ablated in Section 9.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Block Design</head><p>We first discuss modifications to the neural network block that are independent of the inner sequence mixing layer (i.e. outside the core SSD layer).</p><p>Parallel Parameter Projections. Mamba-1 was motivated by an SSM-centric point of view where the selective SSM layer is viewed as a map from 𝑋 ↦ → 𝑌 . The SSM parameters 𝐴, 𝐵, 𝐶 are viewed as subsidiary and are functions of the SSM input 𝑋 . Thus the linear projections defining (𝐴, 𝐵, 𝐶) occur after the initial linear projection to create 𝑋 .</p><p>In Mamba-2, the SSD layer is viewed as a map from 𝐴, 𝑋, 𝐵, 𝐶 ↦ → 𝑌 . It therefore makes sense to produce 𝐴, 𝑋, 𝐵, 𝐶 in parallel with a single projection at the beginning of the block. Note the analogy to standard attention architectures, where 𝑋, 𝐵, 𝐶 correspond to the 𝑄, 𝐾, 𝑉 projections that are created in parallel.</p><p>Note that adopting parallel projections for the 𝐴, 𝐵, 𝐶, 𝑋 inputs to the SSM slightly reduces parameters and more importantly is more amenable to tensor parallelism for larger models, by using standard Megatron sharding patterns (Shoeybi et al. 2019)).</p><p>Extra Normalization. In preliminary experiments, we found that instabilities were prone to arising in larger models. We were able to alleviate this by adding an extra normalization layer (e.g. LayerNorm, GroupNorm, or RMSNorm) to the block right before the final output projection. This usage of a normalization is most directly related to the NormFormer architecture (Shleifer, Weston, and Ott 2021), which also added normalization layers at the end of the MLP and MHA blocks.</p><p>We also note that this change is similar to other recent models related to Mamba-2 that were derived from a linear attention viewpoint. The original linear attention formulation normalizes by a denominator term that emulates the normalization of the softmax function in standard attention. TransNormerLLM <ref type="bibr">(Qin, Dong Li, et al. 2023</ref>) and RetNet (Y. <ref type="bibr" target="#b95">Sun et al. 2023)</ref> find that this normalization is unstable and add an extra LayerNorm or GroupNorm after the linear attention layer. Our extra normalization layer differs slightly from these, occuring after the multiplicative gate branch instead of before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multihead Patterns for Sequence Transformations</head><p>Recall that SSMs are defined as a sequence transformation (Definition 2.1) where:</p><p>• 𝐴, 𝐵, 𝐶 parameters have a state dimension N.</p><p>• They define a sequence transformation R T → R T , which for example can be represented as a matrix 𝑀 ∈ R (T,T) .</p><p>• This transformation operates over an input sequence 𝑋 ∈ R (T,P) , independently over the P axis.</p><p>One can view this as defining one head of the sequence transformation. Definition 7.1 (Multihead patterns). A multihead sequence transformation consists of H independent heads, for a total model dimension of D = d_model. The parameters may be tied across heads, leading to a head pattern.</p><p>The state size N and head dimension P are analogous to the 𝑄𝐾 head dimension and 𝑉 head dimension of attention, respectively. Just as in modern Transformer architectures <ref type="bibr" target="#b20">(Chowdhery et al. 2023;</ref><ref type="bibr" target="#b100">Touvron, Lavril, et al. 2023)</ref>, in Mamba-2 we generally choose these to be constants around 64 or 128; when the model dimension D increases, we increase the number of heads while keeping the head dimensions N and P fixed. In order to describe how to do this, we can transfer and generalize ideas from multihead attention to define similar patterns for SSMs, or any general sequence transformation.</p><p>Multi-head SSM (Multi-head Attn.) 𝑋 (T, H, P)</p><formula xml:id="formula_40">𝐴 (T, H) 𝐵 (T, H, N) 𝐶 (T, H, N)<label>(17)</label></formula><p>Multi-contract SSM (Multi-query Attn.)</p><formula xml:id="formula_41">𝑋 (T, 1, P) 𝐴 (T, H) 𝐵 (T, 1, N) 𝐶 (T, H, N)<label>(18)</label></formula><p>Multi-expand SSM (Multi-key Attn.)</p><formula xml:id="formula_42">𝑋 (T, 1, P) 𝐴 (T, H) 𝐵 (T, H, N) 𝐶 (T, 1, N)<label>(19)</label></formula><p>Multi-input SSM (Multi-value Attn.) 𝑋 (T, H, P)</p><formula xml:id="formula_43">𝐴 (T, H) 𝐵 (T, 1, N) 𝐶 (T, 1, N)<label>(20)</label></formula><p>Multihead SSM (MHS) / Multihead Attention (MHA) Pattern. The classic MHA pattern assumes that the head dimension P divides the model dimension D. The number of heads is defined as H = D/P. Then, H copies of the core sequence transformation are created by creating H independent copies of each parameter. Note that while the MHA pattern was first described for the attention sequence transformation, it can be applied to anything compatible with Definition 2.1. For example, a multi-head SSD layer would accept inputs with shapes according to equation <ref type="bibr" target="#b17">(17)</ref> where the SSD algorithm is broadcasted over the H = n_heads dimension.</p><p>Multi-contract SSM (MCS) / Multi-query Attention (MQA) Pattern. Multi-query attention <ref type="bibr" target="#b90">(Shazeer 2019</ref>) is a clever optimization for attention that can dramatically improve the speed of autoregressive inference, which relies on caching the 𝐾 and 𝑉 tensors. This technique simply avoids giving 𝐾 and 𝑉 the extra head dimension, or in other words broadcasts a single head of (𝐾, 𝑉 ) across all the heads of 𝑄.</p><p>Using the state space duality, we can define an equivalent SSM version of MQA as equation <ref type="bibr" target="#b18">(18)</ref>. Here, 𝑋 and 𝐵 (the SSM analogs of attention's 𝑉 and 𝐾) are shared across the H heads. We also call this the multi-contract SSM (MCS) head pattern, because the 𝐶 parameter which controls the SSM state contraction has independent copies per head.</p><p>We can similarly define a multi-key attention (MKA) or multi-expand SSM (MES) head pattern, where 𝐵 (which controls the SSM expansion) is independent per head while 𝐶 and 𝑋 are shared across heads.</p><p>Multi-input SSM (MIS) / Multi-value Attention (MVA) Pattern. While MQA makes sense for attention because of its KV cache, it is not the natural choice for SSMs. In Mamba, instead, 𝑋 is viewed as the main input to the SSM, and therefore 𝐵 and 𝐶 are parameters that are shared across the input channels. We define a new multi-value attention (MVA) of multi-input SSM (MIS) pattern in equation <ref type="bibr" target="#b20">(20)</ref>, which can again be applied to any sequence transformation such as SSD.</p><p>Armed with this vocabulary, we can characterize the original Mamba architecture more precisely.</p><p>Proposition 7.2. The selective SSM (S6) layer of the Mamba architecture (Gu and Dao 2023) can be viewed as having</p><p>• Head dimension 𝑃 = 1: every channel has independent SSM dynamics 𝐴.</p><p>• Multi-input SSM (MIS) or multi-value attention (MVA) head structure: the 𝐵, 𝐶 matrices (corresponding to 𝐾, 𝑄 in the attention duality) are shared across all channels of the input 𝑋 (corresponding to 𝑉 in attention).</p><p>We can also ablate these head pattern variants when applied to SSD (Section 9.4.3). Interestingly, despite being controlled in parameter counts and total state dimension, there is a noticeable difference in downstream performance. We empirically find that the MVA pattern as originally used in Mamba performs best.</p><p>Grouped Head Patterns. The ideas of multi-query attention can be extended to grouped-query attention (Ainslie et al. 2023): instead of 1 K and V head, one can create G independent K and V heads, where 1 &lt; G and G divides H. This is motivated both by bridging the performance difference between multi-query and multi-head attention, and enabling more efficient tensor parallelism by setting G to be a multiple of the number of shards (Section 8).</p><p>Similarly, the multi-input SSM head pattern used in Mamba-2 can be easily extended to grouped-input SSM (GIS), or synonymously grouped-value attention (GVA). The generalization is straightforward and we omit the details for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Other SSD Extensions from Linear Attention</head><p>We describe here an example of architectural modifications to SSD motivated by linear attention. We ablate these in Section 9.4.3 as a form of negative result, finding that they do not significantly improve performance enough to adopt them as default settings. Nonetheless, these illustrate how the vast literature on attention can be incorporated to define variants of SSD. We treat the choice of kernel feature map as a hyperparameter in the Mamba-2 architecture, and expect other simple modifications inspired by attention to be possible as well.</p><p>Kernel Attention Approximations to Softmax Attention. Many variants of linear attention or kernel attention are motivated by viewing the attention scores softmax(𝑄𝐾 ⊤ ) as composed of</p><p>1. An exponential kernel 𝑍 = exp(𝑄𝐾 ⊤ ), which can be approximated by 𝑍 = 𝜓 (𝑄)𝜓 (𝐾) ⊤ for some kernel feature map.</p><p>2. Normalizing the kernel so that rows sum to 1 via 𝑀 = 𝐺/𝐺11 ⊤ , where the division happens elementwise and 1 is the all 1's vector.</p><p>Exponential Kernel Feature Maps. In Mamba-2, we incorporate a flexible kernel feature map, and apply it to the 𝐵 and 𝐶 branches (corresponding to the 𝐾 and 𝑉 branches in attention). The feature map can also be optionally applied to the 𝑋 (𝑉 ) branch, for simplicity and symmetry. This is represented in Figure <ref type="figure" target="#fig_6">6</ref> by an arbitrary nonlinearity. By default, we simply choose 𝜓 to be an elementwise Swish / SiLU function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Le 2017). We explore other options in the ablations in Section 9.4.3, including feature maps used by Linear Attention, Performer, Random Feature Attention, and cosFormer (Section 4.1.3).</p><p>Incorporating a Normalization (Denominator) Term. To find the denominator term, we simply have to compute 𝑀1. But recall that the final output of the model is just 𝑌 = 𝑀𝑋 (equation ( <ref type="formula" target="#formula_30">16</ref>)). So the normalization terms can be found simply by augmenting 𝑋 with an extra column 1, resulting in a tensor of shape (T, P + 1).</p><p>Note that in this case, the kernel feature map 𝜓 must be positive so that the sum is positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Tensor Parallel</head><p>Tensor parallelism (TP) <ref type="bibr">(Shoeybi et</ref> al. 2019) is a model parallelism technique that splits each layer (e.g., attention, MLP) to run on multiple accelerators such as GPUs. This technique is widely used to train most large models (Brown et al. 2020; Chowdhery et al. 2023; Touvron, Lavril, et al. 2023; Touvron, L. Martin, et al. 2023) on GPU clusters where each node typically has 4-8 GPUs with fast networking such as NVLink. TP was originally developed for the Transformer architecture, and it is not straight-forward to adapt it other architecture.</p><p>We first show the challenge of using TP with the Mamba architecture, and the show how the Mamba-2 architecture is designed to make TP efficient.</p><p>Recall the Mamba architecture, with a single input 𝑢 ∈ R 𝐿×𝑑 (no batching for simplicity), input projection matrices 𝑊 (𝑥 ) ,𝑊 (𝑧 ) ∈ R 𝑑 ×𝑒𝑑 where 𝑒 is the expansion factor (typically 2), and output projection matrix 𝑊 (𝑜 ) ∈ R 𝑒𝑑 ×𝑑 :</p><formula xml:id="formula_44">𝑥 = 𝑢𝑊 (𝑥 ) ⊤ ∈ R 𝐿×𝑒𝑑 𝑧 = 𝑢𝑊 (𝑧 ) ⊤ ∈ R 𝐿×𝑒𝑑 𝑥 𝑐 = conv1d(𝑥) ∈ R 𝐿×𝑒𝑑 (depthwise, independent along 𝑑) Δ, 𝐵, 𝐶 = low-rank projection(𝑥 𝑐 ) 𝑦 = 𝑆𝑆𝑀 𝐴,𝐵,𝐶,Δ (𝑥 𝑐 ) ∈ R 𝐿×𝑒𝑑 (independent along 𝑑) 𝑦 𝑔 = 𝑦 • 𝜙 (𝑧) (gating, e.g., with 𝜙 being SiLU) out = 𝑦 𝑔 𝑊 (𝑜 ) ⊤ ∈ R 𝐿×𝑑 .</formula><p>With TP, suppose that we want to split the computation along 2 GPUs. It is easy to split the input projection matrices 𝑊 (𝑥 ) and 𝑊 (𝑧 ) into two partitions each of size 𝑑 × 𝑒𝑑 2 . Then each GPU would hold half of 𝑥 𝑐 of size 𝐿 × 𝑒𝑑 2 . However, we see that since Δ, 𝐵, 𝐶 are functions are 𝑥 𝑐 , so we would need an extra all-reduce between the GPUs to get the whole of 𝑥 𝑐 before computing Δ, 𝐵, 𝐶. After that the two GPUs can compute the SSM in parallel since they are independent along 𝑑. At the end, we can split the output projection matrices 𝑊 (𝑜 ) into two partitions each of size 𝑒𝑑 2 × 𝑑, and do an all-reduce at the end. Compared to Transformers, we would incur two all-reduces instead of one, doubling the time spent in communication. For large-scale Transformers training, communication might already take a significant fraction of time (e.g. 10-20%), and doubling communication would make Mamba not as efficient for large-scale training.</p><p>With Mamba-2, our goal is to have only one all-reduce per block, similar to attention or MLP blocks in Transformers. As a result, we have the projection to get Δ, 𝐵, 𝐶 directly from 𝑢 instead of from 𝑥 𝑐 , allowing us to split these projection matrices. This implies that we have different sets of Δ, 𝐵, 𝐶 on different GPUs, which is equivalent to having several "groups" of Δ, 𝐵, 𝐶 on a larger "logical GPU". Moreover, we use GroupNorm within each block, with number of groups divisible by the TP degree, so that the GPUs in a TP group do not have a communicate within the block:</p><formula xml:id="formula_45">𝑥 = 𝑢𝑊 (𝑥 ) ⊤ ∈ R 𝐿×𝑒𝑑 𝑧 = 𝑢𝑊 (𝑧 ) ⊤ ∈ R 𝐿×𝑒𝑑 Δ, 𝐵, 𝐶 = projection(𝑢) (one or more groups of Δ, 𝐵, 𝐶 per GPU) 𝑥 𝑐 = conv1d(𝑥) ∈ R 𝐿×𝑒𝑑 (depthwise, independent along 𝑑) 𝑦 = 𝑆𝑆𝑀 𝐴,𝐵,𝐶,Δ (𝑥 𝑐 ) ∈ R 𝐿×𝑒𝑑 (independent along 𝑑)</formula><p>𝑦 𝑔 = 𝑦 • 𝜙 (𝑧) (gating, e.g., with 𝜙 being SiLU) 𝑦 𝑛 = groupnorm(𝑦 𝑔 ) (number of groups divisible by degree of tensor parallel)</p><formula xml:id="formula_46">out = 𝑦 𝑔 𝑊 (𝑜 ) ⊤ ∈ R 𝐿×𝑑 .</formula><p>We see that we only need to split the input projection matrices, and the output projection matrices, and only need to do all-reduce at the end of the block. This is similar to the design of TP for attention and MLP layers. In particular, if we have TP degree 2, we would split 𝑊 (𝑥 ) = [𝑊 (𝑥 )  1 ,𝑊 (𝑥 ) 2 ] with 𝑊 (𝑥 )</p><formula xml:id="formula_47">𝑖 ∈ R 𝑑 ×𝑒𝑑/2 , 𝑊 (𝑧 ) = [𝑊 (𝑧 ) 1 ,𝑊 (𝑧 ) 2 ] with 𝑊 (𝑧 ) 𝑖 ∈ R 𝑑 ×𝑒𝑑/2 , B C X A GN Y B C X A GN Y Layer Input &amp; ! (#) &amp; ! (%) &amp; &amp; (#) &amp; &amp; (%) &amp; ! (') &amp; &amp; (') All-reduce ! Inputs " Outputs # States GPU1 GPU2 GPU3</formula><p>Figure <ref type="figure">7</ref>: (Parallelism with the Mamba-2 Block.) (Left: Tensor Parallelism) We split the input projection matrices 𝑊 (𝑥 ) ,𝑊 (𝑧 ) and the output projection matrix 𝑊 (𝑜 ) . Each SSM head (𝐴, 𝐵, 𝐶, 𝑋 ) ↦ → 𝑌 lives on a single device. Choosing GroupNorm for the final normalization layer avoids extra communication. We need one all-reduce per layer, just like the MLP or attention blocks in a Transformer. (Right: Sequence/Context Parallelism) Analogous to the SSD algorithm, with multiple devices, we can split along the sequence dimension. Each device computes the state of its sequence, then pass that state to the next GPU.</p><p>and</p><formula xml:id="formula_48">𝑊 (𝑜 ) = 𝑊 (𝑜 ) 1 𝑊 (𝑜 ) 2 with 𝑊 (𝑜 ) 𝑖 ∈ R 𝑒𝑑/2×𝑑</formula><p>. For 𝑖 = 1, 2, the TP Mamba-2 layer can be written as:</p><formula xml:id="formula_49">𝑥 (𝑖 ) = 𝑢𝑊 (𝑥 ) 𝑖 ⊤ ∈ R 𝐿×𝑒𝑑/2 𝑧 (𝑖 ) = 𝑢𝑊 (𝑧 ) 𝑖 ⊤ ∈ R 𝐿×𝑒𝑑/2</formula><p>Δ (𝑖 ) , 𝐵 (𝑖 ) , 𝐶 (𝑖 ) = projection(𝑢) (one or more groups of Δ, 𝐵, 𝐶 per GPU)</p><formula xml:id="formula_50">𝑥 (𝑖 ) 𝑐 = conv1d(𝑥 (𝑖 ) ) ∈ R 𝐿×𝑒𝑑/2 𝑦 (𝑖 ) = 𝑆𝑆𝑀 𝐴,𝐵,𝐶,Δ (𝑥 (𝑖 ) 𝑐 ) ∈ R 𝐿×𝑒𝑑/2 𝑦 (𝑖 ) 𝑔 = 𝑦 (𝑖 ) • 𝜙 (𝑧 (𝑖 ) ) 𝑦 (𝑖 ) 𝑛 = groupnorm(𝑦 (𝑖 )</formula><p>𝑔 ) (number of groups divisible by degree of tensor parallel) 𝑖 ) . (summing outputs from all GPUs with an all-reduce)</p><formula xml:id="formula_51">out (𝑖 ) = 𝑦 (𝑖 ) 𝑔 𝑊 (𝑜 ) 𝑖 ⊤ ∈ R 𝐿×𝑑/2 out = ∑︁ 𝑖 out<label>(</label></formula><p>We illustrate tensor parallel with Mamba-2 in Figure <ref type="figure">7</ref> (Left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Sequence Parallelism</head><p>For very long sequences, we might need to split the input and activation to different GPUs along the sequence length dimension. There are two main techniques:</p><p>1. Sequence parallelism (SP) for the residual and normalization operations: first proposed by Korthikanti et al. ( <ref type="formula">2023</ref>), this technique decomposes the all-reduce in TP as reduce-scatter and all-gather. Noticing that the residual and normalization operations are repeated on the same input for all GPUs in the same TP group, SP splits the activations along the sequence length dimension by performing: reduce-scatter, residual and normalization, then all-gather.</p><p>Since the Mamba-2 architecture uses the same residual and normalization structure, SP applies without modification.</p><p>2. Sequence parallelism for the token-mixing operations (attention or SSM), also known as "context parallelism" (CP).</p><p>Several techniques have been developed for attention layer (e.g., Ring attention (Liu, Yan, et al. 2024; Liu, Zaharia, 32 64 128 256 Model dimension 0.00 0.25 0.50 0.75 1.00 Accuracy Sequence Length: 256 32 64 128 256 Model dimension Sequence Length: 512 32 64 128 256 Model dimension Sequence Length: 1024 Attention Based Mamba (N=16) Mamba-2 (N=16) Mamba-2 (N=64) Mamba-2 (N=256) Figure 8: (Multi-Query Associative Recall (MQAR)). Associative recall tasks are challenging for SSMs, which must memorize all relevant information into their recurrent state. The SSD layer combined with improved architecture allows for much larger state sizes in Mamba-2, which performs significantly better than Mamba-1 and even vanilla attention. and Abbeel 2023)), with sophisticated load-balancing technique (Brandon et al. 2023). The difficulty with sequence parallelism in attention is that we can split queries and keys into block, but each query block needs to interact with key blocks, leading to communication bandwidth quadratic in the number of workers.</p><p>With SSMs, we can split the sequence in a simple manner: each worker takes an initial state, compute the SSM with respect to their inputs, return the final state, and pass that final state to the next worker. The communication bandwidth is linear in the number of workers. This decomposition is exactly the same as the block-decomposition in the SSD algorithm (Figure <ref type="figure">5</ref>) to split into blocks / chunks. We illustrate this context parallelism in Figure <ref type="figure">7</ref> (Right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Variable Length</head><p>While pretraining often uses the same sequence lengths for the batch, during finetuning or inference, the model might need to process different input sequences of different lengths. One naive way to handle this case is to right-pad all sequences in the batch to the maximum length, but this can be inefficient if sequences are wildly different lengths. With SSMs and Mamba in particular, we can handle variable sequence lengths by simply treating the whole batch as one long sequence, and avoid passing the states between individual sequences. This is equivalent to simply setting 𝐴 𝑡 = 0 for tokens 𝑡 at the end of one sequence to prevent it from passing information to the token 𝑡 + 1, which belongs to a different sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Empirical Validation</head><p>We empirically evaluate Mamba-2 on synthetic recall tasks that have been challenging for recurrent models (Section 9.1), and standard language modeling pre-training and downstream evaluations (Section 9.2). We validate that our SSD algorithm is much more efficient than Mamba-1 (Section 9.3) and comparable to optimized attention for moderate sequence lengths. Finally, we ablate various design choices in the Mamba-2 architecture (Section 9.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Synthetics: Associative Recall</head><p>Synthetic associative recall tasks have been popular for testing the ability of language models to look up information in their context. Broadly, they involve feeding autoregressive models pairs of key-value associations, and then prompting the model to produce the correct completion upon being shown a previously-seen key. The multi-query associative recall (MQAR) task is a particular formulation of this task that requires the model to memorize multiple associations <ref type="bibr">(Arora, Eyuboglu, Timalsina, et al. 2024</ref>). The original Mamba paper reported results on related synthetic tasks, in particular Selective Copying <ref type="bibr" target="#b40">(Gu and Dao 2023</ref>) and Induction Heads <ref type="bibr" target="#b67">(Olsson et al. 2022</ref>), which can be seen as easier associative recall tasks. The MQAR task is also closely related to "phonebook look-up" tasks which has been shown to be challenging for recurrent models such as SSMs, due to their finite state capacity <ref type="bibr">(De et</ref>   We compare on a challenging version of the MQAR setup from (Arora, Eyuboglu, Zhang, et al. 2024), using a harder task, longer sequences, and smaller models. Our baselines include standard multi-head softmax attention as well as the Based architecture which combines convolutions, local attention, and a linear attention variant.</p><p>Results are shown in Figure <ref type="figure">8</ref>. While Mamba-1 struggles on this task, Mamba-2 performs well across all settings. Surprisingly, it is significantly better than Mamba-1 even when the state sizes are controlled (N = 16). (We are not sure which aspect of the architecture is the predominant factor, which remains a question to explore in future work.) Additionally, this task validates the importance of state size: increasing from N = 16 to N = 64 and N = 256 consistently improves performance on MQAR, as the larger state allows more information (key-value pairs) to be memorized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Language Modeling</head><p>Following standard protocols in LLMs, we train and evaluate the Mamba-2 architecture on standard autoregressive language modeling against other architectures. We compare both pretraining metrics (perplexity) and zero-shot evaluations.</p><p>The model sizes (depth and width) follow GPT3 specifications, from 125m to 2.7B. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in <ref type="bibr" target="#b18">Brown et al. (2020)</ref>. This follows the same setup as reported in Mamba (Gu and Dao 2023); training details are in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.1">Scaling Laws</head><p>For baselines, we compare against both Mamba and its Transformer++ recipe (Gu and Dao 2023), which is based on the PaLM and LLaMa architectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). As Mamba has already demonstrated that it outperforms the standard Transformer architecture ( Figure <ref type="figure">9</ref> shows scaling laws under the standard Chinchilla <ref type="bibr" target="#b50">(Hoffmann et al. 2022</ref>) protocol, on models from ≈ 125𝑀 to ≈ 1.3𝐵 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.2">Downstream Evaluations</head><p>Table <ref type="table" target="#tab_9">1</ref> shows the performance of Mamba-2 on a range of popular downstream zero-shot evaluation tasks, compared to the most well-known open source models at these sizes, most importantly Pythia <ref type="bibr">(Biderman et al. 2023</ref>) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models. ) suggests that a hybrid architecture with both SSM layers and attention layers could improve the model quality over that of a Transformer, or a pure SSM (e.g., Mamba) model, especially for in-context learning. We explore the different ways that SSD layers can be combined with attention and MLP to understand the benefits of each. Empirically we find that having around 10% of the total number of layers being attention performs best. Combining SSD layers, attention layers, and MLP also works better than either pure Transformer++ or Mamba-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSD and Attention</head><p>We find that SSD and attention layers are complementary: by themselves (e.g. in the Mamba-2 architecture vs. Transformer++) their performance (measured by perplexity) is nearly the same, but a mixture of SSD and attention layers outperforms the pure Mamba-2 or Transformer++ architecture. We show some results (Table <ref type="table" target="#tab_10">2</ref>) for the 350M model (48 layers) trained to 7B tokens on the Pile with the GPT-2 tokenizer (same number of parameters, same hyperparameters, same training and validation set). Adding in just a few attention layers already yields notable improvement and strikes the best balance between quality and efficiency. We hypothesize that the SSM layers function well as a general sequence-to-sequence mapping, and attention layers act as a retrieval mechanism to quickly refer to previous tokens in the sequence instead of forcing the model to compress all the context to its memory (SSM states). Hybrid Models with SSD, MLP, and Attention We compare different ways that SSD can be combined with the (gated) MLP and attention layers, and evaluate at the 2.7B scale (64 layers), trained to 300B tokens on the Pile (same number of parameters, same hyperparameters, same training and validation set, same data order):</p><p>1. Transformer++: 32 attention layers and 32 gated MLP, interleaving.</p><p>2. Mamba-2: 64 SSD layers.</p><p>3. Mamba-2-MLP: 32 SSD and 32 gated MLP layers, interleaving.</p><p>4. Mamba-2-Attention: 58 SSD layers and 6 attention layers (at indices 9, 18, 27, 36, 45, 56)<ref type="foot" target="#foot_7">foot_7</ref> .</p><p>5. Mamba-2-MLP-Attention: 28 SSD layers and 4 attention layers, interleaving with 32 gated MLP layers.</p><p>We report the validation perplexity on the Pile, as well as zero-shot evaluation, in Table <ref type="table" target="#tab_11">3</ref>. In general, the quality of Transformer++ and Mamba-2 models are around the same. We see that adding just 6 attention layers noticeably improves over the pure Mamba-2 model (and over Transformer++). Adding MLP layers reduces model quality, but can (i) speed up training and inference due to the simplicity and hardware-efficiency of the MLP layer (ii) be easier to up-cycle to MoE models by replacing MLP layers with mixture-of-experts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Speed Benchmarks</head><p>We benchmark the speed of the SSD algorithm against Mamba's scan implementation and FlashAttention-2 (Figure <ref type="figure" target="#fig_8">10</ref>). SSD, thanks to its reformulation to use matrix multiplication as a subroutine, can exploit specialized matrix multiplication (matmul) units on GPUs, also known as tensor cores. As a result, it is 2-8× faster than Mamba's fused associative scan, which does not leverage matmul units. Due to its linear scaling in sequence length, SSD is faster than FlashAttention-2 starting at sequence length 2𝐾.</p><p>However, we note that the Mamba-2 model as a whole might not be as efficient to train as Transformer at short sequence length (e.g. at 2𝐾), since a Transformer with 𝐿 layers would have 𝐿 2 MLP layers and 𝐿 2 attention layers, while a Mamba-2 model would have 𝐿 SSD layers for the same number of parameters. Generally the MLP layers are very hardware efficient since they consist of simple matrix multiplication and pointwise linearity. As shown in Section 9.2.3, one can also combine  Section 7.1 introduces the Mamba-2 block, which has small modifications to the Mamba-1 block which are partly motivated by the connection to attention and also to improve the scalability of Mamba-2. Table <ref type="table" target="#tab_12">4</ref> ablates these architecture changes to the block, which occur outside of the core SSM layer.</p><p>The ablations validate that parallel projections to create (𝐴, 𝐵, 𝐶, 𝑋 ) saves parameters and performs slightly better than Mamba's sequential projections. More importantly, this modification is amenable to tensor parallelism at larger model sizes (Section 8). Additionally, the extra normalization layer also slightly improves performance. More importantly, preliminary experiments at larger scales observed that it also helps with training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.2">Head Structure</head><p>Section 7.2 describes how the dimensions of the 𝐵, 𝐶, 𝑋 projections can be viewed as a hyperparameter analogous to notions of multi-head attention and multi-query attention. We also showed how the original Mamba architecture is analogous to multi-value attention (Proposition 7.2), which was a choice that naturally developed from the state-space model point of view and was not previously ablated.</p><p>Table <ref type="table" target="#tab_15">5</ref> ablates choices of the multi-head structure for the Mamba-2 architecture. Strikingly, we find a large difference between multi-value and multi-query or multi-key head patterns, despite seeming very similar. Note that this is not explained by the total state size, which is the same for all of them (equal to HPN or the product of the number of heads, head dimension, and state dimension).</p><p>We also compare to multi-head patterns where the number of 𝐶, 𝐵, 𝑋 (analogous to 𝑄, 𝐾, 𝑉 ) heads is equal. We compare against the standard multi-head pattern, as well as one with aggressive sharing where they all have only 1 head. Note that in the latter case, the model still has H different sequence mixers 𝑀, because each head still has a different 𝐴. When parameter matched, these multi-head patterns perform similarly to each other, in between the MVA and MQA/MKA patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.3">Attention Kernel Approximations</head><p>Section 7.3 noted how SSD can be combined with ideas from the linear attention literature, such as various forms of kernel approximations. We ablate several variants of these suggested by previous works in Table <ref type="table" target="#tab_16">6</ref>. These include the cosFormer (Qin, Weixuan Sun, et al. 2022), Random Feature Attention H. Peng et al. 2021, and Positive Random Features (Performer) (Choromanski et al. 2021).</p><p>We also ablate adding a normalization term, akin to the denominator of the softmax function in standard attention. We found that this introduced instabilities to most variants, but slightly improved performance for the ReLU activation function 𝜓 .</p><p>Table 7 also tests more recent proposals to improve linear attention that involve expanding the feature dimension (Based (Arora, Eyuboglu, Zhang, et al. 2024) and ReBased (Aksenov et al. 2024)</p><p>). These linear attention extensions aim to appropriate the exp kernel with a quadratic approximation. ReBased also proposes to replace the QK activation function with a layer normalization; from an SSM-centric view we apply a normalization on top of (𝐵, 𝐶) before applying the SSM function. We note that this technique has been independently proposed as the "QK-Norm" for softmax attention <ref type="bibr" target="#b97">(Team 2024</ref>) and an "internal normalization" for Mamba <ref type="bibr" target="#b60">(Lieber et al. 2024</ref>).</p><p>Overall, Table <ref type="table" target="#tab_16">6</ref> and Table <ref type="table" target="#tab_17">7</ref> found that the kernel approximation methods we tried did not seem to improve over simple pointwise non-linear activation functions for 𝜓 . Thus our default settings for Mamba-2 used 𝜓 (𝑥) = Swish(𝑥) to follow Mamba-1, but we suggest that removing this activation entirely may be a simpler choice that we did not extensively test.</p><p>We emphasize however that SSD and vanilla linear attention differ in the inclusion of the 1-semiseparable mask 𝐿, while the various linear attention methods in the literature were derived to approximate softmax attention without this term; thus, our negative results may be not unexpected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Related Work and Discussion</head><p>The state space duality framework bridges connections between SSMs, structured matrices, and attention. We discuss in more depth the relations between SSD and these concepts more broadly. Using ideas from each of the viewpoints, we also suggest some directions that the SSD framework can be extended in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">State Space Models</head><p>Structured state space models can be characterized along the axes (i) whether it is time-invariant or time-varying.</p><p>(ii) the dimensionality of the system.</p><p>(iii) the structure on the recurrent transitions 𝐴.</p><p>SSD can be described as a selective SSM with SISO dimensions and scalar-identity structure. Dimensionality and State Expansion. An important characteristic of SSD, shared by previous SSMs in its lineage (S4, H3, Mamba), is that it is a single-input single-output (SISO) system where input channels are processed independently. This leads to a much larger effective state size of ND where N is the SSM state size (also called state expansion factor) and D is the standard model dimension. Traditional RNNs either have N = 1 or are multi-input multi-output (MIMO) with dense 𝐵, 𝐶 matrices, either of which leads to a smaller state. While MIMO SSMs have been shown to work well in some domains <ref type="bibr" target="#b63">(Lu et al. 2023;</ref><ref type="bibr" target="#b68">Orvieto et al. 2023</ref>; J. T. Smith, Warrington, and Linderman 2023), Mamba showed that state expansion is crucial for information-dense domains such as language. One of the main advantages of SSD is allowing for even larger state expansion factors without slowing down the model. Many subsequent works have since adopted state expansion (Section 10.4).</p><p>Structure. Compared to previous structured SSMs, the main restriction of SSD is on the expressivity of the state transitions 𝐴 𝑡 . We note that more general SSMs, such as the case of diagonal 𝐴 𝑡 , have the same theoretical efficiency as SSD, but are less hardware-friendly. This is because the dual quadratic form loses its attention-like interpretation and becomes more difficult to compute. Thus compared to Mamba, SSD differs only in a slightly more restrictive form of diagonal 𝐴 𝑡 , and trades off this expressivity for improved hardware efficiency (and ease of implementation).</p><p>We hypothesize that it may be possible to refine our structured matrix algorithms to improve to the general diagonal SSM case as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Structured Matrices</head><p>The first viewpoint of the state space duality adopts the viewpoint of these models as matrix sequence transformations or "matrix mixers": sequence transformations (Definition 2.1) that can be represented as matrix multiplication (by a T × T matrix) along the sequence dimension T.</p><p>Several such matrix mixers have been proposed before, where the primary axis of variation is the representation of the matrix. These An important characterization is that efficient (sub-quadratic) matrix sequence transformations are exactly those which have structured matrix mixers. A core result of the SSD framework is viewing SSMs as matrix mixers with a particular structure -semiseparable matrices (Section 3). The linear vs. quadratic duality then takes the form of structured matrix multiplication vs. naive matrix multiplication.</p><p>The structure matrix representation led to our efficient SSD algorithm through block decompositions of particular semiseparable matrices (Section 6). We note that semiseparable matrices are well-studied in the scientific computing literature, and incorporating those ideas may be a promising avenue for more improvements to state space models. We also suggest that focusing on the matrix mixer viewpoint can lead to more fruitful directions for sequence models, such as designing principled non-causal variants of Mamba, or finding ways to characterize and bridge the gap between softmax attention and sub-quadratic models through analyzing their matrix transformation structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">(Linear) Attention</head><p>Compared to standard (causal) attention, SSD has only two main differences. Second, SSD multiplies the logits matrix by an input-dependent 1-semiseparable mask. Thus this mask can be viewed as replacing the softmax in standard attention. This semiseparable mask can also be viewed as providing positional information. The elements 𝑎 𝑡 act as "gates" in the RNN sense, or a "selection" mechanism (see discussion in Mamba paper), and their cumulative products 𝑎 𝑗:𝑖 control how much interaction is allowed between positions 𝑖 and 𝑗. Positional embeddings (e.g. sinusoidal <ref type="bibr" target="#b103">(Vaswani et al. 2017</ref>), AliBi (Press, N. Smith, and Lewis 2022), and RoPE <ref type="bibr" target="#b94">(Su et al. 2021</ref>)) are an important component of Transformers that are often viewed as heuristics, and the 1-SS mask of SSD can be seen as a more principled form of relative positional embeddings. We note that this view was also posited concurrently by GateLoop <ref type="bibr" target="#b53">(Katsch 2023</ref>).</p><p>The second viewpoint of state space duality is a special case of our more general structured masked attention (SMA) framework, where the duality is revealed as different contraction orderings on a simple 4-way tensor contraction. SMA is a strong generalization of linear attention that is much more general than SSD as well; other forms of structured masks may lead to more variants of efficient attention with different properties than SSD.</p><p>Beside leading to new models, these connections to attention can lead to other directions for understanding SSMs. For example, we are curious whether the phenomenon of attention sinks <ref type="bibr">(Darcet et</ref> al. 2024; Xiao et al. 2024) exist for Mamba models, and more broadly whether interpretability techniques can be transferred to SSMs (Ali, Zimerman, and Wolf 2024). Finally, many other variants of linear attention have been proposed (Arora, Eyuboglu, Timalsina, et al. 2024; Arora, Eyuboglu, Zhang, et al. 2024; Choromanski et al. 2021; H. Peng et al. 2021; Qin, Han, Weixuan Sun, Dongxu Li, et al. 2022; Qin, Weixuan Sun, et al. 2022; Schlag, Irie, and Schmidhuber 2021; Zhang et al. 2024; Zheng, C. Wang, and Kong 2022) (see Section 4.1.3 for descriptions of several of these), and we expect that many techniques can be transferred to SSMs (e.g. Section 7.3).</p><p>We emphasize that SSD does not generalize standard softmax attention, or any other transformation on the attention kernel matrix that does not have a finite feature map 𝜓 . Compared to general attention, SSD's advantage is having a controllable state expansion factor N that compresses the history, compared to quadratic attention's cache of the entire history scaling with sequence length T ≫ N. Concurrent work has starting studying the tradeoffs of these representations, for example on copying and in-context learning tasks <ref type="bibr" target="#b2">(Akyürek et al. 2024;</ref><ref type="bibr" target="#b38">Grazzi et al. 2024;</ref><ref type="bibr" target="#b51">Jelassi et al. 2024;</ref><ref type="bibr" target="#b70">Park et al. 2024)</ref>. We note that Mamba-2 significantly improves on Mamba on some of these capabilities (e.g. as demonstrated by MQAR results in Section 9.1), but more remains to be understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Related Models</head><p>We finally highlight a growing body of recent and concurrent work that have developed sequence models very similar to Mamba and Mamba-2.</p><p>• RetNet (Y. <ref type="bibr" target="#b95">Sun et al. 2023</ref>) and TransNormerLLM <ref type="bibr">(Qin, Dong Li, et al. 2023</ref>) generalize Linear Attention using decay terms instead of a cumulative sum, and propose dual parallel/recurrent algorithms as well as a hybrid "chunkwise" mode. These algorithms can be seen as an instantiation of SSD where 𝐴 𝑡 is time-invariant (constant for all 𝑡); in the SMA interpretation, the mask matrix 𝐿 would be a decay matrix 𝐿 𝑖,𝑗 = 𝛾 𝑖 -𝑗 . These models also differ architecturally in various ways. For example, since they were derived from an attention-centric perspective they preserve the multi-head attention (MHA) pattern; since Mamba-2 was derived from an SSM-centric pattern it preserves the multi-value attention (MVA) or multi-expand SSM (MES) pattern, which we show to be better (Section 9.4).</p><p>• GateLoop (Katsch 2023) concurrently proposed using input-dependent decay factors 𝐴 𝑡 , and developed the same dual quadratic form as in SSD which they call a "surrogate attention" form.</p><p>• Gated Linear Attention (GLA) <ref type="bibr" target="#b108">(Yang et al. 2024</ref>) proposed a variant of linear attention with data-dependent gates, along with efficient algorithms to compute a chunkwise mode and hardware-aware implementations.</p><p>• HGRN <ref type="bibr">(Qin, Yang, and</ref> Zhong 2023) introduced an RNN with input-dependent gates, which was improved to incorporate state expansion in HGRN2 (Qin, Yang, Weixuan Sun, et al. 2024). • Griffin (De et al. 2024) and RecurrentGemma (Botev et al. 2024) showed that an RNN with input-dependent gating, combined with local attention, can be very competitive with strong modern Transformers. Jamba also showed that combining Mamba with a few layers of attention performs very well on language modeling (Lieber et al. 2024). • xLSTM (Beck et al. 2024) improves the xLSTM by adopting the idea of state expansion and other gating, normalization, and stabilization techniques. • RWKV(-4) (B. Peng, Alcaide, et al. 2023) is an RNN based on a different linear attention approximation (the attention-free Transformer (S. Zhai et al. 2021)). It has recently been improved to the RWKV-5/6 (Eagle and Finch) architectures (B. Peng, Goldstein, et al. 2024) by adopting the ideas of selectivity and state expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>We proposed a theoretical framework based on well-studied classes of structured matrices that bridges the conceptual gap between SSMs and attention variants. This framework yields insights on how recent SSMs (e.g. Mamba) perform as well as Transformers on language modeling. Moreover, our theoretical tools provide new ideas to improve SSMs (and potentially Transformers) by connecting the algorithmic and systems advances on both sides. As a demonstration, the framework guides our design of a new architecture (Mamba-2) at the intersection of SSMs and structured attention. In this section we flesh out various algorithms for computing the scalar SSM scan, through the lens of structured matrix decompositions. The scalar SSM scan is defined as computing the recurrent part of the discrete SSM <ref type="bibr" target="#b6">(7)</ref>, in the case when 𝑁 = 1 (i.e. 𝐴 is a scalar). This is commonly used to compute SSMs recurrently; in particular, the case of structured SSMs where 𝐴 is diagonally structured reduces down to this operation, such as in the S5 (J. T. Smith, Warrington, and Linderman 2023) and S6 (Gu and Dao 2023) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Glossary</head><p>The goal of this section is to support a central theme of this paper that efficient algorithms for sequence models can be viewed as structured matrix multiplication algorithms. The various matrix decomposition ideas we show here are related to ideas used to derive fast SSM algorithms (Section 6), as well as directly used as a subroutine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Problem Definition</head><p>Let 𝑎 : (D, ) and 𝑏 : (D, ) be sequences of scalars. The scalar SSM scan is defined as</p><formula xml:id="formula_53">ℎ 𝑡 = 𝑎 𝑡 ℎ 𝑡 -1 + 𝑏 𝑡 .<label>(21)</label></formula><p>Here ℎ -1 can be an arbitrary value representing the previous hidden state to the SSM recurrence; unless otherwise specified, we assume ℎ -1 = 0.</p><p>We also call equation <ref type="bibr" target="#b21">(21)</ref> the cumprodsum (cumulative product sum). Note that the cumprodsum reduces to the cumprod (cumulative product) when 𝑏 = 0 is the additive identity and it reduces to the cumsum (cumulative sum) when 𝑎 = 1 is the multiplicative identity.</p><p>Finally, note that in vectorized form we can write</p><formula xml:id="formula_54">ℎ = 𝑀𝑏 𝑀 =            1 𝑎 1 1 𝑎 2 𝑎 1 𝑎 2 1 . . . . . . . . . . . . 𝑎 𝑇 -1 . . . 𝑎 1 𝑎 𝑇 -1 . . . 𝑎 2 . . . 𝑎 𝑇 -1 1           </formula><p>In other words, this is simply the matrix-vector product by a 1-SS matrix 𝑀.</p><p>Therefore we have three ways of viewing this fundamental primitive operation that are all equivalent:</p><p>• A (scalar) SSM scan.</p><p>• A cumprodsum.</p><p>• A 1-SS matrix-vector multiplication .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Classical Algorithms</head><p>We first describe the two classical ways of computing the SSM scan <ref type="bibr" target="#b21">(21)</ref>, previously used by prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Sequential Recurrence</head><p>The recurrent mode simply computes <ref type="bibr" target="#b21">(21)</ref> one timestep 𝑡 at a time. From the perspective of 1-SS multiplication, this was also described in Section 3.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Parallel Associative Scan</head><p>Second, an important observation is that this recurrence can be turned into an associative scan (E. Martin and Cundy 2018; J. T. Smith, Warrington, and Linderman 2023). This fact is not completely obvious. For example, S5 defined the correct associative scan operator and then showed associativity of the operator through rote calculation.</p><p>A slightly cleaner way to see that this is computable with an associative scan is to turn the multi-term recurrence into a single-term recurrence on a hidden state of size 2 instead of 1:</p><formula xml:id="formula_55">ℎ 𝑡 = 𝑎 𝑡 ℎ 𝑡 -1 + 𝑏 𝑡 ℎ 𝑡 1 = 𝑎 𝑡 𝑏 𝑡 0 1 ℎ 𝑡 -1<label>1</label></formula><p>.</p><p>Then computing all the ℎ 𝑡 is the same as taking the cumulative products of these 2 × 2 matrices. Since matrix multiplication is associative, this can be computed with an associative scan. The associative binary operator is simply matrix multiplication on these particular matrices:</p><formula xml:id="formula_56">𝑎 𝑡 𝑏 𝑡 0 1 𝑎 𝑠 𝑏 𝑠 0 1 = 𝑎 𝑡 𝑎 𝑠 𝑎 𝑡 𝑏 𝑠 + 𝑏 𝑡 0 1</formula><p>.</p><p>Equating the top row yields the same associative scan operator as defined by S5:</p><formula xml:id="formula_57">(𝑎 𝑡 , 𝑏 𝑡 ) ⊗ (𝑎 𝑠 , 𝑏 𝑠 ) = (𝑎 𝑡 𝑎 𝑠 , 𝑎 𝑡 𝑏 𝑠 + 𝑏 𝑡 ).<label>(22)</label></formula><p>The reason why associative scans are important is that they can be parallelized using a divide-and-conquer algorithm (Blelloch 1990). We omit the details of this algorithm, and instead show that the entire associative SSM scan algorithm can be derived from scratch through matrix decompositions (Appendix B.3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Efficient Algorithms via Structured Matrix Decompositions</head><p>We discuss several algorithms for computing the SSM scan, all through the lens of finding structured matrix decompositions of the 1-SS matrix 𝑀. These algorithms or computation modes include</p><p>• A dilated mode where information is propagated 1, 2, 4, 8, . . . steps at a time.</p><p>• A state-passing mode where information is propagated forward in chunks.</p><p>• A fully recurrent mode that increments one step at a time, which is a special case of the state-passing mode.</p><p>• A block decomposition parallel mode where 𝑀 is divided into hierarchical blocks.</p><p>• A scan mode where 𝑀 is divide into equal size blocks and reduced recursively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 Dilated Mode</head><p>This mode factors the 1-SS matrix in a particular way involving increasing "strides". This is best illustrated through a concrete example:</p><formula xml:id="formula_58">𝑀 =                𝑎 0:0 𝑎 1:0 𝑎 1:1 𝑎 2:0 𝑎 2:</formula><p>1 𝑎 2:2 𝑎 3:0 𝑎 3:1 𝑎 3:2 𝑎 3:3 𝑎 4:0 𝑎 4:1 𝑎 4:2 𝑎 4:3 𝑎 4:4 𝑎 5:0 𝑎 5:1 𝑎 5:2 𝑎 5:3 𝑎 5:4 𝑎 5:5 𝑎 6:0 𝑎 6:1 𝑎 6:2 𝑎 6:3 𝑎 6:4 𝑎 6:5 𝑎 6:6 𝑎 7:0 𝑎 7:1 𝑎 7:2 𝑎 7:3 𝑎 7:4 𝑎 7:5 𝑎 7:6 𝑎 7:7</p><formula xml:id="formula_59">               =                𝑎 0:0 𝑎 1:</formula><p>1 𝑎 2:2 𝑎 3:3 𝑎 4:0 𝑎 4:4 𝑎 5:1 𝑎 5:5 𝑎 6:2 𝑎 6:6 𝑎 7:3 𝑎 7:7</p><formula xml:id="formula_60">                              𝑎 0:0 𝑎 1:1 𝑎 2:0 𝑎 2:2 𝑎 3:1 𝑎<label>3</label></formula><p>:3 𝑎 4:2 𝑎 4:4 𝑎 5:3 𝑎 5:5 𝑎 6:4 𝑎 6:6 𝑎 7:5 𝑎 7:7</p><formula xml:id="formula_61">                              𝑎 0:0 𝑎 1:0 𝑎 1:1 𝑎 2:</formula><p>1 𝑎 2:2 𝑎 3:2 𝑎 3:3 𝑎 4:3 𝑎 4:4 𝑎 5:4 𝑎 5:5 𝑎 6:5 𝑎 6:6 𝑎 7:6 𝑎 7:7</p><formula xml:id="formula_62">              </formula><p>Note that this closely resembles the computation of dilated convolutions.</p><p>We also note that this factorization shows that 1-SS matrices are a special case of butterfly matrices, another broad and fundamental type of structured matrix <ref type="bibr" target="#b26">(Dao, Gu, et al. 2019;</ref><ref type="bibr" target="#b27">Dao, Sohoni, et al. 2020</ref>).</p><p>Remark 8. This algoritihm is sometimes described as a "work-inefficient but more parallelizable" prefix sum algorithm (Hillis and Steele Jr 1986), becauses it uses 𝑂 (𝑇 log(𝑇 )) operations but has half the depth/span as the work-efficient associative scan algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 State-Passing (Chunkwise) Mode</head><p>This mode can be viewed as a generalization of the standard recurrent mode where instead of passing forward the recurrent state ℎ one step at a time, we compute the answer on chunks of arbitrary length 𝑘 and pass the state through the chunk. This can also be derived from a simple block decomposition of the 1-SS matrix.</p><p>Remark 9. While we call this "state-passing" to refer to how states are passed from one local segment to another, this is related to the "chunkwise" algorithms proposed by related models (Y.</p><p>Sun et al. 2023; Yang et al. 2024). Consider computing ℎ = 𝑀𝑏 in "chunks": for some index 𝑘 ∈ [𝑇 ], we want to compute ℎ 0:𝑘 or the output up to index 𝑘, and have a way to reduce the problem to a smaller problem on indices [𝑘 : 𝑇 ]. We write 𝑀 as 𝑀 =                 𝑎 0:0 𝑎 1:0 𝑎 1:1 . . . . . . 𝑎 𝑘 -1:0 . . . . . . 𝑎 𝑘 -1:𝑘 -1 𝑎 𝑘:0 . . . . . . 𝑎 𝑘:𝑘 -1 𝑎 𝑘:𝑘 . . . . . . . . . . . . 𝑎 𝑇 -1:0 . . . . . . 𝑎 𝑇 -1:𝑘 -1 𝑎 𝑇 -1:𝑘 . . . 𝑎 𝑇 -1:𝑇 -1                 Let the upper-left triangle be 𝑀 𝐿 , lower-right be 𝑀 𝑅 (left and right subproblems), and lower-left be 𝑀 𝐶 . Divide up 𝑏 into 𝑏 𝐿 = 𝑏 0:𝑘 and 𝑏 𝑅 = 𝑏 𝑘:𝑇 in the same way. Note that 𝑀𝑏 = 𝑀 𝐿 𝑏 𝐿 𝑀 𝑅 𝑏 𝑅 + 𝑀 𝐶 𝑏 𝐿 Also, 𝑀 𝐶 has the rank-1 factorization (this is essentially the defining property of semiseparable matrices)</p><formula xml:id="formula_63">𝑀 𝐶 =        𝑎 𝑘:𝑘 . . . 𝑎 𝑇 -1:𝑘        𝑎 𝑘 𝑎 𝑘 -1:0 • • • 𝑎 𝑘 -1:𝑘 -1 Thus 𝑀 𝐶 𝑏 𝐿 =        𝑎 𝑘:𝑘 . . . 𝑎 𝑇 -1:𝑘        𝑎 𝑘 • (𝑀𝑏) 𝑘 -1 .</formula><p>Here we think of (𝑀𝑏) 𝑘 -1 = ℎ 𝑘 -1 as the "final state" of the left chunk, because the row vector in 𝑀 𝐶 's factorization is the same as the final row of 𝑀 𝐿 . Furthermore, note that the column vector in 𝑀 𝐶 's factorization is the same as the final column of 𝑀 𝑅 . <ref type="foot" target="#foot_8">7</ref> Thus</p><formula xml:id="formula_64">𝑀 𝑅 𝑏 𝑅 + 𝑀 𝐶 𝑏 𝐿 = 𝑀 𝑅          𝑎 𝑘 ℎ 𝑘 -1 + 𝑏 𝑘 𝑏 𝑘+1 . . . 𝑏 𝑇 -1         </formula><p>Finally, we use the observation that 𝑀 𝐿 and 𝑀 𝑅 are self-similar to the original matrix 𝑀; the answers for these two smaller 1-SS matrix multiplications can be performed arbitrarily using any algorithm. In total, the algorithm proceeds as follows:</p><p>1. Compute the left half of the answer ℎ 0:𝑘 using any desired method (i.e. any of the methods for 1-SS multiplication from this section).</p><p>2. Compute the final state ℎ 𝑘 -1 .</p><p>3. Increment the state by one step to modify 𝑏 𝑘 .</p><p>4. Compute the right half of the answer ℎ 𝑘:𝑇 using any desired method.</p><p>In other words, we compute the left subproblem as a black box, pass its final state on to the right problem, and compute the right subproblem as a black box.</p><p>The utility of this method comes from more complicated settings, such as in the general 𝑁 -semiseparable case, and when the input 𝑏 has an additional "batch" dimension (or in other words this is a matrix-matrix instead of matrix-vector multiplication). In this case, we can use an alternate algorithm for the chunks (corresponding to MM by 𝑀 𝐿 and 𝑀 𝑅 ) that does not materialize the full hidden states ℎ. Instead, we skip the hidden states and directly compute the final state ℎ 𝑘 -1 in an alternate way, then "pass" the state to the next chunk.</p><p>Complexity. This method can be very work-efficient because steps 2-3 takes only constant time. Therefore assuming the two subproblems (steps 1 and 4) are linear time, the whole method takes linear time.</p><p>The downside is that this is also sequential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.3 Fully Recurrent Mode</head><p>Note that the fully recurrent mode, where the recurrence is evolved one step at a time <ref type="bibr" target="#b21">(21)</ref>, is simply an instantiation of the state-passing mode with chunk size 𝑘 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.4 (Parallel) Block Decomposition Mode</head><p>This uses the same matrix decomposition as the state-passing mode, but computes subproblems in a different order that trades off computation for parallelization.</p><p>As usual, we write 𝑀 as</p><formula xml:id="formula_65">𝑀 =            1 𝑎 1 1 𝑎 2 𝑎 1 𝑎 2 1 . . . . . . . . . . . . 𝑎 𝑇 -1 . . . 𝑎 1 𝑎 𝑇 -1 . . . 𝑎 2 . . . 𝑎 𝑇 -1 1            =            1 -𝑎 1 1 0 -𝑎 2 1 . . . . . . . . . . . . 0 0 . . . -𝑎 𝑇 -1 1            -1</formula><p>The key observation is again that the bottom-left quadrant of 𝑀 is rank-1. Aside from inspection, another way to see this is by using the RHS, observing that the bottom-left quadrant of it is a trivial rank-1 matrix (it is all 0 except the top-right corner is -𝑎 𝑇 /2 ), and using the Woodbury inversion formula to see that the bottom-left corner of the LHS must also be rank 1. This also provides a way to deduce the rank-1 factorization, which can be verified through inspection:</p><formula xml:id="formula_66">𝑀</formula><p>lower-left-quadrant =        (𝑎 𝑇 /2 . . . 𝑎 1 ) . . . 𝑎 𝑇 /2 . . . . . . . . . (𝑎 𝑇 -1 . . . 𝑎 𝑇 /2 𝑎 𝑇 /2-1 . . . 𝑎 1 ) . . . (𝑎 𝑇 -1 . . . 𝑎 𝑇 /2 )        =        𝑎 𝑇 /2 . . . 𝑎 𝑇 -1 . . . 𝑎 𝑇 /2        (𝑎 𝑇 /2-1 . . . 𝑎 1 ) . . . 𝑎 𝑇 /2-1 1 .</p><p>A second observation is that this matrix is self-similar: any principle submatrix has the same form. In particular, the top-left and bottom-right quadrants are both 1-SS matrices.</p><p>This provides an easy way to perform the matrix multiplication by 𝑀: recurse on the two halves (i.e. top-left and bottomright) in parallel, and then account for the bottom-left submatrix. This "combination" step in the divide-and-conquer algorithm is easy since the submatrix is rank 1. This leads to a parallel algorithm.</p><p>Complexity. Like the state-passing algorithm, this method uses the same block decompositions of the rank-structured semiseparable matrices. The difference is that we recurse on both subproblems in parallel, while the state-passing algorithm handles the left and then right subproblems. This lowers the depth/span of the algorithm from linear to log(𝑇 ). The tradeoff is that the combination step (accounting for the rank-1 bottom-left submatrix) requires linear instead of constant work, so the total work is 𝑂 (𝑇 log(𝑇 )) instead of linear.</p><p>Note also that in the recursion, we can stop at any time and compute the subproblems in any other way. This is a main idea behind the SSD algorithm (Section 6), where we switch to the dual quadratic attention formulation on small subproblems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.5 Associative Scan Mode</head><p>The state passing (chunkwise) algorithm has linear work, but also involves sequential operations.</p><p>The block matrix reduction and dilated modes are parallelizable: they have log(𝑇 ) depth/span. However, they do extra work (𝑂 (𝑇 log(𝑇 )).</p><p>As noted in Appendix B.2.2, there is an algorithm that achieves both 𝑂 (log𝑇 ) depth and 𝑂 (𝑇 ) work by leveraging the associative scan (also called prefix scan) algorithm <ref type="bibr" target="#b8">(Baker et al. 1996</ref>). This algorithm is most easily seen from the SSM scan or cumprodsum view, and even then is not obvious: it requires separately deriving an associative operator <ref type="bibr" target="#b22">(22)</ref>, and then leveraging the parallel/associative/prefix scan algorithm as a black box <ref type="bibr" target="#b13">(Blelloch 1990</ref>).</p><p>Here we show that it is actually possible to derive this parallel scan from leveraging a different matrix decomposition:</p><formula xml:id="formula_67">𝑀 =               </formula><p> 𝑎 0:0 𝑎 1:0 𝑎 1:1 𝑎 2:0 𝑎 2:1 𝑎 2:2 𝑎 3:0 𝑎 3:1 𝑎 3:2 𝑎 3:3 𝑎 4:0 𝑎 4:1 𝑎 4:2 𝑎 4:3 𝑎 4:4 𝑎 5:0 𝑎 5:1 𝑎 5:2 𝑎 5:3 𝑎 5:4 𝑎 5:5 𝑎 6:0 𝑎 6:1 𝑎 6:2 𝑎 6:3 𝑎 6:4 𝑎 6:5 𝑎 6:6 𝑎 7:0 𝑎 7:1 𝑎 7:2 𝑎 7:3 𝑎 7:4 𝑎 7:5 𝑎 7:6 𝑎 7:7                 =                         𝑎 0:0 𝑎 1:0 𝑎 1:1 𝑎 2:2 𝑎 3:2 𝑎 3:3 𝑎 4:4 𝑎 5:4 𝑎 5:5 𝑎 6:6 𝑎 7:6 𝑎 7:7 𝑎 2:2 𝑎 3:2 𝑎 2:1 𝑎 1:0 𝑎 1:1 ⊤ 𝑎 4:4 𝑎 5:4 𝑎 4:1 𝑎 1:0 𝑎 1:1 ⊤ 𝑎 4:4 𝑎 5:4 𝑎 4:3 𝑎 3:2 𝑎 3:3 ⊤ 𝑎 6:6 𝑎 7:6 𝑎 6:1 𝑎 1:0 𝑎 1:1 ⊤ 𝑎 6:6 𝑎 7:6 𝑎 6:3 𝑎 3:2 𝑎 3:3 ⊤ 𝑎 6:6 𝑎 7:6 𝑎 6:1 𝑎 5:4 𝑎 5:5</p><formula xml:id="formula_68">⊤                        </formula><p>Now we proceed in three stages.</p><p>Stage 1. First we compute the answers for each of the diagonal blocks in the multiplication 𝑀𝑏. This produces two numbers, but the first element is unchanged. For example, the second block is going to compute 𝑏 2 and 𝑎 3 𝑏 2 + 𝑏 3 Note that this can be slightly modified with some off-by-one shifting of the indices. An equivalent way to view this algorithm is as the three-step matrix factorization</p><p>𝑀 =                𝑎 0:0 𝑎 1:0 𝑎 1:1 𝑎 2:0 𝑎 2:1 𝑎 2:2 𝑎 3:0 𝑎 3:1 𝑎 3:2 𝑎 3:3 𝑎 4:0 𝑎 4:1 𝑎 4:2 𝑎 4:3 𝑎 4:4 𝑎 5:0 𝑎 5:1 𝑎 5:2 𝑎 5:3 𝑎 5:4 𝑎 5:5 𝑎 6:0 𝑎 6:1 𝑎 6:2 𝑎 6:3 𝑎 6:4 𝑎 6:5 𝑎 6:6 𝑎 7:0 𝑎 7:1 𝑎 7:2 𝑎 7:3 𝑎 7:4 𝑎 7:5 𝑎 7:6 𝑎 7:7                =                𝑎 0:0 𝑎 1:1 𝑎 2:1 𝑎 2:2 𝑎 3:3 𝑎 4:3 𝑎 4:4 𝑎 5:5 𝑎 6:5 𝑎 6:6 𝑎 7:7                               𝑎 0:0 𝑎 1:1 𝑎 2:2 𝑎 3:1 𝑎 3:3 𝑎 4:4 𝑎 5:1 𝑎 5:3 𝑎 5:5 𝑎 6:6 𝑎 7:1 𝑎 7:3 𝑎 7:5 𝑎 7:7                               𝑎 0:0 𝑎 1:0 𝑎 1:1 𝑎 2:2 𝑎 3:2 𝑎 3:3 𝑎 4:4 𝑎 5:4 𝑎 5:5 𝑎 6:6 𝑎 7:6 𝑎 7:7                Note that Stage 1 and Stage 3 require 𝑂 (𝑇 ) work, while Stage 2 reduces to a self-similar problem of half the size. It is easy to check that this requires 𝑂 (𝑇 ) total work and 𝑂 (log𝑇 ) depth/span.</p><p>Remark 10. In fact, it is possible to see that the computation graph of this algorithm is identical to that of the associative scan algorithm described in Appendix B.2.2. The key takeaway is that instead of the steps of (1) recognizing that 𝑀 defines a recurrence (2) observing that the recurrence can be defined with an associative binary operator; there is a completely different perspective of simply finding a structured matrix decomposition algorithm for 𝑀.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Theory Details C.1 Extras: Closure Properties of SSMs</head><p>We present here some additional properties of semiseparable matrices to illustrate their flexibility and utility. This section is not necessary to understand our core results.</p><p>Proposition C.1 (Semiseparable Closure Properties). Semiseparable matrices are closed under several primitive operations.</p><p>• Addition: The sum of an 𝑁 -SS and 𝑃-SS matrix is at most (𝑁 + 𝑃)-SS.</p><p>• Multiplication: The product of an 𝑁 -SS and 𝑃-SS matrix is (𝑁 + 𝑃)-SS.</p><p>• Inverse: The inverse of an 𝑁 -SS matrix is at most (𝑁 + 1)-SS.</p><p>The addition and multiplication properties are easily seen. The inverse property has many proofs; one approach follows immediately from the Woodbury inversion identity, which has also featured prominently in the structured SSM literature (Gu, Goel, and Ré 2022).</p><p>In turn, these imply closure properties of state space models.</p><p>For example, the addition property says that summing two parallel SSM models is still an SSM. The multiplication property says that sequentially composing or chaining two SSMs can still be viewed as an SSM, whose total state size is additive-a somewhat nontrivial fact.</p><p>Finally, the inverse property can let us relate SSMs to other types of models. For example, one can notice that banded matrices are semiseparable, so their inverses are semiseparable. (In fact, the semiseparable family of structure is often motivated by taking inverses of banded matrices <ref type="bibr" target="#b102">(Vandebril et al. 2005)</ref>). Moreover, the fast recurrence properties of semiseparable matrices can be viewed as a consequence of their inverse being banded.</p><p>Remark 11. The fact that 1-SS matrices are simple recurrences <ref type="bibr" target="#b6">(7)</ref> are equivalent to the fact that the inverse of a 1-SS matrix is a 2-banded matrix:</p><formula xml:id="formula_69">𝑀 =            1 𝑎</formula><p>1 1 𝑎 2 𝑎 1 𝑎 2 1 . . . . . . . . . . . . 𝑎 𝑇 -1 . . . 𝑎 1 𝑎 𝑇 -1 . . . 𝑎 2 . . . 𝑎 𝑇 -1 1            =            1 -𝑎 1 1 0 -𝑎 2 1 . . . . . . . . . . . . 0 0 . . . -𝑎 𝑇 -1 1            -1 Thus 𝑦 = 𝑀𝑥 ↔ 𝑀 -1 𝑦 = 𝑥, or            1 -𝑎 1 1 0 -𝑎 2 1 . . . . . . . . . . . . 0 0 . . . -𝑎 𝑇 -1 1            𝑦 = 𝑥 . Or elementwise, 𝑦 𝑡 -𝑎 𝑡 𝑦 𝑡 -1 = 𝑥 𝑡 𝑦 𝑡 = 𝑎 𝑡 𝑦 𝑡 -1 + 𝑥 𝑡 .</p><p>Conversely, we also use these closure results to prove that autoregressive structured attention (under certain assumptions) must be SSMs, allowing us to show that more general families of efficient sequence models including attention variants can be reduced to state space models (Appendix C.2).</p><p>Vectorizing over 𝑡, this can be expressed as a matrix transformation</p><formula xml:id="formula_70">              1 -ℓ 𝑡</formula><p>1 1 . . . . . . . . . -ℓ 𝑡𝑘 . . . -ℓ 𝑡 1 1 . . . . . . . . . . . . . . . 0 . . . -ℓ 𝑇 -1,𝑘 . . . -ℓ 𝑇 -1,1 1                             𝑦 0 𝑦 1 . . . 𝑦 𝑘 . . . 𝑦 𝑇 -1               =               𝜇 0 𝜇 1 . . . 𝜇 𝑘 . . . 𝜇 𝑇 -1                             𝑥 0 𝑥 1 . . . 𝑥 𝑘 . . . 𝑥 𝑇 -1</p><formula xml:id="formula_71">             </formula><p>.</p><p>The 𝜇 diagonal matrix can be moved to the left and folded into the matrix of ℓ coefficients, which remains a 𝑘 + 1-band lower-triangular matrix. But we also have 𝐿 -1 𝑦 = 𝑥, so 𝐿 is the inverse of this matrix.</p><p>Next, note that 𝑘 +1-band matrices are 𝑘 +1-semiseparable by the rank characterization of semiseparability (Definition 3.1). By Proposition C.1, the inverse 𝐿 is therefore at most 𝑘 +2-semiseparable. A slightly stronger bound of 𝑘 +1 can be obtained because of the additional structure of banded matrices. Finally, the characterization of 𝐿 as an order-𝑘 +1 state space model follows from Theorem 3.5. □</p><p>In other words, efficient autoregressive attention is semiseparable SMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details D.1 MQAR Details</head><p>We use a harder version of the task introduced in Based (Arora, Eyuboglu, <ref type="bibr">Zhang, et al. 2024)</ref> where tokens that are not query/key/values are replaced with random tokens. We also use more key-value pairs, longer sequences, and smaller model sizes than the usual variant of MQAR used by prior work, all of which make the task harder.</p><p>For each sequence length 𝑇 ∈ {256, 512, 1024}, we use 𝑇 /4 key-value pairs. The total vocab size is 8192.</p><p>We use a form of curriculum training where training cycles through datasets using (𝑇 /32,𝑇 /16,𝑇 /8,𝑇 /4) key-value pairs, where each dataset has 2 18 ≈ 250000 examples, for a total of 8 epochs through each dataset (total of 2 28 ≈ 270𝑀 examples). The total batch size is 2 18 ≈ 0.25𝑀 tokens (e.g. for 𝑇 = 1024, the batch size is 256).</p><p>All methods use 2 layers with default settings; the attention baseline additionally receives positional embeddings. For each method, we sweep over model dimensions D = {32, 64, 128, 256} and learning rates {10 -3.5 , 10 -2 , 10 -2.5 }. We use a linear decay schedule that drops on every epoch (e.g. the last epoch would have a learning rate 1/8 of the maximum/starting learning rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Scaling Law Details</head><p>All models were trained on the Pile. For the scaling law experiments, we use the GPT2 tokenizer.</p><p>Model Sizes.</p><p>Table 9 specifies the model sizes we use for scaling laws following GPT3 (Brown et al. 2020), First, we changed the batch size of the 1.3B model from 1M tokens to 0.5M tokens for uniformity. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al. 2022), which specify that training tokens should increase proportionally to model size. Training Recipes. All models used the AdamW optimizer with • gradient clip value 1.0 • weight decay 0.1 • no dropout • linear learning rate warmup with cosine decay</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (State Space Models are Semiseparable Matrices.) As sequence transformations, state space models can be represented as a matrix transformation 𝑀 ∈ R (T,T) acting on the sequence dimension T, sharing the same matrix for each channel in a head (Left). This matrix is a semiseparable matrix (Right), which is a rank-structured matrix where every submatrix contained on-and-below the diagonal (Blue) has rank at most N, equal to the SSM's state dimension.</figDesc><graphic coords="9,268.64,121.15,291.51,82.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 4 . 1 (</head><label>41</label><figDesc>(Katharopoulos et al. 2020)). Autoregressive kernel attention, i.e. masked kernel attention with the causal mask, can be computed in 𝑂 (𝑇 ) time by a recurrence taking constant time per step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 4 . 2 .</head><label>42</label><figDesc>Structured masked attention (SMA) (or structured attention for short) is defined as a function on queries/keys/values 𝑄, 𝐾, 𝑉 as well as any structured matrix 𝐿 (i.e. has sub-quadratic matrix multiplication), through the 4-way tensor contraction 𝑌 = contract(TN, SN, SP, TS → TP) (𝑄, 𝐾, 𝑉 , 𝐿).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (Structured Masked Attention.) SMA constructs a masked attention matrix 𝑀 = 𝑄𝐾 ⊤ • 𝐿 for any structured matrix 𝐿, which defines a matrix sequence transformation 𝑌 = 𝑀𝑉 . All instances of SMA have a dual subquadratic form induced by a different contraction ordering, combined with the efficient structured matrix multiplication by 𝐿. Previous examples include Linear Attention (Katharopoulos et al. 2020) and RetNet (Y. Sun et al. 2023). Beyond SSD (1-semiseparable SMA), the focus of this paper, many other potential instantiations of structured attention are possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Listing 1 Full</head><label>1</label><figDesc>PyTorch example of the state space dual (SSD) model. def segsum(x):"""Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.""" T = x.size(-1) x_cumsum = torch.cumsum(x, dim=-1) x_segsum = x_cumsum[..., :, None] -x_cumsum[..., None, :] mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0) x_segsum = x_segsum.masked_fill(~mask, -torch.inf) return x_segsum def ssd(X, A, B, C, block_len=64, initial_states=None): """ Arguments: X: (batch, length, n_heads, d_head) A: (batch, length, n_heads) B: (batch, length, n_heads, d_state) C: (batch, length, n_heads, d_state) Return: Y: (batch, length, n_heads, d_head) """ assert X.dtype == A.dtype == B.dtype == C.dtype assert X.shape[1] % block_len == 0 # Rearrange into blocks/chunks X, A, B, C = [rearrange(x, "b (c l) ... -&gt; b c l ...", l=block_len) for x in (X, A, B, C)] A = rearrange(A, "b c l h -&gt; b h c l") A_cumsum = torch.cumsum(A, dim=-1) # 1. Compute the output for each intra-chunk (diagonal blocks) L = torch.exp(segsum(A)) Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp-&gt;bclhp", C, B, L, X) # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms) decay_states = torch.exp((A_cumsum[:, :, :, -1:] -A_cumsum)) states = torch.einsum("bclhn,bhcl,bclhp-&gt;bchpn", B, decay_states, X) # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms) if initial_states is None: initial_states = torch.zeros_like(states[:, :1]) states = torch.cat([initial_states, states], dim=1) decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0)))) new_states = torch.einsum("bhzc,bchpn-&gt;bzhpn", decay_chunk, states) states, final_state = new_states[:, :-1], new_states[:, -1] # 4. Compute state -&gt; output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms) state_decay_out = torch.exp(A_cumsum) Y_off = torch.einsum('bclhn,bchpn,bhcl-&gt;bclhp', C, states, state_decay_out) # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks) Y = rearrange(Y_diag+Y_off, "b c l h p -&gt; b (c l) h p") return Y, final_state • Parallelization: larger M, N, K terms can leverage specialized matrix multiplication units on modern accelerators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (Mamba-2 Architecture.) The Mamba-2 block simplifies the Mamba block by removing sequential linear projections; the SSM parameters 𝐴, 𝐵, 𝐶 are produced at the beginning of the block instead of as a function of the SSM input 𝑋 . An additional normalization layer is added as in NormFormer (Shleifer, Weston, and Ott 2021), improving stability. The 𝐵 and 𝐶 projections only have a single head shared across the 𝑋 heads, analogous to multi-value attention (MVA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>For transformers, sophisticated techniques have been develop to avoid padding and do load-balancing between GPUs (Zeng et al. 2022; Y. Zhai et al. 2023), or packing multiple sequences in the same batch and adjust the attention mask (Ding et al. 2024; Pouransari et al. 2024).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (Efficiency Benchmarks.) (Left) Our SSD is 2 -8× faster than a Mamba fused scan for large state expansion (𝑁 = 64) and faster than FlashAttention-2 for sequence length 2k and above. (Right) Sequence length 4K: Increasing state expansion slows down the Mamba optimized scan implementation linearly. SSD can handle much larger state expansion factors without much slowdown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>architecture) as well as recent subquadratic architectures (H3 (Dao, D. Y. Fu, et al. 2023), Hyena (Poli et al. 2023), RWKV-4 (B. Peng, Alcaide, et al. 2023), RetNet (Y. Sun et al. 2023)), we omit those in the plot for clarity (see Gu and Dao (2023) for comparisons).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>9. 2 . 3</head><label>23</label><figDesc>Hybrid Models: Combining SSD Layer with MLP and Attention Recent and concurrent work (Dao, D. Y. Fu, et al. 2023; De et al. 2024; Glorioso et al. 2024; Lieber et al. 2024</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>𝐿 2 SSD</head><label>2</label><figDesc>layers and 𝐿2 MLP layers to speed up training at short sequence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>Time Variance (Selectivity). The original structured SSMs (S4) were linear time-invariant (LTI) systems (Gu 2023; Gu, Goel, and Ré 2022) motivated by continuous-time online memorization (Gu, Dao, et al. 2020; Gu, Johnson, Goel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023). Many variants of structured SSMs have been proposed (Dao, D. Y. Fu, et al. 2023; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Ma et al. 2023; J. T. Smith, Warrington, and Linderman 2023), including several that drop the recurrence and focus on the convolutional representation of LTI SSMs (D. Y. Fu et al. 2023; Y. Li et al. 2023; Poli et al. 2023; Qin, Han, Weixuan Sun, B. He, et al. 2023). SSD is a time-varying structured SSM, also known as a selective SSM introduced in Mamba (Gu and Dao 2023). Selective SSMs are closely related to gating mechanisms of RNNs, including classical RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014) as well as more modern variants such as the QRNN (Bradbury et al. 2016), SRU (Lei 2021; Lei et al. 2017), RWKV (B. Peng, Alcaide, et al. 2023), HGRN (Qin, Yang, and Zhong 2023), and Griffin (Botev et al. 2024; De et al. 2024). These RNNs differ in their parameterizations in various ways, most importantly in the lack of a state expansion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>include MLP-Mixer (Tolstikhin et al. 2021) (unstructured matrix), FNet (Lee-Thorp et al. 2021) (Fourier Transform matrix), M2 (Dao, B. Chen, et al. 2022; Dao, Gu, et al. 2019; Dao, Sohoni, et al. 2020; D. Fu et al. 2024) (butterfly/monarch matrix), Toeplitz matrices (Poli et al. 2023; Qin, Han, Weixuan Sun, B. He, et al. 2023), and even more exotic structures (De Sa et al. 2018; Thomas et al. 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>First,</head><figDesc>SSD does not use the softmax activation of standard attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017), which is what gives attention its quadratic complexity. When the softmax is dropped, the sequence can be computed with linear scaling through the linear attention framework (Katharopoulos et al. 2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Stage 2 . 5 . 3 .</head><label>253</label><figDesc>Now consider each of the 2 × 2 blocks factored as a rank-1 matrix in the strictly lower triangular part of the matrix. Note that each of the right side row vectors is the same as the bottom row vector in the diagonal block in its column: in particular the [𝑎 1:0 𝑎 1:1 ], [𝑎 3:2 𝑎 3:3 ], and [𝑎 5:4 𝑎 5:5 ] rows. Therefore we already have the answers to these from Stage 1, which is the second element of all 𝑇 /2 subproblems in Stage 1. If we call this array of elements 𝑏 ′ (of half the size of 𝑏), then we need to multiply 𝑏 ′ by the 1-SS matrix generated by 𝑎 3:-1 , 𝑎 3:1 , 𝑎 5:3 , 𝑎 7:Stage Finally, each of the answers to Stage 2 can be broadcast into two final answers by multiplying by the left-side column vectors: in particular the [𝑎 2:2 𝑎 3:2 ] ⊤ , [𝑎 4:4 𝑎 5:4 ] ⊤ , and [𝑎 6:6 𝑎 7:6 ] ⊤ vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 :</head><label>1</label><figDesc>Models of size ≈ 125𝑀 to ≈ 1.3𝐵 parameters, trained on the Pile. Mamba-2 matches or exceeds the performance of Mamba as well as a strong "Transformer++" recipe. Compared to our Transformer baseline, Mamba-2 is Pareto dominant on performance (perplexity), theoretical FLOPs, and actual wall-clock time.(Zero-shot Evaluations.) Best results for each size in bold, second best unlined. We compare against open source LMs with various tokenizers, trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer (GPT-NeoX-20B). For each model size, Mamba-2 outperforms Mamba, and generally matches Pythia at twice the model size. Full results in Table10.</figDesc><table><row><cell>*034WPSKWGEPI</cell></row></table><note><p>al. 2024; Jelassi et al. 2024).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2 :</head><label>2</label><figDesc>(Combining SSD and Attention Blocks.) Perplexity of a 350M model with 48 layers, with different number of attention layers. Having around a 10% ratio of attention layers performs best. .32 8.29 8.29 8.28 8.26 8.27 8.28 8.30 8.34 8.50 8.68</figDesc><table><row><cell cols="3">Num. Attn Blocks 0 (Mamba-2) 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>15</cell><cell>24</cell><cell>Transformer++</cell></row><row><cell>Perplexity ↓</cell><cell>8.60</cell><cell cols="2">8.38 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 :</head><label>3</label><figDesc>(Zero-shot Evaluations.) Best results for each size in bold. We compare different ways SSD, MLP, and attention layers can be combined, evaluated at 2.7B scale trained to 300B tokens on the Pile.</figDesc><table><row><cell>Model</cell><cell cols="2">Token. Pile</cell><cell cols="9">LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average</cell></row><row><cell></cell><cell></cell><cell cols="2">ppl ↓ ppl ↓</cell><cell>acc ↑</cell><cell>acc ↑</cell><cell cols="2">acc ↑ acc ↑</cell><cell>acc ↑</cell><cell>acc ↑</cell><cell>acc ↑</cell><cell>acc ↑</cell></row><row><cell>Transformer++</cell><cell>NeoX</cell><cell>6.13</cell><cell>3.99</cell><cell>70.3</cell><cell>66.4</cell><cell>75.2</cell><cell>67.7</cell><cell>37.8</cell><cell>63.9</cell><cell>40.4</cell><cell>60.2</cell></row><row><cell>Mamba-2</cell><cell>NeoX</cell><cell>6.09</cell><cell>4.10</cell><cell>69.7</cell><cell>66.6</cell><cell>76.4</cell><cell>69.6</cell><cell>36.4</cell><cell>64.0</cell><cell>38.8</cell><cell>60.2</cell></row><row><cell>Mamba-2-MLP</cell><cell>NeoX</cell><cell>6.13</cell><cell>4.18</cell><cell>69.3</cell><cell>65.0</cell><cell>76.4</cell><cell>68.1</cell><cell>37.0</cell><cell>63.1</cell><cell>38.2</cell><cell>59.6</cell></row><row><cell>Mamba-2-Attention</cell><cell>NeoX</cell><cell cols="2">5.95 3.85</cell><cell>71.1</cell><cell>67.8</cell><cell>75.8</cell><cell>69.9</cell><cell>37.8</cell><cell>65.3</cell><cell>39.0</cell><cell>61.0</cell></row><row><cell cols="2">Mamba-2-MLP-Attention NeoX</cell><cell>6.00</cell><cell>3.95</cell><cell>70.0</cell><cell>66.6</cell><cell>75.4</cell><cell>70.6</cell><cell>38.6</cell><cell>64.6</cell><cell>39.2</cell><cell>60.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 :</head><label>4</label><figDesc>(Ablations: Mamba-2 block.) We ablate the major differences between the Mamba-2 and Mamba-1 neural network blocks (Figure6, Section 7.1). Note that these components are independent of the inner sequence mixing layer; in these ablations, we use SSD for the inner SSM layer (differing from the S6 layer of Mamba-1).</figDesc><table><row><cell>Block</cell><cell cols="4">𝐴𝐵𝐶𝑋 Projections Extra Normalization Parameters Perplexity</cell></row><row><cell cols="2">Mamba-1 Sequential</cell><cell>✗</cell><cell>129.3M</cell><cell>11.76</cell></row><row><cell></cell><cell>Sequential</cell><cell>✓</cell><cell>129.3M</cell><cell>11.54</cell></row><row><cell></cell><cell>Parallel</cell><cell>✗</cell><cell>126.5M</cell><cell>11.66</cell></row><row><cell cols="2">Mamba-2 Parallel</cell><cell>✓</cell><cell>126.5M</cell><cell>11.49</cell></row><row><cell cols="2">9.4 Architecture Ablations</cell><cell></cell><cell></cell></row><row><cell>9.4.1 Block Design</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 5 :</head><label>5</label><figDesc>(Ablations: Multi-head structure.) All models have state expansion factor 𝑁 = 64 and head size 𝑃 = 64 and are trained to Chinchilla scaling law token counts. The number of 𝐴 heads is always equal to the total heads H, i.e. each head has a separate input-dependent 𝐴 decay factor. (Top) 125M models, 2.5B tokens (Bottom) 360M models, 7B tokens SSM Head Pattern Attn. Analog 𝐴 heads 𝐵 heads 𝐶 heads 𝑋 heads Layers Params Ppl.</figDesc><table><row><cell>Multi-input (MIS)</cell><cell>Multi-value (MVA)</cell><cell>24</cell><cell>1</cell><cell>1</cell><cell>24</cell><cell>24</cell><cell>126.5M</cell><cell>11.66</cell></row><row><cell cols="3">Multi-contract (MCS) Multi-query (MQA) 24</cell><cell>1</cell><cell>24</cell><cell>1</cell><cell>24</cell><cell>126.5M</cell><cell>12.62</cell></row><row><cell>Multi-expand (MES)</cell><cell>Multi-key (MKA)</cell><cell>24</cell><cell>24</cell><cell>1</cell><cell>1</cell><cell>24</cell><cell>126.5M</cell><cell>12.59</cell></row><row><cell>Multi-head (MHS)</cell><cell>Multi-head (MHA)</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>15</cell><cell>127.6M</cell><cell>12.06</cell></row><row><cell>Multi-state (MSS)</cell><cell>-</cell><cell>24</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>36</cell><cell>129.6M</cell><cell>12.00</cell></row><row><cell>Multi-input (MIS)</cell><cell>Multi-value (MVA)</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>32</cell><cell>48</cell><cell>361.8M</cell><cell>8.73</cell></row><row><cell cols="3">Multi-contract (MCS) Multi-query (MQA) 32</cell><cell>1</cell><cell>32</cell><cell>1</cell><cell>48</cell><cell>361.8M</cell><cell>9.33</cell></row><row><cell>Multi-expand (MES)</cell><cell>Multi-key (MKA)</cell><cell>32</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>48</cell><cell>361.8M</cell><cell>9.36</cell></row><row><cell>Multi-head (MHS)</cell><cell>Multi-head (MHA)</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>70</cell><cell>361.3M</cell><cell>9.01</cell></row><row><cell>Multi-state (MSS)</cell><cell>-</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>29</cell><cell>357.3M</cell><cell>9.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>(Ablations: Kernel approximations.) We test various proposals for the kernel activation function 𝜓 , including linear attention variants aiming to approximate the exp kernel from standard softmax attention.</figDesc><table><row><cell>Kernel activation 𝜑</cell><cell>Perplexity</cell></row><row><cell>none</cell><cell>11.58</cell></row><row><cell>Swish</cell><cell>11.66</cell></row><row><cell>Exp</cell><cell>11.62</cell></row><row><cell>ReLU</cell><cell>11.73</cell></row><row><cell>ReLU + normalization</cell><cell>11.64</cell></row><row><cell>cosFormer</cell><cell>11.97</cell></row><row><cell>Random Feature Attention</cell><cell>11.57</cell></row><row><cell cols="2">Positive Random Features (Performer) 12.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>(Ablations: Kernel approximations.) We test the (Re)Based methods for linear attention approximations, which involve expanded feature maps. (Top) 130M models. (Top) 380M models with 𝑁 = 256.</figDesc><table><row><cell>Kernel activation 𝜑</cell><cell>Perplexity</cell></row><row><cell>Swish</cell><cell>11.67</cell></row><row><cell>Swish + Taylor (Based)</cell><cell>12.19</cell></row><row><cell>LayerNorm</cell><cell>11.50</cell></row><row><cell cols="2">LayerNorm + Square (ReBased) 11.84</cell></row><row><cell>Swish</cell><cell>8.58</cell></row><row><cell>Swish + Taylor (Based)</cell><cell>8.71</cell></row><row><cell>LayerNorm</cell><cell>8.61</cell></row><row><cell cols="2">LayerNorm + Square (ReBased) 8.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 8 :</head><label>8</label><figDesc>Glossary of notation and terminology; mnemonics bolded. (Top) Frequently used tensor dimensions. (Bottom) Matrices and tensors used in state space models or structured masked attention.</figDesc><table><row><cell>Notation Description</cell></row></table><note><p>⊤ (or 𝐶𝐵 ⊤ ) 𝐿 (Structured) mask matrix (lower-triangular in the causal setting) Definition 4.2 B Efficient Algorithms for the Scalar SSM Scan (1-SS Multiplication)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Technically speaking, these connections only relate to certain flavors of attention; the title of this paper is an homage toKatharopoulos et al. (2020)   which first showed that "Transformers are RNNs".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In this work, this happens only with the 𝐴 parameter of SSMs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In some contexts, it is always clear that the notation 𝑎 𝑖:𝑗 or 𝐴 𝑖:𝑗 means 𝑎 × 𝑖:𝑗 , and the superscript is omitted.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>A different symbol is required for the contraction notation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Note that the block decomposition is valid even with partitions of varying size, e.g. if Q ∤ T, but we assume even divisibility for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>The Mamba-2 Architecture By connecting SSMs and attention, the SSD framework allows us to develop a shared vocabulary and library of techniques for both. In this section we discuss some examples of understanding and modifying SSD layers using ideas originally</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>We describe several systems optimizations for SSMs, in particular the Mamba-2 architecture, for large-scale efficient training and inference. In particular, we focus on tensor parallel and sequence parallel for large-scale training, as a well variable-length sequences for efficient finetuning and inference.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>In small-scale experiments, we find that as long as the attention layers are spaced out, not at the very beginning or at the very end, the model quality does not depend very much on the exact location of the attention layers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>Both these facts can be seen from the Woodbury inverse...</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Angela Wu</rs> for the suggestion on how to efficiently compute the gradient of Δ in a numerically stable manner. We thank <rs type="person">Sukjun Hwang</rs> and <rs type="person">Aakash Lahoti</rs> for assistance with the MQAR experiments.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Autoregressive Masked Attention is Semiseparable-Structured Attention</head><p>We prove Theorem 5.2 from Section 5.2. In Section 4.3 we defined structured attention as a broad generalization of masked attention, where the property of efficiency (i.e. a linear-time form for the kernel attention) is abstracted into the efficiency of structured matrix multiplication. However, beyond computational efficiency, standard linear attention <ref type="bibr">(Katharopoulos et al. 2020</ref>) also has two important properties. First, it is causal, which is required for settings such as autoregressive modeling. Moreover, it has efficient autoregressive generation. In other words, the cost of an autoregressive step -i.e. the incremental cost of computing the output 𝑦 𝑇 upon seeing 𝑥 𝑇 , given that 𝑥 0:𝑇 has already been seen and preprocessedrequires only constant time.</p><p>Here we characterize which instances of SMA have efficient autoregression.</p><p>In the framework of SMA, causality is equivalent to the constraint that the mask 𝐿 is a lower-triangular matrix.</p><p>Characterizing the space of 𝐿 matrices that have efficient autoregression is more difficult. We will use a narrow technical definition of autoregressive processes, in the spirit of classical definitions from the time series literature (e.g. ARIMA processes (Box et al. 2015)).</p><p>Definition C.2. We define an autoregressive transformation 𝑥 ∈ R 𝑇 ↦ → 𝑦 ∈ R 𝑇 of order 𝑘 as one where each output 𝑦 𝑡 depends only on the current input and last 𝑘 outputs:</p><p>Note that the case where 𝐿 is the cumsum matrix is a special case with 𝑘 = 1 and thus 𝑦 𝑡 = 𝑥 𝑡 + 𝑦 𝑡 1 . With this definition, characterizing the space of efficient autoregressive linear transforms follows from the properties of semiseparable matrices. Theorem C.3 formalizes and proves Theorem 5.2.</p><p>Theorem C.3. Let 𝐿 ∈ R 𝑇 ×𝑇 be an efficient autoregressive transformation of order 𝑘. Then 𝐿 is a state space model of order 𝑘 + 1.</p><p>Proof. Let (𝑥, 𝑦) be input and output sequences, so that 𝑦 = 𝐿𝑥. Rearranging the definition <ref type="bibr" target="#b23">(23)</ref>, By default, the peak learning rate is the GPT3 specification.</p><p>Compared to GPT3 recipe, we use an "improved recipe", inspired by changes adopted by popular large language models such as PaLM <ref type="bibr" target="#b20">(Chowdhery et al. 2023</ref>) and LLaMa <ref type="bibr" target="#b100">(Touvron, Lavril, et al. 2023</ref>). These include:</p><p>• linear learning rate warmup with cosine decay to 1𝑒 -5, with a peak value of 5× the GPT3 value</p><p>• no linear bias terms</p><p>• RMSNorm instead of LayerNorm</p><p>• AdamW hyperparameter 𝛽 = (.9, .95) (the GPT3 value) instead of the PyTorch default of 𝛽 = (.9, .999)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Downstream Evaluation Details</head><p>To evaluate downstream performance of fully trained, we train Mamba-2 on 300B tokens on the Pile, using the GPT-NeoX (Black et al. 2022) tokenizer.</p><p>We use the same hyperparameters as the scaling experiments, except with batch size 1M for the 1.3B and 2.7B model. For the 2.7B model, we also follow GPT3 specification (32 layers, dimension 2560).</p><p>For all models, we use 5x the learning rate of the corresponding GPT3 model. Based approximates the exp kernel with a quadratic Taylor expansion exp(𝑥) ≈ 1 + 𝑥 + 𝑥 2 /2, which can be accomplished by the feature map</p><p>ReBased proposes to use the simpler feature map 𝜓 Quadratic (𝑥) = 𝑥 ⊗ 𝑥 corresponding to the kernel transformation 𝑥 2 , but also applies a layer normalization beforehand. We view the layer normalization as an alternative non-linear activation to our default Swish activation, and ablate combinations of these. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Lebrón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13245</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Aksenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Balagansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Vaina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Shaposhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gorbatovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Gavrilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10644</idno>
		<title level="m">Linear Transformers with Learnable Kernel Functions are Better In-Context Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">In-Context Language Learning: Architectures and Algorithms</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Hidden Attention of Mamba Models</title>
		<author>
			<persName><forename type="first">Ameen</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itamar</forename><surname>Zimerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01590[cs.LG]</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zoology: Measuring and Improving Recall in Efficient Language Models</title>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabri</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple Linear Attention Language Models Balance the Recall-Throughput Tradeoff</title>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabri</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silas</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Zinsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Baker</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Graves-Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pade Approximants: Encyclopedia of Mathematics and It&apos;s Applications</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Graves-Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korbinian</forename><surname>Pöppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Spanring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandra</forename><surname>Prudnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04517</idno>
		<title level="m">xLSTM: Extended Long Short-Term Memory</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pythia: A Suite for Analyzing Large Language Models across Training and Scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<idno>PMLR. 2023</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PIQA: Reasoning about Physical Commonsense in Natural Language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on Artificial Intelligence</title>
		<meeting>the AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gpt-NeoX-20B: An Open-source Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06745</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Prefix Sums and Their Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><surname>Blelloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George-Cristian</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruba</forename><surname>Muraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Haroun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><surname>Dadashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07839</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>George Ep Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gwilym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greta</forename><forename type="middle">M</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time Series Analysis: Forecasting and Control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Quasi-recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01576</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Striped attention: Faster ring attention for causal transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ankner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiye</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.09431</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language Models are Few-shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking Attention with Performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PaLM: Scaling Language Modeling with Pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v24/22-1144.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monarch: Expressive structured matrices for efficient and accurate training</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nimit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Grogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
		<idno>PMLR. 2022</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4690" to="4721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaleidoscope: An Efficient, Learnable Representation for All Structured Linear Maps</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimit</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Blonder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vision Transformers Need Registers</title>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</title>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cristian-Muraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruba</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Haroun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srivatsan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.19427</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Two-Pronged Progress in Structured Dense Matrix Vector Multiplication</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Puttagunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1060" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fewer truncations improve language modeling</title>
		<author>
			<persName><forename type="first">Hantian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10830</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On a new class of structured matrices</title>
		<author>
			<persName><forename type="first">Yuli</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israel</forename><surname>Gohberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integral Equations and Operator Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="324" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monarch mixer: A simple sub-quadratic gemm-based architecture</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Grogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sabri Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Spector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple Hardware-efficient Long Convolutions for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">L</forename><surname>Daniel Y Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Framework for Few-shot Language Model Evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5371628" />
		<imprint/>
	</monogr>
	<note>Version v0.0.1. Sept. 2021</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Tokpanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16712</idno>
		<title level="m">Zamba: A Compact 7B SSM Hybrid Model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Is Mamba Capable of In-Context Learning?</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schrodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03170</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Modeling Sequences with Structured State Spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00752</idno>
		<title level="m">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HIPPO: Recurrent Memory with Optimal Polynomial Projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficiently Modeling Long Sequences with Structured State Spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the Parameterization and Initialization of Diagonal State Space Models</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combining Recurrent, Convolutional, and Continuous-time Models with the Linear State Space Layer</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How to Train Your HIPPO: State Space Models with Generalized Basis Projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Diagonal State Spaces are as Effective as Structured State Spaces</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22982" to="22994" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)&quot;. In: arXiv preprint</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data Parallel Algorithms</title>
		<author>
			<persName><forename type="first">Hillis</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><surname>Steele</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An Empirical Analysis of Compute-Optimal Large Language Model Training</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="30016" to="30030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Repeat After Me: Transformers Are Better Than State Space Models at Copying</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jelassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Sham M Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Malach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
		<title level="m">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
		<imprint>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Katsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.01927</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Linear Dynamical Systems as a Core Computational Primitive</title>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16808" to="16820" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Andersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning and Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Fnet: Mixing tokens with fourier transforms</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7633" to="7648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Simple Recurrent Units for Highly Parallelizable Recurrence</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">What Makes Convolutional Models Great on Long Sequence Modeling?</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debadeepta</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Jamba: A Hybrid Transformer-Mamba Language Model</title>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hofit</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhonathan</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Safahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19887</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">World Model on Million-Length Video And Language With RingAttention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.08268</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Ring attention with blockwise transformers for near-infinite context</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01889</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Structured State Space Models for In-Context Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Schroecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feryal</forename><surname>Behbahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Mega: Moving Average Equipped Gated Attention</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Parallelizing Linear Recurrent Neural Nets Over Sequence Length</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">In-context Learning and Induction Heads</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" />
	</analytic>
	<monogr>
		<title level="j">Transformer Circuits Thread</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Resurrecting Recurrent Neural Networks for Long Sequences</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks</title>
		<author>
			<persName><forename type="first">Jongho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeseung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">RWKV: Reinventing RNNs for the Transformer Era</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Arcadinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Grella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kranthi</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13048</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Eagle and Finch: RWKV with matrix-valued states and dynamic recurrence</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Cheah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Ferdinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemysław</forename><surname>Kazienko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.05892</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Random Feature Attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Computing with Quasiseparable Matrices</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Pernet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on International Symposium on Symbolic and Algebraic Computation</title>
		<meeting>the ACM on International Symposium on Symbolic and Algebraic Computation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Exact computations with quasiseparable matrices</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Pernet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hippolyte</forename><surname>Signargout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Villard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04515</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Time and space efficient generators for quasiseparable matrices</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Pernet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Storjohann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Computation</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="224" to="246" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<title level="m">Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>The International Conference on Machine Learning (ICML)</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</title>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Hadi Pouransari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jen</forename><forename type="middle">-</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anasosalu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.13226</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Toeplitz Neural Network for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">The devil in linear transformer</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10340</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weigao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.14995</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">CosFormer: Rethinking Softmax in Attention</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">HGRN2: Gated Linear RNNs with State Expansion</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weigao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07904</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hierarchically Gated Recurrent Neural Network for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Random Features for Large-Scale Kernel Machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Swish: A Self-gated Activation Function</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.059417.1</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Winogrande: An Adversarial Winograd Schema Challenge at Scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Linear Transformers are Secretly Fast Weight Programmers</title>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>PMLR. 2021</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Fast Transformer Decoding: One Write-head is All You Need</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">NormFormer: Improved Transformer Pretraining with Extra Normalization</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09456</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Simplified State Space Layers for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Jimmy Th</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced Transformer with Rotary Position Embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Retentive network: A successor to transformer for large language models</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08621</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Efficient Transformers: A Survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Chameleon: Mixed-Modal Early-Fusion Foundation Models</title>
		<author>
			<persName><forename type="first">Chameleon</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.09818</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Learning Compressed Transforms with Low Displacement Rank</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9052" to="9060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">MLP-Mixer: An All-MLP Architecture for Vision</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Llama: Open and Efficient Foundation Language Models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A bibliography on semiseparable matrices</title>
		<author>
			<persName><forename type="first">Raf</forename><surname>Vandebril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Barel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gene</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Mastronardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calcolo</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="249" to="270" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory</title>
		<author>
			<persName><forename type="first">Shida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.13414</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with Linear Complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Efficient Streaming Language Models with Attention Sinks</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Gated Linear Attention Transformers with Hardware-Efficient Training</title>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a Machine Really Finish Your Sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Boosting distributed training performance of the unpadded bert model</title>
		<author>
			<persName><forename type="first">Jinle</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.08124</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">An Attention Free Transformer</title>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14103</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Bytetransformer: A high-performance transformer boosted for variable-length inputs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengquan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="344" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">The Hedgehog &amp; the Porcupine: Expressive Linear Attentions with Softmax Mimicry</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Kumbong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Linear complexity randomized self-attention mechanism</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno>PMLR. 2022</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="27011" to="27041" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
