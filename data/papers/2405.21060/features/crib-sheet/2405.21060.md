- **Transformers vs. SSMs**: Transformers (decoder-only models) are efficient for language modeling but scale quadratically with sequence length; SSMs (e.g., Mamba) show linear scaling and competitive performance.
  
- **State Space Duality (SSD)**: Framework connecting structured SSMs and attention variants, allowing for algorithmic and systems optimizations from Transformers to be applied to SSMs.

- **Mamba-2 Architecture**: A refined architecture based on Mamba's selective SSM, achieving 2-8Ã— faster performance while maintaining competitiveness with Transformers.

- **Structured Matrices**: Central to the SSD framework; structured semiseparable matrices enable efficient computation of SSMs and reveal new properties and algorithms.

- **Key Equations**:
  - Discrete form of structured SSMs:
    - \( h_t = A h_{t-1} + B x_t \)
    - \( y_t = C^\top h_t \)
  - Continuous-time perspective involves discretization rules for parameters.

- **Linear Attention**: Improved theory of linear attention connects autoregressive attention to linear RNNs, enabling efficient training and inference.

- **Selective SSMs**: Introduced in Mamba, allowing dynamic focus on inputs at each timestep, enhancing performance on information-dense tasks.

- **Efficient Algorithms**: SSD algorithm based on block decompositions of semiseparable matrices, optimizing compute, memory usage, and leveraging modern hardware.

- **Tensor Parallelism (TP)**: Mamba-2 designed to be TP-friendly, reducing synchronization points, enhancing training efficiency for large models.

- **Sequence Parallelism**: Method for training SSMs with long sequences by passing recurrent states between devices, improving memory efficiency.

- **Variable Length Sequences**: Mamba-2 can be trained efficiently without padding tokens, optimizing for finetuning with diverse input lengths.

- **Empirical Validation**: Mamba-2 outperforms Mamba and open-source Transformers on standard downstream evaluations, demonstrating superior perplexity and training efficiency.

- **Open Source**: Model code and pre-trained checkpoints available at [GitHub](https://github.com/state-spaces/mamba).