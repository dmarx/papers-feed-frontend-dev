<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An analytic theory of creativity in convolutional diffusion models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-28">28 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mason</forename><surname>Kamb</surname></persName>
							<email>&lt;kambm@stanford.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Physics</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<region>California</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Physics</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<region>California</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An analytic theory of creativity in convolutional diffusion models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-28">28 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">622E2ECEA6EBA709F1D9A690D6A1A088</idno>
					<idno type="arXiv">arXiv:2412.20292v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median r 2 of 0.90, 0.91, 0.94 on CIFAR10, FashionM-NIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median r 2 ∼ 0.75 on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and related work</head><p>A deep puzzle of generative AI lies in understanding how it produces seemingly endless, apparently creative, output. What is the origin of this creativity, and how precisely is it generated from a finite training set? We answer these questions for convolutional diffusion models of images by deriving the first analytic and interpretable theory of their creativity that can accurately predict their outputs on a case- by-case basis (Fig. <ref type="figure" target="#fig_0">1</ref>), and explain how they are created out of locally consistent patch mosaics of the training data.</p><p>Denoising probabilistic diffusion models (DDPMs) were established in <ref type="bibr" target="#b37">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b14">Ho et al., 2020)</ref> and then unified with score-matching <ref type="bibr" target="#b39">(Song &amp; Ermon, 2019;</ref><ref type="bibr">Song et al., 2020b)</ref>. Denoising diffusion implicit models (DDIMs), an alternative deterministic parameterization which we primarily use in this paper, were established in <ref type="bibr">(Song et al., 2020a)</ref>. Diffusion models now play an important role not only in image generation <ref type="bibr">(Dhariwal &amp; Nichol, 2021;</ref><ref type="bibr" target="#b32">Rombach et al., 2022;</ref><ref type="bibr" target="#b31">Ramesh et al., 2022)</ref>, but also video generation <ref type="bibr">(Ho et al., 2022a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b5">Blattmann et al., 2023)</ref>, drug design <ref type="bibr" target="#b0">(Alakhdar et al., 2024)</ref>, protein folding <ref type="bibr" target="#b43">(Watson et al., 2023)</ref>, and text generation <ref type="bibr" target="#b23">(Li et al., 2023;</ref><ref type="bibr" target="#b10">2022)</ref>. These models are trained to reverse a forward diffusion process that turns the finite training set distribution (a sum of δ-functions over the training points) into an isotropic Gaussian noise distribution, through a time-dependent family of mixtures of Gaussians centered at shrinking data points. Diffusion models are trained to reverse this process by learning and following a score function that points in gradient directions of increasing probability. But therein lies the puzzle of creativity in diffusion models: if the network can learn this ideal score function exactly, then they will implement a perfect reversal of the forward process; this, in turn, will only be able to turn Gaussian noise into memorized training examples. Thus, the fundamental creativity of diffusion models must lie in their failure to achieve the very objective they are trained on: learning the ideal score function. But how can they fail in intelligent ways that lead to many sensible new examples far from the training set?</p><p>Several theoretical and empirical works study the properties of diffusion models. Some works study the sampling properties of these models under the assumption that they learn the ideal score function exactly <ref type="bibr" target="#b4">(Biroli et al., 2024;</ref><ref type="bibr" target="#b10">De Bortoli, 2022)</ref> or up to some small bounded error <ref type="bibr" target="#b2">(Benton et al., 2024)</ref>. Others establish accuracy guarantees on learning the ideal score function under various assumptions on the data distribution, and the hypothesis class of functions <ref type="bibr" target="#b21">(Lee et al., 2022;</ref><ref type="bibr" target="#b6">Chen et al., 2023;</ref><ref type="bibr" target="#b28">Oko et al., 2023;</ref><ref type="bibr" target="#b41">Ventura et al., 2024;</ref><ref type="bibr">Cui &amp; Zdeborová, 2023;</ref><ref type="bibr">Cui et al., 2023)</ref>.</p><p>As noted above, a key limitation of studying diffusion models under the assumption that they (almost) learn the ideal score function is that such models can only generate memorized training examples, at odds with the creativity of diffusion models in practice. For example, they can compose aspects of their training data in combinatorially many novel ways <ref type="bibr" target="#b34">(Sclocchi et al., 2024;</ref><ref type="bibr" target="#b27">Okawa et al., 2024)</ref>. This observation has motivated studies of mechanisms behind generalization in diffusion models that underfit the score-matching objective <ref type="bibr">(Kadkhodaie et al., 2023b;</ref><ref type="bibr" target="#b44">Zhang et al., 2023;</ref><ref type="bibr" target="#b42">Wang et al., 2024)</ref>. Other works connect creativity in diffusion models to the breakdown of memorization in modern Hopfield networks <ref type="bibr" target="#b1">(Ambrogioni, 2023;</ref><ref type="bibr" target="#b18">Hoover et al., 2023;</ref><ref type="bibr" target="#b30">Pham et al., 2024)</ref>. However, none of these works can quantitatively predict individual creative samples from a trained diffusion model on a case-by-case basis.</p><p>To develop theory beyond the memorization regime, we focus on diffusion models with a fully-convolutional backbone, without the self-attention layers introduced in <ref type="bibr" target="#b14">(Ho et al., 2020)</ref>. We identify two fundamental inductive biases that prevent such models from learning the ideal scorefunction: translational equivariance, due to parameter sharing in convolutional layers, and locality, due to the model's finite receptive field size. Remarkably, we show these two simple biases are sufficient to quantitatively explain the creative outputs of convolutional diffusion models.</p><p>Relatedly, <ref type="bibr">(Kadkhodaie et al., 2023a</ref>) also identified locality as a limiting constraint in CNN-based diffusion models, but did not attempt to predict their individual creative outputs. Finally, the results of our analysis exhibit some similarity to very early patch-based texture synthesis methods <ref type="bibr" target="#b12">(Efros &amp; Leung, 1999)</ref>. Our contributions and paper outline are:</p><p>1. We review why diffusion models that learn the ideal score function can only memorize (Sec. 2). 2. We derive minimum mean squared error (MMSE) approximations to the ideal score function subject to locality, equivariance, and/or partially broken equivariance due to image boundaries. Remarkably, we find simple analytic solutions in all cases (Sec. 3.) 3. These solutions lead to a boundary-broken equivariant local score (ELS) machine, which constitutes a fully analytic, mechanistically interpretable theory that can transform noise into creative, structured images without the need for any explicit training process. (Sec. 3). 4. We theoretically characterize samples generated by the ELS machine and show how it achieves exponential creativity through locally consistent patch mosaics composed of different local training set image patches at different locations in each novel sample (Sec. 4). 5. We show our boundary-broken ELS machine is not only analytic and interpretable but also predictive: it can predict, on a case-by-case basis, the outputs of trained UNets and ResNets, achieving median theoryexperiment agreements of r 2 ∼ 0.94, 0.91, 0.90 on MNIST, FashionMNIST, and CIFAR10 for the best architecture on each dataset (Sec. 5). 6. Our comparison between theory and experiment reveals that trained diffusion models exhibit a coarse-tofine generation of spatial structure over time and use image boundaries to anchor image generation (Sec. 5). 7. Our theory reproduces the notorious behavior of diffusion models to generate spatially inconsistent images at fine spatial scales (e.g. incorrect numbers of limbs) and explains its origin in terms of excessive locality at late times in the reverse generative process. (Sec. 5). 8. We compare our purely local ELS machine theory to more powerful trained UNet architectures with nonlocal self-attention (SA) layers. Our local theory can still partially predict their non-local outputs (median r 2 of 0.75 on CIFAR10), but reveal an interesting role for attention in carving out semantically coherent objects from the ELS machine's local patch mosaics (Sec. 6).</p><p>Overall our work illuminates the mechanism of creativity in convolutional diffusion models and forms a foundation for studying more powerful attention-enabled counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The ideal score machine only memorizes</head><p>We first discuss why any diffusion model that learns the ideal score function on a finite dataset can only memorize.</p><p>The key idea behind diffusion models is to reverse a stochastic forward diffusion process that iteratively converts the data distribution π 0 (ϕ), where ϕ ∈ R N is any data point, into a sequence of distributions π t (ϕ) over time t, such that the final distribution π T (ϕ) at time T is an isotropic Gaussian N (0, I). The forward diffusion process usually shrinks the data points toward the origin while adding Gaussian noise, so that when conditioning on any individual data point φ ∼ π 0 , the conditional probability π t (ϕ|φ) becomes the Gaussian N (ϕ| √ ᾱt φ, (1 -ᾱt )I). The noise schedule ᾱt decreases from 1 at t = 0 to 0 at t = T so that the mean √ ᾱt φ of π t (ϕ|φ) shrinks over time, and its variance increases, until π t (ϕ|φ) ∼ N (0, I) for all initial points φ.</p><p>A simple time reversal of this forward process can be obtained by sampling ϕ T ∼ N (0, I) and then flowing it backwards in time from T to 0 under the deterministic flow</p><formula xml:id="formula_0">-φt = γ t (ϕ t + s t (ϕ t )),<label>(1)</label></formula><p>where s t (ϕ) ≡ ∇ ϕ log π t (ϕ) is the score function of the distribution π t (ϕ) under the forward process and γ t depends on the entire noise schedule ᾱt (see App. A for details). The flow in (1) induces a sequence of reverse distributions π R t (ϕ) that exactly reverse the forward process in the sense that π R t (ϕ) = π t (ϕ) for all t ∈ [0, T ]. Intuitively, this reversal occurs because, for any finite dataset D, π t (ϕ) is a mixture of Gaussians centered at shrunken data points,</p><formula xml:id="formula_1">π t (ϕ) = 1 |D| φ∈D N (ϕ| √ ᾱt φ, (1 -ᾱt )I),<label>(2)</label></formula><p>and the score s t (ϕ) points uphill on this mixture. Thus the second term in (1) flows ϕ t , as t decreases, towards shrunken data points, and the first term undoes the shrinking.</p><p>Motivated by this theory, score-based diffusion models attempt to sample the data distribution π 0 (ϕ) by forming an estimate ŝt (ϕ) of the score function s t (ϕ), and then plugging this estimate and initial noise ϕ T ∼ N (0, I) into the reverse flow in (1) to obtain a sample ϕ 0 . We consider what happens when the estimate matches the ideal score function so ŝt (ϕ) = s t (ϕ) on any finite dataset D. Then the score of the Gaussian mixture π t (ϕ) in (2), is (App. A):</p><formula xml:id="formula_2">s t (ϕ) = 1 1 -ᾱt φ∈D ( √ ᾱt φ -ϕ)W t (φ|ϕ),<label>(3)</label></formula><formula xml:id="formula_3">W t (φ|ϕ) = N (ϕ| √ ᾱt φ, (1 -ᾱt )I) φ ′ ∈D N (ϕ| √ ᾱt φ ′ , (1 -ᾱt )I) .<label>(4)</label></formula><p>When s t in (3) is inserted into (1), each term in (3) acts as a force that pulls the sample ϕ towards a shrunken data point √ ᾱt φ as t decreases, weighted by the posterior probability W t (φ|ϕ) that ϕ at time t would have originated from the datapoint φ at time 0 under the forward diffusion.</p><p>The combined reverse dynamics in (1), ( <ref type="formula" target="#formula_2">3</ref>) and ( <ref type="formula" target="#formula_3">4</ref>), which we call the ideal score machine, has an appealing Bayesian guessing game interpretation: the current sample ϕ at time t optimally guesses which data point φ it originated from in the forward process, thereby forming the posterior belief distribution W t (φ|ϕ), and then flows to each (shrunken version) of the data points, weighted by this belief.</p><p>Importantly, since the reverse flow provably reverses the forward diffusion, π R 0 equals the empirical data distribution π 0 , which is a sum of delta functions on the training set. Thus, the ideal score machine memorizes. The mechanism behind memorization can be explained by positive feedback instabilities in the reverse flow. In particular, the closer the sample ϕ is to a shrunken version of a data point φ, the higher the belief W t (φ|ϕ) that ϕ originated from φ, and the stronger the force term ( √ ᾱt φ-ϕ)W t (φ|ϕ) in (3) pulling ϕ even closer to the shrunken φ, which in turn raises the belief W t (φ|ϕ) at earlier t. This positive feedback between belief and force causes the posterior belief distribution W t (φ|ϕ) to rapidly concentrate onto a single data point φ, and so ϕ t flows to this same point φ under the reverse flow (Fig. <ref type="figure" target="#fig_1">2 a.</ref>).</p><p>Thus, any diffusion model that learns the true score s t on a finite dataset D must memorize the training data and cannot creatively generate new samples far from the training data. While we have explained this memorization phenomenon intuitively using the ideal score machine, it has been well established in prior work (e.g. <ref type="bibr" target="#b4">(Biroli et al., 2024)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Equivariant and local score machines</head><p>The failure of creativity in the ideal score machine means that it cannot be a good model of what realistic diffusion models do beyond the memorization regime. We therefore seek simple inductive biases that prevent learning the ideal score function s t in (3) on a finite dataset D. By identifying these inductive biases, we hope to obtain a new theory of what diffusion models do when they creatively generate new samples far from the training data.</p><p>The key observation is that many diffusion models use convolutional neural networks (CNNs) to form an estimate ŝ(ϕ) of the score function. Such CNNs have two prominent inductive biases. The first is translational equivariance due to weight sharing: translating the input image will correspondingly translate the CNN outputs. More generally, networks can be equivariant to arbitrary symmetry groups (e.g. <ref type="bibr" target="#b7">(Cohen &amp; Welling, 2016)</ref>, <ref type="bibr" target="#b17">(Hoogeboom et al., 2022)</ref>). The second is locality: since convolutional filters have narrow support, typical outputs of a CNN depend on their inputs only through a small receptive field of neighboring input pixels. We therefore seek an optimal estimate ŝ(ϕ) of the ideal score in (3) subject to locality and equivariance constraints.</p><p>We start with formal definitions of equivariance and locality. Let M t [ϕ] denote a model score function that takes an input image ϕ and outputs an estimated score ŝt (ϕ) = M t [ϕ].</p><p>Definition 3.1. A model M t is defined to be G-equivariant with respect to the action of a group G on data if for any</p><formula xml:id="formula_4">U ∈ G, M t satisfies M t [U ϕ] = U M t [ϕ].</formula><p>In our case of images, G is the spatial translation group in two dimensions, U ϕ is a translated image, and U M t [ϕ] is the translated score function. In other words, translating the input translates the outputs of an equivariant model in the same way. CNNs are translation equivariant if we impose periodic boundary conditions on the pixels, so that, for example, left translation of the leftmost pixels move them to the rightmost pixels (i.e. circular padding). However, the common practice of zero-padding images at their boundary breaks translation-equivariance; we extend our theory to this case in Sec. 3.4.</p><p>We next turn to locality. For image data, let x be a pixel location, ϕ(x) ∈ R C be the pixel value of image ϕ at location x (where C is the number of color channels) and let M t [ϕ](x) ∈ R C denote the model score function evaluated at pixel location x, which informs how the pixel value ϕ(x) should move under the reverse flow. Also, at each pixel location x, let Ω x denote a local neighborhood of x consisting of a subset of pixels near x, and let ϕ Ωx ∈ R |Ωx|×C be the restriction of pixel values of the entire image ϕ to the |Ω x | pixels in the neighborhood Ω x . We define locality as:</p><formula xml:id="formula_5">Definition 3.2. M t [ϕ] is defined to be Ω-local if, for all images ϕ and all pixel locations x, M t [ϕ](x) depends on ϕ only through ϕ Ωx , i.e. M t [ϕ](x) = M t [ϕ Ωx ](x).</formula><p>Thus if an Ω-local model M t [ϕ] is used in place of s(t) in (1), the instantaneous reverse flow of any pixel value ϕ(x) at location x and time t will not depend on pixel values at any locations outside the local neighborhood Ω x ; it depends only on the image in neighborhood Ω x . In particular, two pixels at distant locations x and y with non-overlapping neighborhoods Ω x and Ω y will make completely independent decisions as to which directions to reverse flow; the portion of the image ϕ Ωy in the neighborhood Ω y of y, cannot instanteously affect the flow direction of the pixel value ϕ(x), and vice versa.</p><p>Next, we consider the optimal minimum mean squared error (MMSE) approximation to the ideal score function s t (ϕ) in (3) under locality and/or equivariance constraints. We provide full derivations in App. B, but the final answers, which we state below, are simple and intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The equivariant score (ES) machine</head><p>We first impose equivariance without locality. The MMSE equivariant approximation to s(t) in ( <ref type="formula" target="#formula_2">3</ref>)-( <ref type="formula" target="#formula_3">4</ref>) is identical in form to the ideal score, except the dataset D is augmented to the orbit of D under the equivariance group G, which we denote by G(D). For example, in our case of images, G(D) corresponds to all possible spatial translations of all images in D. Explicitly, the MMSE equivariant score is given by (see App. B.3 for a proof)</p><formula xml:id="formula_6">M t [ϕ](x) = 1 1 -ᾱt φ∈G(D) ( √ ᾱt φ(x) -ϕ)W t (φ|ϕ) (5) W t (φ|ϕ) = N (ϕ| √ ᾱt φ, (1 -ᾱt )I) φ ′ ∈G(D) N (ϕ| √ ᾱt φ ′ , (1 -ᾱt )I) .<label>(6)</label></formula><p>Replacing the ideal score s(t) in ( <ref type="formula" target="#formula_0">1</ref>) with ( <ref type="formula">5</ref>) yields the equivariant score (ES) machine. While the ideal score machine memorizes the training data (see Sec. 2), the ES machine on images achieves only limited creativity: it can only generate any translate of any training image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The local score (LS) machine</head><p>We next impose locality without equivariance. The MMSE Ω-local approximation to s(t) in ( <ref type="formula" target="#formula_2">3</ref>)-( <ref type="formula" target="#formula_3">4</ref>) is given by</p><formula xml:id="formula_7">M t [ϕ](x) = φ∈D ( √ ᾱt φ(x) -ϕ(x)) 1 -ᾱt W t (φ Ωx |ϕ Ωx ),<label>(7)</label></formula><formula xml:id="formula_8">W t (φ Ωx |ϕ Ωx ) = N (ϕ Ωx | √ ᾱt φ Ωx , (1 -ᾱt )I) φ ′ ∈D N (ϕ Ωx | √ α t φ ′ Ωx , (1 -ᾱt )I) . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Each term in the local M t [ϕ](x) in ( <ref type="formula" target="#formula_7">7</ref>) is identical to each term in s(t) in (3), yielding a force pulling the pixel value ϕ(x) towards a shrunken training set pixel value √ ᾱt φ(x) as before, except for the important change that the global posterior belief W t (φ|ϕ) in ( <ref type="formula" target="#formula_2">3</ref>)-( <ref type="formula" target="#formula_3">4</ref>), that is the same for all pixels x, is now replaced with a local x-dependent belief W t (φ Ωx |ϕ Ωx ) in ( <ref type="formula" target="#formula_7">7</ref>)-( <ref type="formula" target="#formula_8">8</ref>). W t (φ Ωx |ϕ Ωx ) is the posterior probability that a sample image ϕ under the forward process at time t originated from a training image φ at time 0, conditioned on the only information the model M t [ϕ](x) can depend on, namely the restriction ϕ Ωx of the image ϕ to the local neighborhood Ω x at location x. The closer the local image patch ϕ Ωx is to the co-located training image patch φ Ωx , the larger the posterior W t (φ Ωx |ϕ Ωx ) in (8).</p><p>Replacing the ideal score s(t) in ( <ref type="formula" target="#formula_0">1</ref>) with ( <ref type="formula" target="#formula_7">7</ref>) yields the local score (LS) machine. The LS machine can achieve significant combinatorial creativity by allowing local image neighborhoods ϕ Ωx and ϕ Ω x ′ of different pixels x and x ′ to reverse flow close to training image patches φ Ωx and φ ′ Ω x ′ from different training images φ and φ ′ (Fig. <ref type="figure" target="#fig_1">2b</ref>). Indeed the same positive feedback between belief and force that holds for the IS machine at a global level (Sec. 2), also holds for the LS machine at a local level, causing the posterior beliefs W t (φ|ϕ Ωx ) of all pixels x to concentrate on a unique training image, but this training image could be different for different far away pixels. This flow decoupling of local image patches in ϕ t empowers exponential creativity.</p><p>However, an important limitation remains in the LS machine: a local image patch ϕ Ωx at pixel location x must reverse flow close to some local training image patch φ Ωx drawn from the same location x; it cannot flow to a training image patch φ Ω x ′ drawn from a different location x ′ . We next see that adding equivariance removes this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The equivariant local score (ELS) machine</head><p>Further constraining the LS machine with equivariance leads to the ELS machine in which any local image patch at any pixel location x can now flow towards any local training set image patch drawn from any location x ′ not necessarily equal to x, as in the LS machine. This is the local analog of how the IS machine can only generate training set images, but the equivariance constrained ES machine can generate training set images globally translated to any other location.</p><p>To formally express this result, assume all local neighborhoods Ω x for different x have the same shape Ω. For concreteness, one can think of Ω as a P × P square patch of pixels for P odd, with Ω x centered at location x. Then let P Ω (D) denote the set of all possible Ω shaped local training image patches drawn from any training image centered at any location. An element φ ∈ P Ω (D) now lives in R P ×P ×C and denotes the pixel values of some local Ωshaped training image patch centered at some location. Now the optimal MMSE approximation to the ideal score in (3), under both equivariance and locality constraints is (App. B):</p><formula xml:id="formula_10">M t [ϕ](x) = φ∈PΩ(D) ( √ ᾱt φ(0) -ϕ(x)) 1 -ᾱt W t (φ|ϕ, x)<label>(9)</label></formula><formula xml:id="formula_11">W t (φ|ϕ, x) = N (ϕ Ωx | √ ᾱt φ, (1 -ᾱt )I) φ ′ ∈PΩ(D) N (ϕ Ωx | √ ᾱt φ ′ , (1 -ᾱt )I) .<label>(10)</label></formula><p>We note that ( <ref type="formula" target="#formula_10">9</ref>)-( <ref type="formula" target="#formula_11">10</ref>) for the ELS machine is identical to ( <ref type="formula" target="#formula_7">7</ref>)-( <ref type="formula" target="#formula_8">8</ref>) for the LS machine except that: (1) the sum over local training set patches in ( <ref type="formula" target="#formula_10">9</ref>)-( <ref type="formula" target="#formula_11">10</ref>) in determining the flow M t [ϕ](x) for pixel ϕ(x) is no longer restricted to training patches centered at the same location as x; and (2) each pixel x must now track a larger posterior belief state W t (φ|ϕ, x) in (10) about which local training set patch at any location x ′ was the origin of ϕ Ωx , as opposed to the smaller belief state W t (φ Ωx |ϕ Ωx ) in (8) about which local training set patch at the same location x was the origin of ϕ Ωx . In essence, in the Bayesian guessing game interpretation, equivariance removes each pixel's knowledge of its location x, so to guess the origin of its local image patch ϕ Ωx , it must guess both the training image and the location in the training image that it came from under the forward process. This guess then informs the reverse flow. Taken together, the ELS machine can creatively generate exponentially many novel images by mixing and matching local training set patches from any location and placing elements of them at any location in the generated image.</p><p>We call this a patch mosaic model of creativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Breaking equivariance through boundaries</head><p>Due to the common practice of zero padding images at boundaries, CNNs actually break exact translational equivariance. We can modify our ELS machine to handle this broken equivariance (see App. B.2 for details). The key idea is that breaking translation equivariance restores to each pixel some knowledge of its location within the image. For example, if the local image patch ϕ Ωx around pixel location x contains many 0 values, then the pixel can use these to infer its location with respect to the boundary, and use this knowledge in the Bayesian guessing game that determines the reverse flow. In essence, with additional conditioning about its relation to the boundary, ϕ Ωx should only flow to training image patches that are consistent with the observed amount and location of zero-padding. For example, interior, edge, and corner image patches only flow to interior, edge and corner training image patches with the same boundary overlap (Fig. <ref type="figure" target="#fig_7">7</ref>). This is a partial case of complete equivariance breaking in the LS machine, in which pixels know their exact location x, and the local image patch ϕ Ωx only flows to training image patches at the same location x (Fig. <ref type="figure" target="#fig_1">2b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A theory of creativity after convergence</head><p>It is clear that the reverse flow from Gaussian noise ϕ T to final sample ϕ 0 in the ideal score machine converges to a single training set image. But what do the LS, ELS or boundary broken ELS machines converge to at the end of the reverse process if they creatively generate novel samples far from the training data? We answer this question by proving a theorem that characterizes the converged samples ϕ = ϕ 0 at the end of the reverse process (App. B.4).</p><p>Theorem 4.1. For the LS, ELS, and boundary broken ELS machines, assuming lim t→0 ϕ t and lim t→0 ∂ t ϕ t exist, then for every pixel x, ϕ 0 (x) = φ(0) for the unique patch φ ∈</p><formula xml:id="formula_12">P x Ω (D) for which ϕ Ωx is closer in L 2 distance (in R |Ωx|×C ) than other local training set patch φ ′ ∈ P x Ω (D).</formula><p>Intuitively, samples generated from these machines are locally consistent in the sense that they obeying 3 local conditions: ( <ref type="formula" target="#formula_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The simplest example of patch mosaic creativity</head><p>As the simplest possible example illustrating the locally consistent patch mosaic model of creativity for the LS and ELS machines, consider a training set of only two images: an all black and an all white image (Fig. <ref type="figure" target="#fig_2">3a</ref>). A highly expressive diffusion model trained only on these two images would only generate these two images. However, an LS or ELS machine with local 3 × 3 neighborhoods generates exponentially many new samples that are locally consistent patch mosaics (Fig. <ref type="figure" target="#fig_2">3b</ref>): every pixel is either black or white, indicating it is assigned to either an all black or all white 3 × 3 local training set patch. And any 3 × 3 local patch of a generated sample with a central black (white) pixel is closer to the all black (white) training set patch than the other training set patch. Thus local consistency in this special case reduces to the simple condition that the majority color of any 3 × 3 locally generated patch must equal the color of its central pixel. The reader can check that this local consistency holds (with appropriate circular wraparound) at every pixel in Fig. <ref type="figure" target="#fig_2">3b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tests of the theory on trained models</head><p>We next test our theory on two CNN-based architectures, a standard UNet <ref type="bibr" target="#b33">(Ronneberger et al., 2015)</ref> and a ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref> trained on 3 datasets, MNIST, FashionMNIST, and CIFAR10 (see App. C.1 for details of architectures and training). We restrict our attention to these simple datasets because our theory is for CNN-based diffusion models only, and more complex diffusion models with attention and latent spaces are required to model more complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Coarse-to-fine time dependent spatial locality scales</head><p>To compare our theory of ELS and LS machines with experiments, we must first choose a locality scale for the size of the P ×P local patch. We therefore measure it in the trained UNet and ResNet and find, importantly, that it changes from large to small scales as time passes from early (large t) to late (small t) in the reverse flow (Fig. <ref type="figure" target="#fig_4">4a</ref>). We therefore promote the spatial size of the P × P locality window in our ELS and LS machines to a dynamic variable which we calibrate to the UNet and ResNet (Fig. <ref type="figure" target="#fig_4">4bc</ref>). See App. C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Theory predicts trained outputs case-by-case</head><p>We first compare the outputs of the scale-calibrated boundary broken-ELS machine to the outputs of the ResNet and the UNet on a case-by-case basis for the same initial noise samples ϕ T to both the theory and the ResNet or UNet, and we find an excellent match (Fig. <ref type="figure">5ab</ref>). Indeed we find a remarkable and uniform quantitative agreement between the CNN outputs and ELS machine outputs. For ResNets, we find median r 2 values between theory and experiment of 0.94 on MNIST, 0.90 on FashionMNIST, and 0.90 on CIFAR10. For UNets, we find median r 2 values of 0.84 on MNIST, 0.91 on FashionMNIST, and 0.82 on CIFAR10 (see Fig. <ref type="figure" target="#fig_8">8</ref> for the full distribution of r 2 values).</p><p>To our knowledge, this is the first time an analytic theory has explained the creative outputs of a trained neural network generative model to this level of accuracy. Importantly, the ELS machine explains all trained outputs far better than the IS machine (Fig. <ref type="figure" target="#fig_8">8</ref> and Table <ref type="table">1</ref>). See App. D, Fig. <ref type="figure" target="#fig_11">11</ref> to Fig. <ref type="figure" target="#fig_18">18</ref> for many more successful case-by-case theoryexperiment comparisons for the 2 nets and 3 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Boundary driven anchoring of diffusion models</head><p>We also trained circularly padded ResNets on MNIST and CIFAR10, and found a good match between the nonboundary broken ELS machine and experiment (Figs. 9, 17 and 18). Interestingly, in both theory and experiment for MNIST, circular padding yields more texture-like outputs and less localized digit-like outputs, indicating the fundamental importance of boundaries in anchoring diffusion models, for MNIST at least (compare Fig. <ref type="figure" target="#fig_17">17</ref> and Fig. <ref type="figure" target="#fig_11">11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Spatial inconsistencies from excess late-time locality</head><p>Diffusion models notoriously generate spatially inconsistent images at fine spatial scales, e.g. incorrect numbers of fingers and limbs. Indeed, these inconsistencies are considered a tell-tale sign of AI-generated images <ref type="bibr" target="#b3">(Bird &amp; Lotfi, 2024;</ref><ref type="bibr" target="#b36">Shen et al., 2024;</ref><ref type="bibr" target="#b24">Lin et al., 2024)</ref>. Our trained models on FashionMNIST also generate such inconsistencies, e.g. pants with too many or too few legs, shoes with more than one toe, and shirts with an incorrect number of arms. Remarkably, our theory, since it matches trained model outputs on a case by case basis, also reproduces these inconsistencies (Fig. <ref type="figure">5c</ref>). Since our theory is completely mechanistically interpretable, it provides a clear explanation for the origin of these inconsistencies in terms of excessive locality at late stages of the reverse flow. The late-time (t &lt; 0.3) locality for all models is less than about 5 pixels (Fig. <ref type="figure" target="#fig_4">4b</ref>).</p><p>When the locality scale is of this small order, different parts of the image more than a few pixels away must decide whether to develop into e.g. an arm or a pant leg without knowing the total number of limbs in the image; this process frequently results in incorrect numbers of total limbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">The relation between theory and attention</head><p>While the local theory explains the outputs of CNN-based diffusion models on a case by case basis with high accuracy, many diffusion models also include highly non-local selfattention (SA) layers. For example <ref type="bibr" target="#b14">(Ho et al., 2020)</ref>) added SA layers to a UNet (which we call a UNet-SA architecture).</p><p>The non-locality of SA strongly violates the assumptions of our local theory. This violation raises an important question: do the predictions of our local theory bear any resemblance at all to the non-local outputs of trained UNet+SA models?</p><p>To address this question, we compare our existing ELS machine theory with the outputs of a publicly available UNet+SA model pretrained on CIFAR10. <ref type="bibr" target="#b35">(Sehwag, 2024)</ref>. Strikingly, our ELS model, with no modification whatsoever, predicts the UNet+SA outputs on a case-by-case basis with a median of r 2 ∼ 0.75 on 100 sample images. This is substantially higher than the median r 2 ∼ 0.47 of an IS machine baseline on the same images (see Fig. <ref type="figure" target="#fig_10">10</ref> for the entire distribution of r 2 values).</p><p>Qualitatively, the outputs of the UNet+SA model fall into three rough classes in which the UNet+SA produces: (1) a semantically incoherent image which nevertheless strongly resemblances the prediction of the ELS machine (Fig. <ref type="figure" target="#fig_9">19a</ref>);</p><p>(2) a semantically coherent image which has some quantitative correlation with, but little qualitative resemblance to, the ELS machine prediction (Fig. <ref type="figure" target="#fig_9">19b</ref>); and (3) a semantically coherent image that also has a strong resemblance to the less semantically coherent ELS machine outputs (Fig. <ref type="figure" target="#fig_6">6</ref>).</p><p>This third class is the largest of the three and perhaps the  <ref type="figure" target="#fig_8">8</ref> and Table <ref type="table">1</ref> for quantitative r 2 values indicating high match between theory and experiment. (c) Trained CNN diffusion models (right) produce well-known spatial inconsistencies (e.g. 3 legged pants (row 1,4), 3 armed tops (row 3,6), bifurcated shoes (row 2,5)). Remarkably, the ELS theory (left) predicts this behavior and mechanistically explains it through excessive spatial locality at late times in the reverse flow. most interesting. Qualitatively, the UNet+SA appears to carve a semantically coherent object out of the patch mosaic of the ELS machine (compare top and bottom rows of Fig. <ref type="figure" target="#fig_6">6</ref>). For example, the UNet+SA often cuts out a foreground object from the ELS patch mosaic, while smoothing the background and accentuating it from the foreground object. Fig. <ref type="figure" target="#fig_19">20</ref> shows a large set of comparisons between the ELS machine and UNet+SA outputs. While these results show that the ELS theory bears in many cases both quantitative and qualitative resemblance to the UNet+SA outputs, a full quantitative theory of the role of attention in the creativity of diffusion models remains for future investigation. However, the correspondences in Fig. <ref type="figure" target="#fig_6">6</ref>, Fig. <ref type="figure" target="#fig_9">19a</ref>, and Fig. <ref type="figure" target="#fig_19">20</ref> and the ELS correlations (y-axis) in Fig. <ref type="figure" target="#fig_10">10</ref>, suggest the ELS theory provides an important foundation for this endeavor.</p><p>Discussion. Developing a mechanistic understanding of how generative models convert their training data into new creative outputs far from their training data is an important goal in the field of neural network interpretability. We have developed such an understanding for convolutional diffusion models of images that accurately predicts individual outputs on fixed random inputs in terms of the training data, for standard architectures (ResNets and UNets), standard datasets (MNIST, FashionMNIST, CIFAR10), and standard loss functions (score-matching). Moreover, our mechanistically interpretable theory of diffusion models, namely the boundary broken ELS machine, is derived not from intensive and highly detailed analysis of the inner workings of trained networks (modulo matching spatial scales), as in most mechanistic interpretability works, but rather from a first principles approach stemming from analytic solutions for the optimal score subject to only 2 posited inductive biases: locality and equivariance. The strong quantitative agreement between theory and experiment on a case-by-case basis suggests that these two inductive biases are sufficient to explain the creativity of convolution-only diffusion models. We hope this work provides a foundation for understanding the creativity of more powerful attention-enabled diffusion models trained on more complex datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mathematical Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Notation conventions</head><p>In what follows, we use the following notation:</p><p>• D will represent the training set.</p><p>• φ ∈ R N will represent an example from the training set. For images of size L pixels by L pixels by C channels, we have N = L × L × C.</p><p>• ϕ will represent any arbitrary image (or other data) that we are plugging into the score function/diffusion model.</p><p>• x represents a pixel location in an image.</p><p>• For image data, ϕ(x) and φ(x) will represent the pixel values of the images ϕ and φ at pixel location x; both are elements of R C .</p><p>• M [ϕ] : R N → R N represents a model that takes in an image ϕ and produces a new image (e.g. an estimate of the score function). We will denote by M [ϕ](x) ∈ R C the value of the outputs of this model, given an input ϕ, at the pixel location x.</p><p>• ϕ Ωx and φ Ωx will represent the restriction of images ϕ and φ to a neighborhood Ω x around a pixel x. We usually take Ω x to be a square patch of size P × P , with P odd, containing pixel x at the center. In this case, ϕ Ωx and φ Ωx are vectors in R P ×P ×C . However, the theoretical framework supports arbitrary assignments from x → Ω x .</p><p>• For a square image patch φ with an odd-dimension side length, the value φ(0) ∈ R C indicates the pixel at the center of the patch.</p><p>• P Ω (D) will denote the set of all Ω-shaped patches drawn from elements of D.</p><p>• N (x|µ, Σ) represents the PDF of the normal distribution with mean µ and covariance Σ. We also use the short-hand N (µ, Σ) when we do not need to refer to the name of a specific random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Stochastic differential equations (SDEs) and Probability Flow</head><p>In probabilistic modeling, we are often confronted with the problem of sampling from a data distribution whose exact form we do not have access to, or whose form makes direct sampling difficult. Diffusion models are an approach to sampling from such distributions by learning a time-inhomogenous differential equation that transports samples from a simple Gaussian distribution to the more complex distribution of interest.</p><p>More formally, consider a time-dependent (Itô) stochastic differential equation, given as follows:</p><formula xml:id="formula_13">dϕ t = f t (ϕ t ) dt + g t dW t .<label>(11)</label></formula><p>Here W t is a standard Wiener process and dW t is its differential. We call this stochastic process the 'forward' process.</p><p>It starts from the data distribution π 0 (ϕ) and induces a flow on probability distributions π t (ϕ) for t ≥ 0 described by associated Fokker-Planck equation:</p><formula xml:id="formula_14">∂π t (ϕ) ∂t = -∇ • (f t (ϕ)π t (ϕ)) + 1 2 ∇ 2 (g 2 t π t (ϕ)). (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>We will imagine that our forward process is constructed so that as t → ∞ (or as t → T for some finite time T ), π t converges to some tractable π ∞ , typically a Gaussian with finite variance.</p><p>The idea underpinning diffusion models (or, more technically, DDIMs, the deterministic variant of diffusion models considered for the most part in this paper) is to look for a deterministic, time-dependent vector field v t (ϕ) that induces the same flow on distributions as (12). Then one can simply reverse this flow to sample from π 0 (t) by first sampling from the simple distribution ϕ T ∼ π T , then evolving the sample deterministically backwards in time from t = T to t = 0 under the ODE</p><formula xml:id="formula_16">dϕ t dt = v t (ϕ t ). (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>This ODE induces a flow on probability distributions π t (ϕ) described by the advection equation</p><formula xml:id="formula_18">∂π t ∂t = -∇ • [v t (ϕ)π t (ϕ)].<label>(14)</label></formula><p>We want this advection process above to induce the same flow on distributions as the original flow ( <ref type="formula" target="#formula_14">12</ref>), when run in reverse starting, from the simple final distribution π T . (This setup is closely related to 'flow matching' models: see <ref type="bibr" target="#b25">(Lipman et al., 2022)</ref> for a review). Interestingly, v t (ϕ) can be easily identified by rewriting the flow in ( <ref type="formula" target="#formula_14">12</ref>) as</p><formula xml:id="formula_19">∂π t (ϕ) ∂t = -∇ • ([f t (ϕ) - 1 2 g 2 t ∇ log π t (ϕ)]π t (ϕ)). (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>By matching ( <ref type="formula" target="#formula_18">14</ref>) and ( <ref type="formula" target="#formula_19">15</ref>), we find</p><formula xml:id="formula_21">v t (ϕ) = f t (ϕ) - 1 2 g 2 t ∇ log π t (ϕ). (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>This vector field is sometimes known as the 'probability flow.' The function</p><formula xml:id="formula_23">s t (ϕ) = ∇ log π t (ϕ)<label>(17)</label></formula><p>is known as the score function, and contains all of the complicated dependency on the initial distribution π 0 (ϕ) that we would like to capture in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Diffusion models</head><p>The most common choice of forward process ( <ref type="formula" target="#formula_13">11</ref>) is an inhomogenous Ornstein-Uhlenbeck (OU) process process of the following form:</p><formula xml:id="formula_24">dϕ t = -γ t ϕ t + 2γ t dW t (18)</formula><p>for which the probability flow is given by</p><formula xml:id="formula_25">v t (ϕ) = -γ t (ϕ + ∇ log π t (ϕ)). (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>The reason for this choice is that the finite-time marginals π t for this distribution can be sampled from tractably. We can generate samples ϕ t ∼ π t by computing the following linear linear combination:</p><formula xml:id="formula_27">ϕ t = √ ᾱt ϕ 0 + √ 1 -ᾱt η t (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>with ϕ 0 ∼ π 0 a sample from the target distribution and η t ∼ N (0, I) a vector of isotropic Gaussian noise. The values of ᾱt depend on the choice of γ t via the following formula:</p><formula xml:id="formula_29">ᾱt = exp -2 t 0 γ t dt . (<label>21</label></formula><formula xml:id="formula_30">)</formula><p>In practice, the values ᾱt are typically chosen first and γ t is then specified implicitly by this choice. The choice of ᾱt is known as the 'noise schedule' for a diffusion model; typically, we choose ᾱ0 = 1 (so that t = 0 corresponds to uncorrupted sample) and ᾱT = 0 for some large but finite value of T (so that the entire reverse process can take place in finite time). At a distributional level, the solution of ( <ref type="formula" target="#formula_14">12</ref>) for this process is given by</p><formula xml:id="formula_31">π t (ϕ) = π 0 (ϕ 0 )N (ϕ| √ ᾱt ϕ 0 , (1 -ᾱt )I) dϕ 0 . (<label>22</label></formula><formula xml:id="formula_32">)</formula><p>The score function for π t can then be obtained analytically in terms of π 0 :</p><formula xml:id="formula_33">s t (ϕ) = - 1 1 -ᾱt π 0 (ϕ 0 )N (ϕ| √ ᾱt ϕ 0 , (1 -ᾱt )I) π t (ϕ) (ϕ t - √ ᾱt ϕ 0 ) dϕ 0 = - 1 1 -ᾱt P(ϕ 0 |ϕ t = ϕ)(ϕ t - √ ᾱt ϕ 0 ) dϕ 0 . (<label>23</label></formula><formula xml:id="formula_34">)</formula><p>There is an extremely convenient fact about this particular score function that we can take advantage of in order to learn it from data. Given a particular sample ϕ t generated by the forward noising process, the score function is proportional to the conditional expectation of the added noise η t from (20), given ϕ t :</p><formula xml:id="formula_35">s t (ϕ) = - 1 √ 1 -ᾱt ⟨η t |ϕ t = ϕ⟩. (<label>24</label></formula><formula xml:id="formula_36">)</formula><p>This result is known as Tweedie's theorem. A standard result in statistics is that the conditional expectation ⟨η t |ϕ t ⟩ is the functional optimum of the following loss function:</p><formula xml:id="formula_37">L t (f ) = E ϕ0∼π0,ηt∼N (0,I) [∥f (ϕ t (ϕ 0 , η t )) -η t ∥ 2 ]<label>(25)</label></formula><p>for ϕ t defined in (20); the following slightly rescaled loss can be used if score-matching is preferred:</p><formula xml:id="formula_38">L t (f ) = E ϕ0∼π0,ηt∼N (0,I) [ f (ϕ t (ϕ 0 , η t )) + (1 -ᾱt ) -1/2 η t 2 ].<label>(26)</label></formula><p>In practice, we model the score using a single neural network f θ (x, t) for all times t ∈ [0, T ], using the following objective:</p><formula xml:id="formula_39">L(θ) = E t∼U (0,T ),ϕ0∼π0,ηt∼N (0,I) [∥f θ (ϕ t (ϕ 0 , η t ), t) -η t ∥ 2 ].<label>(27)</label></formula><p>A.4. The empirical score function</p><p>In practice, we never have direct access to the data distribution π 0 that we are attempting to sample from; we only have access to the discrete empirical prior defined by a particular training set D:</p><formula xml:id="formula_40">π 0 (ϕ) = 1 |D| φ∈D δ(ϕ -φ).<label>(28)</label></formula><p>At finite time t, the empirical distribution of noised training examples is simply a mixture of Gaussians centered at the (rescaled) training data points:</p><formula xml:id="formula_41">π t (ϕ) = 1 |D| φ∈D N (ϕ| √ ᾱt φ, (1 -ᾱt )I).<label>(29)</label></formula><p>The score function (23) for this distribution is then simply given by</p><formula xml:id="formula_42">s t (ϕ) = - 1 1 -ᾱt φ∈D (ϕ - √ ᾱt φ)W t (φ|ϕ),<label>(30)</label></formula><formula xml:id="formula_43">W t (φ|ϕ) = N (ϕ| √ ᾱt φ, (1 -ᾱt )I) φ ′ ∈D N (ϕ| √ ᾱt φ ′ , (1 -ᾱt )I) .<label>(31)</label></formula><p>Intuitively, this corresponds to computing the conditional average over the added noise, by averaging the proposed noise vectors η t ∝ (ϕ -√ ᾱt φ) between our observed example ϕ and each training example φ, weighted by the probability W (φ|ϕ) of φ being the training example that ϕ originated from. This probability is in turn computed essentially by Bayes theorem: the probability of starting from a training example φ, given the observed ϕ, is given by the likelihood of generating the noise needed to go from φ to ϕ, divided by the likelihood of going from φ ′ to ϕ for all possible training examples φ ′ . Appealingly, the weights W (φ|ϕ) are given by computing a simple soft-max over a simple quadratic loss function -1 2(1-ᾱt) ∥ϕ -√ ᾱt φ∥ 2 for every point in the training set.</p><p>It should be emphasized at this point that the ideal score function is not representative of real diffusion models. Primarily: it always memorizes the training data. More importantly in practice, this memorization property becomes manifest very early in the reverse process for high dimensional data, due to the typically large separation between training points in Euclidean space. This is a manifestation of the curse of dimensionality-it would require an amount of data exponential in the dimension to provide sufficiently good coverage of the underlying space for the ideal empirical score function to well-approximate the true ideal score function over all inputs over all times.</p><p>The failure of the ideal score function as a model for realistic diffusion models suggests that we should try to understand the particular manner in which they fail to optimally solve the task that they are trained on. In particular, we are motivated to look for the implicit and explicit biases and constraints that prevent these models from learning the ideal score function, and then understand what they do instead under these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Formalism</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Optimal local translationally equivariant score matching</head><p>Fully translationally equivariant local models M t can be written in the following way:</p><formula xml:id="formula_44">M t [ϕ](x) = f [ϕ Ωx ],<label>(32)</label></formula><p>where ϕ Ωx is the restriction of ϕ to the neighborhood Ω x around pixel x. In this section, we will use circular boundary conditions, so that if x is near an image border, the neighborhood ϕ Ωx includes the pixels on the opposite side of the image near the corresponding border (we will revisit this in the next section). This functional form reflects the locality constraint by making manifest that the output at a pixel location x depends only on the patch ϕ Ωx around it. Equivariance is reflected in the fact that the output of the model at every point x is determined by the same function of the input patch. f should be thought of as a function mapping R C×|Ω| → R C , where C is the number of channels in the image and |Ω| is the number of pixels in the local patch Ω. The problem of identifying the optimal local/equivariant model can thus be framed as finding the f that minimizes the score matching objective:</p><formula xml:id="formula_45">L = x E ϕ∼πt [∥f [ϕ Ωx ] -s t [ϕ](x)∥ 2 ]<label>(33)</label></formula><p>Writing this out concretely gives</p><formula xml:id="formula_46">L = π t (ϕ) x ∥f (ϕ Ωx ) -s t [ϕ](x)∥ 2 dϕ.<label>(34)</label></formula><p>To find the functional optimum, we vary the objective with respect to f (Φ), with Φ any arbitrary patch, and set this variation to zero. This yields the condition</p><formula xml:id="formula_47">0 = x π t (ϕ)(f (ϕ Ωx ) -s t [ϕ](x))δ(ϕ Ωx -Φ) dϕ.<label>(35)</label></formula><p>We can rearrange this into the following form:</p><formula xml:id="formula_48">f (Φ) x π t (ϕ Ωx = Φ) = x δ(ϕ Ωx -Φ) π t (ϕ)s t [ϕ](x) dϕ = x δ(ϕ Ωx -Φ)∇ ϕ(x) π t (ϕ) dϕ = x ∇ Φ(0) π t (ϕ Ωx = Φ)</formula><p>Here Φ(0) ∈ R C is the pixel value in the center of the patch Φ. π t (ϕ Ωx = Φ) indicates the marginal probability under the distribution π t that the patch ϕ Ωx equals the target patch Φ. The distribution x π t (ϕ Ωx = Φ) is then proportional to the marginal distribution that a randomly-selected Ω-shaped-patch in the image ϕ equals Φ. Dividing through by this marginal, we obtain</p><formula xml:id="formula_49">f (Φ) = ∇ Φ(0) log x π t (ϕ Ωx = Φ)<label>(36)</label></formula><p>i.e. we find that f (Φ) is simply the score function of the modified marginal density x π t (ϕ Ωx = Φ). Since π t (ϕ) is a mixture of Gaussians, the marginal π t (ϕ Ωx = Φ) can be obtained simply and is given by</p><formula xml:id="formula_50">π t (ϕ Ωx = Φ) = φ∈D N (Φ| √ ᾱt φ Ωx , (1 -ᾱt )I).<label>(37)</label></formula><p>Summing over x gives us where P Ω (D) is the set of all Ω patches in the training set D. Finally, taking the derivative with respect to Φ(0) and substituting ϕ Ωx for Φ gives us the final answer for f [ϕ Ωx ], which, when inserted into (32), yields the final answer for M t :</p><formula xml:id="formula_51">x π t (ϕ Ωx = Φ) = φ∈PΩ(D) N (Φ| √ ᾱt φ, (1 -ᾱt )I)<label>(38)</label></formula><formula xml:id="formula_52">M t [ϕ](x) = - 1 1 -ᾱt φ∈PΩ(D) (ϕ(x) - √ ᾱt φ(0))W (φ|ϕ Ωx ) (39) W (φ|ϕ Ωx ) = N (ϕ Ωx | √ ᾱt φ, (1 -ᾱt )I) φ ′ ∈PΩ(D) N (ϕ Ωx | √ ᾱt φ ′ , (1 -ᾱt )I) . (<label>40</label></formula><formula xml:id="formula_53">)</formula><p>We term the reverse diffusion model parameterized by M t i (39) the Equivariant Local Score (ELS) Machine.</p><p>This result has a simple intuitive interpretation. Firstly, it should be noted that the form of the resulting approximation to the score function strongly resembles the form of the true score function (30). In that case, the score function computation could be framed as guessing the added noise by finding the necessary added noise for each possible training set element, computing the likelihood of generating that noise under a Gaussian noise model, and then averaging the possible noises over the entire training set weighted by the Bayesian posterior over each possible noised example.</p><p>The ELS machine (39) can be interpreted similarly. However, a very important distinction is that the Bayes weights are pixelwise-decoupled. Under the exact computation of the score function, the Bayes weights are computed based on all available information in the image, and shared across every pixel; under the locality-constrained approximation, each pixel independently computes a separate set of Bayes weights for each training set element, based on its local receptive field. This decoupling of the belief states of different pixels means that under the reverse denoising process parameterized by (39), different pixels will be drawn towards different elements of the training set. At scales below the locality scale the final denoised images should (roughly) resemble part of a training set image; however, at larger scales, the resulting images will not resemble any particular training set image, but rather a kind of patchwork quilt/mosaic of randomly combined training set images. We make this result more precise in (B.4).</p><p>The role played by equivariance can likewise be interpreted very simply as removing each pixel's ability to locate itself within the image. Position is therefore promoted to a latent variable that must be integrated over, in addition to the training set element itself. This results in needing to compute a Bayes weight not only for each correspondingly-located patch in the training set, but every possible patch in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Adding borders</head><p>There is an ambiguity about the behavior of a convolutional neural network for pixels near enough to the boundary of an image such that the network's receptive field extends past that boundary. One option in that situation is to enforce circular boundary conditions, so that the convolution operation 'wraps around' to the other side upon encountering the boundary. This approach is not typically used in practice; more commonly, 'zero padding' is introduced, wherein pixels outside of the image are treated as zeros for the purposes of the convolution operation.</p><p>In the presence of zero-padding, the results given above concerning the optimal local equivariant approximation to the score are nearly identical; in fact, the fundamental identity (32) still holds. However, we must modify the interpretation of the visible patch ϕ Ωx for a pixel x near the border. Instead of considering the patch to include 'wrapped around' portions of the image, we instead simply extend it with zeros in all locations where it extends past the border.</p><p>When the ELS machine takes as input the patch ϕ Ωx , it computes the conditional probability that it corresponds to a noising of each particular patch in the training set. Formally, getting an exactly zero value at any pixel location occurs with probability zero. Thus, observing a patch ϕ Ωx with zero-padding indicates with probability 1 that the patch is a corruption of a training set patch that came from a location inside the image consistent with the observed border information. We are thus able to write the ELS machine in the presence of a zero-padded boundary as</p><formula xml:id="formula_54">M t [ϕ](x) = - 1 1 -ᾱt φ∈P x Ω (D) (ϕ(x) - √ ᾱt φ(0)) N (ϕ Ωx | √ ᾱt φ, (1 -ᾱt )I) φ ′ ∈P x Ω (D) N (ϕ Ωx | √ ᾱt φ ′ , (1 -ᾱt )I) .<label>(41)</label></formula><p>The only modification to the ELS machine ( <ref type="formula">39</ref>) is that we have replaced the set of all patches P Ω (D) in the sum with the x-dependent patch dictionary P x Ω (D), corresponding to the collection of patches consistent with the border data at location x. These collections are illustrated in figure <ref type="figure" target="#fig_7">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Optimal equivariant score matching for a general symmetry group</head><p>In many diffusion model applications outside of computer vision, equivariance under more general symmetry groups is built in to the architecture of the backbone model. For instance, molecular diffusion models are sometimes made equivariant under E(3), the group of isometries on Euclidean space <ref type="bibr" target="#b17">(Hoogeboom et al., 2022)</ref>. Diffusion transformers <ref type="bibr" target="#b29">(Peebles &amp; Xie, 2023)</ref> are also naturally equivariant under the group of sequence permutations, although this equivariance is broken in a controlled way by the inclusion of positional embeddings. We are thus motivated to study the question of optimality under the constraint of equivariance under a general group of symmetries G, which we define as follows:</p><p>Definition B.1. Let G be a particular group of transformations acting on data ϕ. We say that a model M t is G-equivariant if, for any U ∈ G, our model satisfies</p><formula xml:id="formula_55">M t [U ϕ] = U M t [ϕ].<label>(42)</label></formula><p>The result is given here:</p><p>Theorem B.2. The optimal G-equivariant approximation to the empirical score function (3) under the score matching objective ( <ref type="formula" target="#formula_38">26</ref>) is given by the empirical score function for the dataset G(D) consisting of the orbit of the dataset D under the group G.</p><p>Proof. Let M t be a G-equivariant model. For simplicity, we will assume that M t is being optimized with the following loss:</p><formula xml:id="formula_56">L t = E ϕ∼πt [∥M t [ϕ] -s t (ϕ)∥ 2 ]<label>(43)</label></formula><p>where s t = ∇ ϕ log π t (ϕ) is the ideal score function. First consider the orbit of a single point ϕ 0 under the group G, given by G[ϕ 0 ] = {ϕ : ∃U ∈ G : U ϕ 0 = ϕ}. For any ϕ ∈ G[ϕ 0 ], there is an element U ∈ G such that U -1 ϕ = ϕ 0 , and thus the output of an equivariant model</p><formula xml:id="formula_57">M t [ϕ] is simply U M t [ϕ 0 ].</formula><p>The problem of picking an optimal M t [ϕ] for any ϕ ∈ G[ϕ 0 ] can thus be reduced to a standard linear regression for M t [ϕ 0 ], under the loss</p><formula xml:id="formula_58">Lt = E ϕ∼πt|ϕ∈G(ϕ0) [∥M t [ϕ] -∇ log π t (ϕ)∥ 2 ] = G π t [U -1 ϕ 0 ] π t (G[ϕ 0 ]) M t [ϕ] -U ∇ log π t (U -1 ϕ 0 ) 2 dU</formula><p>where in the second line we have used the property of unitaries that ∥U x∥ 2 = ∥x∥ 2 . Here π t (G[ϕ 0 ]) indicates the probability density assigned to the entire orbit G[ϕ 0 ] by π t . We have used the orbit-stabilizer property to write the integral over the orbit as an integral over the entire group. Despite its complexity this formula represents a standard least-squares objective for M t [ϕ], the minimizer of which is simply the weighted average of the target function U ∇ log π t (U -1 ϕ 0 ) weighted by</p><formula xml:id="formula_59">πt[U -1 ϕ0] πt(G[ϕ0]</formula><p>) . In other words, our optimal G-equivariant model is</p><formula xml:id="formula_60">M t [ϕ] = U ∈G U ∇ log π t [U -1 ϕ] π t (U -1 ϕ) V ∈G π t (V -1 ϕ) dV dU.<label>(44)</label></formula><p>We also repeat this analysis procedure for a pretrained self-attention-enabled UNet trained on CIFAR10 from <ref type="bibr" target="#b35">(Sehwag, 2024)</ref>, with the exception that we re-use the scales calibrated for the CIFAR10 ResNet model rather than re-estimating them for the Self-Attention-enabled model in order to minimize bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Identifying multiscale behavior</head><p>In order to correctly recapitulate the behavior of the models we study, we need to account for a crucial empirical observation: convolutional diffusion models exhibit time-dependent effective receptive field sizes. This behavior is illustrated in figure 4.</p><p>In the left panel, we display an average absolute value of the gradient of the center pixel of the model outputs from two of our CIFAR10-trained diffusion models, with respect to the input image. We plot this at various time steps in the reverse process (with the center pixel omitted for visual clarity). This visualization highlights which areas of the image the center pixel's outputs are sensitive to, an indicator of the degree of locality in the model's output. At T = 1.0 (corresponding to an input of initial white noise), the average gradient spans a large range (for the ResNet, a range clearly constrained by its maximal receptive field size). As the noise level reduces throughout the reverse process, the width of the heatmap decreases, until at the last time step the heatmap is almost entirely concentrated in a single (3 × 3) square.</p><p>To calibrate the time-dependent locality scale of our theoretical model, we compute the reverse trajectories under the CNN-parameterized neural networks for a random validation set. At each time step, we compare the predictions of the model for the added noise and the outputs of ELS machines with a range of scales via cosine similarity. We then pick the representative scale for each time step by picking the median optimal scale across the range of samples. The resulting calibrated scales are shown in the middle panel of figure <ref type="figure" target="#fig_4">4</ref>. We see that the UNet is better described by a larger-scale ELS machine early in the reverse process than its ResNet counterparts, a phenomenon that can probably be linked to the more stringent locality constraints in the latter model. However, as the reverse process continues, both models prefer monotonically smaller scales, until converging to the smallest scale (3 × 3) for the final few denoising steps. These results are in accordance with the visual evidence from the gradient heatmaps in figure <ref type="figure" target="#fig_4">4</ref>.</p><p>At this stage we have no a-priori method for predicting the scales that the models choose to use at each time step. However, the general phenomenon where the model initially starts with a large field of view and decreases it over the course of the reverse process could be anticipated on general grounds. As the noise variance decreases, the noised training distribution separates into a multimodal distribution with a larger and larger number of modes; as t → 0, the number of modes converges to the (very large!) number of training examples. Since the models we consider are equivariant, an optimal model must also in principle represent not just the modes corresponding to the training set, but also to every translated augmentation, which for a 32 × 32 image results in a 1024-fold increase in effective dataset size! However, the emergence of multimodality is delayed when considering only the marginal distributions with respect to a smaller scale, as there are fewer dimensions via which two distinct data points could be distinguished from each other. This suggests that the model may somehow be picking the largest scale that it can a) represent within its receptive field (a constraint more pertinent to the ResNet, which has a smaller maximum receptive field size) and b) for which it can represent the local/equivariant approximation to the score function in a reasonably parameter efficient way, i.e. for which it need not model too many independent modes of the data distribution. However, more work needs to be done in order to understand this phenomenon. Table <ref type="table">1</ref>. A summary of the experimental results of the paper for different datasets and model configurations for each architecture across each dataset. Pixelwise r 2 between theory and neural network images are computed using 100 image samples per configuration; the median across the sample is reported. We compare these results to a baseline consisting of the correlations of the model with the outputs of an ideal score (IS) model, which always outputs memorized training examples. We also report the percentage of samples on which the ELS machine outperforms the output from the ideal score-matching diffusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Arch. Padding Conditional ELS Corr. IS Corr. ELS &gt; IS % MNIST UNet Zeros ✗ 0.84 0.69 0.92 CIFAR10 UNet Zeros ✓ 0.82 0.39 0.99 FashionMNIST UNet Zeros ✓ 0.91 0.80 0.90 MNIST ResNet Zeros ✗ 0.94 0.62 0.97 MNIST ResNet Circular ✗ 0.77 0.16 0.96 CIFAR10 ResNet Zeros ✓ 0.90 0.43 1.00 CIFAR10 ResNet Circular ✓ 0.90 0.36 1.00 FashionMNIST ResNet Zeros ✓ 0.90 0.74 0.97 CIFAR10 UNet + SA Zeros ✓ 0.75 0.47 0.92           </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our analytic theory (left columns) can accurately predict on a case by case basis the outputs of convolutional diffusion models (right columns), with U-Net or ResNet architectures trained on MNIST, CIFAR10 and FashionMNIST (left to right), even when these outputs are highly creative and far from the training data. See Fig. 5, App. C, Fig. 8 and Table 1, and App. D, Fig. 11 to Fig. 18 for many more successful theory-experiment comparisons.</figDesc><graphic coords="1,307.44,175.28,233.99,155.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Ideal score-matching under various constraints. (a) In the IS machine, the entire image (bottom) reverse flows to a single training set image from the training set (top stack). (b,c) In both the LS and ELS machines, different local patches of the image flow to different local patches in the training set. In the LS machine this final training patch must be drawn from the same location (b), while in the ELS machine, it can be drawn from any location (c).</figDesc><graphic coords="3,469.99,72.34,67.28,85.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Exponential creativity through locally consistent patch mosaics. (a) A training set of two images (all black or all white). (b)Creative samples from any local score machine (LS or ELS) with a 3 × 3 locality window and periodic boundary conditions. Local consistency in this special case means every generated pixel is either black or white, and the majority color of every generated 3 × 3 patch equals the color of its central pixel. (c) We note that samples are generated by numerically integrating the reverse flow in (1). If the step size in this integration is too large, one can generate invalid samples with a few cases of broken local consistency (highlighted red patches). In practice in trained diffusion models, this local consistency would only hold approximately.</figDesc><graphic coords="6,196.38,67.34,204.11,59.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>) every pixel x can be uniquely assigned to a local training set patch φ; (2) the pixel value ϕ 0 (x) is exactly equal to the central pixel φ(0) of φ; (3) the rest of the local generated patch ϕ Ωx resembles the local training patch φ more than any other possible training patch. This result characterizes the creative outcome of locally constrained machines as creating locally consistent patch mosaics where every pixel of every local patch in the sample matches the central pixel of the L 2 closest local patch in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Coarse to fine progression of spatial locality in the reverse flow. (a) A heatmap of the average absolute value of the Jacobian from the output score Mt[ϕt](x = 0) at the center pixel x = 0 back to all input pixels ϕ(x ′ ) as a function of x ′ . This receptive field shrinks from large to small as time progresses from early (large t) to late (small t) in the reverse flow. (b) Optimally calibrated values of the spatial locality scale P of the ELS machine as a function of time t (see App. C.2 for details of calibration). (c) A schematic view of the time-dependent LS and ELS machines in which the locality neighborhood shrinks as the reverse time flows from top to bottom.</figDesc><graphic coords="7,311.77,67.06,82.62,149.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>(a) Theory (left) vs. ResNet (right) (b) Theory (left) vs. UNet (right) (c) Inconsistencies in FashionMNIST Figure 5. Match between theory and experiment. (a,b) Each pair of images shows a striking match between the output of the boundary broken ELS machine (left image in each pair) and the output of a trained CNN diffusion model (right image in each pair) when both models are given the same initial noise input. We compare theory with 2 architectures (ResNet in (a), and UNet in (b)) on 3 datasets (MNIST, CIFAR10 and FashionMNIST from top to bottom). See App. D, Fig. 11 to Fig. 18 for many comparisons and Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison between UNet+SA outputs (top row) and ELS machine outputs (bottom row) for the same noise inputs. For this class of inputs, the UNet+SA appears to carve out more semantically coherent objects out of the closely related ELS patch mosaic.</figDesc><graphic coords="8,61.49,615.90,471.37,75.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. In the presence of zero-padded borders, different dictionaries of training set patches are used for the ELS machine computation depending on the contextual information provided by the visible border within the patch. For a central patch (left, red patch) without border information, training set patches (green patches) are sourced from the entire image interior. For edge patches (middle, red patch), training set patches (green patches) are sourced from everywhere along the edge at the same distance from the border. For corner patches (right, red patch), only patches from that exact location are used in the computation.</figDesc><graphic coords="15,189.09,67.06,218.68,87.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Correlations between model outputs and ELS machine/IS baseline on each dataset for zero-padded models. Y axis is ELS machine r 2 , X axis is IS baseline r 2 for each data point in the sample. The ELS machine uniformly outperforms the baseline, with stronger performance for the ResNet on CIFAR10 and MNIST and a stronger performance for the UNet on FashionMNIST.</figDesc><graphic coords="20,102.80,445.19,194.40,151.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure9. Correlations between model outputs and ELS machine/IS baseline on each dataset for circularly-padded models. Y axis is ELS machine r 2 , X axis is IS baseline r 2 for each data point in the sample. The ELS machine uniformly outperforms the baseline. The performance of the ELS machine on circular MNIST is anomalously lower than other configurations, but the degree of outperformance of the ideal score baseline is higher.</figDesc><graphic coords="21,102.80,297.52,194.40,151.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Correlations between model outputs and ELS machine/IS baseline on CIFAR10 for a pretrained Attention-enabled UNet. Y axis is ELS machine r 2 , X axis is IS baseline r 2 for each data point in the sample.</figDesc><graphic coords="21,201.24,524.22,194.40,151.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Further comparison between ResNet (right columns) and ELS Machine (left columns) samples for MNIST. Model is unconditional and has zero padding.</figDesc><graphic coords="23,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Further comparison between UNet (right columns) and ELS machine (left columns) samples for MNIST. Model is unconditional and has zero padding.</figDesc><graphic coords="24,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Further comparison between ResNet (right columns) and ELS Machine (left columns) samples for FashionMNIST. Model is class conditional and has zero padding.</figDesc><graphic coords="25,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Further comparison between UNet (right columns) and ELS machine (left columns) samples for FashionMNIST. Model is class conditional and has zero padding.</figDesc><graphic coords="26,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Further comparison between ResNet (right columns) and ELS machine (left columns) samples for CIFAR10. Model is class conditional and uses zero padding.</figDesc><graphic coords="27,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Further comparison between UNet (right columns) and ELS machine (left columns). Model is class conditional and has zero padding.</figDesc><graphic coords="28,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Further comparison between ResNet (right columns) and ELS machine (left columns) on MNIST. Model is unconditional and has circular padding.</figDesc><graphic coords="29,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Further comparison between ResNet (right columns) and ELS machine (left columns) samples on CIFAR10. Model is class conditional and has circular padding.</figDesc><graphic coords="30,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Further comparison between attention-enabled UNet (right columns) and ELS machine (left columns) samples on CIFAR10. Model is class conditional and has zero padding.</figDesc><graphic coords="32,79.74,104.66,437.40,538.98" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We can do some simple algebra to write this experssion in a more interpretable form:</p><p>where in the last step we have used the fact that U -1 = U † and that ∇ ϕ f (U † ϕ) = U [∇f ](U † ϕ). We now note that</p><p>Since U is unitary, it follows that</p><p>and thus our optimal model is the score function for the empirical noise distribution of the G-augmented dataset, i.e.</p><p>B.4. The sample distribution at t = 0 under a local score approximation</p><p>When the score is learned optimally, the reverse process concentrates the sample distribution on the training dataset as t → 0.</p><p>It is instructive for us to ask what the analogous constraint on the generated samples is for the locality-constrained models that we consider in this paper. The answer is that the flow will concentrate the probability on certain 'locally consistent points' φ, defined as follows. Suppose we are employing an Ω-local approximation M t to the score function, with each individual pixel x using a (possibly identical) dictionary of patches φ ∈ P x Ω . A 'locally consistent point' φ is a point such that for every pixel location x, the value φ(x) is equal to the center pixel φ(0) of the l 2 -closest patch φ ∈ P x Ω to the patch φΩx , i.e. the patch that minimizes φ -φΩx 2 over all patches in P x Ω .</p><p>The reverse flow approximation parameterized by M t will concentrate on locally consistent points. We can formalize this effect in the following theorem:</p><p>Theorem B.3. Suppose we sample an initial point ϕ T from the Gaussian π T , and we evolve this density under the standard reverse process</p><p>where</p><p>Suppose also that the limits lim t→0 ϕ t and lim t→0 ∂ t ϕ t exist for an initial point ϕ T . Then the limit must be a locally consistent point.</p><p>Proof. The assumption that lim t→0 ∂ t ϕ t exists entails that for any point ϕ t on a particular trajectory, the values of ϕ t and -γ t (ϕ t + M t (ϕ t )) must stay bounded as t → 0, which in turn entails that γ t M t (ϕ t ) must likewise stay bounded as t → 0.</p><p>This latter quantity is given at pixel location x by</p><p>The prefactor goes to ∞ as t -1 as t → 0, so it follows that for the derivative to have a finite limit, the right-hand factor must go to zero. As ᾱt → 0, the weights take the limiting values</p><p>and thus the limiting value of the sum is simply</p><p>where</p><p>This value is zero only when φ(x) = φ(0). The condition that this holds for all x is the definition of a locally consistent point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Empirics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Experimental details</head><p>To our ELS machine model of CNN-based diffusion, we examine two different architectures:</p><p>1. UNet: we use a standard UNet <ref type="bibr" target="#b33">(Ronneberger et al., 2015)</ref> with three scales with channel dimensions of 64, 128, 256 respectively. We use residual connections in each UNet block. This model is formally local, but has a maximum receptive field size larger than the 32 × 32 images we consider.</p><p>2. ResNet: we use a minimal 8-layer convolutional neural network, with an upscaling and downscaling layer and 6 intermediate convolutional layers at a channel dimension of 256. Each layer is a single convolutional layer with a kernel size of 3 × 3 and with residual connections <ref type="bibr" target="#b13">(He et al., 2016)</ref> between layers. This model has a formal maximum receptive field size of 17 × 17.</p><p>For all experiments, we train each model for 300 epochs with Adam, using an initial learning rate of 1e-4, a batchsize of 128, and an exponential learning rate schedule that halves the learning rate over the course of 50 epochs. We do not employ normalization layers in any of our models in order to avoid the possibility of information being exchanged nonlocally throughout the image. We do not use weight decay. We sample images using a 20-step discretization of the reverse process, using the analytic form prescribed for DDIM-style models in <ref type="bibr">(Song et al., 2020a)</ref>. We use a cosine noise schedule <ref type="bibr">(Nichol &amp; Dhariwal, 2021)</ref> for each experiment.</p><p>We evaluate each architecture using zero-padded convolutions on the following datasets: MNIST, FashionMNIST, and CIFAR10. For each of the latter datasets, we use class-conditioning. We only train class-unconditional models on MNIST, since we observe that with class-conditioning the models have a strong propensity to memorize. In addition, we evaluate our ResNet architecture using circularly-padded convolutions on CIFAR10 (class-conditional) and <ref type="bibr">MNIST (class-unconditional)</ref>.</p><p>For each neural network on each dataset, we calibrate an associated multiscale ELS model of the network using the procedure described in C.2. The ELS model inherits the class-conditionality of the neural network it is modeling. We then compute the outputs of each neural network on 100 distinct random noise inputs for each dataset, drawn iid from an isotropic normalized Gaussian distribution. For class-conditional models, we additionally sample a label for each seed. We compute the outputs of the corresponding ELS machine on the same seeds/labels. For each example, we compute the pixelwise r 2 between the ELS machine output and the network output. We also compute the outputs of the ideal score-matching diffusion model across all inputs/labels, and compute the pixelwise r 2 between this baseline and the neural network outputs. We report the median r 2 value across the 100 samples for all configurations in table 1, and plot the distribution of ELS correlations/IS correlations in figures 8 and 9. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion models in de novo drug design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alakhdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Washburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">In search of dispersed memories: Generative diffusion models are associative memory networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ambrogioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17290</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nearly d-linear convergence bounds for diffusion models via stochastic localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image classification and explainable identification of ai-generated synthetic images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><surname>Cifake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bonnaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mézard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18491</idno>
		<title level="m">Dynamical regimes of diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4672" to="4712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
		<idno>arXiv [cs.LG]</idno>
		<title level="m">High-dimensional asymptotics of denoising autoencoders</title>
		<imprint>
			<date type="published" when="2023-05-18">18 May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of learning a flow-based generative model from limited sample complexity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2023-10-05">5 October 2023</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convergence of denoising diffusion models under the manifold hypothesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>De Bortoli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.05314</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Equivariant diffusion for molecule generation in 3d</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8867" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Memory in plain sight: A survey of the uncanny resemblances between diffusion models and associative memories</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16750</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multi-scale local conditional probability models of images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02984</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generalization in diffusion models arises from geometry-adaptive harmonic representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02557</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22870" to="22882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion-lm improves controllable text generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4328" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.06574</idno>
		<title level="m">Diffusion models for non-autoregressive text generation: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00045</idno>
		<title level="m">Detecting multimedia generated by large ai models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02747</idno>
		<title level="m">Flow matching for generative modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diffusion models are minimax optimal distribution estimators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="26517" to="26582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Memorization to generalization: The emergence of diffusion models from associative memory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ambrogioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krotov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">October 5-9, 2015. 2015</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A phase transition in diffusion models reveals the hierarchical nature of data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sclocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Favero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wyart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16991</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Minimal implementation of diffusion models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<ptr target="https://github.com/VSehwag/minimal-diffusion" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the spatial inconsistency in classifier-free diffusion guidance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9370" to="9379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Achilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucibello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ambrogioni</surname></persName>
		</author>
		<author>
			<persName><surname>Manifolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.05898</idno>
		<title level="m">random matrices and spectral gaps: The geometric phases of generative diffusion</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Diffusion models learn low-dimensional distributions via subspace clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02426</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">De novo design of protein structure and function with rfdiffusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juergens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Eisenach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ragotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="issue">7976</biblScope>
			<biblScope unit="page" from="1089" to="1100" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The emergence of reproducibility and consistency in diffusion models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
