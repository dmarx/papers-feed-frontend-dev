{
  "arxivId": "2001.04063",
  "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training",
  "authors": "Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, Ming Zhou",
  "abstract": "This paper presents a new sequence-to-sequence pre-training model called\nProphetNet, which introduces a novel self-supervised objective named future\nn-gram prediction and the proposed n-stream self-attention mechanism. Instead\nof optimizing one-step-ahead prediction in the traditional sequence-to-sequence\nmodel, the ProphetNet is optimized by n-step ahead prediction that predicts the\nnext n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for\nthe future tokens and prevent overfitting on strong local correlations. We\npre-train ProphetNet using a base scale dataset (16GB) and a large-scale\ndataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail,\nGigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question\ngeneration tasks. Experimental results show that ProphetNet achieves new\nstate-of-the-art results on all these datasets compared to the models using the\nsame scale pre-training corpus.",
  "url": "https://arxiv.org/abs/2001.04063",
  "issue_number": 286,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/286",
  "created_at": "2025-01-04T15:03:09.857516",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 3,
  "last_read": "2025-01-04T15:03:09.858274",
  "last_visited": "2024-12-26T17:17:59.219Z",
  "main_tex_file": null,
  "published_date": "2020-01-13T05:12:38Z",
  "arxiv_tags": [
    "cs.CL"
  ]
}