- **Model Name**: ProphetNet
- **Objective**: Future n-gram prediction for sequence-to-sequence pre-training.
- **Key Innovations**:
  - **Future n-gram prediction**: Predicts the next n tokens simultaneously instead of one token at a time.
  - **N-stream self-attention mechanism**: Extends the two-stream self-attention to n streams for future token prediction.
- **Loss Function**:
  - Combined loss: 
    \[
    L = - \sum_{j=0}^{n-1} \alpha_j \cdot \sum_{t=1}^{T-j} \log p_\theta(y_{t+j} | y_{<t}, x)
    \]
  - Components:
    - Language modeling loss (one-step prediction).
    - Future n-gram loss (n-step prediction).
- **Attention Mechanism**:
  - Main stream self-attention: Standard masked multi-head attention.
  - Predicting streams: Each stream predicts a specific future token based on the main stream's hidden states.
- **Training Process**:
  - Pre-trained on two datasets: 16GB (base scale) and 160GB (large scale).
  - Fine-tuned on tasks: Abstractive summarization (CNN/DailyMail, Gigaword) and question generation (SQuAD 1.1).
- **Performance**:
  - Achieved state-of-the-art results on all tested datasets.
  - Efficient training: Requires fewer epochs compared to BART and T5.
- **Inference**: 
  - During inference, only the main stream is used to predict the next token, reverting to traditional Seq2Seq behavior.
- **Diagram**: N-stream self-attention mechanism (mermaid markdown not included as it may not be necessary for quick reference).