- Decision to introduce the future n-gram prediction objective
- Choice of n-stream self-attention mechanism
- Selection of base scale (16GB) and large scale (160GB) datasets for pre-training
- Adoption of the mask-based auto-encoder denoising task for Seq2Seq pre-training
- Design decision to share parameters between main stream and predicting streams
- Implementation of the two-stream self-attention mechanism
- Strategy for balancing weights between traditional language modeling and future n-gram prediction
- Decision to disable n-stream self-attention during inference
- Choice of benchmarks for evaluation (CNN/DailyMail, Gigaword, SQuAD 1.1)
- Decision to use token span masking for the denoising task
- Choice of hyperparameters for training (e.g., learning rate, batch size)
- Decision to focus on long-term dependencies in token prediction
- Design choice for the architecture based on Transformer encoder-decoder
- Decision to optimize for n-step ahead prediction instead of one-step ahead
- Choice of attention mechanisms for different streams in the decoder
- Decision to use absolute and relative positional embeddings in the decoder