{
  "arxivId": "2103.13413",
  "title": "Vision Transformers for Dense Prediction",
  "authors": "Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun",
  "abstract": "We introduce dense vision transformers, an architecture that leverages vision\ntransformers in place of convolutional networks as a backbone for dense\nprediction tasks. We assemble tokens from various stages of the vision\ntransformer into image-like representations at various resolutions and\nprogressively combine them into full-resolution predictions using a\nconvolutional decoder. The transformer backbone processes representations at a\nconstant and relatively high resolution and has a global receptive field at\nevery stage. These properties allow the dense vision transformer to provide\nfiner-grained and more globally coherent predictions when compared to\nfully-convolutional networks. Our experiments show that this architecture\nyields substantial improvements on dense prediction tasks, especially when a\nlarge amount of training data is available. For monocular depth estimation, we\nobserve an improvement of up to 28% in relative performance when compared to a\nstate-of-the-art fully-convolutional network. When applied to semantic\nsegmentation, dense vision transformers set a new state of the art on ADE20K\nwith 49.02% mIoU. We further show that the architecture can be fine-tuned on\nsmaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets\nthe new state of the art. Our models are available at\nhttps://github.com/intel-isl/DPT.",
  "url": "https://arxiv.org/abs/2103.13413",
  "issue_number": 785,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/785",
  "created_at": "2025-01-04T14:48:27.532008",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 5,
  "last_read": "2025-01-04T15:01:58.107814",
  "last_visited": "2025-01-04T14:48:49.256000+00:00",
  "main_tex_file": null,
  "published_date": "2021-03-24T18:01:17Z",
  "arxiv_tags": [
    "cs.CV"
  ]
}