- Decision to use Vision Transformers as the backbone for dense prediction tasks
- Choice of encoder-decoder architecture for the model
- Implementation of a convolutional decoder for final predictions
- Strategy for assembling tokens into image-like representations
- Decision to maintain a constant resolution throughout the transformer stages
- Use of multi-headed self-attention for global receptive field
- Selection of training datasets (e.g., ImageNet, NYUv2, KITTI, ADE20K)
- Approach for fine-tuning on smaller datasets
- Decision to compare performance against fully-convolutional networks
- Choice of evaluation metrics (e.g., mIoU for segmentation, relative performance for depth estimation)
- Implementation of fusion modules for combining features at different resolutions
- Decision to use residual convolutional units in the decoder
- Choice of patch size for tokenization in the Vision Transformer
- Decision to include a readout token for global representation
- Strategy for handling spatial information in token representations
- Decision to leverage large-scale training data for model performance improvement
- Choice of model variants (ViT-Base, ViT-Large, ViT-Hybrid) for experimentation
- Decision to focus on improving feature granularity and coherence in predictions
- Approach for addressing memory consumption in the architecture
- Decision to document architectural choices and their impacts on performance