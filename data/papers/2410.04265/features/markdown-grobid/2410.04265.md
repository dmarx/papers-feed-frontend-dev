# AI AS HUMANITY'S SALIERI: QUANTIFYING LINGUISTIC CREATIVITY OF LAN-GUAGE MODELS VIA SYSTEMATIC ATTRIBUTION OF MACHINE TEXT AGAINST WEB TEXT

## Abstract

## 

Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains. 1

## INTRODUCTION

Creativity has long been considered one of the most challenging "holy grail" of human intelligence for AI to mimic [(Hasselberger & Lott, 2023)](#b31). However, Large Language Models (LLMs) such as ChatGPT have taken the world by storm with their creative power. From generating poetry [(Sawicki et al.;](#)[Deng et al., 2024b;](#)[Sawicki et al., 2023)](#b64) and composing music [(Ding et al., 2024;](#b19)[Deng et al., 2024a;](#)[Liang et al., 2024)](#b44) to designing artwork [(Makatura et al., 2024;](#b48)[Jignasu et al., 2023;](#b38)[Lim et al., 2024)](#b45) and crafting compelling narratives [(Yuan et al., 2022;](#b77)[Mirowski et al., 2023a;](#)[Ippolito et al., 2022)](#b34), LLMs take only seconds to produce outputs that would rival or even surpass the work of human creators. This proficiency has even sparked a growing trend of using LLMs for content creation in industrial settings. For example, major studios in Hollywood have integrated LLMs into production processes such as movie scriptwriting [(Carnevale, 2023)](#b8). While studio executives are optimistic about using LLMs to streamline production and reduce costs, Hollywood writers are deeply concerned about being replaced by the rapid integration of LLMs in the industry, leading to a five-month writers' strike [(Koblin & John, 2023)](#b41).

While science fiction writer Ted Chiang characterizes LLMs as a blurry JPEG of the web [(Hubert et al., 2024)](#b32), many others wonder whether AI can indeed match or surpass the creativity of humanity. The advancement of technology has led to the integration of artificial intelligence (AI) in various fields, including literature. However, the question remains: can AI truly understand and create poetry? This paper delves into the complexities of poetry and the limitations of AI in its ability to comprehend and replicate the nuanced emotions and intricacies of language. Through a critical analysis of the relationship between poetry and AI, this paper argues that while AI may be able to generate text that resembles poetry, it lacks the ability to truly … … with Qualcomm for the ZenFone 5Z. That partnership led to the integration of artificial intelligence (AI) in various apps, making them smarter and easier to use … … Artificial Intelligence (AI), such as chat GPT-3 to assist in the process. However, the question remains: can AI fully replace human recruiters? The answer is no … … there has been a significant increase in the use of artificial intelligence (AI) in various fields, including language generation. One such AI language model … … current generation of people with a more intellectual brain. The advancement of technology has led to the creation of an electronic device which is capable of … … but there is a more important point here about the perception of poetry and the limitations that critics and readers impose upon it if they understand poetry as … … but it is too far narrowly limited and inflexible in its ability to comprehend and apply all the relevant facts in order to serve the process of selection, which is better … Literature, poetry, and other forms of noncommercial creative expression challenge the techno-instrumentalist approaches to language, the predictive language generation, informing NLP (large natural language processing models) such as GPT-3 or -4 as well as, more generally, generative AI (text to image, video, audio). Claims that AI systems automate and expedite creativity reflect industry and research priorities of speed, scale, optimization, and frictionlessness driving much artificial intelligence design and application. 

## Prof. Michele Elam

## Preprint

We believe that our study will enhance the understanding of LLMs and guide informed usage of content created by LLMs, by providing an interoperable and scalable measurement to assess creativity in machine texts. Additionally, we hope that the out-of-the-box machine text detection enabled by the CREATIVITY INDEX can empower individuals to discern between human texts and machine texts, fostering a more informed and critical engagement with information in the digital age.

## METHOD

## CREATIVITY INDEX

The key intuition underlying CREATIVITY INDEX is to quantify the degree of linguistic creativity of a given text by estimating how much of that text can be reconstructed by mixing and matching a vast amount of existing text snippets on the web, as shown in Figure [1a](#). Specifically, CREATIVITY INDEX assesses the extent to which the content of the text can be traced back to similar or identical contexts found in other existing texts. This metric is grounded in the notion of originality from creative thinking in psychology literature, which is defined as the statistic rarity of a response or an idea [(Torrance, 1966;](#b68)[Crossley et al., 2016)](#b14).

Concretely, let x be a text whose creativity we aim to quantify, such as a speech transcript or a poem, either human written or machine generated. Let an n-gram of x be any contiguous sequence of n words of x, and let x i:i+n be the n-gram of x starting in the i-th word. Let C be a massive reference corpus of publicly available texts on the web , and let f be a binary function that determines whether an n-gram x i:i+n occurs anywhere in the corpus C. We define the L-uniqueness of a text x as the proportion of words w ∈ x such that none of the n-grams in x that include w occur in the corpus C for n ≥ L-denoted uniq(x, L). Intuitively, L-uniqueness measures the proportion of x's words that are used in novel contexts (here, n-grams), unseen across a vast text collection C. Thus, a higher L-uniqueness implies a higher level of originality of x. Formally, uniq(x, L)

$= ∥x∥ k=1 1{f(x i:i+n , C) = 0 ∀ i ∈ (k -n, k], n ≥ L}/∥x∥, where trivially uniq(x, L) ∈ [0, 1].$Note that when fixing x, the function uniq(x, L) is monotonically increasing as L grows. Its improper integral-n≥L uniq(x, n)-is an indicator of the overall uniqueness of x across various context granularities (i.e., n-gram lengths), and because of uniq(x, L)'s monotonicity it indirectly measures uniqueness growth speed. We thus define CREATIVITY INDEX as n≥L uniq(x, n), with higher CREATIVITY INDEX indicating greater linguistic originality with respect to the corpus C, as shown in Figure [1b](#).

When a text x is part of the reference corpus C, its CREATIVITY INDEX would trivially become zero. This issue often arises with works from famous authors, as their writings are widely available online.

To address this, for human texts written before the cutoff date of the reference corpus, we exclude any document d ∈ C that contains copies, quotations, or citations of x and compute CREATIVITY INDEX using this filtered corpus, detailed in Appendix A.3.

## DJ SEARCH

To enable the use of our CREATIVITY INDEX it is vital to compute it efficiently. For the efficient computation, we introduce DJ SEARCH, a dynamic programming algorithm designed to radpily identify the set of all x's n-grams (n ≥ L) that occur in the corpus C.

A brute force approach would independently check if every n-gram of x occurs in C, performing a quadratic number of f evaluations with respect to x's length, and thus making it too computationally expensive. Instead, we design a two-pointer method [(Laaksonen, 2020)](#b43) that takes only a linear number of f evaluations, as illustrated in Figure [2](#). The key idea is to reduce finding all n-grams occurring in C to identifying the longest n-gram occurring in C starting at each index i: once those have been found, it is trivial to deduce all the n-gram occurring in C by computing their subsequences. Concretely, we progressively analyze the whole document x by iteratively searching for the longest n-gram that starts at each index i and occurs in C, using f as the assessment. Once we have found such longest n-gram starting at i, we crucially reuse computations for i + 1 by noting that f (x i:i+n , C) = 1 implies f (x i+1:i+n , C) = 1. Thus, we always analyze n-grams starting and/or ending at a later endpoint than before, which upper bounds the number of analyzed n-grams (i.e., the number of f calls) to at most 2∥x∥. The implementation is detailed in Appendix A.1.

In addition to minimizing the number f evaluations, DJ SEARCH optimizes the time complexity of each evaluation. f determines whether a n-gram x i:i+n occurs in the corpus C either exactly or in a semantically similar way-e.g., a paraphrase of x i:i+n exists in C.

Semantic similarity is often Preprint Algorithm 5 DJ SEARCH (x, C) Input: text x, reference corpus C, n-gram matching function f (x i: j ,C) minimum n-gram length L Output: set of matched n-gram S i 0, j L, S ? while j  ||x|| do if f (x i: j ,C) = 1 then S S [ {x i: j } j j + 1 else i i + 1 j max(i + L, j) end if end while while AI may be able to generate text that resembles

$✓ ✘ ✘ ✘ ✘ ✓ ✓ ✘ ✘ ✓ ✓ ✓ ✓ Matched n-gram$✓ Found n-gram matches from start to end position ✘ No n-gram matches from start to end position … this paper argues that while AI may be able to generate text that resembles …

Figure [2](#): An illustration of DJ Search algorithm. A brute force approach would independently check if every n-gram of x occurs in C, performing a quadratic number of f evaluations with respect to x's length (i.e., checking every cell in the grid). DJ SEARCH is a two-pointer method that takes only a linear number of f evaluations. By progressively analyzing n-grams starting and/or ending at a later endpoint than before, DJ SEARCH limits the total number of f evaluations to 2||x||. In this example, the minimum n-gram length L is set to 5.

computed using text embeddings, which are fixed-length vector representations of text meanings. This reduces measuring text similarity to computing vector distance. Text embeddings, typically generated by complex models (e.g., BERT [(Devlin et al., 2019)](#b18), RoBERTa [(Liu et al., 2019)](#b47), Span-BERT [(Joshi et al., 2020)](#b39)) lack linearity, requiring independent computation for each n-gram in x and C. To alleviate this issue we use Word Mover's Distance (WMD) [(Kusner et al., 2015)](#b42), an optimal transport-inspired metric that measures distance between two n-grams by combining word embedding distances between each n-gram's words. WMD enables optimizing f 's computation, as pairwise distances between word embeddings can be pre-computed for every pair of words, and then be reused in every function call of f to identify n-grams in C that are semantically similar to the ones in x. The implementation is detailed in Appendix A.2.

To further boost efficiency, and given that occurrences of x i:i+n are more likely in texts similar to x, we estimate f by computing WMD only for the texts in C most similar to x, as identified by BM25 [(Robertson & Walker, 1994)](#b62). Moreover, exact occurrences of x i:i+n in C represent a less costly special case in computing f . We further optimize f 's computation by using Infinigram [(Liu et al., 2024)](#b46), which finds exact matches of x i:i+n in C in milliseconds; WMD is computed only if no matches are found by Infini-gram.

## EVALUATION

How does the creativity of language models compare to humans? We compute the CREATIV-ITY INDEX for machine texts and human texts across three creative writing tasks: novel writing, poetry composition, and speech drafting. For human texts, we use book snippets in the Book-MIA [(Shi et al., 2024)](#b65) dataset, popular modern poems collected by PoemHunter.com, and famous speeches from the American Rhetoric speech bank. For machine texts, we prompt LLMs to generate several paragraphs of novels, poems, or speeches, starting with an initial sentence from existing human writings in each category (see Appendix B.1 for details). We experiment with state-of-the-art LLMs, including [GPT-3 (Brown et al., 2020)](#), ChatGPT [(Ouyang et al., 2022)](#b59), LLaMA 2 Chat [(Touvron et al., 2023)](#b69), Tulu 2 [(Ivison et al., 2023)](#b35), and OLMo Instruct [(Groeneveld et al., 2024)](#b30). For open-source and open-weight models, we use the largest model size available from each model family. We use RedPajama (Computer, 2023), a large-scale English corpus with 900 million web documents, as the reference corpus. The models we analyze are primarily pre-trained on the web data available before the cutoff date of the reference corpus RedPajama. We will discuss later how to handle newer models, such as [GPT-4 (OpenAI et al., 2023)](#), given that it was largely trained on more recent web data and third-party private data, both of which fall outside the reference corpus.

We restrict the matching criteria to verbatim matches only in the first experiment. We will ablate the effect of different matching criteria, prompt formats, decoding strategies, context length, and model sizes in later experiments.

Our primary finding is that humans consistently exhibit a much higher level of creativity compared to any LLM across all tasks (Fig. [3a-c](#)). Averaged across all models, the CREATIVITY INDEX of humans is 52.2% higher[foot_2](#foot_2) than LLMs in novel writing (p = 6.9 × 10 -27 , by Mann-Whitney U test unless otherwise specified; N = 600), 31.1% higher in poetry composition (p = 1.5 × 10 -15 ; N = 600) and 115.3% higher in speech drafting (p = 6.1 × 10 -31 , N = 600). This suggests that human writings are composed of far more unique combinations of words and phrases compared to model generations. On the other hand, the differences in model creativity are much smaller and show very low statistical significance (p = 0.09; N = 1500).

Furthermore, we experiment with different prompt formats on top of ChatGPT, intentionally encouraging creativity in the model's generations by incorporating instructions such as 'push for creative ideas, unique emotions, and original twists,' 'be bold and creative,' or 'you are a creative writer.' (Fig. [4a](#)) For a full list of the prompts we used, please see Appendix B.1. We found that the difference in the CREATIVITY INDEX of ChatGPT across different prompts is minimal, with no statistical significance (p = 0.23; N = 600). We also experimented with different decoding strategies by varying the p value in top-p decoding (Fig. [4b](#)). Although a higher p value resulted in a marginally higher CREATIVITY INDEX, the difference was minimal and not statistically significant (p = 0.23; N = 600). Moreover, we ablate the effect of prompt length by varying the number of sentences from human writings included in the prompt (Fig. [4c](#)). We found that longer prompts tended to result in a slightly higher CREATIVITY INDEX, likely due to the model copying more from the longer human text in the prompt. However, the statistical significance of these differences is very low (p = 0.13; N = 600). Lastly, we analyze the effect of different model sizes for LLaMA 2 Chat and Tulu 2, but do not observe a consistent trend (p = 0.12; N = 600) (Fig. [4d](#)).

How do different matching criteria affect creativity measurement? We experiment with restricting valid matches to verbatim only, and with allowing both verbatim and semantic matches. First, the creativity gap between humans and LLMs becomes even larger when considering semantic matches in addition to verbatim matches (Fig. [3d](#)). Averaged across all models, the CREATIVITY INDEX of human, based on both verbatim and semantic matches, is 102.5% higher than LLMs in novel writing (p = 2.6 × 10 -12 ; N = 600), whereas based on verbatim matches alone, the CRE-ATIVITY INDEX of human is 52.2% higher than LLMs. Second, semantic matches provide more signal for analyzing the uniqueness of longer n-grams (Fig. [3e](#)). For example, while the gap in L-uniqueness at L = 11 between human text and machine text from OLMo Instruct is 3.7% based on verbatim matches alone, this gap widens to 16.3% when considering both verbatim and semantic matches (p = 3.1 × 10 -7 ; N = 600). This indicates that although some of the longer n-grams in machine text may appear unique at the verbatim level, they are similar to certain text snippets in the reference corpus at the content level.

What impact does RLHF have on model creativity? RLHF aims to align model's outputs with human preferences, enhancing LLMs' ability to follow instructions and improving their safety and adaptability. To understand the impact of RLHF on model creativity, we compare the CREATIVITY Preprint INDEX of the LLMs before and after RLHF alignment. Specifically, we experiment with GPT Base [(Brown et al., 2020)](#b7), LLaMA 2 Base [(Touvron et al., 2023)](#b69), and OLMo Base [(Groeneveld et al., 2024)](#b30) and compare their creativity with their counterparts post-RLHF alignment. Our main finding is that the CREATIVITY INDEX of models after RLHF alignment is much lower than those before RLHF (Fig. [3f-g](#)). Based on verbatim match alone, the CREATIVITY INDEX of LLMs reduces by an average of 30.1% after RLHF (p = 1.3 × 10 -12 ; N = 600). Based on both verbatim and semantic matches, the CREATIVITY INDEX of LLMs decreases by an average of 8.9% after RLHF (p = 0.01; N = 600). We notice that the reduction of CREATIVITY INDEX after RLHF is noticeably larger when considering verbatim matches alone. We speculate that models might have learned certain linguistic styles preferred by humans during RLHF, leading to a decreased surface form diversity in its outputs.

How do overlapped n-grams distribute in the reference corpus? In addition to measuring the amount of matched n-grams in a given text, we also investigate the distribution of these n-grams in the reference corpus. We aim to understand whether these matched n-grams are spread across many documents or concentrated in a few. Specifically, we identify the top N documents that contain the highest amount of matched n-grams and result in the minimum L-uniqueness for a given text. This problem can be reduced to the maximum coverage problem [(Nemhauser et al., 1978)](#b56) and approximated using a greedy algorithm. Here, we consider both verbatim and semantic matches.

Our main finding is that the matched n-grams in machine texts are concentrated in fewer documents compared to human texts (Fig. [3h-j](#)). When searching over the top 50 documents, the averaged L-uniqueness (L = 5) for machine texts is 32.8%, which is 73.4% lower than human texts (mean: 56.6%; p = 3.9 × 10 -19 ; N = 600). Conversely, keeping L-uniqueness below 50% requires searching through an average of 41.2 documents for human texts, which is 213.7% more than for machine texts (mean: 13.4; p = 1.6 × 10 -22 ; N = 600). This implies that it's more likely to find some existing documents resemble models' generations than human writings.

How to measure creativity in LLMs trained on data outside of the reference corpus? The CREATIVITY INDEX of GPT-4 would be significantly inflated if computed using the RedPajama corpus, as RedPajama's cutoff date is two years earlier than GPT-4's knowledge cutoff, and GPT-4 is additionally trained on third-party private data that we don't have access to. We hypothesize that LLMs pre-trained on similar web data are likely to memorize and replicate similar patterns. As a result, when comparing the generations of these models, we expect them to be more similar to each other than to human texts, which often contain long-tail patterns. Therefore, to compare the creativity level of GPT-4 with humans, we use a model-generated reference corpus from newer open-weight models with knowledge cutoff dates similar to GPT-4, including the instruction-aligned versions of Gemma-7B [(Team et al., 2024)](#b67), Llama3-8B (AI@Meta, 2024), and Mixtral-7B [(Jiang et al., 2023)](#b37). Specifically, we randomly sample 150k sentences from the RedPajama corpus and prompt these models to generate document-level continuations. Based on the model-generated reference corpus, the average CREATIVITY INDEX of humans is 30.3% higher than GPT-4 in novel writing (p = 2.3 × 10 -12 ; N = 600) (Fig. [3k-l](#)). This suggests that while newer LLMs like GPT-4 may appear more creative when compared to public data, they still learn common patterns from their private training data and tend to emit similar patterns as other LLMs trained on comparable data.

How does the creativity vary among different groups of human? Human populations are diverse and complex, we aim to explore whether writings from different groups of humans exhibit varying levels of creativity. Specifically, we compare the creativity levels among three categories of writings: books published in 2023 from the BookMIA [(Shi et al., 2024)](#b65) dataset, classic literature by famous authors, and popular young adult fictions, both sampled from Goodreads' book lists. Our main finding is that classic literature exhibits a higher creativity level than the other two categories (Fig. [3m](#)). The average CREATIVITY INDEX of classic literature is 21.6% higher than young adult fictions (p = 2.7 × 10 -90 ; N = 3000), and 13.8% higher than books published in 2023 (p = 4.3 × 10 -120 ; N = 3000). We speculate that this elevated creativity in classic literature may stem from its complex themes and ideas, innovative literary techniques, and the richness of its language. In addition to the differences across categories, we also observed noticeable variance in creativity within each category. For example, the CREATIVITY INDEX of 'The Hunger Games' is 35.4% higher than 'Twilight' (p = 1.5 × 10 -19 ; N = 200), even though both books belong to the category of popular young adult fiction.

## Preprint

Can we leverage differences in creativity for detecting machine-generated text? Based on the creativity difference between humans and LLMs, we propose to use CREATIVITY INDEX as a criterion for zero-shot black-box machine text detection. Texts with higher creativity are more likely to be written by human. Our approach is ready to deploy out-of-the-box, requiring no training or prior knowledge of the text generator. In addition to creative writing tasks, we also test our method on detecting machine-generated fake news and theorem proofs. Detecting fake news is crucial for protecting the public from misinformation, while identifying model-generated solutions is important for regulating students' use of LLMs in their coursework. To obtain additional test data, we prompt LLMs to generate news articles based on the fake news headlines from the Misinfo Reaction Frames [(Gabriel et al., 2022)](#b25) and compare them with the real news articles from the XSum [(Narayan et al., 2018)](#b55) dataset. Meanwhile, we prompt LLMs to generate proofs for theorems from the Natu-ralProofs [(Welleck et al., 2022)](#b75) benchmark, and compare them with the ground-truth human-written proofs. The baselines we compare against includes the state-of-the-art zero-shot detector, Detect-GPT [(Mitchell et al., 2023a)](#), which uses the curvature of log probability as the detection criterion, as well as several supervised methods. These include OpenAI's RoBERTa-based detector, fine-tuned on millions of generations from various GPT-2 sized models, and the state-of-the-art supervised detector, Ghostbuster [(Verma et al., 2024)](#b73), fine-tuned on thousands of generations from ChatGPT. We measure performance using the area under the receiver operating characteristic curve (AUROC), which represents the probability that a classifier correctly ranks a randomly-selected human-written example higher than a randomly selected machine-generated example. Our method achieves new state-of-the-art performance in zero-shot detection: it consistently surpasses DetectGPT and Ope-nAI's detector across all domains, with significant improvements in AUROC-30.2% and 26.9%, respectively. It also outperforms the strongest supervised baseline, Ghostbuster-which requires expensive training and data collection-in five out of six domains, achieving an average AUROC improvement of 3.5% (Fig. [3n](#)).

## DISCUSSION

This work investigates the level of linguistic creativity in texts generated by LLMs and written by humans. Our findings suggest that the content and writing style of machine-generated texts may be less original and unique, as they contain significantly more semantic and verbatim matches with existing web texts compared to high-quality human writings. We hypothesize that this limited creativity in models may result from the current data-driven paradigm used to train LLMs. In this paradigm, models are trained to mimic human-written texts during the pre-training stage, and to produce outputs aligned with human preferences during the RLHF stage. As a result, models learn to generate fluent and coherent texts by absorbing and replicating common patterns observed in their training data. This reliance on existing text patterns can restrict their originality, as their outputs are inherently shaped by previously seen examples. In contrast, accomplished authors such as Hemingway go beyond simply mimicking the great writings of others; they craft their own narratives to express their unique opinions, perspectives, and insights, drawing from their personal experiences, emotions, and backgrounds, which translates to the more creative compositions of words and phrases that our method detects. Just as a DJ remixes existing tracks while a composer creates original music, we speculate that LLMs behave more like DJs, blending existing texts to produce impressive new outputs, while skilled human authors, similar to music composers, craft original works.

This work also faces the following limitations. Thirdly, the human authors that this work focuses on are those with relatively high-quality writings available in existing public datasets. While some human writings can be mediocre, tedious and unoriginal, we aim to assess how the creativity levels in impressive LLM outputs compare against the high-quality writings produced by professional human authors. Lastly, we acknowledge that the discussion surrounding the use of LLMs in social and industrial settings is highly complex, and our work here speaks only to a part of it. Besides the creativity of machine-generated content, other Preprint considerations in this discussion include socioeconomic factors and ethical implications, which fall beyond the scope of this paper.

## RELATED WORK

Measuring Creativity in Ideas: Measuring creative thinking and problem solving takes root in early work in psychology [(Torrance, 1966)](#b68), where researchers defined four pillars for creative thinking: fluency, flexibility, originality and elaboration. Crossley et al. ( [2016](#)) later on developed this notion and built on it to expand this to measuring creative writing in students, where they also adopted n-gram novelty for a measure of originality. However, these prior work focus on creativity in humans, and they also do not introduce any automated metrics or measurements.

Measuring Creativity in Machine-generated Text Using Expert Annotators: Closely related to CREATIVITY INDEX is a recent line of work in the generative AI literature comparing the creativity of human writers to that of large language models in different domains such as story telling and journalism [(Chakrabarty et al., 2023;](#b9)[2024;](#)[Anonymous, 2024)](#b2). Similar to us, the approach in this direction often involves prompting an LLM to write an original story or news article, based on some existing premise or press release, and then comparing the machine-generated text to the humanwritten counterparts. These works, however, take a rather subjective approach, where they define and measure creativity based on human expert annotations and whether people perceive the text to be more creative, rather than an objective quantification of novelty that we provide.

Measuring Novelty of N -grams: Finally, closely related to our work in terms of techniques is Nguyen (2024) and [Merrill et al. (2024)](#b49). The former attempts at finding n-gram rules that would cover and predict generations from transformer models, showing that more than 70% of the times transformers follow some pre-set patterns and rules. The latter is more similar to our work as they also measure the novelty of generated n-grams and compare it to human-written text, however they differ from us in tow major ways: (1) they only find verbatim matches, whereas we also match to approximate, semantically similar blocks of text and (2) they compute the percentage of n-grams of a certain length in a text that can be found in the reference corpus, whereas we measure how much of the text can be reconstructed by mixing and matching a vast amount of existing text snippets of varying lengths from the web.

Machine Text Detection: Detecting machine-generated text has been explored for several years using a variety of methods [(Jawahar et al., 2020;](#b36)[Uchendu et al., 2021)](#b71). [Gehrmann et al. (2019)](#b26) and [Dugan et al. (2023)](#b20) demonstrate that even humans tend to struggle to differentiate between text written by humans and machines, highlighting the need for automated detection solutions. Some approaches involve training a classifier in a supervised manner to identify machine-generated text [(Bakhtin et al., 2019;](#b4)[Uchendu et al., 2020)](#b70), while others use a zero-shot detection method [(Solaiman et al., 2019;](#b66)[Ippolito et al., 2020)](#b33). Additionally, there is research on bot detection through question answering [(Wang et al., 2023;](#b74)[Chew & Baird, 2003)](#b11). Recently, [Mitchell et al. (2023b)](#) introduced DetectGPT, a zero-shot method based on the hypothesis that texts produced by a large language model (LLM) are located at local maxima, and thus exhibit negative curvature, in the model's probability distribution. Follow-up work build on DetectGPT by making it faster [(Bao et al., 2024)](#b5) and proposing to use cross-detection when the target model is unknown [(Mireshghallah et al., 2024)](#b50).

## CONCLUSION

We introduce CREATIVITY INDEX, an interoperable and scalable metric designed to quantify the linguistic creativity of a given text by estimating how much of that text can be reconstructed by mixing and matching a vast amount of existing text snippets on the web. To efficiently compute the CREATIVITY INDEX, we developed DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. We find that the creativity index of professional human writers is, on average, 66.2% higher than that of LLMs. Notably, RLHF dramatically reduces the creativity index of LLMs by an average of 30.1%. Furthermore, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot black-box machine text detection. Our method not only surpasses Preprint the strongest zero-shot baseline, DetectGPT, by a significant margin of 30.2%, but also outperforms the strongest supervised baseline, GhostBuster, in five out of six domains. We hope that this study enhances the understanding of LLMs through the lens of linguistic creativity, and fosters informed usage of content created by LLMs in real-world applications.

## Preprint A METHOD DETAILS A.1 IMPLEMENTATION DETAILS OF DJ SEARCH

As discussed in the main text, the deployment of the CREATIVITY INDEX relies on efficiently determining whether each n-gram x i+i+n ∈ x can be found anywhere in the massive reference corpus C of publicly available texts. The function f (x i+i+n , C) is a binary indicator that determines whether an n-gram x i:i+n occurs in C. In line with the definition of CREATIVITY INDEX, we only consider the n-grams x i+i+n such that n ≥ L for some fixed constant L.

While a naive approach to checking whether x i+i+n appears in C for every n-gram x i+i+n ∈ x would take O(|x| 2 ) calls[foot_3](#foot_3) to f (see Algorithm 1), using a two-pointer approach we can radically reduce this to O(|x|) calls (see Algorithm 2). Note that a two-pointer approach does O(|x|) calls to f since in each iteration we advance at least one of the two pointers i and j by 1, and 0 ≤ i, j ≤ |x|.

$Algorithm 1 Naive Computation NGramsFound i,j ← False ∀ i ∈ [0..|x|) and j ∈ [0..|x|) ▷ matrix to store n-gram occurrence for i ∈ [0, 1, ..., |x| -L) do for j ∈ [i + L, ..., |x|) do NGramsFound(i, j) ← f (x i:j , C) end for end for return NGramsFound Algorithm 2 Efficient computation of DJ SEARCH(x, C) NGramsFound i,j ← False ∀ i ∈ [0..|x|) and j ∈ [0..|x|) ▷ matrix to store n-gram occurrence i ← 0, j ← L while j < |x| do NGramsFound(i, j) = f (x i:j , C)$if NGramsFound(i, j) then j ← j + 1 ▷ we will search for x i:j+1 next else i ← i + 1 ▷ since x i:j was not found, x i:j+k will not be found for all k > 0 j ← max(i + L, j)

▷ we only explore L-grams and beyond end if end while return NGramsFound

## A.2 IMPLEMENTATION DETAILS OF WORD MOVER'S DISTANCE

Let w be an n-gram. Let f (w, C) be the function that determines whether w appears in any text d ∈ C, either exactly or as a phrase that is highly similar in meaning to w (e.g., a paraphrase of w). Trivially, f (w, C) := d∈C f (w, d), and here on we will only discuss how to compute f (w, d).

An established approach for finding semantically similar phrases to a given n-gram w is to compute its embedding-embedding(w)-and then independently compute its similarity to the embeddings of all other n-grams to be analyzed. An embedding of a n-gram is a vector that represents the meaning of such n-gram in an k-th dimensional space of fixed size, enabling the comparison of similarity between concepts expressed in different surface forms. This comparison is typically done using cosine similarity, the scaled dot product between the two embeddings being compared. Text embeddings are generated by models specifically trained to this effect (e.g., BERT [(Devlin et al., 2019)](#b18), RoBERTa [(Liu et al., 2019)](#b47), SpanBERT [(Joshi et al., 2020)](#b39)) making their computation expensive at a large scale. Notably, text embeddings usually do not possess linearity, i.e. the embedding of concatenating n-grams w and v cannot be deduced from knowing embedding(w) and embedding(v), and instead needs to be computed from scratch.

Since our goal is to find the n-grams of d that are highly similar to w, using the traditional approach would entail comparing embedding(w) with the embeddings of all n-grams in C, which are approximately d∈C |d| 2 in number. Note that this also implies independently computing ≈ d∈C |d| 2 embeddings, which increases the computation costs significantly. Instead we use Word Mover's Distance [(Kusner et al., 2015)](#b42) (WMD), a method to estimate similarity between two n-grams by combining comparisons between pairs of word embeddings. This enables lifting the requirement to independently computing the embedding for each n-gram in C. Concretely, the Word Movers' Distance between two n-grams w and v is defined as follows:

$D w→v := 1 |w| i∈[0..|w|) min j∈[0..|v|)$1 -cosine similarity(embedding(v j ), embedding(w i ))

$= 1 - 1 |w| i∈[0..|w|) max j∈[0..|v|)$cosine similarity(embedding(v j ), embedding(w i ))

WMD(w, v) := max(D w→v , D v→w )

WMD also pre-filters the words considered in w and v to only include the content words in the analysis (i.e, discards stop-words, such as the, a, an, it, on, ...). Avid readers may notice that Algorithm 3 repeatedly computes the maximum over the same set, and sums of contiguous similarity scores; these can be pre-computed. Algorithm 4 shows these optimizations, resulting in an algorithm of time complexity O(|d|

## Note that D

$• |w| + |d| 2 |w|) = O(|d| 2 |w|),$assuming already computed word embeddings. Note that because there is a fixed vocabulary, all word embeddings as well as cosine similarities of word embedding pairs can be pre-computed.

We described how to compute f (w, d) for a single document d ∈ C, as we have already established that f (w, C) = d∈C f (w, d). To accelerate computation, and given that similar n-grams to x i:i+n are more likely to occur in texts similar to x, we select C's top most likely documents to contain w using a BM25Robertson & Walker (1994) index, denoted C ′ . We then approximate f (w, C) ≈ d∈C ′ f (w, d). As a final optimization, we note that it is unnecessary to compute the costly f (w, C) for finding semantically similar matches for w in the case where w appears exactly in C. To check if w appears exactly in C, we can leverage the existing, less expensive approach Infini-Gram [(Liu et al., 2024)](#b46) and search for the semantic similar matches only if Infini-Gram could not find any exact matches.

## Preprint

Please check if paragraph A contains any copies or quotations from paragraph B.

Here are some examples: Paragraph A: In the end though, I did the required reading, complained bitterly about being bored, wrote the requisite essay, and promptly forgot all about it. "He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish. In the first forty days ... Paragraph B: He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish. In the first forty days a boy had been with him. But after forty days without a fish the boy's parents had told him that the old man was now definitely and finally salao ... Answer: Yes Paragraph A: He was an old man who fished alone in a lobster boat off the Maine coast and he had gone 117 days without taking a crustacean. His luck was not bad, rather his judgment was good (don't fish the Atlantic in winter). Then he met us and for all I know his luck changed. El Campion is due for a change of luck ... Paragraph B: He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish. In the first forty days a boy had been with him. But after forty days without a fish the boy's parents had told him that the old man was now definitely and finally salao ... Answer: No Paragraph A: Santiago, the "old man who fished alone," in Hemingway's "The Old Man and the Sea" appears as one who has an undefeatable character, a loving, cheerful character, and very humble. The writer describes him in this way: "Everything about him was old except his eyes, and they were the same color as the sea ... Paragraph B: He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish. In the first forty days a boy had been with him. But after forty days without a fish the boy's parents had told him that the old man was now definitely and finally salao ... Answer: Yes Paragraph A: He was an old man who could see the form of his god, and a monk, moreover. Izzie had limited ability to communicate directly with her own deity. Much of her life she had proceeded by vague impressions and only glimpsed the great god's image briefly in the depths of meditation ... Paragraph B: He was an old man who fished alone in a skiff in the Gulf Stream and he had gone eighty-four days now without taking a fish. In the first forty days a boy had been with him. But after forty days without a fish the boy's parents had told him that the old man was now definitely and finally salao . 

## B EVALUATION B.1 MACHINE TEXT GENERATION

We experiment with state-of-the-art LLMs: [GPT-3 (Brown et al., 2020](#)) (text-davinci-003), ChatGPT [(Ouyang et al., 2022)](#b59) (gpt-3.5-turbo), LLaMA 2 Chat [(Touvron et al., 2023)](#b69), Tulu 2 [(Ivison et al., 2023)](#b35) and OLMo Instruct [(Groeneveld et al., 2024)](#b30) along with their base model before RLHF: GPT Base (Brown et al., 2020) (davinci-002), LLaMA 2 Base [(Touvron et al., 2023)](#b69) and OLMo Base [(Groeneveld et al., 2024)](#b30). These models are primarily pre-trained on the web data available before the cutoff date of the reference corpus RedPajama (Computer, 2023). We additionally discuss how to handle newer models, such as [GPT-4 (OpenAI et al., 2023)](#), which are largely trained on more recent web data and third-party private data, both of which fall outside the reference corpus RedPajama.

To obtain machine texts, we prompt LLMs to generate several paragraphs of novels, poems, or speeches, starting with an initial sentence taken from existing human writings in each category. To construct test data for machine text detection, we further prompt LLMs to generate news articles based on the fake news headlines from the Misinfo Reaction Frames [(Gabriel et al., 2022)](#b25) and to generate theorem proofs for questions from the NaturalProofs [(Welleck et al., 2022)](#b75) benchmark. The prompts used for each task are illustrated below. For all generations, we use nucleus sampling with p = 0.9 and set the maximum length of the generated texts to 288 tokens. To obtain model-generated reference corpus to compare the CREATIVITY INDEX of GPT-4 with humans, we randomly sample 150k sentences from the RedPajama corpus and prompt openweight LLMs with knowledge cutoff dates similar to GPT-4 to generate document-level continuations. The models we use are the instruction-aligned versions of Gemma-7B [(Team et al., 2024)](#b67) (gemma-7b-it), Llama3-8B (Meta-Llama-3-8B) (AI@Meta, 2024), and Mixtral-7B (Mistral-7B-v0.1) [(Jiang et al., 2023)](#b37). The prompt used to generate continuations is illustrated below. We use nucleus sampling with p = 0.9 and set the maximum length of the generated texts to 2048 tokens.

## Please generate a continuation for the following sentence: [PROMPT SENTENCE]

We additionally experiment with different prompt formats, intentionally encouraging creativity in models' generations by incorporating instructions such as 'push for creative ideas, unique emotions, and original twists,' 'be bold and creative,' or 'you are a creative writer.' Please see blow for a full list of the prompts we tried.

Write a few paragraphs for a novel from the following prompt, pushing for creative ideas, unique emotions, and original twists. Prompt: [PROMPT SENTENCE] Preprint Use the following prompt to write a few paragraphs for a novel with creative, unqiue perspectives or twists. Let your originality shine. Prompt: [PROMPT SENTENCE] Create a few paragraphs from the following prompt for a novel, focusing on novel ideas, emotions, or perspectives. Be as creative as possible. Prompt: [PROMPT SENTENCE] Write a few paragraphs for a novel based on the following prompt, exploring unexpected twists, emotions, or unique perspectives. Be bold and creative. Prompt: [PROMPT SENTENCE] Based on the following prompt, and write a few paragraphs for a novel that explore unexpected twists, deep emotions, or unique perspectives. Let your creativity flow, and don't be afraid to experiment with unconventional ideas or characters Prompt: [PROMPT SENTENCE] As a creative agent, write a few paragraphs for a novel based on the following prompt, bringing your novel ideas and original emotions to life. Prompt: [PROMPT SENTENCE] You are a creative writer, write a few paragraphs for a novel based on the following prompt. Explore unique perspectives and unexpected twists, and let your creativity guide you. Prompt: [PROMPT SENTENCE] You are a creative agent, free to shape this story in any direction. Write a few paragraphs for a novel based on the following prompt, using your imagination to uncover surprises and depth. Prompt: [PROMPT SENTENCE] As a creative writer, your task is to write a few paragraphs for a novel based on the following prompt. Dive into original ideas, explore emotions, and surprise yourself. Prompt: [PROMPT SENTENCE] You are a creative writer who brings stories to life. Write a few paragraphs for a novel based on the following prompt, letting your imagination take bold, unexpected turns. Prompt: [PROMPT SENTENCE]

## B.2 DATASET DETAILS

Reference Corpus: We use RedPajama [(Computer, 2023)](#), the largest web data collection available at the time of this study, as our reference corpus. RedPajama contains 100 billion text documents with 100+ trillion raw tokens from 84 CommonCrawl dumps.

Novel: For human-written novels, we use book snippets from the BookMIA [(Shi et al., 2024)](#b65) dataset. The BookMIA dataset contains approximately 10k book snippets, with an average length of around 650 words per snippet. We randomly sample 100 book snippets from the BookMIA dataset and select the first K sentences of each snippet such that their total length exceeds 256 words, to use as human text. Since novels we use were published after the cutoff date of RedPajama, there's no need for deduplication before DJ SEARCH.

Speech: For the transcripts of human speeches, we randomly sample 100 speeches from the famous speeches available in the American Rhetoric speech bank. For each speech, we randomly sample continuous K sentences such that their total length exceeds 256 words, to use as human text. Since these speeches were made before the cutoff date of RedPajama, deduplication is needed before DJ SEARCH.

Poem: For human-written poems, we randomly sample 100 poems from the popular modern poems collected by PoemHunter.com. Since these poems were published before the cutoff date of RedPajama, deduplication is needed before DJ SEARCH.

News Article: We use news articles from the XSum [(Narayan et al., 2018)](#b55) dataset as the human text for the machine text detection task. The Xsum dataset contains around 200k new articles, with an average length of around 380 words per article. We randomly sample 500 articles to use as human text. Since these news articles were released before the cutoff date of RedPajama, deduplication is needed before DJ SEARCH. For machine-generated fake news, we randomly sample 500 fake news headlines from the Misinfo Reaction Frames [(Gabriel et al., 2022)](#b25), and based on these headlines, LLMs are asked to generate corresponding news articles.

Theorem Proof: We use the ground-truth human-written proofs from the NaturalProofs [(Welleck et al., 2022)](#b75) dataset as the human text for the machine text detection task. The NaturalProofs dataset contains approximately 24k theorems and their corresponding proofs. We randomly sample 500 theorem-proof pairs and use the ground-truth proofs as human text. Since the NaturalProofs dataset was curated after the cutoff date of RedPajama, there's no need for deduplication before DJ SEARCH. For machine-generated math proofs, we prompt LLMs to write proofs for the 500 theorems we sampled.

## B.3 PARAMETERS OF DJ SEARCH

We set the minimum n-gram length L in DJ SEARCH to 5, and set the threshold for Word Mover's Distance to 0.95 for semantic matches. We observe that the L-uniqueness is close to zero for most human and machine texts when L ≤ 5 and close to one when L ≥ 12. Therefore, in practice, we sum up the L-uniqueness for 5 ≤ L ≤ 12 when computing CREATIVITY INDEX.

The only experiment with slightly different parameters is to compare the creativity of GPT-4 with humans. We observed that the L-uniqueness is close to one when L ≥ 7 based on the modelgenerated reference corpus. Therefore, we sum up the L-uniqueness for 5 ≤ L ≤ 7 when computing CREATIVITY INDEX.

## C RELATED WORK

Measuring Creativity in Ideas: Measuring creative thinking and problem solving takes root in early work in psychology [(Torrance, 1966)](#b68), where researchers defined four pillars for creative thinking: fluency, flexibility, originality and elaboration. Crossley et al. ( [2016](#)) later on developed this notion and built on it to expand this to measuring creative writing in students, where they also adopted n-gram novelty for a measure of originality. However, these prior work focus on creativity in humans, and they also do not introduce any automated metrics or measurements.

Measuring Creativity in Machine-generated Text Using Expert Annotators: Closely related to CREATIVITY INDEX is a recent line of work in the generative AI literature comparing the creativity of human writers to that of large language models in different domains such as story telling and journalism [(Chakrabarty et al., 2023;](#b9)[2024;](#)[Anonymous, 2024)](#b2). Similar to us, the approach in this direction often involves prompting an LLM to write an original story or news article, based on some existing premise or press release, and then comparing the machine-generated text to the humanwritten counterparts. These works, however, take a rather subjective approach, where they define and measure creativity based on human expert annotations and whether people perceive the text to be more creative, rather than an objective quantification of novelty that we provide.

## Preprint

Measuring Novelty of N -grams: Finally, closely related to our work in terms of techniques is [Nguyen (2024)](#b57) and [Merrill et al. (2024)](#b49). The former attempts at finding n-gram rules that would cover and predict generations from transformer models, showing that more than 70% of the times transformers follow some pre-set patterns and rules. The latter is more similar to our work as they also measure the novelty of generated n-grams and compare it to human-written text, however they differ from us in tow major ways: (1) they only find verbatim matches, whereas we also match to approximate, semantically similar blocks of text and (2) they compute the percentage of n-grams of a certain length in a text that can be found in the reference corpus, whereas we measure how much of the text can be reconstructed by mixing and matching a vast amount of existing text snippets of varying lengths from the web.

Machine Text Detection: Detecting machine-generated text has been explored for several years using a variety of methods [(Jawahar et al., 2020;](#b36)[Uchendu et al., 2021)](#b71). [Gehrmann et al. (2019)](#b26) and [Dugan et al. (2023)](#b20) demonstrate that even humans tend to struggle to differentiate between text written by humans and machines, highlighting the need for automated detection solutions. Some approaches involve training a classifier in a supervised manner to identify machine-generated text [(Bakhtin et al., 2019;](#b4)[Uchendu et al., 2020)](#b70), while others use a zero-shot detection method [(Solaiman et al., 2019;](#b66)[Ippolito et al., 2020)](#b33). Additionally, there is research on bot detection through question answering [(Wang et al., 2023;](#b74)[Chew & Baird, 2003)](#b11). Recently, [Mitchell et al. (2023b)](#) introduced DetectGPT, a zero-shot method based on the hypothesis that texts produced by a large language model (LLM) are located at local maxima, and thus exhibit negative curvature, in the model's probability distribution. Follow-up work build on DetectGPT by making it faster [(Bao et al., 2024)](#b5) and proposing to use cross-detection when the target model is unknown [(Mireshghallah et al., 2024)](#b50).

Various strategies have been developed to detect machine-generated text in real-world settings. One notable approach is watermarking, which embeds algorithmically detectable patterns into the generated text while maintaining the quality and diversity of the language model's outputs. Initial watermarking techniques for natural language were proposed by [Atallah et al. (2001)](#b3) and have been adapted for neural language model outputs [(Fang et al., 2017;](#b23)[Ziegler et al., 2019)](#b78). Recent advancements include [Abdelnabi & Fritz (2021)](#b0) work on an adversarial watermarking transformer (AWT) for transformer-based language models. Unlike methods dependent on specific model architectures, [Kirchenbauer et al. (2023)](#b40) introduce a watermarking technique applicable to texts generated by any common autoregressive language model. Application of LLMs in Creative Writing: Recent advancements have highlighted the potential of LLMs in supporting various creative writing endeavors, ranging from short stories [(Yang et al., 2022)](#b76) to screenplays [(Mirowski et al., 2023b)](#). Enhancing LLMs to produce text that aligns more closely with human preferences has made them adept at following user instructions, thereby turning them into valuable tools for individuals without technical expertise. This progress has boosted the commercial viability of LLMs as writing aids, which can continue a narrative, describe scenes, or offer feedback. [Chung et al. (2021)](#b12) conducted a review of literature on creativity support tools across various arts, leading to the development of a taxonomy that includes roles, interactions, and technologies. In contrast, [Frich et al. (2019)](#b24) and [Palani et al. (2022)](#b61) focused on how creative practitioners select new tools, highlighting their emphasis on functionality, workflow integration, and performance, and noting that personal recommendations often guide their choices. Additionally, [Gero et al. (2022)](#b27) created a space based on the cognitive process model of writing, influencing interface design decisions. [Gero et al. (2023)](#b28) further explored the social dynamics of AI in creative tasks, revealing a disconnect between writers' objectives and the support provided by computer tools.  

![Please draft an abstract for a paper with the following title: "Poetry Will Not Optimize; or, What Is Literature to AI?"]()

![maneuvering a billion-piece puzzle of psychology and emotion, spirituality and intricacies of language. Even though my puzzle keeps changing as I change and … … existing research in the field of family migration. Through a critical analysis of the relationship between family and (in)security the article offers nuanced insight into … … examination of the role of human creativity in the age of AI. He argues that while AI may be able to produce creative works on its own, it is ultimately humans … … because the ability of automated systems to be able to generate text that resembles what a human might say is huge. If we can just improve question …]()

![Figure 1: a: Example outputs from DJ SEARCH. We asked ChatGPT to generate an abstract based on the title of Prof. Michele Elam's paper, "Poetry Will Not Optimize; or, What Is Literature to AI?" (Elam, 2023) The abstract generated by ChatGPT contains significantly more verbatim and near-verbatim matches with existing texts on the web compared to the original abstract written by Prof. Elam. b: Definition of CREATIVITY INDEX. CREATIVITY INDEX is mathematically equivalent to the area under the L-uniqueness curve across a range of minimum n-gram lengths L. The L-uniqueness of ChatGPT is noticeably lower than that of proficient human writers across various context granularities (i.e., n-gram lengths) in all domains, leading to a significantly higher CREATIVITY INDEX for human writers compared to ChatGPT.]()

![Figure 3: a-c: CREATIVITY INDEX in novel writing (a), poetry composition (b) and speech writing (c) based solely on verbatim matches. d: CREATIVITY INDEX in novel writing considering both verbatim and semantic matches.e: L-uniqueness in novel writing with respect to the minimum n-gram length L for humans and OLMo. f-g: CREATIVITY INDEX of LLMs before and after RLHF in novel writing, based solely on verbatim matches (f) and based on both verbatim and semantic matches (g). h: L-uniqueness in novel writing with respect to number of documents in the reference corpus. i: L-uniqueness when search over the top 50 documents in novel writing. j: The number of reference documents required to keep L-uniqueness below 50% in novel writing. k-l: CREATIVITY INDEX of GPT-4 compared to humans in novel writing based on verbatim matches, using a machinegenerated reference corpus sourced from the instruction-aligned version of Gemma-7B, Llama3-8B, and Mixtral-7B, as well as a combination of all three. m: CREATIVITY INDEX of different groups of human writers. n: Detection AUROC across various domains: our approach sets a new state-ofthe-art for zero-shot detection, even surpassing supervised baselines.]()

![Firstly, the computation of the CREATIVITY INDEX is constrained by the reference corpus used for DJ SEARCH. While open-source LLMs such as OLMo rely on publicly available texts from the internet for their training data, major companies like OpenAI additionally curate private data to train their closed-source LLMs such as ChatGPT. Without incorporating these private data into the reference corpus of DJ SEARCH, the CREATIVITY INDEX of closed-source LLMs may be somewhat inflated. Secondly, the overlap with existing texts identified by DJ SEARCH in models' generations may not conclusively indicate memorization of a specific document. It's possible that these text fragments, or their variations, appear in multiple documents that the model has been trained on, including those outside the reference corpus of DJ SEARCH.]()

![w→v 's definition is asymmetric (D w→v ̸ = D v→w ). Thus, we consider the Word Movers' Distance of two n-grams w and v as the maximum of D w→v and D v→w : w and v are highly similar if their distance is below a threshold δ for both D w→v and D v→w (See Algorithm 3):WMD(w, v) = max(D w→v , D v→w ) < δAlgorithm 3 Conceptual writeup of f (w, d) using Word Mover Distance (WMD) to find the ngrams of a single text d ∈ C that are highly similar to the n-gram w and are of length ≥ L. procedure DIRECTIONALWMD(w, v) return 1 -1 |v| j∈[0..|v|) max i∈[0..|w|) cosine similarity(embedding(w i ), embedding(v j )) end procedure for a ∈ [0, 1, ..., |d|) do for b ∈ [a + L, ..., |d|] do symmetricWMD ← max(directionalWMD(d[a : b), w), directionalWMD(w, d[a : b))) if symmetricWMD < δ then return True end if end for end for return False]()

![Please write a few paragraphs for a novel starting with the following prompt: [PROMPT SENTENCE] Please write a poem starting with the following line: [PROMPT LINE] Please write a speech starting with the following sentence: [PROMPT SENTENCE] Please write a news article based on the given headline: [NEWS HEADLINE] Please provide a proof for the following theorem: [THEOREM QUESTION]]()

![Figure 5: Example outputs from DJ SEARCH based on both verbatim and semantic matches. We prompt LLMs to generate a few paragraphs of a novel, beginning with a first sentence taken from a human-written novel snippet.]()

![Figure 13: Example outputs from DJ SEARCH based on verbatim matches. We prompt LLMs to generate a few paragraphs of a novel, beginning with a first sentence taken from a human-written novel snippet.]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

![]()

Our code and data is available at https://github.com/GXimingLu/creativity_index

The name DJ SEARCH is inspired by the way a DJ creates a remix by blending pieces of existing music.

The percentage difference computed using the formula: CREATIVITY INDEX (human) -CREATIVITY INDEX (model) CREATIVITY INDEX (model)

There are (|x| -L)(|x| -L + 3)/2 spans to analyze if L is the minimum n-gram length to be considered.

