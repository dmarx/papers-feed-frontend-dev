- Decision to utilize Reinforcement Learning from Human Feedback (RLHF) for alignment
- Choice of Proximal Policy Optimization (PPO) as an RLHF technique
- Adoption of Direct Preference Optimization (DPO) as a reparameterization of RLHF
- Introduction of Direct Preference Heads (DPH) for preference signal extraction
- Selection of objective functions for DPH: Separable DPH and Contrastive DPH
- Decision to use a single model for both response generation and reward scoring
- Choice of datasets for fine-tuning and evaluation (GLUE, RACE, etc.)
- Implementation of specific prompt templates for training and evaluation
- Strategy for sampling from datasets during Supervised Fine-Tuning (SFT)
- Use of label smoothing in the DPH objective functions
- Decision to omit certain tasks (e.g., WNLI) during fine-tuning
- Choice of activation functions for pooling in the reward head
- Evaluation metrics for assessing model performance on various tasks
- Decision to release model weights on Hugging Face and code on GitHub
- Approach to handling noisy labels in training through cDPO
- Theoretical analysis of the objective function and its implications for model behavior