# Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads

## Abstract

## 

Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.

## Introduction

Reinforcement Learning from Human Feedback (RLHF) is a technique that can be used to align an agent -such as a Large Language Model (LLM) -to human preferences and lead to more truthful, more helpful, less harmful and more preferred outputs [[31]](#b30). Proximal Policy Optimization (PPO) [[38]](#b37) and Direct Preference Optimization (DPO) [[33]](#b32) are two such aligment techniques which have been extensively used to improve the quality of LLM outputs, leading to instruction following agents or chat assistants which are quickly approaching human-baselines in a variety of knowledge and reasoning tasks [[5,](#b4)[11,](#b10)[43,](#b42)[20,](#b19)[26,](#b25)[37,](#b36)[12]](#b11).

However, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it. One study [[6]](#b5) discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [[4]](#b3) discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters. Ouyang et al. [[31]](#b30) also reports an increased tendency for RLHF models to make up information in closed domain tasks ("hallucination") compared to models trained with SFT alone.

To combat the the risk of RLHF compromising the abilities of an LLM in favor of producing preferable outputs we introduce Direct Preference Heads (DPH), a novel feature based approach that optimises a reward score produced by the LLM rather than optimising the logits produced by language modelling head. DPH can be used in combination with (or without) existing alignment techniques to allow language models to self-evaluate outputs sampled at inference time and select the highest scoring candidate.

We evaluate the performance of DPH using an efficient 551M parameter LM on a variety of commonsense reasoning and Natural Language Understanding (NLU) tasks. All code used to train our models is available on GitHub and we release our model weights on Hugging Face.

## Prior Approaches

Prior approaches to language model alignment involve directly optimizing the logits produced by the language modelling head to increase the likelihood of producing preferable responses while decreasing the likelihood of undesirable responses.

## Reinforcement Learning from Human Feedback (RLHF)

Reinforcement Learning from Human Feedback seeks to learn a reward model from human feedback on completions generated by a language model which can be used to align an LM with human preferences. A typical RLHF pipeline consists of 3 steps: (1) supervised fine-tuning, (2) preference sampling and reward modelling, and (3) RL fine-tuning.

Supervised Fine-Tuning The first step of a standard RLHF pipeline is fine-tuning a pre-trained LM on high quality data for downstream tasks to obtain a model π SFT .

Reward Modelling Next, the SFT model is prompted with input tokens x to produce completions y. These answers are then rated by human labelers which rate the answers based on one or more criteria. A reward model r ϕ (x, y) is then trained to estimate the scores assigned by human labelers using maximum likelihood estimation.

RL Fine-Tuning During the RL phase the learned reward function is used to provide feedback to the language model using the following optimization problem

$max π θ E x∼D,y∼π θ (y|x) [r ϕ (x, y)] -βD KL [π θ (y|x)||π ref (y|x)](1)$where β controls the deviation from the base reference policy π ref , which is typically initialized from π SFT . Due to the non-differentiable nature of language generation this objective must be optimized using a reinforcement learning algorithm such as PPO [[38]](#b37).

## Direct Preference Optimization (DPO)

Direct Preference Optimization was introduced as a reparameterization of RLHF which eliminates both the sampling stage and the reward modelling stages and reformulates alignment procedure as a loss function which can be optimized directly on a dataset of pairs of preferred and dispreferred completions to given prompts. This allows DPO to stably and efficiently converge on an optimal policy using what is effectively a classification loss over positive and negative pairs.

Given a dataset {(x, y w , y l )} where x is the prompt and y w , y l are the preferred and dispreferred completions, we introduce the following loss function:

$L DPO (x, y w , y l ) = -log σ β log π θ (y w |x) π ref (y w |x) -β log π θ (y l |x) π ref (y l |x)(2)$where π θ (y * |x) and π ref (y * |x) are the probabilities of completions y * for prompt x given by the policy model and reference models respectively, and the β parameter controls the deviation from the reference policy.

There also exists an augmentation of DPO namely Conservative DPO (cDPO) [[28]](#b27) which is designed to be more robust to noisy labels through the introduction of label smoothing parameter ϵ. The objective function for cDPO is given by:

$L cDPO (x, y w , y l ) = (1 -ϵ)L DPO (x, y w , y l ) + ϵ L DPO (x, y l , y w )(3)$
## Direct Preference Heads

The hypothesis underlying the Direct Preference Optimization framework of Rafailov et al. [[33]](#b32) is that a "language model is secretly a reward model" thereby making the purpose of Direct Preference Heads to exploit this and extract explicit reward signals without the need of an additional reward model.

## Reward Head

To obtain the rewards from a sequence x; y three components are required: an aggregated hidden state h which is conditioned on the intermediate representations of the language model, a pooling function f which transforms the hidden state, and a learnable vector w dph with the same dimension as the output of f . We then compute the reward r as follows:

$r = f (h) • w dph(4)$To obtain the hidden state we take the output of the last transformer layer for the final token of the sequence, and we experiment with three choices of f : (1) the identity mapping following the convention established by OpenAI's GPT for sequence classification [[32]](#b31), (2) a learnable affine projection with tanh nonlinearity following BERT's pooling function [[15]](#b14), and (3) an inverted bottleneck FFN with SwiGLU activation mirroring the FFN blocks used within the transformer backbone followed by tanh nonlinearity [[39]](#b38).

## Objective Function

We formulate two novel objective functions for our method: a separable objective which maximises positive rewards and minimises negative rewards, and a contrastive objective which maximises the margin between positive and negative rewards. The loss landscapes are illustrated by Figure [1](#fig_1) in the appendix.

## Separable DPH

The Separable DPH loss function given by ( [5](#formula_4)) is a function of the preferred and dispreferred rewards r w , r l , and the label smoothing parameter 0 ≤ ϵ ≤ 0.5 which controls the reward margin.

$L SepDPH (r w , r l ) = -[(1 -ϵ) log σ(r w ) + ϵ log σ(-r w )] -[ϵ log σ(r l ) + (1 -ϵ) log σ(-r l )](5)$Theorem 1. For all ϵ ∈ (0, 0.5] the objective function L SepDPH is convex and will optimize the policy π θ such that the preferred rewards r w produced by the preference head converge towards log 1-ϵ ϵ and the dispreferred rewards r l converge to log ϵ 1-ϵ .

This can be proven by observing the first and second partial derivatives of the loss function with respect to the rewards. The first partial derivative is equal to zero at the points r w = log 1-ϵ ϵ and r l = log ϵ 1-ϵ respectively, and the second partial derivative is strictly positive for all values of r w , r l . A full proof is included in Appendix A.1.

## Contrastive DPH

Like Separable DPH, the loss function for Contrastive DPH given by ( [6](#formula_5)) is function of the preferred and dispreferred rewards r w , r l and the label smoothing parameter 0 ≤ ϵ ≤ 0.5. This version of the loss function optimizes the relative margin between the rewards rather than optimizing the absolute positive and negative rewards as in Separable DPH.

$L ConDPH (r w , r l ) = -(1 -ϵ) log σ(r w -r l ) -ϵ log σ(r l -r w )(6)$Theorem 2. For all ϵ ∈ (0, 0.5] the objective function L ConDPH is convex and will optimize the policy π θ such that the difference between preferred rewards r w and dispreferred rewards r l produced by the preference head will converge to a fixed margin, given by r ∆ = r w -r l = log 1-ϵ ϵ . This can be proven by reparameterising the loss function such that r ∆ = r w -r l and by then considering the first and second partial derivatives with respect to this reward margin. It can be observed that the first partial derivative is equal to zero when r ∆ = log 1-ϵ  ϵ , and the second partial derivative is strictly positive for all values of r ∆ . A full proof is included in Appendix A.2.

## Relation to cDPO

The properties of both Contrastive DPH and Seperable DPH show a strong relationship with Conservative DPO: SepDPH will converge to optimal fixed reward margins above zero for r w and below zero for r l ; ConDPH will converge to optimal fixed reward margins between r w and r l , and cDPO will converge to a fixed delta from the reference model [[28]](#b27). Like Conservative DPO, this makes both Seperable DPH and Contrastive DPH robust to preference label noise and makes training more stable than naive maximum likelihood estimation without label-smoothing.

## Novelty over Traditional Reward Modelling

Although similar to the reward modelling phase of an RLHF pipeline, DPH has some distinct differences which set it apart. DPH does not require an SFT sampling and human labelling stage meaning it can take advantage of pre-constructed preference datasets such as those used for DPO. Typical RLHF also requires multiple models -a reward model, a reference model and a policy model -while DPH requires only a single model to produce both responses and rewards. Unlike other RLHF pipelines such as PPO [[38]](#b37), the rewards produced by DPH are not used for RL fine-tuning; instead, the DPH rewards are to be used to prune candidate generations sampled from the LM at inference time to select the candidate which aligns most with human preferences. This makes DPH an excellent choice for small language models which are (1) more lightweight -and therefore can be efficiently used to generate multiple samples -and, (2) are more prone to degradation when aligned using typical RL techniques [[6,](#b5)[4]](#b3).

## Experimental Setup and Data

## Datasets

We make use of a variety of datasets for fine-tuning and evaluation which are outlined below. The specific prompt templates used for fine-tuning and evaluation are described in Appendix C.

Natural Language Understanding (NLU) For general NLU we make use of the standard GLUE benchmark [[42]](#b41). The overall score for GLUE is computed by the macro-average of unweighted metric averages for all 9 tasks, however we also include a secondary score which does not included the 'problematic' WNLI task following the evaluation used for BERT [[15]](#b14). We opted to omit WNLI during fine-tuning due to the low sample size.

Commonsense Reasoning In accordance with the GPT4All [[1]](#b0) evaluation suite, we use the following datasets to evaluate commonsense reasoning abilities: HellaSwag [[43]](#b42), OpenBookQA [[27]](#b26), WinoGrande [[37]](#b36), ARC [[11]](#b10), BoolQ [[10]](#b9), and PIQA [[8]](#b7).

Reading Comprehension To evaluate reading comprehension abilities we use the RACE dataset [[24]](#b23), a multiple-choice task which requires reasoning over provided passages.

Instruction Following We include the Alpaca [[40]](#b39), OpenOrca [[25]](#b24), and UltraFeedback [[13]](#b12) datasets to train our models for instruction following. We make use of OpenOrca and a cleaned version of Alpaca for SFT, and binarized versions of OpenOrca and UltraFeedback for alignment.

Auxiliary Datasets To provide additional training data for SFT we include the MMLU [[20]](#b19), SQuAD V2 [[35,](#b34)[34]](#b33), Tiny Stories [[16]](#b15), CNN-Dailymail [[29]](#b28) and CoQA [[36]](#b35) training splits. For alignment we only include MMLU and SQuAD V2.

## Prompts and Sampling

Prompts We make use of the ChatML prompt templating scheme [[30]](#b29) with handcrafted system, user and assistant prompts specific to each task. During fine-tuning we mask out the loss for all tokens of the prompt and condition the model on the content of assistant messages including the final <|im_end|> token. During evaluation we select the highest scoring answer using the average log-probabilities of the tokens in the final assistant message, or compute the reward scores on the final <|im_end|> token when evaluating with DPH.

SFT Sampling When sampling from the datasets for SFT we randomly shuffle each dataset and uniformly interleave samples from all tasks in the mix. To control the weighting of samples from each task we fill the context window with n consecutive samples from the same task before sampling from a different task, where n is chosen to be 5 in our experiments. To maximise compute utilisation and minimize unused portions of the context window we make us of Transformer-XL [[14]](#b13) style training with a context window size of 2048 tokens and a recurrent memory size of 2048 tokens.

DPH Sampling When sampling from datasets for DPH alignment we switch from the Transformer-XL style pipeline to typical SFT training, opting to only include single samples in the context window padded to a fixed maximum length. As some of the datasets we use for DPH are intended for SFT rather than alignment (namely GLUE, GPT4All, RACE, MMLU and SQuAD) we synthesise preference pairs where the 'correct' answer is used as the preferred completion and we uniformly sample an 'incorrect' answer from the available choices for the dispreferred completion. This is trivial for most datasets, however we use a special process for the SQuAD V2 dataset; for answerable questions we use "unanswerable" as the dispreferred completion, and for unanswerable questions we use SpaCy to randomly sample a noun span from the context to use as the dispreferred completion.

## Regularization

The hidden states h used to compute the reward scores are likely sub-optimal for computing rewards when initialising π θ from π SFT . As such, it may be desirable to fine-tune some or all parameters in the language model to learn better reward signals. This necessitates the use of regularization to prevent degradation of the models generative capabilities while learning to predict rewards.

Prior Regularization Typical parameter regularization strategies such as weight decay make the assumption that parameters θ follow a zero-mean Normal distribution p(θ) ∼ N (0, 1 β I) leading to an auxiliary loss term β 2 ||θ|| 2 2 . However, when performing transfer-learning or fine-tuning on a pre-trained model this assumption can be harmful and aid in catastrophic forgetting of the model's previously learnt abilities.

An alternative regularization scheme is Prior Regularization [[9,](#b8)[2,](#b1)[18]](#b17) which instead makes the assumption that the fine-tuned parameters are normally distributed around the original parameters

$θ ref , that is θ ∼ N (θ ref , 1 β I), leading to the auxiliary loss term β 2 ||θ -θ ref || 2 2 .$We employ Prior Regularization to limit the divergence of π θ from π SFT while still facilitating the learning of improved hidden state representations for the Direct Preference Head. Pseudocode for optimizer based decoupled prior regularization is included in Appendix B.1.

cDPO Regularization Rather than directly employing a KL Divergence penalty similar to that used in (1) we find that it is possible -and even beneficial -to use Conservative DPO as a means of (1) limiting the divergence of the policy model to a fixed delta from the reference model, and

(2) 'nudging' the model towards generating more preferable outputs which increases the chance of generating a better candidate completion at inference time with fewer sampling steps.

## Training Pipeline

We progressively fine-tune the models in 3 stages: vocab extension, supervised fine-tuning, and DPH alignment. The details of the pre-trained model are included in Appendix D.1.

Vocab Extension Since our model was pre-trained without a chat structure it is necessary to train the embeddings for additional <|im_start|> and <|im_end|> tokens: we freeze all non-embedding parameters and use the same datasets as SFT. We fine-tune the embeddings for 4096 steps with a batch size of 128, a max LR of 6e-5 which warms up over 200 steps followed by cosine decay down to zero, and clip the global gradient norm to 1.

Supervised Fine-Tuning After vocab extension we move onto the SFT step which conditions the model for NLU tasks and instruction following using the sampling and loss masking method described in section 4.2. We fine-tune the model for 6144 steps with a batch size of 128, a max LR of 3e-5 which warms up over 200 steps followed by cosine decay down to zero, prior-regularization applied to all non-embedding parameters with coefficient 0.5, and clip the global gradient norm to 1.

DPH Alignment Using the sampling method described in section 4.2 we jointly learn DPH rewards and perform cDPO alignment. The goal here is to gently push the model towards producing preferable outputs without compromising the model's reasoning abilities, and the priority is to attain the highest validation metrics from the DPH rewards. This requires balancing the two objectives, and as such we introduce weighting parameters α 1 , α 2 to our final joint objective in [(7)](#b6) where L DPH is either L sepDPH or L conDPH . We find α 1 , α 2 = 1 to be a good blance between DPO and DPH in our experiments.

L joint (x, y w , y l , r w , r l ) = α 1 L cDPO (x, y w , y l ) + α 2 L DPH (r w , r l )

We align the model for 23040 steps with a batch size of 64 pairs, a max LR of 3e-6 which warms up over 200 steps followed by cosine decay down to 3e-7, prior-regularization applied to all parameters with coefficient 0.5, and clip the global gradient norm to 1. Following the optimal DPO parameters for OpenHermes-7b-2.5 [[22]](#b21) we use β = 0.6 and chose cDPO ϵ = 0.25 and DPH ϵ = 0.1 for regularisation. Additionally, we apply dropout with p = 0.1 to the outputs of the pooler.

## Compute Resources

All fine-tuning was performed using an NVIDIA A100 SXM4 80GB GPU on a compute cluster, with jobs allocated 24 cores and 160GB of memory. Each checkpoint is saved in FP16 format which consumes about 1.1GB of storage, and the datasets require minimal storage space.

For vocab extension we train for 4096 steps with an average of 7.99 seconds of compute per step which translates to about 9 hours. For supervised fine-tuning we train for 6144 steps with an average of 9.26 seconds of compute per step which translates to about 16 hours. For DPH alignment we train for 23040 steps with an average of 7.21 seconds of compute per step which translates to about 46 hours. The DPH ablations with our models use about 140 hours of compute, and the Qwen ablations use about 60 hours of compute. In total, we used approximately 270 hours of A100 compute to train our models and collect the results included in our paper. We used additional compute for preliminary tests and fixing bugs for silently failing experiments although this wasn't tracked.

## Results

## Evaluation Methodology

As described in Section 4 we use NLU, commonsense reasoning and reading comprehension tasks to measure model capabilities, while the instruction following and auxiliary tasks are used to provide additional training signals. For the NLU tasks we evaluate on the test set of GLUE, providing average scores both with and without WNLI. For reading comprehension we evaluate on the RACE test set.

For commonsense reasoning we follow the LM Evaluation Harness [[17]](#b16) implementations of these tasks, evaluating on the test sets of ARC and OpenBookQA and the validation sets of HellaSwag, WinoGrande, BoolQ and PIQA, which brings our evaluations in line with other models.

For vocab extension and SFT checkpoints we obtain model predictions from the completions with the highest scoring log-probabilities. For the DPH checkpoints we report metrics for both logprobability predictions (Ours DPO ) and predictions chosen from the DPH rewards (Ours DPH ). We use the SwiGLU-based pooler with the separable objective function for all our experiments as we found this combination to perform best overall as shown in Section 5.2.1.

## Natural Language Understanding

Our results for NLU performance are included in Table [1](#tab_0). Note that the results for GPT-1 [[32]](#b31) and BERT [[15]](#b14) are from sub-task specific fine-tunes. It is unsurprising that our model does not outperform BERT Large even though it has more parameters; this is likely due to BERT's task specific fine-tunes in comparison to our model which was jointly trained on several tasks. Despite this our instruction following DPH model achieves a 2.2% higher average GLUE score compared to task-specific GPT-1 fine-tunes and manages to attain the highest overall accuracy and macro-average on RTE and STS-B respectively.

## Commonsense Reasoning

Our results for commonsense reasoning are summarized in Table [2](#tab_1). Note the Pythia [[7]](#b6) and TinyLlama [[44]](#b43) models were not fine-tuned for any specific task but received significantly more pre-training and have much higher parameter counts. With SFT alone we are able to attain comparable performance to TinyLlama using half as many parameters, and when applying DPH alignment we achieve a 7.2% increase over the TinyLlama average score and the highest accuracy in 5 of the 7 tasks.

## Reading Comprehension

Our results for reading comprehension are included in Table [3](#tab_2). The results for GPT-1 were taken from a RACE specific fine-tune, and the results for LLaMA [[41]](#b40) were zero-shot without fine-tuning. Our SFT baseline achieves a higher average accuracy on RACE compared with the non fine-tuned LLaMa models but cannot match the accuracy of the RACE specific GPT-1 fine-tune; however after alignment our model attains a 3.5% higher average over GPT-1 while still maintaining excellent scores on other tasks using the same model weights.

## Ablations

## Pooling Head Function and Objective Choice

We ablate over the three pooling head and two objective function choices. We perform alignment for 7680 steps and report the validation scores in Table [4](#tab_3). For both separable and contrastive objectives the SwiGLU pooler performs best on the three benchmarks, and for both GLUE and RACE the separable objective performs best overall. However during these experiments we discovered that contrastive DPH was achieving higher scores than separable DPH for specifically the sentence completion style tasks like HellaSwag, WinoGrande and PIQA. We hypothesise this is caused by situations where multiple completions to a given prompt may be plausible even though there is only one 'gold' answer, and as such the model benefits from maximising the relative reward margin with the contrastive objective rather than optimising absolute rewards with the separable objective.

## Task Specific Heads

By taking the DPH checkpoint and freezing all backbone parameters it is possible to learn task specific heads and pooling functions for different downstream tasks at the cost of only 19M parameters per task. We train new heads for the three task groups and plot the confusion matrix of each head for each task average in Table [5](#tab_4). We further fine-tune for an additional 7680 steps on each task group using the same training setup as DPH alignment. Unsurprisingly the GLUE and GPT4All heads achieve the highest scores for GLUE and GPT4All benchmarks respectively, however the GPT4All head manages to outperform the RACE head on the RACE benchmark. We hypothesise this may be due to the inclusion of muliple choice QA and reading comprehension tasks in GPT4All which may prove better training signals than the RACE training data alone.

## Model Ablations

Our final experiments involve exploring the behaviour of DPH when applied to frozen language models in an ad-hoc fashion. We experiment using the Qwen 1.5 model family [[3]](#b2) and train only the pooler and reward head weights, reporting results in Table [6](#tab_5). We use an identical training setup to DPH alignment but disable dropout due to the low number of trainable parameters.

Because the model backbone and embeddings remain frozen during alignment the 'Log' scores represent the model's pre-trained (or fine-tuned) capabilities. When observing the difference between the Log scores of the 0.5B Qwen models it is evident that the fine-tuning and alignment used to transform the pre-trained model into the "chat" model resulted in degraded performance across the 3 tasks. This phenomenon is less apparent for the 1.8B models, and actually results in higher GLUE scores for the "chat" variant of the model. This further confirms the hypothesis that alignment can harm the reasoning capabilities of smaller language models. For all models DPH is consistently able to attain higher scores on the GLUE tasks compared to the log probabilities produced by the language modelling head, but the opposite is observed for RACE which suggests the hidden states produced by the frozen backbone do not contain rich enough features for long range modelling tasks such as reading comprehension. We also observe the "chat" variants produce higher task scores for DPH than the non-chat variants which we hypothesise is a result of the authors' fine-tuning with the Chat-ML format which lead to the models' greater understanding of message structure and therefor improved hidden state aggregation for the final end message token.

When we combine these findings with those presented in Section 5.2.2, it becomes evident that the pooling function and reward head exhibit slower convergence when the model backbone is frozen. This observation further supports our hypothesis in Section 4.3, indicating that the hidden states generated by the models are are initially sub-optimal and that further fine-tuning is necessary to optimize these hidden states to achieve the best features for DPH.

6 Discussion

## Future Work

As shown in the results section, DPH is capable of learning to assign higher rewards to preferred outputs and lower rewards to dispreferred outputs which implies the pooling function learns rich features with respect to prompt-completion pairs. We believe that it would be possible to also extract additional information from the output of the pooling function to detect finer grained signals such as helpfulness, humor, creativity, toxic content, etc. This can be achieved by training on a conversational dataset such Open Assistant [[23]](#b22) which contains a variety of human-curated labels in addition to machine-generated labels produced by Detoxify [[19]](#b18).

## Limitations

The main benefit of DPH being its ability to perform alignment without directly effecting the model's output distribution is also its main limitation: unlike other alignment techniques which can help prevent the model generating harmful outputs, DPH is only capable of detecting harmful outputs.

Although we do include DPO alignment in our experiments to reduce the likelihood of harmful outputs, DPH does not require such model alignment to function, which shifts the responsibility of rejecting harmful outputs to the end user or service provider.

## Conclusion

In this paper we introduced Direct Preference Heads, a novel form of language model alignment which is performed at inference time to prune candidate completions for a given prompt. Unlike other alignment techniques which coerce the model into generating human preference aligned outputs, DPH instead produces reward scores for candidate outputs without affecting the actual generation process and therefor avoids the issue of RLHF leading to degraded performance when applied to smaller language models. We formulated two loss functions for DPH and find strong connections to Conservative DPO, implying that DPH is robust to label noise and can be tuned to a specific confidence margin. Finally, we evaluated our methods on a number of NLU, commonsense reasoning and reading Comprehension tasks and found that DPH is able to consistently outperform both our SFT baseline and multiple publicly available language model checkpoints of varying size and training volume.

## Broader Impacts

As with all language modeling systems we cannot guarantee all responses produced by our models are factually correct nor can we guarantee that they are safe and free from harmful content. Our work focuses on creating a system that helps filter out incorrect and harmful messages by scoring candidate outputs, but as with all alignment techniques our models may be susceptible to so-called 'jailbreaks' which can coerce the model into incorrectly assigning a higher score to less desirable content. To maximise safety DPH should be implemented alongside other safety guardrails such as Llama Guard [[21]](#b20) when used for publicly facing chat systems, and we intend for our provided model checkpoints to be used for reproduction of results and further research in the field of alignment.

## A Appendix -Theory

A.1 Full Proof of Theorem 1

We can prove Theorem 1 by examining the partial gradients with respect to the rewards.

$∂ ∂rw L SepDPH (r w , r l ) = ϵ - 1 e rw + 1 (8a) ∂ ∂r l L SepDPH (r w , r l ) = 1 e -r l + 1 -ϵ(8b)$From equations 8a and 8b we find that the partials gradients are both equal to zero at the points r w = log 1-ϵ ϵ and r l = log ϵ 1-ϵ respectively. It is also interesting to note that log 1-ϵ ϵ + log ϵ 1-ϵ = 0 which implies the positive and negative rewards will converge to an equal distance from 0.

$∂ 2 ∂r 2 w L SepDPH (r w , r l ) =$e rw (e rw + 1) 2 (9a)

$∂ 2 ∂r 2 l L SepDPH (r w , r l ) = e r l (e r l + 1) 2(9b)$If we derive the second derivatives for the rewards, as shown in equations 9a and 9b, we find that they are both strictly positive for all values of r w and r l which implies that Separable DPH is convex with respect to the rewards.

## A.2 Full Proof of Theorem 2

We can prove Theorem 2 by examining the partial gradients with respect to the rewards. 

From equations 10a and 10b we can see a symmetry emerge, where the partial gradients with respect to the preferred logits are equal and opposite to the partial gradients with respect to the dispreferred logits. If we reparameterise the loss function such that r ∆ = r w -r l we can derive the following partial derivative

$∂ ∂r∆ L ConDPH (r w , r l ) = ϵ - 1 e r∆ + 1(11)$which is equal to zero for ϵ ∈ (0, 0.5] at the point r ∆ = log 1-ϵ ϵ . If we derive the second derivative of the Contrastive DPH objective function with respect to the reward margin r ∆ we obtain the following formula

$∂ 2 ∂r 2 ∆ L ConDPH (r w , r l ) = e r∆ (e r∆ + 1) 2(12)$which is strictly positive for all values of r ∆ , and -with respect to the reward logits -frames Contrastive DPH as a convex optimization problem with the additional properties of guaranteed convergence to a fixed margin for all ϵ ∈ (0, 0.5].

## A.3 Illustrative Loss Landscape

We provide an illustration of the loss landscapes to give a visual comparison of how our objective functions 'pull' rewards towards the optimal margin bounds. We initialise the embeddings from OPT-125m and use embedding tying for the language modelling head. Since our model dimension is 1536 while the embedding dimension is 768 the model contains an up-projection as the first layer of the backbone and a down-projection for the final layer. There are a total of 18 transformer blocks in the model backbone which use pre-layer norm in the attention and FFN residuals. The attention blocks have 24 attention heads and we use RoPE with a base frequency of 500,000 for positional embedding, and the FFN block uses SwiGLU activation with an intermediate dimension of 4096. The context window of the model is 2048 tokens and the Transformer XL recurrent memory contains 2048 tokens which allows the model to use a sliding window size of up to 4096 tokens at inference without any degradation.

The model was trained for approximately 100 billion tokens on the first 24 shards of The Pile. Each batch is constructed of 480 sequences of 2048 tokens each which are continuously sampled from the datasets shards using queues for the Transformer XL style pre-training method.

We use the LaProp optimizer [[45]](#b44) with β 1 = 0.9, β 2 = 0.95, a max learning rate of 6e-4 which warms up over 2000 steps and cosine decays down to 6e-5, LR-coupled weight decay of 0.1 and global gradient clipping with a max norm of 1.

Each epoch of 256 steps takes 1 hour and 59 minutes on 4x RTX A4500 GPUs. For the full 398 epochs (or 101888 steps) this comes out to around 790 hours or just under 33 days of training time (ignoring time for validation in-between epochs and at the end of training).

![ConDPH (r w , r l ) = ϵ -e r l e r l + e rw (10a)∂ ∂r l L ConDPH (r w , r l ) =e r l e r l + e rw -ϵ]()

![Figure1: The loss landscapes of the DPH loss functions. The red and green points represent the rewards assigned to preferred and dispreferred answers, the vertical lines represent the direction and magnitude of reward gradients, and the blue area represents the optimal margin parameterised by ϵ.]()

![Comparison of GLUE performance. Dashes represent unpublished results. Note that the Spearman correlation for Ours Vocab is misleading and caused by predicting "0" for all test samples.]()

![Comparison of accuracy on the GPT4All test suite.]()

![Comparison of accuracy on the RACE test set.]()

![Comparison of DPH validation scores for different objective and pooler combinations.]()

![Confusion matrix comparing validation scores for alternate heads.]()

![Comparison of validation scores calculated using the log probabilities from the vanilla model checkpoints and reward scores produced by the trained Direct Preference Heads.]()

Preprint. Under review.

