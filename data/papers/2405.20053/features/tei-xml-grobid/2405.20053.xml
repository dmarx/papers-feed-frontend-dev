<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Avelina</forename><surname>Asada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of St Andrews College Gate</orgName>
								<address>
									<postCode>KY16 9AJ</postCode>
									<settlement>St Andrews</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hadji</forename><surname>-Kyriacou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of St Andrews College Gate</orgName>
								<address>
									<postCode>KY16 9AJ</postCode>
									<settlement>St Andrews</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ognjen</forename><surname>Arandjelović</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of St Andrews College Gate</orgName>
								<address>
									<postCode>KY16 9AJ</postCode>
									<settlement>St Andrews</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">660859B5E9B3EFC44B824478727D05F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement Learning from Human Feedback (RLHF) is a technique that can be used to align an agent -such as a Large Language Model (LLM) -to human preferences and lead to more truthful, more helpful, less harmful and more preferred outputs <ref type="bibr" target="#b30">[31]</ref>. Proximal Policy Optimization (PPO) <ref type="bibr" target="#b37">[38]</ref> and Direct Preference Optimization (DPO) <ref type="bibr" target="#b32">[33]</ref> are two such aligment techniques which have been extensively used to improve the quality of LLM outputs, leading to instruction following agents or chat assistants which are quickly approaching human-baselines in a variety of knowledge and reasoning tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>However, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it. One study <ref type="bibr" target="#b5">[6]</ref> discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another <ref type="bibr" target="#b3">[4]</ref> discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters. Ouyang et al. <ref type="bibr" target="#b30">[31]</ref> also reports an increased tendency for RLHF models to make up information in closed domain tasks ("hallucination") compared to models trained with SFT alone.</p><p>To combat the the risk of RLHF compromising the abilities of an LLM in favor of producing preferable outputs we introduce Direct Preference Heads (DPH), a novel feature based approach that optimises a reward score produced by the LLM rather than optimising the logits produced by language modelling head. DPH can be used in combination with (or without) existing alignment techniques to allow language models to self-evaluate outputs sampled at inference time and select the highest scoring candidate.</p><p>We evaluate the performance of DPH using an efficient 551M parameter LM on a variety of commonsense reasoning and Natural Language Understanding (NLU) tasks. All code used to train our models is available on GitHub and we release our model weights on Hugging Face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Approaches</head><p>Prior approaches to language model alignment involve directly optimizing the logits produced by the language modelling head to increase the likelihood of producing preferable responses while decreasing the likelihood of undesirable responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning from Human Feedback (RLHF)</head><p>Reinforcement Learning from Human Feedback seeks to learn a reward model from human feedback on completions generated by a language model which can be used to align an LM with human preferences. A typical RLHF pipeline consists of 3 steps: (1) supervised fine-tuning, (2) preference sampling and reward modelling, and (3) RL fine-tuning.</p><p>Supervised Fine-Tuning The first step of a standard RLHF pipeline is fine-tuning a pre-trained LM on high quality data for downstream tasks to obtain a model π SFT .</p><p>Reward Modelling Next, the SFT model is prompted with input tokens x to produce completions y. These answers are then rated by human labelers which rate the answers based on one or more criteria. A reward model r ϕ (x, y) is then trained to estimate the scores assigned by human labelers using maximum likelihood estimation.</p><p>RL Fine-Tuning During the RL phase the learned reward function is used to provide feedback to the language model using the following optimization problem</p><formula xml:id="formula_0">max π θ E x∼D,y∼π θ (y|x) [r ϕ (x, y)] -βD KL [π θ (y|x)||π ref (y|x)]<label>(1)</label></formula><p>where β controls the deviation from the base reference policy π ref , which is typically initialized from π SFT . Due to the non-differentiable nature of language generation this objective must be optimized using a reinforcement learning algorithm such as PPO <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Direct Preference Optimization (DPO)</head><p>Direct Preference Optimization was introduced as a reparameterization of RLHF which eliminates both the sampling stage and the reward modelling stages and reformulates alignment procedure as a loss function which can be optimized directly on a dataset of pairs of preferred and dispreferred completions to given prompts. This allows DPO to stably and efficiently converge on an optimal policy using what is effectively a classification loss over positive and negative pairs.</p><p>Given a dataset {(x, y w , y l )} where x is the prompt and y w , y l are the preferred and dispreferred completions, we introduce the following loss function:</p><formula xml:id="formula_1">L DPO (x, y w , y l ) = -log σ β log π θ (y w |x) π ref (y w |x) -β log π θ (y l |x) π ref (y l |x)<label>(2)</label></formula><p>where π θ (y * |x) and π ref (y * |x) are the probabilities of completions y * for prompt x given by the policy model and reference models respectively, and the β parameter controls the deviation from the reference policy.</p><p>There also exists an augmentation of DPO namely Conservative DPO (cDPO) <ref type="bibr" target="#b27">[28]</ref> which is designed to be more robust to noisy labels through the introduction of label smoothing parameter ϵ. The objective function for cDPO is given by:</p><formula xml:id="formula_2">L cDPO (x, y w , y l ) = (1 -ϵ)L DPO (x, y w , y l ) + ϵ L DPO (x, y l , y w )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Direct Preference Heads</head><p>The hypothesis underlying the Direct Preference Optimization framework of Rafailov et al. <ref type="bibr" target="#b32">[33]</ref> is that a "language model is secretly a reward model" thereby making the purpose of Direct Preference Heads to exploit this and extract explicit reward signals without the need of an additional reward model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reward Head</head><p>To obtain the rewards from a sequence x; y three components are required: an aggregated hidden state h which is conditioned on the intermediate representations of the language model, a pooling function f which transforms the hidden state, and a learnable vector w dph with the same dimension as the output of f . We then compute the reward r as follows:</p><formula xml:id="formula_3">r = f (h) • w dph<label>(4)</label></formula><p>To obtain the hidden state we take the output of the last transformer layer for the final token of the sequence, and we experiment with three choices of f : (1) the identity mapping following the convention established by OpenAI's GPT for sequence classification <ref type="bibr" target="#b31">[32]</ref>, (2) a learnable affine projection with tanh nonlinearity following BERT's pooling function <ref type="bibr" target="#b14">[15]</ref>, and (3) an inverted bottleneck FFN with SwiGLU activation mirroring the FFN blocks used within the transformer backbone followed by tanh nonlinearity <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective Function</head><p>We formulate two novel objective functions for our method: a separable objective which maximises positive rewards and minimises negative rewards, and a contrastive objective which maximises the margin between positive and negative rewards. The loss landscapes are illustrated by Figure <ref type="figure" target="#fig_1">1</ref> in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Separable DPH</head><p>The Separable DPH loss function given by ( <ref type="formula" target="#formula_4">5</ref>) is a function of the preferred and dispreferred rewards r w , r l , and the label smoothing parameter 0 ≤ ϵ ≤ 0.5 which controls the reward margin.</p><formula xml:id="formula_4">L SepDPH (r w , r l ) = -[(1 -ϵ) log σ(r w ) + ϵ log σ(-r w )] -[ϵ log σ(r l ) + (1 -ϵ) log σ(-r l )]<label>(5)</label></formula><p>Theorem 1. For all ϵ ∈ (0, 0.5] the objective function L SepDPH is convex and will optimize the policy π θ such that the preferred rewards r w produced by the preference head converge towards log 1-ϵ ϵ and the dispreferred rewards r l converge to log ϵ 1-ϵ .</p><p>This can be proven by observing the first and second partial derivatives of the loss function with respect to the rewards. The first partial derivative is equal to zero at the points r w = log 1-ϵ ϵ and r l = log ϵ 1-ϵ respectively, and the second partial derivative is strictly positive for all values of r w , r l . A full proof is included in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Contrastive DPH</head><p>Like Separable DPH, the loss function for Contrastive DPH given by ( <ref type="formula" target="#formula_5">6</ref>) is function of the preferred and dispreferred rewards r w , r l and the label smoothing parameter 0 ≤ ϵ ≤ 0.5. This version of the loss function optimizes the relative margin between the rewards rather than optimizing the absolute positive and negative rewards as in Separable DPH.</p><formula xml:id="formula_5">L ConDPH (r w , r l ) = -(1 -ϵ) log σ(r w -r l ) -ϵ log σ(r l -r w )<label>(6)</label></formula><p>Theorem 2. For all ϵ ∈ (0, 0.5] the objective function L ConDPH is convex and will optimize the policy π θ such that the difference between preferred rewards r w and dispreferred rewards r l produced by the preference head will converge to a fixed margin, given by r ∆ = r w -r l = log 1-ϵ ϵ . This can be proven by reparameterising the loss function such that r ∆ = r w -r l and by then considering the first and second partial derivatives with respect to this reward margin. It can be observed that the first partial derivative is equal to zero when r ∆ = log 1-ϵ  ϵ , and the second partial derivative is strictly positive for all values of r ∆ . A full proof is included in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relation to cDPO</head><p>The properties of both Contrastive DPH and Seperable DPH show a strong relationship with Conservative DPO: SepDPH will converge to optimal fixed reward margins above zero for r w and below zero for r l ; ConDPH will converge to optimal fixed reward margins between r w and r l , and cDPO will converge to a fixed delta from the reference model <ref type="bibr" target="#b27">[28]</ref>. Like Conservative DPO, this makes both Seperable DPH and Contrastive DPH robust to preference label noise and makes training more stable than naive maximum likelihood estimation without label-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Novelty over Traditional Reward Modelling</head><p>Although similar to the reward modelling phase of an RLHF pipeline, DPH has some distinct differences which set it apart. DPH does not require an SFT sampling and human labelling stage meaning it can take advantage of pre-constructed preference datasets such as those used for DPO. Typical RLHF also requires multiple models -a reward model, a reference model and a policy model -while DPH requires only a single model to produce both responses and rewards. Unlike other RLHF pipelines such as PPO <ref type="bibr" target="#b37">[38]</ref>, the rewards produced by DPH are not used for RL fine-tuning; instead, the DPH rewards are to be used to prune candidate generations sampled from the LM at inference time to select the candidate which aligns most with human preferences. This makes DPH an excellent choice for small language models which are (1) more lightweight -and therefore can be efficiently used to generate multiple samples -and, (2) are more prone to degradation when aligned using typical RL techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup and Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We make use of a variety of datasets for fine-tuning and evaluation which are outlined below. The specific prompt templates used for fine-tuning and evaluation are described in Appendix C.</p><p>Natural Language Understanding (NLU) For general NLU we make use of the standard GLUE benchmark <ref type="bibr" target="#b41">[42]</ref>. The overall score for GLUE is computed by the macro-average of unweighted metric averages for all 9 tasks, however we also include a secondary score which does not included the 'problematic' WNLI task following the evaluation used for BERT <ref type="bibr" target="#b14">[15]</ref>. We opted to omit WNLI during fine-tuning due to the low sample size.</p><p>Commonsense Reasoning In accordance with the GPT4All <ref type="bibr" target="#b0">[1]</ref> evaluation suite, we use the following datasets to evaluate commonsense reasoning abilities: HellaSwag <ref type="bibr" target="#b42">[43]</ref>, OpenBookQA <ref type="bibr" target="#b26">[27]</ref>, WinoGrande <ref type="bibr" target="#b36">[37]</ref>, ARC <ref type="bibr" target="#b10">[11]</ref>, BoolQ <ref type="bibr" target="#b9">[10]</ref>, and PIQA <ref type="bibr" target="#b7">[8]</ref>.</p><p>Reading Comprehension To evaluate reading comprehension abilities we use the RACE dataset <ref type="bibr" target="#b23">[24]</ref>, a multiple-choice task which requires reasoning over provided passages.</p><p>Instruction Following We include the Alpaca <ref type="bibr" target="#b39">[40]</ref>, OpenOrca <ref type="bibr" target="#b24">[25]</ref>, and UltraFeedback <ref type="bibr" target="#b12">[13]</ref> datasets to train our models for instruction following. We make use of OpenOrca and a cleaned version of Alpaca for SFT, and binarized versions of OpenOrca and UltraFeedback for alignment.</p><p>Auxiliary Datasets To provide additional training data for SFT we include the MMLU <ref type="bibr" target="#b19">[20]</ref>, SQuAD V2 <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>, Tiny Stories <ref type="bibr" target="#b15">[16]</ref>, CNN-Dailymail <ref type="bibr" target="#b28">[29]</ref> and CoQA <ref type="bibr" target="#b35">[36]</ref> training splits. For alignment we only include MMLU and SQuAD V2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prompts and Sampling</head><p>Prompts We make use of the ChatML prompt templating scheme <ref type="bibr" target="#b29">[30]</ref> with handcrafted system, user and assistant prompts specific to each task. During fine-tuning we mask out the loss for all tokens of the prompt and condition the model on the content of assistant messages including the final &lt;|im_end|&gt; token. During evaluation we select the highest scoring answer using the average log-probabilities of the tokens in the final assistant message, or compute the reward scores on the final &lt;|im_end|&gt; token when evaluating with DPH.</p><p>SFT Sampling When sampling from the datasets for SFT we randomly shuffle each dataset and uniformly interleave samples from all tasks in the mix. To control the weighting of samples from each task we fill the context window with n consecutive samples from the same task before sampling from a different task, where n is chosen to be 5 in our experiments. To maximise compute utilisation and minimize unused portions of the context window we make us of Transformer-XL <ref type="bibr" target="#b13">[14]</ref> style training with a context window size of 2048 tokens and a recurrent memory size of 2048 tokens.</p><p>DPH Sampling When sampling from datasets for DPH alignment we switch from the Transformer-XL style pipeline to typical SFT training, opting to only include single samples in the context window padded to a fixed maximum length. As some of the datasets we use for DPH are intended for SFT rather than alignment (namely GLUE, GPT4All, RACE, MMLU and SQuAD) we synthesise preference pairs where the 'correct' answer is used as the preferred completion and we uniformly sample an 'incorrect' answer from the available choices for the dispreferred completion. This is trivial for most datasets, however we use a special process for the SQuAD V2 dataset; for answerable questions we use "unanswerable" as the dispreferred completion, and for unanswerable questions we use SpaCy to randomly sample a noun span from the context to use as the dispreferred completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Regularization</head><p>The hidden states h used to compute the reward scores are likely sub-optimal for computing rewards when initialising π θ from π SFT . As such, it may be desirable to fine-tune some or all parameters in the language model to learn better reward signals. This necessitates the use of regularization to prevent degradation of the models generative capabilities while learning to predict rewards.</p><p>Prior Regularization Typical parameter regularization strategies such as weight decay make the assumption that parameters θ follow a zero-mean Normal distribution p(θ) ∼ N (0, 1 β I) leading to an auxiliary loss term β 2 ||θ|| 2 2 . However, when performing transfer-learning or fine-tuning on a pre-trained model this assumption can be harmful and aid in catastrophic forgetting of the model's previously learnt abilities.</p><p>An alternative regularization scheme is Prior Regularization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref> which instead makes the assumption that the fine-tuned parameters are normally distributed around the original parameters</p><formula xml:id="formula_6">θ ref , that is θ ∼ N (θ ref , 1 β I), leading to the auxiliary loss term β 2 ||θ -θ ref || 2 2 .</formula><p>We employ Prior Regularization to limit the divergence of π θ from π SFT while still facilitating the learning of improved hidden state representations for the Direct Preference Head. Pseudocode for optimizer based decoupled prior regularization is included in Appendix B.1.</p><p>cDPO Regularization Rather than directly employing a KL Divergence penalty similar to that used in (1) we find that it is possible -and even beneficial -to use Conservative DPO as a means of (1) limiting the divergence of the policy model to a fixed delta from the reference model, and</p><p>(2) 'nudging' the model towards generating more preferable outputs which increases the chance of generating a better candidate completion at inference time with fewer sampling steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Pipeline</head><p>We progressively fine-tune the models in 3 stages: vocab extension, supervised fine-tuning, and DPH alignment. The details of the pre-trained model are included in Appendix D.1.</p><p>Vocab Extension Since our model was pre-trained without a chat structure it is necessary to train the embeddings for additional &lt;|im_start|&gt; and &lt;|im_end|&gt; tokens: we freeze all non-embedding parameters and use the same datasets as SFT. We fine-tune the embeddings for 4096 steps with a batch size of 128, a max LR of 6e-5 which warms up over 200 steps followed by cosine decay down to zero, and clip the global gradient norm to 1.</p><p>Supervised Fine-Tuning After vocab extension we move onto the SFT step which conditions the model for NLU tasks and instruction following using the sampling and loss masking method described in section 4.2. We fine-tune the model for 6144 steps with a batch size of 128, a max LR of 3e-5 which warms up over 200 steps followed by cosine decay down to zero, prior-regularization applied to all non-embedding parameters with coefficient 0.5, and clip the global gradient norm to 1.</p><p>DPH Alignment Using the sampling method described in section 4.2 we jointly learn DPH rewards and perform cDPO alignment. The goal here is to gently push the model towards producing preferable outputs without compromising the model's reasoning abilities, and the priority is to attain the highest validation metrics from the DPH rewards. This requires balancing the two objectives, and as such we introduce weighting parameters α 1 , α 2 to our final joint objective in <ref type="bibr" target="#b6">(7)</ref> where L DPH is either L sepDPH or L conDPH . We find α 1 , α 2 = 1 to be a good blance between DPO and DPH in our experiments.</p><p>L joint (x, y w , y l , r w , r l ) = α 1 L cDPO (x, y w , y l ) + α 2 L DPH (r w , r l )</p><p>We align the model for 23040 steps with a batch size of 64 pairs, a max LR of 3e-6 which warms up over 200 steps followed by cosine decay down to 3e-7, prior-regularization applied to all parameters with coefficient 0.5, and clip the global gradient norm to 1. Following the optimal DPO parameters for OpenHermes-7b-2.5 <ref type="bibr" target="#b21">[22]</ref> we use β = 0.6 and chose cDPO ϵ = 0.25 and DPH ϵ = 0.1 for regularisation. Additionally, we apply dropout with p = 0.1 to the outputs of the pooler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compute Resources</head><p>All fine-tuning was performed using an NVIDIA A100 SXM4 80GB GPU on a compute cluster, with jobs allocated 24 cores and 160GB of memory. Each checkpoint is saved in FP16 format which consumes about 1.1GB of storage, and the datasets require minimal storage space.</p><p>For vocab extension we train for 4096 steps with an average of 7.99 seconds of compute per step which translates to about 9 hours. For supervised fine-tuning we train for 6144 steps with an average of 9.26 seconds of compute per step which translates to about 16 hours. For DPH alignment we train for 23040 steps with an average of 7.21 seconds of compute per step which translates to about 46 hours. The DPH ablations with our models use about 140 hours of compute, and the Qwen ablations use about 60 hours of compute. In total, we used approximately 270 hours of A100 compute to train our models and collect the results included in our paper. We used additional compute for preliminary tests and fixing bugs for silently failing experiments although this wasn't tracked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Methodology</head><p>As described in Section 4 we use NLU, commonsense reasoning and reading comprehension tasks to measure model capabilities, while the instruction following and auxiliary tasks are used to provide additional training signals. For the NLU tasks we evaluate on the test set of GLUE, providing average scores both with and without WNLI. For reading comprehension we evaluate on the RACE test set.</p><p>For commonsense reasoning we follow the LM Evaluation Harness <ref type="bibr" target="#b16">[17]</ref> implementations of these tasks, evaluating on the test sets of ARC and OpenBookQA and the validation sets of HellaSwag, WinoGrande, BoolQ and PIQA, which brings our evaluations in line with other models.</p><p>For vocab extension and SFT checkpoints we obtain model predictions from the completions with the highest scoring log-probabilities. For the DPH checkpoints we report metrics for both logprobability predictions (Ours DPO ) and predictions chosen from the DPH rewards (Ours DPH ). We use the SwiGLU-based pooler with the separable objective function for all our experiments as we found this combination to perform best overall as shown in Section 5.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Natural Language Understanding</head><p>Our results for NLU performance are included in Table <ref type="table" target="#tab_0">1</ref>. Note that the results for GPT-1 <ref type="bibr" target="#b31">[32]</ref> and BERT <ref type="bibr" target="#b14">[15]</ref> are from sub-task specific fine-tunes. It is unsurprising that our model does not outperform BERT Large even though it has more parameters; this is likely due to BERT's task specific fine-tunes in comparison to our model which was jointly trained on several tasks. Despite this our instruction following DPH model achieves a 2.2% higher average GLUE score compared to task-specific GPT-1 fine-tunes and manages to attain the highest overall accuracy and macro-average on RTE and STS-B respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Commonsense Reasoning</head><p>Our results for commonsense reasoning are summarized in Table <ref type="table" target="#tab_1">2</ref>. Note the Pythia <ref type="bibr" target="#b6">[7]</ref> and TinyLlama <ref type="bibr" target="#b43">[44]</ref> models were not fine-tuned for any specific task but received significantly more pre-training and have much higher parameter counts. With SFT alone we are able to attain comparable performance to TinyLlama using half as many parameters, and when applying DPH alignment we achieve a 7.2% increase over the TinyLlama average score and the highest accuracy in 5 of the 7 tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Reading Comprehension</head><p>Our results for reading comprehension are included in Table <ref type="table" target="#tab_2">3</ref>. The results for GPT-1 were taken from a RACE specific fine-tune, and the results for LLaMA <ref type="bibr" target="#b40">[41]</ref> were zero-shot without fine-tuning. Our SFT baseline achieves a higher average accuracy on RACE compared with the non fine-tuned LLaMa models but cannot match the accuracy of the RACE specific GPT-1 fine-tune; however after alignment our model attains a 3.5% higher average over GPT-1 while still maintaining excellent scores on other tasks using the same model weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Pooling Head Function and Objective Choice</head><p>We ablate over the three pooling head and two objective function choices. We perform alignment for 7680 steps and report the validation scores in Table <ref type="table" target="#tab_3">4</ref>. For both separable and contrastive objectives the SwiGLU pooler performs best on the three benchmarks, and for both GLUE and RACE the separable objective performs best overall. However during these experiments we discovered that contrastive DPH was achieving higher scores than separable DPH for specifically the sentence completion style tasks like HellaSwag, WinoGrande and PIQA. We hypothesise this is caused by situations where multiple completions to a given prompt may be plausible even though there is only one 'gold' answer, and as such the model benefits from maximising the relative reward margin with the contrastive objective rather than optimising absolute rewards with the separable objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Task Specific Heads</head><p>By taking the DPH checkpoint and freezing all backbone parameters it is possible to learn task specific heads and pooling functions for different downstream tasks at the cost of only 19M parameters per task. We train new heads for the three task groups and plot the confusion matrix of each head for each task average in Table <ref type="table" target="#tab_4">5</ref>. We further fine-tune for an additional 7680 steps on each task group using the same training setup as DPH alignment. Unsurprisingly the GLUE and GPT4All heads achieve the highest scores for GLUE and GPT4All benchmarks respectively, however the GPT4All head manages to outperform the RACE head on the RACE benchmark. We hypothesise this may be due to the inclusion of muliple choice QA and reading comprehension tasks in GPT4All which may prove better training signals than the RACE training data alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Model Ablations</head><p>Our final experiments involve exploring the behaviour of DPH when applied to frozen language models in an ad-hoc fashion. We experiment using the Qwen 1.5 model family <ref type="bibr" target="#b2">[3]</ref> and train only the pooler and reward head weights, reporting results in Table <ref type="table" target="#tab_5">6</ref>. We use an identical training setup to DPH alignment but disable dropout due to the low number of trainable parameters.</p><p>Because the model backbone and embeddings remain frozen during alignment the 'Log' scores represent the model's pre-trained (or fine-tuned) capabilities. When observing the difference between the Log scores of the 0.5B Qwen models it is evident that the fine-tuning and alignment used to transform the pre-trained model into the "chat" model resulted in degraded performance across the 3 tasks. This phenomenon is less apparent for the 1.8B models, and actually results in higher GLUE scores for the "chat" variant of the model. This further confirms the hypothesis that alignment can harm the reasoning capabilities of smaller language models. For all models DPH is consistently able to attain higher scores on the GLUE tasks compared to the log probabilities produced by the language modelling head, but the opposite is observed for RACE which suggests the hidden states produced by the frozen backbone do not contain rich enough features for long range modelling tasks such as reading comprehension. We also observe the "chat" variants produce higher task scores for DPH than the non-chat variants which we hypothesise is a result of the authors' fine-tuning with the Chat-ML format which lead to the models' greater understanding of message structure and therefor improved hidden state aggregation for the final end message token.</p><p>When we combine these findings with those presented in Section 5.2.2, it becomes evident that the pooling function and reward head exhibit slower convergence when the model backbone is frozen. This observation further supports our hypothesis in Section 4.3, indicating that the hidden states generated by the models are are initially sub-optimal and that further fine-tuning is necessary to optimize these hidden states to achieve the best features for DPH.</p><p>6 Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Future Work</head><p>As shown in the results section, DPH is capable of learning to assign higher rewards to preferred outputs and lower rewards to dispreferred outputs which implies the pooling function learns rich features with respect to prompt-completion pairs. We believe that it would be possible to also extract additional information from the output of the pooling function to detect finer grained signals such as helpfulness, humor, creativity, toxic content, etc. This can be achieved by training on a conversational dataset such Open Assistant <ref type="bibr" target="#b22">[23]</ref> which contains a variety of human-curated labels in addition to machine-generated labels produced by Detoxify <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations</head><p>The main benefit of DPH being its ability to perform alignment without directly effecting the model's output distribution is also its main limitation: unlike other alignment techniques which can help prevent the model generating harmful outputs, DPH is only capable of detecting harmful outputs.</p><p>Although we do include DPO alignment in our experiments to reduce the likelihood of harmful outputs, DPH does not require such model alignment to function, which shifts the responsibility of rejecting harmful outputs to the end user or service provider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Conclusion</head><p>In this paper we introduced Direct Preference Heads, a novel form of language model alignment which is performed at inference time to prune candidate completions for a given prompt. Unlike other alignment techniques which coerce the model into generating human preference aligned outputs, DPH instead produces reward scores for candidate outputs without affecting the actual generation process and therefor avoids the issue of RLHF leading to degraded performance when applied to smaller language models. We formulated two loss functions for DPH and find strong connections to Conservative DPO, implying that DPH is robust to label noise and can be tuned to a specific confidence margin. Finally, we evaluated our methods on a number of NLU, commonsense reasoning and reading Comprehension tasks and found that DPH is able to consistently outperform both our SFT baseline and multiple publicly available language model checkpoints of varying size and training volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impacts</head><p>As with all language modeling systems we cannot guarantee all responses produced by our models are factually correct nor can we guarantee that they are safe and free from harmful content. Our work focuses on creating a system that helps filter out incorrect and harmful messages by scoring candidate outputs, but as with all alignment techniques our models may be susceptible to so-called 'jailbreaks' which can coerce the model into incorrectly assigning a higher score to less desirable content. To maximise safety DPH should be implemented alongside other safety guardrails such as Llama Guard <ref type="bibr" target="#b20">[21]</ref> when used for publicly facing chat systems, and we intend for our provided model checkpoints to be used for reproduction of results and further research in the field of alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix -Theory</head><p>A.1 Full Proof of Theorem 1</p><p>We can prove Theorem 1 by examining the partial gradients with respect to the rewards.</p><formula xml:id="formula_8">∂ ∂rw L SepDPH (r w , r l ) = ϵ - 1 e rw + 1 (8a) ∂ ∂r l L SepDPH (r w , r l ) = 1 e -r l + 1 -ϵ<label>(8b)</label></formula><p>From equations 8a and 8b we find that the partials gradients are both equal to zero at the points r w = log 1-ϵ ϵ and r l = log ϵ 1-ϵ respectively. It is also interesting to note that log 1-ϵ ϵ + log ϵ 1-ϵ = 0 which implies the positive and negative rewards will converge to an equal distance from 0.</p><formula xml:id="formula_9">∂ 2 ∂r 2 w L SepDPH (r w , r l ) =</formula><p>e rw (e rw + 1) 2 (9a)</p><formula xml:id="formula_10">∂ 2 ∂r 2 l L SepDPH (r w , r l ) = e r l (e r l + 1) 2<label>(9b)</label></formula><p>If we derive the second derivatives for the rewards, as shown in equations 9a and 9b, we find that they are both strictly positive for all values of r w and r l which implies that Separable DPH is convex with respect to the rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Full Proof of Theorem 2</head><p>We can prove Theorem 2 by examining the partial gradients with respect to the rewards. </p><p>From equations 10a and 10b we can see a symmetry emerge, where the partial gradients with respect to the preferred logits are equal and opposite to the partial gradients with respect to the dispreferred logits. If we reparameterise the loss function such that r ∆ = r w -r l we can derive the following partial derivative</p><formula xml:id="formula_12">∂ ∂r∆ L ConDPH (r w , r l ) = ϵ - 1 e r∆ + 1<label>(11)</label></formula><p>which is equal to zero for ϵ ∈ (0, 0.5] at the point r ∆ = log 1-ϵ ϵ . If we derive the second derivative of the Contrastive DPH objective function with respect to the reward margin r ∆ we obtain the following formula</p><formula xml:id="formula_13">∂ 2 ∂r 2 ∆ L ConDPH (r w , r l ) = e r∆ (e r∆ + 1) 2<label>(12)</label></formula><p>which is strictly positive for all values of r ∆ , and -with respect to the reward logits -frames Contrastive DPH as a convex optimization problem with the additional properties of guaranteed convergence to a fixed margin for all ϵ ∈ (0, 0.5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Illustrative Loss Landscape</head><p>We provide an illustration of the loss landscapes to give a visual comparison of how our objective functions 'pull' rewards towards the optimal margin bounds. We initialise the embeddings from OPT-125m and use embedding tying for the language modelling head. Since our model dimension is 1536 while the embedding dimension is 768 the model contains an up-projection as the first layer of the backbone and a down-projection for the final layer. There are a total of 18 transformer blocks in the model backbone which use pre-layer norm in the attention and FFN residuals. The attention blocks have 24 attention heads and we use RoPE with a base frequency of 500,000 for positional embedding, and the FFN block uses SwiGLU activation with an intermediate dimension of 4096. The context window of the model is 2048 tokens and the Transformer XL recurrent memory contains 2048 tokens which allows the model to use a sliding window size of up to 4096 tokens at inference without any degradation.</p><p>The model was trained for approximately 100 billion tokens on the first 24 shards of The Pile. Each batch is constructed of 480 sequences of 2048 tokens each which are continuously sampled from the datasets shards using queues for the Transformer XL style pre-training method.</p><p>We use the LaProp optimizer <ref type="bibr" target="#b44">[45]</ref> with β 1 = 0.9, β 2 = 0.95, a max learning rate of 6e-4 which warms up over 2000 steps and cosine decays down to 6e-5, LR-coupled weight decay of 0.1 and global gradient clipping with a max norm of 1.</p><p>Each epoch of 256 steps takes 1 hour and 59 minutes on 4x RTX A4500 GPUs. For the full 398 epochs (or 101888 steps) this comes out to around 790 hours or just under 33 days of training time (ignoring time for validation in-between epochs and at the end of training).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>∂∂rwL</head><figDesc>ConDPH (r w , r l ) = ϵ -e r l e r l + e rw (10a)∂ ∂r l L ConDPH (r w , r l ) =e r l e r l + e rw -ϵ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The loss landscapes of the DPH loss functions. The red and green points represent the rewards assigned to preferred and dispreferred answers, the vertical lines represent the direction and magnitude of reward gradients, and the blue area represents the optimal margin parameterised by ϵ.</figDesc><graphic coords="14,108.00,35.13,194.04,194.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of GLUE performance. Dashes represent unpublished results. Note that the Spearman correlation for Ours Vocab is misleading and caused by predicting "0" for all test samples.</figDesc><table><row><cell>System</cell><cell>Tokens Params</cell><cell>MNLI m/mm</cell><cell>QQP F1/Acc</cell><cell>QNLI Acc</cell><cell>SST-2 Acc</cell><cell>CoLA M Corr</cell><cell>STS-B P/S Corr</cell><cell>MRPC F1/Acc</cell><cell>RTE Acc</cell><cell>Score w/o WNLI</cell><cell>WNLI Acc</cell><cell>Score w/ WNLI</cell></row><row><cell>Ours Vocab</cell><cell cols="5">100B 551M 34.1/34.7 28.2/42.9 50.2 58.0</cell><cell cols="5">0.9 -0.9/99.2 69.4/57.4 50.9 42.8</cell><cell>34.9</cell><cell>41.9</cell></row><row><cell>Ours SFT</cell><cell cols="10">100B 551M 73.6/75.0 59.1/82.8 81.4 90.8 22.7 80.6/92.4 80.6/75.2 71.4 72.0</cell><cell>38.4</cell><cell>68.2</cell></row><row><cell>Ours DPO</cell><cell cols="10">100B 551M 78.8/80.2 65.6/85.6 87.0 93.3 36.5 83.7/94.4 83.9/79.1 73.9 77.0</cell><cell>37.7</cell><cell>72.7</cell></row><row><cell>Ours DPH</cell><cell cols="10">100B +19M 80.0/80.6 65.8/85.3 87.5 94.0 43.8 85.3/93.0 85.5/80.2 75.3 78.6</cell><cell>46.6</cell><cell>75.0</cell></row><row><cell>GPT-1</cell><cell cols="3">32B 117M 82.1/81.4 70.3/ -</cell><cell cols="6">87.4 91.3 45.4 82.0/80.0 82.3/ -56.0</cell><cell>-</cell><cell>-</cell><cell>72.8</cell></row><row><cell>BERT Base</cell><cell cols="3">128B 110M 84.6/83.4 71.2/ -</cell><cell cols="3">90.5 93.5 52.1</cell><cell>-/85.8</cell><cell cols="2">88.9/ -66.4</cell><cell>-</cell><cell>-</cell><cell>78.3</cell></row><row><cell cols="11">BERT Large 128B 340M 86.7/85.9 72.1/89.3 92.7 94.9 60.5 87.6/86.5 89.3/85.4 70.1 82.5</cell><cell>65.1</cell><cell>80.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of accuracy on the GPT4All test suite.</figDesc><table><row><cell>System</cell><cell cols="8">Tokens Params HellaSwag OpenBookQA WinoGrande ARC-Challenge ARC-Easy BoolQ PIQA Average</cell></row><row><cell>Ours Vocab</cell><cell cols="2">100B 551M</cell><cell>36.93</cell><cell>28.60</cell><cell>51.14</cell><cell>26.19</cell><cell>25.67</cell><cell>61.25 65.39 42.17</cell></row><row><cell>Ours SFT</cell><cell cols="2">100B 551M</cell><cell>42.59</cell><cell>45.20</cell><cell>55.01</cell><cell>35.84</cell><cell>47.01</cell><cell>76.24 69.37 53.04</cell></row><row><cell>Ours DPO</cell><cell cols="2">100B 551M</cell><cell>44.83</cell><cell>52.40</cell><cell>57.38</cell><cell>39.76</cell><cell>53.54</cell><cell>79.08 72.36 57.05</cell></row><row><cell>Ours DPH</cell><cell cols="2">100B +19M</cell><cell>59.36</cell><cell>57.40</cell><cell>59.12</cell><cell>41.21</cell><cell>56.82</cell><cell>78.81 68.77 60.21</cell></row><row><cell cols="2">Pythia-1.0B 300B</cell><cell>1.1B</cell><cell>47.16</cell><cell>31.40</cell><cell>53.43</cell><cell>27.05</cell><cell>48.99</cell><cell>60.83 69.21 48.30</cell></row><row><cell cols="2">Pythia-1.4B 300B</cell><cell>1.5B</cell><cell>52.01</cell><cell>33.20</cell><cell>57.38</cell><cell>28.50</cell><cell>54.00</cell><cell>63.27 70.95 51.33</cell></row><row><cell>TinyLlama</cell><cell>3T</cell><cell>1.1B</cell><cell>59.20</cell><cell>36.00</cell><cell>59.12</cell><cell>30.12</cell><cell>55.25</cell><cell>57.83 73.29 52.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of accuracy on the RACE test set.</figDesc><table><row><cell>System</cell><cell>Tokens</cell><cell>Params</cell><cell>RACE-middle</cell><cell>RACE-high</cell><cell>Weighted Average</cell></row><row><cell>Ours Vocab</cell><cell>100B</cell><cell>551M</cell><cell>26.0</cell><cell>24.6</cell><cell>25.0</cell></row><row><cell>Ours SFT</cell><cell>100B</cell><cell>551M</cell><cell>56.1</cell><cell>52.9</cell><cell>53.8</cell></row><row><cell>Ours DPO</cell><cell>100B</cell><cell>551M</cell><cell>65.9</cell><cell>59.8</cell><cell>61.6</cell></row><row><cell>Ours DPH</cell><cell>100B</cell><cell>+19M</cell><cell>66.9</cell><cell>60.6</cell><cell>62.5</cell></row><row><cell>GPT-1</cell><cell>32B</cell><cell>117M</cell><cell>62.9</cell><cell>57.4</cell><cell>59.0</cell></row><row><cell>LLaMA 7B</cell><cell>1T</cell><cell>6.7B</cell><cell>61.1</cell><cell>46.9</cell><cell>51.0</cell></row><row><cell>LLaMA 13B</cell><cell>1T</cell><cell>13B</cell><cell>61.6</cell><cell>47.2</cell><cell>51.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of DPH validation scores for different objective and pooler combinations.</figDesc><table><row><cell>Objective</cell><cell>Pooling Function</cell><cell>Add. Params</cell><cell>GLUE</cell><cell>GPT4All</cell><cell>RACE</cell><cell>HellaSwag</cell><cell>WinoGrande</cell><cell>PIQA</cell></row><row><cell>Separable</cell><cell>Identity</cell><cell>1536</cell><cell>75.06</cell><cell>56.86</cell><cell>56.54</cell><cell>46.63</cell><cell>53.20</cell><cell>65.29</cell></row><row><cell>Separable</cell><cell>BERT Style</cell><cell>2.4M</cell><cell>75.13</cell><cell>55.86</cell><cell>56.62</cell><cell>45.84</cell><cell>52.17</cell><cell>64.69</cell></row><row><cell>Separable</cell><cell>SwiGLU FFN</cell><cell>19M</cell><cell>75.19</cell><cell>57.14</cell><cell>57.60</cell><cell>48.72</cell><cell>53.35</cell><cell>64.96</cell></row><row><cell>Contrastive</cell><cell>Identity</cell><cell>1536</cell><cell>74.99</cell><cell>57.66</cell><cell>54.09</cell><cell>50.93</cell><cell>53.83</cell><cell>66.87</cell></row><row><cell>Contrastive</cell><cell>BERT Style</cell><cell>2.4M</cell><cell>73.91</cell><cell>57.07</cell><cell>55.89</cell><cell>49.98</cell><cell>54.62</cell><cell>67.30</cell></row><row><cell>Contrastive</cell><cell>SwiGLU FFN</cell><cell>19M</cell><cell>74.04</cell><cell>58.28</cell><cell>55.95</cell><cell>51.38</cell><cell>55.80</cell><cell>67.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Confusion matrix comparing validation scores for alternate heads.</figDesc><table><row><cell>Benchmark</cell><cell>Baseline Head</cell><cell>GLUE Head</cell><cell>GPT4All Head</cell><cell>RACE Head</cell></row><row><cell>GLUE</cell><cell>76.12</cell><cell>76.36</cell><cell>76.20</cell><cell>76.13</cell></row><row><cell>GPT4All</cell><cell>60.19</cell><cell>60.13</cell><cell>60.29</cell><cell>60.24</cell></row><row><cell>RACE</cell><cell>64.17</cell><cell>64.05</cell><cell>64.48</cell><cell>64.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of validation scores calculated using the log probabilities from the vanilla model checkpoints and reward scores produced by the trained Direct Preference Heads.</figDesc><table><row><cell>System</cell><cell>GLUE Log</cell><cell>GPT4All Log</cell><cell>RACE Log</cell><cell>GLUE DPH</cell><cell>GPT4All DPH</cell><cell>RACE DPH</cell></row><row><cell>Qwen1.5-0.5B</cell><cell>41.94</cell><cell>53.11</cell><cell>51.38</cell><cell>45.69</cell><cell>48.52</cell><cell>41.21</cell></row><row><cell>Qwen1.5-0.5B-Chat</cell><cell>39.82</cell><cell>49.70</cell><cell>50.32</cell><cell>48.99</cell><cell>49.72</cell><cell>46.90</cell></row><row><cell>Qwen1.5-1.8B</cell><cell>47.03</cell><cell>62.53</cell><cell>68.14</cell><cell>59.18</cell><cell>51.61</cell><cell>46.56</cell></row><row><cell>Qwen1.5-1.8B-Chat</cell><cell>53.85</cell><cell>61.69</cell><cell>67.47</cell><cell>62.38</cell><cell>54.47</cell><cell>53.33</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>References</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Note that three of the GLUE tasks have no license specified on their homepages nor within their publications: CoLA claims their dataset falls under "fair use," no concrete license can be found for RTE nor its pre-cursors, and SST-2 does not specify a license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Prompt Templates</head><p>For brevity, we only include the prompt templates of the tasks we use for evaluation. All other prompt templates are listed within the code repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 GLUE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE -CoLA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System User Assistant</head><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentence, answer the question with "yes" or "no".</p><p>Sentence: {{sentence}} Question: Does this sentence make sense?</p><p>Answer:</p><p>{{no | yes}}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE -MNLI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System User Assistant</head><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given a premise statement and a hypothesis statment, respond with "True" if the premise entails the hypothesis, respond with "False" if the premise contradicts the hypothesis, or respond with "Neither" if the statements are neurtral.</p><p>Premise: {{premise}} Hypothesis: {{hypothesis}} Question: True, False or Neither?</p><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentences, answer the question with "yes" or "no".</p><p>Sentence 1: {{sentence1}}</p><p>Sentence 2: {{sentence2}}</p><p>Question: Do both sentences mean the same thing?</p><p>Answer:</p><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentences, answer the question with "yes" or "no". Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentences, answer the question with "yes" or "no". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE -RTE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>User Assistant</p><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentences, answer the question with "yes" or "no". Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentence, answer the question with "positive" or "negative".</p><p>Sentence: {{sentence}} Question: Is this sentence positive or negative?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>User Assistant</p><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentences, answer the question with a number between 0 and 5.</p><p>Sentence 1: {{sentence1}}</p><p>Sentence 2: {{sentence2}}</p><p>Question: On a scale of 0 to 5 how similar are Sentence 1 and Sentence 2?</p><p>Answer:</p><p>Below is an instruction that describes a task. Write a response that appropriately completes the request using the provided answer options.</p><p>Given the following sentences, answer the question with "yes" or "no".</p><p>Sentence 1: {{sentence1}} </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo</title>
		<author>
			<persName><forename type="first">Yuvanesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Duderstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mulyar</surname></persName>
		</author>
		<ptr target="https://github.com/nomic-ai/gpt4all" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hal Daumé III au2. Frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengguang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clémentine</forename><surname>Fourrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Sanseviero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" />
	</analytic>
	<monogr>
		<title level="j">Open llm leaderboard</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The poison of alignment</title>
		<author>
			<persName><forename type="first">Aibek</forename><surname>Bekbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungbae</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yerzat</forename><surname>Dulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Yamazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<title level="m">Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Piqa</surname></persName>
		</author>
		<title level="m">Reasoning about physical commonsense in natural language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptation of maximum entropy capitalizer: Little data can help a lot</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="382" to="399" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Training verifiers to solve math word problems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Ultrafeedback</surname></persName>
		</author>
		<title level="m">Boosting language models with high-quality feedback</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tinystories: How small can language models be and still speak coherent english?</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Strategies for conceptual change in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Grachten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cancino</forename><surname>Chacón</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="https://github.com/unitaryai/detoxify" />
	</analytic>
	<monogr>
		<title level="j">Detoxify. Github</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Laura Hanu and Unitary team</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krithika</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tontchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Testuggine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<title level="m">Llama guard: Llm-based input-output safeguard for human-ai conversations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Preference tuning llms with direct preference optimization methods</title>
		<author>
			<persName><surname>Osanseviero</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/blog/blob/main/pref-tuning.md" />
	</analytic>
	<monogr>
		<title level="j">Github</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannic</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotiris</forename><surname>Dimitri Von Rütte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Rui</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><surname>Barhoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richárd</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><surname>Nagyfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Shahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Glushkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Dantuluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mattick</surname></persName>
		</author>
		<title level="m">Openassistant conversations -democratizing large language model alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Wing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bleys</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanvichet</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName><surname>Teknium</surname></persName>
		</author>
		<ptr target="https://https://huggingface.co/Open-Orca/OpenOrca" />
		<title level="m">Openorca: An open dataset of gpt augmented flan traces</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Truthfulqa</surname></persName>
		</author>
		<title level="m">Measuring how models mimic human falsehoods</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<ptr target="https://ericmitchell.ai/cdpo.pdf" />
		<title level="m">A note on dpo with noisy preferences &amp; relationship to ipo</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://github.com/openai/openai-python/blob/f" />
		<imprint/>
	</monogr>
	<note>Chat markup language 7ccce126325ea35b6e5224ab954652c97a74896/chatml.md</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">WINOGRANDE: an adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Glu variants improve transformer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Tinyllama: An open-source small language model</title>
		<author>
			<persName><forename type="first">Peiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianduo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Laprop: Separating momentum and adaptivity in adam</title>
		<author>
			<persName><forename type="first">Zhikang</forename><forename type="middle">T</forename><surname>Liu Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahito</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Ueda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
