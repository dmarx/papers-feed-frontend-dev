# KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge Sources

## Abstract

## 

We present a Wikidata-based framework, called KIF, for virtually integrating heterogeneous knowledge sources. KIF is written in Python and is released as open-source. It leverages Wikidata's data model and vocabulary plus user-defined mappings to construct a unified view of the underlying sources while keeping track of the context and provenance of their statements. The underlying sources can be triplestores, relational databases, CSV files, etc., which may or may not use the vocabulary and RDF encoding of Wikidata. The end result is a virtual knowledge base which behaves like an "extended Wikidata" and which can be queried using a simple but expressive pattern language, defined in terms of Wikidata's data model. In this paper, we present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF.

## Introduction

Knowledge source integration is the problem of meaningfully combining multiple knowledge sources. The problem is harder when (i) the sources are heterogeneous, i.e., adopt different vocabularies, formats, storage technologies, etc.; and (ii) the intended integration is virtual, i.e., is to be done dynamically, at query time. In the Semantic Web, where "knowledge source" usually means a set of OWL ontologies, the integration problem is often reduced to the ontology matching problem [[20]](#b20). In practice, however, determining correspondences between concepts and properties in ontologies is just one of the many issues involved. More often than not, the sources to be integrated don't support OWL or even RDF (think of graph databases, relational databases, REST APIs, CSV dumps, etc.) and either are just too large or change too frequently to be converted and ingested statically into a single knowledge base.

A scenario like this requires a more comprehensive solution. One that combines virtualization mechanisms to provide federated access to the sources, mapping mechanisms to reconcile differences in their vocabularies, and provenance mechanisms to enable telling which piece of knowledge came from which source. Although there are in the literature proposals to tackle each of these tasks separately, few attempt to provide an integrated solution.

In this paper, we present KIF [1](#foot_0) , an open-source Python framework that uses Wikidata's data model and vocabulary [[24]](#b24) as a lingua franca to integrate heterogeneous knowledge sources. KIF is our attempt at a comprehensive solution to the hard version of the knowledge integration problem-the version in which the sources are heterogeneous and the integration is virtual.

The core abstraction of KIF is the store. A store is an interface to a "Wikidata view" of a knowledge source, obtained by interpreting the source's content as a set of Wikidata-like statements. For example, KIF's built-in CSV store constructs Wikidata view of a CSV file by interpreting each of its cells (line-column pair) as a statement where the subject is given by the entity described by the line, the property is given by the header of the column, and the value by the content of the cell. Another built-in store type is the SPARQL store. It constructs a Wikidata view of a SPARQL endpoint by interpreting certain patterns in the endpoint's RDF graph as Wikidata-like statements.

Having Wikidata as the target brings some advantages. First, KIF inherits a tried-and-tested data model with native support for structured data values plus the notions of references and qualifiers, used to represent provenance and context information. Second, if desired, one can reuse Wikidata's vast vocabulary, which at the time of writing has more than 110M items and 11K properties, covering various domains. Third, one can easily combine statements produced by any KIF store with statements coming from Wikidata itself, which can be accessed via an ordinary SPARQL store. Such a combination is done using a mixer store, which virtually merges statements of one or more child stores.

The key to the flexibility of the store abstraction lies in its query interface. KIF stores are queried using a simple but expressive pattern language defined in terms of Wikidata's data model. KIF includes a pattern compiler targeting SPARQL which can be parameterized with custom mappings to generate queries in RDF encodings other than Wikidata's. A mapping to the RDF encoding of PubChem [[13]](#b13) (a large public base of chemical knowledge) is also included in the distribution. As we discuss later, this mapping was used together with other things to build an application that integrates statements about chemical compounds coming from PubChem, Wikidata, and IBM CIRCA [[11]](#b11) (a relational database of chemical data extracted from patents and other sources).

The rest of the paper is organized as follows. Section 2 presents some background on Wikidata. Section 3 presents the design and implementation of KIF. Section 4 discusses the evaluation of KIF over a use case in the domain of chemistry. Section 5 discusses some related work. Section 6 presents our conclusions and future work.

## Background

Wikidata [[24]](#b24) is a sister project of Wikipedia and it's also one of largest bases of structured knowledge on the Web. Although we have been using the term "Wikidata data model", the data model used by Wikidata actually comes from Wikibase, which is the open-source framework underlying Wikidata. Wikibase is a project of its own. It can be used to create other knowledge bases following the same data model as Wikidata but with different content and purposes [[6]](#b5).

## Wikibase Data Model

Wikibase's data model [[17]](#b17) consists of entities and statements about entities. In Wikidata's UI, statements are grouped into "entity pages". Figure [1](#fig_0) shows the page of entity Q2270, which stands for the chemical compound benzene. [2](#foot_1) Every entity has a label, a description, and one or more aliases. In Figure [1](#fig_0), these are shown in the header at the top of the page. After the header comes the "Statements" section which groups statements about the entity being described. A statement consists of two parts: subject and snak. The subject is the entity about which the statement is made. The snak is the statement's claim. It associates a property with either a specific value, some unspecified value, or no value.

Figure [1](#fig_0) depicts two statements which can be read as follows:

"benzene has an LD50 of 4,699-4,701 milligrams per kilogram" (1) "benzene has an LD50 of 87-89 milligrams per kilogram"

(2) LD50 (or median lethal dose) is a toxicity unit that measures the dose of a substance that is required to kill half the members of a tested population. The subject of statements ( [1](#)) and ( [2](#)) is the same, "benzene" (Q2270). Their snak is of the form property-value. The property of both is "median lethal dose (LD50)" (P2240). The value of (1) is "4,700 ±1 mg/kg" and the value of ( [2](#)) is "88 ±1 mg/kg". Note that the data model distinguishes between items (identified with "Q") and properties (identified with "P"). Only the latter can occur as the first component of snaks.

In Python, using the KIF constructors (see Figure [2](#)), statements (1) and ( [2](#)) are written as follows:

$>>> stmt1 = Statement($Item( benzene ), ... ValueSnak(Property( LD50 ), Quantity(4700, mg/kg , 4699, 4701))) >>> stmt2 = Statement(Item( benzene ), ... ValueSnak(Property( LD50 ), Quantity(88, mg/kg , 87, 89)))

We write x for the URL of entity x, e.g., benzene stands for [http://www.wikidata.org/entity/Q2270](http://www.wikidata.org/entity/Q2270). Back to Figure [1](#fig_0), the qualifiers and references associated with each statement are shown below the statement's value (see the boxes "qualifiers" and "opened references"). Qualifiers are extra snaks which qualify what is being said by the statement's main snak; references (or reference records) are sets of snaks which keep provenance information.

The qualifiers of statement [(1)](#b0) shown in Figure [1](#fig_0) are written as follows: >>> qualifiers_of_stmt1 = [ ... ValueSnak(Property( route of administration ), Item( oral administration )) ... ValueSnak(Property( applies to taxon ), Item( laboratory mouse ))]

Note that the qualifiers in this case convey information that is crucial to interpret the statement, namely, the route of administration and the affected animal. The references shown for statement [(1)](#b0) in Figure [1](#fig_0) are written as follows: >>> references_of_stmt1 = [ ... ReferenceRecord( # 1st reference ... ValueSnak(Property( stated in ), Item( PubChem )), ... ValueSnak(Property( PubChem CID ), String('241')), ... ValueSnak(Property( language of work or name ), Item( English )), ... ValueSnak(Property( retrieved ), Time('2024-04-12'))), ... ReferenceRecord( # 2nd reference ... ValueSnak(Property( reference URL ), IRI('[http://www.cdc.gov](http://www.cdc.gov)...')))] stmt ::= Statement(entity , snak ) -claim about entity entity ::= item | property item ::= Item(iri ) -person or thing property ::= Property(iri ) -(binary) relation snak ::= ValueSnak(property , value) -"property has value" | SomeValueSnak(property ) -"property has some value" | NoValueSnak(property ) -"property has no value" value ::= entity | data-value data-value ::= iri | text | string | external-id | quantity | time iri ::= IRI(s) -IRI text ::= Text(s, lang?) -text in a given language string ::= String(s) -string external-id ::= ExternalId(s) -external id quantity ::= Quantity(n, item?, n?, n?) -numerical quantity time ::= Time(ts, i?, i?, item?) -date or time reference ::= ReferenceRecord(snak +) rank ::= Preferred | Normal | Deprecated Fig. 2. Constructors of data model objects in KIF. "?" means zero-or-one; "+" means one-or-more; s is a Python string; lang is a Python string containing language tag such as "en"; n is a number; i is an integer; and ts is a date-time timestamp.

The last piece of metadata associated with statements is the rank which can be either "preferred", "normal", or "deprecated". In Figure [1](#fig_0), the rank is represented symbolically by the two triangles and circle which occur on the left of the statement's value. A filled upper triangle means preferred rank; a filled circle means normal rank; and a filled lower triangle means deprecated rank. As can be seen in Figure [1](#fig_0), statements (1) and ( [2](#)) have normal rank.

## Wikidata RDF Encoding

Wikidata defines an RDF encoding for its data model which is also adopted by Wikibase [[18,](#b18)[25]](#b25). The format varies slightly depending on whether it is used in a data dump or observed from Wikidata's query service. The version we describe here is that of the query service.

In Wikidata's RDF encoding, each statement is represented at two levels. The first level, called truthy, keeps a shallow representation of the statement as a single RDF triple. For example, the truthy encoding of statement (1) of Figure [1](#fig_0), namely, "benzene (Q2270) has an LD50 (P2240) of 4,700±1 mg/kg", consists of the single triple: wd:Q2270 "4700"ˆˆxsd:decimal

$wdt:P2240 ( †)$The namespace wd: indicates an entity and wdt: indicates a truthy application of a property. Some statements are fully characterized at the truthy level. But, as illustrated by ( †), this is not always the case. Note that the unit, lower-, and upper-bounds associated with the value 4700 are not represented in ( †). In general, when the statement's value is a structured data-value, like a quantity or time value, a single literal is used to represent it at the truthy level. This is the so called simple value of the statement. For quantity values, the simple value is is just the numerical amount.

The second level of the encoding keeps the full representation of the statement. It uses reification to capture the deep value of the statement plus its qualifiers, references, and rank. Figure [3](#)  Fig. [3](#). RDF representation of the statement "Benzene (Q2270) has an LD50 (P2240) of 4,700 ±1 mg/kg (Q21091747)" considering only the qualifier "route of administration (P636) is oral administration (Q285166)" and the reference record "reference URL (P854) is [https://www.cdc.gov/niosh-rtecs/CY155CC0.html](https://www.cdc.gov/niosh-rtecs/CY155CC0.html)".

In Figure [3](#), the shaded nodes are the reified ones. The single underscore (_) indicates that their ids are opaque (hence not shown in the figure). Node wds:_ represents the statement. Predicates p:P2240 and ps:P2240 are used to connect the subject "benzene" (wd:Q2270) to the statement and the statement to its simple value, i.e., the number 4700 in decimal notation.

The deep value of the statement is represented by node wdv:_. It has type wikibase:QuantityValue and is connected to the unit mg/kg (wd:Q21091747), the lower-bound 4699, and the upper-bound 4701. The rank of the statement is connected via predicate wikibase:rank.

Moving to qualifiers, predicate pq:P636 connects the qualifier "route of administration" (P636) with value "oral administration" (wd:Q285166) to the statement. Finally, predicate prov:wasDerivedFrom connects to the statement the reference record represented by node wdref:_. Its content (the snak "reference URL" (P854) with value "[https://www.cdc.gov/](https://www.cdc.gov/)...") is encoded using predicate pr:P854 and a (simple) IRI value.

## Design and Implementation

KIF is an integration framework based on Wikidata. The idea behind it is to use Wikidata to standardize the syntax and possibly the vocabulary of the underlying knowledge sources. Users can then query the sources through patterns described in terms of Wikidata's data model. The integration done by KIF is virtual in the sense that syntax and vocabulary translations happen dynamically, at query time, guided by user-provided mappings.

As we mentioned before, the core abstraction of KIF is the store. A store is an interface to a knowledge source. This can be a SPARQL endpoint, REST API, RDF file, CSV file, etc. The job of the store is to construct a "Wikidata view" of the knowledge source. The prototypical store is the SPARQL store, which we describe next.

## SPARQL Store

The SPARQL store reads Wikidata-like statements from a given SPARQL endpoint. Here is how we create a SPARQL store pointing to WDQS, the public SPARQL query service of Wikidata:

>>> from kif_lib import * >>> Wikidata = Store('sparql', '[https://query.wikidata.org/sparql](https://query.wikidata.org/sparql)')

At line 1, we import the namespace of KIF, whose Python module is called kif_lib. At line 2, we create a new store of type "sparql" pointing to WDQS and assign the result to variable Wikidata. As we did not pass an explicit mapping to the store constructor, it assumes the endpoint speaks Wikidata's RDF encoding, which is the case for WDQS.

We can read statements from the newly created Wikidata store as follows:

$>>> it =$Wikidata.filter( ... subject=Item('[http://www.wikidata.org/entity/Q2270](http://www.wikidata.org/entity/Q2270)'), ... property=Property('[http://www.wikidata.org/entity/P2240](http://www.wikidata.org/entity/P2240)')) >>> next(it) Statement(Item(IRI('[http://www.wikidata.org/entity/Q2270](http://www.wikidata.org/entity/Q2270)')), ValueSnak(Property(IRI('[http://www.wikidata.org/entity/P2240](http://www.wikidata.org/entity/P2240)')), Quantity(4700, Item(IRI('[http://www.wikidata.org/entity/Q21091747](http://www.wikidata.org/entity/Q21091747)')), 4699, 4701))) ֒→ ֒→ ֒→

At lines 3-5, we ask for statements with subject "benzene" (Q2270) and property "median lethal dose (LED50)" (P2240). The result is an iterator which is assigned to variable it. At line 6, we consume one statement from it whose content is shown in line 7. Note that this is the same statement (1) of Section 2.

## Patterns

As we mentioned, in KIF, queries are specified as patterns defined in terms of Wikidata's data model. The filter() call in line 3 above is actually just a wrapper to a match() call, which evaluates a pattern over the knowledge source.

We can rewrite the previous filter() in terms of match() as follows:

$>>> x =$Variable('x') >>> pat = Statement(Item('[http://www.wikidata.org/entity/Q2270](http://www.wikidata.org/entity/Q2270)'), ... ValueSnak(Property('[http://www.wikidata.org/entity/P2240](http://www.wikidata.org/entity/P2240)'), x)) >>> it = Wikidata.match(pat)

At line 8, we create the pattern variable x and use it to build the pattern pat in lines 9-10. KIF patterns are templates for objects of the data model, i.e., objects in which certain parts are replaced by variables. Pattern pat is a template for statements whose subject is "benzene" (Q2270) and whose snak is a value-snak with property "LD50" (P2240) and value x, i.e., any value.

Before detailing how the match() call in line 11 works (see Section 3.3), let us make a quick detour to show two other features of KIF.

The vocabulary module When writing data model or pattern objects, we can use KIF's vocabulary module to make the code less verbose. For example, we can rewrite the previous pattern pat (lines 9-10) more concisely as follows:

>>> from kif_lib.vocabulary import wd >>> pat1 = Statement(wd.Q(2270), ValueSnak(wd.P(2240), x))

At line 12, we import KIF's Wikidata vocabulary module wd. At line 13, we use wd.Q() and wd.P() to construct the item "benzene" (Q2270) and property "LD50" (P2240) without having to write their URLs. But we can do better:

$>>> pat2 = wd.median_lethal_dose(wd.Q(2270), x)$This constructs the same pattern by applying property wd.median_lethal_dose, which is predeclared in the wd module, as if it were a Python function to arguments wd.Q(2270) and x. That the three versions construct exactly the same statement pattern object can be checked by a simple equality test:

## >>> pat == pat1 and pat1 == pat2 True

Constraints Suppose we want to restrict the statements that match the previous pattern to those with a value in the range 4000-7000 mg/kg. We can do that by using method where() to constraint the pattern as follows: Method where() takes a boolean expression of variables. Here the resulting pattern, pat3 (lines [[17]](#b17)[[18]](#b18)[[19]](#b19), is a new pattern equal to pat with the added constraint that the matched statement's value must be a quantity in mg/kg with amount x such that 4000 ≤ x ≤ 7000, and with any lower-and upper-bound values. This ends our detour. We can now get back to the match() method.

## The match() Method

Method match() is the workhorse of the store API. It must be implemented by all store types and is the basis of the implementation of most other store API methods, including filter().

The match() method takes a pattern p as argument and returns a match object which when iterated generates all instances of p found in the store. The actual implementation of match() varies from store to store. But the general idea can be described as follows. First, the store compiles pattern p into a query q, written in query language of knowledge source. Then it evaluates q over the source, producing a result set R such that each result b in R is a binding of variables in q. Finally, for each variable-value pair (x, v) in b, the store replaces the variable corresponding to x in p by the value v, generating a new match.

To make matters more concrete, let p be pattern pat3 defined at the end of Section 3.2 (lines 17-19). Figure [4](#) shows the steps taken by a SPARQL store evaluate the call match(p).

In step (1), the SPARQL store instantiates a new SPARQL compiler. Since no SPARQL mapping was given to the compiler, it assumes a default mapping targeting the RDF encoding of Wikidata.

In steps ( [2](#)) and ( [3](#)), the store uses the compiler to compile pattern p into a SPARQL query q and a substitution θ. Note that compilation is compositional, i.e., the compilation of p is defined in terms of the compilation of its subpatterns. The substitution θ is a mapping from subpatterns of p into variables of q. For instance, the θ of Figure [4](#) specifies that variable ?x of query q corresponds to variable x of pattern p.

In steps (4) and ( [5](#)), the SPARQL store sends query q to the source's SPARQL endpoint and receives as a result the SPARQL result R. A SPARQL result is essentially a set of bindings of the variables selected by the query. Figure [4](#) shows that R contains at least two bindings for variable ?x, namely, 4700 and 6400.

## SPARQL Store

## SPARQL Compiler SPARQL Endpoint

(1) match(p)

$(2) p (3) q, θ(4) q$(5) R = {(?x, 4700), (?x, 6400), . . .} # ?_v3 := psv:P2240 wd:Q2270 ?_v4 ?_v1 .

# ?_v1 := wds:_ ?_v1 ?_v3 ?_v2 .

# ?_v2 := wdv:_ ?_v1 ?_v0 ?x . ?_v2 rdf:type wikibase:QuantityValue . ?_v2 wikibase:quantityAmount ?x . ?_v2 wikibase:quantityUnit wd:Q21091747 . OPTIONAL { ?_v2 wikibase:quantityLowerBound ?_v5 . } OPTIONAL { ?_v2 wikibase:quantityUpperBound ?_v6 . } FILTER (?x >= 4700 && ?x <= 7000) } Fig. [4](#). Evaluation of match(p) over a SPARQL store.

Finally, in step [(6)](#b5), for each binding b in R, the SPARQL store replaces the SPARQL variables in θ by their values in b and applies the resulting substitution to the original pattern p to obtain a new match. For example, the first match shown in step (6) of Figure [4](#) is obtained by computing θ[?x := 4700](p), i.e., replacing ?x by 4700 in θ and applying the resulting substitution to p. The result in this case is statement (1) of Section 2.

## SPARQL Mapping for PubChem

The SPARQL store can be used to read statements from any SPARQL endpoint, provided it is supplied with an appropriate mapping. One such mapping already included in KIF is for the RDF distribution of PubChem [[8,](#b7)[13]](#b13). Here is how we create a SPARQL store pointing to a local installation of PubChem's RDF:

$>>>$from kif_lib.store.sparql.mapping import PubChemMapping >>> PubChem = Store('sparql', '[http://localhost:1234/sparql](http://localhost:1234/sparql)', ... mapping=PubChemMapping()) We won't go into detail here, but object PubChemMapping() (line 22) tells the SPARQL store (actually, the SPARQL compiler) how to translate KIF patterns into graph patterns over PubChem's RDF graph. The resulting store, PubChem (line 21), behaves as an ordinary SPARQL store. We can use it, for example, to obtain the mass (P2067) of benzene from PubChem: >>> it = PubChem.filter( ... subject=wd.InChIKey('UHOVQNZJYSORNB-UHFFFAOYSA-N'), ... property=wd.mass) >>> next(it) == wd.mass( ... Item('[http://rdf.ncbi.nlm.nih.gov/pubchem/compound/CID241](http://rdf.ncbi.nlm.nih.gov/pubchem/compound/CID241)'), ... Quantity('78.11', wd.dalton))

There are a couple of things to note here. First, the PubChem mapping provided by KIF adopts the Wikidata vocabulary whenever possible. For instance, it maps property "mass" (P2067) to the appropriate property in Pub-Chem. The mapping also uses Wikidata units, e.g., "dalton" (Q483261) above. What it doesn't do is translate the ids of PubChem compounds. This explains the non-Wikidata URL in the subject of the returned statement (line 27). This is also the reason we didn't use the Wikidata id of benzene, wd.Q(2270), in the subject of the filter() call above. Had we done that the result would be empty. Instead, we used the value-snak wd.InChIKey('UHOV...'). That is, we set the subject to any entity whose InChIKey (P235) is equal to "UHOV. . . ", which happens to be the InChIKey of benzene. InChIKey is a universal identifier for chemical compounds which is defined in both Wikidata and PubChem.

Here is the pattern corresponding to the previous filter() (lines 23-25):

>>> x, y = Variables('x', 'y') >>> pat4 = wd.mass(x, y).where( ... wd.InChIKey(x, 'UHOVQNZJYSORNB-UHFFFAOYSA-N'))

It now should be clear what is happening: filter() is searching for statements with subject x, property "mass", and value y, such that x is an entity with "InChIKey" equal to "UHOV. . . ". This example illustrates the use of a statement pattern (line 32) as a boolean constraint, i.e., as an argument to the where() call. The constraint in this case plays the role of a fingerprint, i.e., a test that identifies an entity indirectly, here using a universal identifier instead of a local one. The support for this kind of fingerprinting technique is crucial for enabling meaningful queries over combinations of knowledge sources, such as those obtained via mixer stores.

## The Mixer Store

The mixer store combines one or more child stores into a new store which behaves as their virtual union. For example, we can combine Wikidata (line 2) and PubChem (lines [[21]](#b21)[[22]](#b22) into a new store mix of type "mixer" as follows: From the user's point of view, mix (line 33) is a store like any other. Its content can be thought of as the union of the statements in Wikidata and PubChem.

At lines 34-35, we ask mix for statements with subject "benzene", and assign the first two results to variables stmt1 and stmt2, respectively. One possibility here, for example, is that mix returns first a statement from Wikidata and then a statement from PubChem, say, those in lines 6-7 and 26-28.

One way to determine which statement came from which child store, is to instruct them to attach extra references to their statements. For instance, here is how we can instruct Wikidata to attach an extra reference to statements:

$>>> Wikidata.extra_references = [ ... ReferenceRecord(wd.reference_URL(Wikidata.iri))]$Now every statement produced by the Wikidata store will be associated to one extra reference stating that the statement's "reference URL" (P854) is the address of the endpoint set in Wikidata. We won't go into details here, but the references of a statement can be obtained using the store API method get_annotations().

To decouple statements from qualifiers, references, and rank-and avoid opaque ids-KIF introduces the notion of an annotation. An annotation (or annotation record ) is a triple containing qualifiers (set of snaks), references (set of reference records), and rank.

In KIF, statements are identified by their content (subject and snak) and can be associated with one or more annotation records in a store. This deviates from the Wikidata RDF representation [[18]](#b18), in which statements are identified by opaque ids which carry its qualifiers, references, and rank. The rationale of KIF's approach is to relieve users from having to deal with opaque, meaningless ids-push this work to the framework.

## Other Store Types and Methods

Besides the SPARQL store and the mixer store, KIF comes with an RDF store and a CSV store. The RDF store reads statements from RDF files. It is essentially a SPARQL store that uses RDFLib [[21]](#b21) to load RDF files and evaluate SPARQL queries over their contents. The CSV store reads statements from CSV files. It expects a mapping specifying how line-columns pairs are to be interpreted as statements. Currently, the CSV store is implemented as a wrapper to the RDF store which first converts the CSV to RDF before loading it. We have plans for a more direct, non-RDF-based implementation though.

All stores implement a common store API, containing core methods filter() and match() discussed before plus methods to get statement annotations (qualifiers, references, and ranks) and entity descriptors (labels, aliases, and descriptions). The store API also has convenience methods for testing and counting pattern occurrences, and methods for obtaining store metadata. For more details, see the documentation of KIF [[12]](#b12).

## Use Case and Evaluation

The research on KIF was in part motivated by the development of a chat-bot for the domain of chemistry. Depending on the user's question, the bot would retrieve statements about chemical compounds from various sources and present them as "known facts". There were three main requirements: (i) the retrieved statements should be comparable, i.e., they should use the same vocabulary when talking about the same things; (ii) their origin should be traceable, i.e., statements should come with provenance information; and (iii) it should be easy to add new sources to the system.

Figure [5](#fig_5) shows the instantiation of KIF used in the chat-bot application. A mixer store is used to combine three SPARQL stores: one pointing to a local installation of PubChem's RDF [[8]](#b7) running on Virtuoso [[7]](#b6), one pointing to Wikidata's public SPARQL endpoint (WDQS), and one pointing to a local SPARQL endpoint built by Ontop over the SQL endpoint of IBM CIRCA. IBM CIRCA [[11]](#b11) is a relational database of chemical data extracted from patents and other sources. In the chat-bot application, we used Ontop to build a Wikidata-compatible SPARQL endpoint to access parts of its schema. Ontop [[27]](#b27) is an ontology-based data access tool. It uses R2RML [[5]](#b4) mappings to translate SPARQL queries to SQL at query time. The R2RML mappings tell Ontop how to map tables in the relational database into concepts and properties of the target ontology (in our case, Wikidata's).

## Evaluation

For the evaluation, we used the setup shown in Figure [5](#fig_5) without the IBM CIRCA part, i.e., essentially the setup shown in line 33 of Section 3.5. Our goal was to measure the overhead of KIF when evaluating simple application-level queries over the mixer. By overhead, we mean time spent in KIF (Python code) versus time spent in the SPARQL endpoints (outside the Python code). By simple application-level queries, we mean statement patterns meaningful to users.

For the experiment, we generated patterns of the forms p(x,v 1 ).where(q(x,v 2 )) and p(v 1 ,x).where(q(x,v 2 )),

where x is a pattern variable, p and q are properties, v 1 is a value or variable, and v 2 is a value. Since we wanted matches in both Wikidata and PubChem, we restricted p to the properties in PubChem whose domain or range is a chemical compound (e.g., mass (P2067), has part (P527), trading name (P6427), etc.) and q to those which are compound identifiers (e.g., InChIKey (P235), canonical SMILES (P233), ChEBI ID (P683), etc.). We evaluated each of the generated patterns over the mixer of Figure [5](#fig_5) and collected those which (i) matched at least one statement in both Wikidata and PubChem, and (ii) took at least 0.75s to run in each of the endpoints. We then sorted the patterns by number of matches and selected the last 100 patterns. We ended up with patterns Q0-Q99, whose evaluation times are shown in Figure [6](#fig_6). For each pattern Q i in Figure [6](#fig_6), the red line (Total) indicates the time taken to evaluate and consume all results of the call match(Q i ) over the mixer. The orange and green lines (Wikidata and PubChem) indicate the time taken to send the SPARQL queries to the endpoints and receive the results. The blue line (KIF) indicates the time spent in KIF where KIF = Total -(Wikidata + PubChem).

As expected, the bulk of the time (on average 97.9%) was spent in the endpoints, in particular, PubChem. The overhead of KIF is negligible, especially when the number of matches is smaller than the page-size configured in KIF. In this experiment, we used a page of size 100, which is the default. This means that to consume a response with 1000 matches, KIF has to perform 10 requests. This explains the increase in the overhead of KIF after Q70. Nevertheless, evaluation time is dominated by the SPARQL endpoints.

## Related Work

There are three classes of work related to KIF.

First, there are the ontology-based data access (OBDA) systems [[26]](#b26). These are systems such as Ontop [[27]](#b27) which, given a mapping between a data source and target ontology, are able to evaluate over the data source queries written in terms of the ontology. In ODBA, the data source is a relational database, the ontology is written in a DL-Lite [[2]](#b1) language (e.g., OWL 2 QL), the query language is SPARQL, and query evaluation is done under the SPARQL 1.1 entailment regime [[26]](#b26). Also, OBDA usually means virtual access: the SPARQL query is transformed into an SQL query on-the-fly and evaluated over the database. (To be feasible, the SQL query needs to be heavily optimized, as the transformation from SPARQL often leads to a blow-up in size [[10]](#b9).) Different from OBDA systems, KIF fixes the target syntax and (possibly) the vocabulary to those of Wikidata. Although it doesn't attempt to do any kind of reasoning on its own, its SPARQL store can be used seamlessly with any reasoning-enabled endpoint. Also, KIF doesn't attempt to provide an interface for arbitrarily complex queries. The filter() method, in particular, was inspired by the work on linked data fragments [[22]](#b22) and TPF [[23]](#b23). It is a lightweight filtering interface which can be used reliably by applications.

Another thing that distinguishes the KIF from OBDA systems is that the latter largely ignore the problem of query federation [[10]](#b9). (One notable exception is Squerall [[16]](#b16).) That said, OBDA systems such as Ontop can be used to enable KIF to interface with relational databases, as illustrated in Section 4.

The second class of work related to KIF are RDF integration systems based on SPARQL query-rewriting [[4,](#b3)[15]](#b15). These are similar to OBDA systems but target RDF. The system is given an RDF source, a target ontology, a mapping between the source schema and the target ontology, and a SPARQL query written in terms of the ontology. Its job is to translate the original SPARQL query into a new SPARQL query and evaluated it over the RDF source on-the-fly.

The problem of SPARQL-rewriting (or ontology-mediated query translation) is closely related to the ontology matching problem. But despite the vast liter-ature on ontology matching [[20]](#b20), there is little research on using the produced mappings for querying RDF sources, especially when federations of sources are considered [[3]](#b2). An early work that uses SPARQL rewriting for integrating multiple RDF sources is [[15]](#b15). However, it doesn't cover the result reconciliation problem, i.e., using the mappings to reconcile the results of the queries in the terms of the target ontology. A more recent proposal which covers query-translation and result reconciliation is [[3]](#b2). The previous remarks on the differences between KIF and OBDA systems also apply to SPARQL rewriting systems.

The third class of related work are knowledge graph construction (KGC) systems. These are systems like SPARQL-Generate [[14]](#b14) and SPARQL Anything [[1]](#b0), which use SPARQL to describe the transformation of non-RDF sources into an RDF dataset. For this purpose, SPARQL-Generate extends the syntax of SPARQL, while SPARQL Anything overrides the SERVICE clause. Both of these systems allow users to query non-RDF resources on-the-fly. But different from KIF, they don't attempt perform any kind of ontology-mediated mapping. The user must use the vocabulary of each source and specify the desired transformations explicitly, for each query.

One problem largely ignored by the three classes of work cited above is the representation and tracking of provenance. In contrast, KIF tackles this problem from the start: statements carry provenance information which is preserved while they traverse the framework.

## Conclusion

This paper presented KIF, a framework that uses Wikidata to virtually integrate heterogeneous knowledge sources. Having Wikidata as the integration model brings some advantages, including a flexible data model and access to a huge vocabulary covering a wide range of topics. This is particularly important when the integration problem is open-ended: when we don't know which kind of knowledge source we might need to integrate next, or when the sources deal with completely different subjects. In the domain of chemistry, as illustrated in Section 2, when talking about toxicity we often need to refer not only to chemicals but also to things like symptoms, diseases, and even animals-all of these are already covered by Wikidata's vocabulary. That said, KIF is not restricted to Wikidata's vocabulary. If needed, the stores can extend their vocabulary with new entities and properties (cf. the use of PubChem URLs as item ids by the PubChem RDF mapping in Section 3.4).

Moving to the implementation of KIF, we are currently adding support for parallel requests to the library. This will speed up not only the mixer but also any store that needs to split large queries into multiple requests. As we mentioned in Section 3.6, we are also working on an alternative, non-RDF-based implementation of the CSV store. A more important change is the addition of a mutable store API, which will allow users write onto stores. The idea is to use SPARQL update queries in the case of the SPARQL store.

Finally, on the theoretical side we are working on the formalization of the semantics of KIF patterns, inspired by the work on WShEx [[9]](#b8).

![Fig. 1. Part of Wikidata's entity page of benzene. (Adapted from [19].)]()

![depicts the full representation of statement (1) of Figure1considering only one qualifier and one reference record.]()

![>>> pat3 = wd.median_lethal_dose(wd.Q(2270), ... Quantity(x, wd.milligram_per_kilogram).where( ... x.ge(4000) & x.le(7000)))]()

![6) θ[?x := 4700](p), θ[?x := 6400](p), . . . where: p = wd.median_lethal_dose(wd.Q(2270), Quantity(x, wd.milligram_per_kilogram).where(x.ge(4000) & x.le(7000))) θ = {(Variable('x') , ?x)} q = SELECT * WHERE { wd:P2240 wikibase:claim ?_v4 . # ?_v4 := p:P2240 wd:P2240 wikibase:statementProperty ?_v0 . # ?_v0 := ps:P2240 wd:P2240 wikibase:statementValue ?_v3 .]()

![>>> mix = Store('mixer', [Wikidata, PubChem]) >>> stmt1, stmt2, *rest = mix.filter( ... subject=wd.InChIKey('UHOVQNZJYSORNB-UHFFFAOYSA-N'))]()

![Fig. 5. KIF instantiation integrating PubChem, Wikidata, and IBM CIRCA.]()

![Fig.6. KIF overhead. The x-axis shows the queries sorted by number of matches. The numbers above the red line indicate the number of matches for a particular query. The y-axis shows the time in seconds (in log scale). On average, 2.1% of the time was spent in KIF, 12.4% in Wikidata, and 85.5% in PubChem.]()

https://github.com/IBM/kif

https://www.wikidata.org/wiki/Q2270

