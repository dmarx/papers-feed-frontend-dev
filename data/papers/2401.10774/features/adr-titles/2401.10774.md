- Decision to implement multiple decoding heads for parallel token prediction
- Choice of tree-based attention mechanism for candidate processing
- Selection of fine-tuning procedures (MEDUSA-1 and MEDUSA-2)
- Decision to keep the backbone model frozen during MEDUSA-1 fine-tuning
- Strategy for joint training of MEDUSA heads and backbone model in MEDUSA-2
- Adoption of self-distillation for training data generation in absence of datasets
- Implementation of typical acceptance scheme for candidate selection
- Use of temperature as a threshold in the acceptance scheme
- Decision to evaluate MEDUSA on various model sizes and training settings
- Choice to focus experiments on batch size of one for personal use cases
- Decision to initialize MEDUSA heads to align with original model predictions
- Strategy for managing computational resources during fine-tuning
- Decision to utilize quantization techniques for optimization
- Choice of activation function (SiLU) for MEDUSA heads
- Decision to avoid the use of a separate draft model for speculative decoding
- Strategy for handling distribution shifts between MEDUSA heads and original model