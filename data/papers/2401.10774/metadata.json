{
  "arxivId": "2401.10774",
  "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding Heads",
  "authors": "Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao",
  "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires\nsequential computation, with each step reliant on the previous one's output.\nThis creates a bottleneck as each step necessitates moving the full model\nparameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods such as speculative decoding have been suggested to address this issue,\ntheir implementation is impeded by the challenges associated with acquiring and\nmaintaining a separate draft model. In this paper, we present Medusa, an\nefficient method that augments LLM inference by adding extra decoding heads to\npredict multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism, Medusa constructs multiple candidate continuations and verifies them\nsimultaneously in each decoding step. By leveraging parallel processing, Medusa\nsubstantially reduces the number of decoding steps required. We present two\nlevels of fine-tuning procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM, enabling better prediction accuracy of Medusa\nheads and higher speedup but needing a special training recipe that preserves\nthe backbone model's capabilities.\n  Moreover, we propose several extensions that improve or expand the utility of\nMedusa, including a self-distillation to handle situations where no training\ndata is available and a typical acceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate Medusa on models of various\nsizes and training procedures. Our experiments demonstrate that Medusa-1 can\nachieve over 2.2x speedup without compromising generation quality, while\nMedusa-2 further improves the speedup to 2.3-3.6x.",
  "url": "https://arxiv.org/abs/2401.10774",
  "issue_number": 395,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/395",
  "created_at": "2025-01-04T15:02:15.845680",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 20,
  "last_read": "2025-01-04T15:02:15.846437",
  "last_visited": "2024-12-28T07:11:26.727Z",
  "main_tex_file": null,
  "published_date": "2024-01-19T15:48:40Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL"
  ]
}