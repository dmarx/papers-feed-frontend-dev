# Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints

## Abstract

## 

Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. Depending on the differentiability of the objective function, we propose two different sampling methods. For differentiable objectives, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. For non-differentiable objectives, we propose an iterative importance sampling strategy using the diffusion model as the proposal distribution. Comprehensive experiments on a synthetic dataset, six real-world black-box optimization datasets, and a multi-objective molecule optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines.

## Introduction

Optimization problems are ubiquitous in real-world applications when approaching search problems [[7]](#b6), partial differential equations [[53]](#b52), molecular design [[89]](#b88). While significant advancements have been made in resolving a broad spectrum of abstract optimization problems with analytically known objective functions and constraints [[10,](#b9)[84,](#b83)[80]](#b79), optimization in real-world scenarios remains challenging since the exact nature of the objective is often unknown, and access to constraints is limited [[23]](#b22). For example, it is challenging to incorporate the closed-form constraints on a molecule to be synthesizable or design an objective function for target chemical properties.

Previous studies have identified problems with unknown objective functions as black-box optimization problems [[23,](#b22)[1]](#b0). In such scenarios, the only way to obtain the objective value is through running a simulation [[68]](#b67) or conducting a real-world experiment [[91]](#b90), which might be expensive and nondifferentiable. A prevalent approach to this challenge involves learning a surrogate model with available data to approximate the objective function which can be implemented in either an online [[92,](#b91)[90,](#b89)[98]](#b97) or offline manner [[105,](#b104)[106]](#b105).

However, there is a significant lack of research focused on scenarios where analytic constraints are absent. The only works that deal with unknown constraints are from the derivative-free optimization community [[3,](#b2)[4,](#b3)[78]](#b77). However, these methods can only be applied to simple low-dimensional problems and cannot be applied to more complex problems such as molecule and protein optimization. In practice, overlooking these feasibility constraints during optimization can result in spurious solutions. For instance, the optimization process might yield a molecule with the desired chemical property but cannot be physically synthesized [[35,](#b34)[30]](#b29), which would require restarting the optimization from different initializations [[63,](#b62)[54]](#b53).

To restrict the search space to the set of feasible solutions, we propose to perform optimization within the support of the data distribution or the data manifold. Indeed, in practice, one usually has an extensive set of samples satisfying the necessary constraints even when the constraints are not given explicitly. For instance, the set of synthesizable molecules can be described by the distribution of natural products [[6,](#b5)[110]](#b109). To learn the data distribution, we focus on using diffusion models, which recently demonstrated the state-of-the-art performance in image modeling [[49,](#b48)[97]](#b96), video generation [[48]](#b47), and 3D synthesis [[82]](#b81). Moreover, [[81,](#b80)[24]](#b23) theoretically demonstrated that diffusion models can learn the data distributed on a lower dimensional manifold embedded in the representation space, which is often the case of the feasibility constraints.

To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of two densities: i) a Boltzmann density with energy defined by the objective function and ii) the density of the data distribution. The former concentrates around the global minimizers in the limit of zero temperature [[52,](#b51)[39]](#b38), while the latter removes the non-feasible solutions by yielding the zero target density outside the data manifold. Depending on whether the objective function is differentiable or non-differentiable, we propose two different sampling methods. When the objective function is differentiable, we propose a two-stage sampling strategy: (i) a guided diffusion process acts as a warm-up stage to provide initialization of data samples on the manifold, and (ii) we ensure convergence to the target distribution via Markov Chain Monte Carlo (MCMC). When the objective is non-differentiable, we propose an iterative importance sampling strategy using diffusion models to gradually improve the proposal distribution.

The main contributions of this work are: [(1)](#b0) We reformulate the problem of optimization under unknown constraints as a sampling problem from the product of the data distribution and the Boltzmann distribution defined by the objective function. [(2)](#b1) We propose two different sampling methods with diffusion models, depending on the differentiability of the objective function. (3) Empirically, we validate the effectiveness of our proposed framework on a synthetic toy example, six real-world offline black-box optimization tasks as well as a multi-objective molecule optimization task. We find that our method, named DIFFOPT, can outperform state-of-the-art methods or achieve competitive performance across these tasks.

## Background

Problem definition. Consider an optimization problem with objective function h : R d → R. Additionally, we consider a feasible set C, which is a subset of R d .

Our goal is to find the set of minimizers {x ⋆ i } M i=1 of the objective h within the feasible set C. This can be expressed as the following constrained optimization problem {x ⋆ i } M i=1 = arg min x∈C h(x). However, in our specific scenario, the explicit formulation of the feasible set C is unavailable. Instead, we can access a set of points D = {x i } N i=1 sampled independently from the feasible set C. Optimization via sampling. If we do not consider the constraints, under mild assumption, the optimization process is equivalent to sample from a Boltzmann distribution q β (x) ∝ exp[-βh(x)], in the limit where β → ∞, where β is an inverse temperature parameter. This is the result of the following proposition which can be found in [[52,](#b51)[Theorem 2.1]](#). Proposition 1. Assume that h ∈ C 3 (R d , R). Assume that {x ⋆ i } M i=1 is the set of minimizers of h. Let p be a density on R d such that there exists i 0 ∈ {1, . . . , M } with p(x ⋆ i0 ) > 0. Then Q β the distribution with density w.r.t the Lebesgue measure ∝ q β (x)p(x) weakly converges to Q ∞ as β → ∞ and we have that

$Q ∞ = M i=1 a i δ x ⋆ i / M i=1 a i ,(a)$Diffusion model

## Unconstrained sampling

DiffOPT Minimizer (b)

Figure [1](#): Constrained optimization as a sampling from the product of densities. That is, we minimize the objective function h(x) (red stars denote the minimizers) within the feasible set C, which is given by samples {x i } N i=1 ∼ p(x). This problem is equivalent to sampling from the density π β (x) ∝ p(x) exp[-βh(x)], which concentrates around minimizers of h(x) within the feasible set C. The distribution we sample from is shown on the left and the trajectory we take to sample is shown on the right.

$with a i = p(x ⋆ i ) det(∇ 2 h(x ⋆ i )) -1/2 .$Based on this proposition, [[39]](#b38) proposed a tempered method for global optimization. However, the proposed temperature schedule scales logarithmically with the number of steps; hence, the total number of iterations scales exponentially, hindering this method's straightforward application. In practice, we can sample from the density with the target high β, see [[83,](#b82)[74,](#b73)[25]](#b24).

Product of Experts. To constrain our optimization procedure to the feasible set C, we propose to model the target density as a product of experts [[47]](#b46), a modeling approach representing the "unanimous vote" of independent models. Given the density models of m ∈ N "experts" {q i (x)} m i=1 , the target density of their product is defined as

$π(x) ∝ m i=1 q i (x) .$Hence, if one of the experts yields zero density at x, the total density of the product at x is zero.

Diffusion Models. Given a dataset D = {x i } N i=1 ∼ p data (x) ⊗N with p data concentrated on the feasible set C, we will learn a generative model p such that p ≈ p data using a diffusion model [[93,](#b92)[96,](#b95)[49]](#b48). The density p will then be used in our product of experts model to enforce the feasibility constraints, see Section 3.1. In diffusion models, we first simulate a forward noising process starting from data distribution p 0 (x) = p data (x) which converges to the standard Gaussian distribution p T (x) ≈ N (0, Id) as T → ∞. The forward process is defined by the following stochastic differential equation (SDE)

$dx t = f (x t , t)dt + g(t)dw t , x 0 ∼ p data (x), 0 ≤ t ≤ T ,$where f : R d → R d is a vector-valued drift function, g(t) : R → R is a scalar-valued diffusion coefficient, and (w t ) t≥0 is a d-dimensional Brownian motion. Then, under mild assumptions, the reverse process that generates data from normal noise follows the backward SDE [[45,](#b44)[2]](#b1)

$dx t = -f (x t , τ ) + g 2 (τ )∇ x log p τ (x t ) dt + g(τ )dw t ,$where τ = T -t and ∇ x log p τ (x) is the score function which is modeled by a time-dependent neural network via the score matching objective

$E t λ(t)E x 0 E x t |x 0 ∥s θ (x t , t) -∇ x log p t|0 (x t |x 0 )∥ 2 2$, where p t|0 (x t |x 0 ) is the conditional density of the forward SDE starting at x 0 , and λ(t) > 0 is a weighting function. Under assumptions on f and g, there exists α, σ such that for any t ∈ [0, T ], x t = α t x 0 + σ t ε, where ε ∼ N (0, Id), and therefore one does not need to integrate the forward SDE to sample (x 0 , x t ). Recent works have shown that diffusion models can detect the underlying data manifold supporting the data distribution [[81,](#b80)[24]](#b23). This justifies the use of the output distribution of a diffusion model as a way to identify the feasible set.

## Proposed Method

In this section, we present our method, DIFFOPT. First, we formulate the optimization process as a sampling problem from the product of the data distribution concentrated on the manifold and the Boltzmann distribution defined by the objective function. Then we propose two sampling methods with diffusion models for different types of objective functions.

## Constrained Optimization as Sampling from Product of Experts

We recall that q β (x) ∝ exp[-βh(x)], where h is the objective function. While it is possible to sample directly from q β (x), the generated samples may fall outside of the feasible set C, defined by the dataset D. To address this, we opt to sample from the product of the data distribution and the Bolzmann distribution induced by the objective function. Explicitly, our goal is to sample from a distribution π β (x) defined as

$π β (x) ∝ p(x)q β (x).(1)$Using Proposition 1, we have that π β concentrates on the feasible minimizers of h as β → +∞.

It should be noted that Equation (1) satisfies the following properties: (a) it assigns high likelihoods to points x that simultaneously exhibit sufficiently high likelihoods under both the base distributions p(x) and q β (x); (b) it assigns low likelihoods to points x that display close-to-zero likelihood under either one or both of these base distributions. This approach ensures that the generated samples not only remain within the confines of the data manifold but also achieve low objective values.

In practice, the objective function h can be either differentiable or non-differentiable. In the following sections, we will propose two sampling methods using diffusion models, for each type respectively.

## Differentiable Objective: Two-stage Sampling with Optimization-guided Diffusion

Under mild assumptions, the following SDE converges to π β w.r.t. the total variation distance [[86]](#b85)

$dx t = ∇ x log π β (x t )dt + √ 2dw t .(2)$where the gradient of the unnormalized log-density can be conveniently expressed as the sum of the scores, i.e., ∇ x log π β (x) = ∇ x log p(x) + ∇ x log q β (x). Furthermore, one can introduce a Metropolis-Hastings (MH) correction step to guarantee convergence to the target distribution when using discretized version of Equation (2) [[31]](#b30), which is known as the Metropolis-Adjusted-Langevin-Algorithm (MALA) [[41]](#b40). We provide more details for both algorithms in Appendix C.

Theoretically, sampling from the density in Equation ( [1](#formula_6)) can be done via MALA. However, in practice, the efficiency of this algorithm significantly depends on the choice of the initial distribution and the step size schedule. The latter is heavily linked with the Lipschitz constant of log q β which controls the stability of the algorithm. Large values of β, necessary to get accurate minimizers, also yield large Lipschitz constants which in turn impose small stepsizes. Moreover, the gradient of the log-density can be undefined outside the feasibility set C.

To circumvent these practical issues, we propose sampling in two stages: a warm-up stage and a sampling stage. The former aims to provide a good initialization for the sampling stage. The sampling stage follows the Langevin dynamics for further correction. The pseudocode for both stages is provided in Appendix D.

Stage I: Warm-up with guided diffusion. In imaging inverse problems, it is customary to consider guided diffusion models to enforce some external condition, see [[20,](#b19)[21,](#b20)[94,](#b93)[117]](#b116). In our setting, we adopt a similar strategy where the guidance term is given by βh, i.e., we consider

$dx t = [-f (x t , τ ) + g 2 (τ )s θ (x t , τ ) -β∇h(x t )]dt + g(τ )dw t , τ = T -t.(3)$Theorem 1. Under assumptions on p, h, we denote p β T -t the distribution of Equation (3) at time t and there exists C > 0 such that for any

$x ∈ R d (1/C)p β 0 (x) ≤ p β 0 (x) ≤ C pβ 0 (x)$, where p β 0 is the output of the warm-up guided diffusion process and pβ 0 (x) = p 0 (x) exp[log(β 0 )W β 0 (x)], with W β 0 (x) = ∆h(x) + ⟨∇ log p β 0 (x), ∇h(x)⟩ and β 0 is the inverse temperature parameter at the end of the process The proof is postponed to Appendix H. As an immediate consequence of Theorem 1, we have that lim β0→∞ p β 0 (x ⋆ ) = +∞ for every local strict minimizer x ⋆ of h within the support of p 0 (x), and lim β0→∞ p β 0 (x ⋆ ) = 0 for x ⋆ outside the support of p 0 (x), see Appendix H. That is, p β 0 concentrates on the feasible local minimizers of h as β 0 → +∞.

Theorem 1 indicates that the guided diffusion process in the first stage yields a more effectively initialized distribution within the data manifold, bounded both above and below by a product of experts related to the original constrained optimization problem. However, it is known that guided diffusion cannot accurately sample from the product of experts q β [[29,](#b28)[36]](#b35), see more details in Appendix E. While additional contrastive training of a surrogate objective has been proposed [[73]](#b72), in this work, we do not consider such complex corrections. Instead, we rely on ideas from MCMC literature to ensure convergence.

Stage II: Further correction with Langevin dynamics. In the second stage, we further use Langevin dynamics for accurate sampling from π β (x). The gradient of the log-density of the data distribution ∇ log p(x) can be obtained by setting the time of the score function to 0, i.e., s θ (x, 0). The unadjusted Langevin algorithm is then given by

$x k+1 = x k + (s θ (x k , 0) -β∇ x h(x k ))∆t + √ 2∆tz,$where ∆t is the step size and z comes from a Gaussian distribution. In practice, we find that a constant β is enough for this stage. When using the score-based parameterization s θ (x, t), we cannot access the unnormalized log-density of the distribution. Therefore, we cannot use the MH correction step. Although the sampler is not exact without MH correction, it performs well in practice.

To further incorporate the MH correction step, we can adopt an energy-based parameterization following [[29]](#b28):

$E θ (x, t) = -1 2 ∥NN θ (x, t)∥ 2$, where E θ (x, t) is the energy function of the data distribution, and NN θ (x, t) is a vector-output neural network. An additional benefit of MH correction is that it can enforce hard constraints. If x k+1 is outside the data distribution (p(x) = 0), this point cannot be accepted. We provide more details of this energy-based parameterization in Appendix G.

## Non-differentiable Objective: Iterative Importance Sampling with Diffusion Models

When gradients of the objective function are unavailable, we can use self-normalized importance sampling (SNIS) [[87]](#b86), which is easy and fast to implement. SNIS first proposes several particles from a proposal distribution. Then, resample the particles according to their weights, calculated as the ratio of the target density to the proposal density.

However, the performance of SNIS is heavily determined by the proposal distribution. A good proposal distribution should be close to the target distribution. To address this, we employ an iterative importance sampling with diffusion models to improve the initial proposal distribution. The pseudocode of our derivative-free sampling algorithm is provided in Appendix F.

Initialization with SNIS. We begin by randomly sampling S particles {x 0 s } S s=1 from the diffusion model p θ (x), assigning each particle a weight w 0 s = π β (x 0 s ) p θ (x 0 s ) = q β (x 0 s ). We then resample S particles based on the normalized weights w0 s = w 0 s S j=1 w 0 j using multinomial sampling with replacement.

Diffusion-guided proposal for iterative importance sampling. At the k-th iteration, we have S particles {x k-1 s } S s=1 resampled from the previous iteration. The key insight of our method is that these resampled particles form a proposal distribution closer to the target distribution. However, these resampled particles tend to be repetitive and lack diversity. To address this, we use a diffusion model to diversify the resampled particles, ensuring that the proposal distribution remains diverse and closely approximates the target distribution. Henceforth, we will use x k s and x k s,0 interchangeably to refer to the particle at time 0. We also omit the arrows denoting forward/reverse process of diffusion models for simplicity.

The diversify process works as follows: we first add noise to the particles x k-1 s,0 through a forward diffusion process until a randomly sampled time t to get x k-1 s,t ∼ p t|0 (x k-1 s,t |x k-1 s,0 ). Then we denoise them back to obtain new particles x k s,0 ∼ p 0|t (x k s,0 |x k-1 s,t ). This proposal can be written as

$Q(x k s |x k-1 s ) = p t|0 (x k-1 s,t |x k-1 s,0 )p 0|t (x k s,0 |x k-1 s,t ) dx k-1 s,t .$We use the Monte Carlo method to compute the marginalization by drawing J samples x k-1 s,t,j for each particle:

$Q(x k s |x k-1 s ) ≈ 1 J J j=1 p 0|t (x k s,0 |x k-1 s,t,j ).$Sampling x k-1 s,t,j is straightforward as the forward process in diffusion models admits a closed-form conditional Gaussian distribution:

$p t|0 (x k-1 s,t |x k-1 s,0 ) = N (x k-1 s,t |α t x k-1 s,0 , σ 2 t Id). Following [95], we approximate p(x k s,0 |x k-1 s,t ) as N ( x k-1 s,t,j +σts θ (x k-1 s,t,j ,t) αt , σ 2 t 1+σt Id). The marginalization Q(x k s$) is computed by enumerating all the particles from last iteration, i.e., 1

$S S s ′ =1 Q(x k s |x k-1 s ′ ). Each particle is assigned a weight w k s = π β (x k s ) Q(x k s )$. We then resample S particles based on the normalized weights. This process is iterated for K steps.

Note that by involving only resampling and diffusion models in each iteration, we can ensure staying within the data manifold, thereby satisfying hard constraints.

## Related Work

Diffusion models and data manifold. Diffusion models have demonstrated impressive performance in various generative modeling tasks, such as image generation [[97,](#b96)[49]](#b48), video generation [[48]](#b47), and protein synthesis [[112,](#b111)[42]](#b41). Several studies reveal diffusion models implicitly learn the data manifold [[81,](#b80)[26,](#b25)[28,](#b27)[113]](#b112). This feature of diffusion models has been used to estimate the intrinsic dimension of the data manifold in [[99]](#b98). Moreover, the concentration of the samples on a manifold can be observed through the singularity of the score function. This phenomenon is well-understood from a theoretical point of view and has been acknowledged in [[24,](#b23)[17]](#b16).

Optimization as sampling problems. Numerous studies have investigated the relationship between optimization and sampling [[74,](#b73)[100,](#b99)[108,](#b107)[114,](#b113)[18,](#b17)[19]](#b18). Sampling-based methods have been successfully applied in various applications of stochastic optimization when the solution space is too large to search exhaustively [[67]](#b66) or when the objective function exhibits noise [[11]](#b10) or countless local optima [[14,](#b13)[13]](#b12). A prominent solution to global optimization is through sampling with Langevin dynamics [[39]](#b38), which simulates the evolution of particles driven by a potential energy function. Furthermore, simulated annealing [[59]](#b58) employs local thermal fluctuations enforced by Metropolis-Hastings updates to escape local minima [[76,](#b75)[44]](#b43). More recently, [[120]](#b119) employs generative flow networks to amortize the cost of the sampling process for combinatorial optimization with both closed-form objectives and constraints.

Learning for optimization. Recently, there has been a growing trend of adopting machine learning methods for optimization tasks. The first branch of work is model-based optimization, which focuses on learning a surrogate model for the black-box objective function. This model can be developed in either an online [[92,](#b91)[90,](#b89)[98,](#b97)[121]](#b120), or an offline manner [[118,](#b117)[107,](#b106)[34,](#b33)[16,](#b15)[119]](#b118). Additionally, some research [[65,](#b64)[64,](#b63)[58]](#b57) has explored the learning of stochastic inverse mappings from function values to the input domain, utilizing generative models such as generative adversarial nets [[40]](#b39) and diffusion models [[97,](#b96)[49]](#b48).

The second branch, known as "Learning to Optimize", involves training a neural network to address fully specified optimization problems. In these works, a model is trained using a distribution of homogeneous instances, to achieve generalization on similar unseen instances. Various learning paradigms have been used in this context, including supervised learning [[71,](#b70)[37]](#b36), reinforcement learning [[69,](#b68)[57,](#b56)[61]](#b60), unsupervised learning [[56,](#b55)[111,](#b110)[77]](#b76), and generative modeling [[101,](#b100)[70]](#b69). Several surveys [[8,](#b7)[62,](#b61)[75]](#b74) have covered different facets of this research area. In contrast to our approach, these works typically involve explicitly defined objectives and constraints.

Optimization under unknown constraints. Existing work on optimization under unknown constraints are based on derivative-free optimization methods, such as generalized pattern search [[3]](#b2), mesh adaptive direct search [[4]](#b3), line search [[33,](#b32)[72]](#b71), the Frank-Wolfe algorithm [[109]](#b108), and stochastic zeroth-order constraint extrapolation [[78]](#b77).

However, these works differ from ours and are not directly comparable. First, the settings are different-they require oracle evaluation of whether a proposed solution violates constraints during the optimization process, whereas we do not need this. We only require data samples from the feasible set. Second, these works are hardly applied to the high-dimensional problems that we focus on, such as molecule optimization, protein design, and robot morphology optimization.

Step 1

Step 10

Step 100

Step 500

Step 1000 Step 1

Step 10

Step 100

Step 500

Step 1000 

## Experiment

In this section, we conduct experiments on (1) a synthetic Branin task, (2) six real-world offline black-box optimization tasks, and (3) a multi-objective molecule optimization task. Finally, we do ablation studies to verify the effectiveness of each model design of DIFFOPT.

## Synthetic Branin Function Optimization

We first validate our model on a synthetic Branin task. The Branin function [[27]](#b26) is given as

$f (x 1 , x 2 ) = a(x 2 -bx 2 1 + cx 1 -r) 2 + s(1 -t)cos(x 1 ) + s, where a = 1, b = 5.1 4π 2 , c = 5 π , r = 6, s = 10, t = 1 8π$. The function has three global minimas, (-π, 12.275), (π, 2.275), and (9.42478, 2.475).

Optimization with unknown constraints. To assess the capability of DIFFOPT in optimizing functions under unknown constraints, we generate a dataset of 6,000 points, uniformly distributed within the feasible domain shaped like an oval. An effective optimizer is expected to infer the feasible space from the dataset and yield solutions strictly within the permissible region, i.e., (-π, 12.275) and (π, 2.275). We train a diffusion model with Variance Preserving (VP) SDE [[97]](#b96) on this dataset. More details of the experimental setup are provided in Appendix I. Figure [2](#fig_0) illustrates the sampling trajectory of DIFFOPT, clearly demonstrating its capability in guiding the samples towards the optimal points confined to the feasible space.

Compatible with additional known constraints. Our framework is also adaptable to scenarios with additional, known constraints C ′ . In such instances, we introduce an extra objective function whose Boltzmann density is uniform within the constraint bounds and 0 otherwise, i.e., q ′ (x) ∝ exp[β ′ • I(x ∈ C ′ )], with I(•) being the indicator function. To demonstrate this capability, we incorporate a closed-form linear constraint alongside the implicit constraint represented by the dataset. This new constraint narrows the feasible solutions to only (π, 2.275). As depicted in Figure [3](#fig_1), DIFFOPT effectively navigates towards the sole viable minimizer within the constrained space delineated by the data-driven and explicitly stated constraints. This feature is particularly beneficial in practical applications, such as molecular optimization, where imposing additional spatial or structure constraints might be necessary during optimizing binding affinities with different protein targets [[30]](#b29).

## Offline Black-box Optimization

We further evaluate DIFFOPT on the offline black-box optimization task, wherein a logged dataset is utilized to train a surrogate model that approximates the objective function. The surrogate model is Table 2: Results on the multi-objective molecule optimization task. Sum denotes the total objective (QED + GSK3B -SA). SA is normalized from the range 1-10 to 0-1. We report the mean and standard deviation across five random seeds. The best results are bolded, and the second best is underlined. Top-1 denotes the best solution found, and Top-10 denotes the average of the best ten solutions found. Invalidity denotes the number of invalid molecules in the 1000 generated samples.

trained on finite data and may have large fitness errors beyond the data distribution. At inference time, if we directly apply gradient ascent on the surrogate objective, it may produce out-of-distribution designs that "fool" the learned surrogate model into outputting a high value. Therefore, we need to constrain the optimization process within the data distribution because the trained surrogate is only reliable within this range. However, this data distribution constraints cannot be expressed analytically, and therefore this task can be viewed as an instance of optimization with unknown constraints.

Following [[64]](#b63), we conduct evaluation on six tasks of DesignBench [[106]](#b105). Superconductor is to optimize for finding a superconducting material with high critical temperature. Ant nad D'Kitty is to optimize the robot morphology. TFBind8 and TFBind10 are to find a DNA sequence that maximizes binding affinity with specific transcription factors. ChEMBL is to optimize the drugs for a particular chemical property.

Baselines. We compare DIFFOPT with multiple baselines, including gradient ascent, Bayesian optimization (GP-qEI) [[64]](#b63), REINFORCE [[102]](#b101), evolutionary algorithm (CAM-ES) [[43]](#b42), and recent methods like MINS [[65]](#b64), COMs [[105]](#b104), CbAS [[12]](#b11) and DDOM [[64]](#b63). We follow [[64]](#b63) and set the sampling budget as 256. More details of the experimental setups are provided in Appendix I.

Results. Table [1](#tab_0) shows the performance on the six datasets for all the methods. As we can see, DIFFOPT achieves an average rank of 2.0, the best among all the methods. We achieve the best result on 4 tasks. Particularly, in the Superconductor task, DIFFOPT surpasses all baseline methods by a significant margin, improving upon the closest competitor by 9.6%. The exceptional performance of DIFFOPT is primarily due to its application of a diffusion model to learn the valid data manifold directly from the data set, thus rendering the optimization process significantly more reliable. In contrast, the gradient ascent method, which relies solely on optimizing the trained surrogate model, is prone to settle on suboptimal solutions. Moreover, while DDOM [[64]](#b63) employs a conditional diffusion model to learn an inverse mapping from objective values to the input space, its ability to generate samples is confined to the maximum values present in the offline dataset. This limitation restricts its ability to identify global maximizers within the feasible space. The experimental results also demonstrate that DIFFOPT can consistently outperform DDOM except for the Ant Dataset. On Ant, we find that the suboptimal performance of DIFFOPT is because the surrogate model is extremely difficult to train. We provide more analyses in Appendix J.

## Multi-objective Molecule Optimization

An additional advantage of incorporating the data distribution constraints for offline black-box optimization is their direct impact on enhancing the validity of generated solutions. However, it's worth noting that DesignBench lacks a specific metric for assessing validity. Therefore, we further test on a multi-objective molecule optimization task and extend our evaluation to include validity. In this task, we have three objectives: the maximization of the quantitative estimate of drug-likeness (QED), and the activity against glycogen synthase kinase three beta enzyme (GSK3B), and the minimization of the synthetic accessibility score (SA). Following [[55,](#b54)[32]](#b31), we utilize a pre-trained autoencoder from [[55]](#b54) to map discrete molecular structures into a continuous low-dimensional latent space and train neural networks as proxy functions for predicting molecular properties to guide the optimization. The detailed experimental setups are provided in Appendix I.

Results. For each method, we generate 1,000 candidate solutions, evaluating the three objective metrics solely on those that are valid. As we can see from Table [2](#), DIFFOPT can achieve the best validity performance among all the methods. In terms of optimization performance, DIFFOPT can achieve the best overall objective value. We further report the average of the top 10 solutions found by each model and find that DiffOPT is also reliable in this scenario. This multi-objective optimization setting is particularly challenging, as different objectives can conflict with each other. The superior performance of DIFFOPT is because we formulate the optimization problem as a sample problem from the product of experts, which is easy for compositions of various objectives.

## Derivative-free Optimization

Hypervolume In this subsection, we explore the scenario where the objective function is not differentiable and examine the effectiveness of the proposed iterative importance sampling with diffusion models. We continue using the molecule optimization task. Here, we do not train a surrogate objective using offline data but instead directly use the oracle function from Rdkit [[85]](#b84). We compare our method with evolutionary algorithm (EA; [[43]](#b42)), which is a common technique in derivative-free multi-objective optimization. Additionally, we compare our method with one-step importance sampling (IS) using diffusion models as the proposal distribution. Table [3](#tab_2) provides the performance of all the methods. As we can see, DIFFOPT can achieve the best hypervolume and validity among all the methods.

## Ablation Study

In this section, we conduct ablation studies to explore the two-stage sampling. We also study the impact of annealing strategies and sample efficiency of DIFFOPT in Appendix K. Impact of two-stage sampling. Table [4](#tab_3) shows the impact of two-stage sampling on performance. Our findings reveal that even after the initial stage, DIFFOPT outperforms the top-performing baseline on both datasets. Relying solely on Langevin dynamics, without the warm-up phase of guided diffusion, results in significantly poorer results. This aligns with our discussion in Section 3.2, where we attributed this failure to factors such as the starting distribution, the schedule for step size adjustments, and the challenges posed by undefined gradients outside the feasible set. Integrating both stages yields a performance improvement as the initial stage can provide a better initialization within the data manifold for the later stage (Theorem 1). Adding the MH correction step further enhances results, leading to the best performance observed.

## Conclusion

In this paper, we propose DIFFOPT to solve optimization problems where analytic constraints are unavailable. We learn the unknown feasible space from data using a diffusion model and then reformulate the original problem as a sampling problem from the product of (i) the density of the data distribution learned by the diffusion model and (ii) the Boltzmann density defined by the objective function. For differentiable objectives, we propose a two-stage framework consisting of a guided diffusion stage for warm-up and a Langevin dynamics stage for further correction. For non-differentiable objectives, we propose an iterative importance sampling method with diffusion models as the proposal distribution. Our experiments validate the effectiveness of DIFFOPT. Due to the space limit, we discuss limitations and future work in Appendix A.

## D Pseudocode of the

Two-Stage Sampling 17 E Illustration of Why Guided Diffusion Cannot Sample from the Product of Experts 17 F Pseudocode of the Proposed Derivative-free Sampling 18 G Energy-based Parameterization 18 H End Distribution of the Warm-up Stage 19 I Experiment Details 22 I.1 Computing Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 I.2 Synthetic Branin Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 I.3 Offline Black-box Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 I.4 Multi-objective Molecule Optimization . . . . . . . . . . . . . . . . . . . . . . . 23 J Analysis on Ant Dataset 24 K Additional Ablation Studies 25

A Limitations and Future Work

We discuss limitations and possible extensions of DIFFOPT. (i) Manifold preserving. The guided diffusion may deviate from the manifold that the score network is trained, leading to error accumulations. One approach to mitigate this is to incorporate manifold constraints during the guided diffusion phase [[21,](#b20)[46]](#b45). (ii) Online learning. We have applied DIFFOPT in the offline black-box optimization (BBO) setting. Considering the unknown constraints not only benefits the offline setting but also helps the online BBO. In the context of online BBO, we propose molecules and subsequently receive evaluations from the ground-truth simulator at each iteration to train the surrogate objective. Proposing a higher proportion of valid molecules can significantly increase the sampling efficiency of training the surrogate objective.

## B Broader Impacts

Optimization techniques can be used to solve a wide range of real-world problems, from decision making (planning, reasoning, and scheduling), to solving PDEs, and to designing new drugs and materials. The method we present in this paper extends the scope of the previous study to a more realistic setting where (partial) constraints for optimization problems are unknown, but we have access to samples from the feasible space. We expect that by learning the feasible set from data, our work can bring a positive impact to the community in accelerating solving real-world optimization problems and finding more realistic solutions. However, care should be taken to prevent the method from being used in harmful settings, such as optimizing drugs to enhance detrimental side effects.

## C (Metropolis-adjusted) Langevin Dynamics.

Langevin dynamics is a class of Markov Chain Monte Carlo (MCMC) algorithms that aims to generate samples from an unnormalized density π(x) by simulating the differential equation

$dx t = ∇ x log π(x t )dt + √ 2dw t .(4)$Theoretically, the continuous SDE of Equation ( [4](#formula_17)) is able to draw exact samples from π(x). However, in practice, one needs to discretize the SDE using numerical methods such as the Euler-Maruyama method [[60]](#b59) for simulation. The Euler-Maruyama approximation of Equation ( [4](#formula_17)) is given by

$x t+∆t = x t + ∇ log π(x) + √ 2∆tz, z ∼ N (0, 1), (5$$)$where ∆t is the step size. By drawing x 0 from an initial distribution and then simulating the dynamics in Equation ( [4](#formula_17)), we can generate samples from π(x) after a 'burn-in' period. This algorithm is known as the Unadjusted Langevin Algorithm (ULA) [[86]](#b85), which requires ∇ log π(x) to be L-Lipschitz for stability.

The ULA always accepts the new sample proposed by Equation ( [5](#formula_18)). In contrast, to mitigate the discretization error when using a large step size, the Metropolis-adjusted Langevin Algorithm (MALA) [[41]](#b40) uses the Metropolis-Hastings algorithm to accept or reject the proposed sample. Specifically, we first generate a proposed update x with Equation ( [4](#formula_17)), then with probability min(1, π(x)N (x|x+∆t•∇ x log π(x),2∆t) π(x)N (x|x+∆t•∇x log π(x),2∆t) ), we set x t+∆t = x, otherwise x t+∆ = x t . We provide the pseudocode of both algorithms in Algorithm 1.

## Algorithm 1 Sampling via the (Metropolis-Adjusted) Langevin dynamics

Require: unnormalized density π(x), step size ∆t 1: x ∼ initial distribution 2: X = ∅ 3: for number of iterations do 4:

$x = x + ∇ x log π(x)∆t + √ 2∆t • z, z ∼ N (0, 1) 5:$if applying Metropolis-Hastings test then 6:

u ∼ Uniform[0, 1]

## 7:

log P accept = log π(x)N (x|x+∆t•∇ x log π(x),2∆t) π(x)N (x|x+∆t•∇x log π(x),2∆t)

8:

if log P accept > log u, then x ← x 9:

else 10:

x ← x 11:

end if

## 12:

X ← X ∪ x 13: end for 14: Return X

## D Pseudocode of the Two-Stage Sampling

The pseudocode for the proposed two-stage sampling method is provided in Algorithm 2.

## E Illustration of Why Guided Diffusion Cannot Sample from the Product of Experts

The primary limitation of relying solely on Stage I is its inability to theoretically sample from our desired true target distribution, the product of distributions π β ∝ p(x)q β (x). This is because the score of the diffused marginal distribution does not directly correspond to the aggregate of the scores from each individual distribution,

$∇ x log π t β (x t ) = ∇ x log p 0 (x 0 )q β (x 0 )p t|0 (x t |x 0 )dx 0 ̸ = ∇ x log q β (x t ) + ∇ x log p 0 (x 0 )p t|0 (x t |x 0 )dx 0 , pt($xt) t > 0, where p t|0 (x t |x 0 ) is the conditional density of the forward SDE starting at x 0 . Algorithm 2 Sampling via DIFFOPT for differentiable objective Require: inverse temperature schedule β(t), diffusion volatility schedule g(t) and drift f (x, t), score model s θ (x t , t), energy function of the data distribution E θ (x, t) if applying the MH correction. 1: X ← ∅ 2: Sample x 0 ∼ N (0, Id) 3: // Stage I: Warm-up with guided diffusion. 4: for t = 0, . . . , T do 5:

Draw z ∼ N (0, Id), define τ = T -t 6:

$x t+∆t ← x t + [-f (x t , τ ) + g 2 (τ )s θ (x t , τ ) 7: -β(τ )∇ xt h(x t )]∆t + g(τ )$√ ∆tz 8: end for 9: // Stage II: Further correction with Langevin dynamics. 10: for t = T, . . . , T ′ do 11:

Draw z ∼ N (0, Id)

$12: x ← x + [s θ (x, 0) -β∇h(x)]∆t + √ 2∆tz 13:$if applying Metropolis-Hastings test then 14:

$u ∼ Uniform[0, 1]$15:

$ℓ θ (x) = E θ (x, 0) -βh(x)$16:

$ℓ θ (x) = E θ (x, 0) -βh(x) 17: ℓ(x, x) = -∥x -x -∆t[s θ (x, 0) -β∇h(x)]∥ 2 18: ℓ(x, x) = -∥x -x -∆t[s θ (x, 0) -β∇h(x)]∥ 2$19:

$ℓ acc = ℓ θ (x) -ℓ θ (x) + (ℓ(x, x) -ℓ(x, x))/(2∆t) 20:$if ℓ acc > log(u), then x ← x 21:

else 22:

x ← x 23:

end if

## 24:

X ← X ∪ {x} 25: end for 26: Return X Therefore we have to include Langevin dynamics (Stage II) as an optional stage to correct the bias introduced in Stage I. This stage can provide a theoretical guarantee for drawing exact samples from the product of distributions, despite the empirical observation that it offers only marginal performance improvements.

It's important to note that Stage I is essential because its output focuses on feasible minimizers under specific conditions, offering an improved initialization for Langevin dynamics. This has been demonstrated in our ablation study.

## F Pseudocode of the Proposed Derivative-free Sampling

The pseudocode of our derivative-free sampling algorithm is provided in Algorithm 3.

## G Energy-based Parameterization

In a standard diffusion model, we learn the score of the data distribution directly as s θ (x, t) = ∇ log p t (x). This parameterization can be used for ULA, which only requires gradients of loglikelihood. However, to incorporate the Metropolis-Hastings (MH) correction step, access to the unnormalized density of the data distribution is necessary to calculate the acceptance probability.

To enable the use of MH correction, we can instead learn the energy function of the data distribution, i.e., p(x, t) ∝ e E θ (x,t) . The simplest approach is to use a scalar-output neural network, denoted as NN θ (x, t) : R d × R → R, to parameterize E θ (x, t). By taking the gradient of this energy function with respect to the input x, we can derive the score of the data distribution. However, existing works have shown that this parameterization can cause difficulties during model training [[88]](#b87). Following the approach by [[29]](#b28), we define the energy function as

$E θ (x, t) = -1 2 |NN θ (x, t)| 2 2$, where NN θ (x, t) is a vector-output neural network mapping from R d × R to R. Consequently, the score of the data distribution is represented as s θ (x, t) = -NN θ (x, t)∇ x NN θ (x, t).

Algorithm 3 Sampling via DIFFOPT for non-differentiable objective Require: inverse temperature schedule β(t), diffusion volatility schedule g(t) and drift f (x, t), score model s θ (x t , t). 1: // Initialization 2: Sample S particles {x 0 s } S s=1 from diffusion model 3: Compute w 0 s Add noise to the particle by forward diffusion until time t:

$= π β (x 0 s ) p θ (x 0 s ) = q β (x 0 s )$$x k-1 s,t ∼ p t|0 (x k-1 s,t |x k-1 s,0 ) 12:$Denoise the particle through backward diffusion:

$x k s,0 ∼ p 0|t (x k s,0 |x k-1 s,t ) 13: Sample x k-1 s,t,j ∼ p t|0 (x k-1 s,t |x k-1 s,0 ) = N (x k-1 s,t |α t x k-1 s,0 , σ 2 t 1+σt$Id) for J times 14:

$Q(x k s |x k-1 s ) ≈ 1 J J j=1 N xs,t,j +σts θ (x k-1 s,t,j ,t) αt , σ 2 t 1+σt Id 15: Marginalize to get Q(x k s ) ≈ 1 S S s=1 Q(x k s |x k-1 s ) 16:$end for

$17: Compute w k s = π β (x k s ) Q(x k s ) for each particle 18:$Normalize the weight for each particle:

$wk s = w k s S j=1 w k j 19:$Resample M particles {x k s } S s=1 according to the weights 20: end for 21: Return X

## H End Distribution of the Warm-up Stage

In this section, we study in further detail the warm-up stage of DIFFOPT. We recall that we consider a process of the following form

$dy β t = [-f (y β t , 1-t)+g(1-t) 2 ∇ log p 1-t (y β t )-β(1-t)∇h(y β t )]dt+g(1-t)dw t , y β 0 ∼ x T(6$) where T = 1 and p t is the density w.r.t. Lebesgue measure of the distribution of x t where

$dx t = f (t, x t )dt + g(t)dw t , x 0 ∼ p 0 .(7)$We recall that under mild assumption [[15]](#b14), we have that

$(ŷ t ) t∈[0,1] = (x 1-t ) t∈[0,1] satisfies dŷ t = [-f (ŷ t , 1 -t) + g(1 -t) 2 ∇ log p 1-t (ŷ t )]dt + g(1 -t)dw t , ŷ0 = x T .$Let us highlight some differences between ( [6](#formula_35)) and the warm-up process described in Algorithm 2. First, we note that we do not consider an approximation of the score but the real score function ∇ log p t . In addition, we do not consider a discretization of ( [6](#formula_35)). This difference is mainly technical. The discretization of diffusion processes is a well-studied topic, and we refer to [[26,](#b25)[9,](#b8)[22,](#b21)[17]](#b16) for a thorough investigation. Our contribution to this work is orthogonal as we are interested in the role of h on the distribution. Our main result is Proposition 4 and details how the end distribution of the warm-up process concentrates on the minimizers of h, which also support the data distribution p 0 .

We first show that under assumptions on h, q β T , the density w.r.t. the Lebesgue measure of y T has the same support as p 0 . We denote by supp(p 0 ) the support of p 0 . We consider the following assumption. Assumption 1. We have that for any t ∈ [0, 1], g(t) = g(0) and f (t, x) = -γ 0 x with γ 0 > 0. Assume that p 0 has bounded support, i.e. there exists R > 0 such that supp(p 0 ) ⊂ B(0, R) with B(0, R) the ball with center 0. In addition, assume that h is Lipschitz.

Then, we have the following proposition. This can also be rewritten as

$∂ t p β t + ⟨f t -g 2 t ∇ log p β t , ∇p β t ⟩ + div(f t -g 2 t ∇ log p t )p β t + (g 2 t /2)∆p β t + div(β t ∇hp β t ) = 0.(10$) Finally, this can be rewritten as

$∂ t p β t +⟨f t -g 2 t ∇ log p t , ∇p β t ⟩+(g 2 t /2)∆p β t +div(f t -g 2 t ∇ log p t )p β t +β t {⟨∇h, ∇ log p β t ⟩+∆h}p β t = 0.$In what follows, we denote

$µ t = f t -g 2 t ∇ log p t , V t = div(f t -g 2 t ∇ log p t ), W β t = ⟨∇h, ∇ log p β t ⟩ + ∆h.(11$) Hence, using [(10)](#b9) we have

$∂p β t + ⟨µ t , ∇ log p β t ⟩ + (g 2 t /2)∆p β t + V t p β t + β t W β t p β t = 0.(12$) Similarly, using [(9)](#b8) we have

$∂p t + ⟨µ t , ∇ log p t ⟩ + (g 2 t /2)∆p t + V t p t = 0.(13)$Therefore, combining (13), A2 and [79, Theorem 7.13] we get that for any

$x ∈ R d p 0 (x) = E[exp[ 1 0 V t (z t )dt]p T (z T ) | z 0 = x],$with dz t = µ t dt + g t dw t and z 0 = x. Similarly, combining [(12)](#b11), A2 and [79, Theorem 7.13] we get that for any

$x ∈ R d p β 0 (x) = E[exp[ 1 0 V t (z t )dt] exp[ 1 0 β t W β t (z t )dt]p T (z T ) | z 0 = x].(14)$Using ( [11](#formula_40)), we have that

$1 0 β t W β t (z t )dt = 1 0 β t {⟨∇h, ∇ log p β t ⟩ + ∆h}(z t )dt. Hence, we have 1 0 β t W β t (z t )dt = log(β 0 ){⟨∇h, ∇ log p β 0 ⟩ + ∆h}(z 0 ) + 1 0 β t (W β t (z t ) -W β 0 (z 0 ))dt. Using A2 we have that -C ≤ 1 0 β t W β t (z t )dt -log(β 0 ){⟨∇h, ∇ log p β 0 ⟩ + ∆h}(z 0 ) ≤ C . Hence, -C+log(β 0 ){⟨∇h, ∇ log p β 0 ⟩+∆h}(z 0 ) ≤ 1 0 β t W β t (z t )dt ≤ C+log(β 0 ){⟨∇h, ∇ log p β 0 ⟩+∆h}(z 0 )$. Combining this result with [(14)](#b13) we get that for any

$x ∈ R d p β 0 (x) ≤ E[exp[ 1 0 V t (z t )dt]p T (z T ) | z 0 = x] exp[log(β 0 ){⟨∇h, ∇ log p β 0 + ∆h⟩}(x)] exp[C] = p 0 (x) exp[log(β 0 ){⟨∇h, ∇ log p β 0 + ∆h⟩}(x)] exp[C]. Similarly, we have for any x ∈ R d p β 0 (x) ≥ p 0 (x) exp[log(β 0 ){⟨∇h, ∇ log p β 0 ⟩ + ∆h}(x)] exp[-C],$which concludes the proof.

In Proposition 3, we show that the output distribution of the warm-up process is upper and lower bounded by a product of experts comprised of (i) p 0 which ensures the feasibility conditions (ii) exp[log(β 0 )W β 0 ] related to the optimization of the objective. While Proposition 3 gives an explicit form for p β 0 , it does not provide insights on the properties of this distribution. However, we can still infer some limiting properties.

$Proposition 4. If x ⋆ is a local strict minimizer of h in the support of p 0 then lim β0→∞ p β 0 (x ⋆ ) = +∞. If x ⋆ is a local strict minimizer of h not in the support of p 0 then lim β0→∞ p β 0 (x ⋆ ) = 0.$Proof. The case where x ⋆ is not in support of p 0 is trivial using Proposition 3. We now assume that x ⋆ is in the support of p 0 . Using Proposition 3 and that ∇h(x ⋆ ) = 0 and ∆h(x ⋆ ) > 0 since the local minimizer x ⋆ is strict, we get that lim β0→∞ p β 0 (x ⋆ ) = +∞, which concludes the proof.

In particular, Proposition 4 shows that the limit distribution of y β T concentrates around the minimizers of h, which is the expected behavior of increasing the inverse temperature. What is also interesting is that we only target the minimizers of h, which are inside the support of p 0 . This is our primary goal, which is constrained optimization of h.

• TFBind10: DNA sequence optimization. Similar to TFBind8, this task aims to find the DNA sequence of length ten that has the maximum binding affinity with transcription factor SIX6_REF_R1. The design space consists of all possible designs of nucleotides. The size of the dataset is 10,000, with a dimension of 10. Since the affinity for the entire design space is available, it uses the ground truth as a direct oracle. • ChEMBL: molecule activity optimization. This task aims to find molecules with a high MCHC value when paired with assay CHEMBL3885882. The dataset consists of 441 samples of dimension 31.

Baselines. We compare with eight baselines on DesignBench tasks. The results of all the baselines are from [[64]](#b63). Gradient ascent learns a surrogate model of the objective function and generates the optimal solution by iteratively performing gradient ascent on the surrogate model. CbAS learns a density model in the design space coupled with a surrogate model of the objective function. It iteratively generates samples and refines the density model on the new samples during training. GP-qEI fits a Gaussian Process on the offline dataset. It employs the quasi-Expected-Inprovement (qEI) acquisition function from Botorch [[5]](#b4) for Bayesian optimization. MINS learns an inverse map from objective value back to design space using a Generative Adversarial Network (GAN). It then obtains optimal solutions through conditional generation. REINFORCE parameterizes a distribution over the design space and adjusts this distribution in a direction that maximizes the efficacy of the surrogate model. COMS learns a conservative surrogate model by regularizing the adversarial samples. It then utilizes gradient ascent to discover the optimal solution. CMAES enhances a distribution over the optimal design by adapting the covariance matrix according to the highest-scoring samples selected by the surrogate model. DDOM learns a conditional diffusion model to learn an inverse mapping from the objective value to the input space.

Implementation details. We build the score network s θ using a simple feed-forward network. This network consists of two hidden layers, each with a width of 1024 units, and employs ReLU as the activation function. The forward process is a Variance Preserving (VP) SDE [[97]](#b96). We set the noise variance limits to a minimum of 0.01 and a maximum of 2.0.

For the surrogate models, we explore various network architectures tailored to different datasets, including Long short-term memory (LSTM) [[50]](#b49), Gaussian Fourier Network, and Deep Kernel Learning (DKL) [[115,](#b114)[116]](#b115). LSTM network uses a single-layer LSTM unit with a hidden dimension of 1024, followed by 1 hidden layer with a dimension of 1024, utilizing ReLU as the activation function. In the Gaussian Fourier regressor, Gaussian Fourier embeddings [[103]](#b102) are applied to the inputs x and t. These embeddings are then processed through a feed-forward network with 3 hidden layers, each of 1024 width, utilizing Tanh as the activation function. This regressor is time-dependent, and its training objective follows the method used by [[97]](#b96) for training time-dependent classifiers in conditional generation. For DKL, we use the ApproximateGP class in Gpytorch[foot_0](#foot_0) , which consists of a deep feature extractor and a Gaussian process (GP). The feature extractor is a simple feed-forward network consisting of 2 hidden layers with a width of 500 and 50, respectively, and ReLU activations.

The GP uses radial basis function (RBF) kernel.

We use a fixed learning rate of 0.001 and a batch size of 128 for both the diffusion and surrogate models. During testing, we follow the evaluation protocol from the [[64]](#b63), sampling 256 candidate solutions. We apply different annealing strategies for different datasets. Specifically, we apply exponential annealing for TFBind8, superconductor, D'Kitty, and ChEMBL. The exponential annealing strategy is defined as

$β(τ ) = β max [1 -exp(-100(T -τ ))]$, where τ = T -t, and a constant β for Ant and TFBind10. Though exponential annealing usually exhibits better performance, we leave the exploration of exponential annealing on TFBind10 and D'Kitty for future work due to time limit. The step size ∆t is 0.001 for the first stage, and 0.0001 for the second stage.

Detailed hyperparameters and network architectures for each dataset are provided in Table [5](#).

## I.4 Multi-objective Molecule Optimization

Dataset details. We curate the dataset by encoding 10000 molecules (randomly selected) from the ChEMBL dataset [[38]](#b37) with HierVAE [[55]](#b54), a commonly used molecule generative model based on VAE, which takes a hierarchical procedure in generating molecules by building blocks. Since Annealing strategy β max Surrogate model TFBind8 Exponential 200 Gaussian Fourier TFBind10 Constant 20 Gaussian Fourier Superconductor Exponential 100 Gaussian Fourier Ant Exponential 30 Gaussian Fourier D'Kitty Constant 3e4 DKL ChEMBL Exponential 100 LSTM Table [5](#): Implementation details on design-bench. β max is the value of β at the end of the annealing process. Oracle details. We evaluate three commonly used molecule optimization oracles including synthesis accessibility (SA), quantitative evaluation of drug-likeness (QED) and activity again target GSK3B from RDKit [[66]](#b65) and TDC [[51]](#b50). All three oracles take as input a SMILES string representation of a molecule and return a scalar value of the property. The oracles are non-differentiable.

Implementation details. We build the score network s θ of the diffusion model using a 2-layer MLP architecture. This network features 1024 hidden dimensions and utilizes the ReLU activation function.

The forward process adheres to a Variance Preserving (VP) SDE proposed by [[97]](#b96). We calibrate the noise variance within this model, setting its minimum at 0.01 and maximum at 2.0.

For the surrogate model of the objective function, we use the ApproximateGP class in Gpytorch[foot_1](#foot_1) , which consists of a deep feature extractor and a Gaussian process. The feature extractor is a simple feed-forward network with two hidden layers, having widths of 500 and 50, respectively, and both employ ReLU activation functions. Regarding model optimization, we apply a fixed learning rate of 0.001 for the diffusion model and 0.01 for the surrogate model. Additionally, we set a batch size of 128 and conduct training over 1000 epochs for both models. For the sampling process, we use a consistent inverse temperature β = 10[foot_2](#foot_2) for all the three objectives. The step size ∆t is 0.001 for the first stage, and 0.0001 for the second stage.

We sample 1000 candidate solutions at test time for all the methods. For DDOM [[64]](#b63), we use their implementation 4 . For other baselines, we use the implementations provided by DesignBench [5](#foot_3) . We tune the hyper-parameters of all the baselines as suggested in their papers.

For derivative-free optimization, the number of iterations is set to 100 for both the evolutionary algorithm and DIFFOPT. The number of particles for all methods remains the same as before, i.e., 1000.

## J Analysis on Ant Dataset

As shown in Table [1](#tab_0), DIFFOPT can only achieve subpar performance on the Ant dataset. This underperformance is primarily due to the difficulty in training the surrogate objective function. An illustration of this challenge is provided in Figure [5](#), where the training loss of the surrogate objective    

## K Additional Ablation Studies

Impact of annealing strategies. We study the influence of different annealing strategies for β during the guided diffusion stage, focusing on the superconductor and TFBind8 datasets. We explore three strategies: constant, linear annealing, and exponential annealing. Figure [6](#fig_5)(a) presents the performance across various diffusion steps. We find that our method is not particularly sensitive to the annealing strategies. However, it is worth noting that exponential annealing exhibits a marginal performance advantage over the others.

We also investigate how the value of β at the end of annealing, denoted as β max , affects model performance in Figure [6](#fig_5)(b). We find that increasing β max initially leads to better performance. However, beyond a certain threshold, performance begins to decrease. It is noteworthy that the optimal value varies across different annealing strategies. Particularly, at β max = 0, the model reverts to a pure diffusion process, exhibiting the lowest performance due to the lack of guidance from the objective function.

## Sample efficiency

We explore the sample efficiency of DIFFOPT at both training and testing stages. Figure [7](#fig_6) shows the performance of various methods versus the ratio of training data on Superconductor, TFBind8 and multi-objective molecule optimization. As we can see, on all the three datasets, DiffOPT can outperform all.

Our method is also sample efficient during inference. Figure [8](#fig_7) shows the performance versus number of samples at inference stage. Notably, on both Superconductor and TFBind8, DiffOPT consistently outperforms all the baseline methods for various sample sizes during inference.

It is also important to highlight that our method consistently achieves much greater sample efficiency than DDOM at both training and inference stages, despite both approaches leveraging diffusion models.

NeurIPS Paper Checklist

1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In this paper, we propose a new sampler using diffusion models for optimization under unknown constraints. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]

We have discussed the limitations and future work in Appendix A. Guidelines:

• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.

• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.

For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

## Theory Assumptions and Proofs

Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide the complete proof of Theorem 1 in Appendix H. Guidelines:

• The answer NA means that paper does not include experiments requiring code.

• Please see the NeurIPS code and data submission guidelines ([https://nips.cc/ public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https: //nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

## Experimental Setting/Details

Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the details in Appendix I. Guidelines:

• The answer NA means that the paper does not include experiments.

• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.

## Experiment Statistical Significance

Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars have been provided for all results. Guidelines:

• The answer NA means that the paper does not include experiments.

• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

• The paper should discuss whether and how consent was obtained from people whose asset is used.

• At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

![Figure 2: Sampling trajectory of DiffOPT in the synthetic Branin experiment with unknown constraints. Red stars denote the minimizes, and the blue region denotes the feasible space from which training data is sampled. DIFFOPT can effectively navigate towards the two feasible minimizers.]()

![Figure 3: Sampling trajectory of DiffOPT in the synthetic Branin experiment with additional known constraints. Red stars denote the minimizers, the blue region denotes the feasible space from which training data is sampled and the pink region denotes the feasible space defined by the added given constraints. DIFFOPT can effectively navigate towards the unique minimizer at the intersection of the two feasible spaces.]()

![Normalize the weight w0 s =Resample S particles {x 0 s } S s=1 according to the weights 6: // Iterative Importance Sampling 7:for k = 1, • • • , K + 1 do Sample t ∼ U[0, T ]]()

![Figure 5: Training loss of the surrogate objective on Ant dataset validity is important for molecules, we ensure HierVAE can decode all the randomly selected encoded molecules. We split all the datasets into training and validation sets by 9:1.]()

![Figure 6: Impact of annealing strategies and β max in the guided diffusion stage. β max is the value of β at the end of annealing.]()

![Figure 7: Impact of the number of training data.]()

![Figure 8: Impact of the number of samples at testing time.]()

![Results of offline black-box optimization on DesignBench. We report the mean and standard deviation across five random seeds. The best results are bolded, and the second best is underlined.]()

![Results of derivative-free optimization on the multi-objective molecule optimization task.]()

![Stage II + MH 114.945 ± 3.615 0.989 ± 0.021Ablation study on the two-stage sampling.]()

https://docs.gpytorch.ai/en/stable/_modules/gpytorch/models/approximate_gp.html# ApproximateGP

https://docs.gpytorch.ai/en/stable/_modules/gpytorch/models/approximate_gp.html# ApproximateGP

https://github.com/siddarthk97/ddom

https://github.com/brandontrabucco/design-bench

