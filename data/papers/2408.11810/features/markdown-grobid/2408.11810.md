# Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models

## Abstract

## 

Diffusion Models have emerged as powerful generative models for high-quality image synthesis, with many subsequent image editing techniques based on them. However, the ease of text-based image editing introduces significant risks, such as malicious editing for scams or intellectual property infringement. Previous works have attempted to safeguard images from diffusion-based editing by adding imperceptible perturbations. These methods are costly and specifically target prevalent Latent Diffusion Models (LDMs), while Pixeldomain Diffusion Models (PDMs) remain largely unexplored and robust against such attacks. Our work addresses this gap by proposing a novel attacking framework with a feature representation attack loss that exploits vulnerabilities in denoising UNets and a latent optimization strategy to enhance the naturalness of protected images. Extensive experiments demonstrate the effectiveness of our approach in attacking dominant PDM-based editing methods (e.g., SDEdit) while maintaining reasonable protection fidelity and robustness against common defense methods. Additionally, our framework is extensible to LDMs, achieving comparable performance to existing approaches.

## Introduction

In recent years, Generative Diffusion Models (GDMs) [(Ho, Jain, and Abbeel 2020;](#b6)[Song, Meng, and Ermon 2021)](#b27) emerged as powerful generative models that can produce high-quality images, propelling advancements in image editing and artistic creations. The ease of using these models to edit [(Meng et al. 2021;](#b17)[Wang, Zhao, and Xing 2023;](#b31)[Zhang et al. 2023)](#b36) or synthesize new image [(Dhariwal and Nichol 2021;](#b2)[Rombach et al. 2022)](#b21) has raised concerns about potential malicious usage and intellectual property infringement. For example, malicious users could effortlessly craft fake images with someone's identity or mimic the style of a specific artist. An effective protection against these threats is regarded as the diffusion model generating corrupted images or unrelated images to original inputs. Researchers have made significant strides in crafting imperceptible adversarial perturbation on images to safeguard them from being edited by diffusion-based models.

Previous works such as PhotoGuard [(Salman et al. 2023](#b23)) and Glaze [(Shan et al. 2023)](#b24) have effectively attacked La-* These authors contributed equally. SDEdit SDEdit Original Image Original Edit Result Protected Image Protected Edit Result AtkPDM Protection Against Diffusion-based Image Editing tent Diffusion Models (LDMs) by minimizing the latent distance between the protected images and their target counterparts. PhotoGuard first introduce attacking either encoders or diffusion process in LDMs via Projected Gradient Descent (PGD) [(Madry et al. 2018)](#b16) for the protection purpose; however, it requires backpropagating the entire diffusion process, making it prohibitively expensive. Subsequent works AdvDM [(Liang et al. 2023)](#) and Mist [(Liang and Wu 2023)](#) leverage the semantic loss and textural loss combined with Monte Carlo method to craft adversarial images both effectively and efficiently. Diff-Protect [(Xue et al. 2024a](#)) further improve adversarial effectiveness and optimization speed via Score Distillation Sampling (SDS) [(Poole et al. 2022)](#b19), setting the state-of-the-art performance on LDMs. However, previous works primarily focus on LDMs, and attacks on Pixel-domain Diffusion Models (PDMs) remain arXiv:2408.11810v1 [cs.CV] 21 Aug 2024 largely unexplored. [Xue et al. (Xue et al. 2024a](#)) also highlighted a critical limitation of current methods: the attacking effectiveness is mainly attributed to the vulnerability of the VAE encoders in LDM; however, PDMs don't have such encoders, making current methods hard to transfer to PDMs. The latest work [(Xue and Chen 2024)](#) has attempted to attack PDMs, but the result suggests that PDMs are robust to pixel-domain perturbations. Our goal is to mitigate the gap between these limitations.

In this paper, we propose an innovative framework designed to effectively attack PDMs. Our approach includes a novel feature attacking loss that exploits the vulnerabilities in denoising UNet to distract the model from recognizing the correct semantics of the image, a fidelity loss that acts as optimization constraints that ensure the imperceptibility of adversarial image and controls the attack budget, and a latent optimization strategy utilizing victim-model-agnostic VAEs to further enhance the naturalness of our adversarial image closer to the original. With extensive experiments on different PDMs, the results show that our method is effective and affordable while robust to traditional defense methods and exhibiting attack transferability in the black-box setting.

In addition, our approach outperforms current semanticloss-based and PGD-based methods, reaching state-of-theart performance on attacking PDMs. Our contributions are summarized as follows:

1. We propose a novel attack framework targeting PDMs , achieving state-of-the-art performance in safeguarding images from being edited by SDEdit. 2. We propose a novel feature attacking loss design to distract UNet feature representation effectively. 3. We propose a latent optimization strategy via modelagnostic VAEs to enhance the naturalness of our adversarial images.

2 Related Work

## Evasion Attack for Diffusion Model

The prevalence of diffusion-based image editing techniques has posed the challenges of protecting images from maliciously editing. These editing techniques are mainly based on SDEdit [(Meng et al. 2021)](#b17) or its variations which can be applied to both PDM and LDM to produce the editing results. In general, the editing starts from first transforming the clean image (or the clean latent) into the noisy one by introducing Gaussian noise for the forward diffusion process followed by performing a series of reverse diffusion sampling steps with new conditions. In addition, based on different number of forward diffusion steps, the method could control the extent of the editing results obeying the new conditions while preserving the original image semantics. Notably, a small number of forward steps allow the editing results faithful to the original image, and more variations are introduced when larger forward step value is applied.

To counteract SDEdit-based editing, H. Salman et al. first proposed PhotoGuard [(Salman et al. 2023)](#b23) to introduce two attacking paradigms based on Projected Gradient Descent (PGD) [(Madry et al. 2018](#b16)). The first is the Encoder Attack, which aims to disrupt the latent representations of the Variational Autoencoder (VAE) of the LDMs, and the second is the Diffusion Attack, which focuses more on disrupting the entire diffusion process of the LDMs.

The Encoder Attack is simple yet effective, but the attacking results are sub-optimal due to its less flexibility for optimization than the Diffusion Attack. Although the Diffusion Attack achieves better attack results, it is prohibitively expensive due to its requirement of backpropagation through all the diffusion steps. In the following, we further introduce more relevant work for these attacks along with another common attack for diffusion models.

Diffusion Attacks. Despite the cost of performing the Diffusion Attack, the higher generalizability and universally applicable nature drive previous works focusing on disrupting the process with lower cost. Liang et al. [(Liang et al. 2023)](#) proposed AdvDM to utilize the diffusion training loss as their attacking semantic loss. Then, AdvDM performs gradient ascent with the Monte Carlo method, aiming to disrupt the denoising process without calculating full backpropagation. Mist [(Liang and Wu 2023](#)) also incorporates semantic loss and performs constrained optimization via PGD to achieve better attacking performance.

Encoder Attacks. On the other hand, researchers found that VAEs in widely adopted LDMs are more vulnerable to attack at a lower cost while avoiding attacking the expensive diffusion process. [(Salman et al. 2023;](#b23)[Liang and Wu 2023;](#)[Shan et al. 2023;](#b24)[Xue et al. 2024b)](#), focus on disrupting the latent representation in LDM via PGD and highlights the encoder attacks are more effective against LDMs.

Conditional Module Attacks. Most of the LDMs contain conditional modules for steering generation, previous works [(Shan et al. 2023](#b24)[(Shan et al. , 2024;;](#b25)[Lo et al. 2024](#b15)) exploited the vulnerability of text conditioning modules. By disrupting the cross-attention between text concepts and image semantics, these methods could effectively interfere with the diffusion model for capturing the image-text alignment, thereby realizing the attack.

Limitations of Current Methods. To our knowledge, previous works primarily focus on adversarial attacks for LDMs, while attacks on PDMs remain unexplored. Xue et al. [(Xue and Chen 2024)](#) further emphasized the difficulty of attacking PDMs. However, in our work, we find that by crafting an adversarial image to corrupt the intermediate representation of diffusion UNet, we can achieve promising attack performance for PDMs, while the attack is also compatible with LDMs. Moreover, inspired by [(Laidlaw, Singla, and Feizi 2021;](#b9)[Liu et al. 2023](#b14)) which utilizes LPIPS [(Zhang et al. 2018)](#b35) as distortion measure, we also propose a novel attacking loss as the measure to craft better adversarial images for PDMs.

## Background on Diffusion Models

Score-based models and diffusion models allow to generate samples starting from easy-to-sample Gaussian noise to complex target distributions via iteratively applying the score function of learned distribution during sampling, i.e., the gradient of underlying probability distribution ∇ x log p(x) with respect to x. However, the exact estimations of the score functions are intractable.

To bypass this problem, Yang Song et al. proposed slice score matching [(Song et al. 2020), and Ho et al. proposed](#) Denoising Diffusion Probability Model (DDPM) (Ho, Jain, and Abbeel 2020) that first gradually perturbs the clean data with linear combinations of Gaussian noise and clean data, as x t = √ ᾱt x + √ 1ᾱt ϵ t via the predefined timestep schedulers where t ∈ [0, T ] and ϵ t ∼ N (0, I), then they finally become isotropic Gaussian noise as time reaches T , this is also referred as forward diffusion. The goal is to train a time-dependent neural network that can learn to denoise noisy samples given corresponding timestep t. Specifically, the training objective is the expectation over noise estimation MSE, which is formulated as

$E t,x,ϵt [∥ϵ t -ϵ θ (x t , t)∥ 2 2 ]$, where ϵ θ denotes the parametrized neural network, DDPM adopted UNet [(Ronneberger, Fischer, and Brox 2015)](#b22) as their noise estimating network. During inference time, we first generate a random Gaussian sample, then iteratively apply the noise estimation network ϵ θ and perform denoising operations to generate a new clean sample of the learned distribution. Particularly, Song et.al proposed DDIM [(Song, Meng, and Ermon 2021)](#b27) that generalized the DDPM sampling formulation as:

$x t-1 = √ ᾱt-1 x t - √ 1 -ᾱt ϵ θ (x t , t) √ ᾱt + 1 -ᾱt-1 -σ 2 t ϵ θ (x t , t) + σ t ϵ t .(1)$With

$σ t = (1 -ᾱt-1 )/(1 -ᾱt )• 1 -ᾱt /ᾱ t-1$, Equation 1 becomes DDPM, and when σ t = 0, the sampling process become deterministic as proposed in DDIM since the added noise during each sampling step is null.

## Methodology

## Threat Model and Problem Setting

The malicious user collects an image x from the internet and uses SDEdit [(Meng et al. 2021)](#b17) to generate unauthorized image translations or editing, denoted as SDEdit(x, t), that manipulates the original input image x.

Our work aims to safeguard the input image x from the authorized manipulations by crafting an adversarial image x adv by adding imperceptible perturbation to disrupt the reverse diffusion process of SDEdit for corrupted editions.

For example, we want the main object of the image, e.g., the cat in the source image x as shown in Figure [2](#) unable to be reconstructed by the reverse diffusion process. Meanwhile, the adversarial image should maintain similarity to the source image to ensure fidelity. The reason why we target SDEdit as our threat model is that it is recognized as the most common and general operation in diffusion-based unconditional image translations and conditional image editing. Additionally, it has been incorporated into various editing pipelines [(Tsaban and Passos 2023;](#b29)[Zhang et al. 2023](#b36)).

Here we focus on the unconditional image translations for our main study, as they are essential in both unconditional

$x M x t x adv x adv t SDEdit(x) SDEdit(x adv ) max L attack min L fidelity t t -1 Forward Process$
## Image Manifold

Figure [2](#): Conceptual illustration of our method. We randomly forward both the clean image x and adversarial image x adv to noise level t, then utilize our feature attacking loss to maximize the feature distance between noisy latent x t and x adv t in the reverse process of diffusion models while imposing our fidelity loss as a constraint to ensure the adversarial image from being deviated from the original image. We update the x adv in latent space instead of in pixel space to ensure the naturalness of x adv . and conditional editing pipelines. Formally, our objective to effectively safeguard images while maintaining fidelity is formulated as:

$max x adv ∈M d(SDEdit(x, t), SDEdit(x adv , t)) subject to d ′ (x, x adv ) ≤ δ,(2)$where M indicates natural image manifold, d, d ′ indicate image distance functions and ϵ denotes the fidelity budget.

In the following sections, we first present a conceptual illustration of our method, followed by our framework for solving the optimization problem. We then discuss the novel design of our attacking loss and fidelity constraints, which provide more efficient criteria compared to previous methods for solving the image protection optimization problem using PGD. Finally, we introduce an advanced design to enhance image protection quality by latent optimization via victim-model-agnostic VAE.

## Overview

To achieve effective protection against diffusion-based editing, we aim to push the protected sample away from the original clean sample by disrupting the intermediate step in the

$x x adv x t x adv t Image Fidelity Constraint Feature Attack W 2 (f t , f adv t ) Add Noise f t = N (µ t , Σ t ) f adv t = N (µ adv t , Σ adv t ) z adv Alternating Latent Optimization Original Image Adversarial Image Add Noise L attack L fidelity One-time Initialization U θ U θ D E$Figure [3](#): Overview of our AtkPDM + algorithm: Starting from the leftmost latent of the initial adversarial image z adv , we first decode back to pixel-domain to perform forward diffusion with both x and x adv and feed them to frozen victim UNet. We then extract the feature representation in UNet to calculate our L attack , aiming to distract the recognition of image semantics. We also calculate our L fidelity in pixel-domain to constrain the optimization. Finally, the z adv is being alternatively updated by loss gradients. reverse diffusion process. For practical real-world applications, it's essential to ensure the protected image is perceptually similar to the original image. In practice, we uniformly sample the value of the forward diffusion step t ∼ [0, T ] to generate noisy images and then perform optimization to craft the adversarial image x adv via our attacking and fidelity losses, repeating the same process n times or until convergence. Figure [2](#) depicts these two push-and-pull criteria during different noise levels, the successful attack is reflected in the light orange line where the reverse sample moves far away from the normal edition of the image. More specifically, our method can be formulated as follows:

$max x adv ∈M E t,xt|x,x adv t |x -L attack (x t , x adv t ) subject to L fidelity (x, x adv ) ≤ δ,(3)$where δ denotes the attacking budget. The details of the attacking loss L attack and the fidelity loss L fidelity will be discussed in the following sections.

Framework. Our framework is illustrated in Figure [3](#). We fix two identical victim UNets to extract feature representations of clean and protected samples for optimizing to push away from each other. A protection loss is jointly incorporated to constrain the optimization. After N iterations, we segment out only the protecting main object of the image for better imperceptibility of image protection.

## Proposed Losses

We propose two novel losses as optimization objectives to craft an adversarial example efficiently without running through all the diffusion steps. Attacking loss is designed to distract the feature representation in denoising UNet; Protection loss is a constraint to ensure the image quality. For notation simplicity, we first define the samples x, x adv in different forwarded steps. Let F(x, t, ϵ) = √ ᾱt x + √ 1ᾱt ϵ be the diffusion forward process. Given timestep t sample from [0, T ], noises ϵ 1 , ϵ 2 sample from N (0, I). We denote x t = F(x, t, ϵ 1 ), and x adv t = F(x adv , t, ϵ 1 ).

Attacking Loss. Our goal is to define effective criteria that could finally distract the reverse denoising process. PhotoGuard [(Salman et al. 2023)](#b23) proposed to backpropagate through all the steps of the reverse denoising process via PGD, however, this approach is prohibitively expensive, Diff-Protect [(Xue et al. 2024b)](#) proposed to avoid the massive cost by leveraging Score Distillation [(Poole et al. 2022)](#b19) in optimization. However, Diff-protect relies heavily on gradients of attacking encoder of an LDM as stated in their results. In PDM, we don't have such an encoder to attack; nevertheless, we find that the denoising UNet has a similar structure to encoder-decoder models, and some previous works [(Lin and Yang 2024;](#b13)[Li et al. 2023](#b10)) characterize this property to accelerate and enhance the generation. From our observations of the feature roles in denoising UNets, we hypothesize that distracting specific inherent feature representation in UNet blocks could lead to effectively crafting an adversarial image. In practice, we first extract the feature representations of forwarded images x t and x adv t in frozen UNet blocks of timestep t. Then, we adopt 2-Wasserstein distance [(Arjovsky, Chintala, and Bottou 2017)](#b0) to measure the discrepancy in feature space. Note that we take the negative of the calculated distance, as we aim to pull the x adv t away from x t . Formally, the attacking loss L attack is defined as:

$L attack (x t , x adv t ) = -W 2 U (mid) θ (x t ), U (mid)$θ (x adv t ) . (4) Assuming the feature distributions approximate Gaussian distributions expressed by mean µ t and µ adv t , and nonsingular covariance matrices Σ t and Σ adv t . The calculation of the 2-Wasserstein distance between two normal distributions is viable through the closed-form solution [(Dowson and Landau 1982;](#b3)[Olkin and Pukelsheim 1982;](#)[Chen, Georgiou, and Tannenbaum 2018)](#b1):

$W 2 2 (N (µ t , Σ t ), N (µ adv t , Σ adv t )) = ∥µ t -µ adv t ∥ 2 2 + trace(Σ t + Σ adv t -2(Σ adv t 1 2 Σ t Σ adv t 1 2 ) 1 2 ).$(

$)5$Fidelity Loss. To control the attack budget for adversarial image quality, we design a constraint function that utilizes the feature extractor from a pretrained classifier for calculating fidelity loss. In our case, we sum up the 2-Wasserstein feature losses of L different layers. Specifically, we define L fidelity as:

$L fidelity (x t , x adv t ) = L ℓ=1 W 2 (ϕ ℓ (x), ϕ ℓ (x adv )),(6)$where W 2 denotes 2-Wasserstein distance and ϕ ℓ denotes layer ℓ of the feature extractor.

## Alternating Optimization for Adversarial Image

We solve the constrained optimization problems via alternating optimization to craft the adversarial images, detailed optimization loop of AtkPDM + is provided in Algorithm 1. AtkPDM algorithm and the derivation of the alternating optimization are provided in Appendix.

## Latent Optimization via Pretrained-VAE

Previous works suggest that diffusion models have a strong capability of resisting adversarial perturbations (Xue and Chen 2024), making them hard to attack via pixel-domain optimization. Moreover, they are even considered good purifiers of adversarial perturbations [(Nie et al. 2022](#b18)). Here we propose a latent optimization strategy that crafts the "perturbation" in latent space. We adopt a pre-trained Variational Autoencoder (VAE) [(Kingma and Welling 2014)](#b7) to convert images to their latent space, and the gradients will be used to update the latent, after N iterations or losses converge, we decode back via decoder D to pixel domain as our final protected image. The motivation for adopting VAE is inspired by MPGD [(He et al. 2024](#b5)). This strategy is effective for crafting a robust adversarial image against pixel-domain diffusion models while also better preserving the protection quality rather than only incorporating fidelity constraints.

The constraint optimization thereby becomes: max

$z adv E t,$xt|x,x adv t |D(z adv ) -L attack (x t , x adv t ) subject to L fidelity (x, D(z adv )) ≤ δ. (7) Algorithm 1: AtkPDM + 1: Input: Image to be protected x, attack budget δ > 0, step size γ1, γ2 > 0, VAE encoder E, and VAE decoder D 2: Initialization: x adv ← x, Lattack ← ∞ 3: Encode adversarial image to latent space: z adv ← E(x adv ) 4: while Lattack not convergent do 5: Decode adversarial latent to pixel space: x adv ← D(z adv ) 6: Sample timestep: t ∼ [0, T ] 7: Sample noise: ϵ1, ϵ2 ∼ N (0, I) 8: Compute original noisy sample: xt ← F (x, t, ϵ1) 9: Compute adversarial noisy sample: x adv t ← F (x adv , t, ϵ2) 10: Update z adv by Gradient Descent: z adv ← z adv -γ1 sign(∇ z adv Lattack(xt, x adv t

)) 11:

while Lfidelity(x, D(z adv )) > δ do 12:

z adv ← z adv -γ2∇ z adv Lfidelity(x, D(z adv )) 13:

end while 14: end while 15: Decode adversarial latent to pixel space: x adv ← D(z adv ) 16: return x adv Detailed latent optimization loop is provided in Algorithm 1.

## Experiment Results

In this section, we examine the attack effectiveness and robustness of our approach under extensive settings.

## Experiment Settings

Implementation Details. We conduct all our experiments in white box settings and examine the effectiveness of our attacks using SDEdit [(Meng et al. 2021)](#b17). For the Variational Autoencoder (VAE) [(Kingma and Welling 2014)](#b7) in our AtkPDM + , we utilize the VAE provided by StableDiffusion V1.5 [(Rombach et al. 2022)](#b21). We run all of our experiments with 300 optimization steps, which is empirically determined, to balance attacking effectiveness and image protection quality with reasonable speed. Other loss parameters and running time are provided in the Appendix. The implementation is built on the Diffusers library. All the experiments are conducted with a single Nvidia Tesla V100 GPU.

Victim Models and Datasets. We test our approach on PDMs with three open-source checkpoints on Hug-gingFace, specifically "google/ddpm-ema-church-256", "google/ddpm-cat-256" and "google/ddpm-ema-celebahq-256". For the results reported in Table [1](#tab_3), we run 30 images for each victim model. Additionally, for generalizability in practical scenarios, we synthesize the data with half randomly from the originally trained dataset and another half from randomly crawled with keywords from the Internet.

Baseline Methods and Evaluation Metrics. To the best of our knowledge, previous methods have mainly focused on LDMs, and effective PDM attacks have not yet been developed, however, we still implement Projected Gradient Ascent (PGAscent) with their proposed semantic loss by [(Salman et al. 2023;](#b23)[Liang et al. 2023;](#)[Liang and Wu 2023;](#)[Xue et al. 2024b](#)). Notably, Diff-Protect [(Xue et al. 2024b)](#) proposed to minimize the semantic loss is surprisingly better than maximizing the semantic loss, we also

Methods Adversarial Image Quality Attacking Effectiveness SSIM ↑ PSNR ↑ LPIPS ↓ SSIM ↓ PSNR ↓ LPIPS ↑ IA-Score ↓ Church PGAscent 0.37 ± 0.09 28.17 ± 0.22 0.73 ± 0.16 0.89 ± 0.05 31.06 ± 1.94 0.17 ± 0.09 0.93 ± 0.04 Diff-Protect 0.39 ± 0.07 28.03 ± 0.12 0.67 ± 0.11 0.82 ± 0.05 31.90 ± 1.08 0.23 ± 0.07 0.91 ± 0.04 AtkPDM (Ours) 0.75 ± 0.03 28.22 ± 0.10 0.26 ± 0.04 0.75 ± 0.04 29.61 ± 0.23 0.40 ± 0.05 0.76 ± 0.06 atkPDM + (Ours) 0.81 ± 0.03 28.64 ± 0.19 0.13 ± 0.02 0.79 ± 0.04 30.05 ± 0.47 0.33 ± 0.07 0.81 ± 0.06 Cat PGAscent 0.48 ± 0.09 28.34 ± 0.18 0.65 ± 0.12 0.96 ± 0.02 32.32 ± 2.49 0.10 ± 0.05 0.97 ± 0.03 Diff-Protect 0.33 ± 0.10 28.03 ± 0.15 0.80 ± 0.15 0.90 ± 0.05 33.94 ± 1.93 0.18 ± 0.08 0.95 ± 0.03 atkPDM (Ours)

0.71 ± 0.06 28.47 ± 0.18 0.29 ± 0.05 0.83 ± 0.03 30.73 ± 0.51 0.39 ± 0.05 0.81 ± 0.04 atkPDM + (Ours) 0.83 ± 0.04 29.41 ± 0.37 0.09 ± 0.02 0.93 ± 0.01 33.02 ± 0.74 0.18 ± 0.02 0.92 ± 0.01 Face PGAscent 0.48 ± 0.05 28.75 ± 0.18 0.64 ± 0.10 0.99 ± 0.00 37.96 ± 1.75 0.02 ± 0.01 0.99 ± 0.00 Diff-Protect 0.25 ± 0.04 28.09 ± 0.20 0.91 ± 0.11 0.95 ± 0.02 35.33 ± 1.62 0.08 ± 0.04 0.96 ± 0.02 atkPDM (Ours) 0.56 ± 0.04 28.01 ± 0.22 0.36 ± 0.04 0.74 ± 0.03 29.14 ± 0.36 0.40 ± 0.05 0.62 ± 0.07 atkPDM + (Ours) 0.81 ± 0.04 28.39 ± 0.20 0.12 ± 0.03 0.86 ± 0.03 30.26 ± 0.72 0.24 ± 0.07 0.80 ± 0.08 Table 2: Quantitative results of our adversarial images against defense methods. Both Crop-and-Resize and JPEG Compression fail to defend our attack. "None" indicates no defense is applied, as the baseline for comparison.

adopted this method in attacking PDMs and denote as Diff-Protect. To quantify the adversarial image visual quality, we adopted Structural Similarity (SSIM) [(Wang et al. 2004](#b30)), Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS) [(Zhang et al. 2018)](#b35). We also inherit these three metrics, but negatively to quantify the effectiveness of our attack. We also adopted Image Alignment Score (IA-Score) [(Kumari et al. 2023](#b8)) that leverages CLIP [(Radford et al. 2021)](#b20) to calculate the cosine similarity of image encoder features. In distinguishing from previous methods, to more faithfully reflect the attack effectiveness, we fix the same seed of the random generator when generating clean and adversarial samples, then calculate the scores based on the paired samples.

## Attack Effectiveness on PDMs

As quantitatively reported in Table [1](#tab_3) and qualitative results in Figure [4](#fig_1), compared to previous PGD-based methods incorporating semantic loss, i.e., negative training loss of diffusion models, our method exhibits superior performance in both adversarial image quality and attacking effectiveness. And our reported figures has generally stable as reflected in lower standard deviation. It is worth noting that even if the adversarial image qualities of the PGD-based methods are far worse than ours, their attacking effectiveness still falls short, suggesting that PDMs are robust against traditional perturbation methods, this finding is also aligned with pre- vious works [(Xue et al. 2024b;](#)[Xue and Chen 2024)](#). For AtkPDM + , combined with our latent optimization strategy, the adversarial image quality has enhanced while slightly affecting the attacking effectiveness, still outperforming the previous methods.

## Against Defense Methods

We examine the robustness of our approach against two widely recognized and effective defense methods for defending against adversarial attacks as reported in Table [2](#).

Crop and Resize. Noted by Diff-Protect, crop and resize is simple yet the most effective defense method against their attacks on LDMs. We also test our method against this defense using their settings, i.e., cropping 20% of the adversarial image and then resizing it to its original dimensions.

JPEG Compression. Sandoval-Segura et al. [(Sandoval-Segura, Geiping, and Goldstein 2023)](#b23) demonstrated that JPEG compression is a simple yet effective adversarial defense method. In our experiments, we implement the JPEG compression at a quality setting of 25%. The quantitative results in Table [2](#) demonstrate that our method is robust against these two defense methods, with four of the metrics listed in Table [2](#) are not worse than no defenses. Surprisingly, these defense methods even make the adversarial image more effective than cases without defense.  

## Black Box Transferability

We craft adversarial images with the proxy model, "google/ddpm-ema-church-256", in white-box settings and test their transferability to another "google/ddpm-bedroom-256" model for black-box attacks. Under identical validation settings, Table [3](#tab_5) reveals only a slight decrease in attack effectiveness metrics, indicating successful black-box transferability.

## Effectiveness of Latent Optimization via VAE

We first incorporate our VAE latent optimization strategy in the previous semantic-loss-based PGAscent. From Table [4](#tab_6), without using L fidelity , latent optimization has significantly enhanced the adversarial image quality and even slightly improved the attacking effectiveness. Adopting latent optimization in our approach enhances visual quality with a negligible decrease in attacking effectiveness. Surprisingly, incorporating our L fidelity with current PGD-based method will drastically decrease the adversarial image quality despite its attack performing better than ours. This may be due to different constrained optimization problem settings.

## Conclusion

In this paper, we present the first framework designed to effectively protect images manipulation by Pixel-domain Diffusion Models (PDMs). We demonstrate that while denoising UNets appear robust to conventional PGD-based attacks, their feature space remains vulnerable to attack. Our proposed feature attacking loss exploit the vulnerabilities to empower adversarial images to mislead PDMs, thereby producing low-quality output images. We approach this image protection problem as a constrained optimization problem, solving it through alternating optimization. Additionally, our latent optimization strategy via VAE enhances the natural-ness of our adversarial images. Extensive experiments validate the efficacy of our method, achieving state-of-the-art performance in attacking PDMs.

## Supplementary Material

A More Implementation Details

The feature extractor for calculating L fidelity is VGG16 (Simonyan and Zisserman 2014) with IMAGENET1K-V1 checkpoint. We use the SDEdit with the forward step t = 500 for our main study results as it balances faithfulness to the original image and flexibility for editing. Empirically, we choose to randomly sample the forward step t ∼ [0, 500] to enhance the optimization efficiency. The average time to optimize 300 steps for an image on a single Nvidia Tesla V100 is about 300 seconds. The estimated average memory usage is about 24GB.

Table 5 provides the details of the step sizes that we use to attack different models. Models Step Size γ1 (Lattack) γ2 (Lfidelity) google/ddpm-ema-church-256 100/255 40/255 google/ddpm-cat-256 100/255 5/255 google/ddpm-ema-celebahq-256 50/255 35/255

Table 5: The step sizes used for different models during optimization.

## B More Experimental Results

## B.1 Attack Effectiveness on Latent Diffusion Models

We propose the feature representation attacking loss which can be adapted to target any UNet-based diffusion models. Hence, it is applicable to attack LDM using our proposed framework. We follow the evaluation settings of the previous works [(Xue et al. 2024b](#)) for fair comparisons. Quantitative results are shown in Table [6](#). Compared to previous LDM-specified methods, our method could achieve comparable results. This finding reflects the general vulnerability in UNet-based diffusion models that can be exploited to craft adversarial images against either PDMs or LDMs.

## B.2 Qualitative Demonstration of Corrupting UNet Feature during Sampling

We qualitatively show an example of our attack effectiveness regarding UNet representation discrepancies in Figure [5](#). We compare a clean and an adversarial image using the same denoising process. Then, we take the feature maps of the second-last decoder block layer, close to the final predicted noise, to demonstrate their recognition of input image semantics. The results in Figure [5](#) show that from t = 500, the feature maps of each pair start with a similar structure, then as the t decreases, the feature maps gradually have higher discrepancies, suggesting our method, by attacking the middle representation of UNet, can effectively disrupt the reverse denoising process and mislead to corrupted samples.

## B.3 Qualitative Results of Loss Ablation

Figure [6](#) presents qualitative results of loss ablation where i., ii., and iii. indicate performing PGAscent with different con-figurations. i. utilizes only semantic loss; ii. utilizes semantic loss with our latent optimization strategy; iii. utilizes both semantic loss, our proposed L fidelity and latent optimization. The results show that our L fidelity and latent optimization can enhance the adversarial image quality of PGAscent. Moreover, comparing our proposed two methods, AtkPDM + generates a more natural adversarial image than AtkPDM while maintaining attack effectiveness.

## B.4 Different Forward Time-step Sampling

When using Monte Carlo sampling for optimization, the forward time step t * is sampled uniformly. We study the scenario that when t * is fixed for optimization. As shown in Figure [7](#), a primary result shows that when attacking t * = 400 to t * = 500, the attacking effectiveness is better than other time steps. In practice, we can not know user-specified t * for editing in advance; however, this suggests that diffusion models have a potential temporal vulnerability that can be further exploited to increase efficiency.

## B.5 More Qualitative Results

We provide more qualitative results in Figure [8](#) to showcase that our method can significantly change or corrupt the generated results with little modification on adversarial images. In contrast, previous methods add obvious perturbation to adversarial images but still fail to change the edited results to achieve the safeguarding goal.

## B.6 Example of Loss Curves

Figure [9](#) shows an example of our loss trends among optimization steps. L attack has decreasing trend as the optimization step increases. L fidelity has an increasing trend and converges to satisfy the constraint of the attack budget δ.

## C Limitations

While our method can deliver acceptable attacks on PDMs, its visual quality is still not directly comparable to the results achieved on LDMs, indicating room for further improvement. More generalized PDM attacks should be further explored.

## D Societal Impacts

Our work will not raise potential concerns about diffusion model abuses. Our work is dedicated to addressing these issues by safeguarding images from being infringed.

![Figure 1: Overview of our attack scenario. Diffusion-based image editing can generate high-quality image variation based on the clean input image. However, by adding carefully crafted perturbation to the clean image, the diffusion process will be disrupted, producing a corrupted image or unrelated image semantics to the original image.]()

![Figure 4: Qualitative results compared to previous methods: our adversarial images can effectively corrupt the edited results without significant fidelity decrease. The same column shares the same random seed for fair comparison.]()

![Quantitative Results in attacking different PDMs. The best is marked in red and the second best is marked in blue. Errors denote one standard deviation of all images in our test datasets.]()

![Quantitative results of black box attack. We use the same set of adversarial images and feed to white box and black box models to examine the black box transferability.]()

![± 0.09 28.17 ± 0.22 0.73 ± 0.16 0.89 ± 0.05 31.06 ± 1.94 0.17 ± 0.09 0.93 ± 0.04 Lsemantic ✓ 0.80 ± 0.05 29.78 ± 0.42 0.17 ± 0.03 0.82 ± 0.05 30.43 ± 0.75 0.15 ± 0.06 0.92 ± 0.04 Lsemantic + Lfidelity ✓ 0.82 ± 0.05 30.30 ± 0.81 0.13 ± 0.03 0.90 ± 0.03 31.24 ± 1.19 0.08 ± 0.03 0.96 ± 0.02Quantitative results of ablation study. The best is in bold and the second best is underlined. Errors denote one standard deviation of all images in our test datasets.]()

