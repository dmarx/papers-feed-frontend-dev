<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chun-Yen</forename><surname>Shih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li-Xuan</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia-Wei</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ernie</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cheng-Fu</forename><surname>Chou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D70DCBD7736A72A3B52388CD43CBD950</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion Models have emerged as powerful generative models for high-quality image synthesis, with many subsequent image editing techniques based on them. However, the ease of text-based image editing introduces significant risks, such as malicious editing for scams or intellectual property infringement. Previous works have attempted to safeguard images from diffusion-based editing by adding imperceptible perturbations. These methods are costly and specifically target prevalent Latent Diffusion Models (LDMs), while Pixeldomain Diffusion Models (PDMs) remain largely unexplored and robust against such attacks. Our work addresses this gap by proposing a novel attacking framework with a feature representation attack loss that exploits vulnerabilities in denoising UNets and a latent optimization strategy to enhance the naturalness of protected images. Extensive experiments demonstrate the effectiveness of our approach in attacking dominant PDM-based editing methods (e.g., SDEdit) while maintaining reasonable protection fidelity and robustness against common defense methods. Additionally, our framework is extensible to LDMs, achieving comparable performance to existing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Generative Diffusion Models (GDMs) <ref type="bibr" target="#b6">(Ho, Jain, and Abbeel 2020;</ref><ref type="bibr" target="#b27">Song, Meng, and Ermon 2021)</ref> emerged as powerful generative models that can produce high-quality images, propelling advancements in image editing and artistic creations. The ease of using these models to edit <ref type="bibr" target="#b17">(Meng et al. 2021;</ref><ref type="bibr" target="#b31">Wang, Zhao, and Xing 2023;</ref><ref type="bibr" target="#b36">Zhang et al. 2023)</ref> or synthesize new image <ref type="bibr" target="#b2">(Dhariwal and Nichol 2021;</ref><ref type="bibr" target="#b21">Rombach et al. 2022)</ref> has raised concerns about potential malicious usage and intellectual property infringement. For example, malicious users could effortlessly craft fake images with someone's identity or mimic the style of a specific artist. An effective protection against these threats is regarded as the diffusion model generating corrupted images or unrelated images to original inputs. Researchers have made significant strides in crafting imperceptible adversarial perturbation on images to safeguard them from being edited by diffusion-based models.</p><p>Previous works such as PhotoGuard <ref type="bibr" target="#b23">(Salman et al. 2023</ref>) and Glaze <ref type="bibr" target="#b24">(Shan et al. 2023)</ref> have effectively attacked La-* These authors contributed equally. SDEdit SDEdit Original Image Original Edit Result Protected Image Protected Edit Result AtkPDM Protection Against Diffusion-based Image Editing tent Diffusion Models (LDMs) by minimizing the latent distance between the protected images and their target counterparts. PhotoGuard first introduce attacking either encoders or diffusion process in LDMs via Projected Gradient Descent (PGD) <ref type="bibr" target="#b16">(Madry et al. 2018)</ref> for the protection purpose; however, it requires backpropagating the entire diffusion process, making it prohibitively expensive. Subsequent works AdvDM <ref type="bibr">(Liang et al. 2023)</ref> and Mist <ref type="bibr">(Liang and Wu 2023)</ref> leverage the semantic loss and textural loss combined with Monte Carlo method to craft adversarial images both effectively and efficiently. Diff-Protect <ref type="bibr">(Xue et al. 2024a</ref>) further improve adversarial effectiveness and optimization speed via Score Distillation Sampling (SDS) <ref type="bibr" target="#b19">(Poole et al. 2022)</ref>, setting the state-of-the-art performance on LDMs. However, previous works primarily focus on LDMs, and attacks on Pixel-domain Diffusion Models (PDMs) remain arXiv:2408.11810v1 [cs.CV] 21 Aug 2024 largely unexplored. <ref type="bibr">Xue et al. (Xue et al. 2024a</ref>) also highlighted a critical limitation of current methods: the attacking effectiveness is mainly attributed to the vulnerability of the VAE encoders in LDM; however, PDMs don't have such encoders, making current methods hard to transfer to PDMs. The latest work <ref type="bibr">(Xue and Chen 2024)</ref> has attempted to attack PDMs, but the result suggests that PDMs are robust to pixel-domain perturbations. Our goal is to mitigate the gap between these limitations.</p><p>In this paper, we propose an innovative framework designed to effectively attack PDMs. Our approach includes a novel feature attacking loss that exploits the vulnerabilities in denoising UNet to distract the model from recognizing the correct semantics of the image, a fidelity loss that acts as optimization constraints that ensure the imperceptibility of adversarial image and controls the attack budget, and a latent optimization strategy utilizing victim-model-agnostic VAEs to further enhance the naturalness of our adversarial image closer to the original. With extensive experiments on different PDMs, the results show that our method is effective and affordable while robust to traditional defense methods and exhibiting attack transferability in the black-box setting.</p><p>In addition, our approach outperforms current semanticloss-based and PGD-based methods, reaching state-of-theart performance on attacking PDMs. Our contributions are summarized as follows:</p><p>1. We propose a novel attack framework targeting PDMs , achieving state-of-the-art performance in safeguarding images from being edited by SDEdit. 2. We propose a novel feature attacking loss design to distract UNet feature representation effectively. 3. We propose a latent optimization strategy via modelagnostic VAEs to enhance the naturalness of our adversarial images.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evasion Attack for Diffusion Model</head><p>The prevalence of diffusion-based image editing techniques has posed the challenges of protecting images from maliciously editing. These editing techniques are mainly based on SDEdit <ref type="bibr" target="#b17">(Meng et al. 2021)</ref> or its variations which can be applied to both PDM and LDM to produce the editing results. In general, the editing starts from first transforming the clean image (or the clean latent) into the noisy one by introducing Gaussian noise for the forward diffusion process followed by performing a series of reverse diffusion sampling steps with new conditions. In addition, based on different number of forward diffusion steps, the method could control the extent of the editing results obeying the new conditions while preserving the original image semantics. Notably, a small number of forward steps allow the editing results faithful to the original image, and more variations are introduced when larger forward step value is applied.</p><p>To counteract SDEdit-based editing, H. Salman et al. first proposed PhotoGuard <ref type="bibr" target="#b23">(Salman et al. 2023)</ref> to introduce two attacking paradigms based on Projected Gradient Descent (PGD) <ref type="bibr" target="#b16">(Madry et al. 2018</ref>). The first is the Encoder Attack, which aims to disrupt the latent representations of the Variational Autoencoder (VAE) of the LDMs, and the second is the Diffusion Attack, which focuses more on disrupting the entire diffusion process of the LDMs.</p><p>The Encoder Attack is simple yet effective, but the attacking results are sub-optimal due to its less flexibility for optimization than the Diffusion Attack. Although the Diffusion Attack achieves better attack results, it is prohibitively expensive due to its requirement of backpropagation through all the diffusion steps. In the following, we further introduce more relevant work for these attacks along with another common attack for diffusion models.</p><p>Diffusion Attacks. Despite the cost of performing the Diffusion Attack, the higher generalizability and universally applicable nature drive previous works focusing on disrupting the process with lower cost. Liang et al. <ref type="bibr">(Liang et al. 2023)</ref> proposed AdvDM to utilize the diffusion training loss as their attacking semantic loss. Then, AdvDM performs gradient ascent with the Monte Carlo method, aiming to disrupt the denoising process without calculating full backpropagation. Mist <ref type="bibr">(Liang and Wu 2023</ref>) also incorporates semantic loss and performs constrained optimization via PGD to achieve better attacking performance.</p><p>Encoder Attacks. On the other hand, researchers found that VAEs in widely adopted LDMs are more vulnerable to attack at a lower cost while avoiding attacking the expensive diffusion process. <ref type="bibr" target="#b23">(Salman et al. 2023;</ref><ref type="bibr">Liang and Wu 2023;</ref><ref type="bibr" target="#b24">Shan et al. 2023;</ref><ref type="bibr">Xue et al. 2024b)</ref>, focus on disrupting the latent representation in LDM via PGD and highlights the encoder attacks are more effective against LDMs.</p><p>Conditional Module Attacks. Most of the LDMs contain conditional modules for steering generation, previous works <ref type="bibr" target="#b24">(Shan et al. 2023</ref><ref type="bibr" target="#b25">(Shan et al. , 2024;;</ref><ref type="bibr" target="#b15">Lo et al. 2024</ref>) exploited the vulnerability of text conditioning modules. By disrupting the cross-attention between text concepts and image semantics, these methods could effectively interfere with the diffusion model for capturing the image-text alignment, thereby realizing the attack.</p><p>Limitations of Current Methods. To our knowledge, previous works primarily focus on adversarial attacks for LDMs, while attacks on PDMs remain unexplored. Xue et al. <ref type="bibr">(Xue and Chen 2024)</ref> further emphasized the difficulty of attacking PDMs. However, in our work, we find that by crafting an adversarial image to corrupt the intermediate representation of diffusion UNet, we can achieve promising attack performance for PDMs, while the attack is also compatible with LDMs. Moreover, inspired by <ref type="bibr" target="#b9">(Laidlaw, Singla, and Feizi 2021;</ref><ref type="bibr" target="#b14">Liu et al. 2023</ref>) which utilizes LPIPS <ref type="bibr" target="#b35">(Zhang et al. 2018)</ref> as distortion measure, we also propose a novel attacking loss as the measure to craft better adversarial images for PDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background on Diffusion Models</head><p>Score-based models and diffusion models allow to generate samples starting from easy-to-sample Gaussian noise to complex target distributions via iteratively applying the score function of learned distribution during sampling, i.e., the gradient of underlying probability distribution ∇ x log p(x) with respect to x. However, the exact estimations of the score functions are intractable.</p><p>To bypass this problem, Yang Song et al. proposed slice score matching <ref type="bibr">(Song et al. 2020), and Ho et al. proposed</ref> Denoising Diffusion Probability Model (DDPM) (Ho, Jain, and Abbeel 2020) that first gradually perturbs the clean data with linear combinations of Gaussian noise and clean data, as x t = √ ᾱt x + √ 1ᾱt ϵ t via the predefined timestep schedulers where t ∈ [0, T ] and ϵ t ∼ N (0, I), then they finally become isotropic Gaussian noise as time reaches T , this is also referred as forward diffusion. The goal is to train a time-dependent neural network that can learn to denoise noisy samples given corresponding timestep t. Specifically, the training objective is the expectation over noise estimation MSE, which is formulated as</p><formula xml:id="formula_0">E t,x,ϵt [∥ϵ t -ϵ θ (x t , t)∥ 2 2 ]</formula><p>, where ϵ θ denotes the parametrized neural network, DDPM adopted UNet <ref type="bibr" target="#b22">(Ronneberger, Fischer, and Brox 2015)</ref> as their noise estimating network. During inference time, we first generate a random Gaussian sample, then iteratively apply the noise estimation network ϵ θ and perform denoising operations to generate a new clean sample of the learned distribution. Particularly, Song et.al proposed DDIM <ref type="bibr" target="#b27">(Song, Meng, and Ermon 2021)</ref> that generalized the DDPM sampling formulation as:</p><formula xml:id="formula_1">x t-1 = √ ᾱt-1 x t - √ 1 -ᾱt ϵ θ (x t , t) √ ᾱt + 1 -ᾱt-1 -σ 2 t ϵ θ (x t , t) + σ t ϵ t .<label>(1)</label></formula><p>With</p><formula xml:id="formula_2">σ t = (1 -ᾱt-1 )/(1 -ᾱt )• 1 -ᾱt /ᾱ t-1</formula><p>, Equation 1 becomes DDPM, and when σ t = 0, the sampling process become deterministic as proposed in DDIM since the added noise during each sampling step is null.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Threat Model and Problem Setting</head><p>The malicious user collects an image x from the internet and uses SDEdit <ref type="bibr" target="#b17">(Meng et al. 2021)</ref> to generate unauthorized image translations or editing, denoted as SDEdit(x, t), that manipulates the original input image x.</p><p>Our work aims to safeguard the input image x from the authorized manipulations by crafting an adversarial image x adv by adding imperceptible perturbation to disrupt the reverse diffusion process of SDEdit for corrupted editions.</p><p>For example, we want the main object of the image, e.g., the cat in the source image x as shown in Figure <ref type="figure">2</ref> unable to be reconstructed by the reverse diffusion process. Meanwhile, the adversarial image should maintain similarity to the source image to ensure fidelity. The reason why we target SDEdit as our threat model is that it is recognized as the most common and general operation in diffusion-based unconditional image translations and conditional image editing. Additionally, it has been incorporated into various editing pipelines <ref type="bibr" target="#b29">(Tsaban and Passos 2023;</ref><ref type="bibr" target="#b36">Zhang et al. 2023</ref>).</p><p>Here we focus on the unconditional image translations for our main study, as they are essential in both unconditional</p><formula xml:id="formula_3">x M x t x adv x adv t SDEdit(x) SDEdit(x adv ) max L attack min L fidelity t t -1 Forward Process</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Manifold</head><p>Figure <ref type="figure">2</ref>: Conceptual illustration of our method. We randomly forward both the clean image x and adversarial image x adv to noise level t, then utilize our feature attacking loss to maximize the feature distance between noisy latent x t and x adv t in the reverse process of diffusion models while imposing our fidelity loss as a constraint to ensure the adversarial image from being deviated from the original image. We update the x adv in latent space instead of in pixel space to ensure the naturalness of x adv . and conditional editing pipelines. Formally, our objective to effectively safeguard images while maintaining fidelity is formulated as:</p><formula xml:id="formula_4">max x adv ∈M d(SDEdit(x, t), SDEdit(x adv , t)) subject to d ′ (x, x adv ) ≤ δ,<label>(2)</label></formula><p>where M indicates natural image manifold, d, d ′ indicate image distance functions and ϵ denotes the fidelity budget.</p><p>In the following sections, we first present a conceptual illustration of our method, followed by our framework for solving the optimization problem. We then discuss the novel design of our attacking loss and fidelity constraints, which provide more efficient criteria compared to previous methods for solving the image protection optimization problem using PGD. Finally, we introduce an advanced design to enhance image protection quality by latent optimization via victim-model-agnostic VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overview</head><p>To achieve effective protection against diffusion-based editing, we aim to push the protected sample away from the original clean sample by disrupting the intermediate step in the</p><formula xml:id="formula_5">x x adv x t x adv t Image Fidelity Constraint Feature Attack W 2 (f t , f adv t ) Add Noise f t = N (µ t , Σ t ) f adv t = N (µ adv t , Σ adv t ) z adv Alternating Latent Optimization Original Image Adversarial Image Add Noise L attack L fidelity One-time Initialization U θ U θ D E</formula><p>Figure <ref type="figure">3</ref>: Overview of our AtkPDM + algorithm: Starting from the leftmost latent of the initial adversarial image z adv , we first decode back to pixel-domain to perform forward diffusion with both x and x adv and feed them to frozen victim UNet. We then extract the feature representation in UNet to calculate our L attack , aiming to distract the recognition of image semantics. We also calculate our L fidelity in pixel-domain to constrain the optimization. Finally, the z adv is being alternatively updated by loss gradients. reverse diffusion process. For practical real-world applications, it's essential to ensure the protected image is perceptually similar to the original image. In practice, we uniformly sample the value of the forward diffusion step t ∼ [0, T ] to generate noisy images and then perform optimization to craft the adversarial image x adv via our attacking and fidelity losses, repeating the same process n times or until convergence. Figure <ref type="figure">2</ref> depicts these two push-and-pull criteria during different noise levels, the successful attack is reflected in the light orange line where the reverse sample moves far away from the normal edition of the image. More specifically, our method can be formulated as follows:</p><formula xml:id="formula_6">max x adv ∈M E t,xt|x,x adv t |x -L attack (x t , x adv t ) subject to L fidelity (x, x adv ) ≤ δ,<label>(3)</label></formula><p>where δ denotes the attacking budget. The details of the attacking loss L attack and the fidelity loss L fidelity will be discussed in the following sections.</p><p>Framework. Our framework is illustrated in Figure <ref type="figure">3</ref>. We fix two identical victim UNets to extract feature representations of clean and protected samples for optimizing to push away from each other. A protection loss is jointly incorporated to constrain the optimization. After N iterations, we segment out only the protecting main object of the image for better imperceptibility of image protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Proposed Losses</head><p>We propose two novel losses as optimization objectives to craft an adversarial example efficiently without running through all the diffusion steps. Attacking loss is designed to distract the feature representation in denoising UNet; Protection loss is a constraint to ensure the image quality. For notation simplicity, we first define the samples x, x adv in different forwarded steps. Let F(x, t, ϵ) = √ ᾱt x + √ 1ᾱt ϵ be the diffusion forward process. Given timestep t sample from [0, T ], noises ϵ 1 , ϵ 2 sample from N (0, I). We denote x t = F(x, t, ϵ 1 ), and x adv t = F(x adv , t, ϵ 1 ).</p><p>Attacking Loss. Our goal is to define effective criteria that could finally distract the reverse denoising process. PhotoGuard <ref type="bibr" target="#b23">(Salman et al. 2023)</ref> proposed to backpropagate through all the steps of the reverse denoising process via PGD, however, this approach is prohibitively expensive, Diff-Protect <ref type="bibr">(Xue et al. 2024b)</ref> proposed to avoid the massive cost by leveraging Score Distillation <ref type="bibr" target="#b19">(Poole et al. 2022)</ref> in optimization. However, Diff-protect relies heavily on gradients of attacking encoder of an LDM as stated in their results. In PDM, we don't have such an encoder to attack; nevertheless, we find that the denoising UNet has a similar structure to encoder-decoder models, and some previous works <ref type="bibr" target="#b13">(Lin and Yang 2024;</ref><ref type="bibr" target="#b10">Li et al. 2023</ref>) characterize this property to accelerate and enhance the generation. From our observations of the feature roles in denoising UNets, we hypothesize that distracting specific inherent feature representation in UNet blocks could lead to effectively crafting an adversarial image. In practice, we first extract the feature representations of forwarded images x t and x adv t in frozen UNet blocks of timestep t. Then, we adopt 2-Wasserstein distance <ref type="bibr" target="#b0">(Arjovsky, Chintala, and Bottou 2017)</ref> to measure the discrepancy in feature space. Note that we take the negative of the calculated distance, as we aim to pull the x adv t away from x t . Formally, the attacking loss L attack is defined as:</p><formula xml:id="formula_7">L attack (x t , x adv t ) = -W 2 U (mid) θ (x t ), U (mid)</formula><p>θ (x adv t ) . (4) Assuming the feature distributions approximate Gaussian distributions expressed by mean µ t and µ adv t , and nonsingular covariance matrices Σ t and Σ adv t . The calculation of the 2-Wasserstein distance between two normal distributions is viable through the closed-form solution <ref type="bibr" target="#b3">(Dowson and Landau 1982;</ref><ref type="bibr">Olkin and Pukelsheim 1982;</ref><ref type="bibr" target="#b1">Chen, Georgiou, and Tannenbaum 2018)</ref>:</p><formula xml:id="formula_8">W 2 2 (N (µ t , Σ t ), N (µ adv t , Σ adv t )) = ∥µ t -µ adv t ∥ 2 2 + trace(Σ t + Σ adv t -2(Σ adv t 1 2 Σ t Σ adv t 1 2 ) 1 2 ).</formula><p>(</p><formula xml:id="formula_9">)<label>5</label></formula><p>Fidelity Loss. To control the attack budget for adversarial image quality, we design a constraint function that utilizes the feature extractor from a pretrained classifier for calculating fidelity loss. In our case, we sum up the 2-Wasserstein feature losses of L different layers. Specifically, we define L fidelity as:</p><formula xml:id="formula_10">L fidelity (x t , x adv t ) = L ℓ=1 W 2 (ϕ ℓ (x), ϕ ℓ (x adv )),<label>(6)</label></formula><p>where W 2 denotes 2-Wasserstein distance and ϕ ℓ denotes layer ℓ of the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Alternating Optimization for Adversarial Image</head><p>We solve the constrained optimization problems via alternating optimization to craft the adversarial images, detailed optimization loop of AtkPDM + is provided in Algorithm 1. AtkPDM algorithm and the derivation of the alternating optimization are provided in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Latent Optimization via Pretrained-VAE</head><p>Previous works suggest that diffusion models have a strong capability of resisting adversarial perturbations (Xue and Chen 2024), making them hard to attack via pixel-domain optimization. Moreover, they are even considered good purifiers of adversarial perturbations <ref type="bibr" target="#b18">(Nie et al. 2022</ref>). Here we propose a latent optimization strategy that crafts the "perturbation" in latent space. We adopt a pre-trained Variational Autoencoder (VAE) <ref type="bibr" target="#b7">(Kingma and Welling 2014)</ref> to convert images to their latent space, and the gradients will be used to update the latent, after N iterations or losses converge, we decode back via decoder D to pixel domain as our final protected image. The motivation for adopting VAE is inspired by MPGD <ref type="bibr" target="#b5">(He et al. 2024</ref>). This strategy is effective for crafting a robust adversarial image against pixel-domain diffusion models while also better preserving the protection quality rather than only incorporating fidelity constraints.</p><p>The constraint optimization thereby becomes: max</p><formula xml:id="formula_11">z adv E t,</formula><p>xt|x,x adv t |D(z adv ) -L attack (x t , x adv t ) subject to L fidelity (x, D(z adv )) ≤ δ. (7) Algorithm 1: AtkPDM + 1: Input: Image to be protected x, attack budget δ &gt; 0, step size γ1, γ2 &gt; 0, VAE encoder E, and VAE decoder D 2: Initialization: x adv ← x, Lattack ← ∞ 3: Encode adversarial image to latent space: z adv ← E(x adv ) 4: while Lattack not convergent do 5: Decode adversarial latent to pixel space: x adv ← D(z adv ) 6: Sample timestep: t ∼ [0, T ] 7: Sample noise: ϵ1, ϵ2 ∼ N (0, I) 8: Compute original noisy sample: xt ← F (x, t, ϵ1) 9: Compute adversarial noisy sample: x adv t ← F (x adv , t, ϵ2) 10: Update z adv by Gradient Descent: z adv ← z adv -γ1 sign(∇ z adv Lattack(xt, x adv t</p><p>)) 11:</p><p>while Lfidelity(x, D(z adv )) &gt; δ do 12:</p><p>z adv ← z adv -γ2∇ z adv Lfidelity(x, D(z adv )) 13:</p><p>end while 14: end while 15: Decode adversarial latent to pixel space: x adv ← D(z adv ) 16: return x adv Detailed latent optimization loop is provided in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Results</head><p>In this section, we examine the attack effectiveness and robustness of our approach under extensive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>Implementation Details. We conduct all our experiments in white box settings and examine the effectiveness of our attacks using SDEdit <ref type="bibr" target="#b17">(Meng et al. 2021)</ref>. For the Variational Autoencoder (VAE) <ref type="bibr" target="#b7">(Kingma and Welling 2014)</ref> in our AtkPDM + , we utilize the VAE provided by StableDiffusion V1.5 <ref type="bibr" target="#b21">(Rombach et al. 2022)</ref>. We run all of our experiments with 300 optimization steps, which is empirically determined, to balance attacking effectiveness and image protection quality with reasonable speed. Other loss parameters and running time are provided in the Appendix. The implementation is built on the Diffusers library. All the experiments are conducted with a single Nvidia Tesla V100 GPU.</p><p>Victim Models and Datasets. We test our approach on PDMs with three open-source checkpoints on Hug-gingFace, specifically "google/ddpm-ema-church-256", "google/ddpm-cat-256" and "google/ddpm-ema-celebahq-256". For the results reported in Table <ref type="table" target="#tab_3">1</ref>, we run 30 images for each victim model. Additionally, for generalizability in practical scenarios, we synthesize the data with half randomly from the originally trained dataset and another half from randomly crawled with keywords from the Internet.</p><p>Baseline Methods and Evaluation Metrics. To the best of our knowledge, previous methods have mainly focused on LDMs, and effective PDM attacks have not yet been developed, however, we still implement Projected Gradient Ascent (PGAscent) with their proposed semantic loss by <ref type="bibr" target="#b23">(Salman et al. 2023;</ref><ref type="bibr">Liang et al. 2023;</ref><ref type="bibr">Liang and Wu 2023;</ref><ref type="bibr">Xue et al. 2024b</ref>). Notably, Diff-Protect <ref type="bibr">(Xue et al. 2024b)</ref> proposed to minimize the semantic loss is surprisingly better than maximizing the semantic loss, we also</p><p>Methods Adversarial Image Quality Attacking Effectiveness SSIM ↑ PSNR ↑ LPIPS ↓ SSIM ↓ PSNR ↓ LPIPS ↑ IA-Score ↓ Church PGAscent 0.37 ± 0.09 28.17 ± 0.22 0.73 ± 0.16 0.89 ± 0.05 31.06 ± 1.94 0.17 ± 0.09 0.93 ± 0.04 Diff-Protect 0.39 ± 0.07 28.03 ± 0.12 0.67 ± 0.11 0.82 ± 0.05 31.90 ± 1.08 0.23 ± 0.07 0.91 ± 0.04 AtkPDM (Ours) 0.75 ± 0.03 28.22 ± 0.10 0.26 ± 0.04 0.75 ± 0.04 29.61 ± 0.23 0.40 ± 0.05 0.76 ± 0.06 atkPDM + (Ours) 0.81 ± 0.03 28.64 ± 0.19 0.13 ± 0.02 0.79 ± 0.04 30.05 ± 0.47 0.33 ± 0.07 0.81 ± 0.06 Cat PGAscent 0.48 ± 0.09 28.34 ± 0.18 0.65 ± 0.12 0.96 ± 0.02 32.32 ± 2.49 0.10 ± 0.05 0.97 ± 0.03 Diff-Protect 0.33 ± 0.10 28.03 ± 0.15 0.80 ± 0.15 0.90 ± 0.05 33.94 ± 1.93 0.18 ± 0.08 0.95 ± 0.03 atkPDM (Ours)</p><p>0.71 ± 0.06 28.47 ± 0.18 0.29 ± 0.05 0.83 ± 0.03 30.73 ± 0.51 0.39 ± 0.05 0.81 ± 0.04 atkPDM + (Ours) 0.83 ± 0.04 29.41 ± 0.37 0.09 ± 0.02 0.93 ± 0.01 33.02 ± 0.74 0.18 ± 0.02 0.92 ± 0.01 Face PGAscent 0.48 ± 0.05 28.75 ± 0.18 0.64 ± 0.10 0.99 ± 0.00 37.96 ± 1.75 0.02 ± 0.01 0.99 ± 0.00 Diff-Protect 0.25 ± 0.04 28.09 ± 0.20 0.91 ± 0.11 0.95 ± 0.02 35.33 ± 1.62 0.08 ± 0.04 0.96 ± 0.02 atkPDM (Ours) 0.56 ± 0.04 28.01 ± 0.22 0.36 ± 0.04 0.74 ± 0.03 29.14 ± 0.36 0.40 ± 0.05 0.62 ± 0.07 atkPDM + (Ours) 0.81 ± 0.04 28.39 ± 0.20 0.12 ± 0.03 0.86 ± 0.03 30.26 ± 0.72 0.24 ± 0.07 0.80 ± 0.08 Table 2: Quantitative results of our adversarial images against defense methods. Both Crop-and-Resize and JPEG Compression fail to defend our attack. "None" indicates no defense is applied, as the baseline for comparison.</p><p>adopted this method in attacking PDMs and denote as Diff-Protect. To quantify the adversarial image visual quality, we adopted Structural Similarity (SSIM) <ref type="bibr" target="#b30">(Wang et al. 2004</ref>), Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b35">(Zhang et al. 2018)</ref>. We also inherit these three metrics, but negatively to quantify the effectiveness of our attack. We also adopted Image Alignment Score (IA-Score) <ref type="bibr" target="#b8">(Kumari et al. 2023</ref>) that leverages CLIP <ref type="bibr" target="#b20">(Radford et al. 2021)</ref> to calculate the cosine similarity of image encoder features. In distinguishing from previous methods, to more faithfully reflect the attack effectiveness, we fix the same seed of the random generator when generating clean and adversarial samples, then calculate the scores based on the paired samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Attack Effectiveness on PDMs</head><p>As quantitatively reported in Table <ref type="table" target="#tab_3">1</ref> and qualitative results in Figure <ref type="figure" target="#fig_1">4</ref>, compared to previous PGD-based methods incorporating semantic loss, i.e., negative training loss of diffusion models, our method exhibits superior performance in both adversarial image quality and attacking effectiveness. And our reported figures has generally stable as reflected in lower standard deviation. It is worth noting that even if the adversarial image qualities of the PGD-based methods are far worse than ours, their attacking effectiveness still falls short, suggesting that PDMs are robust against traditional perturbation methods, this finding is also aligned with pre- vious works <ref type="bibr">(Xue et al. 2024b;</ref><ref type="bibr">Xue and Chen 2024)</ref>. For AtkPDM + , combined with our latent optimization strategy, the adversarial image quality has enhanced while slightly affecting the attacking effectiveness, still outperforming the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Against Defense Methods</head><p>We examine the robustness of our approach against two widely recognized and effective defense methods for defending against adversarial attacks as reported in Table <ref type="table">2</ref>.</p><p>Crop and Resize. Noted by Diff-Protect, crop and resize is simple yet the most effective defense method against their attacks on LDMs. We also test our method against this defense using their settings, i.e., cropping 20% of the adversarial image and then resizing it to its original dimensions.</p><p>JPEG Compression. Sandoval-Segura et al. <ref type="bibr" target="#b23">(Sandoval-Segura, Geiping, and Goldstein 2023)</ref> demonstrated that JPEG compression is a simple yet effective adversarial defense method. In our experiments, we implement the JPEG compression at a quality setting of 25%. The quantitative results in Table <ref type="table">2</ref> demonstrate that our method is robust against these two defense methods, with four of the metrics listed in Table <ref type="table">2</ref> are not worse than no defenses. Surprisingly, these defense methods even make the adversarial image more effective than cases without defense.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Black Box Transferability</head><p>We craft adversarial images with the proxy model, "google/ddpm-ema-church-256", in white-box settings and test their transferability to another "google/ddpm-bedroom-256" model for black-box attacks. Under identical validation settings, Table <ref type="table" target="#tab_5">3</ref> reveals only a slight decrease in attack effectiveness metrics, indicating successful black-box transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effectiveness of Latent Optimization via VAE</head><p>We first incorporate our VAE latent optimization strategy in the previous semantic-loss-based PGAscent. From Table <ref type="table" target="#tab_6">4</ref>, without using L fidelity , latent optimization has significantly enhanced the adversarial image quality and even slightly improved the attacking effectiveness. Adopting latent optimization in our approach enhances visual quality with a negligible decrease in attacking effectiveness. Surprisingly, incorporating our L fidelity with current PGD-based method will drastically decrease the adversarial image quality despite its attack performing better than ours. This may be due to different constrained optimization problem settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present the first framework designed to effectively protect images manipulation by Pixel-domain Diffusion Models (PDMs). We demonstrate that while denoising UNets appear robust to conventional PGD-based attacks, their feature space remains vulnerable to attack. Our proposed feature attacking loss exploit the vulnerabilities to empower adversarial images to mislead PDMs, thereby producing low-quality output images. We approach this image protection problem as a constrained optimization problem, solving it through alternating optimization. Additionally, our latent optimization strategy via VAE enhances the natural-ness of our adversarial images. Extensive experiments validate the efficacy of our method, achieving state-of-the-art performance in attacking PDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A More Implementation Details</p><p>The feature extractor for calculating L fidelity is VGG16 (Simonyan and Zisserman 2014) with IMAGENET1K-V1 checkpoint. We use the SDEdit with the forward step t = 500 for our main study results as it balances faithfulness to the original image and flexibility for editing. Empirically, we choose to randomly sample the forward step t ∼ [0, 500] to enhance the optimization efficiency. The average time to optimize 300 steps for an image on a single Nvidia Tesla V100 is about 300 seconds. The estimated average memory usage is about 24GB.</p><p>Table 5 provides the details of the step sizes that we use to attack different models. Models Step Size γ1 (Lattack) γ2 (Lfidelity) google/ddpm-ema-church-256 100/255 40/255 google/ddpm-cat-256 100/255 5/255 google/ddpm-ema-celebahq-256 50/255 35/255</p><p>Table 5: The step sizes used for different models during optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Attack Effectiveness on Latent Diffusion Models</head><p>We propose the feature representation attacking loss which can be adapted to target any UNet-based diffusion models. Hence, it is applicable to attack LDM using our proposed framework. We follow the evaluation settings of the previous works <ref type="bibr">(Xue et al. 2024b</ref>) for fair comparisons. Quantitative results are shown in Table <ref type="table">6</ref>. Compared to previous LDM-specified methods, our method could achieve comparable results. This finding reflects the general vulnerability in UNet-based diffusion models that can be exploited to craft adversarial images against either PDMs or LDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Qualitative Demonstration of Corrupting UNet Feature during Sampling</head><p>We qualitatively show an example of our attack effectiveness regarding UNet representation discrepancies in Figure <ref type="figure">5</ref>. We compare a clean and an adversarial image using the same denoising process. Then, we take the feature maps of the second-last decoder block layer, close to the final predicted noise, to demonstrate their recognition of input image semantics. The results in Figure <ref type="figure">5</ref> show that from t = 500, the feature maps of each pair start with a similar structure, then as the t decreases, the feature maps gradually have higher discrepancies, suggesting our method, by attacking the middle representation of UNet, can effectively disrupt the reverse denoising process and mislead to corrupted samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Qualitative Results of Loss Ablation</head><p>Figure <ref type="figure">6</ref> presents qualitative results of loss ablation where i., ii., and iii. indicate performing PGAscent with different con-figurations. i. utilizes only semantic loss; ii. utilizes semantic loss with our latent optimization strategy; iii. utilizes both semantic loss, our proposed L fidelity and latent optimization. The results show that our L fidelity and latent optimization can enhance the adversarial image quality of PGAscent. Moreover, comparing our proposed two methods, AtkPDM + generates a more natural adversarial image than AtkPDM while maintaining attack effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Different Forward Time-step Sampling</head><p>When using Monte Carlo sampling for optimization, the forward time step t * is sampled uniformly. We study the scenario that when t * is fixed for optimization. As shown in Figure <ref type="figure">7</ref>, a primary result shows that when attacking t * = 400 to t * = 500, the attacking effectiveness is better than other time steps. In practice, we can not know user-specified t * for editing in advance; however, this suggests that diffusion models have a potential temporal vulnerability that can be further exploited to increase efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 More Qualitative Results</head><p>We provide more qualitative results in Figure <ref type="figure">8</ref> to showcase that our method can significantly change or corrupt the generated results with little modification on adversarial images. In contrast, previous methods add obvious perturbation to adversarial images but still fail to change the edited results to achieve the safeguarding goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Example of Loss Curves</head><p>Figure <ref type="figure">9</ref> shows an example of our loss trends among optimization steps. L attack has decreasing trend as the optimization step increases. L fidelity has an increasing trend and converges to satisfy the constraint of the attack budget δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limitations</head><p>While our method can deliver acceptable attacks on PDMs, its visual quality is still not directly comparable to the results achieved on LDMs, indicating room for further improvement. More generalized PDM attacks should be further explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Societal Impacts</head><p>Our work will not raise potential concerns about diffusion model abuses. Our work is dedicated to addressing these issues by safeguarding images from being infringed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our attack scenario. Diffusion-based image editing can generate high-quality image variation based on the clean input image. However, by adding carefully crafted perturbation to the clean image, the diffusion process will be disrupted, producing a corrupted image or unrelated image semantics to the original image.</figDesc><graphic coords="1,412.01,347.58,320.35,65.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative results compared to previous methods: our adversarial images can effectively corrupt the edited results without significant fidelity decrease. The same column shares the same random seed for fair comparison.</figDesc><graphic coords="7,106.92,54.00,398.16,279.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Results in attacking different PDMs. The best is marked in red and the second best is marked in blue. Errors denote one standard deviation of all images in our test datasets.</figDesc><table><row><cell>Defense Method</cell><cell cols="4">Attacking Effectiveness SSIM ↓ PSNR ↓ LPIPS ↑ IA-Score ↓</cell></row><row><cell>Crop-and-Resize</cell><cell>0.68</cell><cell>29.28</cell><cell>0.42</cell><cell>0.79</cell></row><row><cell>JPEG Comp.</cell><cell>0.78</cell><cell>29.82</cell><cell>0.36</cell><cell>0.79</cell></row><row><cell>None</cell><cell>0.79</cell><cell>30.05</cell><cell>0.33</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results of black box attack. We use the same set of adversarial images and feed to white box and black box models to examine the black box transferability.</figDesc><table><row><cell>Setting</cell><cell cols="4">Attacking Effectiveness SSIM ↓ PSNR ↓ LPIPS ↑ IA-Score ↓</cell></row><row><cell>White Box</cell><cell>0.79</cell><cell>30.05</cell><cell>0.33</cell><cell>0.81</cell></row><row><cell>Black Box</cell><cell>0.86</cell><cell>30.25</cell><cell>0.29</cell><cell>0.85</cell></row><row><cell>Difference</cell><cell>0.07</cell><cell>0.20</cell><cell>0.04</cell><cell>0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>± 0.09 28.17 ± 0.22 0.73 ± 0.16 0.89 ± 0.05 31.06 ± 1.94 0.17 ± 0.09 0.93 ± 0.04 Lsemantic ✓ 0.80 ± 0.05 29.78 ± 0.42 0.17 ± 0.03 0.82 ± 0.05 30.43 ± 0.75 0.15 ± 0.06 0.92 ± 0.04 Lsemantic + Lfidelity ✓ 0.82 ± 0.05 30.30 ± 0.81 0.13 ± 0.03 0.90 ± 0.03 31.24 ± 1.19 0.08 ± 0.03 0.96 ± 0.02Quantitative results of ablation study. The best is in bold and the second best is underlined. Errors denote one standard deviation of all images in our test datasets.</figDesc><table><row><cell>Losses</cell><cell>VAE</cell><cell>Adversarial Image Quality SSIM ↑ PSNR ↑ LPIPS ↓</cell><cell>SSIM ↓</cell><cell>Attacking Effectiveness PSNR ↓ LPIPS ↑</cell><cell>IA-Score ↓</cell></row><row><cell cols="6">Lsemantic 0.37 Lattack + Lfidelity (Ours) 0.75 ± 0.03 28.22 ± 0.10 0.26 ± 0.04 0.75 ± 0.04 29.61 ± 0.23 0.40 ± 0.05 0.76 ± 0.06</cell></row><row><cell>Lattack + Lfidelity (Ours)</cell><cell>✓</cell><cell cols="4">0.81 ± 0.03 28.64 ± 0.19 0.13 ± 0.02 0.79 ± 0.04 30.05 ± 0.47 0.33 ± 0.07 0.81 ± 0.06</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0.81 ± 0.03 29.45 ± 0.13 0.25 ± 0.05 0.14 ± 0.03 27.95 ± 0.13 0.76 ± 0.04 0.48 ± 0.05 Diff-Protect (-) 0.79 ± 0.03 29.92 ± 0.15 0.24 ± 0.06 0.15 ± 0.03 28.00 ± 0.14 0.71 ± 0.04 0.48 ± 0.05 Diff-Protect (+) 0.79 ± 0.04 29.47 ± 0.11 0.26 ± 0.06 0.17 ± 0.04 28.00 ± 0.15 0.69 ± 0.04 0.49 ± 0.06 AtkPDM (Ours) 0.82 ± 0.02 30.40 ± 0.27 0.24 ± 0.05 0.14 ± 0.03 27.96 ± 0.17 0.74 ± 0.02 0.47 ± 0.04 AtkPDM + (Ours) 0.61 ± 0.07 29.17 ± 0.32 0.20 ± 0.02 0.27 ± 0.06 28.07 ± 0.18 0.66 ± 0.05 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details of Our Proposed Algorithm E.1 AtkPDM Algorithm without Latent Optimization</head><p>Algorithm 2: AtkPDM 1: Input: Image to be protected x, attack budget δ &gt; 0, and step size γ1, γ2 &gt; 0 2: Initialization:</p><p>Sample noise: ϵ1, ϵ2 ∼ N (0, I) 6:</p><p>Compute original noisy sample: xt ← F (x, t, ϵ1) 7:</p><p>Compute adversarial noisy sample:</p><p>Update x adv by Gradient Descent:</p><p>while Lfidelity(x adv , x) &gt; δ do 10:</p><p>x adv ← x adv -γ2∇ x adv Lfidelity(x adv , x) 11:</p><p>end while 12: end while 13: return x adv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 2-Wasserstein Distance Between Two Normal Distribution</head><p>Consider the normal distributions N t := N (µ t , Σ t ) and</p><p>) denote a joint distribution over the product space R n × R n . The 2-Wasserstein distance between N t and N adv t is defined as:</p><p>Using properties of the mean and covariance, we have the following identities:</p><p>= trace E[(x tµ t )(x adv t µ adv t ) ⊤ . Thus, the 2-Wasserstein distance can be expressed as:</p><p>where J is the joint covariance matrix of N t and N adv t , defined as:</p><p>and C is the covariance matrix between N t and N adv t :</p><p>. By the Schur complement, the problem can be formulated as a semi-definite programming (SDP) problem: maximum trace(C),</p><p>The closed-form solution for C derived from the SDP is: Finally, the closed-form solution for the 2-Wasserstein distance between the two normal distributions is given by:</p><p>(10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Alternating Optimization</head><p>Let y = x adv , by Lagrange relaxation <ref type="bibr" target="#b14">(Liu et al. 2023</ref>), the objective function can be expressed as:</p><p>where λ &gt; 0 is the Lagrange multiplier and F 1 , F 2 are defined as</p><p>The optimization is carried out in an alternating manner as follows:</p><p>To solve Equation <ref type="formula">14</ref>, we employ the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b4">(Goodfellow, Shlens, and Szegedy 2015)</ref>. The update is given by:</p><p>For Equation <ref type="formula">15</ref>, we utilize Gradient Descent, resulting in the following update:</p><p>Note that the gradient of F 2 can be derived as follows:</p><p>where</p><p>Please note that after references, we also provide more results presented in Figures <ref type="figure">6,</ref> <ref type="figure">7</ref>, 8, and 9, please refer to subsequent pages.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Step</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal transport for Gaussian mixture models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6269" to="6278" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Fréchet distance between multivariate normal distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multivariate analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="450" to="455" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manifold Preserving Guided Diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uesaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-concept customization of text-to-image diffusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1931" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual Adversarial Robustness: Defense Against Unseen Threat Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09608</idno>
		<title level="m">Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12683</idno>
		<title level="m">Mist: Towards Improved Adversarial Examples for Diffusion Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04578</idno>
		<title level="m">Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00110</idno>
		<title level="m">Diffusion Model with Perceptual Loss</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Instruct2Attack: Language-Guided Semantic Adversarial Attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15551</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distraction is All You Need: Memory-Efficient Image Immunization against Diffusion-Based Image Editing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="24462" to="24471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Guided image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Olkin, I.; and Pukelsheim, F. 1982. The distance between two random vectors with given dispersion matrices</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09608</idno>
	</analytic>
	<monogr>
		<title level="m">Diffusion Models for Adversarial Purification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="257" to="263" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14988</idno>
		<title level="m">DreamFusion: Text-to-3D using 2D Diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khaddaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sandoval-Segura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06588</idno>
		<idno>arXiv:2304.02234</idno>
		<title level="m">JPEG compressed images can bypass protections against ai editing</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Raising the Cost of Malicious AI-Powered Image Editing</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04222</idno>
		<title level="m">Protecting Artists from Style Mimicry by Text-to-Image Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Passananti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13828</idno>
		<title level="m">Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="574" to="584" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Tsaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.00522</idno>
		<title level="m">LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stylediffusion: Controllable disentangled style transfer via diffusion models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7677" to="7689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2024a. Diffusionbased adversarial sample generation for improved stealthiness and controllability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.13320</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Toward effective protection against diffusion based mimicry through score distillation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12832</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inversion-based style transfer with diffusion models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10146" to="10156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
