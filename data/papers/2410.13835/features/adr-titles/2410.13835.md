- Definition of extreme-token phenomena
- Identification of attention sinks
- Analysis of value-state drains
- Examination of residual-state peaks
- Active-dormant mechanism in attention heads
- Mutual reinforcement mechanism during pretraining
- Selection of the Bigram-Backcopy (BB) task for experiments
- Architectural choices for transformer models
- Optimization strategies (Adam vs. SGD)
- Replacement of softmax with ReLU in attention heads
- Impact of input domain on attention head activity
- Consistency of findings between BB task and pretrained LLMs
- Proposed strategies for mitigating extreme-token phenomena
- Evaluation of attention dynamics in pretrained LLMs
- Interpretation of attention maps in the context of extreme-token phenomena
- Implications for long-context inference and model quantization
- Relationship between extreme-token phenomena and interpretability challenges
- Documentation of empirical results and theoretical predictions
- Future research directions based on findings