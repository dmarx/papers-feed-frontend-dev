# Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs

## Abstract

## 

Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called "sink tokens" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.

We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures-transformers with one to three layers-trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.

.

## Introduction

Recent analyses of transformer-based open-source large language models (LLMs), such as GPT-2 [(Radford et al., 2019)](#b45), Llama-2 [(Touvron et al., 2023)](#b53), Llama-3 [(Dubey et al., 2024)](#b14), [Mixtral (Jiang et al., 2023)](#), and Pythia [(Biderman et al., 2023)](#b3), have revealed three intriguing phenomena:

-Attention sinks [(Xiao et al., 2023b)](#): In many attention heads, the initial token consistently attracts a large portion of the attention weights. Other special tokens, such as the delimiter token, can also draw significant attention weights. These tokens are collectively referred to as sink tokens.

-Value-state drains [(Guo et al., 2024)](#b24): For the attention heads that exhibit attention sinks, the value states of sink tokens are consistently much smaller than those of other tokens.

-Residual-state peaks [(Sun et al., 2024)](#b49): The residual states of sink tokens, excluding those from the first and last layers, exhibit significantly larger norms compared to other tokens.

These phenomena often appear together and consistently occur in various pretrained LLMs, which we collectively refer to as the extreme-token phenomena. Figure [1](#fig_1) illustrates these phenomena in Llama-3.1-8B-Base, s Summer is warm . Winter is cold . Head 0 Head 10 s Summer is warm . Winter is cold . s Summer is warm . Winter is cold . Head 20 s Summer is warm . Winter is cold . The empirical distribution of the norms of value states over all layers and all heads. We exclude 2% of the outlier values to help visualization. We observe the value-state drain phenomenon: the value state of the ⟨s⟩ token is much smaller than those of other tokens on average. Right (c): The norm of the residual stream states, measured at the output of each layer. We observe the residual-state peak phenomenon: the ⟨s⟩ token's residual states have significantly larger norms than those of other tokens from layers 1 to 30. We present the extreme-token phenomena over other input sequences in Appendix F.

using a fixed prompt sentence: "⟨s⟩Summer is warm⟨period⟩ Winter is cold⟨period⟩". Here, the first token, ⟨s⟩ (the Beginning-of-Sequence token), serves as the sink token. As shown in the figure, the sink token receives disproportionately high attention weights, exhibits significantly smaller value states, and has much larger residual state norms compared to other tokens. It is important to note that the first token does not have to be ⟨s⟩ to act as a sink token; other tokens appearing first in the sequence can also serve this role. Additionally, in models such as Llama-2, a delimiter token can also function as the sink token.

The extreme-token phenomena have posed several challenges for pretrained transformers in downstream tasks. For instance, sink tokens require special treatment during long-context inference [(Xiao et al., 2023b;](#)[Han et al., 2023;](#b26)[Yu et al., 2024;](#b63)[Chen et al., 2024)](#b8) and model quantization [(Dettmers et al., 2022;](#b13)[Liu et al., 2024;](#b38)[Son et al., 2024)](#b48) to maintain high levels of performance. Additionally, attention sinks have reduced the interpretability of attention maps in vision transformers [(Darcet et al., 2023)](#b10). To address these issues, [Sun et al. (2024)](#b49) and [Darcet et al. (2023)](#b10) propose adding a "special token" to transformers to serve as the sink token, preventing other tokens from becoming sinks. However, even this special token still exhibits extremetoken phenomena. Despite these efforts, no prior work has satisfiably explained the mechanisms behind the extreme-token phenomena. [Xiao et al. (2023b)](#) proposes a hypothesis for why they occur, suggesting that models tend to dump unnecessary attention values to specific tokens.

This work aims to demystify the extreme-token phenomena in LLMs. We demonstrate that these phenomena arise from an active-dormant mechanism in attention heads (cf. Claim 1), coupled with a mutualreinforcement mechanism during pretraining (cf. Claim 2). We support these statements through studies on simplified transformer architectures and tasks, a dynamical theory for these models, and experiments on pretrained LLMs. The structure of the paper and our key contributions are outlined as follows:

1. In Section 2, we train one-to three-layer transformers on a simple task called the Bigram-Backcopy (BB) task, which also displays extreme-token phenomena similar to those observed in LLMs. We show that attention sinks and value-state drains are a consequence of the active-dormant mechanism (cf. Claim 1). Both theoretically and empirically, we demonstrate that mutual reinforcement mechanism (cf. Claim 2) dynamically drives these phenomena: attention sinks and value-state drains reinforce one another, leading to a stable phase where all query tokens generate near identical attention logits for the keys of extreme tokens. Additionally, empirical results reveal that residual-state peaks arise from the interaction between this mutual reinforcement mechanism and the Adam optimization algorithm.

BB-task Theory BB-task Experiments LLM Experiments ∆logit •,⟨s⟩ log-growth

$✓ ✓ ⋆ ∥Val ⟨s⟩ ∥ monotonic decrease ✓ ✓ ✓ ∥Res ⟨s⟩ ∥ linear growth ⋆ ✓ ✓ logit •,⟨s⟩ concentration ✓ ✓ ✓$Table [1](#): Consistency of the quantitative properties across the theoretical and empirical results of the Bigram-Backcopy task and empirical results of LLMs. A ✓ denotes a consistent result, while a ⋆ denotes an inconclusive result. The logit •,⟨s⟩ denotes logits corresponding to the key of the extreme token and queries of all non-extreme tokens, i.e., the Qry ⊤ • Key ⟨s⟩ . The ∆logit •,⟨s⟩ = logit •,⟨s⟩ -Mean[logit •,others ] is a progress measure for attention sinks. The ∥Val ⟨s⟩ ∥ denotes the value state norm of the extreme token, and ∥Res ⟨s⟩ ∥ denotes the residual state norm of the extreme token. See Section 1.2 for the definitions of these notations.

2. In Section 3, we demonstrate the active-dormant mechanism in pre-trained LLMs by showing that many attention heads transition between active and dormant phases based on the input domain. Specifically, we identify an interpretable active-dormant head (Layer 16, Head 25 in Llama 2-7B-Base [(Touvron et al., 2023)](#b53)) that activates on GitHub data but remains dormant on Wikipedia data. Moreover, in examining the dynamics of OLMo-7B-0424 [(Groeneveld et al., 2024)](#b21), we observe the same mutual reinforcement mechanism and stable phase, consistent with those found in the BB task. This demonstrates that the simple BB model captures both the static and dynamic properties of extreme-token phenomena in LLMs and accurately predicts their behavior.

3. Importantly, the quantitative properties of extreme-token dynamics show strong consistency among the theoretical and empirical results of the Bigram-Backcopy task and the empirical performance of OLMo.

In particular, we consistently observe the sink-logits concentration phenomenon, where the logits corresponding to the key of the extreme token and the queries of all non-extreme tokens (logit •,⟨s⟩ ) are nearly identical-an observation not previously documented in the literature. We summarize the aligned results between the theoretical and empirical findings of the Bigram-Backcopy task and the empirical performance of LLMs in Table [1](#).

4. We propose architectural and optimization modifications to mitigate the extreme-token phenomena. Specifically, we demonstrate that replacing SoftMax with ReLU activations in attention heads eliminates extreme-token phenomena in the BB task, while switching from Adam to SGD removes the residual-state peak phenomenon. We discuss the possibility that similar modifications could mitigate extreme-token phenomena in LLMs.

## Related work

Several studies independently identified the "attention sink" phenomenon in language models and vision transformers, where attention weights were found to be concentrated on a few tokens [(Xiao et al., 2023b;](#)[Darcet et al., 2023;](#b10)[Han et al., 2023;](#b26)[Zhai et al., 2023;](#b65)[Elhage et al., 2023;](#b16)[Dettmers et al., 2022)](#b13). Recent research has provided more detailed characterizations of this attention pattern and the attention sink phenomenon [(Fu, 2024;](#b19)[Sun et al., 2024)](#b49). [Sun et al. (2024)](#b49) attributed the attention sink to the massive activation of the hidden representations of the corresponding tokens. Both [Sun et al. (2024)](#b49) and [Zhai et al. (2023)](#b65) discussed methods for mitigating the attention sink by modifying the model and training recipes.

Additionally, recent studies have leveraged the attention sink phenomenon to develop improved quantization and more efficient inference algorithms [(Liu et al., 2024;](#b38)[Chen et al., 2024;](#b8)[Yu et al., 2024;](#b63)[Son et al., 2024;](#b48)[Lin et al., 2024a;](#)[Bondarenko et al., 2023;](#b6)[Hu et al., 2024)](#b28). A concurrent work by [Gu et al. (2024)](#b22) studied how optimization, data distribution, loss function, and model architecture in LM pre-training influence the emergence of attention sink, showing that replacing the softmax function with sigmoid can prevent attention sink emergence in models up to 1B parameters.

The dynamics of transformers are studied under various simplifications, including linear attention structures [(Zhang et al., 2023;](#b66)[Ahn et al., 2024)](#b1), reparametrizations [(Tian et al., 2023b)](#), NTK [(Deora et al., 2023)](#b11), often in the setting of in-context linear regression [(Ahn et al., 2023;](#b0)[Wu et al., 2023a;](#)[Zhang et al., 2024)](#b67) and structured sequences [(Bietti et al., 2024;](#b4)[Nichani et al., 2024;](#b43)[Tian et al., 2023a)](#). Notably, [Zhang et al. (2023)](#b66); [Huang et al. (2023)](#b29); [Kim et al. (2024)](#b33) demonstrate that a one-layer attention head trained via gradient descent converges to a model that effectively performs in-context regression. [Bietti et al. (2024)](#b4) shows the fast learning of bigram memorization and the slow development of in-context abilities. [Tian et al. (2023a)](#) shows the scan and snap dynamics in reparametrized one-layer transformers. [Reddy (2023)](#b46) simplifies the structure of the induction head, showing the connection between the sharp transitions of in-context learning dynamics and the nested nonlinearities of multi-layer operations.

Mechanistic interpretability is a growing field focused on understanding the internal mechanisms of language models in solving specific tasks [(Elhage et al., 2021;](#)[Geva et al., 2023;](#)[Meng et al., 2022;](#b40)[Nanda et al., 2023;](#b42)[Olsson et al., 2022;](#b44)[Bietti et al., 2024;](#b4)[Wang et al., 2022;](#b56)[Feng and Steinhardt, 2023;](#b18)[Todd et al., 2023)](#b52). This includes mechanisms like the induction head and function vector for in-context learning [(Elhage et al., 2021;](#)[Olsson et al., 2022;](#b44)[Todd et al., 2023;](#b52)[Bietti et al., 2024)](#b4), the binding ID mechanism for binding tasks [(Feng and Steinhardt, 2023)](#b18), association-storage mechanisms for factual identification tasks [(Meng et al., 2022)](#b40), and a complete circuit for indirect object identification tasks [(Wang et al., 2022)](#b56). The task addressed in this paper is closely related to [Bietti et al. (2024)](#b4), who explored synthetic tasks where tokens are generated from either global or context-specific bigram distributions. Several other studies have also employed synthetic tasks to explore neural network mechanisms [(Charton, 2022;](#b7)[Liu et al., 2022;](#b39)[Nanda et al., 2023;](#b42)[Allen-Zhu and Li, 2023;](#)[Zhu and Li, 2023;](#)[Guo et al., 2023;](#b23)[Zhang et al., 2022;](#b68)[Lin et al., 2023)](#b36).

A line of work focuses on quantizing neural networks using low-bit fixed-point representations [(Jacob et al., 2018;](#b30)[Zafrir et al., 2019;](#b64)[Lin et al., 2020;](#b37)[Nagel et al., 2021;](#b41)[Gholami et al., 2022)](#b20), such as INT8 [(Lin et al., 2020;](#b37)[Dettmers et al., 2022)](#b13) or INT4 [(Yao et al.;](#b19)[Wu et al., 2023b;](#)[Dettmers and Zettlemoyer, 2023)](#b12) to save memory usage and computational cost. In LLMs, the extreme-token phenomena lead to substantial performance degradation after quantization [(Bondarenko et al., 2021)](#b5) and have become a key focus of recent research [(Fan et al., 2020;](#b17)[Yao et al., 2022;](#)[Lin et al., 2024a;](#)[Hu et al., 2024)](#b28). [Dettmers et al. (2022)](#b13) and [Lin et al. (2024b)](#) propose mixed-precision approaches, using FP16 for outlier values and INT8 for others, enabling large model quantization without performance loss. [Xiao et al. (2023a)](#) rescales the weights and activations to reduce magnitudes of outliers, and [Bondarenko et al. (2023)](#b6) proposes modified attention structures to remove outliers, making language models easier to quantize.

We note that [Gurnee et al. (2024)](#b25) proposed Attention Deactivation Neurons, [Bondarenko et al. (2023)](#b6) proposed the "no-op" hypothesis, and [Xiao et al. (2023b)](#) proposed the "dump unnecessary attention" conjecture as mechanisms of attention sinks. In contrast, we explain the extreme-token phenomena through the active-dormant and mutual reinforcement mechanisms, offering the proof of their emergence within training dynamics in a toy model and providing empirical evidence of these mechanisms in LLMs.

## Preliminaries and notations

While different LLMs may use slightly varying transformer architectures, most use the structure proposed by [Vaswani (2017)](#b54), with the key modification being the shift from post-norm to pre-norm. We represent the tokenized input sequence of length n, with positional embeddings included, as

$H = [h 1 , . . . , h n ] ∈ R d×n ,$where h i denotes the ith input token, and d is the embedding dimension. We denote the layer-normalization operation as LN, the column-wise SoftMax operation as SoftMax, the causal-mask as mask, and the pointwise ReLU function as ReLU.

The transformer architecture applies causal-attention and MLP layers iteratively to the input sequence H.

A causal-attention layer with M heads is represented as

$Attn(•), parameterized by {(Q m , K m , V m , O m )} m : Attn(H) := M -1 m=0 attn m (H) ∈ R d×n ,(1)$where each attention head attn m (•) is given by

$attn m (H) := O m V m LN(H)SoftMax mask LN(H) ⊤ K ⊤ m Q m LN(H) .(2)$We denote the attention map as

$Map = SoftMax mask LN(H) ⊤ K ⊤ m Q m LN(H)$, and typically plot its transpose, Map ⊤ , in figures.

An MLP layer, denoted mlp(•), has parameters (W 1 , W 2 ):

$mlp(H) := W 2 ReLU(W 1 LN(H)) ∈ R d×n .$(3)

An L-layer transformer consists of a composition of L self-attention and MLP layers with residual connection structure. Given an input H (0) ∈ R d×n , the output of the L-layer transformer, H (L) , is computed as follows:

$H (ℓ+1) = H (ℓ+1/2) + mlp (ℓ) H (ℓ+1/2) , H (ℓ+1/2) = H (ℓ) + Attn (ℓ) H (ℓ) , ℓ ∈ {0, . . . , L -1}. (4)$For consistency between the code and the text, we adopt zero-indexing throughout this paper, meaning that attention head and layer indices begin at 0 instead of 1.

For the output H (ℓ+1) of layer ℓ, we define the residual state Res v of a token v ∈ {0, 1, . . . , n -1} as the vth column of H (ℓ+1) . For a specific layer ℓ with input H (ℓ) ∈ R d×n , and for a specific attention head m with query, key, and value matrices (Q, K, V, O), we define the query, key, and value states

$(Qry v , Key v , Val v ) of a token v ∈ [n]$as the vth columns of QH (ℓ) , KH (ℓ) , and OVH (ℓ) , respectively[foot_0](#foot_0) . The attention logit logit v ′ ,v is defined as the (v ′ , v)th element of (H (ℓ) ) ⊤ Q ⊤ KH (ℓ) . For notation simplicity, we omit the dependence on ℓ and m in (Qry v , Key v , Val v , logit v ′ ,v ), as these will be clear from context. Additionally, for a fixed token v, we use the shorthand logit •,v for the set

${logit v ′ ,v | v ′ ∈ V}.$We use ⟨s⟩ to refer to the "Beginning-of-Sequence" token. Since the ⟨s⟩ token consistently behaves as an extreme token in LLMs, we often refer to ⟨s⟩ and extreme tokens interchangeably. We also abuse the notation by writing (Qry ⟨s⟩ , Key ⟨s⟩ , Val ⟨s⟩ ) to represent the query, key, and value states of the ⟨s⟩ token.

2 Extreme-token Phenomena in the Bigram-Backcopy Task

In this section, we analyze simple transformers trained on the Bigram-Backcopy (BB) task, a simple model that exhibits extreme-token phenomena. We demonstrate the active-dormant mechanism (cf. Claim 1) and mutual reinforcement mechanism (cf. Claim 2) within the BB task and provide predictions for the behavior of sink tokens, which will be validated through LLM experiments in the following section.

The Bigram-Backcopy task is a data-generation model that consists of two sub-tasks: Bigram-transition and Backcopy. In this model, each sequence begins with the ⟨s⟩ token, followed by tokens sampled according to a pre-determined bigram transition probability P (in other words, a Markov chain). When specific trigger tokens are encountered, instead of sampling according to the transition P, the preceding token is copied to the next position. An illustration of the Bigram-Backcopy task is provided in Figure [2a](#fig_2). Following [Bietti et al. (2024)](#b4), we select the transition P and the vocabulary V with |V| = V = 64 based on the estimated characterlevel bigram distribution from the tiny Shakespeare dataset. In all experiments, the set of trigger tokens, T , is fixed and consists of the |T | = 3 most frequent tokens from the unigram distribution. Consequently, the non-trigger token set, V \ T , comprises 61 tokens.

## One-layer transformer exhibits attention sinks and value-state drains

On the Bigram-Backcopy task, we pre-train a standard one-layer transformer with a single SoftMax attn head and one mlp layer. Unless otherwise specified, the model is trained using Adam for 10, 000 steps, achieving near-optimal prediction accuracy. Detailed training procedures are provided in Appendix C.1. Figure [2b](#fig_2) shows that the trained transformer exhibits the attention sink phenomenon, where the ⟨s⟩ token captures a significant proportion of the attention weights. More importantly, the attention weights display interpretable patterns: all non-trigger tokens exhibit attention sinks, while the attention for trigger tokens is concentrated on their preceding positions. Additionally, Figure [2c](#fig_2) reveals a value-state drain phenomenon similar to that observed in LLMs, suggesting that, for non-trigger tokens, the attn head contributes minimal value to the residual stream. We provide additional attention patterns on different input sequences in Appendix C.2.

(a) The Bigram-Backcopy task <s> <s> <s> <s> t 1 t k-1 t 2 t 1 … t k t k+1 = t k P( ⋅ | t k ) t k-1 t 2 t 1 … t k t k+1 = t k P( ⋅ | t k ) P( ⋅ | t k ) Always start with ⟨s⟩ At trigger tokens, always copy its backward token to the next position Sample the next token by bigramtransition 𝖯( ⋅ | previous token) v v n a u t u h ⟨s⟩ … t (b) Attention pattern s v t v n a u t u h s v t v n a u t u h 0 1 (c) Small value states s v t v n a u t u h Tokens 0 5 10 15 20 Norms of Value States The active-dormant mechanism of the attention head. Inspired by the interpretable attention weight patterns observed, we propose the active-dormant mechanism. For any given token, an attention head is considered active if it makes a significant contribution to the residual state, and dormant if its contribution is minimal. As illustrated in Figure [2b](#fig_2), when trained on the BB task, the attention head is active for trigger tokens and dormant for non-trigger tokens.

Figure [4a](#fig_3) demonstrates that the mlp layer is responsible for the Bigram task whereas the attn head takes care of the Backcopy task. When the mlp layer is zeroed out, the backcopy loss remains significantly better than a random guess, but the bigram loss degrades to near-random levels. Conversely, when the attn layer is zeroed out, the backcopy loss becomes worse than a random guess, while the bigram loss remains unaffected. This indicates that on trigger tokens, the attn head is active and handles the backcopy task, whereas on nontrigger tokens, the attn head is dormant, allowing the mlp layer to handle the Bigram task. We summarize the active-dormant mechanism of the attn head in Claim 1.

Claim 1 (Active-dormant mechanism). Attention heads of pre-trained models are often governed by the active-dormant mechanism, exhibiting two phases:

(1) Dormant phase: On non-trigger tokens, the attn head assigns dominant weights to the ⟨s⟩ token, adding minimal value to the residual stream and having little impact on the model's output.

(2) Active phase: On trigger tokens, the attn head assigns dominant attention weights to relevant context tokens, adding substantial value to the residual stream and significantly impacting the model's output.

## Attention Head

Active Dormant

$0 v 1 v 2 v 3 v 4 × 0 0 0 0 0 Output Values h 0 h 1 h 2 h 3 h 4 h′ 0 h′ 1 h′ 2 h′ 3 h′ 4 Sequence 1 Sequence 2 0 ⋆ ⋆ ⋆ ⋆ Figure 3: Active-dormant mechanism$The growth of attention logits on the ⟨s⟩ token and the decrease in its value state norms. Figure [4b](#fig_3) illustrates the training dynamics of excess risks, attention weights, attention logits (for each token v n at position n in the prompt, we compute ∆logit •,⟨s⟩ ≡ mean n [⟨Qry vn , Key ⟨s⟩ ⟩mean i (⟨Qry vn , Key vi )⟩], which serves as a progress measure for attention sinks), and value state norms for the ⟨s⟩ token. All values are rescaled to the 0 to 1 range to highlight trends rather than absolute values. Both the Bigram and Backcopy excess risks decrease to nearly zero within the first 1000 steps, with the Bigram excess risk approaching zero faster than the Backcopy risk. As the Backcopy risk decreases, the attention weights on the ⟨s⟩ token begin to increase, suggesting a connection between the formation of attention sinks and the backcopy function in the attention heads. After the first 1000 steps, although both Bigram and Backcopy excess risks have nearly reached zero, the attention logits and weights on the ⟨s⟩ token continue to increase, while the value state norm of the ⟨s⟩ token continues to decrease. While this is an intriguing phenomenon, The excess risks, attention weights, attention logits, and value state norms for the ⟨s⟩ token throughout the training dynamics. Each curve is rescaled to fall within a 0 to 1 range. On the right side of (b), the horizontal axis is logarithmically scaled. The ∆logit •,⟨s⟩ curve represents the mean of attention logits from all given non-trigger query tokens v on the ⟨s⟩ token, normalized by the mean of attention logits for other tokens. The shaded area represents the 90% uncertainty interval on the distribution over all non-trigger tokens.

## Input sequence

Output logits

$MLP( ) = log( ) v p v Attention SoftMax × 0 α v α v α u λ 0 0 0 0 0 0 0 0 0 0 β ξ v e v ξ u e u 0 ξ v e v Attention weights Value states ⟨s⟩ v t v u ⟨s⟩ v v u t$Figure [5](#): Simplified transformer architecture. The output logits are computed by summing the contributions from both the mlp layer and the attn head. The predicted probabilities are obtained by applying the SoftMax function to these output logits. The mlp layer is assumed to provide the Markov transition probabilities for non-trigger tokens, while the attn head is parameterized by attention logits and value states, as described in Eq. ( [6](#formula_13)), (7), and (8). Additionally, the trainable variables, denoted by (α, β) ∈ R V × R V , represent the attention logits and value states of the ⟨s⟩ token.

our next goal is to understand why the attention logits and value state norms continue to evolve toward extreme values.

## Analysis of a minimally-sufficient transformer architecture

In this section, we analyze the training dynamics of transformers on the BB task, focusing on a simplified architecture that retains the attention sinks and value-state-drains phenomena. We analyze the regime when the Bigram transition probability is fully learned, and the Backcopy task is partially learned (i.e., after step 200 in Figure [4b](#fig_3)), and we focus on the dynamics of the attention logits and value states. Readers who are more interested in the results than the theoretical analysis can skip the detailed analysis and proceed directly to the statement of the mutual reinforcement mechanism in Claim 2.

Let V (of size V ) denote the set of all tokens excluding the ⟨s⟩ token, and let T represent the set of all trigger tokens. For any v ∈ V, we define p vk = P(k|v) as the next-token Markov transition probability, and p v = (p v1 , . . . , p vV ) ⊤ ∈ ∆(V) as the transition vector in the simplex. The embedding map is denoted by ebd : [n] × V → R D , where for a token v ∈ V at position i ∈ [n], the embedded vector is ebd i (v). The ⟨s⟩ token always appears at position 0, and we denote its embedding vector by ebd(⟨s⟩). For simplicity, we abuse the notation and use the sequence itself, [⟨s⟩, v 1 , . . . , v n ] where {v k } k∈[n] ⊆ V, to represent the embedding of the sequence. (n+1) with ⟨s⟩ as the zeroth token, we define the predicted probability of the next token as SoftMax(TF(H) n ), where TF(H) n ∈ R D is the last column of TF(H) ∈ R D×(n+1) , defined as

## Given an input sequence

$H = [⟨s⟩, v 1:n ] ∈ R D×$$TF(•) = attn(•) + mlp(•), attn(H) = VHSoftMax(mask(H ⊤ K ⊤ QH)), mlp(H) = W 2 ReLU(W 1 H). (5)$The simplified transformer architecture TF is a parallel summation of the attn head and the mlp layer, with no layer normalization. This parallel summation is a reasonable simplification, as sequential attn and mlp layers can effectively simulate parallel attn and mlp operations. Notice that we have redefined the notations of attn and mlp in this section, which are simplified versions of Eq. ( [2](#formula_3)) and (3).

Simplification and reparameterization of the model. To simplify the analysis of the training dynamics, we further reduce the model by restricting the (K, Q, V, W 1 , W 2 ) matrices to follow the patterns observed in the later training stages (i.e., after step 200 of the training in Figure [4b](#fig_3)).

• Restricted Attention Pattern. Based on the intuition from Figure [2b](#fig_2), we know that eventually only a few attention logits are non-trivial. Thus, we assume that the model has learned the attention pattern by this stage (which is reasonable given that the Backcopy risk is already small after step 200 in Figure [4b](#fig_3)).

We parameterize the attention logits on the ⟨s⟩ key-token as (α ⟨s⟩ ; α v1 ; . . . ; α vn ), restrict the attention logits for any trigger query-token to (0, . . . , λ, 0) (where the second-to-last coordinate is λ), and set all other logits to zero. Specifically, we restrict:

$ebd(⟨s⟩) ⊤ K ⊤ Q • ebd i (v) = α v • 1{v ̸ ∈ T } for v ∈ V, i ∈ [n], ebd i (v) ⊤ K ⊤ Q • ebd j (v) = λ • 1{v ∈ T , i = j -1} for v, v ∈ V, i, j ∈ [n].(6)$Notice that this naturally implies α v = 0 for v ∈ T .

• Restricted Value Pattern. At later stages of the training dynamics, we observe that the value states for each token are nearly a scaled version of the one-hot encoding vector. We assume this observed pattern and parameterize the value state of v by ξ v e v ∈ R V . For the ⟨s⟩ token, we parameterize its value state by β ∈ R V . Specifically, we restrict

$V • ebd(⟨s⟩) = β ∈ R V , V • ebd i (v) = ξ v e v ∈ R V , with ξ v = 0 for v ∈ T , and ξ v ≥ 0 for v ∈ V \ T .(7)$• MLP Layer Perfectly Predicts the Transition Probability. Notice that the mlp layer handles the Bigram task. By step 200 in Figure [4b](#fig_3), the Bigram risk has nearly vanished. Therefore, we assume that the mlp layer outputs the Markov transition probabilities p v for non-trigger tokens v, and zero for trigger tokens. Specifically, we restrict:

$mlp(ebd i (v)) = log p v • 1{v ̸ ∈ T } for v ∈ V. (8$$)$These reparameterizations are illustrated in Figure [5](#). Theorem 1 establishes the existence of a transformer architecture that satisfies the restrictions and reparameterizations outlined above. Furthermore, this restricted transformer can generate the ground-truth transitions of the BB model when certain parameters diverge.

Theorem 1 (Existence of reparameterization that solves the BB task; informal). For any parameters

$(α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R)$, there exists a one-layer transformer as described in (5) with weight matrices (Q, K, V, W 1 , W 2 ) such that Eq. ( [6](#formula_13)), (7), and (8) hold. Furthermore, there exists a sequence of parameters where min v∈V α v → ∞, min v∈V ξ v → ∞, λ → ∞, and β = 0, such that this transformer generates the ground-truth transitions of the BB model in the limit.

The formal statement and proof of Theorem 1 are provided in Appendix A.1.

Dynamic analyses of the reparameterized model. To analyze the later stage training dynamics, we adopt the reparameterization given in Eq. ( [6](#formula_13)), (7), and (8) as our assumption. We further define M k = n i=1 1{v i = k}, M = (M 1 , . . . , M V ), and M = k∈V M k = n. Substituting these into Eq. ( [5](#)), for a non-trigger token v ∈ V \ T , the output of the attention layer with input sequence H = [⟨s⟩, v 1:n-1 , v] is given by

$TF(H) n = log p v + e αv e αv + M β + V k=1 M k ξ k e αv + M • e k .(9)$Therefore, for the non-trigger token v, the cross-entropy loss between the true Markov transition p v and the predicted transition SoftMax(TF(H) n ) is given by

$loss v (α v , β) = V k=1 p vk log V i=1 p vi exp e αv β i + M i ξ i e αv + M - e αv β k + M k ξ k e αv + M -log p vk . (10$$)$For simplicity, we neglect the loss on trigger tokens and assume that ({M i } i∈[V ] , M ) remain fixed across different positions in the input sequences. [2](#foot_1) We then consider the total loss as the average of the losses on each non-trigger token, weighted by its proportion in the stable distribution {π v } v∈V , given by

$loss(α, β) = v∈V\T π v • loss v (α v , β). (11$$)$We assume that ξ and λ are fixed, and that α (the attention logits of the ⟨s⟩ token) and β (the value state norms of the ⟨s⟩ token) are trainable variables, as we are interested in the dynamics of the attention logits and value state norm for the ⟨s⟩ token. The following theorem illustrates the logarithmic growth of the attention logits α, the shrinkage of value states β, and the stable phase of these two variables.

Theorem 2. Consider the gradient flow of the loss function loss(α, β). Assume ξ v ≥ 0 for any v and π v > 0 for any v ∈ V, and {M i • ξ i } i∈V are not all equal.

(a) (Attention logits grow logarithmically, reinforced by small value states) Fix β = β • 1 for a constant β, and consider the gradient flow over α. With any initial value α(0), there exists r(t) with norm uniformly bounded in time, such that

$α(t) = 1 2 log t • 1 + r(t).(12)$(b) (Value state shrinks to a small constant vector, reinforced by large attention logits

$) Fix α = α • 1 for a constant α, define β(0) = V -1 [ v β v (0)] and B = V -1 [ v M v ξ v ]. Consider the gradient flow over β. As t → ∞, we have β(t) → β ⋆ = [β(0) + e -α B] • 1 -e -α • M • ξ.(13)$(c) (Stable phase: Sink-logits concentration) Consider the gradient flow over the variables (α, β). Any vector of the following form

$α = α • 1, β = c • 1 -e -α • M • ξ, α, c ∈ R (14)$is a stationary point. These are all global minimizers of loss(α, β).

The proof of Theorem 2 is provided in Appendix A.2, A.3, and A.4. We offer two key remarks: (1) As α v → ∞, a Taylor expansion of the gradient ∂loss/∂α v suggests that dα v /dt ∝ exp(-2α v ), which leads to the logarithmic growth of α v . Similar logarithmic growth has been reported in the literature under different setups [(Tian et al., 2023a;](#)[Zhu et al., 2024)](#b69); (2) The stable phase described in Theorem 2(c) seems to imply that the system can remain stable without attention sinks, as it does not require α to be large. However, in practice, models trained on the BB task tend to converge to a stable phase where α is relatively large.

The formation of attention sinks and value-state drains. Below, we explain how Theorem 2 reveals the mutual reinforcement mechanism behind the formation of attention sinks and value-state drains.

(a) When the value states of the ⟨s⟩ token are small and constant, β = β • 1, Theorem 2(a) shows that the attention logits on the ⟨s⟩ token α(t) ≈ α(t)1 for α(t) = (1/2) log t, grow logarithmically. This demonstrates that the presence of a small constant value state (β = β • 1) reinforces the formation of attention sinks (α(t) ≈ α(t) • 1 for α(t) increases logarithmically).

(b) When the attention logits of the ⟨s⟩ token are large and constant,

$α = α • 1 for α → ∞, Theorem 2(b)$shows that the value states of the ⟨s⟩ token β(t) → β(0) • 1. Starting with a random Gaussian initialization for β(0), we have

$∥β(t)∥ 2 ≈ ∥β(0) • 1∥ 2 ≈ ∥β(0)∥ 2 / √ V$, where V is the vocabulary size, typically large. This indicates that attention sinks (α = α • 1 for large α) reinforces the formation of value-state drains (β(t) → β • 1 for small β).

(c) In the later stages of the dynamics, both the attention logits and value states of the ⟨s⟩ token stabilize, as described in 2(c). The attention logits remain constant at α = α • 1 with large α, while the value states become small,

$β = [β(0) + e -α B] • 1 -e -α • M • ξ.$Based on these theoretical insights, we summarize the dynamical mechanism underlying attention sinks and value-state drains: For any attention head given a specific prompt, if the model can accurately predict the next token without using the attention head, but adding any value state from previous tokens-except for certain special tokens-worsens the prediction, the attention head will become dormant, forming an attention sink at those special tokens. This phenomenon is induced by the mutual reinforcement mechanism, as described below:

Claim 2 (Mutual reinforcement mechanism). Dynamically, attention sinks and value-state drains arise through mutual reinforcement:

(a) The SoftMax mechanism shifts attention weights towards tokens that exhibit value-state drains, reinforcing these tokens as attention sinks. (b) Attention sinks on these extreme tokens further suppress their value states, reinforcing their role as value-state drains. (c) The mutual reinforcement stabilizes when all non-trigger tokens have large, nearly identical attention logits on the extreme token. Due to the causal mask, the training dynamics favor the ⟨s⟩ token as the extreme token.

$α + (1-α) β = Attention Head Output Attention sink α → 1 Value state drain β → 0 v | v | > 0 with Figure 6: Mutual reinforce- ment mechanism$Experimental verification of the quantitative prediction. Revisiting Figure [4b](#fig_3), which illustrates the dynamics of a single-layer transformer model trained with Adam on the BB task, we observe that ∆logit •,⟨s⟩ exhibits growth rates consistent with Theorem 2. In this context, ∆logit •,⟨s⟩ corresponds to α, as all other attention logits are assumed to be zero under the assumptions of Theorem 2. When plotted on a logarithmic scale, the ∆logit •,⟨s⟩ curve grows approximately linearly between 1,000 and 10,000 steps, then accelerates before stabilizing around 100,000 steps. Meanwhile, the norm of the value state ∥Val ⟨s⟩ ∥ 2 decreases monotonically. The simultaneous increase in attention weights and decrease in value-state norms demonstrate the mutual reinforcement mechanism during the training process.

To further validate that Theorem 2 accurately captures the dynamics of the original model, we constructed a simplified model based on Eq. ( [6](#formula_13)), (7), and (8), and trained the parameters

$(α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R)$using Adam. The resulting training curves closely resemble those of the one-layer transformer, also displaying the mutual reinforcement mechanism. A detailed description of the experiment can be found in Appendix C.3.

Generality of the theoretical prediction. Although Theorem 2 focuses on a specific BB task with a simplified architecture and loss function, the underlying principles are broadly applicable to more general settings. In particular, we expect that the formation of extreme tokens in LLMs follows a similar mutual reinforcement mechanism. Indeed, Theorem 2 is essentially based on the following two key assumptions: (1) even with a specific attention head attn zeroed out, the LLM can still accurately predict the next token, implying that the attention head is better off dormant; and (2) for the attention head attn, value states of previous tokens-except for certain special tokens-remain relevant for specific tasks and therefore do not vanish. Under these assumptions, we anticipate the formation of attention sinks and value-state drains for the attention head attn and such special tokens. In Section 3, we explore how these phenomena are formed during the training dynamics of LLMs, finding that the empirical results align with the theory.

Replacing SoftMax by ReLU attention removes attention sinks and value-state drains. As a consequence of our theory, we predict that training using ReLU attention in place of SoftMax attention will prevent the mutual reinforcement mechanism. Without SoftMax, the training dynamics no longer push the attention weights toward the ⟨s⟩ token, which remains zero throughout training. In the absence of attention sinks, the dynamics no longer push down the value state norm, and the mutual reinforcement mechanism breaks. Figure [7a](#fig_4) presents the training dynamics on the BB task using ReLU instead of SoftMax attention, showing that both the Bigram and Backcopy risk converge to the Bayes risk after 200 training steps, but the attention logits of ⟨s⟩ do not increase, and the value state does not shrink, confirming our prediction.

## The emergence of residual-state peaks

In this section, we experimentally investigate the residual-state peaks phenomenon. We observe that no residual-state peaks occur in the single-layer transformer trained on the BB task. To explore this further, we train slightly deeper transformers on the BB task and track the residual state norm after layer 0. We observe that two-layer models do not exhibit residual-state peaks, while models with three or more layers do. Additional experimental results are provided in Appendix B.1 and B.2.

Massive residual state at layer 0 output induces attention sinks and value-state drains in the middle layer. To investigate the relationship between massive residual states and attention sinks, we train on the BB task using the "attn+mlp+attn+mlp+mlp" model, which is the minimal structure that shows the massive residual states phenomena. We perform intervention by analyzing how the model's behavior changes after zeroing out layer 0 (the first "attn+mlp" block). Before and after zeroing, we compute the difference in ∥Res ⟨s⟩ ∥ and Mean v [∥Res v ∥] at the layer 0 output, and compute logit •,⟨s⟩ and ∥Val ⟨s⟩ ∥ in the middle layer. After zeroing out, the residual state norm becomes non-massive, and attention logits and the value state norm return to a normal level. This confirms that the residual-state peak contributes to the attention sink and value-state-drain phenomena in the middle layer of pre-trained transformers.

Linear growth of residual-state norm with Adam training.  

## Extreme-token Phenomena in pretrained LLMs

In this section, we investigate extreme-token phenomena in open-source pretrained LLMs. In Section 3.1, we analyze the static behavior of these phenomena in Llama 2-7B-Base [(Touvron et al., 2023)](#b53), confirming the existence of the active-dormant mechanism in LLMs. Notably, we identify a specific head that is active on GitHub samples but dormant on Wikipedia samples. In Section 3.2, we examine the dynamic behavior of extreme-token phenomena during the pretraining of OLMo-7B [(Groeneveld et al., 2024)](#b21). We show that the attention logits, value states norm, and residual states norm of the sink token(s) in OLMo reflect behavior similar to that of the simpler BB model. Specifically, the simultaneous formation of attention sinks and value-state drains gives evidence for the mutual reinforcement mechanism.

## Active-dormant mechanism in LLMs

Our study of the BB model leads to the following prediction with respect to the extreme-token phenomena, which we hypothesize also applies to LLMs:

Attention heads are controlled by an active-dormant mechanism (cf. Claim 1). The presence of attention sinks and value-state drains indicates that an attention head is in a dormant phase.

This hypothesis suggests that in LLMs, whether an attention head becomes a sink depends on the context. Specifically, the attention head may become entirely irrelevant for selecting the next tokens in certain contexts or tasks, but not in others. When this irrelevance occurs, the attention head transitions into an attention sink. This hypothesis was confirmed in small transformers and the BB task, as demonstrated in Section 2. The total attention mass on extreme tokens ⟨s⟩ and "Delim"(⟨period⟩) at Layer 24, averaged across all attention heads. The horizontal axis is logarithmically scaled after step 10k. We observe a rapid increase followed by stabilization within the range [0.9, 1] for the rest of training, consistent with our predictions. Middle (b): The value state norms of each token at Layer 24 during training, averaged over all heads. The horizontal axis is logarithmically scaled after step 10k. Initially, the value states of all tokens shrink, eventually converging, while the value states of the extreme tokens shrink to significantly lower levels compared to other tokens. Accordingly, we aim to identify instances of attention heads in pretrained LLMs that exhibit this activedormant behavior, i.e., heads that are dormant in some domains but active in others. In Figure [8](#fig_6), we display a particular attention head-Layer 16 Head 25 (L16H25) of Llama 2-7B-Base [(Touvron et al., 2023)](#b53)-which demonstrates a clear active-dormant distinction across two distinct contexts (e.g., tokens from the GitHub subset versus the Wikipedia subset of RedPajama [(Computer, 2023)](#b9)). While many attention heads show similar context-dependent behavior (see Appendix D), we focus on this one because the conditions for its activation are straightforward and interpretable, whereas other heads may have more nuanced criteria.

Figure [8a](#fig_6) shows the attention maps of L16H25 on samples from both the GitHub and Wikipedia subsets of RedPajama. It demonstrates that L16H26 is dormant (i.e., an attention sink) on samples from Wikipedia, which resemble prose, and active (i.e., not an attention sink) on samples from GitHub, which resemble code. Additionally, Figure [8b](#fig_6) compares the loss difference when L16H25 is zeroed out for prompts from both domains. The results show that zeroing out this head significantly decreases model performance on GitHub sequences, while having minimal impact on Wikipedia sequences. This observation also confirms the head behaves as dormant in some contexts and active in others-in some contexts, removing this head has no effect on model performance, while in others, its removal causes significant performance drops.

## Extreme-token phenomena along training dynamics of LLMs

Our study of the BB model leads to the following prediction about the dynamical behavior of the extremetoken phenomena, which we hypothesize also applies to LLMs:

Attention heads undergo an attention-increasing and value-state-shrinking phase driven by the mutual reinforcement mechanism (cf. Claim 2). This is followed by a stable phase, where all non-trigger tokens have large, nearly identical attention logits on the extreme token. Simultaneously, the residual state norms of the extreme tokens increase linearly during pretraining.

We confirm these predictions below. To observe the training dynamics of a large-scale LLM, we use the setup of OLMo-7B-0424 [(Groeneveld et al., 2024)](#b21) (henceforth just referred to as OLMo), which provides open-sourced weights at various stages of their training.[foot_2](#foot_2) For our analysis, we inspect OLMo at multiple • Key Others ]. The horizontal axis is logarithmically scaled after step 10k. We observe that ∆logit •,⟨s⟩ increases approximately in logarithmic scale during training steps 10k to 100k, matching the decreasing phase of the value states in Figure [9b](#fig_7). Right (b): Attention logits of the last token's query state against all token's key states for pretrained OLMo. In this experiment, we generate 128 randomly sampled test tokens with IDs from 100 to 50000 in the OLMo tokenizer. We append each token separately to the test phrase "Summer is warm⟨period⟩ Winter is cold⟨period⟩", creating 128 different samples, which we feed to the LLM to examine the model behavior. We plot the distribution of (un-shifted) attention logits logit •,v = Qry ⊤ test Key v across all heads at Layer 24 and all test tokens. The distribution of logit •,⟨s⟩ and logit •,Delim have considerably small variance compared with other logits, confirming the sink-logits concentration phenomenon.

checkpoints: every 500 steps for the first 10,000 steps, then at 25,000 steps, 50,000 steps, and every 50,000 steps up to 449,000 steps (approximately the end of their training). [4](#foot_3) The input we use for this analysis is again "Summer is warm⟨period⟩ Winter is cold⟨period⟩"[foot_4](#foot_4) In this prompt, the "Delim" token, namely "⟨period⟩", also becomes a sink token along with ⟨s⟩. We believe this occurs because the period is not semantically meaningful and is not useful for predicting future tokens (cf. Appendix G.2) Figure [9](#fig_7) illustrates the dynamics of attention weights, value state norms, and the residual state norms for attention heads in Layer 24 of OLMo. The figure shows that the average attention on extreme tokens (⟨s⟩ and Delim) increases rapidly at the beginning of training before stablizing, while the value state norms of these extreme tokens decrease during training steps 10k-100k. The synchronized evolution of attention weights and value state norms aligns with the prediction of the mutual reinforcement mechanism. Additionally, the residual states of ⟨s⟩ increase linearly, while those of other tokens converge to a small number. Figure [10](#fig_9) provides a more detailed examination of the attention logits in Layer 24 of OLMo. Figure [10a](#fig_9) presents the dynamics of the difference in attention logits, showing that ∆logit •,⟨s⟩ increase during training steps 10k-100k, matching the decreasing phase of the value states. Figure [10b](#fig_9) also demonstrates the sink-logits concentration phenomenon. Specifically, it shows that the sink logits will eventually converge to a stable phase, in which logits corresponding to the key of the sink token and queries of all non-sink tokens are nearly identical. These findings coincide with the dynamical behavior predicted by the BB model, as outlined in Theorem 2(c) and corroborated by the experimental results in Figure [4](#fig_3).

## Conclusions

In this work, we investigated the extreme-token phenomena, specifically attention sinks, value-state drains, and residual-state peaks. We analyzed simple transformers trained on the Bigram-Backcopy (BB) task, both theoretically and empirically, demonstrating that these models exhibit the same extreme-token phenomena observed in large language models (LLMs). Building on the insights from the BB task, we made several detailed predictions about the behavior of extreme-token phenomena in LLMs. In particular, we identified the active-dormant mechanism governing attention heads in both the BB model and LLMs, with attention sinks and value-state drains serving as indicators of dormant phase, and a mutual reinforcement mechanism that induces these phenomena during pretraining. Using insights from these mechanisms, we applied simple modifications to the model architecture and optimization procedure, effectively mitigating the extremetoken phenomena in the BB model. Overall, our work uncovers the underlying mechanisms of extreme-token phenomena and suggests potential pathways to mitigate these issues during LLM pretraining.

We believe the most compelling direction for future work is to explore whether eliminating the extreme-token phenomena is essential or beneficial for building powerful transformer-based LLMs. While it is possible to mitigate these phenomena through simple modifications to the architecture or training algorithms, it remains unclear whether their elimination significantly improves downstream tasks such as inference and quantization. Given the resource-intensive nature of pretraining large-scale LLMs, we anticipate that pretraining a model at the scale of GPT-2 could both provide valuable insight into this issue and help point the way to architectures that can reduce the pretraining burden. 

## A Proofs of Theorem 1 and 2

We introduce new notations that are frequently used in the proofs. Recall that in Eq. ( [11](#formula_21)), we used {π v } v∈V to denote the stable distribution across all tokens. We further define the stable distribution excluding trigger tokens as follows:

$π ∈ R V , πi = π i 1{i ∈ V \ T }. (15$$)$Section 2.2 defines the bigram transition probability in the Bigram-Backcopy task as p vk = P(k | v). We further define the bigram transition probability matrix as

$P =    p 11 . . . p 1V . . . . . . . . . p V 1 . . . p V V    =    p ⊤ 1 . . . p ⊤ V    . (16$$)$Given a token v, define the predicted probability at token v as the logit output passed through the softmax activation. Let H = [⟨s⟩; v 1:n-1 ; v]. Using the form of TF(H) n defined in Eq. ( [9](#formula_18)), we denote

$l v = SoftMax(TF(H) n ) = (l v1 , . . . , l vV ), with l vi = p vi exp Miξi+e α βi e α +M V k=1 p vk exp M k ξ k +e α β k e α +M .(17)$Similar to Eq. ( [16](#formula_33)), we define the full output probability matrix as

$L =    l 11 . . . l 1V . . . . . . . . . l V 1 . . . l V V    =    l ⊤ 1 . . . l ⊤ V    . (18$$)$Using the notation l v and πv , we can rewrite the loss functions defined in Eq. ( [10](#formula_19)) and Eq. ( [11](#formula_21)) as follows:

$loss v (α v , β) = - V k=1 p vk log l vk , loss v (α, β) = V v=1 πv loss v (α v , β). (19$$)$We always have that k p vk = 1 and k l vk = 1. The total variation norm and KL-divergence are then defined as:

$∥p v -l v ∥ TV = k |p vk -l vk |, KL(p v || l v ) = - k p vk log(l vk /p vk ).(20)$Given any vector u = [u 1 ; . . . ; u d ], define the corresponding diagonal matrix as

$diag(u) =       u 1 0 . . . 0 . . . . . . . . . . . . . . . . . . 0 . . . 0 u d       .$Given any p v defined in Eq. ( [16](#formula_33)), denote

$G P v = diag(p v ) -p v p ⊤ v , G L v = diag(l v ) -l v l ⊤ v . (21$$)$We now present technical lemmas concerning

$G P v and G l v . Lemma A.1. The matrices G P v ∈ R V ×V and G L v ∈ R V ×V are positive semi-definite for any v ∈ V.$Proof of Lemma A.1. Since V k=1 p vk = 1 and V k=1 l vk = 1 for any v, we have that

$(G P v ) ii = p i -p 2 i = p i ( k̸ =i p k ) ≥ k̸ =i |(G P v ) ik |, (G L v ) ii = l i -l 2 i = l i ( k̸ =i l k ) ≥ k̸ =i |(G L v ) ik |.$This shows that both G P v and G L v are diagonally dominant matrices. By Corollary 6.2.27 in Horn and Johnson (2012), they are positive semi-definite.

Lemma A.2. Suppose that πv > 0 for any v ∈ V \ T . For any η ∈ R V with η ⊥ 1, there exists ω > 0 such that

$η ⊤ V k=1 πk G P k η ≥ ω∥η∥ 2 2 .$Proof of Lemma A.2. Denote the null spaces of G P v for v ∈ V as S v . We solve for each S v . Setting

$G P v η = 0 gives that [p vj -p vj ( k p vk )]η j = 0 for any j ∈ V.$If p vj ̸ = 0, we divide each side with p vj and get that η j = k p vk η k . As a result, we get that

$S v = {η | η j is constant for p vj ̸ = 0}.$Since all π k > 0, for any k ∈ V \ T , there is v ∈ V \ T such that p vk > 0, we get that

$∩ v∈V\T S v = {c • 1 | c ∈ R}. Since η ⊥ 1, we get that η ⊥ ∩ v∈V\T S v . We denote the minimal non-zero eigenvalues of G L v for v ∈ V \ T as λ. We get that η ⊤ V k=1 πk G P k η ≥ min v∈V\T πv λ∥η∥ 2 2 .$Setting ω = λ • min v∈V\T πv > 0, this proves Lemma A.2.

Lemma A.3. Given ω defined in Lemma A.2, suppose that

$max v,k |p vk -l vk | = δ ≤ min {ω/(6V ), 1}.(22)$For any η ∈ R V with η ⊥ 1, we have that

$η ⊤ V k=1 πk G L k η ≥ ω 2 ∥η∥ 2 2 . Proof of Lemma A.3. Denote δ = max v,k |p vk -l vk |. Suppose that δ ≤ 1. For any k ∈ V \ T , we can verify that (G P k ) ij -(G L k ) ij ≤ 3δ, for any i, j ∈ [V ]. We denote E = V k=1 πk G P k - V k=1 πk G L k . Therefore, |E ij | ≤ 3δ for any i, j ∈ [V ]. This means that η ⊤ Eη ≤ ∥E∥ 2 ∥η∥ 2 2 ≤ ∥E∥ F ∥η∥ 2 2 ≤ V • 3δ • ∥η∥ 2 2 .$As a result, when δ ≤ min {ω/(6V ), 1}, we get that

$η ⊤ V k=1 πk G L k η ≥ ω∥η∥ 2 2 -η ⊤ Eη ≥ ω 2 ∥η∥ 2 2 .$This proves Lemma A.3.

## A.1 Proof of Theorem 1

We denote the hidden dimension as d and the sequence length as N . Recall that the token v at position i is encoded as ebd i (v). We begin with the assumption regarding the transformer's embedding dimension:

$Assumption A. We have {ebd 0 (⟨s⟩)} ∪ {ebd i (v)} i∈{0}∪[N -1],v∈V ⊆ R d , where the embedding dimension d ≥ V N + 1.$Assumption A requires a large embedding dimension d ≥ V N + 1. This assumption is used to ensure that there are enough orthonormal bases in the embedding space. Given the fact that there are O(exp(d)) approximately linearly independent vectors for large d [(Vershynin, 2018)](#b55), it is possible to relax the assumption to be d ≫ log(V N ). However, since Assumption A pertains only to the construction of λ for trigger tokens and is unrelated to Theorem 2, we adopt it to simplify the proof of Theorem 1.

Theorem A.4 (Formal statement of Theorem 1). Let Assumption A hold. For any parameters

$(α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R)$, there exists a one-layer transformer (5) with weight matrices (Q, K, V, W 1 , W 2 ) such that Eq. ( [6](#formula_13)), ( [7](#formula_14)), and (8) hold. Consider the Bigram-Backcopy task, where given an input

$H = [⟨s⟩; v 1:n-1 , v],$the ground-truth transition gives

$P(v ′ | H) = p vv ′ for v ∈ V \ T , and P(v ′ | H) = 1{v ′ = v n-1 } for v ∈ T .$There exists a sequence min v∈V α v → ∞, min v∈V ξ v → ∞, λ → ∞, and β = 0 such that this transformer generates the ground-truth transition in the limit, i.e.,

$SoftMax(TF(H) n ) → P( • |H).(23)$Proof of Theorem A.4.

Step 1. Construction for the attention head. We let {ebd 0 (⟨s⟩

$)} ∪ {ebd i (v)} i∈{0}∪[N -1],v∈V ∪ {e v } v∈V to be a set of orthonormal basis in R d , and denote {η i } i∈{0}∪[N -1] ⊆ R d by a set of orthonormal basis in R d (the existence is guaranteed by Assumption A). Therefore, for any parameters (α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R), there exists a query matrix Q ∈ R d×N such that Q • ebd i (v) = λη i-1 for i > 1, v ∈ T , Q • ebd i (v) = α v η 0 for i > 0, v ∈ V \ T .(24)$Meanwhile, there is a key matrix K ∈ R d×N such that

$K • ebd i (v) = η i for i > 0, v ∈ V, K • ebd 0 (⟨s⟩) = η 0 .(25)$Denote {e v } v∈V as an orthonormal basis in R V . There is a matrix V ∈ R d×V such that

$V • ebd i (v) = ξ v e v ∈ R V , with ξ v = 0 for v ∈ T , and ξ v ≥ 0 for v ∈ V \ T . V • ebd 0 (⟨s⟩) = β ∈ R V .(26)$This construction matches Eq. ( [6](#formula_13)) and ( [7](#formula_14)).

As a result, for v n ∈ V \ T , by Eq. ( [5](#)), denoting H = [⟨s⟩; v 1:n-1 ; v n ] and attn(H) n to be the last column of attn(H), we have

$attn(H) n = n i=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd i (v i )]V • ebd i (v i ) n j=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd j (v j )] = exp[α vn η ⊤ 0 η 0 ] • β + n i=1 exp[α vn η ⊤ 0 η i ]ξ vi • e vi exp[α vn η ⊤ 0 η 0 ] + n j=1 exp[α vn η ⊤ 0 η j ] = e αv n e αv n + n • β + n i=1 1 e αv n + n • ξ vi e vi .$For v n ∈ T , we have

$attn(H) n = n i=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd i (v i )]V • ebd i (v i ) n j=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd j (v j )] = exp[λη ⊤ n-1 η 0 ] • β + n i=1 exp[λη ⊤ n-1 η i ]ξ vi • e vi exp[λη ⊤ n-1 η 0 ] + n j=1 exp[λη ⊤ n-1 η j ] = 1 e λ + n • β + i̸ =n-1 1 e λ + n$• ξ vi e vi + e λ e λ + n

• ξ vn-1 e vn-1 .

Step 2. Construction for the MLP layer. Further, define the weights for the mlp layer such that

$W 1 • ebd i (v) = e v ∈ R V , W 2 e v = log p v • 1{v ̸ ∈ T } ∈ R V for i ∈ [N ], v ∈ V,(27)$where {e v } is the eorthonormal basis in R V and p v ∈ R V is defined in Eq. ( [16](#formula_33)). As a result, mlp(H

$) n = W 2 ReLU(W 1 ebd n (v)) = W 2 e v = log p v • 1{v / ∈ T }.$This matches the Eq. ( [8](#formula_15)).

Step 3. The output of the transformer. By Eq. ( [5](#)) again, on non-trigger token v ∈ V \ T , the transformer output gives that

$TF(H) n = mlp(ebd n (v)) + attn(H) n = log p v + e αv n e αv n + n • β + n i=0 1 e αv n + n • ξ vi e vi .$On trigger token v ∈ T , the transformer output gives that

$TF(H) n = mlp(ebd n (v)) + attn(H) n = 1 e λ + n • β + i̸ =n-1 1 e λ + n$• ξ vi e vi + e λ e λ + n

• ξ vn-1 e vn-1 .

There exists a sequence min v∈V α v → ∞, min v∈V ξ v → ∞, λ → ∞, and β = 0, we get that

$SoftMax[TF(H) n ] → p vn for n > 0, v n ∈ V \ T , SoftMax[TF(H) n ] → (1{v = v n-1 }) v∈V for n > 0, v n ∈ T .$This proves Eq. ( [23](#formula_57)), indicating that the transformer output matches the ground truth transition. This finishes the proof of Theorem A.4.

## A.2 Proof of Theorem 2(c): Stable phase

We first state Lemma A.5 and Proposition A.6 that are used to prove Theorem 2(c). Lemma A.5 computes the gradients of l ik as defined in Eq. ( [17](#formula_35)).

Lemma A.5. Given l ik defined in Eq. ( [17](#formula_35)), for any i, k, v, and any value of α v and β v , we have that

$∂l ik ∂α v = 1{i = v}l ik e αi (e αi + M ) 2 M β k -M k ξ k - V j=1 l ij (M β j -M j ξ j ) , ∂l ik ∂β v = e αi e αi + M [l ik 1{k = v} -l ik l iv ].$Furthermore, we have Proof of Lemma A.5. We repeatedly use the following two facts:

$∂ exp M k ξ k +e α i β k e α i +M ∂α v = 1{i = v}e αi (M β k -M k ξ k ) (e αi + M ) 2 exp M k ξ k + e αi β k e αi + M , ∂ exp M k ξ k +e α i β k e α i +M ∂β v = 1{k = v}e αi e αi + M exp M k ξ k + e αi β k e αi + M .$When i ̸ = v, l ik has zero gradients with respect to α v . When i = v, we have that

$∂l vk ∂α v = l vk e αv M β k -M k ξ k (e αv + M ) 2 - l vk V i=1 p vi e αv M βi-Miξi (e αv +M ) 2 exp Miξi+e αv βi e αv +M V i=1 p vi exp Miξi+e αv βi e αv +M = e αv (e αv + M ) 2 l vk [M β k -M k ξ k ] -l vk V j=1 l vj (M β j -M j ξ j ) ,$and

$∂l ik ∂β v = e αi e αi + M l ik 1{k = v} - e α i e α i +M p iv exp Mvξv+e α i βv e α i +M p ik exp M k ξ k +e α i β k e α i +M V j=1 p ij exp Mj ξj +e α i βj e α i +M 2 = e αi e αi + M [l ik 1{k = v} -l ik l iv ].$We can verify that

$V k=1 ∂l ik ∂α v = e αv (e αv + M ) 2 V k=1 l vk [M β k -M k ξ k ] -l vk V j=1 l vj (M α j -M j ξ j ) = e αv (e αv + M ) 2 V k=1 l vk [M β k -M k ξ k ] - V j=1 l vj (M α j -M j ξ j ) = 0, and V v=1 ∂l ik ∂β v = e αi e αi + M V v=1 [l ik 1{k = v} -l ik l iv ] = e αi e αi + M [l ik -l ik ] = 0.$This finishes the proof of Lemma A.5.

Proposition A.6 computes the gradient of loss with respect to α and β, giving the ODE of the gradient flow.

Proposition A.6. Consider the gradient flow of optimizing loss(α, β) given by

$α(t) = -∇ α loss(α(t), β(t)), β(t) = -∇ β loss(α(t), β(t)). (28$$)$Simplifying the dynamics using Lemma A.5 gives that

$αv (t) = πv e αv (e αv + M ) 2 V i=1 (p vi -l vi )(M β i -M i ξ i ), βv (t) = V k=1 πk e α k [p kv -l kv ] e α k + M .$Proof of Proposition A.6. Taking the derivative of loss(α, β) gives that ∂loss(α, β)

$∂α v = πv V k=1 p vk • -1 l vi • ∂l vi ∂α v = πv e αv (e αv + M ) 2 V i=1 l vi [M β i -M i ξ i ] - V k=1 p vk [M β k -M k ξ k ] = πv e αv (e αv + M ) 2 V k=1 [l vk -p vk ][M β k -M k ξ k ] .$Similarly, we have that ∂loss(α, β)

$∂β v = V j=1 πj V k=1$p Plug them in Eq. ( [28](#formula_73)) proves Proposition A.6.

Theorem A.7 (Restatement the stable phase part in Theorem 2(c)). Assume ξ v ≥ 0 for any v, π v > 0 for any v ∈ V, and {M i • ξ i } i∈V are not all equal. Consider the gradient flow over the variables (α, β), i.e., ( α(t), β(t)) = -∇ α,β loss(α(t), β(t)). Any vector of the following form

$α ⋆ = α • 1, β ⋆ = c • 1 -e -α • M • ξ, α, c ∈ R(29)$is a stationary point. These are all global minimizers of loss(α, β).

Proof of Theorem A.7. When α = α ⋆ and β = β ⋆ , given l vi defined in Eq. ( [17](#formula_35)) with any v and i, we have that Plug l vi into ∂loss(α, β)/∂α and ∂loss(α, β)/∂β, we have

$l vi = p vi exp Miξi+e α βi e α +M V k=1 p vk exp M k ξ k +e α β k$$∂loss(α, β) ∂α v α ⋆ ,β ⋆ = πv e αv (e αv + M ) 2 V k=1 (l vk -p vk )[W β k -M k ξ k ] = 0, ∂loss(α, β) ∂β v α ⋆ ,β ⋆ = V k=1 πk e α k [l kv -p kv ] e α k + M = 0.$This shows that α = α ⋆ and β = β ⋆ are stationary points. We further compute the second-order derivative using Lemma A.5. To simplify the notation, we use

$z k = W β k -M k ξ k and z = [z 1 , . . . , z V ]. We have that ∂ 2 loss(α, β) ∂α i ∂α v α ⋆ ,β ⋆ = 1{v = i} • πv e α (e α + M ) 2 V k=1 ∂l ik ∂α v z k = 1{v = i} • πv e 2α (e α + M ) 4 V k=1 l ik z 2 k - V k=1 l ik z k 2 = 1{v = i} • πv e 2α (e α + M ) 4 V k=1 p ik z 2 k - V k=1 p ik z k 2 ,$where in the last line, we plugged in l vi = p vi for any v and i. Similarly, we compute the second order derivatives with respect to α i and β v ,

$∂ 2 loss(α, β) ∂α i ∂β v α ⋆ ,β ⋆ = πi e α (e α + M ) 2 V k=1 ∂l ik ∂β v z k = πi e 2α (e α + M ) 3 p iv z k -p iv V k=1 p ik z k .$With the same manner, we compute the second order derivatives with respect to β i and β v ,

$∂ 2 loss(α, β) ∂β i ∂β v α ⋆ ,β ⋆ = V k=1 ∂l ki ∂β v πk e α e α + M = e 2α (e α + M ) 2 V k=1 {π k [1{v = i}p kv -p ki p kv ]}.$Combining the above computations gives that

$Hessian(loss(α ⋆ , β ⋆ )) = ∇ 2 α loss(α, β) ∇ α ∇ β loss(α, β) ∇ β ∇ α loss(α, β) ∇ 2 α loss(α, β) , with ∇ 2 α loss(α, β) = e 2α (e α + M ) 4 diag π • [z ⊤ G P 1 z; . . . ; G P V z] , ∇ α ∇ β loss(α, β) = e 2α (e α + M ) 3 diag π [z ⊤ G P 1 ; . . . ; z ⊤ G P V ], ∇ 2 β loss(α, β) = e 2α (e α + M ) 2 V k=1 πk G P k ,$where G P k is defined in Eq. ( [21](#formula_42)). Furthermore, there exists U such that U Hessian(loss(α

$⋆ , β ⋆ ))U ⊤ = Diag-Hessian(loss(α ⋆ , β ⋆ )), with Diag-Hessian(loss(α ⋆ , β ⋆ )) = ∇ 2 α loss(α, β) 0 0 e 2α (e α +M ) 2 B$, where the B is given by

$B = V k=1 πk G P k -(z ⊤ G P k z) -1 G P k zz ⊤ G P k .$To prove that B is positive semi-definite, consider any vector η with ∥η∥ 2 = 1:

$η ⊤ Bη = V k=1 πk η ⊤ G P k η - η ⊤ G P k zz ⊤ G P k η z ⊤ G P k z .$Since G P k is positive semi-definite, the Cauchy inequality gives that

$z ⊤ G P k η ≤ z ⊤ G P k zη ⊤ G P k η.$As a result, we have that

$η ⊤ Bη ≥ V k=1 πk η ⊤ G P k η - z ⊤ G P k zη ⊤ G P k η z ⊤ G P k z = 0.$This shows that B is positive semi-definite. Therefore, Hessian(loss(α ⋆ , β ⋆ )) is positive semi-definte. This proves Theorem A.7.

A.3 Proof of Theorem 2(a): Attention sinks Theorem A.8 (Restatement of the attention sink part in Theorem 2(a)). Assume ξ v ≥ 0 for any v, π v > 0 for any v ∈ V, and {M i • ξ i } i∈V are not all equal. Fix β = β • 1 for a constant β, and consider the gradient flow of the loss function loss(α, β) over α, i.e., α(t) = -∇loss(α(t), β). With any initial value α(0), there exists r(t) with norm uniformly bounded in time, such that

$α(t) = 1 2 log t • 1 + r(t). (30$$)$Proof of Theorem A.8. We separately analyze each entry of α. Focusing on α v , to simplify the notation, we introduce a random variable φ such that

$P(φ = M k ξ k ) = p vk . Denote u = e αv .$Therefore, using Lemma A.6, we get that

$du dt = πv e 2αv (e αv + M ) 2 V i=1 (p vi -l vi )(M β i -M i ξ i ).$We take in β = c • 1 and expand the expression of du/dt. This gives us that

$du dt = πv u 2 (u + M ) 2 V k=1 p vk e M k ξ k /(u+M ) M k ξ k - V k=1 p vk e M k ξ k /(u+M ) V k=1 p vk M k ξ k V k=1 p vk e M k ξ k /(u+M ) = πv u 2 (u + M ) 2$Cov(e φ u+M , φ) Ee φ u+M .

Since both e x/(u+M ) and x are monotonically increasing with respect to x, du/dt ≥ 0. Therefore, u is monotonically increasing, and we have that

$u(t) 2 [u(t) + M ] 2 ≥ u(0) 2 [u(0) + M ] 2 , Ee φ u(t)+M ≤ Ee φ u(0)+M .$Meanwhile, the first and second order Taylor expansions of e φ/ (u+M ) give that

$e φ u+M = 1 + θ 1 (φ)φ u + M , e φ u+M = 1 + φ u + M + θ 2 (φ) φ u + M 2 ,$where both θ 1 (φ) and θ 2 (φ)φ 2 are monotonically increasing functions of φ. We also have the bound that

$θ(φ) ≤ exp max k M k ξ k u(0) + M -1 / max k M k ξ k u(0) + M -1 = C θ .$We choose δ as defined in Eq.( [22](#formula_50)). When t is sufficiently large, we have that

$loss(α, β(t)) ≤ loss(α, β ⋆ ) + 1 min k∈V\T πk • 2δ 2 .$The convexity further implies that for any β = θβ(t) + (1 -θ)β ⋆ (θ ∈ (0, 1)), we have that

$loss(α, β) ≤ loss(α, β ⋆ ) + 1 min k∈V\T πk • 2δ 2 .$Denote lv = l v (α, β) as l evaluated on (α, β). Using the definition of the KL-divergence in Eq. ( [20](#formula_40)), we have that

$V v=1 πv KL(p v || lv ) = loss(α, β(t)) -loss(α, β ⋆ ) ≤ 1 min k∈V\T πk • 2δ 2 .$This further implies that KL(p v || lv ) ≤ 2δ 2 for any v. Using Pinsker's inequality, we get that

$V k=1 |p vk -lvk | = ∥p v -l v ∥ TV ≤ KL(p v || lv )/2 ≤ δ. Therefore, max v,k p vk -lvk ≤ δ. Lemma A.5 gives that V v=1 βv (t) = 0. Therefore, V v=1 β v (t)/V = β(0). The choice of β ⋆ guarantees that β ⋆ = β(0). This shows that β(t) -β ⋆ ⊥ 1. Using Lemma A.3, there exists ω > 0 such that (β(t) -β ⋆ ) ⊤ ∇ 2 β loss(α, β)(β(t) -β ⋆ ) = (β(t) -β ⋆ ) ⊤ V k=1 πk G L k (β(t) -β ⋆ ) ≥ ω 2 ∥β(t) -β ⋆ ∥ 2 2 .$Using Taylor expansion, we have that

$loss(α, β ⋆ ) -loss(α, β(t)) = -∇ β loss(α, β(t))(β(t) -β ⋆ ) + 1 2 (β(t) -β ⋆ ) ⊤ ∇ 2 β loss(α, β)(β(t) -β ⋆ ) ≥ -∇ β loss(α, β(t))(β(t) -β ⋆ ) + ω 2 ∥β(t) -β ⋆ ∥ 2 2 ≥ - 1 2ω ∥∇ β loss(α, β(t))∥ 2 2 .$This shows that loss(α, β(t)) satisfies the Polyak-Lojasiewicz (PL) condition [(Karimi et al., 2016)](#b32) when t is sufficiently large. This proves Theorem A.9.

## B The Linear Growth of the Residual States

B.1 The minimal model structure to recapitulate residual state peak We give more details for the claim in Section 2.3, stating that "The residual-state peaks require a threelayer structure." Figure 11 presents the difference of residual norms between the ⟨s⟩ token and others (∥Res ⟨s⟩ ∥ -E v̸ =⟨s⟩ [∥Res v ∥]), with different combinations of model structures. The 3 × TF and 2 × TF + mlp are the architectures that demonstrate clear evidence of residual state peaks. 2 × TF 3 × TF 2 × TF+MLP MLP+TF+MLP Attn+TF+MLP 0 40 100 140 Res s E v s [ Res v ]  s , t ,\nTH t HE t ENo ? t s , t , \n T H t H E t E N o ? t 0 1 (b) Layer 1 s , t ,\nTH t HE t ENo ? t s , t , \n T H t H E t E N o ? t 0 1 (c) Layer 2 s , t ,\nTH t HE t ENo ? t s , t , \n T H t H E t E N o ? t 0 1  2. The layer-norm operations cause the fast decay of the magnitude of the gradients.

3. Adam induces diminishing gradients to be constant updates, leading to the linear growth for the norm of the residual state of the extreme token.

To support the claim, we use the simplified model in Section 2, including the residual state norm. Denote the layer-norm operation as LayerNorm. Heuristically, we can split the residual state Res ⟨s⟩ to a summation of two directions.

$Res ⟨s⟩ = m • η + ε,$where η, ε ∈ R V with ∥η∥ 2 = ∥ε∥ 2 = 1, and η ⊤ ε = ρ > 0. The η corresponds to the direction of Key ⟨s⟩ in the original transformer, and ε corresponds to other directions. Assume that the attention logit from the token v to the ⟨s⟩ token in layer 1 is given by logit

$v,⟨s⟩ = α v = αv η ⊤ LayerNorm(Res ⟨s⟩ ) = αv • m + ρ m 2 + 2mρ + 1 . (32$$)$We assume that the scalars m and α are trainable, quantifying the norm of the residual states and magnitude of attention sinks. In the loss function loss v as defined in Eq. ( [10](#formula_19)), we replace α v by the expression as in Eq. ( [32](#formula_104)), so that the loss function becomes a function of (α v , β, m), denoted as

$loss v ( αv , β, m) = loss v (α v , β),$We then consider the total loss as the average of the losses on each non-trigger token, weighted by its proportion in the stable distribution {π v } v∈V , given by

$loss( α, β, m) = v∈V\T π v • loss v ( αv , β, m).(33)$Proposition B.1. Assume ξ v ≥ 0 for any v, {W k β k } k∈V are not all equal, and ρ > 0. Fix β = 0, and consider the gradient flow of loss( α, β, m) over α and m. With any initial value αv (0) > 0 for any v and m(0) > 0, we have that

$ṁ(t) = O log t √ tm 3 .$Proof of Proposition B.1. The chain rule gives that

$αv (t) = αv • m + ρ m 2 + 2mρ + 1 ,and$$ṁ(t) = V v=1 αv αv • dLayerNorm(Res ⟨s⟩ ) dt .$With the initial values, ṁ(t) ≥ 0 and αv (t) ≥ 0. We have m(t) ≥ 0 for any t. Hence,

$αv ∈ [ρ αv , αv ].$Therefore, α = 2 -1 log t1 + r(t) with r(t) uniformly bounded over time. Furthermore, we have that

$ṁ(t) = V v=1 αv αv • dLayerNorm(Res ⟨s⟩ ) dt = O log t √ t • 1 -ρ 2 (m 2 + 2mρ + 1) 3/2 = O log t √ tm 3 .$This proves Proposition B.1.

We use simulation to demonstrate the effect of Adam. We train the scalar m using Adam with gradient dm = log t/[ √ tm 3 ]. We set β 1 = 0.9, β 2 = 0.999, weight decay= 10 -8 , and the learning rate lr = 0.3. Figure [15](#fig_18) presents the training dynamics of m. We observe the linear growth after a warming-up phase. In contrast, when trained by SGD with learning rate lr = 0.3, m remains small. The results match transformer models on BB-task as in Figure [7c](#fig_4). 

## C Ablations

## C.1 Experimental details

We provide more details for experiments in Section 2. We train transformers with positional embedding, prelayer norm, SoftMax activation in attn, and ReLU activation in mlp. We use Adam with constant learning rate 0.0003, β 1 = 0.9, β 2 = 0.99, ε = 10 -8 , and a weight decay of 0.01. We choose a learning rate of 0.03 for the SGD. In each training step, we resample from the BB task with a batch size of B = 512 and sequence length N = 256. Unless otherwise specified, the model is trained for 10, 000 steps. Results are consistent across different random seeds.

## C.2 Additional attention plots of a 1-layer transformer trained on the BB task

We provide more attention plots of the 1-layer transformer on sequences other than those shown in Figure [2b](#fig_2).

Figure [16](#fig_20) presents more attention-weight heat maps of the one-layer transformer model trained on the BB task. All attention maps show the attention sink phenomenon. Some non-trigger tokens present attention patterns other than attention sink. For example, trigger tokens serve as attention sinks in some inputs in Figure [16c](#fig_20). s , t , \n T H t H E t E N o ? t s , t , \n T H t H E t E N o ? t 0 1 (b) Sequence 1 s f u t u s . \n\n H a d y , t , s f u t u s . \n \n H a d y , t , 0 1 (c) Sequence 2 s i s t s ? t ? t ? \n T I V I f s i s t s ? t ? t ? \n T I V I f 0 1 C.4 The Bigram-Backcopy task without the ⟨s⟩ token.

We train a one-layer transformer on the BB task without the ⟨s⟩ token. Figure [19](#fig_22) shows that the zeroth token is not a sink token. Instead, trigger tokens and delimiter tokens seem to become sink tokens. In particular, the observation that delimiter tokens become extreme matches the observation in LLMs that delimiter tokens may also become extreme tokens (cf. Appendix G.2). , t ,\nTH t HE t ENo ? t ? , t , \n T H t H E t E N o ? t ? 0 1 (b) Value state norms , t , \n T H t H E t Tokens 0 5 10 15 20 Norms of Value States    the alignment of different states is caused purely by the token's global importance or meaning imparted via pretraining. The ⟨s⟩ token has no semantic meaning in the context of prose tokens, so its key state is not aligned with key states of meaningful prose tokens. Also, delimiter tokens, often considered secondary attention sinks (cf. Appendix G.2), have the most aligned key states to the key state of the ⟨s⟩ token, and are also the tokens with the least semantic meaning in the prose context. Thus, we identify that, at least in this restricted example, query state and key state alignment depends heavily on the contextual semantics of the token. We perform a series of ablations to understand which components of the network promote the residual state peaks. We find that ablating either the zeroth or first layer's MLP is sufficient to remove the residual state peak phenomenon, while no other layer-level ablation can do it.

Value-state drains. The value states of the ⟨s⟩ token at Layer 0 Head 31 are already near zero, as demonstrated in Figure [24a](#fig_26). While the delimiter tokens, which are less semantically meaningful in the prose context, have smaller value states than the rest, they are not as small as the value state of the ⟨s⟩ token which is guaranteed to not have any semantics.

Residual state peaks. Residual state peaks are caused by the first two layers' MLPs. In particular, we perform several ablations, comparing between the residual state norms in a later layer (24) of an un-edited forward pass versus forward passes where we force the output of either multiple layers, a single layer, an attention block, or an MLP to be zero (and hence remove its contribution from the residual stream). As shown in Figure [24b](#fig_26), ablating either Layer 0's or Layer 1's MLP is sufficient to remove the residual state peak. In particular, the second-largest token at Layer 24 in each ablation (including the original setup) has norm between 29 and 38, so the interventions ensure that all tokens have similar size.

![Figure 1: Extreme-token phenomena in Llama 3.1. We evaluate the attention weights, value states norm, and residual states norm on the Llama 3.1-8B-Base model, where the input sentence is "⟨s⟩Summer is warm⟨period⟩ Winter is cold⟨period⟩". Left (a): The attention weights across multiple heads at Layer 24. We observe the attention sink phenomenon: the ⟨s⟩ token attracts a significant portion of the overall attention weight. Middle (b):The empirical distribution of the norms of value states over all layers and all heads. We exclude 2% of the outlier values to help visualization. We observe the value-state drain phenomenon: the value state of the ⟨s⟩ token is much smaller than those of other tokens on average. Right (c): The norm of the residual stream states, measured at the output of each layer. We observe the residual-state peak phenomenon: the ⟨s⟩ token's residual states have significantly larger norms than those of other tokens from layers 1 to 30. We present the extreme-token phenomena over other input sequences in Appendix F.]()

![Figure 2: Experiments on the Bigram-Backcopy task. Left (a): The data generation procedure for the Bigram-Backcopy task. Here we fix 't', 'e', and the space character (' ') as trigger tokens. The BB task samples bigram transitions for non-trigger tokens and backcopies for trigger tokens. Middle (b): The attention map of a given prompt. Trigger tokens are marked in red. The attention head at non-trigger tokens is dormant and displays attention sinks. Right (c): The value state norms for the prompt. The ⟨s⟩ token has the smallest norm.]()

![Figure 4: Interventions and dynamics of one-layer transformer on the Bigram-Backcopy task. Left (a): Excess risks for a one-layer model trained on the Bigram-Backcopy (BB) task under various interventions. Right (b):The excess risks, attention weights, attention logits, and value state norms for the ⟨s⟩ token throughout the training dynamics. Each curve is rescaled to fall within a 0 to 1 range. On the right side of (b), the horizontal axis is logarithmically scaled. The ∆logit •,⟨s⟩ curve represents the mean of attention logits from all given non-trigger query tokens v on the ⟨s⟩ token, normalized by the mean of attention logits for other tokens. The shaded area represents the 90% uncertainty interval on the distribution over all non-trigger tokens.]()

![Figure 7: Left (a): The training dynamics of the single-layer ReLU attention transformer on the BB task. Middle (b): The intervention results on the attn+mlp+attn+mlp+mlp architecture. The attention sink and value-state peak of the middle attn layer disappear after zeroing out attn + mlp of layer 0. Right (c): The evolution of massive norms in a three-layer transformer trained with Adam, SGD, and using a ReLU attention transformer. Notably, only the three-layer model with Softax attention trained using Adam results in the formation of residual-state peaks.]()

![Figure7cshows the residual-state norms of the ⟨s⟩ token at the layer 0 output of three-layer transformers during pre-training on the BB task. The results indicate that training the transformer with Adam leads to a linear increase in residual state norms.]()

![Figure 8: Active-dormant mechanism of Layer 16 Head 25 (L16H25) of Llama 2-7B-Base. We observe that L16H25 is active on GitHub data and dormant on Wikipedia data, both sourced from RedPajama-1T (Computer, 2023). Left (a): Attention weights of L16H25, prompted by three randomly selected samples from each domain. Right (b): Results of an intervention study showing the change in cross-entropy loss when the output of L16H25 (specifically, its value states) is set to zero across sequences in both domains. The findings indicate that the model's performance for GitHub data, measured by cross-entropy loss, strongly relies on the output of this attention head.]()

![Figure 9: Attention weights, value state norms, and residual state norms of Layer 24 during the training dynamics of OLMo. Left (a):The total attention mass on extreme tokens ⟨s⟩ and "Delim"(⟨period⟩) at Layer 24, averaged across all attention heads. The horizontal axis is logarithmically scaled after step 10k. We observe a rapid increase followed by stabilization within the range [0.9, 1] for the rest of training, consistent with our predictions. Middle (b): The value state norms of each token at Layer 24 during training, averaged over all heads. The horizontal axis is logarithmically scaled after step 10k. Initially, the value states of all tokens shrink, eventually converging, while the value states of the extreme tokens shrink to significantly lower levels compared to other tokens. Figure (a) and (b) coincide with the trends in Figure 4b under the BB task. Right (c): The residual state norms of each token at Layer 24 during training. The residual state norm of ⟨s⟩ increases linearly in magnitude throughout training, matching Figure 7c in the BB task.]()

![Figure (a) and (b) coincide with the trends in Figure 4b under the BB task. Right (c): The residual state norms of each token at Layer 24 during training. The residual state norm of ⟨s⟩ increases linearly in magnitude throughout training, matching Figure 7c in the BB task.]()

![Figure 10: Attention logits of Layer 24. Left (a): Attention logits difference of all tokens' query states against ⟨s⟩'s key state during training. The difference in attention logits is computed as ∆logit •,⟨s⟩ = Qry ⊤ • Key ⟨s⟩ -Mean[Qry ⊤• Key Others ]. The horizontal axis is logarithmically scaled after step 10k. We observe that ∆logit •,⟨s⟩ increases approximately in logarithmic scale during training steps 10k to 100k, matching the decreasing phase of the value states in Figure9b. Right (b): Attention logits of the last token's query state against all token's key states for pretrained OLMo. In this experiment, we generate 128 randomly sampled test tokens with IDs from 100 to 50000 in the OLMo tokenizer. We append each token separately to the test phrase "Summer is warm⟨period⟩ Winter is cold⟨period⟩", creating 128 different samples, which we feed to the LLM to examine the model behavior. We plot the distribution of (un-shifted) attention logits logit •,v = Qry ⊤ test Key v across all heads at Layer 24 and all test tokens. The distribution of logit •,⟨s⟩ and logit •,Delim have considerably small variance compared with other logits, confirming the sink-logits concentration phenomenon.]()

![work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Preliminaries and notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Extreme-token Phenomena in the Bigram-Backcopy Task 5 2.1 One-layer transformer exhibits attention sinks and value-state drains . . . . . . . . . . . . . . 5 2.2 Analysis of a minimally-sufficient transformer architecture . . . . . . . . . . . . . . . . . . . . 7 2.3 The emergence of residual-state peaks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3 Extreme-token Phenomena in pretrained LLMs 12 3.1 Active-dormant mechanism in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Extreme-token phenomena along training dynamics of LLMs . . . . . . . . . . . . . . . . . . 13 4 Conclusions 14 A Proofs of Theorem 1 and 2 21 A.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.2 Proof of Theorem 2(c): Stable phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.3 Proof of Theorem 2(a): Attention sinks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 A.4 Proof of Theorem 2(b): Value-state drains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B The Linear Growth of the Residual States 31 B.1 The minimal model structure to recapitulate residual state peak . . . . . . . . . . . . . . . . 31 B.2 Additional plots for the three-layer transformer trained on BB task . . . . . . . . . . . . . . . 31 B.3 Potential mechanism for linear growth of the residual state peak in multi-layer models . . . . 31 C Ablations 34 C.1 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 C.2 Additional attention plots of a 1-layer transformer trained on the BB task . . . . . . . . . . . 34 C.3 Statics and dynamics of the simplified model in Theorem 2 . . . . . . . . . . . . . . . . . . . 34 C.4 The Bigram-Backcopy task without the ⟨s⟩ token. . . . . . . . . . . . . . . . . . . . . . . . . 34 D More Attention Heads in Dormant and Active Phase 36 E Fine-Grained Static Mechanisms for Extreme-Token Phenomena 37 F Extreme-Token Phenomena Over Many Samples 39 G Assorted Caveats 41 G.1 Multiple attention sinks vs. one attention sink . . . . . . . . . . . . . . . . . . . . . . . . . . 41 G.2 The role of a fixed ⟨s⟩ token in the Active-Dormant mechanism . . . . . . . . . . . . . . . . 41]()

![∂α v= 0 for any i, v, α, and β,V v=1∂l ik ∂β v = 0 for any i, k, α, and β.]()

![jk e αj l jv e αj + M -e αj 1{k = v} e αj + M = V j=1 πj e αj [l jv -p jv ] e αj + M .]()

![vi exp e α c e α +M V k=1 p vk exp e α c e α +M = p vi .]()

![Figure 11: Minimal structures to elicit residual state peaks. We use A + B + C to indicate the model with structure A, B, C in layers 0, 1, and 2, respectively.]()

![Figure 12: Attention weight patterns of three-layer transformer trained on the BB task]()

![Figure 13: Value state norms of three-layer transformer trained on the BB task (a) Layer 0]()

![Figure 15: With the gradient formula in Proposition B.1, Adam causes linear growth of m.]()

![Figure 16: Additional attention plots of the one-layer transformer trained on the Bigram-Backcopy task.]()

![Figure 17: The simplified model structure trained on the BB task.]()

![Figure 19: Attention weights and value state norms of a one-layer transformer trained on the BB task without the ⟨s⟩ token.]()

![Figure 20: Layer 16 Head 20 of Llama 2-7B-Base. We do not observe difference between the Wikipedia data and the Github data.]()

![Figure 21: Layer 16 Head 28 of Llama 2-7B-Base. The head is more dormant on the GitHub data, and more active on the Wikipedia data.]()

![Figure24: Left (a): Value-state drains at Layer 0 Head 31 of Llama 3.1-8B-Base. We observe that the value state associated with ⟨s⟩ is already much smaller than every other semantically meaningful token, and still smaller than the delimiter tokens in the same sentence. Right (b): Ablation study on the cause of the residual state peak in Llama 3.1-8B-Base. We perform a series of ablations to understand which components of the network promote the residual state peaks. We find that ablating either the zeroth or first layer's MLP is sufficient to remove the residual state peak phenomenon, while no other layer-level ablation can do it.]()

![]()

We define the value state as OVH rather than VH since what is added to the residual state is essentially OVH.

We note that Reddy (2023) makes a similar simplification in analyzing induction heads.

We did not analyze Llama for dynamics, as they do not provide open-source intermediate checkpoints along pretraining.

For the single 150,000-step checkpoint, we observed that its statistics were outliers, which we hypothesize is due to a system failure. We address this by using the average of nearby checkpoints to represent its statistics.

Note that OLMo does not have a ⟨s⟩ token, but attention sinks still form in the majority of heads. In particular, the first token always behaves as an attention sink. We discuss this further in Appendix G.2.

