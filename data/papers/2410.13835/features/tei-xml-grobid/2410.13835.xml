<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</title>
				<funder ref="#_82wVNRW">
					<orgName type="full">Amazon Research Award</orgName>
				</funder>
				<funder ref="#_5tj63QY #_epJ3Tph">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_wxTj7b7">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_JkJ8q58">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-08">November 8, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
							<email>tianyu_guo@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Druv</forename><surname>Pai</surname></persName>
							<email>druvpai@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
							<email>jiantao@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>michael_jordan@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
							<email>songmei@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-08">November 8, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">C15A0EB6EFA193E08B2BFB347EA1D9E2</idno>
					<idno type="arXiv">arXiv:2410.13835v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called "sink tokens" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.</p><p>We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures-transformers with one to three layers-trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.</p><p>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent analyses of transformer-based open-source large language models (LLMs), such as GPT-2 <ref type="bibr" target="#b45">(Radford et al., 2019)</ref>, Llama-2 <ref type="bibr" target="#b53">(Touvron et al., 2023)</ref>, Llama-3 <ref type="bibr" target="#b14">(Dubey et al., 2024)</ref>, <ref type="bibr">Mixtral (Jiang et al., 2023)</ref>, and Pythia <ref type="bibr" target="#b3">(Biderman et al., 2023)</ref>, have revealed three intriguing phenomena:</p><p>-Attention sinks <ref type="bibr">(Xiao et al., 2023b)</ref>: In many attention heads, the initial token consistently attracts a large portion of the attention weights. Other special tokens, such as the delimiter token, can also draw significant attention weights. These tokens are collectively referred to as sink tokens.</p><p>-Value-state drains <ref type="bibr" target="#b24">(Guo et al., 2024)</ref>: For the attention heads that exhibit attention sinks, the value states of sink tokens are consistently much smaller than those of other tokens.</p><p>-Residual-state peaks <ref type="bibr" target="#b49">(Sun et al., 2024)</ref>: The residual states of sink tokens, excluding those from the first and last layers, exhibit significantly larger norms compared to other tokens.</p><p>These phenomena often appear together and consistently occur in various pretrained LLMs, which we collectively refer to as the extreme-token phenomena. Figure <ref type="figure" target="#fig_1">1</ref> illustrates these phenomena in Llama-3.1-8B-Base, s Summer is warm . Winter is cold . Head 0 Head 10 s Summer is warm . Winter is cold . s Summer is warm . Winter is cold . Head 20 s Summer is warm . Winter is cold . The empirical distribution of the norms of value states over all layers and all heads. We exclude 2% of the outlier values to help visualization. We observe the value-state drain phenomenon: the value state of the ⟨s⟩ token is much smaller than those of other tokens on average. Right (c): The norm of the residual stream states, measured at the output of each layer. We observe the residual-state peak phenomenon: the ⟨s⟩ token's residual states have significantly larger norms than those of other tokens from layers 1 to 30. We present the extreme-token phenomena over other input sequences in Appendix F.</p><p>using a fixed prompt sentence: "⟨s⟩Summer is warm⟨period⟩ Winter is cold⟨period⟩". Here, the first token, ⟨s⟩ (the Beginning-of-Sequence token), serves as the sink token. As shown in the figure, the sink token receives disproportionately high attention weights, exhibits significantly smaller value states, and has much larger residual state norms compared to other tokens. It is important to note that the first token does not have to be ⟨s⟩ to act as a sink token; other tokens appearing first in the sequence can also serve this role. Additionally, in models such as Llama-2, a delimiter token can also function as the sink token.</p><p>The extreme-token phenomena have posed several challenges for pretrained transformers in downstream tasks. For instance, sink tokens require special treatment during long-context inference <ref type="bibr">(Xiao et al., 2023b;</ref><ref type="bibr" target="#b26">Han et al., 2023;</ref><ref type="bibr" target="#b63">Yu et al., 2024;</ref><ref type="bibr" target="#b8">Chen et al., 2024)</ref> and model quantization <ref type="bibr" target="#b13">(Dettmers et al., 2022;</ref><ref type="bibr" target="#b38">Liu et al., 2024;</ref><ref type="bibr" target="#b48">Son et al., 2024)</ref> to maintain high levels of performance. Additionally, attention sinks have reduced the interpretability of attention maps in vision transformers <ref type="bibr" target="#b10">(Darcet et al., 2023)</ref>. To address these issues, <ref type="bibr" target="#b49">Sun et al. (2024)</ref> and <ref type="bibr" target="#b10">Darcet et al. (2023)</ref> propose adding a "special token" to transformers to serve as the sink token, preventing other tokens from becoming sinks. However, even this special token still exhibits extremetoken phenomena. Despite these efforts, no prior work has satisfiably explained the mechanisms behind the extreme-token phenomena. <ref type="bibr">Xiao et al. (2023b)</ref> proposes a hypothesis for why they occur, suggesting that models tend to dump unnecessary attention values to specific tokens.</p><p>This work aims to demystify the extreme-token phenomena in LLMs. We demonstrate that these phenomena arise from an active-dormant mechanism in attention heads (cf. Claim 1), coupled with a mutualreinforcement mechanism during pretraining (cf. Claim 2). We support these statements through studies on simplified transformer architectures and tasks, a dynamical theory for these models, and experiments on pretrained LLMs. The structure of the paper and our key contributions are outlined as follows:</p><p>1. In Section 2, we train one-to three-layer transformers on a simple task called the Bigram-Backcopy (BB) task, which also displays extreme-token phenomena similar to those observed in LLMs. We show that attention sinks and value-state drains are a consequence of the active-dormant mechanism (cf. Claim 1). Both theoretically and empirically, we demonstrate that mutual reinforcement mechanism (cf. Claim 2) dynamically drives these phenomena: attention sinks and value-state drains reinforce one another, leading to a stable phase where all query tokens generate near identical attention logits for the keys of extreme tokens. Additionally, empirical results reveal that residual-state peaks arise from the interaction between this mutual reinforcement mechanism and the Adam optimization algorithm.</p><p>BB-task Theory BB-task Experiments LLM Experiments ∆logit •,⟨s⟩ log-growth</p><formula xml:id="formula_0">✓ ✓ ⋆ ∥Val ⟨s⟩ ∥ monotonic decrease ✓ ✓ ✓ ∥Res ⟨s⟩ ∥ linear growth ⋆ ✓ ✓ logit •,⟨s⟩ concentration ✓ ✓ ✓</formula><p>Table <ref type="table">1</ref>: Consistency of the quantitative properties across the theoretical and empirical results of the Bigram-Backcopy task and empirical results of LLMs. A ✓ denotes a consistent result, while a ⋆ denotes an inconclusive result. The logit •,⟨s⟩ denotes logits corresponding to the key of the extreme token and queries of all non-extreme tokens, i.e., the Qry ⊤ • Key ⟨s⟩ . The ∆logit •,⟨s⟩ = logit •,⟨s⟩ -Mean[logit •,others ] is a progress measure for attention sinks. The ∥Val ⟨s⟩ ∥ denotes the value state norm of the extreme token, and ∥Res ⟨s⟩ ∥ denotes the residual state norm of the extreme token. See Section 1.2 for the definitions of these notations.</p><p>2. In Section 3, we demonstrate the active-dormant mechanism in pre-trained LLMs by showing that many attention heads transition between active and dormant phases based on the input domain. Specifically, we identify an interpretable active-dormant head (Layer 16, Head 25 in Llama 2-7B-Base <ref type="bibr" target="#b53">(Touvron et al., 2023)</ref>) that activates on GitHub data but remains dormant on Wikipedia data. Moreover, in examining the dynamics of OLMo-7B-0424 <ref type="bibr" target="#b21">(Groeneveld et al., 2024)</ref>, we observe the same mutual reinforcement mechanism and stable phase, consistent with those found in the BB task. This demonstrates that the simple BB model captures both the static and dynamic properties of extreme-token phenomena in LLMs and accurately predicts their behavior.</p><p>3. Importantly, the quantitative properties of extreme-token dynamics show strong consistency among the theoretical and empirical results of the Bigram-Backcopy task and the empirical performance of OLMo.</p><p>In particular, we consistently observe the sink-logits concentration phenomenon, where the logits corresponding to the key of the extreme token and the queries of all non-extreme tokens (logit •,⟨s⟩ ) are nearly identical-an observation not previously documented in the literature. We summarize the aligned results between the theoretical and empirical findings of the Bigram-Backcopy task and the empirical performance of LLMs in Table <ref type="table">1</ref>.</p><p>4. We propose architectural and optimization modifications to mitigate the extreme-token phenomena. Specifically, we demonstrate that replacing SoftMax with ReLU activations in attention heads eliminates extreme-token phenomena in the BB task, while switching from Adam to SGD removes the residual-state peak phenomenon. We discuss the possibility that similar modifications could mitigate extreme-token phenomena in LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Several studies independently identified the "attention sink" phenomenon in language models and vision transformers, where attention weights were found to be concentrated on a few tokens <ref type="bibr">(Xiao et al., 2023b;</ref><ref type="bibr" target="#b10">Darcet et al., 2023;</ref><ref type="bibr" target="#b26">Han et al., 2023;</ref><ref type="bibr" target="#b65">Zhai et al., 2023;</ref><ref type="bibr" target="#b16">Elhage et al., 2023;</ref><ref type="bibr" target="#b13">Dettmers et al., 2022)</ref>. Recent research has provided more detailed characterizations of this attention pattern and the attention sink phenomenon <ref type="bibr" target="#b19">(Fu, 2024;</ref><ref type="bibr" target="#b49">Sun et al., 2024)</ref>. <ref type="bibr" target="#b49">Sun et al. (2024)</ref> attributed the attention sink to the massive activation of the hidden representations of the corresponding tokens. Both <ref type="bibr" target="#b49">Sun et al. (2024)</ref> and <ref type="bibr" target="#b65">Zhai et al. (2023)</ref> discussed methods for mitigating the attention sink by modifying the model and training recipes.</p><p>Additionally, recent studies have leveraged the attention sink phenomenon to develop improved quantization and more efficient inference algorithms <ref type="bibr" target="#b38">(Liu et al., 2024;</ref><ref type="bibr" target="#b8">Chen et al., 2024;</ref><ref type="bibr" target="#b63">Yu et al., 2024;</ref><ref type="bibr" target="#b48">Son et al., 2024;</ref><ref type="bibr">Lin et al., 2024a;</ref><ref type="bibr" target="#b6">Bondarenko et al., 2023;</ref><ref type="bibr" target="#b28">Hu et al., 2024)</ref>. A concurrent work by <ref type="bibr" target="#b22">Gu et al. (2024)</ref> studied how optimization, data distribution, loss function, and model architecture in LM pre-training influence the emergence of attention sink, showing that replacing the softmax function with sigmoid can prevent attention sink emergence in models up to 1B parameters.</p><p>The dynamics of transformers are studied under various simplifications, including linear attention structures <ref type="bibr" target="#b66">(Zhang et al., 2023;</ref><ref type="bibr" target="#b1">Ahn et al., 2024)</ref>, reparametrizations <ref type="bibr">(Tian et al., 2023b)</ref>, NTK <ref type="bibr" target="#b11">(Deora et al., 2023)</ref>, often in the setting of in-context linear regression <ref type="bibr" target="#b0">(Ahn et al., 2023;</ref><ref type="bibr">Wu et al., 2023a;</ref><ref type="bibr" target="#b67">Zhang et al., 2024)</ref> and structured sequences <ref type="bibr" target="#b4">(Bietti et al., 2024;</ref><ref type="bibr" target="#b43">Nichani et al., 2024;</ref><ref type="bibr">Tian et al., 2023a)</ref>. Notably, <ref type="bibr" target="#b66">Zhang et al. (2023)</ref>; <ref type="bibr" target="#b29">Huang et al. (2023)</ref>; <ref type="bibr" target="#b33">Kim et al. (2024)</ref> demonstrate that a one-layer attention head trained via gradient descent converges to a model that effectively performs in-context regression. <ref type="bibr" target="#b4">Bietti et al. (2024)</ref> shows the fast learning of bigram memorization and the slow development of in-context abilities. <ref type="bibr">Tian et al. (2023a)</ref> shows the scan and snap dynamics in reparametrized one-layer transformers. <ref type="bibr" target="#b46">Reddy (2023)</ref> simplifies the structure of the induction head, showing the connection between the sharp transitions of in-context learning dynamics and the nested nonlinearities of multi-layer operations.</p><p>Mechanistic interpretability is a growing field focused on understanding the internal mechanisms of language models in solving specific tasks <ref type="bibr">(Elhage et al., 2021;</ref><ref type="bibr">Geva et al., 2023;</ref><ref type="bibr" target="#b40">Meng et al., 2022;</ref><ref type="bibr" target="#b42">Nanda et al., 2023;</ref><ref type="bibr" target="#b44">Olsson et al., 2022;</ref><ref type="bibr" target="#b4">Bietti et al., 2024;</ref><ref type="bibr" target="#b56">Wang et al., 2022;</ref><ref type="bibr" target="#b18">Feng and Steinhardt, 2023;</ref><ref type="bibr" target="#b52">Todd et al., 2023)</ref>. This includes mechanisms like the induction head and function vector for in-context learning <ref type="bibr">(Elhage et al., 2021;</ref><ref type="bibr" target="#b44">Olsson et al., 2022;</ref><ref type="bibr" target="#b52">Todd et al., 2023;</ref><ref type="bibr" target="#b4">Bietti et al., 2024)</ref>, the binding ID mechanism for binding tasks <ref type="bibr" target="#b18">(Feng and Steinhardt, 2023)</ref>, association-storage mechanisms for factual identification tasks <ref type="bibr" target="#b40">(Meng et al., 2022)</ref>, and a complete circuit for indirect object identification tasks <ref type="bibr" target="#b56">(Wang et al., 2022)</ref>. The task addressed in this paper is closely related to <ref type="bibr" target="#b4">Bietti et al. (2024)</ref>, who explored synthetic tasks where tokens are generated from either global or context-specific bigram distributions. Several other studies have also employed synthetic tasks to explore neural network mechanisms <ref type="bibr" target="#b7">(Charton, 2022;</ref><ref type="bibr" target="#b39">Liu et al., 2022;</ref><ref type="bibr" target="#b42">Nanda et al., 2023;</ref><ref type="bibr">Allen-Zhu and Li, 2023;</ref><ref type="bibr">Zhu and Li, 2023;</ref><ref type="bibr" target="#b23">Guo et al., 2023;</ref><ref type="bibr" target="#b68">Zhang et al., 2022;</ref><ref type="bibr" target="#b36">Lin et al., 2023)</ref>.</p><p>A line of work focuses on quantizing neural networks using low-bit fixed-point representations <ref type="bibr" target="#b30">(Jacob et al., 2018;</ref><ref type="bibr" target="#b64">Zafrir et al., 2019;</ref><ref type="bibr" target="#b37">Lin et al., 2020;</ref><ref type="bibr" target="#b41">Nagel et al., 2021;</ref><ref type="bibr" target="#b20">Gholami et al., 2022)</ref>, such as INT8 <ref type="bibr" target="#b37">(Lin et al., 2020;</ref><ref type="bibr" target="#b13">Dettmers et al., 2022)</ref> or INT4 <ref type="bibr" target="#b19">(Yao et al.;</ref><ref type="bibr">Wu et al., 2023b;</ref><ref type="bibr" target="#b12">Dettmers and Zettlemoyer, 2023)</ref> to save memory usage and computational cost. In LLMs, the extreme-token phenomena lead to substantial performance degradation after quantization <ref type="bibr" target="#b5">(Bondarenko et al., 2021)</ref> and have become a key focus of recent research <ref type="bibr" target="#b17">(Fan et al., 2020;</ref><ref type="bibr">Yao et al., 2022;</ref><ref type="bibr">Lin et al., 2024a;</ref><ref type="bibr" target="#b28">Hu et al., 2024)</ref>. <ref type="bibr" target="#b13">Dettmers et al. (2022)</ref> and <ref type="bibr">Lin et al. (2024b)</ref> propose mixed-precision approaches, using FP16 for outlier values and INT8 for others, enabling large model quantization without performance loss. <ref type="bibr">Xiao et al. (2023a)</ref> rescales the weights and activations to reduce magnitudes of outliers, and <ref type="bibr" target="#b6">Bondarenko et al. (2023)</ref> proposes modified attention structures to remove outliers, making language models easier to quantize.</p><p>We note that <ref type="bibr" target="#b25">Gurnee et al. (2024)</ref> proposed Attention Deactivation Neurons, <ref type="bibr" target="#b6">Bondarenko et al. (2023)</ref> proposed the "no-op" hypothesis, and <ref type="bibr">Xiao et al. (2023b)</ref> proposed the "dump unnecessary attention" conjecture as mechanisms of attention sinks. In contrast, we explain the extreme-token phenomena through the active-dormant and mutual reinforcement mechanisms, offering the proof of their emergence within training dynamics in a toy model and providing empirical evidence of these mechanisms in LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Preliminaries and notations</head><p>While different LLMs may use slightly varying transformer architectures, most use the structure proposed by <ref type="bibr" target="#b54">Vaswani (2017)</ref>, with the key modification being the shift from post-norm to pre-norm. We represent the tokenized input sequence of length n, with positional embeddings included, as</p><formula xml:id="formula_1">H = [h 1 , . . . , h n ] ∈ R d×n ,</formula><p>where h i denotes the ith input token, and d is the embedding dimension. We denote the layer-normalization operation as LN, the column-wise SoftMax operation as SoftMax, the causal-mask as mask, and the pointwise ReLU function as ReLU.</p><p>The transformer architecture applies causal-attention and MLP layers iteratively to the input sequence H.</p><p>A causal-attention layer with M heads is represented as</p><formula xml:id="formula_2">Attn(•), parameterized by {(Q m , K m , V m , O m )} m : Attn(H) := M -1 m=0 attn m (H) ∈ R d×n ,<label>(1)</label></formula><p>where each attention head attn m (•) is given by</p><formula xml:id="formula_3">attn m (H) := O m V m LN(H)SoftMax mask LN(H) ⊤ K ⊤ m Q m LN(H) .<label>(2)</label></formula><p>We denote the attention map as</p><formula xml:id="formula_4">Map = SoftMax mask LN(H) ⊤ K ⊤ m Q m LN(H)</formula><p>, and typically plot its transpose, Map ⊤ , in figures.</p><p>An MLP layer, denoted mlp(•), has parameters (W 1 , W 2 ):</p><formula xml:id="formula_5">mlp(H) := W 2 ReLU(W 1 LN(H)) ∈ R d×n .</formula><p>(3)</p><p>An L-layer transformer consists of a composition of L self-attention and MLP layers with residual connection structure. Given an input H (0) ∈ R d×n , the output of the L-layer transformer, H (L) , is computed as follows:</p><formula xml:id="formula_6">H (ℓ+1) = H (ℓ+1/2) + mlp (ℓ) H (ℓ+1/2) , H (ℓ+1/2) = H (ℓ) + Attn (ℓ) H (ℓ) , ℓ ∈ {0, . . . , L -1}. (4)</formula><p>For consistency between the code and the text, we adopt zero-indexing throughout this paper, meaning that attention head and layer indices begin at 0 instead of 1.</p><p>For the output H (ℓ+1) of layer ℓ, we define the residual state Res v of a token v ∈ {0, 1, . . . , n -1} as the vth column of H (ℓ+1) . For a specific layer ℓ with input H (ℓ) ∈ R d×n , and for a specific attention head m with query, key, and value matrices (Q, K, V, O), we define the query, key, and value states</p><formula xml:id="formula_7">(Qry v , Key v , Val v ) of a token v ∈ [n]</formula><p>as the vth columns of QH (ℓ) , KH (ℓ) , and OVH (ℓ) , respectively<ref type="foot" target="#foot_0">foot_0</ref> . The attention logit logit v ′ ,v is defined as the (v ′ , v)th element of (H (ℓ) ) ⊤ Q ⊤ KH (ℓ) . For notation simplicity, we omit the dependence on ℓ and m in (Qry v , Key v , Val v , logit v ′ ,v ), as these will be clear from context. Additionally, for a fixed token v, we use the shorthand logit •,v for the set</p><formula xml:id="formula_8">{logit v ′ ,v | v ′ ∈ V}.</formula><p>We use ⟨s⟩ to refer to the "Beginning-of-Sequence" token. Since the ⟨s⟩ token consistently behaves as an extreme token in LLMs, we often refer to ⟨s⟩ and extreme tokens interchangeably. We also abuse the notation by writing (Qry ⟨s⟩ , Key ⟨s⟩ , Val ⟨s⟩ ) to represent the query, key, and value states of the ⟨s⟩ token.</p><p>2 Extreme-token Phenomena in the Bigram-Backcopy Task</p><p>In this section, we analyze simple transformers trained on the Bigram-Backcopy (BB) task, a simple model that exhibits extreme-token phenomena. We demonstrate the active-dormant mechanism (cf. Claim 1) and mutual reinforcement mechanism (cf. Claim 2) within the BB task and provide predictions for the behavior of sink tokens, which will be validated through LLM experiments in the following section.</p><p>The Bigram-Backcopy task is a data-generation model that consists of two sub-tasks: Bigram-transition and Backcopy. In this model, each sequence begins with the ⟨s⟩ token, followed by tokens sampled according to a pre-determined bigram transition probability P (in other words, a Markov chain). When specific trigger tokens are encountered, instead of sampling according to the transition P, the preceding token is copied to the next position. An illustration of the Bigram-Backcopy task is provided in Figure <ref type="figure" target="#fig_2">2a</ref>. Following <ref type="bibr" target="#b4">Bietti et al. (2024)</ref>, we select the transition P and the vocabulary V with |V| = V = 64 based on the estimated characterlevel bigram distribution from the tiny Shakespeare dataset. In all experiments, the set of trigger tokens, T , is fixed and consists of the |T | = 3 most frequent tokens from the unigram distribution. Consequently, the non-trigger token set, V \ T , comprises 61 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-layer transformer exhibits attention sinks and value-state drains</head><p>On the Bigram-Backcopy task, we pre-train a standard one-layer transformer with a single SoftMax attn head and one mlp layer. Unless otherwise specified, the model is trained using Adam for 10, 000 steps, achieving near-optimal prediction accuracy. Detailed training procedures are provided in Appendix C.1. Figure <ref type="figure" target="#fig_2">2b</ref> shows that the trained transformer exhibits the attention sink phenomenon, where the ⟨s⟩ token captures a significant proportion of the attention weights. More importantly, the attention weights display interpretable patterns: all non-trigger tokens exhibit attention sinks, while the attention for trigger tokens is concentrated on their preceding positions. Additionally, Figure <ref type="figure" target="#fig_2">2c</ref> reveals a value-state drain phenomenon similar to that observed in LLMs, suggesting that, for non-trigger tokens, the attn head contributes minimal value to the residual stream. We provide additional attention patterns on different input sequences in Appendix C.2.</p><p>(a) The Bigram-Backcopy task &lt;s&gt; &lt;s&gt; &lt;s&gt; &lt;s&gt; t 1 t k-1 t 2 t 1 … t k t k+1 = t k P( ⋅ | t k ) t k-1 t 2 t 1 … t k t k+1 = t k P( ⋅ | t k ) P( ⋅ | t k ) Always start with ⟨s⟩ At trigger tokens, always copy its backward token to the next position Sample the next token by bigramtransition 𝖯( ⋅ | previous token) v v n a u t u h ⟨s⟩ … t (b) Attention pattern s v t v n a u t u h s v t v n a u t u h 0 1 (c) Small value states s v t v n a u t u h Tokens 0 5 10 15 20 Norms of Value States The active-dormant mechanism of the attention head. Inspired by the interpretable attention weight patterns observed, we propose the active-dormant mechanism. For any given token, an attention head is considered active if it makes a significant contribution to the residual state, and dormant if its contribution is minimal. As illustrated in Figure <ref type="figure" target="#fig_2">2b</ref>, when trained on the BB task, the attention head is active for trigger tokens and dormant for non-trigger tokens.</p><p>Figure <ref type="figure" target="#fig_3">4a</ref> demonstrates that the mlp layer is responsible for the Bigram task whereas the attn head takes care of the Backcopy task. When the mlp layer is zeroed out, the backcopy loss remains significantly better than a random guess, but the bigram loss degrades to near-random levels. Conversely, when the attn layer is zeroed out, the backcopy loss becomes worse than a random guess, while the bigram loss remains unaffected. This indicates that on trigger tokens, the attn head is active and handles the backcopy task, whereas on nontrigger tokens, the attn head is dormant, allowing the mlp layer to handle the Bigram task. We summarize the active-dormant mechanism of the attn head in Claim 1.</p><p>Claim 1 (Active-dormant mechanism). Attention heads of pre-trained models are often governed by the active-dormant mechanism, exhibiting two phases:</p><p>(1) Dormant phase: On non-trigger tokens, the attn head assigns dominant weights to the ⟨s⟩ token, adding minimal value to the residual stream and having little impact on the model's output.</p><p>(2) Active phase: On trigger tokens, the attn head assigns dominant attention weights to relevant context tokens, adding substantial value to the residual stream and significantly impacting the model's output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Head</head><p>Active Dormant</p><formula xml:id="formula_9">0 v 1 v 2 v 3 v 4 × 0 0 0 0 0 Output Values h 0 h 1 h 2 h 3 h 4 h′ 0 h′ 1 h′ 2 h′ 3 h′ 4 Sequence 1 Sequence 2 0 ⋆ ⋆ ⋆ ⋆ Figure 3: Active-dormant mechanism</formula><p>The growth of attention logits on the ⟨s⟩ token and the decrease in its value state norms. Figure <ref type="figure" target="#fig_3">4b</ref> illustrates the training dynamics of excess risks, attention weights, attention logits (for each token v n at position n in the prompt, we compute ∆logit •,⟨s⟩ ≡ mean n [⟨Qry vn , Key ⟨s⟩ ⟩mean i (⟨Qry vn , Key vi )⟩], which serves as a progress measure for attention sinks), and value state norms for the ⟨s⟩ token. All values are rescaled to the 0 to 1 range to highlight trends rather than absolute values. Both the Bigram and Backcopy excess risks decrease to nearly zero within the first 1000 steps, with the Bigram excess risk approaching zero faster than the Backcopy risk. As the Backcopy risk decreases, the attention weights on the ⟨s⟩ token begin to increase, suggesting a connection between the formation of attention sinks and the backcopy function in the attention heads. After the first 1000 steps, although both Bigram and Backcopy excess risks have nearly reached zero, the attention logits and weights on the ⟨s⟩ token continue to increase, while the value state norm of the ⟨s⟩ token continues to decrease. While this is an intriguing phenomenon, The excess risks, attention weights, attention logits, and value state norms for the ⟨s⟩ token throughout the training dynamics. Each curve is rescaled to fall within a 0 to 1 range. On the right side of (b), the horizontal axis is logarithmically scaled. The ∆logit •,⟨s⟩ curve represents the mean of attention logits from all given non-trigger query tokens v on the ⟨s⟩ token, normalized by the mean of attention logits for other tokens. The shaded area represents the 90% uncertainty interval on the distribution over all non-trigger tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input sequence</head><p>Output logits</p><formula xml:id="formula_10">MLP( ) = log( ) v p v Attention SoftMax × 0 α v α v α u λ 0 0 0 0 0 0 0 0 0 0 β ξ v e v ξ u e u 0 ξ v e v Attention weights Value states ⟨s⟩ v t v u ⟨s⟩ v v u t</formula><p>Figure <ref type="figure">5</ref>: Simplified transformer architecture. The output logits are computed by summing the contributions from both the mlp layer and the attn head. The predicted probabilities are obtained by applying the SoftMax function to these output logits. The mlp layer is assumed to provide the Markov transition probabilities for non-trigger tokens, while the attn head is parameterized by attention logits and value states, as described in Eq. ( <ref type="formula" target="#formula_13">6</ref>), (7), and (8). Additionally, the trainable variables, denoted by (α, β) ∈ R V × R V , represent the attention logits and value states of the ⟨s⟩ token.</p><p>our next goal is to understand why the attention logits and value state norms continue to evolve toward extreme values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analysis of a minimally-sufficient transformer architecture</head><p>In this section, we analyze the training dynamics of transformers on the BB task, focusing on a simplified architecture that retains the attention sinks and value-state-drains phenomena. We analyze the regime when the Bigram transition probability is fully learned, and the Backcopy task is partially learned (i.e., after step 200 in Figure <ref type="figure" target="#fig_3">4b</ref>), and we focus on the dynamics of the attention logits and value states. Readers who are more interested in the results than the theoretical analysis can skip the detailed analysis and proceed directly to the statement of the mutual reinforcement mechanism in Claim 2.</p><p>Let V (of size V ) denote the set of all tokens excluding the ⟨s⟩ token, and let T represent the set of all trigger tokens. For any v ∈ V, we define p vk = P(k|v) as the next-token Markov transition probability, and p v = (p v1 , . . . , p vV ) ⊤ ∈ ∆(V) as the transition vector in the simplex. The embedding map is denoted by ebd : [n] × V → R D , where for a token v ∈ V at position i ∈ [n], the embedded vector is ebd i (v). The ⟨s⟩ token always appears at position 0, and we denote its embedding vector by ebd(⟨s⟩). For simplicity, we abuse the notation and use the sequence itself, [⟨s⟩, v 1 , . . . , v n ] where {v k } k∈[n] ⊆ V, to represent the embedding of the sequence. (n+1) with ⟨s⟩ as the zeroth token, we define the predicted probability of the next token as SoftMax(TF(H) n ), where TF(H) n ∈ R D is the last column of TF(H) ∈ R D×(n+1) , defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given an input sequence</head><formula xml:id="formula_11">H = [⟨s⟩, v 1:n ] ∈ R D×</formula><formula xml:id="formula_12">TF(•) = attn(•) + mlp(•), attn(H) = VHSoftMax(mask(H ⊤ K ⊤ QH)), mlp(H) = W 2 ReLU(W 1 H). (5)</formula><p>The simplified transformer architecture TF is a parallel summation of the attn head and the mlp layer, with no layer normalization. This parallel summation is a reasonable simplification, as sequential attn and mlp layers can effectively simulate parallel attn and mlp operations. Notice that we have redefined the notations of attn and mlp in this section, which are simplified versions of Eq. ( <ref type="formula" target="#formula_3">2</ref>) and (3).</p><p>Simplification and reparameterization of the model. To simplify the analysis of the training dynamics, we further reduce the model by restricting the (K, Q, V, W 1 , W 2 ) matrices to follow the patterns observed in the later training stages (i.e., after step 200 of the training in Figure <ref type="figure" target="#fig_3">4b</ref>).</p><p>• Restricted Attention Pattern. Based on the intuition from Figure <ref type="figure" target="#fig_2">2b</ref>, we know that eventually only a few attention logits are non-trivial. Thus, we assume that the model has learned the attention pattern by this stage (which is reasonable given that the Backcopy risk is already small after step 200 in Figure <ref type="figure" target="#fig_3">4b</ref>).</p><p>We parameterize the attention logits on the ⟨s⟩ key-token as (α ⟨s⟩ ; α v1 ; . . . ; α vn ), restrict the attention logits for any trigger query-token to (0, . . . , λ, 0) (where the second-to-last coordinate is λ), and set all other logits to zero. Specifically, we restrict:</p><formula xml:id="formula_13">ebd(⟨s⟩) ⊤ K ⊤ Q • ebd i (v) = α v • 1{v ̸ ∈ T } for v ∈ V, i ∈ [n], ebd i (v) ⊤ K ⊤ Q • ebd j (v) = λ • 1{v ∈ T , i = j -1} for v, v ∈ V, i, j ∈ [n].<label>(6)</label></formula><p>Notice that this naturally implies α v = 0 for v ∈ T .</p><p>• Restricted Value Pattern. At later stages of the training dynamics, we observe that the value states for each token are nearly a scaled version of the one-hot encoding vector. We assume this observed pattern and parameterize the value state of v by ξ v e v ∈ R V . For the ⟨s⟩ token, we parameterize its value state by β ∈ R V . Specifically, we restrict</p><formula xml:id="formula_14">V • ebd(⟨s⟩) = β ∈ R V , V • ebd i (v) = ξ v e v ∈ R V , with ξ v = 0 for v ∈ T , and ξ v ≥ 0 for v ∈ V \ T .<label>(7)</label></formula><p>• MLP Layer Perfectly Predicts the Transition Probability. Notice that the mlp layer handles the Bigram task. By step 200 in Figure <ref type="figure" target="#fig_3">4b</ref>, the Bigram risk has nearly vanished. Therefore, we assume that the mlp layer outputs the Markov transition probabilities p v for non-trigger tokens v, and zero for trigger tokens. Specifically, we restrict:</p><formula xml:id="formula_15">mlp(ebd i (v)) = log p v • 1{v ̸ ∈ T } for v ∈ V. (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>These reparameterizations are illustrated in Figure <ref type="figure">5</ref>. Theorem 1 establishes the existence of a transformer architecture that satisfies the restrictions and reparameterizations outlined above. Furthermore, this restricted transformer can generate the ground-truth transitions of the BB model when certain parameters diverge.</p><p>Theorem 1 (Existence of reparameterization that solves the BB task; informal). For any parameters</p><formula xml:id="formula_17">(α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R)</formula><p>, there exists a one-layer transformer as described in (5) with weight matrices (Q, K, V, W 1 , W 2 ) such that Eq. ( <ref type="formula" target="#formula_13">6</ref>), (7), and (8) hold. Furthermore, there exists a sequence of parameters where min v∈V α v → ∞, min v∈V ξ v → ∞, λ → ∞, and β = 0, such that this transformer generates the ground-truth transitions of the BB model in the limit.</p><p>The formal statement and proof of Theorem 1 are provided in Appendix A.1.</p><p>Dynamic analyses of the reparameterized model. To analyze the later stage training dynamics, we adopt the reparameterization given in Eq. ( <ref type="formula" target="#formula_13">6</ref>), (7), and (8) as our assumption. We further define M k = n i=1 1{v i = k}, M = (M 1 , . . . , M V ), and M = k∈V M k = n. Substituting these into Eq. ( <ref type="formula">5</ref>), for a non-trigger token v ∈ V \ T , the output of the attention layer with input sequence H = [⟨s⟩, v 1:n-1 , v] is given by</p><formula xml:id="formula_18">TF(H) n = log p v + e αv e αv + M β + V k=1 M k ξ k e αv + M • e k .<label>(9)</label></formula><p>Therefore, for the non-trigger token v, the cross-entropy loss between the true Markov transition p v and the predicted transition SoftMax(TF(H) n ) is given by</p><formula xml:id="formula_19">loss v (α v , β) = V k=1 p vk log V i=1 p vi exp e αv β i + M i ξ i e αv + M - e αv β k + M k ξ k e αv + M -log p vk . (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>For simplicity, we neglect the loss on trigger tokens and assume that ({M i } i∈[V ] , M ) remain fixed across different positions in the input sequences. <ref type="foot" target="#foot_1">2</ref> We then consider the total loss as the average of the losses on each non-trigger token, weighted by its proportion in the stable distribution {π v } v∈V , given by</p><formula xml:id="formula_21">loss(α, β) = v∈V\T π v • loss v (α v , β). (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>We assume that ξ and λ are fixed, and that α (the attention logits of the ⟨s⟩ token) and β (the value state norms of the ⟨s⟩ token) are trainable variables, as we are interested in the dynamics of the attention logits and value state norm for the ⟨s⟩ token. The following theorem illustrates the logarithmic growth of the attention logits α, the shrinkage of value states β, and the stable phase of these two variables.</p><p>Theorem 2. Consider the gradient flow of the loss function loss(α, β). Assume ξ v ≥ 0 for any v and π v &gt; 0 for any v ∈ V, and {M i • ξ i } i∈V are not all equal.</p><p>(a) (Attention logits grow logarithmically, reinforced by small value states) Fix β = β • 1 for a constant β, and consider the gradient flow over α. With any initial value α(0), there exists r(t) with norm uniformly bounded in time, such that</p><formula xml:id="formula_23">α(t) = 1 2 log t • 1 + r(t).<label>(12)</label></formula><p>(b) (Value state shrinks to a small constant vector, reinforced by large attention logits</p><formula xml:id="formula_24">) Fix α = α • 1 for a constant α, define β(0) = V -1 [ v β v (0)] and B = V -1 [ v M v ξ v ]. Consider the gradient flow over β. As t → ∞, we have β(t) → β ⋆ = [β(0) + e -α B] • 1 -e -α • M • ξ.<label>(13)</label></formula><p>(c) (Stable phase: Sink-logits concentration) Consider the gradient flow over the variables (α, β). Any vector of the following form</p><formula xml:id="formula_25">α = α • 1, β = c • 1 -e -α • M • ξ, α, c ∈ R (14)</formula><p>is a stationary point. These are all global minimizers of loss(α, β).</p><p>The proof of Theorem 2 is provided in Appendix A.2, A.3, and A.4. We offer two key remarks: (1) As α v → ∞, a Taylor expansion of the gradient ∂loss/∂α v suggests that dα v /dt ∝ exp(-2α v ), which leads to the logarithmic growth of α v . Similar logarithmic growth has been reported in the literature under different setups <ref type="bibr">(Tian et al., 2023a;</ref><ref type="bibr" target="#b69">Zhu et al., 2024)</ref>; (2) The stable phase described in Theorem 2(c) seems to imply that the system can remain stable without attention sinks, as it does not require α to be large. However, in practice, models trained on the BB task tend to converge to a stable phase where α is relatively large.</p><p>The formation of attention sinks and value-state drains. Below, we explain how Theorem 2 reveals the mutual reinforcement mechanism behind the formation of attention sinks and value-state drains.</p><p>(a) When the value states of the ⟨s⟩ token are small and constant, β = β • 1, Theorem 2(a) shows that the attention logits on the ⟨s⟩ token α(t) ≈ α(t)1 for α(t) = (1/2) log t, grow logarithmically. This demonstrates that the presence of a small constant value state (β = β • 1) reinforces the formation of attention sinks (α(t) ≈ α(t) • 1 for α(t) increases logarithmically).</p><p>(b) When the attention logits of the ⟨s⟩ token are large and constant,</p><formula xml:id="formula_26">α = α • 1 for α → ∞, Theorem 2(b)</formula><p>shows that the value states of the ⟨s⟩ token β(t) → β(0) • 1. Starting with a random Gaussian initialization for β(0), we have</p><formula xml:id="formula_27">∥β(t)∥ 2 ≈ ∥β(0) • 1∥ 2 ≈ ∥β(0)∥ 2 / √ V</formula><p>, where V is the vocabulary size, typically large. This indicates that attention sinks (α = α • 1 for large α) reinforces the formation of value-state drains (β(t) → β • 1 for small β).</p><p>(c) In the later stages of the dynamics, both the attention logits and value states of the ⟨s⟩ token stabilize, as described in 2(c). The attention logits remain constant at α = α • 1 with large α, while the value states become small,</p><formula xml:id="formula_28">β = [β(0) + e -α B] • 1 -e -α • M • ξ.</formula><p>Based on these theoretical insights, we summarize the dynamical mechanism underlying attention sinks and value-state drains: For any attention head given a specific prompt, if the model can accurately predict the next token without using the attention head, but adding any value state from previous tokens-except for certain special tokens-worsens the prediction, the attention head will become dormant, forming an attention sink at those special tokens. This phenomenon is induced by the mutual reinforcement mechanism, as described below:</p><p>Claim 2 (Mutual reinforcement mechanism). Dynamically, attention sinks and value-state drains arise through mutual reinforcement:</p><p>(a) The SoftMax mechanism shifts attention weights towards tokens that exhibit value-state drains, reinforcing these tokens as attention sinks. (b) Attention sinks on these extreme tokens further suppress their value states, reinforcing their role as value-state drains. (c) The mutual reinforcement stabilizes when all non-trigger tokens have large, nearly identical attention logits on the extreme token. Due to the causal mask, the training dynamics favor the ⟨s⟩ token as the extreme token.</p><formula xml:id="formula_29">α + (1-α) β = Attention Head Output Attention sink α → 1 Value state drain β → 0 v | v | &gt; 0 with Figure 6: Mutual reinforce- ment mechanism</formula><p>Experimental verification of the quantitative prediction. Revisiting Figure <ref type="figure" target="#fig_3">4b</ref>, which illustrates the dynamics of a single-layer transformer model trained with Adam on the BB task, we observe that ∆logit •,⟨s⟩ exhibits growth rates consistent with Theorem 2. In this context, ∆logit •,⟨s⟩ corresponds to α, as all other attention logits are assumed to be zero under the assumptions of Theorem 2. When plotted on a logarithmic scale, the ∆logit •,⟨s⟩ curve grows approximately linearly between 1,000 and 10,000 steps, then accelerates before stabilizing around 100,000 steps. Meanwhile, the norm of the value state ∥Val ⟨s⟩ ∥ 2 decreases monotonically. The simultaneous increase in attention weights and decrease in value-state norms demonstrate the mutual reinforcement mechanism during the training process.</p><p>To further validate that Theorem 2 accurately captures the dynamics of the original model, we constructed a simplified model based on Eq. ( <ref type="formula" target="#formula_13">6</ref>), (7), and (8), and trained the parameters</p><formula xml:id="formula_30">(α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R)</formula><p>using Adam. The resulting training curves closely resemble those of the one-layer transformer, also displaying the mutual reinforcement mechanism. A detailed description of the experiment can be found in Appendix C.3.</p><p>Generality of the theoretical prediction. Although Theorem 2 focuses on a specific BB task with a simplified architecture and loss function, the underlying principles are broadly applicable to more general settings. In particular, we expect that the formation of extreme tokens in LLMs follows a similar mutual reinforcement mechanism. Indeed, Theorem 2 is essentially based on the following two key assumptions: (1) even with a specific attention head attn zeroed out, the LLM can still accurately predict the next token, implying that the attention head is better off dormant; and (2) for the attention head attn, value states of previous tokens-except for certain special tokens-remain relevant for specific tasks and therefore do not vanish. Under these assumptions, we anticipate the formation of attention sinks and value-state drains for the attention head attn and such special tokens. In Section 3, we explore how these phenomena are formed during the training dynamics of LLMs, finding that the empirical results align with the theory.</p><p>Replacing SoftMax by ReLU attention removes attention sinks and value-state drains. As a consequence of our theory, we predict that training using ReLU attention in place of SoftMax attention will prevent the mutual reinforcement mechanism. Without SoftMax, the training dynamics no longer push the attention weights toward the ⟨s⟩ token, which remains zero throughout training. In the absence of attention sinks, the dynamics no longer push down the value state norm, and the mutual reinforcement mechanism breaks. Figure <ref type="figure" target="#fig_4">7a</ref> presents the training dynamics on the BB task using ReLU instead of SoftMax attention, showing that both the Bigram and Backcopy risk converge to the Bayes risk after 200 training steps, but the attention logits of ⟨s⟩ do not increase, and the value state does not shrink, confirming our prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The emergence of residual-state peaks</head><p>In this section, we experimentally investigate the residual-state peaks phenomenon. We observe that no residual-state peaks occur in the single-layer transformer trained on the BB task. To explore this further, we train slightly deeper transformers on the BB task and track the residual state norm after layer 0. We observe that two-layer models do not exhibit residual-state peaks, while models with three or more layers do. Additional experimental results are provided in Appendix B.1 and B.2.</p><p>Massive residual state at layer 0 output induces attention sinks and value-state drains in the middle layer. To investigate the relationship between massive residual states and attention sinks, we train on the BB task using the "attn+mlp+attn+mlp+mlp" model, which is the minimal structure that shows the massive residual states phenomena. We perform intervention by analyzing how the model's behavior changes after zeroing out layer 0 (the first "attn+mlp" block). Before and after zeroing, we compute the difference in ∥Res ⟨s⟩ ∥ and Mean v [∥Res v ∥] at the layer 0 output, and compute logit •,⟨s⟩ and ∥Val ⟨s⟩ ∥ in the middle layer. After zeroing out, the residual state norm becomes non-massive, and attention logits and the value state norm return to a normal level. This confirms that the residual-state peak contributes to the attention sink and value-state-drain phenomena in the middle layer of pre-trained transformers.</p><p>Linear growth of residual-state norm with Adam training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extreme-token Phenomena in pretrained LLMs</head><p>In this section, we investigate extreme-token phenomena in open-source pretrained LLMs. In Section 3.1, we analyze the static behavior of these phenomena in Llama 2-7B-Base <ref type="bibr" target="#b53">(Touvron et al., 2023)</ref>, confirming the existence of the active-dormant mechanism in LLMs. Notably, we identify a specific head that is active on GitHub samples but dormant on Wikipedia samples. In Section 3.2, we examine the dynamic behavior of extreme-token phenomena during the pretraining of OLMo-7B <ref type="bibr" target="#b21">(Groeneveld et al., 2024)</ref>. We show that the attention logits, value states norm, and residual states norm of the sink token(s) in OLMo reflect behavior similar to that of the simpler BB model. Specifically, the simultaneous formation of attention sinks and value-state drains gives evidence for the mutual reinforcement mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Active-dormant mechanism in LLMs</head><p>Our study of the BB model leads to the following prediction with respect to the extreme-token phenomena, which we hypothesize also applies to LLMs:</p><p>Attention heads are controlled by an active-dormant mechanism (cf. Claim 1). The presence of attention sinks and value-state drains indicates that an attention head is in a dormant phase.</p><p>This hypothesis suggests that in LLMs, whether an attention head becomes a sink depends on the context. Specifically, the attention head may become entirely irrelevant for selecting the next tokens in certain contexts or tasks, but not in others. When this irrelevance occurs, the attention head transitions into an attention sink. This hypothesis was confirmed in small transformers and the BB task, as demonstrated in Section 2. The total attention mass on extreme tokens ⟨s⟩ and "Delim"(⟨period⟩) at Layer 24, averaged across all attention heads. The horizontal axis is logarithmically scaled after step 10k. We observe a rapid increase followed by stabilization within the range [0.9, 1] for the rest of training, consistent with our predictions. Middle (b): The value state norms of each token at Layer 24 during training, averaged over all heads. The horizontal axis is logarithmically scaled after step 10k. Initially, the value states of all tokens shrink, eventually converging, while the value states of the extreme tokens shrink to significantly lower levels compared to other tokens. Accordingly, we aim to identify instances of attention heads in pretrained LLMs that exhibit this activedormant behavior, i.e., heads that are dormant in some domains but active in others. In Figure <ref type="figure" target="#fig_6">8</ref>, we display a particular attention head-Layer 16 Head 25 (L16H25) of Llama 2-7B-Base <ref type="bibr" target="#b53">(Touvron et al., 2023)</ref>-which demonstrates a clear active-dormant distinction across two distinct contexts (e.g., tokens from the GitHub subset versus the Wikipedia subset of RedPajama <ref type="bibr" target="#b9">(Computer, 2023)</ref>). While many attention heads show similar context-dependent behavior (see Appendix D), we focus on this one because the conditions for its activation are straightforward and interpretable, whereas other heads may have more nuanced criteria.</p><p>Figure <ref type="figure" target="#fig_6">8a</ref> shows the attention maps of L16H25 on samples from both the GitHub and Wikipedia subsets of RedPajama. It demonstrates that L16H26 is dormant (i.e., an attention sink) on samples from Wikipedia, which resemble prose, and active (i.e., not an attention sink) on samples from GitHub, which resemble code. Additionally, Figure <ref type="figure" target="#fig_6">8b</ref> compares the loss difference when L16H25 is zeroed out for prompts from both domains. The results show that zeroing out this head significantly decreases model performance on GitHub sequences, while having minimal impact on Wikipedia sequences. This observation also confirms the head behaves as dormant in some contexts and active in others-in some contexts, removing this head has no effect on model performance, while in others, its removal causes significant performance drops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extreme-token phenomena along training dynamics of LLMs</head><p>Our study of the BB model leads to the following prediction about the dynamical behavior of the extremetoken phenomena, which we hypothesize also applies to LLMs:</p><p>Attention heads undergo an attention-increasing and value-state-shrinking phase driven by the mutual reinforcement mechanism (cf. Claim 2). This is followed by a stable phase, where all non-trigger tokens have large, nearly identical attention logits on the extreme token. Simultaneously, the residual state norms of the extreme tokens increase linearly during pretraining.</p><p>We confirm these predictions below. To observe the training dynamics of a large-scale LLM, we use the setup of OLMo-7B-0424 <ref type="bibr" target="#b21">(Groeneveld et al., 2024)</ref> (henceforth just referred to as OLMo), which provides open-sourced weights at various stages of their training.<ref type="foot" target="#foot_2">foot_2</ref> For our analysis, we inspect OLMo at multiple • Key Others ]. The horizontal axis is logarithmically scaled after step 10k. We observe that ∆logit •,⟨s⟩ increases approximately in logarithmic scale during training steps 10k to 100k, matching the decreasing phase of the value states in Figure <ref type="figure" target="#fig_7">9b</ref>. Right (b): Attention logits of the last token's query state against all token's key states for pretrained OLMo. In this experiment, we generate 128 randomly sampled test tokens with IDs from 100 to 50000 in the OLMo tokenizer. We append each token separately to the test phrase "Summer is warm⟨period⟩ Winter is cold⟨period⟩", creating 128 different samples, which we feed to the LLM to examine the model behavior. We plot the distribution of (un-shifted) attention logits logit •,v = Qry ⊤ test Key v across all heads at Layer 24 and all test tokens. The distribution of logit •,⟨s⟩ and logit •,Delim have considerably small variance compared with other logits, confirming the sink-logits concentration phenomenon.</p><p>checkpoints: every 500 steps for the first 10,000 steps, then at 25,000 steps, 50,000 steps, and every 50,000 steps up to 449,000 steps (approximately the end of their training). <ref type="foot" target="#foot_3">4</ref> The input we use for this analysis is again "Summer is warm⟨period⟩ Winter is cold⟨period⟩"<ref type="foot" target="#foot_4">foot_4</ref> In this prompt, the "Delim" token, namely "⟨period⟩", also becomes a sink token along with ⟨s⟩. We believe this occurs because the period is not semantically meaningful and is not useful for predicting future tokens (cf. Appendix G.2) Figure <ref type="figure" target="#fig_7">9</ref> illustrates the dynamics of attention weights, value state norms, and the residual state norms for attention heads in Layer 24 of OLMo. The figure shows that the average attention on extreme tokens (⟨s⟩ and Delim) increases rapidly at the beginning of training before stablizing, while the value state norms of these extreme tokens decrease during training steps 10k-100k. The synchronized evolution of attention weights and value state norms aligns with the prediction of the mutual reinforcement mechanism. Additionally, the residual states of ⟨s⟩ increase linearly, while those of other tokens converge to a small number. Figure <ref type="figure" target="#fig_9">10</ref> provides a more detailed examination of the attention logits in Layer 24 of OLMo. Figure <ref type="figure" target="#fig_9">10a</ref> presents the dynamics of the difference in attention logits, showing that ∆logit •,⟨s⟩ increase during training steps 10k-100k, matching the decreasing phase of the value states. Figure <ref type="figure" target="#fig_9">10b</ref> also demonstrates the sink-logits concentration phenomenon. Specifically, it shows that the sink logits will eventually converge to a stable phase, in which logits corresponding to the key of the sink token and queries of all non-sink tokens are nearly identical. These findings coincide with the dynamical behavior predicted by the BB model, as outlined in Theorem 2(c) and corroborated by the experimental results in Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we investigated the extreme-token phenomena, specifically attention sinks, value-state drains, and residual-state peaks. We analyzed simple transformers trained on the Bigram-Backcopy (BB) task, both theoretically and empirically, demonstrating that these models exhibit the same extreme-token phenomena observed in large language models (LLMs). Building on the insights from the BB task, we made several detailed predictions about the behavior of extreme-token phenomena in LLMs. In particular, we identified the active-dormant mechanism governing attention heads in both the BB model and LLMs, with attention sinks and value-state drains serving as indicators of dormant phase, and a mutual reinforcement mechanism that induces these phenomena during pretraining. Using insights from these mechanisms, we applied simple modifications to the model architecture and optimization procedure, effectively mitigating the extremetoken phenomena in the BB model. Overall, our work uncovers the underlying mechanisms of extreme-token phenomena and suggests potential pathways to mitigate these issues during LLM pretraining.</p><p>We believe the most compelling direction for future work is to explore whether eliminating the extreme-token phenomena is essential or beneficial for building powerful transformer-based LLMs. While it is possible to mitigate these phenomena through simple modifications to the architecture or training algorithms, it remains unclear whether their elimination significantly improves downstream tasks such as inference and quantization. Given the resource-intensive nature of pretraining large-scale LLMs, we anticipate that pretraining a model at the scale of GPT-2 could both provide valuable insight into this issue and help point the way to architectures that can reduce the pretraining burden. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs of Theorem 1 and 2</head><p>We introduce new notations that are frequently used in the proofs. Recall that in Eq. ( <ref type="formula" target="#formula_21">11</ref>), we used {π v } v∈V to denote the stable distribution across all tokens. We further define the stable distribution excluding trigger tokens as follows:</p><formula xml:id="formula_31">π ∈ R V , πi = π i 1{i ∈ V \ T }. (<label>15</label></formula><formula xml:id="formula_32">)</formula><p>Section 2.2 defines the bigram transition probability in the Bigram-Backcopy task as p vk = P(k | v). We further define the bigram transition probability matrix as</p><formula xml:id="formula_33">P =    p 11 . . . p 1V . . . . . . . . . p V 1 . . . p V V    =    p ⊤ 1 . . . p ⊤ V    . (<label>16</label></formula><formula xml:id="formula_34">)</formula><p>Given a token v, define the predicted probability at token v as the logit output passed through the softmax activation. Let H = [⟨s⟩; v 1:n-1 ; v]. Using the form of TF(H) n defined in Eq. ( <ref type="formula" target="#formula_18">9</ref>), we denote</p><formula xml:id="formula_35">l v = SoftMax(TF(H) n ) = (l v1 , . . . , l vV ), with l vi = p vi exp Miξi+e α βi e α +M V k=1 p vk exp M k ξ k +e α β k e α +M .<label>(17)</label></formula><p>Similar to Eq. ( <ref type="formula" target="#formula_33">16</ref>), we define the full output probability matrix as</p><formula xml:id="formula_36">L =    l 11 . . . l 1V . . . . . . . . . l V 1 . . . l V V    =    l ⊤ 1 . . . l ⊤ V    . (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>Using the notation l v and πv , we can rewrite the loss functions defined in Eq. ( <ref type="formula" target="#formula_19">10</ref>) and Eq. ( <ref type="formula" target="#formula_21">11</ref>) as follows:</p><formula xml:id="formula_38">loss v (α v , β) = - V k=1 p vk log l vk , loss v (α, β) = V v=1 πv loss v (α v , β). (<label>19</label></formula><formula xml:id="formula_39">)</formula><p>We always have that k p vk = 1 and k l vk = 1. The total variation norm and KL-divergence are then defined as:</p><formula xml:id="formula_40">∥p v -l v ∥ TV = k |p vk -l vk |, KL(p v || l v ) = - k p vk log(l vk /p vk ).<label>(20)</label></formula><p>Given any vector u = [u 1 ; . . . ; u d ], define the corresponding diagonal matrix as</p><formula xml:id="formula_41">diag(u) =       u 1 0 . . . 0 . . . . . . . . . . . . . . . . . . 0 . . . 0 u d       .</formula><p>Given any p v defined in Eq. ( <ref type="formula" target="#formula_33">16</ref>), denote</p><formula xml:id="formula_42">G P v = diag(p v ) -p v p ⊤ v , G L v = diag(l v ) -l v l ⊤ v . (<label>21</label></formula><formula xml:id="formula_43">)</formula><p>We now present technical lemmas concerning</p><formula xml:id="formula_44">G P v and G l v . Lemma A.1. The matrices G P v ∈ R V ×V and G L v ∈ R V ×V are positive semi-definite for any v ∈ V.</formula><p>Proof of Lemma A.1. Since V k=1 p vk = 1 and V k=1 l vk = 1 for any v, we have that</p><formula xml:id="formula_45">(G P v ) ii = p i -p 2 i = p i ( k̸ =i p k ) ≥ k̸ =i |(G P v ) ik |, (G L v ) ii = l i -l 2 i = l i ( k̸ =i l k ) ≥ k̸ =i |(G L v ) ik |.</formula><p>This shows that both G P v and G L v are diagonally dominant matrices. By Corollary 6.2.27 in Horn and Johnson (2012), they are positive semi-definite.</p><p>Lemma A.2. Suppose that πv &gt; 0 for any v ∈ V \ T . For any η ∈ R V with η ⊥ 1, there exists ω &gt; 0 such that</p><formula xml:id="formula_46">η ⊤ V k=1 πk G P k η ≥ ω∥η∥ 2 2 .</formula><p>Proof of Lemma A.2. Denote the null spaces of G P v for v ∈ V as S v . We solve for each S v . Setting</p><formula xml:id="formula_47">G P v η = 0 gives that [p vj -p vj ( k p vk )]η j = 0 for any j ∈ V.</formula><p>If p vj ̸ = 0, we divide each side with p vj and get that η j = k p vk η k . As a result, we get that</p><formula xml:id="formula_48">S v = {η | η j is constant for p vj ̸ = 0}.</formula><p>Since all π k &gt; 0, for any k ∈ V \ T , there is v ∈ V \ T such that p vk &gt; 0, we get that</p><formula xml:id="formula_49">∩ v∈V\T S v = {c • 1 | c ∈ R}. Since η ⊥ 1, we get that η ⊥ ∩ v∈V\T S v . We denote the minimal non-zero eigenvalues of G L v for v ∈ V \ T as λ. We get that η ⊤ V k=1 πk G P k η ≥ min v∈V\T πv λ∥η∥ 2 2 .</formula><p>Setting ω = λ • min v∈V\T πv &gt; 0, this proves Lemma A.2.</p><p>Lemma A.3. Given ω defined in Lemma A.2, suppose that</p><formula xml:id="formula_50">max v,k |p vk -l vk | = δ ≤ min {ω/(6V ), 1}.<label>(22)</label></formula><p>For any η ∈ R V with η ⊥ 1, we have that</p><formula xml:id="formula_51">η ⊤ V k=1 πk G L k η ≥ ω 2 ∥η∥ 2 2 . Proof of Lemma A.3. Denote δ = max v,k |p vk -l vk |. Suppose that δ ≤ 1. For any k ∈ V \ T , we can verify that (G P k ) ij -(G L k ) ij ≤ 3δ, for any i, j ∈ [V ]. We denote E = V k=1 πk G P k - V k=1 πk G L k . Therefore, |E ij | ≤ 3δ for any i, j ∈ [V ]. This means that η ⊤ Eη ≤ ∥E∥ 2 ∥η∥ 2 2 ≤ ∥E∥ F ∥η∥ 2 2 ≤ V • 3δ • ∥η∥ 2 2 .</formula><p>As a result, when δ ≤ min {ω/(6V ), 1}, we get that</p><formula xml:id="formula_52">η ⊤ V k=1 πk G L k η ≥ ω∥η∥ 2 2 -η ⊤ Eη ≥ ω 2 ∥η∥ 2 2 .</formula><p>This proves Lemma A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 1</head><p>We denote the hidden dimension as d and the sequence length as N . Recall that the token v at position i is encoded as ebd i (v). We begin with the assumption regarding the transformer's embedding dimension:</p><formula xml:id="formula_53">Assumption A. We have {ebd 0 (⟨s⟩)} ∪ {ebd i (v)} i∈{0}∪[N -1],v∈V ⊆ R d , where the embedding dimension d ≥ V N + 1.</formula><p>Assumption A requires a large embedding dimension d ≥ V N + 1. This assumption is used to ensure that there are enough orthonormal bases in the embedding space. Given the fact that there are O(exp(d)) approximately linearly independent vectors for large d <ref type="bibr" target="#b55">(Vershynin, 2018)</ref>, it is possible to relax the assumption to be d ≫ log(V N ). However, since Assumption A pertains only to the construction of λ for trigger tokens and is unrelated to Theorem 2, we adopt it to simplify the proof of Theorem 1.</p><p>Theorem A.4 (Formal statement of Theorem 1). Let Assumption A hold. For any parameters</p><formula xml:id="formula_54">(α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R)</formula><p>, there exists a one-layer transformer (5) with weight matrices (Q, K, V, W 1 , W 2 ) such that Eq. ( <ref type="formula" target="#formula_13">6</ref>), ( <ref type="formula" target="#formula_14">7</ref>), and (8) hold. Consider the Bigram-Backcopy task, where given an input</p><formula xml:id="formula_55">H = [⟨s⟩; v 1:n-1 , v],</formula><p>the ground-truth transition gives</p><formula xml:id="formula_56">P(v ′ | H) = p vv ′ for v ∈ V \ T , and P(v ′ | H) = 1{v ′ = v n-1 } for v ∈ T .</formula><p>There exists a sequence min v∈V α v → ∞, min v∈V ξ v → ∞, λ → ∞, and β = 0 such that this transformer generates the ground-truth transition in the limit, i.e.,</p><formula xml:id="formula_57">SoftMax(TF(H) n ) → P( • |H).<label>(23)</label></formula><p>Proof of Theorem A.4.</p><p>Step 1. Construction for the attention head. We let {ebd 0 (⟨s⟩</p><formula xml:id="formula_58">)} ∪ {ebd i (v)} i∈{0}∪[N -1],v∈V ∪ {e v } v∈V to be a set of orthonormal basis in R d , and denote {η i } i∈{0}∪[N -1] ⊆ R d by a set of orthonormal basis in R d (the existence is guaranteed by Assumption A). Therefore, for any parameters (α ∈ R V , β ∈ R V , ξ ∈ R V , λ ∈ R), there exists a query matrix Q ∈ R d×N such that Q • ebd i (v) = λη i-1 for i &gt; 1, v ∈ T , Q • ebd i (v) = α v η 0 for i &gt; 0, v ∈ V \ T .<label>(24)</label></formula><p>Meanwhile, there is a key matrix K ∈ R d×N such that</p><formula xml:id="formula_59">K • ebd i (v) = η i for i &gt; 0, v ∈ V, K • ebd 0 (⟨s⟩) = η 0 .<label>(25)</label></formula><p>Denote {e v } v∈V as an orthonormal basis in R V . There is a matrix V ∈ R d×V such that</p><formula xml:id="formula_60">V • ebd i (v) = ξ v e v ∈ R V , with ξ v = 0 for v ∈ T , and ξ v ≥ 0 for v ∈ V \ T . V • ebd 0 (⟨s⟩) = β ∈ R V .<label>(26)</label></formula><p>This construction matches Eq. ( <ref type="formula" target="#formula_13">6</ref>) and ( <ref type="formula" target="#formula_14">7</ref>).</p><p>As a result, for v n ∈ V \ T , by Eq. ( <ref type="formula">5</ref>), denoting H = [⟨s⟩; v 1:n-1 ; v n ] and attn(H) n to be the last column of attn(H), we have</p><formula xml:id="formula_61">attn(H) n = n i=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd i (v i )]V • ebd i (v i ) n j=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd j (v j )] = exp[α vn η ⊤ 0 η 0 ] • β + n i=1 exp[α vn η ⊤ 0 η i ]ξ vi • e vi exp[α vn η ⊤ 0 η 0 ] + n j=1 exp[α vn η ⊤ 0 η j ] = e αv n e αv n + n • β + n i=1 1 e αv n + n • ξ vi e vi .</formula><p>For v n ∈ T , we have</p><formula xml:id="formula_62">attn(H) n = n i=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd i (v i )]V • ebd i (v i ) n j=0 exp[ebd n (v n ) ⊤ Q ⊤ K • ebd j (v j )] = exp[λη ⊤ n-1 η 0 ] • β + n i=1 exp[λη ⊤ n-1 η i ]ξ vi • e vi exp[λη ⊤ n-1 η 0 ] + n j=1 exp[λη ⊤ n-1 η j ] = 1 e λ + n • β + i̸ =n-1 1 e λ + n</formula><p>• ξ vi e vi + e λ e λ + n</p><p>• ξ vn-1 e vn-1 .</p><p>Step 2. Construction for the MLP layer. Further, define the weights for the mlp layer such that</p><formula xml:id="formula_63">W 1 • ebd i (v) = e v ∈ R V , W 2 e v = log p v • 1{v ̸ ∈ T } ∈ R V for i ∈ [N ], v ∈ V,<label>(27)</label></formula><p>where {e v } is the eorthonormal basis in R V and p v ∈ R V is defined in Eq. ( <ref type="formula" target="#formula_33">16</ref>). As a result, mlp(H</p><formula xml:id="formula_64">) n = W 2 ReLU(W 1 ebd n (v)) = W 2 e v = log p v • 1{v / ∈ T }.</formula><p>This matches the Eq. ( <ref type="formula" target="#formula_15">8</ref>).</p><p>Step 3. The output of the transformer. By Eq. ( <ref type="formula">5</ref>) again, on non-trigger token v ∈ V \ T , the transformer output gives that</p><formula xml:id="formula_65">TF(H) n = mlp(ebd n (v)) + attn(H) n = log p v + e αv n e αv n + n • β + n i=0 1 e αv n + n • ξ vi e vi .</formula><p>On trigger token v ∈ T , the transformer output gives that</p><formula xml:id="formula_66">TF(H) n = mlp(ebd n (v)) + attn(H) n = 1 e λ + n • β + i̸ =n-1 1 e λ + n</formula><p>• ξ vi e vi + e λ e λ + n</p><p>• ξ vn-1 e vn-1 .</p><p>There exists a sequence min v∈V α v → ∞, min v∈V ξ v → ∞, λ → ∞, and β = 0, we get that</p><formula xml:id="formula_67">SoftMax[TF(H) n ] → p vn for n &gt; 0, v n ∈ V \ T , SoftMax[TF(H) n ] → (1{v = v n-1 }) v∈V for n &gt; 0, v n ∈ T .</formula><p>This proves Eq. ( <ref type="formula" target="#formula_57">23</ref>), indicating that the transformer output matches the ground truth transition. This finishes the proof of Theorem A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 2(c): Stable phase</head><p>We first state Lemma A.5 and Proposition A.6 that are used to prove Theorem 2(c). Lemma A.5 computes the gradients of l ik as defined in Eq. ( <ref type="formula" target="#formula_35">17</ref>).</p><p>Lemma A.5. Given l ik defined in Eq. ( <ref type="formula" target="#formula_35">17</ref>), for any i, k, v, and any value of α v and β v , we have that</p><formula xml:id="formula_68">∂l ik ∂α v = 1{i = v}l ik e αi (e αi + M ) 2 M β k -M k ξ k - V j=1 l ij (M β j -M j ξ j ) , ∂l ik ∂β v = e αi e αi + M [l ik 1{k = v} -l ik l iv ].</formula><p>Furthermore, we have Proof of Lemma A.5. We repeatedly use the following two facts:</p><formula xml:id="formula_69">∂ exp M k ξ k +e α i β k e α i +M ∂α v = 1{i = v}e αi (M β k -M k ξ k ) (e αi + M ) 2 exp M k ξ k + e αi β k e αi + M , ∂ exp M k ξ k +e α i β k e α i +M ∂β v = 1{k = v}e αi e αi + M exp M k ξ k + e αi β k e αi + M .</formula><p>When i ̸ = v, l ik has zero gradients with respect to α v . When i = v, we have that</p><formula xml:id="formula_70">∂l vk ∂α v = l vk e αv M β k -M k ξ k (e αv + M ) 2 - l vk V i=1 p vi e αv M βi-Miξi (e αv +M ) 2 exp Miξi+e αv βi e αv +M V i=1 p vi exp Miξi+e αv βi e αv +M = e αv (e αv + M ) 2 l vk [M β k -M k ξ k ] -l vk V j=1 l vj (M β j -M j ξ j ) ,</formula><p>and</p><formula xml:id="formula_71">∂l ik ∂β v = e αi e αi + M l ik 1{k = v} - e α i e α i +M p iv exp Mvξv+e α i βv e α i +M p ik exp M k ξ k +e α i β k e α i +M V j=1 p ij exp Mj ξj +e α i βj e α i +M 2 = e αi e αi + M [l ik 1{k = v} -l ik l iv ].</formula><p>We can verify that</p><formula xml:id="formula_72">V k=1 ∂l ik ∂α v = e αv (e αv + M ) 2 V k=1 l vk [M β k -M k ξ k ] -l vk V j=1 l vj (M α j -M j ξ j ) = e αv (e αv + M ) 2 V k=1 l vk [M β k -M k ξ k ] - V j=1 l vj (M α j -M j ξ j ) = 0, and V v=1 ∂l ik ∂β v = e αi e αi + M V v=1 [l ik 1{k = v} -l ik l iv ] = e αi e αi + M [l ik -l ik ] = 0.</formula><p>This finishes the proof of Lemma A.5.</p><p>Proposition A.6 computes the gradient of loss with respect to α and β, giving the ODE of the gradient flow.</p><p>Proposition A.6. Consider the gradient flow of optimizing loss(α, β) given by</p><formula xml:id="formula_73">α(t) = -∇ α loss(α(t), β(t)), β(t) = -∇ β loss(α(t), β(t)). (<label>28</label></formula><formula xml:id="formula_74">)</formula><p>Simplifying the dynamics using Lemma A.5 gives that</p><formula xml:id="formula_75">αv (t) = πv e αv (e αv + M ) 2 V i=1 (p vi -l vi )(M β i -M i ξ i ), βv (t) = V k=1 πk e α k [p kv -l kv ] e α k + M .</formula><p>Proof of Proposition A.6. Taking the derivative of loss(α, β) gives that ∂loss(α, β)</p><formula xml:id="formula_76">∂α v = πv V k=1 p vk • -1 l vi • ∂l vi ∂α v = πv e αv (e αv + M ) 2 V i=1 l vi [M β i -M i ξ i ] - V k=1 p vk [M β k -M k ξ k ] = πv e αv (e αv + M ) 2 V k=1 [l vk -p vk ][M β k -M k ξ k ] .</formula><p>Similarly, we have that ∂loss(α, β)</p><formula xml:id="formula_77">∂β v = V j=1 πj V k=1</formula><p>p Plug them in Eq. ( <ref type="formula" target="#formula_73">28</ref>) proves Proposition A.6.</p><p>Theorem A.7 (Restatement the stable phase part in Theorem 2(c)). Assume ξ v ≥ 0 for any v, π v &gt; 0 for any v ∈ V, and {M i • ξ i } i∈V are not all equal. Consider the gradient flow over the variables (α, β), i.e., ( α(t), β(t)) = -∇ α,β loss(α(t), β(t)). Any vector of the following form</p><formula xml:id="formula_78">α ⋆ = α • 1, β ⋆ = c • 1 -e -α • M • ξ, α, c ∈ R<label>(29)</label></formula><p>is a stationary point. These are all global minimizers of loss(α, β).</p><p>Proof of Theorem A.7. When α = α ⋆ and β = β ⋆ , given l vi defined in Eq. ( <ref type="formula" target="#formula_35">17</ref>) with any v and i, we have that Plug l vi into ∂loss(α, β)/∂α and ∂loss(α, β)/∂β, we have</p><formula xml:id="formula_79">l vi = p vi exp Miξi+e α βi e α +M V k=1 p vk exp M k ξ k +e α β k</formula><formula xml:id="formula_80">∂loss(α, β) ∂α v α ⋆ ,β ⋆ = πv e αv (e αv + M ) 2 V k=1 (l vk -p vk )[W β k -M k ξ k ] = 0, ∂loss(α, β) ∂β v α ⋆ ,β ⋆ = V k=1 πk e α k [l kv -p kv ] e α k + M = 0.</formula><p>This shows that α = α ⋆ and β = β ⋆ are stationary points. We further compute the second-order derivative using Lemma A.5. To simplify the notation, we use</p><formula xml:id="formula_81">z k = W β k -M k ξ k and z = [z 1 , . . . , z V ]. We have that ∂ 2 loss(α, β) ∂α i ∂α v α ⋆ ,β ⋆ = 1{v = i} • πv e α (e α + M ) 2 V k=1 ∂l ik ∂α v z k = 1{v = i} • πv e 2α (e α + M ) 4 V k=1 l ik z 2 k - V k=1 l ik z k 2 = 1{v = i} • πv e 2α (e α + M ) 4 V k=1 p ik z 2 k - V k=1 p ik z k 2 ,</formula><p>where in the last line, we plugged in l vi = p vi for any v and i. Similarly, we compute the second order derivatives with respect to α i and β v ,</p><formula xml:id="formula_82">∂ 2 loss(α, β) ∂α i ∂β v α ⋆ ,β ⋆ = πi e α (e α + M ) 2 V k=1 ∂l ik ∂β v z k = πi e 2α (e α + M ) 3 p iv z k -p iv V k=1 p ik z k .</formula><p>With the same manner, we compute the second order derivatives with respect to β i and β v ,</p><formula xml:id="formula_83">∂ 2 loss(α, β) ∂β i ∂β v α ⋆ ,β ⋆ = V k=1 ∂l ki ∂β v πk e α e α + M = e 2α (e α + M ) 2 V k=1 {π k [1{v = i}p kv -p ki p kv ]}.</formula><p>Combining the above computations gives that</p><formula xml:id="formula_84">Hessian(loss(α ⋆ , β ⋆ )) = ∇ 2 α loss(α, β) ∇ α ∇ β loss(α, β) ∇ β ∇ α loss(α, β) ∇ 2 α loss(α, β) , with ∇ 2 α loss(α, β) = e 2α (e α + M ) 4 diag π • [z ⊤ G P 1 z; . . . ; G P V z] , ∇ α ∇ β loss(α, β) = e 2α (e α + M ) 3 diag π [z ⊤ G P 1 ; . . . ; z ⊤ G P V ], ∇ 2 β loss(α, β) = e 2α (e α + M ) 2 V k=1 πk G P k ,</formula><p>where G P k is defined in Eq. ( <ref type="formula" target="#formula_42">21</ref>). Furthermore, there exists U such that U Hessian(loss(α</p><formula xml:id="formula_85">⋆ , β ⋆ ))U ⊤ = Diag-Hessian(loss(α ⋆ , β ⋆ )), with Diag-Hessian(loss(α ⋆ , β ⋆ )) = ∇ 2 α loss(α, β) 0 0 e 2α (e α +M ) 2 B</formula><p>, where the B is given by</p><formula xml:id="formula_86">B = V k=1 πk G P k -(z ⊤ G P k z) -1 G P k zz ⊤ G P k .</formula><p>To prove that B is positive semi-definite, consider any vector η with ∥η∥ 2 = 1:</p><formula xml:id="formula_87">η ⊤ Bη = V k=1 πk η ⊤ G P k η - η ⊤ G P k zz ⊤ G P k η z ⊤ G P k z .</formula><p>Since G P k is positive semi-definite, the Cauchy inequality gives that</p><formula xml:id="formula_88">z ⊤ G P k η ≤ z ⊤ G P k zη ⊤ G P k η.</formula><p>As a result, we have that</p><formula xml:id="formula_89">η ⊤ Bη ≥ V k=1 πk η ⊤ G P k η - z ⊤ G P k zη ⊤ G P k η z ⊤ G P k z = 0.</formula><p>This shows that B is positive semi-definite. Therefore, Hessian(loss(α ⋆ , β ⋆ )) is positive semi-definte. This proves Theorem A.7.</p><p>A.3 Proof of Theorem 2(a): Attention sinks Theorem A.8 (Restatement of the attention sink part in Theorem 2(a)). Assume ξ v ≥ 0 for any v, π v &gt; 0 for any v ∈ V, and {M i • ξ i } i∈V are not all equal. Fix β = β • 1 for a constant β, and consider the gradient flow of the loss function loss(α, β) over α, i.e., α(t) = -∇loss(α(t), β). With any initial value α(0), there exists r(t) with norm uniformly bounded in time, such that</p><formula xml:id="formula_90">α(t) = 1 2 log t • 1 + r(t). (<label>30</label></formula><formula xml:id="formula_91">)</formula><p>Proof of Theorem A.8. We separately analyze each entry of α. Focusing on α v , to simplify the notation, we introduce a random variable φ such that</p><formula xml:id="formula_92">P(φ = M k ξ k ) = p vk . Denote u = e αv .</formula><p>Therefore, using Lemma A.6, we get that</p><formula xml:id="formula_93">du dt = πv e 2αv (e αv + M ) 2 V i=1 (p vi -l vi )(M β i -M i ξ i ).</formula><p>We take in β = c • 1 and expand the expression of du/dt. This gives us that</p><formula xml:id="formula_94">du dt = πv u 2 (u + M ) 2 V k=1 p vk e M k ξ k /(u+M ) M k ξ k - V k=1 p vk e M k ξ k /(u+M ) V k=1 p vk M k ξ k V k=1 p vk e M k ξ k /(u+M ) = πv u 2 (u + M ) 2</formula><p>Cov(e φ u+M , φ) Ee φ u+M .</p><p>Since both e x/(u+M ) and x are monotonically increasing with respect to x, du/dt ≥ 0. Therefore, u is monotonically increasing, and we have that</p><formula xml:id="formula_95">u(t) 2 [u(t) + M ] 2 ≥ u(0) 2 [u(0) + M ] 2 , Ee φ u(t)+M ≤ Ee φ u(0)+M .</formula><p>Meanwhile, the first and second order Taylor expansions of e φ/ (u+M ) give that</p><formula xml:id="formula_96">e φ u+M = 1 + θ 1 (φ)φ u + M , e φ u+M = 1 + φ u + M + θ 2 (φ) φ u + M 2 ,</formula><p>where both θ 1 (φ) and θ 2 (φ)φ 2 are monotonically increasing functions of φ. We also have the bound that</p><formula xml:id="formula_97">θ(φ) ≤ exp max k M k ξ k u(0) + M -1 / max k M k ξ k u(0) + M -1 = C θ .</formula><p>We choose δ as defined in Eq.( <ref type="formula" target="#formula_50">22</ref>). When t is sufficiently large, we have that</p><formula xml:id="formula_98">loss(α, β(t)) ≤ loss(α, β ⋆ ) + 1 min k∈V\T πk • 2δ 2 .</formula><p>The convexity further implies that for any β = θβ(t) + (1 -θ)β ⋆ (θ ∈ (0, 1)), we have that</p><formula xml:id="formula_99">loss(α, β) ≤ loss(α, β ⋆ ) + 1 min k∈V\T πk • 2δ 2 .</formula><p>Denote lv = l v (α, β) as l evaluated on (α, β). Using the definition of the KL-divergence in Eq. ( <ref type="formula" target="#formula_40">20</ref>), we have that</p><formula xml:id="formula_100">V v=1 πv KL(p v || lv ) = loss(α, β(t)) -loss(α, β ⋆ ) ≤ 1 min k∈V\T πk • 2δ 2 .</formula><p>This further implies that KL(p v || lv ) ≤ 2δ 2 for any v. Using Pinsker's inequality, we get that</p><formula xml:id="formula_101">V k=1 |p vk -lvk | = ∥p v -l v ∥ TV ≤ KL(p v || lv )/2 ≤ δ. Therefore, max v,k p vk -lvk ≤ δ. Lemma A.5 gives that V v=1 βv (t) = 0. Therefore, V v=1 β v (t)/V = β(0). The choice of β ⋆ guarantees that β ⋆ = β(0). This shows that β(t) -β ⋆ ⊥ 1. Using Lemma A.3, there exists ω &gt; 0 such that (β(t) -β ⋆ ) ⊤ ∇ 2 β loss(α, β)(β(t) -β ⋆ ) = (β(t) -β ⋆ ) ⊤ V k=1 πk G L k (β(t) -β ⋆ ) ≥ ω 2 ∥β(t) -β ⋆ ∥ 2 2 .</formula><p>Using Taylor expansion, we have that</p><formula xml:id="formula_102">loss(α, β ⋆ ) -loss(α, β(t)) = -∇ β loss(α, β(t))(β(t) -β ⋆ ) + 1 2 (β(t) -β ⋆ ) ⊤ ∇ 2 β loss(α, β)(β(t) -β ⋆ ) ≥ -∇ β loss(α, β(t))(β(t) -β ⋆ ) + ω 2 ∥β(t) -β ⋆ ∥ 2 2 ≥ - 1 2ω ∥∇ β loss(α, β(t))∥ 2 2 .</formula><p>This shows that loss(α, β(t)) satisfies the Polyak-Lojasiewicz (PL) condition <ref type="bibr" target="#b32">(Karimi et al., 2016)</ref> when t is sufficiently large. This proves Theorem A.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The Linear Growth of the Residual States</head><p>B.1 The minimal model structure to recapitulate residual state peak We give more details for the claim in Section 2.3, stating that "The residual-state peaks require a threelayer structure." Figure 11 presents the difference of residual norms between the ⟨s⟩ token and others (∥Res ⟨s⟩ ∥ -E v̸ =⟨s⟩ [∥Res v ∥]), with different combinations of model structures. The 3 × TF and 2 × TF + mlp are the architectures that demonstrate clear evidence of residual state peaks. 2 × TF 3 × TF 2 × TF+MLP MLP+TF+MLP Attn+TF+MLP 0 40 100 140 Res s E v s [ Res v ]  s , t ,\nTH t HE t ENo ? t s , t , \n T H t H E t E N o ? t 0 1 (b) Layer 1 s , t ,\nTH t HE t ENo ? t s , t , \n T H t H E t E N o ? t 0 1 (c) Layer 2 s , t ,\nTH t HE t ENo ? t s , t , \n T H t H E t E N o ? t 0 1  2. The layer-norm operations cause the fast decay of the magnitude of the gradients.</p><p>3. Adam induces diminishing gradients to be constant updates, leading to the linear growth for the norm of the residual state of the extreme token.</p><p>To support the claim, we use the simplified model in Section 2, including the residual state norm. Denote the layer-norm operation as LayerNorm. Heuristically, we can split the residual state Res ⟨s⟩ to a summation of two directions.</p><formula xml:id="formula_103">Res ⟨s⟩ = m • η + ε,</formula><p>where η, ε ∈ R V with ∥η∥ 2 = ∥ε∥ 2 = 1, and η ⊤ ε = ρ &gt; 0. The η corresponds to the direction of Key ⟨s⟩ in the original transformer, and ε corresponds to other directions. Assume that the attention logit from the token v to the ⟨s⟩ token in layer 1 is given by logit</p><formula xml:id="formula_104">v,⟨s⟩ = α v = αv η ⊤ LayerNorm(Res ⟨s⟩ ) = αv • m + ρ m 2 + 2mρ + 1 . (<label>32</label></formula><formula xml:id="formula_105">)</formula><p>We assume that the scalars m and α are trainable, quantifying the norm of the residual states and magnitude of attention sinks. In the loss function loss v as defined in Eq. ( <ref type="formula" target="#formula_19">10</ref>), we replace α v by the expression as in Eq. ( <ref type="formula" target="#formula_104">32</ref>), so that the loss function becomes a function of (α v , β, m), denoted as</p><formula xml:id="formula_106">loss v ( αv , β, m) = loss v (α v , β),</formula><p>We then consider the total loss as the average of the losses on each non-trigger token, weighted by its proportion in the stable distribution {π v } v∈V , given by</p><formula xml:id="formula_107">loss( α, β, m) = v∈V\T π v • loss v ( αv , β, m).<label>(33)</label></formula><p>Proposition B.1. Assume ξ v ≥ 0 for any v, {W k β k } k∈V are not all equal, and ρ &gt; 0. Fix β = 0, and consider the gradient flow of loss( α, β, m) over α and m. With any initial value αv (0) &gt; 0 for any v and m(0) &gt; 0, we have that</p><formula xml:id="formula_108">ṁ(t) = O log t √ tm 3 .</formula><p>Proof of Proposition B.1. The chain rule gives that</p><formula xml:id="formula_109">αv (t) = αv • m + ρ m 2 + 2mρ + 1 ,<label>and</label></formula><formula xml:id="formula_110">ṁ(t) = V v=1 αv αv • dLayerNorm(Res ⟨s⟩ ) dt .</formula><p>With the initial values, ṁ(t) ≥ 0 and αv (t) ≥ 0. We have m(t) ≥ 0 for any t. Hence,</p><formula xml:id="formula_111">αv ∈ [ρ αv , αv ].</formula><p>Therefore, α = 2 -1 log t1 + r(t) with r(t) uniformly bounded over time. Furthermore, we have that</p><formula xml:id="formula_112">ṁ(t) = V v=1 αv αv • dLayerNorm(Res ⟨s⟩ ) dt = O log t √ t • 1 -ρ 2 (m 2 + 2mρ + 1) 3/2 = O log t √ tm 3 .</formula><p>This proves Proposition B.1.</p><p>We use simulation to demonstrate the effect of Adam. We train the scalar m using Adam with gradient dm = log t/[ √ tm 3 ]. We set β 1 = 0.9, β 2 = 0.999, weight decay= 10 -8 , and the learning rate lr = 0.3. Figure <ref type="figure" target="#fig_18">15</ref> presents the training dynamics of m. We observe the linear growth after a warming-up phase. In contrast, when trained by SGD with learning rate lr = 0.3, m remains small. The results match transformer models on BB-task as in Figure <ref type="figure" target="#fig_4">7c</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Experimental details</head><p>We provide more details for experiments in Section 2. We train transformers with positional embedding, prelayer norm, SoftMax activation in attn, and ReLU activation in mlp. We use Adam with constant learning rate 0.0003, β 1 = 0.9, β 2 = 0.99, ε = 10 -8 , and a weight decay of 0.01. We choose a learning rate of 0.03 for the SGD. In each training step, we resample from the BB task with a batch size of B = 512 and sequence length N = 256. Unless otherwise specified, the model is trained for 10, 000 steps. Results are consistent across different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional attention plots of a 1-layer transformer trained on the BB task</head><p>We provide more attention plots of the 1-layer transformer on sequences other than those shown in Figure <ref type="figure" target="#fig_2">2b</ref>.</p><p>Figure <ref type="figure" target="#fig_20">16</ref> presents more attention-weight heat maps of the one-layer transformer model trained on the BB task. All attention maps show the attention sink phenomenon. Some non-trigger tokens present attention patterns other than attention sink. For example, trigger tokens serve as attention sinks in some inputs in Figure <ref type="figure" target="#fig_20">16c</ref>. s , t , \n T H t H E t E N o ? t s , t , \n T H t H E t E N o ? t 0 1 (b) Sequence 1 s f u t u s . \n\n H a d y , t , s f u t u s . \n \n H a d y , t , 0 1 (c) Sequence 2 s i s t s ? t ? t ? \n T I V I f s i s t s ? t ? t ? \n T I V I f 0 1 C.4 The Bigram-Backcopy task without the ⟨s⟩ token.</p><p>We train a one-layer transformer on the BB task without the ⟨s⟩ token. Figure <ref type="figure" target="#fig_22">19</ref> shows that the zeroth token is not a sink token. Instead, trigger tokens and delimiter tokens seem to become sink tokens. In particular, the observation that delimiter tokens become extreme matches the observation in LLMs that delimiter tokens may also become extreme tokens (cf. Appendix G.2). , t ,\nTH t HE t ENo ? t ? , t , \n T H t H E t E N o ? t ? 0 1 (b) Value state norms , t , \n T H t H E t Tokens 0 5 10 15 20 Norms of Value States    the alignment of different states is caused purely by the token's global importance or meaning imparted via pretraining. The ⟨s⟩ token has no semantic meaning in the context of prose tokens, so its key state is not aligned with key states of meaningful prose tokens. Also, delimiter tokens, often considered secondary attention sinks (cf. Appendix G.2), have the most aligned key states to the key state of the ⟨s⟩ token, and are also the tokens with the least semantic meaning in the prose context. Thus, we identify that, at least in this restricted example, query state and key state alignment depends heavily on the contextual semantics of the token. We perform a series of ablations to understand which components of the network promote the residual state peaks. We find that ablating either the zeroth or first layer's MLP is sufficient to remove the residual state peak phenomenon, while no other layer-level ablation can do it.</p><p>Value-state drains. The value states of the ⟨s⟩ token at Layer 0 Head 31 are already near zero, as demonstrated in Figure <ref type="figure" target="#fig_26">24a</ref>. While the delimiter tokens, which are less semantically meaningful in the prose context, have smaller value states than the rest, they are not as small as the value state of the ⟨s⟩ token which is guaranteed to not have any semantics.</p><p>Residual state peaks. Residual state peaks are caused by the first two layers' MLPs. In particular, we perform several ablations, comparing between the residual state norms in a later layer (24) of an un-edited forward pass versus forward passes where we force the output of either multiple layers, a single layer, an attention block, or an MLP to be zero (and hence remove its contribution from the residual stream). As shown in Figure <ref type="figure" target="#fig_26">24b</ref>, ablating either Layer 0's or Layer 1's MLP is sufficient to remove the residual state peak. In particular, the second-largest token at Layer 24 in each ablation (including the original setup) has norm between 29 and 38, so the interventions ensure that all tokens have similar size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Extreme-token phenomena in Llama 3.1. We evaluate the attention weights, value states norm, and residual states norm on the Llama 3.1-8B-Base model, where the input sentence is "⟨s⟩Summer is warm⟨period⟩ Winter is cold⟨period⟩". Left (a): The attention weights across multiple heads at Layer 24. We observe the attention sink phenomenon: the ⟨s⟩ token attracts a significant portion of the overall attention weight. Middle (b):The empirical distribution of the norms of value states over all layers and all heads. We exclude 2% of the outlier values to help visualization. We observe the value-state drain phenomenon: the value state of the ⟨s⟩ token is much smaller than those of other tokens on average. Right (c): The norm of the residual stream states, measured at the output of each layer. We observe the residual-state peak phenomenon: the ⟨s⟩ token's residual states have significantly larger norms than those of other tokens from layers 1 to 30. We present the extreme-token phenomena over other input sequences in Appendix F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experiments on the Bigram-Backcopy task. Left (a): The data generation procedure for the Bigram-Backcopy task. Here we fix 't', 'e', and the space character (' ') as trigger tokens. The BB task samples bigram transitions for non-trigger tokens and backcopies for trigger tokens. Middle (b): The attention map of a given prompt. Trigger tokens are marked in red. The attention head at non-trigger tokens is dormant and displays attention sinks. Right (c): The value state norms for the prompt. The ⟨s⟩ token has the smallest norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interventions and dynamics of one-layer transformer on the Bigram-Backcopy task. Left (a): Excess risks for a one-layer model trained on the Bigram-Backcopy (BB) task under various interventions. Right (b):The excess risks, attention weights, attention logits, and value state norms for the ⟨s⟩ token throughout the training dynamics. Each curve is rescaled to fall within a 0 to 1 range. On the right side of (b), the horizontal axis is logarithmically scaled. The ∆logit •,⟨s⟩ curve represents the mean of attention logits from all given non-trigger query tokens v on the ⟨s⟩ token, normalized by the mean of attention logits for other tokens. The shaded area represents the 90% uncertainty interval on the distribution over all non-trigger tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Left (a): The training dynamics of the single-layer ReLU attention transformer on the BB task. Middle (b): The intervention results on the attn+mlp+attn+mlp+mlp architecture. The attention sink and value-state peak of the middle attn layer disappear after zeroing out attn + mlp of layer 0. Right (c): The evolution of massive norms in a three-layer transformer trained with Adam, SGD, and using a ReLU attention transformer. Notably, only the three-layer model with Softax attention trained using Adam results in the formation of residual-state peaks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Figure7cshows the residual-state norms of the ⟨s⟩ token at the layer 0 output of three-layer transformers during pre-training on the BB task. The results indicate that training the transformer with Adam leads to a linear increase in residual state norms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Active-dormant mechanism of Layer 16 Head 25 (L16H25) of Llama 2-7B-Base. We observe that L16H25 is active on GitHub data and dormant on Wikipedia data, both sourced from RedPajama-1T (Computer, 2023). Left (a): Attention weights of L16H25, prompted by three randomly selected samples from each domain. Right (b): Results of an intervention study showing the change in cross-entropy loss when the output of L16H25 (specifically, its value states) is set to zero across sequences in both domains. The findings indicate that the model's performance for GitHub data, measured by cross-entropy loss, strongly relies on the output of this attention head.</figDesc><graphic coords="12,83.91,167.50,244.29,84.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Attention weights, value state norms, and residual state norms of Layer 24 during the training dynamics of OLMo. Left (a):The total attention mass on extreme tokens ⟨s⟩ and "Delim"(⟨period⟩) at Layer 24, averaged across all attention heads. The horizontal axis is logarithmically scaled after step 10k. We observe a rapid increase followed by stabilization within the range [0.9, 1] for the rest of training, consistent with our predictions. Middle (b): The value state norms of each token at Layer 24 during training, averaged over all heads. The horizontal axis is logarithmically scaled after step 10k. Initially, the value states of all tokens shrink, eventually converging, while the value states of the extreme tokens shrink to significantly lower levels compared to other tokens. Figure (a) and (b) coincide with the trends in Figure 4b under the BB task. Right (c): The residual state norms of each token at Layer 24 during training. The residual state norm of ⟨s⟩ increases linearly in magnitude throughout training, matching Figure 7c in the BB task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>Figure (a) and (b) coincide with the trends in Figure 4b under the BB task. Right (c): The residual state norms of each token at Layer 24 during training. The residual state norm of ⟨s⟩ increases linearly in magnitude throughout training, matching Figure 7c in the BB task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Attention logits of Layer 24. Left (a): Attention logits difference of all tokens' query states against ⟨s⟩'s key state during training. The difference in attention logits is computed as ∆logit •,⟨s⟩ = Qry ⊤ • Key ⟨s⟩ -Mean[Qry ⊤• Key Others ]. The horizontal axis is logarithmically scaled after step 10k. We observe that ∆logit •,⟨s⟩ increases approximately in logarithmic scale during training steps 10k to 100k, matching the decreasing phase of the value states in Figure9b. Right (b): Attention logits of the last token's query state against all token's key states for pretrained OLMo. In this experiment, we generate 128 randomly sampled test tokens with IDs from 100 to 50000 in the OLMo tokenizer. We append each token separately to the test phrase "Summer is warm⟨period⟩ Winter is cold⟨period⟩", creating 128 different samples, which we feed to the LLM to examine the model behavior. We plot the distribution of (un-shifted) attention logits logit •,v = Qry ⊤ test Key v across all heads at Layer 24 and all test tokens. The distribution of logit •,⟨s⟩ and logit •,Delim have considerably small variance compared with other logits, confirming the sink-logits concentration phenomenon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Preliminaries and notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Extreme-token Phenomena in the Bigram-Backcopy Task 5 2.1 One-layer transformer exhibits attention sinks and value-state drains . . . . . . . . . . . . . . 5 2.2 Analysis of a minimally-sufficient transformer architecture . . . . . . . . . . . . . . . . . . . . 7 2.3 The emergence of residual-state peaks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3 Extreme-token Phenomena in pretrained LLMs 12 3.1 Active-dormant mechanism in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Extreme-token phenomena along training dynamics of LLMs . . . . . . . . . . . . . . . . . . 13 4 Conclusions 14 A Proofs of Theorem 1 and 2 21 A.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.2 Proof of Theorem 2(c): Stable phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.3 Proof of Theorem 2(a): Attention sinks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 A.4 Proof of Theorem 2(b): Value-state drains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B The Linear Growth of the Residual States 31 B.1 The minimal model structure to recapitulate residual state peak . . . . . . . . . . . . . . . . 31 B.2 Additional plots for the three-layer transformer trained on BB task . . . . . . . . . . . . . . . 31 B.3 Potential mechanism for linear growth of the residual state peak in multi-layer models . . . . 31 C Ablations 34 C.1 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 C.2 Additional attention plots of a 1-layer transformer trained on the BB task . . . . . . . . . . . 34 C.3 Statics and dynamics of the simplified model in Theorem 2 . . . . . . . . . . . . . . . . . . . 34 C.4 The Bigram-Backcopy task without the ⟨s⟩ token. . . . . . . . . . . . . . . . . . . . . . . . . 34 D More Attention Heads in Dormant and Active Phase 36 E Fine-Grained Static Mechanisms for Extreme-Token Phenomena 37 F Extreme-Token Phenomena Over Many Samples 39 G Assorted Caveats 41 G.1 Multiple attention sinks vs. one attention sink . . . . . . . . . . . . . . . . . . . . . . . . . . 41 G.2 The role of a fixed ⟨s⟩ token in the Active-Dormant mechanism . . . . . . . . . . . . . . . . 41</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>∂α v= 0 for any i, v, α, and β,V v=1∂l ik ∂β v = 0 for any i, k, α, and β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>jk e αj l jv e αj + M -e αj 1{k = v} e αj + M = V j=1 πj e αj [l jv -p jv ] e αj + M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>p</head><figDesc>vi exp e α c e α +M V k=1 p vk exp e α c e α +M = p vi .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Minimal structures to elicit residual state peaks. We use A + B + C to indicate the model with structure A, B, C in layers 0, 1, and 2, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Attention weight patterns of three-layer transformer trained on the BB task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: Value state norms of three-layer transformer trained on the BB task (a) Layer 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: With the gradient formula in Proposition B.1, Adam causes linear growth of m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Additional attention plots of the one-layer transformer trained on the Bigram-Backcopy task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Figure 17: The simplified model structure trained on the BB task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Attention weights and value state norms of a one-layer transformer trained on the BB task without the ⟨s⟩ token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Layer 16 Head 20 of Llama 2-7B-Base. We do not observe difference between the Wikipedia data and the Github data.</figDesc><graphic coords="36,72.00,244.85,269.10,93.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Layer 16 Head 28 of Llama 2-7B-Base. The head is more dormant on the GitHub data, and more active on the Wikipedia data.</figDesc><graphic coords="36,76.19,492.03,269.10,93.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 24 :</head><label>24</label><figDesc>Figure24: Left (a): Value-state drains at Layer 0 Head 31 of Llama 3.1-8B-Base. We observe that the value state associated with ⟨s⟩ is already much smaller than every other semantically meaningful token, and still smaller than the delimiter tokens in the same sentence. Right (b): Ablation study on the cause of the residual state peak in Llama 3.1-8B-Base. We perform a series of ablations to understand which components of the network promote the residual state peaks. We find that ablating either the zeroth or first layer's MLP is sufficient to remove the residual state peak phenomenon, while no other layer-level ablation can do it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="41,135.18,459.21,341.63,180.39" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We define the value state as OVH rather than VH since what is added to the residual state is essentially OVH.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We note that Reddy (2023) makes a similar simplification in analyzing induction heads.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We did not analyze Llama for dynamics, as they do not provide open-source intermediate checkpoints along pretraining.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For the single 150,000-step checkpoint, we observed that its statistics were outliers, which we hypothesize is due to a system failure. We address this by using the average of nearby checkpoints to represent its statistics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Note that OLMo does not have a ⟨s⟩ token, but attention sinks still form in the majority of heads. In particular, the first token always behaves as an attention sink. We discuss this further in Appendix G.2.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>TG thanks <rs type="person">Yaodong Yu</rs>, <rs type="person">Licong Lin</rs>, and <rs type="person">Ruiqi Zhang</rs> for insightful discussions. YB thanks <rs type="person">Caiming Xiong</rs> and <rs type="person">Huan Wang</rs> for the many insightful discussions in the early stages of this work. This project is supported by <rs type="funder">NSF</rs> <rs type="grantNumber">DMS-2210827</rs>, <rs type="grantNumber">CCF-2315725</rs>, <rs type="grantNumber">CAREER DMS-2339904</rs>, <rs type="grantNumber">ONR N00014-24-S-B001</rs>, a <rs type="institution">UC Berkeley College of Engineering fellowship</rs>, an <rs type="funder">Amazon Research Award</rs>, a <rs type="grantName">Google Research Scholar Award</rs>, an Okawa <rs type="grantName">Foundation Research Grant</rs>, and the <rs type="funder">European Union</rs> (<rs type="grantNumber">ERC-2022-SYG-OCEAN-101071601</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5tj63QY">
					<idno type="grant-number">DMS-2210827</idno>
				</org>
				<org type="funding" xml:id="_epJ3Tph">
					<idno type="grant-number">CCF-2315725</idno>
				</org>
				<org type="funding" xml:id="_wxTj7b7">
					<idno type="grant-number">CAREER DMS-2339904</idno>
				</org>
				<org type="funding" xml:id="_82wVNRW">
					<idno type="grant-number">ONR N00014-24-S-B001</idno>
					<orgName type="grant-name">Google Research Scholar Award</orgName>
				</org>
				<org type="funding" xml:id="_JkJ8q58">
					<idno type="grant-number">ERC-2022-SYG-OCEAN-101071601</idno>
					<orgName type="grant-name">Foundation Research Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Therefore, we get two more inequalities:</p><p>We bound du/dt and get that du dt ≤ πv Cov(e φ u+M , φ)</p><p>By solving the ODE, we get that</p><p>To give a lower bound, we have that</p><p>Therefore, u ≥ Ct + C2 . In conclusion, we have that</p><p>with r v bounded. This proves Theorem A.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 2(b): Value-state drains</head><p>Theorem A.9 (Restatement of Theorem 2(b)). Assume ξ v ≥ 0 for any v, π v &gt; 0 for any v ∈ V, and</p><p>Consider the gradient flow of the loss function loss(α, β) over β for fixed α, i.e.,</p><p>Proof of Theorem A.9. We plug β ⋆ into the loss and get that loss(α,</p><p>where G L k is defined in Eq. ( <ref type="formula">21</ref>). Lemma A.1 indicates that it is positive semi-definite. Therefore, we have that loss(α, β(t)) → loss(α, β ⋆ ) as t → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Fine-Grained Static Mechanisms for Extreme-Token Phenomena</head><p>In this section, we identify more fine-grained static mechanisms for extreme-token phenomena in Llama 3.1-8B-Base. To do this, we identify circuits for the origin of attention sinks and small value states. Then, using ablation studies, we study the origin of massive norms.    We observe that the key state of ⟨s⟩ have low correlation with other key states, but high correlation with other query states. Meanwhile, all semantically meaningful (i.e., not delimiter) tokens have highly correlated key states.</p><p>Attention sinks and global contextual semantics. There are many attention heads that exhibit attention sinks at layer 0, and the ⟨s⟩ token is always the sink token (see Figure <ref type="figure">22</ref>). From now on until the end of this section, we restrict our attention to Head 31 of Layer 0, which is an attention sink. These attention sinks are caused by two linear-algebraic factors, demonstrated in Figure <ref type="figure">23</ref>.</p><p>1. The key state of the ⟨s⟩ token has small dot product with all other key states.</p><p>2. The query states of all tokens are nearly orthogonal to the key states of all tokens except the ⟨s⟩ token.</p><p>These two facts combine to ensure that the key state of the ⟨s⟩ token is picked out by each query state, causing the attention sink. Since these query and key states are produced without any cross-token interaction,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Extreme-Token Phenomena Over Many Samples</head><p>In this section we show that the extreme-token phenomena, and our predictions from the BB model, exhibit in prompts other than "Summer is warm. Winter is cold." To this end, we use 128 samples from the Wikipedia dataset, each truncated to 8 tokens. Figure <ref type="figure">25</ref> provides aggregate statistics of extreme-token phenomena in Llama 3.1-8B, which are similar to the fine-grained statistics over a single prompt from Figure <ref type="figure">1</ref>. Figure <ref type="figure">26</ref> provides aggregate statistics of the development of extreme-token phenomena over the training dynamics of OLMo, which are similar to the fine-grained statistics over a single prompt from Figure <ref type="figure">9</ref> and Figure <ref type="figure">10</ref>. Figure <ref type="figure">25</ref>: Extreme token phenomena over many samples in Llama 3.1-8B-Base. Left (a): Let A be the attention weight tensor, of shape (batch size=128, # heads=32, # tokens=8, # tokens=8) at Layer 24 of Llama 3.1-8B-Base. We calculate the tensor A, of shape (batch size=128, # heads=32, # tokens=8), which measures the average attention mass on the key tokens, by the following calculation:</p><p>We expect, for an attention sink head h on sample b, that A bh0 is large, and A bhj is small for all j ≥ 1. We indeed see this by plotting the distribution of A :,:,j for each j, which shows that almost all attention mass is concentrated on the ⟨s⟩token with high probability, showing the same thing as the individual attention head analysis in Figure <ref type="figure">1</ref>   We then study the dynamics of sink tokens versus non-sink tokens. In these experiments we observe that token 0 is (almost) always a sink token, which we discuss further in Appendix G.2. Top left (a): The average attention scores A bhj for j as a sink token versus non-sink tokens. We observe that attention sinks form in nearly all heads and samples: the attention mass on top tokens nearly always sums to 1, and moreover the sinks develop relatively early in training. Top right (b): We observe that the normalized attention logits of non-sink tokens initially increase until the formation of an attention sink, and then approximately converge to a stable phase with similar logits on token 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom left (c):</head><p>We observe that the value states of all tokens except the first sink token (token 0) rapidly converges to steady state, while the first sink token has a much lower value state norm than all other tokens. Bottom right (d):</p><p>We observe that the norm of the residual state of token 0 increases linearly during pretraining, while all other tokens' residual states do not. Our results mirror and confirm the single-sample detailed analysis conducted in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Assorted Caveats</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Multiple attention sinks vs. one attention sink</head><p>As we have seen, attention heads in the BB task (Section 2), Llama 2-7B-Base (Section 3.1), and OLMo (Section 3.2) exhibit multiple sink tokens. That is, when heads in these models are dormant, they tend to have two sink tokens. For the LLMs in this group, at least on prose data, the ⟨s⟩ token as well as the first delimiter token (e.g., representing . or ;) are sink tokens. Meanwhile, Llama-3.1-8B-Base (Section 3) only ever has one attention sink on prose data, and the ⟨s⟩ token is always the sink token. Here, we offer a possible explanation of this phenomenon. For the BB task, multiple sink tokens are necessary to solve the task. For LLMs, we believe this distinction may be explained by the relative proportion of coding data, in which delimiters have a greater semantic meaning than prose, within the training set. For instance, OLMo was trained on DOLMA <ref type="bibr" target="#b47">(Soldaini et al., 2024)</ref>, which has around 411B coding tokens. Meanwhile, Llama 2 used at most (2T × 0.08 =) 0.16T coding tokens. Finally, Llama 3.1 used around (15.6T × 0.17 =) 2.6T coding tokens <ref type="bibr" target="#b14">(Dubey et al., 2024)</ref>. On top of the raw count being larger, coding tokens are a larger proportion of the whole pretrtraining dataset for Llama 3.1 compared to other model families. Thus, during training, the presence of delimiters would not be considered unhelpful towards next-token prediction, since such delimiters carry plenty of semantics in a wide variety of cases. Our earlier hypothesis in Section 3.1 proposes that only tokens which lack semantics in almost all cases are made to be sink tokens. This could be a reason for the distinction.</p><p>G.2 The role of a fixed ⟨s⟩ token in the Active-Dormant mechanism Some models, such as OLMo, are not trained with a ⟨s⟩ token. Despite this, the first token of the input still frequently develops into a sink token. We can study the effect of positional encoding of the tokens on the attention sink phenomenon by shuffling the tokens before inputting them into the transformer, and observing how and why attention sinks form. If we do this with the phrase "Summer is warm. Winter is cold." with OLMo, we observe that at Layer 24, there are many attention sink heads where the first token and first delimiter token share attention mass, even if the sentence is jumbled up and makes no grammatical sense. This points towards the observation that without a ⟨s⟩ token, the attention sink formation uses both positional data and, to a greater degree, the semantic data of each token. We leave studying this effect in greater detail to future work.</p><p>Figure <ref type="figure">27</ref>: Attention sinks with shuffled input in Layer 24 of OLMo. In order to understand the impact of positional encodings when there is no ⟨s⟩ token, we shuffle the input of the test string "Summer is warm. Winter is cold." in OLMo. We observe that there is still an attention sink on token 0, despite it being a random token that does not usually start sentences or phrases (since it is uncapitalized). This shows that the positional embedding, say via RoPE, has a large impact on the formation of attention sinks -when the semantics of each token have switched positions, the attention sink still forms on the zeroth token.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Linear attention is (maybe) all you need (to understand transformer optimization)</title>
		<author>
			<persName><forename type="first">Kwangjun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhak</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01082</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transformers learn to implement preconditioned gradient descent for in-context learning</title>
		<author>
			<persName><forename type="first">Kwangjun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 1, context-free grammar</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13673</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Birth of a transformer: A memory viewpoint</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Cabannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding and overcoming the challenges of efficient transformer quantization</title>
		<author>
			<persName><forename type="first">Yelysei</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12948</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantizable transformers: Removing outliers by helping attention heads do nothing</title>
		<author>
			<persName><forename type="first">Yelysei</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="75067" to="75096" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What is my math transformer doing? Three results on interpretability and generalization</title>
		<author>
			<persName><forename type="first">François</forename><surname>Charton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00170</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06764</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Together</forename><surname>Computer</surname></persName>
		</author>
		<author>
			<persName><surname>Redpajama</surname></persName>
		</author>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<title level="m">An open source recipe to reproduce Llama training dataset</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vision transformers need registers</title>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16588</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the optimization and generalization of multi-head attention</title>
		<author>
			<persName><forename type="first">Puneesh</forename><surname>Deora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rouzbeh</forename><surname>Ghaderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Thrampoulidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12680</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The case for 4-bit precision: k-bit inference scaling laws</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7750" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">8-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<title level="s">LLM.int</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30318" to="30332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The Llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mathematical framework for transformer circuits</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transformer Circuits Thread</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Privileged bases in the transformer residual stream</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training with quantization noise for extreme model compression</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07320</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jiahai</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17191</idno>
		<title level="m">How do language models bind entities in context? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How do language models put attention weights over long context? Yao Fu&apos;s Notion</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14767</idno>
		<ptr target="https://yaofu.notion.site/" />
	</analytic>
	<monogr>
		<title level="m">How-Do-Language-Models-put-Attention-Weights-over-Long-Context-10250219d5ce42e8b465087c383a034e? pvs=4. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models</title>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of quantization methods for efficient neural network inference</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Low-Power Computer Vision</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="291" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">When attention sink emerges in language models: An empirical view</title>
		<author>
			<persName><forename type="first">Xiangming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengzhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.10781</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How do transformers learn in-context beyond simple functions? A case study on learning with representations</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10616</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention score is not all you need for token importance indicator in KV cache reduction: Value also matters</title>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.12335</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Wes</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Horsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Carl Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">Rezaei</forename><surname>Kheirkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hathaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12181</idno>
		<title level="m">Universal neurons in GPT2 language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">LM-Infinite: Simple on-the-fly length generalization for large language models</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.16137</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Yao-Chieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hsuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Po</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.03828</idno>
		<title level="m">Outlier-efficient hopfield layers for large transformer-based models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05249</idno>
		<title level="m">-context convergence of transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmeticonly inference</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016</title>
		<meeting><address><addrLine>Riva del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">September 19-23, 2016. 2016</date>
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Transformers are minimax optimal nonparametric in-context learners</title>
		<author>
			<persName><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename><surname>Nakamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12186</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haobo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingzhi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linzhan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Duquant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01721</idno>
		<title level="m">Distributing outliers via dual transformation makes stronger quantized llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Awq: Activation-aware weight quantization for on-device llm compression and acceleration</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="87" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining</title>
		<author>
			<persName><forename type="first">Licong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08566</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ye</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08034</idno>
		<title level="m">Towards fully 8-bit integer inference for the transformer model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ruikang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoli</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01241</idno>
		<title level="m">Intactkv: Improving large language model quantization by keeping pivot tokens intact</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards understanding grokking: An effective theory of representation learning</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouail</forename><surname>Kitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Niklas S Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="34651" to="34663" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Locating and editing factual associations in gpt</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17359" to="17372" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Fournarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Ali Amjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelysei</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08295</idno>
		<title level="m">A white paper on neural network quantization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05217</idno>
		<title level="m">Progress measures for grokking via mechanistic interpretability</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How transformers learn causal structure with gradient descent</title>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14735</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<title level="m">-context learning and induction heads</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The mechanistic basis of data dependence and abrupt learning in an in-context classification task</title>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Prefixing attention sinks can mitigate activation outliers for large language model quantization</title>
		<author>
			<persName><forename type="first">Seungwoo</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woohyun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuyeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.12016</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17762</idno>
		<title level="m">Massive activations in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scan and Snap: Understanding training dynamics and token composition in 1-layer transformer</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="71911" to="71947" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00535</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Millicent</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15213</idno>
		<title level="m">Function vectors in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">High-Dimensional Probability: An Introduction with Applications in Data Science</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Variengien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buck</forename><surname>Shlegeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00593</idno>
		<title level="m">Interpretability in the wild: a circuit for indirect object identification in GPT-2 small</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">How many pretraining tasks are needed for in-context learning of linear regression</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08391</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Understanding int4 quantization for language models: latency speedup, composability, and failure cases</title>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="37524" to="37539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Smoothquant: Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickael</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="38087" to="38099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Efficient streaming language models with attention sinks</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17453</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Efficient and affordable post-training quantization for large-scale transformers</title>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y Zeroquant</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.01861" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Zeroquant: Efficient and affordable post-training quantization for large-scale transformers</title>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27168" to="27183" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration</title>
		<author>
			<persName><forename type="first">Zhongzhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huihong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Celine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2406.15765</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Q8bert: Quantized 8bit bert</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="36" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Stabilizing transformer training by preventing attention entropy collapse</title>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="40770" to="40803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Trained transformers learn linear models in-context</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Frei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09927</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">In-context learning of a linear transformer block: Benefits of the MLP component and one-step GD initialization</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14951</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Unveiling transformers with LEGO: A synthetic reasoning task</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arturs</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04301</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Yuandong Tian, and Stuart Russell. Towards a theoretical understanding of the &apos;reversal curse&apos; via training dynamics</title>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baihe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaolun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04669</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 3.1, knowledge storage and extraction</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14316</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
