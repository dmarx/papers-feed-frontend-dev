{
  "arxivId": "2410.13835",
  "title": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs",
  "authors": "Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei",
  "abstract": "Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.",
  "url": "https://arxiv.org/abs/2410.13835",
  "issue_number": 0,
  "issue_url": "",
  "created_at": "2025-01-04T06:52:39.613983",
  "state": "open",
  "labels": [
    "paper"
  ],
  "total_reading_time_seconds": 16,
  "last_read": "2025-01-04T06:52:39.614755",
  "last_visited": "2024-12-30T22:17:13.815000+00:00",
  "main_tex_file": null,
  "published_date": "2024-10-17T17:54:06Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}