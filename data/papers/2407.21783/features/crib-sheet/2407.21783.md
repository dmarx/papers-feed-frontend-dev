- **Model Overview**: Llama 3 is a herd of foundation language models with 8B, 70B, and 405B parameters, designed for multilinguality, coding, reasoning, and tool usage.
  
- **Architecture**: The largest model (405B parameters) is a dense Transformer with:
  - **Layers**: 126
  - **Model Dimension**: 16,384
  - **FFN Dimension**: 53,248
  - **Attention Heads**: 128
  - **Peak Learning Rate**: \(8 \times 10^{-5}\)
  - **Vocabulary Size**: 128,000 tokens

- **Context Window**: Supports a context window of up to 128K tokens.

- **Training Data**: Pre-trained on a corpus of approximately 15T multilingual tokens, significantly more than Llama 2's 1.8T tokens.

- **Training Scale**: Utilized \(3.8 \times 10^{25}\) FLOPs for pre-training, nearly 50Ã— more than Llama 2.

- **Key Development Levers**:
  - **Data**: Improved quality and quantity through rigorous pre-processing and filtering.
  - **Scale**: Larger model size and longer training duration for smaller models.
  - **Complexity Management**: Standard dense Transformer architecture for stability; simple post-training procedures (SFT, RS, DPO).

- **Safety Features**: Introduction of Llama Guard 3 for input and output safety.

- **Multimodal Capabilities**: Ongoing development of models for image, video, and speech recognition through:
  - **Vision Adapter**: Integrates image encoder into language model using cross-attention layers.
  - **Speech Adapter**: Converts speech encodings into token representations for language model input.

- **Pre-Training Process**:
  - **Data Curation**: Involves PII filtering, text extraction, and de-duplication (URL, document, line-level).
  - **Heuristic Filtering**: Removes low-quality documents based on various criteria (e.g., "dirty word" counting, token distribution).
  - **Quality Filtering**: Utilizes model-based classifiers to select high-quality tokens.

- **Scaling Laws**: Models are designed according to scaling laws to optimize performance based on training budget.

- **Performance Evaluation**: Llama 3 models perform comparably to leading models like GPT-4 across various benchmarks, with smaller models outperforming alternatives with similar parameters.

- **Public Release**: All models are released under the Llama 3 Community License, promoting innovation in AI research.

- **Future Work**: Multimodal models for image, video, and speech are still under development and not yet released.