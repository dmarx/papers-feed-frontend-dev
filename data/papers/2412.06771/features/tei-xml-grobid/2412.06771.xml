<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</title>
				<funder ref="#_AhY6JJB">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-09">9 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meera</forename><surname>Hahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contributions</orgName>
								<address>
									<addrLine>1 Google DeepMind</addrLine>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nithish</forename><surname>Kannen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rich</forename><surname>Galt</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<addrLine>5 Google DeepMind</addrLine>
									<settlement>London Cambridge</settlement>
									<region>MA</region>
									<country>UK USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kartikeya</forename><surname>Badola</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<addrLine>5 Google DeepMind</addrLine>
									<settlement>London Cambridge</settlement>
									<region>MA</region>
									<country>UK USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zi</forename><surname>Wang</surname></persName>
							<email>wangzi@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contributions</orgName>
								<address>
									<addrLine>1 Google DeepMind</addrLine>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-09">9 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">1D8813B6281A99D8BDADFD3C266FFC46</idno>
					<idno type="arXiv">arXiv:2412.06771v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A fundamental challenge in the development of AI agents is how to foster effective and efficient multiturn communication and collaboration with human users to achieve user-defined goals, especially when faced with the common issue of vague or incomplete instructions from humans. We focus specifically on text-to-image (T2I) generation, where recent advancements <ref type="bibr" target="#b3">(Baldridge et al., 2024;</ref><ref type="bibr" target="#b4">Betker et al., 2023;</ref><ref type="bibr" target="#b66">Podell et al., 2023;</ref><ref type="bibr">Yu et al., 2023)</ref> have enabled the creation of stunning images from complex text descriptions. However, users often struggle to describe the image they would like to generate in a way that T2I systems can fully understand. This leads to unsatisfactory results and repeated iterations of prompts.</p><p>The prompt underspecification problem arises from the inherent ambiguity of natural language, the different assumptions that humans make and the vast space of potential images that can be generated from a single prompt <ref type="bibr">(Hutchinson et al., 2022)</ref>. Imagine a prompt generate an image of a rabbit next to a cat. This seemingly simple prompt leaves many important aspects underspecified: What kind of rabbit? What color is the cat? What is their relative positions? What is the background? While a T2I model can generate an image with a rabbit and a cat in it, it is unlikely that the image captures the specific details a specific user has in mind. For example, people in Holland might assume it is common for rabbits to have lop ears, but people in New England might expect to see a cottontail rabbit with straight ears. The combination of all these factors can lead to a frustrating cycle of trial-and-error, with the user repeatedly refining their prompt in an attempt to steer the model towards the desired output <ref type="bibr" target="#b37">(Huang et al., 2024;</ref><ref type="bibr" target="#b72">Sun and Guo, 2023;</ref><ref type="bibr">Vodrahalli and Zou, 2024)</ref>.</p><p>Instead of relying on passive T2I models that simply generate images based on potentially vague user instructions, we pursue a quest for agency in T2I generation. The T2I agents should actively engage with human users to provide a collaborative and interactive experience for image creation. We envision that these T2I agents will be able to <ref type="bibr">(1)</ref> express and visualize their beliefs and uncertainty about user intents, (2) allow human users to directly control their beliefs beyond just text descriptions, and (3) proactively seek clarification from the human user to iteratively align their understanding with what the human user intends to generate.</p><p>In this work, we develop simple prototypes of such agents. At the core of those agent prototypes, we build in a graph-based symbolic belief state, named belief graph, for agents to understand its own uncertainty about possible entities (e.g., rabbit) that might appear in the image, attributes of entities (e.g., rabbit's color), relations between entities and so on. Given a user prompt, we use an LLM and constrain its generation to the graph structure of beliefs, which include probability estimates on the appearance of entities and the possible values for attributes and relations. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the interface and features of the prototypes. In particular, the agent can ask questions based on its uncertainty. For example, a very simple strategy is to find the most uncertain attribute of an entity (e.g., rabbit's color) and use an LLM to phrase a question about the attribute (e.g., What is the color of the rabbit?). The agent can also guide users to directly edit uncertain items in the graph.</p><p>Based on user answers to agent questions or direct edits in the belief graph, the agent uses an LLM to update the prompt. It also transitions to a new belief graph by modifying the uncertainty of items clarified by the user. Based on the updated prompt, the agent calls an off-the-shelf T2I model to generate images. The structure of our agent prototypes is highly modular, making it easy to improve each component individually, e.g., changing the strategy for asking questions, updating the belief graph construction method, and switching to better LLMs or T2I models when they become available.</p><p>To evaluate the utility of our agent prototypes, we conduct both automatic evaluations and human studies. We develop automatic evaluation pipelines to assess the effectiveness and efficiency of the T2I agents when interacting with simulated users with underspecified prompts answering questions based on their pre-fixed intents. The human studies aim to understand how helpful simple T2I agents can be and evaluate how good the agents' questions and generated images are.</p><p>We also create a hand-curated benchmark called DesignBench which contains aesthetic scenes with multiple entities and interactions between entities; it also contains both a short and long caption. DesignBench features diversity between photo-realism, animation and multiple styles allowing a robust testing with the use case of artists and designers in mind.</p><p>We run automatic evaluations on DesignBench, the COCO dataset <ref type="bibr" target="#b51">(Lin et al., 2014)</ref> and Im-ageInWords <ref type="bibr" target="#b22">(Garg et al., 2024)</ref>. Results show that our agents can achieve at least 2 times higher VQAScore <ref type="bibr" target="#b52">(Lin et al., 2024)</ref> than the traditional single-turn T2I generation within 5 turns of interaction. In our human study, over 90% human subjects expect proactive clarifications to be helpful, about 85% find belief graphs helpful, and 58% think the question asking feature of agents could deliver value to their work very soon, or immediately. Participants prefer images generated by our simple agents over those of single-turn T2I systems in more than 80% cases of the 550 prompt-image pairs used in the human study.</p><p>Our contributions: <ref type="bibr">(1)</ref> the first explainable and controllable belief graph used for T2I, (2) novel design and prototypes for T2I agents that adaptively ask clarification questions and present belief graphs; (3) a new automatic evaluation pipeline with simulated users to assess question-asking skills of T2I agents; and (4) DesignBench: a new T2I agent benchmark. <ref type="foot" target="#foot_0">1</ref> Appendix B details the novelty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>From the very outset of artificial intelligence, a core challenge has been to develop intelligent agents capable of representing knowledge and taking actions to acquire knowledge necessary for achieving their goals <ref type="bibr" target="#b55">(McCarthy and Hayes, 1969;</ref><ref type="bibr" target="#b56">Minsky, 1974;</ref><ref type="bibr" target="#b59">Moore, 1985;</ref><ref type="bibr" target="#b62">Nilsson, 2009;</ref><ref type="bibr" target="#b68">Russell and Norvig, 2016)</ref>. Our work is an attempt to address this challenge for intelligent T2I agents.</p><p>In machine learning and statistics, efficient data acquisition has been extensively studied for many problems, including active learning <ref type="bibr" target="#b13">(Cohn et al., 1996;</ref><ref type="bibr" target="#b21">Gal et al., 2017;</ref><ref type="bibr" target="#b34">Houlsby et al., 2011;</ref><ref type="bibr" target="#b67">Ren et al., 2021;</ref><ref type="bibr" target="#b69">Settles, 2009;</ref><ref type="bibr" target="#b78">Wang et al., 2018)</ref>, Bayesian optimization <ref type="bibr" target="#b2">(Auer, 2002;</ref><ref type="bibr" target="#b23">Garnett, 2023;</ref><ref type="bibr" target="#b31">Hennig and Schuler, 2012;</ref><ref type="bibr" target="#b47">Kushner, 1964;</ref><ref type="bibr" target="#b58">Mockus, 1974;</ref><ref type="bibr" target="#b71">Srinivas et al., 2010;</ref><ref type="bibr" target="#b77">Wang and Jegelka, 2017;</ref><ref type="bibr">Wang et al., 2024b)</ref>, reinforcement learning <ref type="bibr" target="#b27">(Ghavamzadeh et al., 2015;</ref><ref type="bibr" target="#b40">Kaelbling et al., 1996;</ref><ref type="bibr" target="#b73">Sutton, 2018)</ref> and experimental design <ref type="bibr" target="#b6">(Chaloner and Verdinelli, 1995;</ref><ref type="bibr" target="#b44">Kirk, 2009)</ref>. We reckon that T2I agents should also be capable of actively seeking important information from human users to quickly reduce uncertainty <ref type="bibr" target="#b81">(Wang et al., 2024c)</ref> and generate satisfying images. In §E, we detail the implementation of action selection strategies for our T2I agents.</p><p>In human-computer interaction, researchers have been extensively studying how to best enable Human-AI interaction especially from user experience perspectives <ref type="bibr" target="#b1">(Amershi et al., 2019;</ref><ref type="bibr" target="#b5">Cai et al., 2019;</ref><ref type="bibr" target="#b9">Chen et al., 2024;</ref><ref type="bibr" target="#b33">Höök, 2000;</ref><ref type="bibr">Kim et al., 2023;</ref><ref type="bibr" target="#b63">Norman, 1994;</ref><ref type="bibr" target="#b74">Viégas and Wattenberg, 2023;</ref><ref type="bibr" target="#b89">Yang et al., 2020)</ref>. Interface design for AI is becoming increasingly challenging due to the lack of transparency <ref type="bibr" target="#b9">(Chen et al., 2024;</ref><ref type="bibr" target="#b74">Viégas and Wattenberg, 2023)</ref>, uncertainty about AI capability and complex outputs <ref type="bibr" target="#b89">(Yang et al., 2020)</ref>. We aim to build user-friendly agents, and an indispensable component is their interface to enable them to effectively act and observe, as detailed in §G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretebaility.</head><p>Surfacing an agent's belief overlaps with interpretability as both aim to understand model or agent's internal. Some methods leverage LLM's natural language interface to surface their reasoning (e.g., chain of thought <ref type="bibr">(Wei et al., 2023a)</ref>), sometime interactively <ref type="bibr">(Wang et al., 2024a)</ref>. While these approaches make accessible explanations, whether the explanations represent truth has been questioned <ref type="bibr" target="#b8">(Chen et al., 2023;</ref><ref type="bibr" target="#b49">Lanham et al., 2023;</ref><ref type="bibr">Wei et al., 2023b)</ref>. Some studies indicate explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations <ref type="bibr" target="#b90">(Ye and Durrett, 2022)</ref>. In this work, the belief graph does not correspond to the distribution over outputs of the T2I model itself conditioned on the underspecified prompt. Instead, the belief graph is designed to align with the distribution over images generated by the agent, since the agent can construct detailed prompts according to its belief, and feed them into a high-quality T2I model.</p><p>Text-to-Image (T2I) generation. Text-to-image prompts can be ambiguous, subjective <ref type="bibr">(Hutchinson et al., 2022)</ref>, or challenging to represent visually <ref type="bibr" target="#b85">(Wiles et al., 2024)</ref>. Different users often have distinct requirements for image generation, including personal preferences <ref type="bibr" target="#b82">(Wei et al., 2024)</ref>, style constraints <ref type="bibr" target="#b79">(Wang et al., 2023)</ref>, and individual interpretations <ref type="bibr" target="#b91">(Yin et al., 2019)</ref>. To create images that better align with users' specific needs and interpretations, it is essential to actively communicate and interact with the user to understand the user's intent.</p><p>Multi-turn T2I. Current multi-turn T2I systems typically focus on multi-turn user instructions. <ref type="bibr" target="#b37">Huang et al. (2024)</ref>; <ref type="bibr" target="#b72">Sun and Guo (2023)</ref> propose multi-modal interactive dialogue systems which passively respond to user's natural language instructions. Mini DALL•E 3 <ref type="bibr" target="#b48">(Lai et al., 2023)</ref> builds an interactive T2I framework that accepts user instructions and responds to user questions. <ref type="bibr">Vodrahalli and Zou (2024)</ref> collected and analyzed a dataset of human-AI interactions where users iteratively refine prompts for T2I models to generate images similar to goal images (goal images are only visible to users). This may require users to actively try prompts to understand model behaviors. On the contrary, our work aims to reduce the burden on the user by actively asking questions to understand user intents.</p><p>A core challenge in multi-turn T2I is consistency <ref type="bibr">(Cheng et al., 2024a,b;</ref><ref type="bibr" target="#b93">Zeqiang et al., 2023)</ref>. <ref type="bibr">Hu et al. (2024)</ref> introduce Instruct-Imagen, which is a model that follows complex multi-modal instructions. AudioStudio <ref type="bibr">(Cheng et al., 2024a</ref>) is a multi-turn T2I framework aimed at subject consistencies while generating diverse and coherent images. These consistency improvement methods can potentially be integrated into our T2I agents since they are highly modular. However, as an ablation, we only focus on the sequential decision making capability of agents to elicit user intents.</p><p>Evaluating image-prompt alignment is important for T2I models. Relevant metrics can be embedding-based, such as CLIPScore <ref type="bibr" target="#b32">(Hessel et al., 2022)</ref>, ALIGNScore <ref type="bibr">(Zha et al., 2023)</ref>, VQAbased such as TIFA <ref type="bibr" target="#b11">(Hu et al., 2023)</ref>, DSG <ref type="bibr" target="#b11">(Cho et al., 2023)</ref> and VQAScore <ref type="bibr" target="#b52">(Lin et al., 2024)</ref>, and captioning-based like LLMScore <ref type="bibr" target="#b54">(Lu et al., 2023)</ref>. Approaches such as PickScore <ref type="bibr" target="#b45">(Kirstain et al., 2023)</ref>, ImageReward <ref type="bibr" target="#b88">(Xu et al., 2023)</ref> and HPS-v2 <ref type="bibr" target="#b86">(Wu et al., 2023)</ref> finetune models on human ratings to devise a metric that aligns with human preferences. Recently, diversity of generated images <ref type="bibr" target="#b60">(Naeem et al., 2020)</ref> is also becoming an important metric of measurement to track progress, especially in the geo-cultural context <ref type="bibr" target="#b30">(Hall et al., 2024;</ref><ref type="bibr" target="#b42">Kannen et al., 2024)</ref>.</p><p>In this work, we develop an automatic approach to evaluate agent-user conversations. We adopt VQAScore <ref type="bibr" target="#b52">(Lin et al., 2024)</ref> to evaluate the alignment between a ground truth prompt and an image generated by an agent after interactions with a simulated user. Other T2I metrics can also be used.</p><p>Prompt expansion is a widely known technique to improve image generation <ref type="bibr" target="#b4">(Betker et al., 2023)</ref>. ImageinWords <ref type="bibr" target="#b22">(Garg et al., 2024)</ref> proposes to obtain high-quality hyper-detailed captions for images, which significantly improve quality of image generation. <ref type="bibr" target="#b14">Datta et al. (2024)</ref> present a generic prompt expansion framework used along Text-to-Image generation and show an increase in user satisfaction through human study. While our work can be viewed as a method to adaptively expand a T2I prompt based on user feedback<ref type="foot" target="#foot_1">foot_1</ref> , evaluating our method as a prompt expansion tool is outside of our scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>The belief graph in our work is closely related to symbolic world representations.</p><p>World states. In classical AI, researchers use symbolic representations to describe the world state <ref type="bibr" target="#b39">(Kaelbling and Lozano-Pérez, 2011;</ref><ref type="bibr" target="#b55">McCarthy and Hayes, 1969;</ref><ref type="bibr" target="#b56">Minsky, 1974</ref><ref type="bibr" target="#b57">Minsky, , 1988;;</ref><ref type="bibr" target="#b65">Pasula et al., 2007)</ref>. For example, in the blocks world <ref type="bibr" target="#b0">(Alkhazraji et al., 2020;</ref><ref type="bibr" target="#b28">Ginsberg and Smith, 1988;</ref><ref type="bibr" target="#b29">Gupta and Nau, 1992)</ref>, a state can be</p><formula xml:id="formula_0">𝑖𝑠_𝑏𝑙𝑜𝑐𝑘(𝑎) ∧ 𝑖𝑠_𝑟𝑒𝑑 (𝑎) ∧ 𝑜𝑛_𝑡𝑎𝑏𝑙𝑒(𝑎) ∧ 𝑖𝑠_𝑏𝑙𝑜𝑐𝑘(𝑏) ∧ 𝑖𝑠_𝑏𝑙𝑢𝑒(𝑏) ∧ 𝑜𝑛(𝑏, 𝑎),</formula><p>describing that there are a red block and a blue block, referred to as 𝑎 and 𝑏, block 𝑎 is on a table, and block 𝑏 is on 𝑎. Such world states must include entities (e.g., 𝑎 and 𝑏), their attributes (e.g., position 𝑜𝑛_𝑡𝑎𝑏𝑙𝑒, characteristics 𝑖𝑠_𝑏𝑙𝑜𝑐𝑘) and relations (e.g., 𝑜𝑛(𝑏, 𝑎)) which are critical for enabling a robot to know and act in the world.</p><p>In linguistics, <ref type="bibr" target="#b15">Davidson (1965</ref><ref type="bibr">Davidson ( , 1967a,b) ,b)</ref> introduce logic-based formalisms of meanings of sentences. The semantics of a sentence is decomposed to a set of atomic propositions, such that no propositions can be added or removed from the set to represent the meaning of the sentence. <ref type="bibr" target="#b11">(Cho et al., 2023)</ref> propose Davidsonian Scene Graph (DSG) which represent an image description as a set of atomic propositions (and corresponding questions about each proposition) to evaluate T2I alignment.</p><p>We borrow the same concept as symbolic world representations and scene graphs, except that the agent needs to represent an imaginary world. The image generation problem can be viewed as taking a picture of the imaginary world. The world state should include all entities that are in the picture, together with their attributes and relations.</p><p>Belief states. Term "belief state" <ref type="bibr" target="#b41">(Kaelbling et al., 1998;</ref><ref type="bibr" target="#b61">Nilsson, 1986)</ref> has been used to describe a distribution over states. E.g., for block 𝑎, we might have 𝑝(𝑜𝑛_𝑡𝑎𝑏𝑙𝑒(𝑎)) = 0.5 and 𝑝(¬𝑜𝑛_𝑡𝑎𝑏𝑙𝑒(𝑎)) = 0.5, which means the agent is unsure whether the block is on a table. To represent the T2I agent's belief on which image to generate, we need to consider the distribution over all possible "worlds" in which the picture can be taken. This distribution can be described by the probabilities that an entity appears in the picture, an attribute gets assigned a certain value, etc.</p><p>A core difference between our belief graphs and classic belief states is that belief graphs do not need pre-defined predicates (e.g., 𝑜𝑛(𝑏, 𝑎)), but instead automatically produce useful predicates using LLMs. This makes belief graphs much more generalizable across domains. More details in §B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proactive T2I agent design</head><p>We provide high-level principles and design that guide our agent how to behave and interact with users to generate desired images from text through multi-turn interactions. The goal of the agent is to generate images that match the user's intended image as closely as possible with minimal back-andforth, particularly in cases with underspecified prompts and the agent needs to gather information proactively. This requires a decision strategy on information gathering to trade off between the cost of interactions and the quality of generated images. The formal problem definition can be found in §C.</p><p>We equip the agent with the ability to gather information in two ways: ask clarification questions ( §4.1) and express its uncertainty and understanding in a way that users can edit ( §4.2). Once a piece of information is collected from a user, the agent also need to update its questions and uncertainty ( §4.3). To enable all these agent behaviors, we need to situate the agent in an interface to effectively communicate with users ( §G). In the following, we introduce the design of the above components under the interface, to ensure information efficiency for T2I generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">What kind of questions should be asked?</head><p>We explain considerations in question asking and examples of strategies in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Principles</head><p>We identify the following principles for an agent to ask the user questions about the underspecified prompt and their intended image: (i) Relevance: The question should be based on the user prompt. (ii) Uncertainty Reduction: The question should aim to reduce the agent's uncertainty about the attributes and contents of the image, the objects, the spatial layout, and the style. (iii) Easy-to-Answer: The question should be as concise and direct as possible to ensure it is not too difficult for the user to answer. (iv) No Redundancy: The question should not collect information present in the history of interactions with the user. The Relevance and No Redundancy principles are self-explanatory, we detail the other two principles below.</p><p>The Uncertainty Reduction principle aims to let agent elicit information about various characteristics of the desired image, which the agent is unsure of.</p><p>First, the agent needs to know what characteristics of images are important. Some examples include: (i) Attributes of the subjects, such as breed, size, or color, with questions like What kind of rabbit? What color is the cat?; (ii) Spatial relationships between the subjects, such as proximity and relative position (Are the rabbit and cat close to each other? Are they facing each other?); (iii) Background information, such as location, style and time of day (Are they in a park or at home?); and (iv) Implicit entities that might not be explicitly mentioned in the initial prompt but are relevant to the user's vision (Are there any other animals or people present?).</p><p>Second, the agent needs to know its own uncertainty about those characteristics. In the agent's belief, the uncertainty is explicit. One strategy is to form questions about the image characteristics that the agent is most uncertain about. We discuss more in §E.2.</p><p>Third, the agent needs to update its own uncertainty once the user gives a response to its question (a.k.a. transition in §4.3). Then, it can construct questions again based on its updated uncertainty estimates. This iterative clarification process allows the agent to progressively refine its understanding of the user's intent and generate an image that more accurately reflects their desired output.</p><p>The Easy-to-Answer principle aims to reduce users' effort to respond to questions. One way is to have the agent provide some answer options, where options are what the agent believes likely to appear. E.g., What color is the cat? (a) Black (b) Brown (c) Orange (d) Other (please specify).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Examples of question-asking strategies</head><p>Given the agent belief constructed from the user prompt (more details in §4.2), several basic approaches can be employed following the above principles. We construct simple agents with the following strategies, which are implemented and used in our experiments.</p><p>• Ag1 ( §E.5): Rule-based question generation, which leverages predefined rules or heuristics to identify salient attributes, entities, or relationships that require clarification. For example, an LLM could be used to estimate the importance and likelihood of different components within the belief, and a heuristic could be applied to prioritize the most crucial elements for questioning.</p><p>• Ag2 ( §E.6): Belief-guided question generation, which involves using natural language to represent the current understanding encapsulated in the belief. This representation, along with the conversation history, is provided as input to an LLM, guiding it to generate clarification questions.</p><p>• Ag3 ( §E.7): Direct question generation, which write the above question-asking principles in a prompt for an LLM to generate a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Interacting with the user based on agent beliefs</head><p>The Uncertainty Reduction principle inspires the usage of belief graphs for the agent to directly express uncertainty, in addition to reflecting uncertainty through questions. Instead of using hardcoded symbols in classic belief representations <ref type="bibr" target="#b20">(Fikes and Nilsson, 1971)</ref> described in §3, we employ LLMs to generate names and values for entities, attributes and relations. As a result, this belief construction method can generalize across any prompts. Algorithm 1 summarizes how an agent parses a prompt to a belief graph and allows user interaction<ref type="foot" target="#foot_2">foot_2</ref> . All agents in §4.1.2 use the same kind of belief graphs. Parse entity attributes and relations from entities and MP (E.9, E.10) 6:</p><p>Display belief graph, and collect interaction feedback (F) 7:</p><p>Update MP: MP ← MP + F (E.12) 8: end for Entities. In addition to (a) entities mentioned in the user prompt, a belief graph also includes (b) implicit entities not mentioned in the prompt but likely to appear, e.g., pet owner in the context of a pet-related scene; and (c) background entities, such as image style, time of day, location, which play important roles in constructing the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attributes and relations.</head><p>While the prompt might mention some attributes of a certain entity, they are not enough to describe the exact details of that entity. Hence the agent have to imagine the relevant attributes for each entity, and construct a list of possible values along with their associated probabilities (e.g., the color attribute for the cat entity might have values like black, white, gray with corresponding probabilities). Similarly the agent may have to imagine the possible relations between entities, e.g., spatial relation between rabbit and cat might include values like close, far, touching.</p><p>Importance scores. While the agent can be uncertain about many aspects of the user's intended image, some are more important than others. E.g., for prompt "a rabbit and a cat", the agent might be very uncertain about the exact color of a carpet that might appear in the image, but rabbit and cat are more important than the carpet. We enable agents to estimate an importance score for each entity, attribute and relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracting beliefs and enabling interactions.</head><p>A simple idea is to use a large language model (LLM) via in-context learning. §E.1 details how an LLM may analyze the user prompt to identify entities, their attributes, and the relations between them, effectively translating the natural language input into a structured representation within the belief. Once the belief is extracted, a user can edit the belief to adjust uncertainty levels, confirm existence of entities etc, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transition</head><p>The agent belief undergoes a transition whenever the agent receives new information through user feedback, either user answers from the agent question or user interactions with the graph-based belief interface (Figure <ref type="figure" target="#fig_0">1</ref>). This transition process integrates information from the initial user prompt, the conversation history, interaction and the previous belief to generate an updated belief of the user's desired image. We use a simple approach: Generate a comprehensive prompt that summarizes all interactions and information gathered thus far. This merged prompt is then used to re-generate the belief, effectively incorporating the new information into a refreshed representation. The implementation details can be found in §E.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct 2 types of experiments to study the effectiveness of the proposed agent design: automatic evaluation which uses a simulated user to converse with a T2I agent and human study which studies the efficacy of our framework with human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Automatic evaluation</head><p>We simulate the user-agent conversation using self-play <ref type="bibr" target="#b70">(Shah et al., 2018)</ref> between two LLMs. The conversation starts with an arbitrarily chosen image to represent the goal image from a T2I model that the user has in mind<ref type="foot" target="#foot_3">foot_3</ref> . Along with this ground truth image, a user has a detailed prompt (i.e., the ground truth) in mind that describes the image in high-detail. We use the algorithm similar to Ag2 (detailed in §E.4) to simulate the user, where the questions are answered based on the ground truth prompt and the belief graph generated from the ground truth prompt. We run the agent-user conversation for a total of 15 turns<ref type="foot" target="#foot_4">foot_4</ref> and compute different metrics at the end of each turn. More details of the simulated user can be found in the appendix, including the prompts provided to the LLM when simulating the user are provided. Figure <ref type="figure">2</ref> part b shows the multi-turn set up that we use in our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Setups for agents and baseline</head><p>Baselines. We use a standard T2I model as a baseline, which directly generates an image based on a prompt without asking any questions. We refer to this baseline as 'T2I'.</p><p>Agents. We use Ag1, Ag2 and Ag3 with question-asking strategies introduced in §4.1.2. The creation and updates to the belief graph ( §4.2), as well as transitions to prompt ( §4.3) are consistent among all multi-turn agents. Further implementation details of each agent can be found in §E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Selection.</head><p>In this work we use an off-the shelve Text-to-Image (T2I) model and a Multi-Modal Large Language (MLLM) model and build the different components of our agent on top of these models. We keep these models consistent across all agents for fair comparison. We implement the agent on top of the Gemini 1.5 (Gemini Team Google, 2024) using the default temperature and a 32K context length. The in-context examples and the exact prompt used at each step of the agent pipeline is detailed in §E.8 - §E.15. More agent implementation details are provided in §E. For T2I generation, we use Imagen 3 <ref type="bibr" target="#b3">(Baldridge et al., 2024)</ref> across all baselines given it's recency and prompt-following capabilities. We used both the models served publically using the Vertex API <ref type="foot" target="#foot_5">6</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Datasets.</head><p>Our multi-turn agents aim to facilitate the generation of complex images, a process that often requires users to iteratively refine text-to-image (T2I) prompts until the generated image aligns with their mental picture. To evaluate these agents, we curate datasets comprising complex scenes involving multiple subjects, interactions, backgrounds, and styles. Each dataset consists of tuples: (I, 𝑝 0 , 𝑐, 𝑏 𝑔𝑡 ), where I represents the target image, 𝑝 0 is an initial (basic) prompt describing only the primary elements of the scene, 𝑐 is a ground truth caption providing a detailed description of I, including spatial layout, background elements, and style, and 𝑏 𝑔𝑡 is the ground truth belief graph constructed via parsing 𝑐. The initial prompt 𝑝 0 is intentionally less detailed than 𝑐 to necessitate multi-turn refinement. This framework allows us to assess the agent's ability to guide the user towards the target image I starting from a simplified prompt.</p><p>Existing image-caption datasets primarily focus on simple scenes <ref type="bibr">(Deng et al., 2009;</ref><ref type="bibr" target="#b19">Deng, 2012;</ref><ref type="bibr" target="#b46">Krizhevsky et al., 2009)</ref> or focus on very specific categories <ref type="bibr" target="#b50">(Liao et al., 2022;</ref><ref type="bibr" target="#b53">Liu et al., 2016)</ref>. With the aim for complex realistic images for testing the robustness of the agents, we evaluate over the validation split of the Coco-Captions dataset <ref type="bibr" target="#b7">(Chen et al., 2015)</ref>. Five independent human generated captions are provided for each image in the dataset. These captions are often short and describe the basic elements contained in the image and the interactions between objects or persons in the image. We therefore select the shortest of the five human-generated captions and use this as a starting prompt 𝑝 0 . We then use Gemini 1.5 Pro to expand the starting prompt by adding more details of the attributes of the entities in the image as well as the style and image composition which results in the ground truth caption. We also use the ImageInWords <ref type="bibr" target="#b22">(Garg et al., 2024)</ref> dataset which takes a diverse set of realstic and cartoon images and has human annotators create dense detailed captions that describe attribute and relationships between objects in the image. In ImageInWords evaluations we use the long human annotation as the ground truth caption.</p><p>While COCO-Captions and ImageInWords provide complex, real-world images across diverse backgrounds, it lacks the artistic or non-photorealistic imagery often desired by designers and artists seeking to generate content outside the distribution of typical training data. To better evaluate our target for flexible use cases such as by artists, we introduce DesignBench, a novel dataset comprising 30 scenes specifically designed for this purpose. Each scene follows the (I, 𝑝 0 , 𝑐, 𝑏 𝑔𝑡 ) format described earlier. DesignBench includes a mix of cartoon graphics, photorealistic yet improbable scenes, and artistic photographic images. Examples from DesignBench and a comparison with COCO-Captions are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Metrics</head><p>The outputs produced by the agent include a final generated image, a final caption and a final belief graph. We evaluate the agents across these modalities and evaluate their alignment to the ground truth image I, caption 𝑐 and belief 𝑏 𝑔𝑡 , using the following metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-Text Similarity:</head><p>We use 2 metrics for comparing the ground truth caption and the generated caption: 1) T2T -embedding-similarity computed using Gemini 1.5 Pro 7 and 2) DSG <ref type="bibr" target="#b12">(Cho et al., 2024)</ref> adapted to parse text prompts into Davidsonian scene graph using the released code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Image Similarity (I2I):</head><p>We compute cosine similarity between the groundtruth image and the generated image from the agent prompt. We use image features from DINOv2 <ref type="bibr" target="#b64">(Oquab et al., 2024)</ref> model following prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-Image Similarity:</head><p>We compare the ground truth prompt with the generated image (T2I) using VQAScore <ref type="bibr" target="#b52">(Lin et al., 2024)</ref>. We use the author released implementation of the metric and use Gemini 1.5 Pro as the underlying MLLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative log likelihood (NLL):</head><p>We construct the ground truth state of the image in the form of a belief graph but with no uncertainty. We then approximately compute the NLL of the ground truth state given the belief of the agent at each turn, by assuming the independence of all entities, attributes and relations, and summing their log probabilities 8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results from automated evaluation</head><p>The results from the automatic evaluations in Table <ref type="table" target="#tab_0">1</ref> show the I, 𝑐 and 𝑏 𝑔𝑡 against each agents final generated image, text and state. All show the mean and standard deviation of the similarity metric at the final agent state. The blue row shows the baseline method which performs no updates to the prompt and instead applies the T2I model to the first prompt. Therefore this baseline represents the lower bound performance.</p><p>To add quantitative validity to the ground truth caption generation we perform Text to Image (VQA) Similarity between the ground truth caption and the ground truth over all images in the DesignBench dataset. The mean T2I VQA similarity between the ground truth caption and ground truth image is 0.99999985 with a median 1.0, and standard deviation of 4.5e-07. The mean is extremely close to 1 as expected of an accurate and well formed caption. These numbers can be compared to the T2I column of Table <ref type="table" target="#tab_0">1</ref> to observe the delta between the ground truth caption and generated captions.</p><p>The results in Table <ref type="table" target="#tab_0">1</ref> show that significant gains in performance come from using proactive 7 Text embeddings are obtained from Embeddings API: <ref type="url" target="https://ai.google.dev/gemini-api/docs/embeddings">https://ai.google.dev/gemini-api/docs/embeddings</ref>. 8 This approximation does not account for potential similarities in the names of entities or attributes. This could lead to approximation errors if, for example, the model confuses "Persian cat" with "Siamese cat" due to their similar names. Addressing this limitation would require incorporating semantic similarity measures into the NLL computation.</p><p>multi-turn agents. The blue row shows the simplest baseline which directly uses a T2I model and performs no updates to the initial prompt 𝑝 0 . We see that all of the multi-turn agents far exceed the baseline T2I model on both datasets and all metrics. Ag3 (the LLM agent that does not explicitly utilize the belief graph to generate questions) show superior performance across all metrics.</p><p>The plots in Figure <ref type="figure" target="#fig_2">3</ref> show the T2T, I2I, T2I and NLL metrics, averaged across all images in the ImageInWords dataset, per turn for 15 turns. We see that the multi-turn agents all improve in every metric as they increase the number of interactions. Interestingly we see the T2T and the T2I VQA similarity metric seems to plateau or decrease after about 10 interactions, while the I2I scores continue to increase. The NLL metric shows large performance gains of the Ag3 agent in comparison to all other methods. The plots in Figure <ref type="figure" target="#fig_0">12</ref> shows the T2T DSG metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of quantitative results</head><p>The evaluations on the COCO-captions, ImageInWords, DesignBench datasets show similar results and highlight the same patterns across the different agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Turn agents show clear advantage:</head><p>The immediate take away is the baseline which does not use multi-turn interaction and instead passes in the original prompt into the T2I model performs worse than the multi-turn agents on all metrics on both datasets. This confirms our hypothesis that the current T2I agents often produce less desirable images given ambiguity in prompts. In Figure <ref type="figure">2</ref> we see real outputs of the multi-turn set up with the Ag3 agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs being a part of agents play a significant role:</head><p>The best performers (Ag2 and Ag3) both query and LLM to provide a question to ask the user based on contextual information such as the belief graph and conversation history. They query the LLM to construct a concise and clear question but don't impose further constraints on the question construction. Ag1 provides a programatic template for how the LLM should construct the question based on its belief graph and does not provide any conversation history information. Examples of dialogs and the generated questions produced by the three agents can be found in the Appendix in Figure <ref type="figure" target="#fig_5">6</ref>. This figure demonstrates that the templated question creation leads to extremely specific questions that often gather minimal information in return. This is an intrinsic limitation of hard coded question selection strategy but also can be an issue of the heuristic scores we defined for question selection in Ag1. In contrast, Ag2 and Ag3 generate questions that are more open-ended thus allowing the user to provide more nuanced details which in  Table <ref type="table">2</ref> | Perceived helpfulness of proposed features (% of users) rated by 143 raters.</p><p>consequence enhance the agent's image knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question prompts with question-asking principles show advantage over those with beliefs:</head><p>The Ag3 agent (which uses an LLM with question generation instructions about entity, attributes etc related to the belief) dominates across all datasets on almost every metric. Ag2 uses the belief explicitly to construct questions by passing the belief into the LLM as information from which to generate the next question. When inspecting the reasoning steps of Ag2, we found that Ag2 excessively relies on importance scores in beliefs to ask questions, and if the importance scores are not estimated properly, the quality of the questions decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Human studies on generated images and dialogues</head><p>To verify the automatic evaluations of the agents, we performed human studies in which participants were asked to rate the generated images and dialogues along different axes. The detailed design of the studies can be found in Figure <ref type="figure">20</ref>, Figure <ref type="figure" target="#fig_14">21</ref> and Figure <ref type="figure">22</ref>.</p><p>Participants are asked to rate the images produced by the three proposed multi-turn agents and a single-turn T2I model against a Ground Truth image for which the original prompt was derived and the answers to the agents questions were derived. Approximately 550 image-dialog pairs per</p><p>0% 25% 50% 75% 100% 1 -Best 2 -Moderate 3 -Low 4 -Worst No Agreement Ag3 Ag2 Ag1 T2I Rank of Generated Image in Terms of Content Correctness 0% 25% 50% 75% 100% 1 -Best 2 -Moderate 3 -Low 4 -Worst No Agreement Ag3 Ag2 Ag1 T2I Rank of Generated Image in Terms of Style and Aesthetics Figure 4 | Human Rating of the Generated Images. Ratings are based on Content Correctness and Style and Aesthetics. Each human rater is given the Ground Truth Image and Prompt to compare the Generated Image against. Very Close Fairly close some fair differences Equally close and different Fairly different some similarities Very different no similarities Ag1 Ag2 Ag3 Similarity of Generated Image Vs. Prompt and Dialogue The question is too long and complex The question doesn't c important and uncertain attributes The question is irrelevant to the goal image and prompt The question is unclear and ambiguous The question doesn't gain any new information None of the above 0.00% 25.00% 50.00% 75.00% Ag1 Ag2 Ag3 Distribution of Issues in the Questions produced by the Agents agent are rated using 3 human raters. The generated images were presented in a random order and were unlabeled and the human rater was tasked with ranking the images from best to worst. The results from the study are shown in Figure <ref type="figure">4</ref>. We used 3 raters per set of images and therefore in cases where raters did not agree this has been noted via the No Agreement Column. The graph shows that the agentic systems are selected as the best generated image over the single-turn T2I in 80%+ of cases for both content and style.</p><p>To validate the generated dialog, human raters are asked to mark any issues a question contains that could pose a disturbance to the user. Approximately 8k questions per agent are rated. The results are shown in Figure <ref type="figure" target="#fig_4">5</ref>, where we see that the agentic systems have issues with their questions in 14% or less cases. For the Ag2 and Ag3 the common complaint is that the question is too long while the most common issue for Ag1 is that the question does not gain any new information.</p><p>Human raters are also asked to rank the correspondence of each image to the agent-user dialog and original prompt. Approximately 1.5k image-dialog pairs are rated using 3 human raters. Results in Figure <ref type="figure" target="#fig_4">5</ref> show that for all of our agents, more than 96% of the 1.5k image-dialog pairs are rated as very close or fairly close with some differences. This high rating shows the viability of the T2I model employed by the agents, as well as the agents' ability to combine the dialogue into a coherent prompt to feed the T2I model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Human studies on the agent interface</head><p>To get real user feedback on the agent interface, we performed a human survey with the objective of understanding user frustrations and validating our solutions. We gathered data from 143 participants who all identified to be regular T2I users (at least once a month). Participants were presented with four hypothesized frustrations (prompt misinterpretation, many iterations, inconsistent generations, incorrect assumptions) and three potential mitigating features (clarifications, entity graph, relationship graph; more details in §H).</p><p>Table <ref type="table">4</ref> in Appendix confirms the prevalence of hypothesized frustrations amongst users, with 83% experiencing occasional, frequent, or very frequent frustration due to prompt iterations, followed by 70% for misinterpretations, 71% for inconsistent generations, and 60% experiencing frustration due to incorrect assumptions. Most acutely 55% of participants reported frequent or very frequent frustration due to the prompt iteration frequency necessary. In Table <ref type="table">2</ref>, we report the mitigation features that are likely to help. Clarifications reported the highest likelihood to help current workflows (91% could / likely / very likely to be helpful), followed by entity graphs (88% could / likely / very likely to be helpful) and relationship graphs (86% could / likely / very likely to be helpful). Clarifications were expected to deliver value immediately / very soon by 58%.</p><p>Overall these suggest strong user desire for &amp; likelihood for success of features that reduce iterations and mitigate misinterpretations in T2I generation. Full explanations of the hypothesized frustrations, mitigation and responses splits are in §H. All respondents were compensated for their time as per market rates, and were recruited by our vendor to ensure diversity across age, gender, and T2I usage in terms of models, frequency and purpose (work and non work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and conclusion</head><p>This work introduces a design for agents that assist users in generating images through an interactive process of proactive question asking and belief graph refinement. By dynamically updating its understanding of the user's intent, the agent facilitates a more collaborative and precise approach to image generation. Moreover, presenting the agent's belief graph can be a generalizable method for AI transparancy, which is an important factor given the increasing complexity of modern AI models.</p><p>Modular design. Our agent prototypes are highly modular: the agents use frozen T2I models to generate images based on the prompts that the agent updated. Therefore when a better off-the-shelf T2I model becomes available, it can be directly plugged into the agents and the system will achieve better performance without any additional adaptation 9 .</p><p>Personalized content. By asking clarification questions, our agents enable a more customizable and personalized content creation experience. Because different groups of people may perceive helpfulness and harmfulness of contents differently, learning more about the user through clarification questions before generation can potentially mitigate risks of generating contents that are offensive to each specific user, and increase likelihoods of producing helpful outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future work.</head><p>Alternative to the modular design, one can explore generating images directly from belief graphs and fine-tuning LLM/VLMs on text/image trajectories that include asking questions. These may require a) collecting data such as gold-standard trajectories or annotations on the quality of trajectories of human-agent conversations and b) new approaches to fine-tune the model on multi-turn trajectories of images and text, which can potentially improve the performance of the agent. 9 T2T scores in Table <ref type="table" target="#tab_0">1</ref> ablates the T2I model and only performs similarity on the captions. Our agents have achieved a 92%+ T2T score, showing that their performance can be boosted by adopting better T2I models. atoms are instantiated predicates <ref type="bibr" target="#b0">(Alkhazraji et al., 2020;</ref><ref type="bibr">Garrett et al., 2020a,b)</ref>, so that whenever an action is applied, the agent can apply transition by adding and deleting items in the set according to the precondition and effect of the action. For T2I tasks, it is more convenient to use a graph to represent the world state associated with an image, since entities and relations naturally form a set of nodes (entities) and edges (relations between entities). Each component of the graph can also have probabilities, making it easy to turn a world state into a belief state using the same data structure. Hence we represent T2I agent beliefs using graphs. The agent can directly update the graph for each transition instead of going through a set or list.</p><p>• Interpretability and controllability: The graph structure makes our agent belief more interpretable than traditional belief states, since we can visualize and progressively disclose the graph to the human user. Moreover, each node or relation in the belief graph has associated descriptions, making it easy for the user to understand and potentially edit every component of the belief graph. In our human studies, about 85% of raters found the belief graph useful. To the best of our knowledge, our work is the first to use the graph-based belief state for human-AI interaction. 3. Automated evaluation of T2I agents: We propose a novel automated evaluation approach for T2I agents using self-play. The agent interacts with a simulated user that has access to the original image and its long caption. See §5.1 and §E.4 for the full details of how the simulated user is constructed. This evaluation pipeline is easy to use and can help the future development of T2I agents. 4. DesignBench: We envision that a significant fraction of T2I users are artists and designers, and it is important to ensure that T2I agents are evaluated for these use cases. Hence we create DesignBench, featuring photo-realism, animation and multiple styles with short and long captions. DesignBench can be directly plugged into our automated evaluation to streamline the evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Formalism of the agent and its objective</head><p>We define an interactive T2I agent as a ⟨𝐵, 𝐴, 𝑂, 𝜏, 𝜋⟩ tuple, where we have</p><p>• 𝑆: a representation space of images,</p><p>• 𝐵: a space of agent beliefs,</p><p>• 𝐴: a space of actions that the agent can take,</p><p>• 𝑂: a space of agent observations of the user,</p><p>• transition function 𝜏 : 𝐵 × 𝐴 × 𝑂 ↦ → 𝐵 for updating beliefs given new interactions,</p><p>• action selection strategy 𝜋 : 𝐵 ↦ → 𝐴, which specifies which action to take given a belief.</p><p>For each user-initiated interaction, we assume that there exists a specific intent 𝑠 ∈ 𝑆, where 𝑆 is the space of all possible user intents. For a T2I task, we assume that the intent is the image the user would like to generate, and the intent stays the same throughout the interaction with an agent. We discuss more about the validity of this assumption in §6.</p><p>Each type of T2I agents can have a unique user intent representation, belief representation, construction of the action space, and user interface design to obtain observations of users.</p><p>In §4, we show the examples for these components.</p><p>We use a score function, 𝑓 : 𝐵 × 𝑆 ↦ → ℝ, to evaluate the alignment between an agent belief and a user intent at any turn of the interaction. Function 𝑓 can only be evaluated in hindsight once the user intent is revealed. The agent does not have direct access to function 𝑓 since the user intent is hidden</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Image Initial Prompt</head><p>A photo of a cake that is adorned with berries sitting on a table. from the agent. However, the agent may construct a probabilistic distribution over function 𝑓 based on its belief about the user intent. The goal of the agent is to maximize function 𝑓 with as few turns of interaction with the user as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization of Multi-Turn Agent-User Dialogs and Generated Images</head><p>In Figure <ref type="figure" target="#fig_5">6</ref>, we show examples of multi-turn dialogs between simulated users and the three agents in Section 5. We also visualize the generated images in Figure <ref type="figure">7</ref>, Figure <ref type="figure">8</ref>, Figure <ref type="figure">9</ref> and Figure <ref type="figure" target="#fig_0">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation details of agent prototypes</head><p>We propose three T2I agents, each characterized by a unique configuration of ⟨𝐵, 𝐴, 𝑂, 𝜏, 𝜋⟩ tuples:</p><p>• Ag1: Heuristic Score Agent. This agent incorporates a human-defined heuristic score based on the belief to guide question generation. This heuristic score reflects the perceived importance of different aspects of the belief in driving the conversation forward; • Ag2: Belief-prompted Agent. This agent leverages an LLM to generate questions by processing both the conversation history and a structured representation of the belief. • Ag3: Principle-prompted Agent. This agent generates questions directly from the conversation history based on the principles introduced in §4.1.1. The question asking strategy of Ag3 relies solely on the implicit knowledge and reasoning capabilities of the underlying LLM.</p><p>Goal Image Ag3 Ag2 Ag1 T2I-Only  history, and the current belief, utilizing an ICL prompt ( §E.14) to guide question generation. The LLM then formulates a clarification question aimed at eliciting information about key features of the image, naturally prioritizing those with higher Importance to ask score within the belief. • Ask Important Clarification Question directly (𝐴𝐼𝐶𝑄 𝑏𝑎𝑠𝑒 ): This strategy relies on the LLM's inherent ability to identify important aspects of the user prompt and conversation history. The LLM ( §E.13) generates an important clarification question based on its implicit understanding of the user's needs, without explicitly relying on the structured information in the belief.</p><p>Ag1 employs 𝑀 𝐻 𝐼𝑆 strategy for question generation. This strategy leverages the importance scores assigned to entities, attributes, and relations within the belief graph. It identifies the element with the highest heuristic importance score and formulates a question aimed at eliciting further information about that specific element. The question is then verbalized using the LLM described in Section §E.15.</p><p>Ag2 utilizes the parsed belief graph as the basis for question generation. It employs the 𝐴𝐼𝐶𝑄 𝐵 strategy, which leverages the structured information within the belief graph to generate targeted clarification questions.</p><p>Ag3 relies solely on the conversation history for question generation. It employs the 𝐴𝐼𝐶𝑄 𝑏𝑎𝑠𝑒 strategy, which leverages the LLM's ability to understand the ongoing dialogue and identify key areas requiring further clarification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Implementation of belief transition</head><p>Both Ag1 and Ag2 require belief updating to incorporate new information gained during the interaction in order to compose clarification questions. At each turn, we perform prompt merging to create a comprehensive prompt that summarizes the conversation history. This merged prompt is then used for belief parsing to obtain an updated belief graph. For Ag2 (and Ag3), this updated belief graph directly informs the subsequent interaction. For Ag1, it incorporates additional post-processing mechanisms to enhance memory and prevent redundant questioning: (i) Redundancy elimination: If an attribute or relation has already been addressed in the conversation history, the corresponding user response is assigned as the sole candidate with a probability of 1.0, and its importance score is set to 0. This prevents the agent from repeatedly asking about the same information. (ii) Information retention: If an attribute or relation from the conversation history is absent in the updated belief graph, it is explicitly added. This ensures that the agent retains crucial information even if it's not explicitly present in the latest parsed belief graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. User simulation</head><p>To simulate end-to-end agent-user interactions, we implement a user simulator that mimics human question-answering behavior. This simulator operates as follows:</p><p>• It generates a belief graph based on a ground truth prompt, representing the user's intended image. This serves as the simulator's internal representation of the desired image. • Mirroring the 𝐴𝐼𝐶𝑄 𝐵 strategy, the simulator takes the ground truth prompt, conversation history, and its current belief graph as input. It then leverages an ICL prompt (see §E.14) to generate a response to the agent's question. This ensures that the simulator's answers are consistent with its internal belief graph and the ongoing conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Ag1: Heuristic Score Agent</head><p>The Heuristic score agent leverages the importance scores and probabilities within the belief graph to guide its question-asking strategy. The underlying principle is to identify and inquire about the entity, attribute, or relation that exhibits both high importance and high uncertainty. This aligns with the uncertainty reduction principle discussed in §4.1.1, which emphasizes minimizing uncertainty through targeted questioning. To achieve this, we define a heuristic importance score as formulated in Equation <ref type="formula">1</ref>, and the agent then selects the attribute or relation with the highest heuristic importance score as the focus of its inquiry. To facilitate easy answering, we utilize an LLM to generate userfriendly questions with multiple-choice options. For example, the agent might ask: What color of the rabbit do you have in <ref type="bibr">mind? a. black , b. white, c. brown. d. unkown</ref>. If none of these options , what color of the rabbit do you have in mind?. This format allows users to simply select the most appropriate option or provide their own answer if needed.</p><p>Here's a summary of Ag1's implementation: (i) Belief Representation: The agent's belief comprises the merged prompt and the current belief graph. (ii) Select Action: 𝑀 𝐻 𝐼𝑆 strategy is employed to identify the attribute or relation of interest based on the heuristic importance score. (iii) Verbalize Action: An LLM ( §E.15) is used to generate a clear and concise question about the selected attribute or relation. (iv) Answer Question: The user simulator provides an answer to the agent's question, mimicking human response behavior. (v) Transition: The agent updates the merged prompt with the new information, re-generates the belief graph based on the updated prompt, and applies the post-processing logic outlined in §E.3 to ensure consistency and prevent redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. Ag2: Belief-prompted Agent</head><p>The Ag2 agent incorporates the belief graph into its decision-making process but adopts a different approach compared to Ag1. Instead of relying on a heuristic score, Ag2 leverages the full capacity of an LLM to generate questions. It provides the LLM with comprehensive information, including the merged prompt, belief graph, and conversation history, allowing the LLM to formulate the most informative questions possible. To guide the LLM towards generating effective questions, we incorporate specific instructions in the prompt, emphasizing the following principles: The question should be as concise and direct as possible. The question should aim to obtain the most information about the style, entities, attributes, spatial layout and other contents of the image. Remember to ask for information that are critical to knowing the critical details of the image that is important to the user. The question should reduce your uncertainty about the user intent as much as possible.</p><p>Here's a summary of Ag2's implementation: (i) Belief Representation: The same as Ag1, the agent's belief consists of the merged prompt and the current belief graph. (ii) Select Action: 𝐴𝐼𝐶𝑄 𝐵 strategy is employed, which leverages an LLM to generate a question based on the comprehensive input information. (iii) Verbalize Action: Since the LLM directly generates the question, no separate verbalization step is required. (iv) Answer Question: The user simulator provides an answer to the agent's question. (v) Transition: The agent updates the merged prompt with the new information and re-generates the belief graph based on the updated prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7. Ag3: Principle-prompted Agent</head><p>A simple and effective implementation of LLM-based multi-modal dialogue systems is to use the context to store the history of conversations between the system and the user, and directly generate the next response based on the context.</p><p>To align with the principles outlined in §4.1.1, we guide the LLM's question generation with a prompt ( §E.13) that emphasizes all principles: Based on the original prompt and chat history please provide a question to ask about the image. The question should be as concise and direct as possible. The question should aim to learn more about the attributes and contents of the image, the objects, the spatial layout, and the style. The prompt also includes the history of conversation. This strategy aims to generate questions that are easy for users to understand and answer, while effectively reducing the agent's uncertainty about the desired image.</p><p>Here's a summary of Ag3's implementation: (i) Belief Representation: The same as Ag1 and Ag2, the agent's belief consists of the merged prompt and the current belief graph. (ii) Select Action: 𝐴𝐼𝐶𝑄 𝑏𝑎𝑠𝑒 strategy is employed, which leverages an LLM to generate a question based on the conversation history. (iii) Verbalize Action: The LLM directly generates the question, so no separate verbalization step is needed. (iv) Answer Question: The user simulator provides an answer to the agent's question. (v) Transition: The same as Ag2, the agent updates the merged prompt with the new information and re-generates the belief graph based on the updated prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>57</head><p>}}, 58 {{ 59 "name": "tree ", 60 "importance_to_ask_score": 0, 61 " description ": " trees in the background", 62 "entity_type ": " explicit ", 63 "probability_of_appearing": 0 64 }} 65 {{ 66 "name": "camera angle", 67 "importance_to_ask_score": 0.8, 68 " description ": "the camera angle of the image", 69 "entity_type ": "background", 70 "probability_of_appearing":</p><p>1.0 71 }}, 72 {{ 73 "name": "weather", 74 "importance_to_ask_score": 0.8, 75 " description ": "weather", 76 "entity_type ": "background", 77 "probability_of_appearing": 1.0 78 }}, 79 {{ 80 "name": "image style ", 81 "importance_to_ask_score": 1.0, 82 " description "the style of the image", 83 "entity_type ": "background", 84 "probability_of_appearing": 1.0 85 }}, 86 {{ 87 "name": "background color", 88 "importance_to_ask_score": 0.8, 89 " description ": "the background color of the image", 90 "entity_type ": "background", 91 "probability_of_appearing": 0.5 92 }} 93 ] 94 95 ... [[a few additional examples]] ... 96 97 98 Identify the entities given the input given below. Strictly stick to the format. 99 Input: {{ 100 "user_prompt": "{user_prompt}" 101 }} 102 Output:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.9. Attribute Parser Prompt Instruction</head><p>1 Given a text-to-image prompt and a particular entity described in the prompt, and your goal is to identify a list possible attributes that could describe the particular entity . Output Requirements: 2 3 1. if this attribute has already existed as an entity in other existing entity list , then do not include it . 4 2. the attribute candidate could be a mixed of values like ' color A and color B '. 5 3. The output should be a json parse-able format: 6 7 name (str): The name of the attribute . 8 importance_to_ask_score (float): The importance score of asking a question about this attribute to reduce the uncertainty of what the image is given the user prompt. This is a number between 0 and 1, higher means more important. Consider these factors when assigning scores: 1. Increate the score for attributes that are the primary attributes of an important entity; 2. significantly increase the score for attributes that could strongly influence the generation or portrayal of OTHER attributes in the scene; 3. descrease the score for attributes that are already well specified in the prompt. For example, a breed of a dog would impact other attributes like color , size , etc . So the breed attribute should have a higher importance score than color, size , etc . Assign a much lower score if the attribute ' s value is already mentioned in the user prompt. 9 candidates ( List of names and probabilities): List of possible values that the attribute can take. Make sure to generate atleast 5 or more possible values. These should be realistic for the given entity . For each attribute , returns the probability that the user wants this candidate based on the user prompt. If it ' s already mentioned by the user, only generate one candidate (the mentioned one) and assign 1.0 as the probability . The sum of probabilities over all candidates shall be 1. Also infer the probability based on the prompt. For example, for a dog with breed Samoyed, the color attribute has a very high probability of white. 10 11 Below are two examples of input and output pairs: 12 13 Example 1: 14 Input: {{ 15 "user_prompt": "generate an image of a white rabbit running on grass", 16</p><p>" entity ": " rabbit ", 17 " other_existing_entities ": "grass" <ref type="bibr">18 }} 19</ref> Output: [ 20 {{ 21 "name": "color ", 22 "importance_to_ask_score": 0.9, 23 "candidates": {{"white":1.0}} 24 }}, 25 {{ 26 "name": "breed", 27 "importance_to_ask_score": 1.0, 28 "candidates": {{"Dutch": 0.20, 29 "Mini Lop": 0.15, 30 "Netherland Dwarf": 0.15, 31 "Lionhead": 0.10, 32 "Flemish Giant": 0.10, 33 "Mini Rex": 0.10,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.11. Verbalization Prompt Instruction</head><p>1 The chat history is as follows : 2 question: {action. verbalized_action} and answer: {observation}.</p><p>3 Turn the question and action into a single declarative sentence that describes the answer -do not phrase it as a question. Example output: the firetruck in the image is red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.12. Merge Prompt Prompt Instruction</head><p>1 You are writing a prompt for a text-to-image model based on user feedback. The original prompt is {prompt}. The user has provided some additional information: { additional_info}. Please write a new prompt for the text-to-image model. The new prompt should be a meaningful sentence or a paragraph that combines the original prompt and the additional information. Do not add any new information that is not mentioned in the prompt or the additional information. Make sure the information in the original prompt is not changed. Make sure the additional information is included in the new prompt. Make sure the new prompt is a description of an image. If the additional information or the original prompt specifically says that a thing does not exist in the image, you should make sure the new prompt mentions that this thing does not exist in the image. DO NOT generate rationale or anything that is not part of a description of the image. 4 Each entity has "name", "descriptions ", "importance to ask score" and " probability of appearing". "Name" is the identifier of the entity . "Descriptions" is the description of the entity . "Importance to ask score" is how important it is for the agent to ask whether the entity exists . Probability of appearing" is the probability the agent estimated that this entity exits in the image. 5 6 Each entity has a list of attributes . Each attribute has "name", "importance to ask score" and "candidates". "Name" is the identifier of the attribute . "Importance to ask score" is how important it is to ask about the exact value for the attribute of the entity . "Candidates" is a list of possible values for the attribute . 7 8 Each candidate value has a probability that describes how likely this candidate value should be assigned to the attribute . 9 For example, "Attribute Name: color, Importance to ask Score: 0.9, Candidates: [white: 0.5, black: 0.5]" means the color is either white or black, each with 0.5 probability .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.13. 𝐴𝐼𝐶𝑄 𝑏𝑎𝑠𝑒 Prompt Instruction</head><p>If you ask about attributes , you should ask about the attribute with the highest uncertainty. Your uncertainty can be judged by the probabilities . If the probabilities are 0.5 and 0.5, you are uncertain. If the probabilities are 0.1 and 0.9, you are fairly certain . 10 11 The agent belief is : 12 { belief_state . __str__()} 13 14 Based on the user prompt "{user_prompt}" and the belief of the agent, please provide a question to ask about the image. The question should be as concise and direct as possible . The question should aim to obtain the most information about the style , entities , attributes , spatial layout and other contents of the image. Remember to ask for information that are critical to knowing the critical details of the image that is important to the user. The question should reduce your uncertainty about the user intent as much as possible. DO NOT ask question that can be answered by common sense. DO NOT ask question that are obvious to answer based on the user prompt "{user_prompt}". DO NOT ask any question about information present in the following user-agent dialogue within &lt;dialogue&gt; and &lt;/dialogue&gt; markers. 15 16 &lt;dialogue&gt; 17 {conversation} 18 &lt;/dialogue&gt; 19 20 DO NOT ask any question that has been asked in the dialogue above. 21 22 Your question does not have to be entirely decided by the belief . You can construct any question that make yourself more confident about what the image is. 23 Think step by step and reason about your uncertainty of the image to generate. Make sure to ask only one question. Make sure it is not very difficult for the user to answer.</p><p>For example, do not ask a very very long question, which can take the user a long time to read and answer. 24 Make sure that you question the answer within &lt;question&gt; and &lt;/question&gt; markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.15. HSA Question Prompt Instruction</head><p>1 You are constructing a text-to-image (T2I) prompt and want more details from the user. 2 You have to ask a question about the the most important entity or the attribute of the most important entity. 3 We have entity types: ( i ) explicit : directly ask question with options; ( ii ) implicit : ask whether this entity required for the image with yes or no as options; ( iii ) background: ignore the attribute value and directly ask the value of the entity . (iv) relation : add keyword like ' relation ' to emphasize this entity is a relation . 4 Construct a simple question that directly asks this information from the user and also provides option that the user can pick from. Ask only one question and follow it with options. 5 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Automated evaluation</head><p>In Algorithm 2, we show the user-agent self-play procedures that we used to perform all automated evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageInWords T2T DSG</head><p>Coco Captions T2T DSG</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DesignBench T2T DSG</head><p>Figure <ref type="figure" target="#fig_0">12</ref> | DSG score comparison between ground truth prompt and agent generated prompt reported at each turn. The performance of all agents increase with increase in number of turns.</p><p>Algorithm 2 User-Agent Self-Play Algorithm</p><p>1: Input: Initial prompt 𝑝 0 , User 𝑢, Agent 𝑎 (with 𝑝 0 ), 𝑚𝑎𝑥_𝑡𝑢𝑟𝑛𝑠 2: Output: Refined prompt 𝑝 𝑓 3: 𝑝 𝑓 ← 𝑝 0 4: for 𝑡𝑢𝑟𝑛_𝑖𝑑 = 0 to 𝑚𝑎𝑥_𝑡𝑢𝑟𝑛𝑠 -1 do 5: 𝑎𝑐𝑡𝑖𝑜𝑛 ← 𝑎.SelectAction() 6: 𝑞𝑢𝑒𝑠𝑡𝑖𝑜𝑛 ← 𝑎.VerbalizeAction(𝑎𝑐𝑡𝑖𝑜𝑛) 7: 𝑎𝑛𝑠𝑤𝑒𝑟 ← 𝑢.AnswerQuestion(𝑞𝑢𝑒𝑠𝑡𝑖𝑜𝑛) 8: 𝑎.Transition(𝑎𝑐𝑡𝑖𝑜𝑛, 𝑎𝑛𝑠𝑤𝑒𝑟) 9: 𝑝 𝑓 ← 𝑎. 𝑝𝑟𝑜𝑚𝑝𝑡 10: end for 11: return 𝑝 𝑓</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Details on the agent interface</head><p>Below is a showcase of how users could interact with the belief graph and clarifications in a hypothesised interface, to better iterate their inputs, to reach higher a quality and satisfaction of outputs. This is a crudely hypothesised, intentionally simple interface for the sake of research, but could be iterated and improved upon in many ways depending on application and users.</p><p>1. Default state On load of the app, there would be a text prompt input and space for output images, as is common across typical T2I interfaces. There would also be space for the user to view either clarifications from the model, or a graph interface, as part of the overall "input" section as these would act as a further input for future model output iterations. See Figure <ref type="figure" target="#fig_9">13</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Output images, with Clarifications</head><p>Once the user has submitted the prompt and the model has responded, there would be a set of images, as initial outputs from the users prompt. Below the input prompt would be a set of "Clarifications" in its populated state. These clarifications would ask the user specific questions that would be necessary to increase the specificity of the prompt, for the model to get a more accurate results aligned to the users intention, or to help the user realise their intention. Options would be given of the highest probability options for each Clarification, but the user could also fill in a totally new option via a free text field. Once answered by selection or text input, the clarifications would be added to the above, primary prompt for regeneration when the user selects. See Figure <ref type="figure" target="#fig_10">14</ref> below as reference. stream_c App INPUT A dominating image of the Eiffel Tower with the Olympic rings prominently displayed on the structure. Create CLARIFICATIONS GRAPH &lt;prev. 1 of 23 next&gt; Reload refresh Q: Where exactly on the Eiffel Tower the Olympic located? Top Middle Bottom Side Type here Q: What style is the Eiffel Tower depicted in? Photorealistic Digital painting Sketch 3D Render Type here Q: What is the angle from which the Eiffel Tower is depicted ? Front Side High Low angle Type here Q: What is the overall lighting condition in the image? Bright sunlight Overcast soft light Type here Image goes here Image goes here Image goes here goes here </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Entities &amp; Attributes</head><p>Instead of the clarifications, the user could select to instead view a Graph by clicking Tab above the clarifications themselves. This graph would be populated will all Entities from the prompt explicit and implicit visually defined differently (in this diagram by the dotted line surrounds implicit entities, but is a filled line when surrounding explicit entities). The graph layout will be structured, depicting relationships concentrically i.e. "on", "in" or "under" for example, will become child entities, and be displayed within the parent entities' boundary. For example a 'Mug' that has the relationship of 'on' a 'Table' entity, will sit within the boundary of 'Table ', as also would a 'Plate' if that had the same child-parent relationship.</p><p>Below the Graph would also be a list of 'cards' (i.e. boxed groups of information), one for each "explicit" or "implicit" entity. Within each card a user could see the status of implicit / explicit, and change this status to confirm or deny its presence. The user could also see a list of "attributes" associated to that entity, which the model has assumed. Each of these attributes could be changed by interacting with a list of alternatives via drop down. These lists are determined in terms of which items and order of items, based on the probability by which the model sees them, ordered with higest first. This probability would be made clear to the user to define the order by seeing the peercentage next to the label. See Figure <ref type="figure" target="#fig_4">15</ref> below as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Relationships</head><p>The user would also be able to change the state of the Graph and Cards,</p><p>stream_c App INPUT A dominating image of the Eiffel Tower with the Olympic rings prominently displayed on the structure. Create CLARIFICATIONS GRAPH Background entities 'Camera angle' 'Lighting' 'Daytime' 9. 10. 11. Physical Entities 'Eiffel Tower' 1. 4. 'Sky' (implicit) 'Olympic Rings' 'People' 6. 'Trees' (implicit) 7. 'Seine River' (implicit) 8. 'Clouds' (implicit) 5. 'Paris Cityscape' (implicit) 3. 10. 11. 12. 13. 14. 15. View: center_f Entities &amp; attributes swap_hor Relationships &lt;prev. 1 of 3 next&gt; 1. Eiffel Tower Explicit -100% conf. Description: Eiffel Tower, a prominent landmark in Paris, France" Existence: Yes check No Attributes: Material: Iron arrow_dr Height: Original arrow_dr Detail level: High arrow_dr State of repair: Well-maintained arrow_dr 2. Olympic Rings Explicit -100% Description: "The Olympic rings, a symbol of the Olympic Games" Existence: Yes check No Attributes: Size: Large arrow_dr Material: Metal arrow_dr Position: Near top arrow_dr Orientation: Horizontal arrow_dr 3. Paris Cityscape -82% conf. Description: "The surrounding urban landscape of Paris, France" Existence: Yes No Attributes: Weather: Sunny arrow_dr Time day: Midday arrow_dr Season: arrow_dr Architectural style emphasis: Haussmann arrow_dr OUTPUTS Image goes here Image goes here Image goes Image here Figure 15 | Interface with Graph displaying Entities, with cards below enabling a user to change attributes associated to each entity. to instead focus on the relationships between entities, by toggling to "Relations". In this state the user would be able to focus on two specific entities (e.g. 'mug' and 'table'), see the description of the relationship (e.g. 'the mug is sitting on the table') and if desired change the relationship to an alternative (e.g. 'on', changed to 'under') via a drop down of options which the model determined as alternative options ordered by probability, as per attributes. See Figure Figure 16 below as reference.</p><p>Once any of these changes are made the user could initiate a regeneration via the updated prompt to create a new set of output images, which can then be further refined via the same method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Details on user studies for the agent interface</head><p>Below we describe the exact guideline definitions we shared with the user for a user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.0.1. Hypothesized Frustrations</head><p>We presented participants with the following hypothesized frustrations related to T2I model usage:</p><p>1. Prompt Misinterpretation: The model misunderstands complex relationships between entities in the input prompt. 2. Many Prompt Iterations: The model does not immediately generate what the user intends, requiring numerous iterative changes to the input prompt. 3. Inconsistent Generations: The model reinterprets the input prompt differently between iterations, causing unwanted changes in the generated images. 4. Incorrect Assumptions: The model makes incorrect assumptions or no assumptions when encountering gaps in the details provided in the input prompt, leading to undesired outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph of Prompt Relationships:</head><p>A visual representation of relationships between entities in the prompt, allowing users to see and edit these relationships. E.g., seeing that "donut" is "next to" "coffee" and allowing the user to change the relationship to "on top of."</p><p>The questions asked for each feature were:</p><p>1. "How likely this feature is to help your current workflow if you had it now?". With response options of: "Very unlikely to help", "Unlikely to help", "Could help", "Likely to help", "Very likely to help". 2. "How soon would this feature deliver value to your work?" with response options of: "Very soon / immediately", "Sometime, "Not very soon".</p><p>Image references were given for each Feature as listed out below:</p><p>1. Clarifications: stream_c App INPUT A dominating image of the Eiffel Tower with the Olympic rings prominently displayed on the structure. Create Input clarifications (5) Q: Where exactly on the Eiffel Tower are the Olympic rings located? Top Middlecheck Bottom Side Q: What style is the Eiffel Tower depicted in? Photorealistic Digital painting Sketchcheck 3D Render Q: What is the angle from which the Eiffel Tower is depicted ? Front Side High angle Low anglecheck Q: What is the overall lighting condition in the image? bright sunlightcheck Overcast soft light Reload refresh OUTPUTS Image goes here Image goes here Image goes here Image goes here Figure 17 | Stimulus image in the survey to test the Model clarifications feature. 2. Graph of Prompt Entities: stream_c App INPUT A chocolate glazed donut topped with chopped nuts sits on a plate with a bite taken out of it, next to a full mug of coffee Create Input graph Context 'Plate' 'Donut' 'Nuts' 'Mug' 'Coffee' 1. 'In Kitchen' (assumed) 'Morning Light' (assumed) 'Kitchen' (assumed) 'Table' (assumed) 5. 6. 7. 8. 9. 4. 2. 3. Entities center_f Entities &amp; Attributes 6. Assumed entity: Table (94% conf.) Description: "a table on which the plate, donut, mug and coffee sit on" Existence: Yes check No Attributes: Shape: Round arrow_dr Size: Small arrow_dr Table cloth None arrow_dr Material: Wood arrow_dr Other items Empty arrow_dr 7.  5. Relation name: mug-table Description: "mug sitting on a table Spatial relationship: "on" arrow_dr 6. Relation name: plate-table Description: "plate sitting on a table" Spatial relationship: "on" arrow_dr Reload refresh OUTPUTS Image goes here goes here Image here Image goes here Figure 19 | Stimulus image in the survey to test the Model Graph of Entity Relations feature. Template of Human Rater Task 1: Evaluation of Issues in Individual a. 1, 2, equal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head><p>Task #1 (10 turns * 25 images) * 2 methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Dialogue Analysis:</head><p>Instructions: Pretend you are in the following scenario -you are asking an AI model to create an image you have in mind, this image is displayed on the left. You first prompt the model with a short non-detailed description of the image, this description is called "original prompt" and is below the goal image on the left.</p><p>The model proceeds to ask you various questions to understand the specificities of the image you are trying to create. The dialogue between you and the AI model is shown in the middle column. You will go through the dialog turn by turn and answer the same rater questions about each turn so just focus on the highlighted turn rather than the entire dialog.</p><p>Your job in this task is to rate the clarity, soundness and efficiency of the highlighted question in the dialogue. This question was asked by the AI model in order to generate an image similar to the one you had in mind. Provide your rating by answering the questions in the rightmost column. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Image Analysis:</head><p>Pretend you are in the following scenario -you asking an AI model to create an image you have in mind, this image is displayed on the left. You first prompt the model with a short non-detailed description of the image, this description is called "original prompt" and is below the goal image on the left.</p><p>The model proceeds to ask various questions to understand the specificities of the image you are trying to create. The dialogue between you and the AI model is shown in the middle column.</p><p>Based on the original prompt and the human and AI dialogue -the AI model produces a final image shown in the 3rd column.</p><p>Your job in this task is to rate the produced image based on how well it fits the goal image and separately how well it fits the prompt and the dialogue. Do this by answering the questions in the rightmost column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal Image Original Prompt</head><p>A fluffy gray and white cat with a butterfly on its face. Your job in this task is to compare the two model's produced images against each other given the goal image and original prompt. Do this by answering the questions in the rightmost column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal Image</head><p>Original Prompt A fluffy gray and white cat with a butterfly on its face. Rank which image matches best to the goal image in terms of style and aesthetics?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pilot Study</head><p>The pilot study will consist of 20 triplets of prompt, dialog and generated image. We aim to get 1 rater to rate each of these triplets. We will use the same 20 triplets for each of the 3 rater tasks outlined above, leading to a total of 60 tasks completed by single raters.</p><p>Figure <ref type="figure">22</ref> | A template of the task presented to human raters. Human raters are asked to rate the images produced by the three proposed multi-turn agents and a single-turn T2I model against a Ground Truth image for which the original prompt was derived and the answers to the agents questions were derived. Approximately 550 image-dialog pairs per agent are rated using 3 human raters. The generated images were presented in a random order and were unlabeled and the human rater was tasked with ranking the images from best to worst. The results from the study are shown in Figure <ref type="figure">4</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Belief Parsing and interaction 1: Input: Initial Prompt (IP) 2: Initialization: Merged Prompt (MP) ← IP 3: for 𝑡𝑢𝑟𝑛 ← 1 to 𝑚𝑎𝑥_𝑡𝑢𝑟𝑛 do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure 2 | a) Each column displays the output of an agent after 15 turns -the right most column shows target image, which belongs to DesignBench. b) A visualization of the multi-turn set up in the experiments. These are real generated outputs and simulated user outputs at turns 3, 10 and 15.</figDesc><graphic coords="9,63.50,253.64,209.54,82.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 |</head><label>3</label><figDesc>Figure 3 | ImageInWords results, including (a) T2T, (b) I2I, (c) T2I, (d) NLL scores. Agents trend to increase performance up to 10 turns.</figDesc><graphic coords="12,295.96,222.52,194.24,145.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Feature</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 |</head><label>5</label><figDesc>Figure 5 | Human ratings for the dialogues. The left graph shows the rating of how well the final generated image corresponds to the original prompt and dialogue. The right graph shows the distribution of issues of questions per agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Ag1Question:Figure 6 |</head><label>6</label><figDesc>Figure 6 | Real multi-turn dialogs generated by the Ag1, Ag2, and Ag3 agents on an image from DesignBench. The figure additionally shows the image generated after the 5 turn dialog per agent.</figDesc><graphic coords="23,131.87,90.18,132.97,305.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 |Figure 8 |Figure 9 |Figure 10 |</head><label>78910</label><figDesc>Figure 7 | Agent Generated Image Outputs on DesignBench: a chart of the generated image outputs of the four main Agent types in comparison to the goal image. Each column displays the output of a different agent and the right most column shows the goal image that the agents aimed to recreate.Each agent was provided with the same starting prompt and iterated for 15 turns, with the exception of the "T2I" agent column which produces an image from the starting prompt. Ag1, Ag2 and Ag3 refer to the Agents described in §E. Each agent uses the same T2I model to produce the final image. The goal images displayed here are from our DesignBench dataset described in the experiments section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 |</head><label>11</label><figDesc>Figure 11 | An example of the belief graph data structure for a given prompt in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>1 ... [[ Instruction for the first question]] ... 2 3 The original prompt was: {self.original_prompt} -Based on the original prompt please provide a question to ask about the image. The question should be as concise and direct as possible . The question should aim to learn more about the attributes and contents of the image, the objects , the spatial layout, and the style . Make sure that you question the answer within &lt;question&gt; and &lt;/question&gt; markers 4 5 ... [[ Instruction for the following question]] ... 6 7 Based on the chat history please provide a new question to ask about the image. the chat history is as follows and is enclosed in &lt;chat_history&gt; and &lt;/chat_history&gt; markers:{self.chat_history} &lt;/chat_history&gt; The question should be as concise and direct as possible. The question should aim to learn more about the attributes and contents of the image, the objects , the spatial layout, and the style . Make sure that you question the answer within &lt;question&gt; and &lt;/question&gt; markers.' E.14. 𝐴𝐼𝐶𝑄 𝐵 Prompt Instruction 1 You are an intelligent agent that helps users generate images. Before generating the image requested by the user, you should ask the most important clarification questions to make sure you understand the key features of the image. 2 The user describes the image as: {user_prompt}. 3 The following is your belief of what the image contains, including the entities , attributes of each entity and relations between entities .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 |</head><label>13</label><figDesc>Figure 13 | Default state of a possible interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 |</head><label>14</label><figDesc>Figure 14 | Interface once prompt has been input with clarifications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 |</head><label>18</label><figDesc>Figure 18 | Stimulus image in the survey to test the Model Graph of Entities and Attributes feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>name: donut-coffee Description: "donut next to a mug of coffee" Spatial relationship: "next to" arrow_dr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>Figure 20 | An example of the template presented to human raters. Human raters are asked to mark any issues a question contains that could pose a disturbance to the user. Approximately 8k questions per Agent are rated. The results are shown in Figure 5.</figDesc><graphic coords="44,94.07,351.57,128.06,128.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 21 |</head><label>21</label><figDesc>Figure 21 | An example of the template presented to human raters. Human raters are asked to rank the correspondence of each image to the agent-user dialog and original prompt. Approximately 1.5k image-dialog pairs are rated using 3 human raters. Results in Figure 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 |</head><label>1</label><figDesc>Automatic</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>T2T ↑</cell><cell cols="2">I2I (DINO) ↑ T2I (VQAScore)↑</cell><cell>NLL↓</cell><cell>DSG (T2T)↑</cell></row><row><cell></cell><cell>T2I</cell><cell cols="2">0.8757±.03 0.5170±.16</cell><cell>0.2976±.45</cell><cell cols="2">520.0645±161.3 0.5904±.05</cell></row><row><cell>Coco-Captions</cell><cell>Ag1 Ag2</cell><cell cols="2">0.9440±.02 0.6269±.12 0.9461±.02 0.6141±.13</cell><cell>0.5831±.49 0.6632±.46</cell><cell cols="2">508.4014±158.5 0.7555±.08 481.7224±154.5 0.8344±.08</cell></row><row><cell></cell><cell>Ag3</cell><cell cols="2">0.9501±.02 0.6575±.10</cell><cell>0.7751±.39</cell><cell cols="2">446.5679±151.8 0.9001±.05</cell></row><row><cell></cell><cell>T2I</cell><cell cols="2">0.8807±.02 0.5154±.15</cell><cell>0.3711±.47</cell><cell cols="2">459.9053±200.2 0.6815±.70</cell></row><row><cell>ImageInWords</cell><cell>Ag1 Ag2</cell><cell cols="2">0.9429±.02 0.5548±.15 0.9382±.02 0.5645±.15</cell><cell>0.5058±.48 0.5701±.48</cell><cell cols="2">449.8927±196.1 0.8162±.08 444.5227±193.7 0.8791±.07</cell></row><row><cell></cell><cell>Ag3</cell><cell cols="2">0.9418±.02 0.5875±.14</cell><cell>0.6624±.45</cell><cell cols="2">429.4636±194.5 0.9124±.06</cell></row><row><cell></cell><cell>T2I</cell><cell cols="2">0.8740±.02 0.5439±.12</cell><cell>0.3528±.48</cell><cell cols="2">320.8898±93.7 0.6074±.08</cell></row><row><cell>DesignBench</cell><cell>Ag1 Ag2</cell><cell cols="2">0.9365±.02 0.5943±.12 0.9384±.02 0.6417±.11</cell><cell>0.6848±.46 0.8553±.34</cell><cell cols="2">295.1974±69.2 0.8285±.08 271.2604±81.9 0.9181±.06</cell></row><row><cell></cell><cell>Ag3</cell><cell cols="2">0.9429±.02 0.6924±.12</cell><cell>0.9545±.21</cell><cell cols="2">257.4352±67.5 0.9485±.04</cell></row></table><note><p>evaluation results on Coco-Captions, ImageInWords, and DesignBench. Our agents show large performance gains in all metrics over a standard T2I model alone.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code and DesignBench can be found at https://github.com/google-deepmind/proactive_t2i_agents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Samples from the agent belief can be used to construct expanded prompts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The clarification question part of the interaction is omitted for simplicity</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This assumption only applies to the experiments. In practice, users don't necessarily have an image in mind, but they can get inspirations from the belief graphs and questions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>While 15 turns is a suggested approximation of interaction time, accounting for varying difficulty between images, any number of turns can be used with this evaluation approach.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://cloud.google.com/vertex-ai</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>Name is a unique identifier for the entity; Importance to ask score: A numerical value indicating the entity's perceived importance in satisfying the user's request. Entities with higher scores are prioritized during question generation, as they are likely to reduce uncertainty and contribute significantly to the final image; Description provides a textual description of the entity; probability of appearing estimates likelihood of the entity being present in the generated image; Attributes is for understanding the detailed attributes of the entities.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Jason Baldridge</rs> and <rs type="person">Zoubin Ghahramani</rs> for insightful discussions on multiturn <rs type="grantNumber">T2I</rs> and belief states, <rs type="person">Mahima Pushkarna</rs> for the help and consultation on user study. We would also like to thank <rs type="person">Richard Song</rs> and <rs type="person">Noah Fiedel</rs> for feedback on the paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AhY6JJB">
					<idno type="grant-number">T2I</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Author contributions</head><p>All authors contributed to writing.</p><p>• Meera Hahn (meerahahn@google.com): Led automated evaluation and human evaluation of agents. Created DesignBench. Contributed to agent design and development. • Wenjun Zeng (wenjunzeng@google.com): Significant contribution to agent design and development, as well as open-sourcing. • Nithish Kannen (nitkan@google.com): Significant contribution to experiments and early versions of agents. Led open-sourcing. • Rich Galt (richgalt@google.com): Led agent interface design and human studies.</p><p>• Kartikeya Badola (kbadola@google.com): Contributed to experiments and early versions of agents.</p><p>• Been Kim (beenkim@google.com): Advised project direction with critical feedback.</p><p>• Zi Wang (wangzi@google.com): Proposed and initiated project. Led agent design and development.</p><p>Advised project. Contributed to evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Novelty and contributions</head><p>In this section, we emphasize the novelty and contributions of this work.</p><p>1. System design of proactive T2I agents.</p><p>• Novel human-agent interaction modalities: Prior to our work, human users typically interact with current T2I systems by giving additional instructions or refining the prompt.</p><p>To the best of our knowledge, our work is the first to propose a proactive T2I agent system that is able to ask clarification questions and present its belief graph for the user to edit. • Novel human-agent interaction interface: We designed a new interface to best enable the clarification and belief graph interaction modalities. We have not seen these features in any T2I, or other generative media apps that are publicly live to date, signifying to us total uniqueness. Our human studies showed that at least 85% of raters found each component of the interface useful for their workflow, for us proving that these are both novel and useful. • Novel design of different T2I agents that enable the proposed interaction modalities. Please see §4 for the full details of the design principles and construction of those T2I agent prototypes (Ag1, Ag2, Ag3). 2. Our belief graphs significantly differs from classic belief states in the following ways:</p><p>• Hardcoded predicates v.s. Automatically-generated predicates: Traditionally, constructing classic symbolic belief states requires a pre-defined set of predicates such as "on(a, b)", "is_red(a)", "at_position(robot, x, y, z)" and it is non-trivial to learn new predicates that can be used and generalized to new tasks <ref type="bibr" target="#b65">(Pasula et al., 2007;</ref><ref type="bibr" target="#b87">Xia et al., 2019)</ref>. Typically the pre-defined set of predicates are written by system developers and hardcoded into classic AI systems <ref type="bibr" target="#b20">(Fikes and Nilsson, 1971)</ref>. Our belief graphs do not require any pre-defined predicates. Instead, we propose to construct symbolic beliefs using a sequential in-context learning (ICL) method with LLMs (Algorithm 1). Our method can be generalized across a wide range of T2I tasks and achieve high performance (see our comprehensive results on Coco, Imageinwords, DesignBench). • Applications: To the best of our knowledge, classic symbolic beliefs are mostly used for robot planning, and we are the first to use symbolic belief graphs to assist T2I tasks. • Data structure: Because of the application to planning, a symbolic world state is usually implemented and stored as a set or list of literals, i.e., atoms or negation of atoms where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Implementation of agent beliefs</head><p>Technically, an agent belief 𝑏 ∈ 𝐵 is represented in two complementary forms: (i) Merged prompt: This is a natural language representation that summarizes the entire conversation history up to the current turn. It provides a comprehensive textual overview of the user's requests, feedback, and any clarifications exchanged with the agent. (ii) Belief graph: This is a symbolic representation derived from the merged prompt. It parses the natural language text into a structured format, capturing key elements like entities, attributes, relationships, and associated probabilities. This structured representation facilitates more precise reasoning and decision-making by the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Merging. An LLM ( §E.11) summarizes the latest interaction, encapsulating the agent's question and the user's response into a concise textual representation. This step distills the essential information exchanged during the interaction. Another LLM ( §E.12) merges the existing merged prompt (containing the accumulated information from previous interactions) and the summarized interaction at the current turn. This creates an updated prompt that reflects the evolving understanding of the user's intent.</p><p>Belief Parsing. See an example of the belief graph fig. <ref type="figure">11</ref>. We employ three specialized parsers trained via in-context learning (ICL): entity parser ( §E.8) analyzes the user prompt to identify and extract a list of relevant entities.; attribute parser ( §E.9) takes user prompt and an entity as the input to extract a list of attributes associated with that entity; relation parser ( §E.10) takes the user prompt and a list of entities as input and identifies relationships between those entities. Each entity is associated with meta information like name, importance to ask score, description, probability of appearing, a list of attributes like color, position, etc 10 . Each attribute contains meta information like name, importance to ask score, a list of possible values for the attribute along with their associated probabilities, etc. Each relation includes meta information such as: name, description, spatial relation, importance to ask score, entity 1 and entity 2, whether the relation is bidirectional, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Implementation of action</head><p>From an information theoretic perspective, an optimal action is the one that maximizes the information gain between the observation and the belief, i.e. 𝑎 𝑡 = arg max 𝑎 𝐻 (𝑜 𝑖-1 ; 𝑏 𝑖-1 | 𝑎) -𝐻 (𝑜 𝑖 ; 𝑏 𝑖 | 𝑎). However, directly optimizing this objective can be computationally challenging. Therefore, we explore several heuristic strategies to effectively reduce uncertainty:</p><p>• Maximize the overall heuristic importance score (𝑀 𝐻 𝐼𝑆):This strategy focuses on maximizing the overall importance score of the entities, attributes, and relations within the belief. We further ask a question regarding an attribute or relation by maximizing the overall heuristic importance score. The score can be modeled as: </p><p>Here 𝐼𝑆, 𝑃, 𝐸𝑛𝑡 represents importance to ask score, probability of appearing, and entropy of the probabilities respectively and 𝑒, 𝑎, 𝑐, 𝑟 represents entity, attribute, candidate list, relation respectively. 'People' (implicit)</p><p>13.</p><p>(implicit) 14.</p><p>'Seine River' Explanations of terms were given to users of:</p><p>1. "Entities" are single items that are intended to be in the image e.g. "Cat" and "Ball", from "make a sketch of a Cat playing with a Ball" 2. "Prompt" means the text written to communicate the intended output image e.g. the sentence "make a sketch of a Cat playing with a Ball" is the "Prompt", also known as "Input" 3. "Iterations" are each set of different image outputs by the model, taken from a different input, or even the same input just regenerated</p><p>The question asked for each Frustration were: "Please score the below frustrations (or issues) that could be related to Text to Image AI Generation"."Rank in terms of how much they relate to your current usage, with your most commonly used model or app."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.0.2. Hypothesized Features</head><p>We proposed following features as potential solutions to address the identified frustrations:</p><p>1. Clarifications: The model would ask specific clarifying questions about uncertainties in the prompt. These details would then be incorporated into subsequent iterations. For example: "Is the cat playing with: 1. a ball of wool, or 2. a tennis ball?" 2. Graph of Prompt Entities: A visual representation of all entities in the prompt as a graph, allowing users to see and edit attributes of each entity. E.g., seeing that the model has assigned "round," "small," and "wooden" as attributes to "table" and allowing the user to change them to "square" and "metal."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.0.3. Human Study Results</head><p>Table <ref type="table">3</ref> details the T2I usage frequency of the human subjects. Table <ref type="table">4</ref> shows the percentage of human subjects that reported different kinds of frustrations in their experience of using T2I. Table <ref type="table">5</ref> summarizes the results on expected speed of value delivered from different features of our agent prototypes. These results highlight the impact of our contributions. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alkhazraji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frorath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grützner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Helmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liebetraut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName><surname>Wülfing</surname></persName>
		</author>
		<author>
			<persName><surname>Pyperplan</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3700819</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3700819" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3700819</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guidelines for human-ai interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration tradeoffs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brichtova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.07009</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Imagen 3. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving image generation with better captions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/papers/dall-e-3.pdf" />
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human-centered tools for coping with imperfect algorithms during medical decision-making</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian experimental design: A review</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaloner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Verdinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="page" from="273" to="304" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Do models explain themselves? counterfactual simulatability of natural language explanations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.08678" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Designing a dashboard for transparency and control of conversational ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Depodesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Seow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07882</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Zai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01388</idno>
		<title level="m">Crafting consistent subjects in multi-turn interactive image generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.18919</idno>
		<idno>arXiv:2310.18235</idno>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Theatergen: Character management with llm for consistent multi-turn image generation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.18235" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal or Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prompt expansion for adaptive text-to-image generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.acl-long.189" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024-08">Aug. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3449" to="3476" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Theories of meaning and learnable languages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1964 International Congress for Logic, Methodology, and Philosophy of Science</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bar-Hillel</surname></persName>
		</editor>
		<meeting>the 1964 International Congress for Logic, Methodology, and Philosophy of Science</meeting>
		<imprint>
			<publisher>North-Holland Publishing</publisher>
			<date type="published" when="1965">1965</date>
			<biblScope unit="page" from="383" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Truth and meaning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophy, Language, and Artificial Intelligence: Resources for Processing Natural Language</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="93" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The logical form of action sentences. The Logic of Decision and Action</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>best of the web</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strips: A new approach to the application of theorem proving to problem solving</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Imageinwords: Unlocking hyper-detailed image descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.02793" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PDDLstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Planning and Scheduling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online replanning in belief space for partially observable task and motion problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.05530" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="483" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reasoning about action i: A possible worlds approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Ginsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="195" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the complexity of blocks-world planning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Nau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="223" to="254" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dig in: Evaluating disparities in image generations with indicators for geographic diversity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Soriano</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.06198" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Entropy search for information-efficient global optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1809" to="1837" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Clipscore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Steps to take before intelligent user interfaces become real</title>
		<author>
			<persName><forename type="first">K</forename><surname>Höök</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interacting with computers</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="426" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Instruct-imagen: Image generation with multi-modal instruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08857</idno>
		<title level="m">Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Underspecification in scene description-to-depiction tasks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical task and motion planning in the now</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal or Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Beyond aesthetics: Cultural competence in text-to-image models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dave</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.06863" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">help me help the ai&quot;: Understanding how explainability can support human-ai interaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monroy-Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Experimental design. The SAGE Handbook of Quantitative Methods in Psychology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kirk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="23" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pick-a-pic: An open dataset of user preferences for text-to-image generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kirstain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matiana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fluids Engineering</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Mini-DALLE3: Interactive text to image by prompting large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.07653" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Measuring faithfulness in chain-of-thought reasoning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lanham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lukošiūtė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Telleen-Lawton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brauner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.13702" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11404</idno>
		<title level="m">The artbench dataset: Benchmarking generative models with artworks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Evaluating text-to-visual generation with image-to-text generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.01291" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Some philosophical problems from the standpoint of artificial intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence 4</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Meltzer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</editor>
		<imprint>
			<publisher>Edinburgh University Press</publisher>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="463" to="502" />
		</imprint>
	</monogr>
	<note>reprinted in McC90</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A framework for representing knowledge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<ptr target="https://dspace.mit.edu/bitstream/handle/1721.1/6089/AIM-306.pdf" />
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
		<respStmt>
			<orgName>A.I. Laboratory, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Society of Mind. Simon and Schuster</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On Bayesian methods for seeking the extremum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mockus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Techniques IFIP Technical Conference</title>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A formal theory of knowledge and action. Formal theories of the commonsense world</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="319" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.09797" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Probabilistic logic</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="87" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The Quest for Artificial Intelligence</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">How might people interact with agents</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="68" to="71" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.07193" />
	</analytic>
	<monogr>
		<title level="m">Learning robust visual features without supervision</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning symbolic models of stochastic domains</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Pasula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal or Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="309" to="352" />
			<date type="published" when="2007">2007</date>
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A survey of deep active learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Artificial intelligence: A modern approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Computer Sciences Technical Report</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Building a conversational agent overnight with dialogue self-play</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.04871" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dsg-gan: Multi-turn text-to-image synthesis via dual semantic-stream guidance with global and local linguistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2023">200271. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Bradford Book</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.02469</idno>
		<title level="m">The system model and the user model: Exploring ai dashboard design</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Artwhisperer: A dataset for characterizing human-ai interactions in artistic creations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vodrahalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Llmcheckup: Conversational examination of large language models via interpretability tools and self-explanations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anikina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feldhus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Genabith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Möller</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.12576" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Max-value entropy search for efficient Bayesian optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Active model learning and diverse action sampling for task and motion planning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4107" to="4114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image generation with style image constraint</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3291" to="3303" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Pre-trained gaussian processes for bayesian optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">212</biblScope>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Gaussian process probes (GPP) for uncertaintyaware probing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2024">2024c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Powerful and flexible: Personalized text-to-image generation via reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.06642</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Chainof-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.11903" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Larger language models do in-context learning differently</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.03846" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Revisiting text-to-image evaluation with gecko</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kajić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knutsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16820</idno>
	</analytic>
	<monogr>
		<title level="m">On metrics, prompts, and human ratings</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning sparse relational transition models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Imagereward: Learning and evaluating human preferences for text-to-image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Re-examining whether, why, and how human-ai interaction is uniquely difficult to design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">The unreliability of explanations in few-shot prompting for textual reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.03401" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-to-image generation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2327" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Scaling autoregressive multi-modal models: Pretraining and instruction tuning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Golovneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheynin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02591</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Mini-dalle3: Interactive text to image by prompting large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zeqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xizhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jifeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07653</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Alignscore: Evaluating factual consistency with a unified alignment function, 2023. 11 12 The output should be list and each entry should be</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>formated as a JSON dict with the following fields : 13 14 &quot;name&quot;: The name of the entity</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Make sure that this is a number between 0 and 1, higher means more important. Consider these factors when assigning scores: 1. Increate the score for entities that are the primary focus or subject of the prompt; 2. increase the score for entities that could strongly influence the layout of the image, such as the position or portrayal of other entities in the scene; 3. significantlydescrease the score for entities that are already well specified in the prompt; 4. significantlyincrease the score for implicit entities that are likely to appear in the image and their appearance can significantly impact the image</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>importance_to_ask_score&quot;: The importance score of asking a question about this entity to reduce the uncertainty of what the image is given the user prompt A short description of the entity . 17 &quot;entity_type &quot;: The type of this entitiy . It could be either explicit , implicit , background. No other value is allowed</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">The probability of the entity appearing in the image. This is a number between 0 and 1. You should assign a probability with the following rules in mind</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">If the prompt says an entity does not exist , assign a 0.0 probability . Because the entity does not exist</title>
		<imprint/>
	</monogr>
	<note>you should also assign 0 to importance_to_ask_score of this entity . 20 2. If the prompt indicates an entity definitly exists in the image, assign a 1.0 probability</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">If the prompt does not say anything about the existence of the entity , assign a probability between 0 and 1. This probability is higher if the entity is more likely</title>
		<imprint/>
	</monogr>
	<note>to appear in the image given the context specified by the prompt</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">If the prompt says an entity exists but there is an indication that the entity is not likely to appear in the image, assign a probability between 0 and 1, higher if the entity is more likely</title>
		<imprint/>
	</monogr>
	<note>to appear in the image</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">24 Below is an example input and output pair: 25 Example1: 26 Input: {{ 27 &quot;user_prompt&quot;: &quot;generate an image of a lionhead rabbit running on grass with sun shining. There is no trees in the background</title>
		<idno>28 }} 29 Output: [ 30 {{ 31 &quot;name</idno>
		<imprint/>
	</monogr>
	<note>rabbit &quot;, 32 &quot;importance_to_ask_score&quot;: 0.5, 33 &quot; description &quot;: &quot;a lionhead rabbit &quot;, 34 &quot;entity_type &quot;: &quot; explicit &quot;, 35 &quot;probability_of_appearing&quot;: 1.0</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title/>
		<author>
			<persName><surname>}}</surname></persName>
		</author>
		<idno>37 {{ 38</idno>
		<imprint/>
	</monogr>
	<note>name&quot;: &quot;grass &quot;, 39 &quot;importance_to_ask_score&quot;: 0.5, 40 &quot; description &quot;: &quot;grass &quot;, 41 &quot;entity_type &quot;: &quot; explicit &quot;, 42 &quot;probability_of_appearing&quot;: 1.0</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName><surname>}}</surname></persName>
		</author>
		<idno>44 {{ 45</idno>
		<imprint/>
	</monogr>
	<note>name&quot;: &quot;sun&quot;, 46 &quot;importance_to_ask_score&quot;: 0.1, 47 &quot; description &quot;: &quot;sun is shining &quot;, 48 &quot;entity_type &quot;: &quot; explicit &quot;, 49 &quot;probability_of_appearing&quot;: 0.3</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName><surname>}}</surname></persName>
		</author>
		<idno>51 {{ 52 &quot;name&quot;: &quot;sun light</idno>
		<imprint/>
	</monogr>
	<note>53 &quot;importance_to_ask_score&quot;: 0.1, 54 &quot; description &quot;: &quot;sun light shining on the grass and the rabbit &quot;, 55 &quot;entity_type &quot;: &quot; explicit &quot;, 56 &quot;probability_of_appearing&quot;: 1.0 34 &quot;English Angora&quot;: 0.08, 35 &quot;Mini Satin &quot;: 0.05, 36 &quot;Himalayan&quot;: 0.05, 37 &quot; Californian &quot;: 0.02}}</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName><surname>}}</surname></persName>
		</author>
		<idno>39 {{ 40 &quot;name&quot;: &quot;age</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
	<note>41 &quot;importance_to_ask_score&quot;: 0.1, 42 &quot;candidates&quot;: {{&quot;adult &quot;: 0.6, 43 &quot;baby&quot;: 0.2, 44 &quot;senior &quot;: 0.2}} 45 }} 46 a few additional examples]] ..</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">50 Generate attributes given the input given below. Do not include other entities in the attributes . Strictly stick to the format</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
		</imprint>
	</monogr>
	<note>Input: {{ 52 &quot;user_prompt&quot;: &quot;{user_prompt}&quot;, 53 &quot; entity &quot;: &quot;{ entity .name}&quot;, 54 &quot; other_existing_entities &quot;: &quot;{ existing_entities }&quot; 55 }} 56 Output: E.10. Relation Parse Prompt Instruction</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Given a text-to-image prompt and a list of entity described in the prompt, your goal is to identify a list of entity pairs that have relations between them. Ignore entity pairs without relations . The output should be a json parse-able format</title>
		<imprint/>
	</monogr>
	<note>No comma after the last element of the list</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">The name of the relation. Use &apos; entity1-entity2&apos; as the format</title>
		<imprint/>
	</monogr>
	<note>Output: 8 name (str</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A short description of the relation . 10 spatial_relation (map from potential relation candidates to probability ): Possible spatial relations between the two entities . If a relation is mentioned in the user prompt, assign 1.0 as the probability . The sum of probabilities over all relation candidates shall be 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">This is a number between 0 and 1, higher means more important. Assign a higher score if the two entities are very important, the relation between them is very unclear, and the relation is very important for the layout of the image. 12 name_entity_1 (str): The name of the first entity . 13 name_entity_2 (str): The name of the second entity. 14 is_bidirectional (bool): Whether the relation is bidirectional . 15 16 Below is an example input and output pair: 17 Example1: 18 Input: {{ 19 &quot;user_prompt&quot;: &quot;generate an image of a lionhead rabbit sitting on grass , and a eagle is flying through the sky</title>
		<imprint/>
	</monogr>
	<note>11 importance_to_ask_score (float): The importance score of asking a question regarding this relation to reduce entropy There is a tree in the background.&quot;, 20 &quot; entity &quot;: [&quot; rabbit &quot;, &quot;grass &quot;, &quot;eagle &quot;, &quot; tree &quot;</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title/>
		<author>
			<persName><surname>}}</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="50" to="51" />
		</imprint>
	</monogr>
	<note>a few additional examples</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
